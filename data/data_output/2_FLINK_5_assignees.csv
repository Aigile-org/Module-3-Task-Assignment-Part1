id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13308032	It will throw Invalid lambda deserialization Exception when writing to elastic search with new format	"My job follows:
{code:java}
// 
create table csv( pageId VARCHAR, eventId VARCHAR, recvTime VARCHAR) with ( 'connector' = 'filesystem',
 'path' = '/Users/ohmeatball/Work/flink-sql-etl/data-generator/src/main/resources/user3.csv',
 'format' = 'csv'
 )
-----------------------------------------
CREATE TABLE es_table (
  aggId varchar ,
  pageId varchar ,
  ts varchar ,
  expoCnt int ,
  clkCnt int
) WITH (
'connector' = 'elasticsearch',
'hosts' = 'http://localhost:9200',
'index' = 'cli_test',
'document-id.key-delimiter' = '$',
'sink.bulk-flush.interval' = '1000',
'format' = 'json'
)
-----------------------------------------
INSERT INTO es_table
SELECT  pageId,eventId,cast(recvTime as varchar) as ts, 1, 1 from csv;
{code}
The full exception follows:
{code:java}
Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:471) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:459) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:155) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]Caused by: java.io.IOException: unexpected exception type at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1682) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1254) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
Caused by: java.lang.reflect.InvocationTargetExceptionCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreCaused by: java.lang.IllegalArgumentException: Invalid lambda deserialization at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink$Builder.$deserializeLambda$(ElasticsearchSink.java:80) ~[flink-sql-connector-elasticsearch7_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151]
 at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
{code}
Notice: everything works fine with former connector grammer."	FLINK	Closed	1	1	3568	pull-request-available
13187210	Add skip to next strategy	Add skip to next strategy, that should discard all partial matches that started with the same element as found match.	FLINK	Resolved	3	4	3568	pull-request-available
13391141	Add a global flag for enabling/disabling final checkpoints	We should have a feature toggle for the final checkpoint story.	FLINK	Closed	3	7	3568	pull-request-available
13340904	Avro Confluent Registry SQL format does not support adding nullable columns	"The {{AvroSchemaConverter#convertToSchema}} generates a union with ""null"" for nullable logical types, but it does not set the default value to null. In turn it makes it impossible to generate a backwards compatible schema from a DDL statement.

Example:
1. Create a table: {{CREATE TABLE t (id INT NOT NULL) WITH (/* avro confluent format*/)}}
2. Create a new table over the same topic or alter the old table with {{CREATE TABLE newT(id INT NOT NULL, optionalDescription STRING) WITH (/* avro confluent format */)}}
3. When reading from {{newT}} records inserted into {{t}} it will fail, because the {{optionalDescription}} has no default value."	FLINK	Closed	2	1	3568	pull-request-available
13248587	Include table examples in flink-dist	"We want to treat the table api as first-class API. We already included in the lib directory flink.
We should also include some examples of the table api in the distribution.

Before that we should strip all the dependency and just include the classes from  example module."	FLINK	Closed	2	4	3568	pull-request-available
13550488	SupportsReadingMetadata is not applied when loading a CompiledPlan	"If a few conditions are met, we can not apply ReadingMetadata interface:
# source overwrites:
 {code}
    @Override
    public boolean supportsMetadataProjection() {
        return false;
    }
 {code}
# source does not implement {{SupportsProjectionPushDown}}
# table has metadata columns e.g.
{code}
CREATE TABLE src (
  physical_name STRING,
  physical_sum INT,
  timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL
)
{code}
# we query the table {{SELECT * FROM src}}

It fails with:
{code}
Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)
{code}

The reason is {{SupportsReadingMetadataSpec}} is created only in the {{PushProjectIntoTableSourceScanRule}}, but the rule is not applied when 1 & 2"	FLINK	Closed	3	1	3568	pull-request-available
13437148	JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate failed on azure	"
{code:java}
2022-03-31T06:11:52.2333685Z Mar 31 06:11:52 [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 35.288 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase
2022-03-31T06:11:52.2336004Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate  Time elapsed: 15.008 s  <<< FAILURE!
2022-03-31T06:11:52.2336907Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2337353Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2337876Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2338631Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2339436Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2340599Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate(JobMasterStopWithSavepointITCase.java:136)
2022-03-31T06:11:52.2342251Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2342896Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2343608Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2344234Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2344873Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2345590Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2346498Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2347221Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2347922Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2348580Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2349222Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2349860Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2350502Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2351172Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2352095Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2352949Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2353643Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2354298Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2354909Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2355535Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2356505Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2357142Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2357771Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2358400Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2359014Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2359614Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2360221Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2371694Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2372907Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2373992Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2375195Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2376592Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2377778Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2379338Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2380786Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2382151Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2383487Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2384979Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2386341Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2387454Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2389081Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2390447Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2391930Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2393389Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2394759Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2395544Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2396673Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2397347Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2397932Z Mar 31 06:11:52 
2022-03-31T06:11:52.2398639Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend  Time elapsed: 15.004 s  <<< FAILURE!
2022-03-31T06:11:52.2399342Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2399793Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2400311Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2400837Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2401633Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2402751Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend(JobMasterStopWithSavepointITCase.java:130)
2022-03-31T06:11:52.2403623Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2404247Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2404961Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2405936Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2406676Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2407520Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2408242Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2409245Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2409940Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2410604Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2411358Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2412174Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2412786Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2413640Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2414856Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2416140Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2417502Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2418495Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2419110Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2419737Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2420361Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2420986Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2421601Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422359Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422969Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2423569Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2424331Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2424922Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2425464Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2426334Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2427379Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2428432Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2429538Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2430713Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2431900Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2433166Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2434372Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2435500Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2436771Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2437877Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2439206Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2440452Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2441694Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2442881Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2443999Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2445104Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2446367Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2447434Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2448170Z Mar 31 06:11:52 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5183"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13374114	Unnecessary entries in sql hbase-1.4 connector NOTICE file	"The NOTICE file for flink-sql-connector-hbase-1.4 lists dependencies that it does not bundle:

* commons-configuration:commons-configuration:1.7
* org.apache.hbase:hbase-prefix-tree:1.4.3
* org.apache.hbase:hbase-procedure:1.4.3"	FLINK	Closed	2	7	3568	pull-request-available
13296468	Support creating tables using other tables definition	We should be able to create a Table based on properties of other tables. This includes merging the properties and creating a new Table based on that.	FLINK	Closed	3	7	3568	pull-request-available
13364698	NullPointerException on restore in PojoSerializer	"As originally reported in [thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Schema-Evolution-Cannot-restore-from-savepoint-after-deleting-field-from-POJO-td42162.html], after removing a field from a class restore from savepoint fails with the following exception:

{code}
2021-03-10T20:51:30.406Z INFO  org.apache.flink.runtime.taskmanager.Task:960 … (6/8) (d630d5ff0d7ae4fbc428b151abebab52) switched from RUNNING to FAILED. java.lang.Exception: Exception while creating StreamOperatorStateContext.
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:253)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:901)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:415)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedCoProcessOperator_c535ac415eeb524d67c88f4a481077d2_(6/8) from any of the 1 provided restore options.
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:307)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:135)
        ... 6 common frames omitted
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
        at org.apache.flink.runtime.state.memory.MemoryStateBackend.createKeyedStateBackend(MemoryStateBackend.java:347)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:291)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
        ... 8 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.<init>(PojoSerializer.java:123)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:186)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:56)
        at org.apache.flink.api.common.typeutils.CompositeSerializer$PrecomputedParameters.precompute(CompositeSerializer.java:228)
        at org.apache.flink.api.common.typeutils.CompositeSerializer.<init>(CompositeSerializer.java:51)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializer.<init>(TtlStateFactory.java:250)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:359)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:330)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546)
        at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
        at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:505)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.snapshotsToRestoreSerializers(NestedSerializersSnapshotDelegate.java:225)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.getRestoredNestedSerializers(NestedSerializersSnapshotDelegate.java:83)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at org.apache.flink.runtime.state.StateSerializerProvider.previousSchemaSerializer(StateSerializerProvider.java:189)
        at org.apache.flink.runtime.state.StateSerializerProvider.currentSchemaSerializer(StateSerializerProvider.java:164)
        at org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.getStateSerializer(RegisteredKeyValueStateBackendMetaInfo.java:136)
        at org.apache.flink.runtime.state.heap.StateTable.getStateSerializer(StateTable.java:315)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:54)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:36)
        at org.apache.flink.runtime.state.heap.StateTable.<init>(StateTable.java:98)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.<init>(CopyOnWriteStateTable.java:49)
        at org.apache.flink.runtime.state.heap.AsyncSnapshotStrategySynchronicityBehavior.newStateTable(AsyncSnapshotStrategySynchronicityBehavior.java:41)
        at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.newStateTable(HeapSnapshotStrategy.java:243)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.createOrCheckStateForMetaInfo(HeapRestoreOperation.java:185)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:152)
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
        ... 12 common frames omitted
{code}
"	FLINK	Closed	1	1	3568	pull-request-available
13313439	Can not select fields with JdbcCatalog	"A query which selects fields from a table will fail if we set the PostgresCatalog as default.

Steps to reproduce:
 # Create postgres catalog and set it as default
 # Create any table (in any catalog)
 # Query that table with {{SELECT field FROM t}} (Important it must be a field name not '{{*}}'
 #  The query will fail

Stack trace:
{code}
org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement.
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:100) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:91) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:257) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:211) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. null
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
Caused by: java.lang.UnsupportedOperationException
	at org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.getFunction(AbstractJdbcCatalog.java:261) ~[?:?]
	at org.apache.flink.table.catalog.FunctionCatalog.resolvePreciseFunctionReference(FunctionCatalog.java:570) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lambda$resolveAmbiguousFunctionReference$2(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
	at org.apache.flink.table.catalog.FunctionCatalog.resolveAmbiguousFunctionReference(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:370) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:99) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:73) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.makeNullaryCall(SqlValidatorImpl.java:1754) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5987) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6154) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6140) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectExpr(SqlValidatorImpl.java:5574) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:452) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4255) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3523) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
{code}

The problem is that Calcite will try to check first if there is a built-in function with that name that allows calls without parenthesis. Therefore it will query the {{FunctionCatalog}} for that function. The logic in {{org.apache.flink.table.catalog.FunctionCatalog#lookupFunction}} is such that it will call {{JdbcCatalog#getFunction}} in the end, which in case of {{AbstractJdbcCatalog}} throws {{UnsupportedOperationException}}.
"	FLINK	Closed	1	1	3568	pull-request-available
13285020	Introduce a Java Expression DSL	Introduce the basic expressions. The new Java expression DSL should be feature equivalent to string based expression parser. It does not support calls with new inference stack yet.	FLINK	Closed	3	7	3568	pull-request-available
13317135	Docker e2e tests are failing on CI	"{code}
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=host&nocache=1&rm=1&session=z0y5c0io3wt7m3uqfb7zo7uds&shmsize=0&t=test_docker_embedded_job&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied
{code}
Will have to wait for [~rmetzger] to get back."	FLINK	Closed	3	4	3568	pull-request-available
13240421	Convert CatalogView to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	"Similar to [FLINK-12257] we should convert Flink's views to Calcite's views.

The tricky part is that we have to pass around the SqlParser somehow."	FLINK	Closed	3	7	3568	pull-request-available
13341804	Building flink-dist docker image does not work without python2	"The script {{common_docker.sh}} in function {{start_file_server}} tests existence of {{python3}}, but executes command using {{python}}:

{code}
    command -v python3 >/dev/null 2>&1
    if [[ $? -eq 0 ]]; then
      python ${TEST_INFRA_DIR}/python3_fileserver.py &
      return
    fi
{code}

The script {{python3_fileserver.py}} uses python2 {{SocketServer}} which does not exist in python3. It should use {{socketserver}}."	FLINK	Closed	2	1	3568	pull-request-available
13236795	Introduce Table API Planner interface	The planner interface is the bridge between base API and different planner modules. 	FLINK	Closed	3	4	3568	pull-request-available
13363745	DegreesWithExceptionITCase crash	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14422&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b

"	FLINK	Closed	1	7	3568	pull-request-available, test-stability
13141178	Scala shell broken for Flip6	"I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?

I followed the instructions from the Flink master branch so I did the following.
{code:java}
git clone https://github.com/apache/flink.git 
cd flink mvn clean package -DskipTests 
cd build-target
 ./bin/start-scala-shell.sh local{code}
{{And Here is the code I ran}}
{code:java}
val dataStream = senv.fromElements(1, 2, 3, 4)
dataStream.countWindowAll(2).sum(0).print()
senv.execute(""My streaming program""){code}
{{And I finally get this exception}}
{code:java}
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281){code}
 "	FLINK	Resolved	1	1	3568	pull-request-available
13583654	JSON_QUERY should return a well formatted nested objects/arrays for ARRAY<STRING>	"{code}
SELECT JSON_QUERY('{""items"": [{""itemId"":1234, ""count"":10}]}', '$.items' RETURNING ARRAY<STRING>)
{code}

returns

{code}
['{itemId=1234, count=10}']
{code}

but it should:

{code}
['{""itemId"":1234, ""count"":10}']
{code}

We should call jsonize for Collection types here: https://github.com/apache/flink/blob/f6f88135b3a5fa5616fe905346e5ab6dce084555/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/SqlJsonUtils.java#L268"	FLINK	Closed	3	1	3568	pull-request-available
13324973	Remove deprecated methods in ExecutionConfig	"We can remove:

- ExecutionConfig#isLatencyTrackingEnabled (deprecated in 1.7) 

Additionally, we should remove no-ops methods in ExecutionConfig.

    - ExecutionConfig#disable/enableSysoutLogging (deprecated in 1.10)
    - ExecutionConfig#set/isFailTaskOnCheckpointError (deprecated in 1.9) 

They are {{Public}}, however they became no-op operations, which can be argued already broke the stability guarantees."	FLINK	Closed	3	7	3568	pull-request-available
13556216	Implement type inference for reinterpret_cast function	https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/Reinterpret.scala	FLINK	Closed	3	7	3568	pull-request-available
13231976	Port utility methods for extracting fields information from TypeInformation	We need those methods in the api-module in order to create {{Table}} out of {{DataSet/Stream}}.	FLINK	Closed	3	4	3568	pull-request-available
13301012	flink legacy planner should not use CollectionEnvironment any more	"As discussed in https://github.com/apache/flink/pull/11794，{{CollectionEnvironment}} is not a good practice, as it is not going through all the steps that a regular user program would go. We should change the tests to use {{LocalEnvironment}}. 

commit ""Introduce CollectionPipelineExecutor for CollectionEnvironment ([c983ac9|https://github.com/apache/flink/commit/c983ac9c49b7b58394574efdde4f39e8d33a8582])""  should also be reverted at that moment."	FLINK	Closed	3	4	3568	pull-request-available
13380486	Performance regression on 25.05	"Tests such as:
* multiInputMapSink
* multiInputOneIdleMapSink
* readFileSplit

show regressions.

Regression in run for range: 80ad5b3b51-bb597ea-1621977169

It is most probably caused by: https://github.com/apache/flink/commit/ee9f9b227a7703c2688924070c4746a70bff3fd8"	FLINK	Closed	3	4	3568	pull-request-available
13572433	Parsing temporal table join throws cryptic exceptions	"1. Wrong expression type in {{AS OF}}:
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF 'o.rowtime' AS r "" +
      ""ON o.currency = r.currency
{code}

throws: 

{code}
java.lang.AssertionError: cannot convert CHAR literal to class org.apache.calcite.util.TimestampString
{code}

2. Not a simple table reference in {{AS OF}}
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF o.rowtime + INTERVAL '1' SECOND AS r "" +
      ""ON o.currency = r.currency
{code}

throws:
{code}
java.lang.AssertionError: no unique expression found for {id: o.rowtime, prefix: 1}; count is 0
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13434368	TimestampsAndWatermarksOperator should not propagate WatermarkStatus	The lifecycle/scope of WatermarkStatus is tightly coupled with watermarks. Upstream watermarks are cut off in the TimestampsAndWatermarksOperator and therefore watermark statuses should be cut off as well.	FLINK	Closed	3	1	3568	pull-request-available
13269264	Use higher granularity units in generated docs for Duration & MemorySize if possible	"It was mentioned on two occasions (https://github.com/apache/flink/pull/10216#discussion_r346866339, https://github.com/apache/flink/pull/10217/files#r347491314) that it would be better to use a higher granularity units if it is possible.

Right now for a default value of {{Duration.ofMinutes(1)}} the generated documentation will be printed as:

{{60000ms}}

but it would be better readable to print it as:

{{1min}}"	FLINK	Closed	3	4	3568	pull-request-available
13391350	InputStatus should not contain END_OF_RECOVERY	"We added the END_OF_RECOVERY enum value in order to support recovery of unaligned checkpoints with rescaling.

However the InputStatus is expose in a public interface via {{SourceReader}}. At the same time it is not a valid value which the {{SourceReader}} can return.

We should internally replace the InputStatus with an internal equivalent."	FLINK	Closed	3	1	3568	pull-request-available
13300004	Add open method to DeserializationSchema	"Additionally add support for it in connectors:
* Kafka
* PubSub
* RabbitMQ
* Kinesis"	FLINK	Closed	3	7	3568	pull-request-available
13263712	Update documentation regarding Temporary Objects	"* update references to deprecated methods
* describe the concept of temporary tables"	FLINK	Closed	3	7	3568	pull-request-available
13596954	Support LEAD/LAG functions in Table API	We should natively support LAG/LEAD functions in Table API.	FLINK	Open	3	4	3568	pull-request-available
13304842	TableEnvironment fromValues not work with map type and SQL	"{code:java}
Map<Integer, Integer> mapData = new HashMap<>();
      mapData.put(1, 1);
      mapData.put(2, 2);
      Row row = Row.of(mapData);
      tEnv().createTemporaryView(""values_t"", tEnv().fromValues(Collections.singletonList(row)));
      Iterator<Row> iter = tEnv().executeSql(""select * from values_t"").collect();

      List<Row> results = new ArrayList<>();
      while (iter.hasNext()) {
         results.add(iter.next());
      }
      System.out.println(results);
{code}
Not work, will occur exception:
{code:java}
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP f0) NOT NULL
converted type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP NOT NULL f0) NOT NULL
{code}
If change to {{Iterator<Row> iter = tEnv().from(""values_t"").execute().collect();}} will work."	FLINK	Closed	3	1	3568	pull-request-available
13201055	Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution	"If a time interval was specified with {{YEAR TO MONTH}} resolution like e.g.:
{code}
SELECT * 
FROM Mytable
GROUP BY 
    TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)
{code}
it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution."	FLINK	Closed	3	1	3568	pull-request-available
13320624	Create an uber jar when packaging flink-avro for sql client	"Currently users have to provide dependencies such as avro, jackson-core-asl, jackson-mapper-asl and joda-time in the job jar for DataStream jobs, or manually copy them into flink/lib in SQL jobs when using avro formatting.

It's better to generate an uber jar including these dependencies when packaging flink-avro module. "	FLINK	Closed	3	4	3568	pull-request-available
13238695	Port TableEnvironment to flink-api modules	"{{TableEnvironments}} should be a purely API class(es). Current implementation should be split into {{CatalogManager}} and {{Planner}}. The {{Planner}} should be discovered based on configuration. This will allow using either the legacy or the Blink planner.

This applies to the {{StreamTableEnvironment}}. The {{BatchTableEnvironment}} will be left as is. One will be able to use the new {{StreamTableEnvironment}} for stream processing with the legacy planner or stream and batch for the Blink {{Planner}}."	FLINK	Closed	3	4	3568	pull-request-available
13565276	Set ALWAYS ChainingStrategy in TemporalSort	Similarly to FLINK-27992 we should ALWAYS chaining strategy in TemporalSort operator	FLINK	Closed	3	1	3568	pull-request-available
13303573	BatchTableEnvironment#fromValues(Object... values) throws StackOverflowError 	"The Error can be reproduced by following code:
{code:java}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);
tEnv.fromValues(1L, 2L, 3L);
{code}
The Error is as following:
{code:java}
Exception in thread ""main"" java.lang.StackOverflowErrorException in thread ""main"" java.lang.StackOverflowError at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110) at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.expressions.ApiExpressionUtils.convertArray(ApiExpressionUtils.java:142) at org.apache.flink.table.expressions.ApiExpressionUtils.objectToExpression(ApiExpressionUtils.java:100) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1030) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163)
...{code}"	FLINK	Closed	3	1	3568	pull-request-available
13313068	Can not create a catalog from user jar	"The {{CREATE CATALOG}} statement does not work if the catalog implementation comes from the user classloader. The problem is that {{org.apache.flink.table.planner.operations.SqlToOperationConverter#convertCreateCatalog}} uses the {{SqlToOperationConverter}} classloader.

We should use {{Thread.currentThread().getContextClassloader()}} for now.

One of the ways to reproduce it is try to create e.g. a postgres catalog with the {{flink-connector-jdbc}} passed as an additional jar to {{sql--client}}"	FLINK	Closed	2	1	3568	pull-request-available
13383342	Deprecate/Remove StreamOperator#dispose method	"As discussed in the ML thread (e.g. https://lists.apache.org/thread.html/r34a05c77bddb2a7cb550c0b820d2a4aa8e1be882fc81bee501fb74e8%40%3Cdev.flink.apache.org%3E) we want to clean up the contract of {{close}} and {{dispose}} methods. 

We suggest introducing a new method finish(), deprecate or remove the dispose method and extract finish() part out of the close()."	FLINK	Closed	3	7	3568	pull-request-available
13297293	Improve literals conversion in ExpressionConverter	"There are couple of issues with the {{ExpressionResolver}} and literals conversion:
1. There is a lot of code duplication
2. Precision of certain types might get lost e.g. BINARY, CHAR"	FLINK	Closed	3	1	3568	pull-request-available
13370144	Test display last n exceptions/causes for job restarts in Web UI	This is the testing task for FLINK-6042. We should test whether the root causes for multiple restarts are properly displayed in the web UI.	FLINK	Closed	1	4	3568	pull-request-available, release-testing
13378788	Incompatible subtask mappings while resuming from unaligned checkpoints	"A user [reported|https://lists.apache.org/x/list.html?user@flink.apache.org:lte=1M:Flink%201.13.0%20reactive%20mode:%20Job%20stop%20and%20cannot%20restore%20from%20checkpoint] that he encountered an internal error while resuming during reactive mode. There isn't an immediate connection to reactive mode, so it's safe to assume that one rescaling case was not covered.

{noformat}
Caused by: java.lang.IllegalStateException: Incompatible subtask mappings: are multiple operators ingesting/producing intermediate results with varying degrees of parallelism?Found RescaleMappings{mappings=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179], [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]]} and RescaleMappings{mappings=[[0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98, 105, 112, 119, 126, 133, 140, 147, 154, 161, 168, 175, 182, 189, 196, 203], [1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99, 106, 113, 120, 127, 134, 141, 148, 155, 162, 169, 176, 183, 190, 197, 204], [2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86, 93, 100, 107, 114, 121, 128, 135, 142, 149, 156, 163, 170, 177, 184, 191, 198, 205], [3, 10, 17, 24, 31, 38, 45, 52, 59, 66, 73, 80, 87, 94, 101, 108, 115, 122, 129, 136, 143, 150, 157, 164, 171, 178, 185, 192, 199, 206], [4, 11, 18, 25, 32, 39, 46, 53, 60, 67, 74, 81, 88, 95, 102, 109, 116, 123, 130, 137, 144, 151, 158, 165, 172, 179, 186, 193, 200, 207], [5, 12, 19, 26, 33, 40, 47, 54, 61, 68, 75, 82, 89, 96, 103, 110, 117, 124, 131, 138, 145, 152, 159, 166, 173, 180, 187, 194, 201, 208], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97, 104, 111, 118, 125, 132, 139, 146, 153, 160, 167, 174, 181, 188, 195, 202, 209]]}.
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.checkSubtaskMapping(TaskStateAssignment.java:322) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.getInputMapping(TaskStateAssignment.java:306) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.reDistributeInputChannelStates(StateAssignmentOperation.java:409) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignAttemptState(StateAssignmentOperation.java:193) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignStates(StateAssignmentOperation.java:139) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1566) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1646) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphAndRestoreState(AdaptiveScheduler.java:986) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$createExecutionGraphAndRestoreStateAsync$25(AdaptiveScheduler.java:976) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.BackgroundTask.lambda$new$0(BackgroundTask.java:57) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.lang.Thread.run(Thread.java:834) ~[?:?]
{noformat}

Here it seems that the same gate gets input from a range-partitioned and a round-robin partitioned channel at the same time. During the implementation of FLINK-19801, we couldn't find such a case and optimized the implementation accordingly.

We have asked the user to provide his topology."	FLINK	Closed	1	1	3568	pull-request-available
13556214	Implement type inference for Over function	"https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/overOffsets.scala

Functions:
* OVER
* CURRENT_RANGE
* CURRENT_ROW
* UNBOUNDED_ROW
* UNBOUNDED_RANGE"	FLINK	Closed	3	7	3568	pull-request-available
13580266	BlockStatementGrouper uses lots of memory	"For deeply nested {{if else}} statements {{BlockStatementGrouper}} uses loads of memory and fails with OOM quickly.

When running JMs with around 400mb a query like:
{code}
select case when orderid = 0 then 1 when orderid = 1 then 2 when orderid
    = 2 then 3 when orderid = 3 then 4 when orderid = 4 then 5 when orderid = 5 then
    6 when orderid = 6 then 7 when orderid = 7 then 8 when orderid = 8 then 9 when
    orderid = 9 then 10 when orderid = 10 then 11 when orderid = 11 then 12 when orderid
    = 12 then 13 when orderid = 13 then 14 when orderid = 14 then 15 when orderid
    = 15 then 16 when orderid = 16 then 17 when orderid = 17 then 18 when orderid
    = 18 then 19 when orderid = 19 then 20 when orderid = 20 then 21 when orderid
    = 21 then 22 when orderid = 22 then 23 when orderid = 23 then 24 when orderid
    = 24 then 25 when orderid = 25 then 26 when orderid = 26 then 27 when orderid
    = 27 then 28 when orderid = 28 then 29 when orderid = 29 then 30 when orderid
    = 30 then 31 when orderid = 31 then 32 when orderid = 32 then 33 when orderid
    = 33 then 34 when orderid = 34 then 35 when orderid = 35 then 36 when orderid
    = 36 then 37 when orderid = 37 then 38 when orderid = 38 then 39 when orderid
    = 39 then 40 when orderid = 40 then 41 when orderid = 41 then 42 when orderid
    = 42 then 43 when orderid = 43 then 44 when orderid = 44 then 45 when orderid
    = 45 then 46 when orderid = 46 then 47 when orderid = 47 then 48 when orderid
    = 48 then 49 when orderid = 49 then 50 when orderid = 50 then 51 when orderid
    = 51 then 52 when orderid = 52 then 53 when orderid = 53 then 54 when orderid
    = 54 then 55 when orderid = 55 then 56 when orderid = 56 then 57 when orderid
    = 57 then 58 when orderid = 58 then 59 when orderid = 59 then 60 when orderid
    = 60 then 61 when orderid = 61 then 62 when orderid = 62 then 63 when orderid
    = 63 then 64 when orderid = 64 then 65 when orderid = 65 then 66 when orderid
    = 66 then 67 when orderid = 67 then 68 when orderid = 68 then 69 when orderid
    = 69 then 70 when orderid = 70 then 71 when orderid = 71 then 72 when orderid
    = 72 then 73 when orderid = 73 then 74 when orderid = 74 then 75 when orderid
    = 75 then 76 when orderid = 76 then 77 when orderid = 77 then 78 when orderid
    = 78 then 79 when orderid = 79 then 80 when orderid = 80 then 81 when orderid
    = 81 then 82 when orderid = 82 then 83 when orderid = 83 then 84 when orderid
    = 84 then 85 when orderid = 85 then 86 when orderid = 86 then 87 when orderid
    = 87 then 88 when orderid = 88 then 89 when orderid = 89 then 90 when orderid
    = 90 then 91 when orderid = 91 then 92 when orderid = 92 then 93 when orderid
    = 93 then 94 when orderid = 94 then 95 when orderid = 95 then 96 when orderid
    = 96 then 97 when orderid = 97 then 98 when orderid = 98 then 99 when orderid
    = 99 then 100 when orderid = 100 then 101 when orderid = 101 then 102 when orderid
    = 102 then 103 when orderid = 103 then 104 when orderid = 104 then 105 when orderid
    = 105 then 106 when orderid = 106 then 107 when orderid = 107 then 108 when orderid
    = 108 then 109 when orderid = 109 then 110 when orderid = 110 then 111 when orderid
    = 111 then 112 when orderid = 112 then 113 when orderid = 113 then 114 when orderid
    = 114 then 115 when orderid = 115 then 116 when orderid = 116 then 117 when orderid
    = 117 then 118 when orderid = 118 then 119 when orderid = 119 then 120 when orderid
    = 120 then 121 when orderid = 121 then 122 when orderid = 122 then 123 when orderid
    = 123 then 124 when orderid = 124 then 125 when orderid = 125 then 126 when orderid
    = 126 then 127 when orderid = 127 then 128 when orderid = 128 then 129 when orderid
    = 129 then 130 when orderid = 130 then 131 when orderid = 131 then 132 when orderid
    = 132 then 133 when orderid = 133 then 134 when orderid = 134 then 135 when orderid
    = 135 then 136 when orderid = 136 then 137 when orderid = 137 then 138 when orderid
    = 138 then 139 when orderid = 139 then 140 when orderid = 140 then 141 when orderid
    = 141 then 142 when orderid = 142 then 143 when orderid = 143 then 144 when orderid
    = 144 then 145 when orderid = 145 then 146 when orderid = 146 then 147 when orderid
    = 147 then 148 when orderid = 148 then 149 when orderid = 149 then 150 when orderid
    = 150 then 151 when orderid = 151 then 152 when orderid = 152 then 153 when orderid
    = 153 then 154 when orderid = 154 then 155 when orderid = 155 then 156 when orderid
    = 156 then 157 when orderid = 157 then 158 when orderid = 158 then 159 when orderid
    = 159 then 160 when orderid = 160 then 161 when orderid = 161 then 162 when orderid
    = 162 then 163 when orderid = 163 then 164 when orderid = 164 then 165 when orderid
    = 165 then 166 when orderid = 166 then 167 when orderid = 167 then 168 when orderid
    = 168 then 169 when orderid = 169 then 170 when orderid = 170 then 171 when orderid
    = 171 then 172 when orderid = 172 then 173 when orderid = 173 then 174 when orderid
    = 174 then 175 when orderid = 175 then 176 when orderid = 176 then 177 when orderid
    = 177 then 178 when orderid = 178 then 179 when orderid = 179 then 180 when orderid
    = 180 then 181 when orderid = 181 then 182 when orderid = 182 then 183 when orderid
    = 183 then 184 when orderid = 184 then 185 when orderid = 185 then 186 when orderid
    = 186 then 187 when orderid = 187 then 188 when orderid = 188 then 189 when orderid
    = 189 then 190 when orderid = 190 then 191 when orderid = 191 then 192 when orderid
    = 192 then 193 when orderid = 193 then 194 when orderid = 194 then 195 when orderid
    = 195 then 196 when orderid = 196 then 197 when orderid = 197 then 198 when orderid
    = 198 then 199 when orderid = 199 then 200 when orderid = 200 then 201 when orderid
    = 201 then 202 when orderid = 202 then 203 when orderid = 203 then 204 when orderid
    = 204 then 205 when orderid = 205 then 206 when orderid = 206 then 207 when orderid
    = 207 then 208 when orderid = 208 then 209 when orderid = 209 then 210 when orderid
    = 210 then 211 when orderid = 211 then 212 when orderid = 212 then 213 when orderid
    = 213 then 214 when orderid = 214 then 215 when orderid = 215 then 216 when orderid
    = 216 then 217 when orderid = 217 then 218 when orderid = 218 then 219 when orderid
    = 219 then 220 when orderid = 220 then 221 when orderid = 221 then 222 when orderid
    = 222 then 223 when orderid = 223 then 224 when orderid = 224 then 225 when orderid
    = 225 then 226 when orderid = 226 then 227 when orderid = 227 then 228 when orderid
    = 228 then 229 when orderid = 229 then 230 when orderid = 230 then 231 when orderid
    = 231 then 232 when orderid = 232 then 233 when orderid = 233 then 234 when orderid
    = 234 then 235 when orderid = 235 then 236 when orderid = 236 then 237 when orderid
    = 237 then 238 when orderid = 238 then 239 when orderid = 239 then 240 when orderid
    = 240 then 241 when orderid = 241 then 242 when orderid = 242 then 243 when orderid
    = 243 then 244 when orderid = 244 then 245 when orderid = 245 then 246 when orderid
    = 246 then 247 when orderid = 247 then 248 when orderid = 248 then 249 when orderid
    = 249 then 250 else 9999 end case_when_col from sample_data_1;
{code}

fails with an OOM. (Yes, I know the query can be simplified, but it shows the case)."	FLINK	Resolved	3	4	3568	pull-request-available
13387961	All records are processed in the close stage in ContinuousFileReaderOperatorBenchmark	The {{TARGET_COUNT_REACHED_LATCH}} is not correctly reset after the warmup iterations and thus subsequent runs process all records in the {{CLOSE}} stage of the {{ContinuousFileReaderOperator}} testing something different than anticipated.	FLINK	Closed	3	1	3568	pull-request-available
13296470	Support parsing LIKE clause in CREATE TABLE statement	We should support the CREATE TABLE ... LIKE syntax in SqlParser	FLINK	Closed	3	7	3568	pull-request-available
13553409	MATCH_RECOGNIZE AFTER MATCH clause can not be deserialised from a compiled plan	"{code}
        String sql =
                ""insert into MySink""
                        + "" SELECT * FROM\n""
                        + "" MyTable\n""
                        + ""   MATCH_RECOGNIZE(\n""
                        + ""   PARTITION BY vehicle_id\n""
                        + ""   ORDER BY `rowtime`\n""
                        + ""   MEASURES \n""
                        + ""       FIRST(A.`rowtime`) as startTime,\n""
                        + ""       LAST(A.`rowtime`) as endTime,\n""
                        + ""       FIRST(A.engine_temperature) as Initial_Temp,\n""
                        + ""       LAST(A.engine_temperature) as Final_Temp\n""
                        + ""   ONE ROW PER MATCH\n""
                        + ""   AFTER MATCH SKIP TO FIRST B\n""
                        + ""   PATTERN (A+ B)\n""
                        + ""   DEFINE\n""
                        + ""       A as LAST(A.engine_temperature,1) is NULL OR A.engine_temperature > LAST(A.engine_temperature,1),\n""
                        + ""       B as B.engine_temperature < LAST(A.engine_temperature)\n""
                        + ""   )MR;"";
        util.verifyJsonPlan(String.format(sql, afterClause));
{code}

fails with:

{code}
Could not resolve internal system function '$SKIP TO LAST$1'. This is a bug, please file an issue. (through reference chain: org.apache.flink.table.planner.plan.nodes.exec.serde.JsonPlanGraph[""nodes""]->java.util.ArrayList[3]->org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch[""matchSpec""]->org.apache.flink.table.planner.plan.nodes.exec.spec.MatchSpec[""after""])
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13324219	Remove deprecated RuntimeContext#getAllAccumulators	"We could  remove the deprecated:
{code}
RuntimeContext#getAllAcumullators
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13556206	Remove old expression stack leftovers for time functions	"Remove leftovers from https://issues.apache.org/jira/browse/FLINK-13785

There are some parts of the time functions that have not been removed e.g. https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/time.scala and some code in https://github.com/apache/flink/blob/b6000f6e589128ae1fd1e0e7d063a1b6ff1fcc20/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/PlannerExpressionConverter.scala"	FLINK	Closed	3	7	3568	pull-request-available
13360332	Document possible recommended usage of Bounded{One/Multi}Input.endInput and emphasize that they could be called multiple times	"It is too tempting to use these api, especially {{BoundedOneInput.endInput}}, to commit final result before FLIP-147 delivered. And this will cause re-commit after failover as [~gaoyunhaii] has pointed out in FLINK-21132.

I have [pointed|https://github.com/apache/iceberg/issues/2033#issuecomment-784153620] this out in [apache/iceberg#2033|https://github.com/apache/iceberg/issues/2033], please correct me if I was wrong.

cc [~aljoscha] [~pnowojski] [~roman_khachatryan]"	FLINK	Closed	10200	4	3568	auto-deprioritized-major, pull-request-available, stale-minor
13212945	Create CatalogManager to manage multiple catalogs and encapsulate Calcite schema	"Flink allows for more than one registered catalogs. {{CatalogManager}} class is the holding class to manage and encapsulate the catalogs and their interrelations with Calcite.

Following section describes how table resolution should work:

h4. PATH resolution:

First look into DEFAULT PATH: cat or cat.db. Then in the root. This is also the behavior of Calcite. We should mimic this behavior in Flink.
Example:

{noformat}
root:
  |- builtin
      |- default
          |- tab1
          |- tab2
      |- db1
          |- tab1
      |- clashing
          |- tab1
  |- cat1
      |- db1
          |- tab1
          |- tab2
  |- extCat1
      |- tab1
      |- clashing
          |- tab1
      |- extCat2
          |- tab1
          |- tab2
  |- clashing (ExternalCatalog)
      |- tab1
{noformat}
      
There is always a default catalog, initially set to builtin and default database initially set to default.

{noformat}
Assume 
default path = builtin then
  default.tab1 = builtin.default.tab1
  tab1 = error
  cat1.db1.tab1 = cat1.db1.tab1
  clashing.tab1 = builtin.clashing.tab1
default path = extCat1 then
  tab1 = extCat1.tab1
  clashing.tab1 = extCat1.clashing.tab1
default path = extCat1.extCat2 (do not support further nesting) then
  tab1 = extCat1.extCat2.tab1
  clashing.tab1 = clashing.tab1
{noformat}
  

h4. Structure in the Planner(Calcite-specific)

{noformat}
root: CatalogManagerSchema(CatalogManager)
     |- CatalogCalciteSchema(ReadableCatalog)
         |- DatabaseCalciteSchema (ReadableCatalog scoped to DB)
             |- Table
     |- ExternalCatalogSchema
         |- Table
         |- ExternalCatalogSchema
             |- Table
{noformat}

h4. Structure in the API
{noformat}    
CatalogManager:
    |- ReadableCatalog
      |- CatalogTable
    |- ExternalCatalog
      |- ExternalCatalog
      |- ExternalCatalogTable
{noformat}"	FLINK	Resolved	3	7	3568	pull-request-available
13194127	Add a switch to run_test to configure if logs should be checked for errors/excepions	After adding the switch, we should disable log checking for nightly-tests that currently fail (or fix the test).	FLINK	Closed	1	4	3568	pull-request-available
13574746	AggregateQueryOperations produces wrong asSerializableString representation	"A table API query:
{code}
        env.fromValues(1, 2, 3)
                .as(""number"")
                .select(col(""number"").count())
                .insertInto(TEST_TABLE_API)
{code}

produces

{code}
INSERT INTO `default`.`timo_eu_west_1`.`table_api_basic_api` SELECT `EXPR$0` FROM (
    SELECT (COUNT(`number`)) AS `EXPR$0` FROM (
        SELECT `f0` AS `number` FROM (
            SELECT `f0` FROM (VALUES 
                (1),
                (2),
                (3)
            ) VAL$0(`f0`)
        )
    )
    GROUP BY 
)
{code}

which is missing a grouping expression"	FLINK	Closed	3	1	3568	pull-request-available
13382184	Tasks are blocked while broadcasting stream status	"On a cluster I observed symptoms of tasks being blocked for long time, causing long delays with unaligned checkpointing. 99% of those cases were caused by `broadcastEmit` of the stream status

{noformat}
2021-06-04 14:41:44,049 ERROR org.apache.flink.runtime.io.network.buffer.LocalBufferPool   [] - Blocking wait [11059 ms] for an available buffer.
java.lang.Exception: Stracktracegenerator
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:323) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:290) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:338) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:314) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:246) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:142) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:67) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.writeStreamStatus(RecordWriterOutput.java:136) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.streamstatus.AnnouncedStatus.ensureActive(AnnouncedStatus.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:182) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.pollNext(IteratorSourceReader.java:98) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:294) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:69) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:422) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:680) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:635) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:619) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{noformat}

*I have seen this happening both in source and network tasks.* ~80% cases were in the source tasks

{{broadcastEmit}} can easily bypass our non blocking checks. There are two questions:
# why is the stream idling so much? It’s like almost every millisecond it’s broadcasting status
# should we optimise this? Broadcasting CBs and other events is not an issue, as those are events that do not request/require buffers"	FLINK	Closed	2	1	3568	pull-request-available
13316803	Kerberized YARN per-job on Docker test failed to download JDK 8u251	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
{code}
+ mkdir -p /usr/java/default
+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie
+ tar --strip-components=1 -xz -C /usr/java/default/

gzip: stdin: not in gzip format
tar: Child returned status 1
{code}"	FLINK	Closed	1	1	3568	pull-request-available
13192203	YarnConfigurationITCase.testFlinkContainerMemory test instability	"Test appeared to fail (by a narrow margin) without a reason randomly:

{noformat}
Failed tests: 
  YarnConfigurationITCase.testFlinkContainerMemory:182 
Expected: is a numeric value within <0.1> of <1.0>
     but: <0.8913043478260869> differed by <0.008695652173913077>
{noformat}

https://api.travis-ci.org/v3/job/442246057/log.txt"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13398595	Document FLIP-147 capabiliites and limitations	We should document how to enable the checkpointing after tasks finish as well as limitations we are aware of.	FLINK	Closed	2	7	3568	pull-request-available
13362233	 CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107

{code:java}
[ERROR] testAsyncCheckpointFailureTriggerJobFailed(org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase)  Time elapsed: 38.623 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:62)
	at org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed(CheckpointFailureManagerITCase.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
"	FLINK	Closed	3	1	3568	pull-request-available, test-stability
13228910	Convert CatalogTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	In FLINK-11476, we created CatalogManager to hook up planner with unified catalog APIs. What's missing there is, at the very last step, convert CatalogBaseTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs, like how {{ExternalTableUtil.fromExternalCatalogTable()}} works to convert the old {{ExternalCatalogTable}} to a Calcite table	FLINK	Closed	3	7	3568	pull-request-available
13409619	Remove scala suffix from respective benchmarks dependencies	With FLINK-24018 few dependencies lost its scala suffix. We should remove it in benchmark dependencies to test against newest artifacts.	FLINK	Closed	3	1	3568	pull-request-available
13404250	Add Flink 1.14 MigrationVersion	"Currently the largest MigrationVersion is 1.13. We need newer versions to add more serializer compatibility tests.
"	FLINK	Closed	3	4	3568	pull-request-available
13231708	Fix failling e2e test test_streaming_sql.sh	"https://travis-ci.org/apache/flink/jobs/535231108

{code}
==============================================================================
Running 'Streaming SQL end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-37856266807
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.9-SNAPSHOT-bin/flink-1.9-SNAPSHOT
Starting cluster.
Starting standalonesession daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Dispatcher REST endpoint is up.
[INFO] 1 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 2 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 3 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting execution of program
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(INTEGER NOT NULL correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
expression type is RecordType(INTEGER correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
set is rel#150:LogicalWindowAggregate.NONE(input=HepRelVertex#139,group={},correct=SUM($1),w$start=start('w$),w$end=end('w$),w$rowtime=rowtime('w$),w$proctime=proctime('w$))
expression is LogicalWindowAggregate#167
	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:57)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:234)
	at org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.onMatch(ExtendedAggregateExtractProjectRule.java:90)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:559)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:418)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:255)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:214)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:201)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlanner(TableEnvImpl.scala:282)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlannerSequentially(TableEnvImpl.scala:248)
	at org.apache.flink.table.api.TableEnvImpl.optimizeNormalizeLogicalPlan(TableEnvImpl.scala:204)
	at org.apache.flink.table.api.StreamTableEnvImpl.optimize(StreamTableEnvImpl.scala:738)
	at org.apache.flink.table.api.StreamTableEnvImpl.translate(StreamTableEnvImpl.scala:787)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:100)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:83)
	at org.apache.flink.sql.tests.StreamSQLTestProgram.main(StreamSQLTestProgram.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:269)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:742)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:272)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:204)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:983)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1056)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1056)
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13180226	WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow	"{{WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow}} failed on Travis.

https://api.travis-ci.org/v3/job/418629694/log.txt"	FLINK	Closed	2	1	3568	test-stability
13486345	BatchExecutionKeyedStateBackend is using incorrect ExecutionConfig when creating serializer	"{{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend#getOrCreateKeyedState}} is using freshly constructed {{ExecutionConfig}}, instead of the one configured by the user from the environment.


{code:java}
    public <N, S extends State, T> S getOrCreateKeyedState(
            TypeSerializer<N> namespaceSerializer, StateDescriptor<S, T> stateDescriptor)
            throws Exception {
        checkNotNull(namespaceSerializer, ""Namespace serializer"");
        checkNotNull(
                keySerializer,
                ""State key serializer has not been configured in the config. ""
                        + ""This operation cannot use partitioned state."");

        if (!stateDescriptor.isSerializerInitialized()) {
            stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
        }
{code}

The correct one could be obtained from {{env.getExecutionConfig()}} in {{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend#createKeyedStateBackend}} "	FLINK	Closed	4	1	3568	pull-request-available
13208537	KafkaITCase.testConcurrentProducerConsumerTopology times out on Travis	"The {{KafkaITCase.testConcurrentProducerConsumerTopology}} times out on Travis.

{code}
20:26:29.579 [ERROR] Errors: 
20:26:29.579 [ERROR]   KafkaITCase.testConcurrentProducerConsumerTopology:73->KafkaConsumerTestBase.runSimpleConcurrentProducerConsumerTopology:824->KafkaTestBase.deleteTestTopic:206->Object.wait:502->Object.wait:-2 Â» TestTimedOut
{code}

The solution might as simple as increasing the timeout.

https://api.travis-ci.org/v3/job/476975725/log.txt"	FLINK	Closed	2	4	3568	pull-request-available, test-stability
13031245	Fails AkkaRpcServiceTest#testTerminationFuture	"{code}
testTerminationFuture(org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest)  Time elapsed: 1.013 sec  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 1000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at akka.remote.Remoting.start(Remoting.scala:179)
	at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:634)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:142)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:119)
	at akka.actor.ActorSystem$.create(ActorSystem.scala:67)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
	at org.apache.flink.runtime.akka.AkkaUtils$.createDefaultActorSystem(AkkaUtils.scala:114)
	at org.apache.flink.runtime.akka.AkkaUtils.createDefaultActorSystem(AkkaUtils.scala)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.testTerminationFuture(AkkaRpcServiceTest.java:134)
{code} in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest while testing current master 1.2.0 branch "	FLINK	Closed	3	1	3568	test-stability
13371934	JobMasterStopWithSavepointITCase failed due to status is FAILING	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16405&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4472


{code:java}
[ERROR] Failures: 
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheSuspend:133->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate:138->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR] Errors: 
[ERROR]   JobMasterStopWithSavepointITCase.suspendWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished:103->stopWithSavepointNormalExecutionHelper:113->setUpJobGraph:307 » IllegalState
[ERROR]   JobMasterStopWithSavepointITCase.testRestartCheckpointCoordinatorIfStopWithSavepointFails:237 » IllegalState
[INFO] 
[ERROR] Tests run: 1645, Failures: 2, Errors: 2, Skipped: 51

{code}
"	FLINK	Resolved	2	1	3568	test-stability
13326530	Clean up the UnilateralSortMerger	"This is a preparation step for [FLIP-140|https://cwiki.apache.org/confluence/display/FLINK/FLIP-140%3A+Introduce+bounded+style+execution+for+keyed+streams]. The purpose of the task is two-folds:
* break down the implementation into a more composable pieces
* introduce a way to produce records in a push-based manner instead of pull-based with additional reading thread."	FLINK	Closed	3	7	3568	pull-request-available
13278176	Preserve logs from BashJavaUtils and make them part of TM logs	"In FLINK-13983 we introduced BashJavaUtils utility to call in taskmanager.sh before starting TM and calculate memory configuration for the JVM process of TM.

Ideally, it would be nice to preserve BashJavaUtils logs and make them part of the TM logs. Currently, logging for BashJavaUtils is configured from the class path and can differ from TM logging. Moreover TM logging can rewrite BashJavaUtils even if we align their loggings (e.g. log4j.appender.file.append=false in default log4j.properties  for Flink)."	FLINK	Closed	1	4	3568	pull-request-available
13597368	TIMESTAMPDIFF can not be string serialized	TIMESTAMPDIFF can not be properly string serialized, because TIMEPOINTUNIT can not be serialized.	FLINK	In Progress	3	1	3568	pull-request-available
13205141	Exception in code generation when ambiguous columns in MATCH_RECOGNIZE	"Query:
{code}
SELECT *
FROM Ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol, price
  ORDER BY proctime
  MEASURES
    A.symbol AS symbol,
    A.price AS price
  PATTERN (A)
  DEFINE
    A AS symbol = 'a'
) AS T
{code}

throws a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. 

I've also created a calcite ticket to fix it on calcite's side: [CALCITE-2747]"	FLINK	Closed	3	1	3568	pull-request-available
13185500	Savepoints should be counted as retained checkpoints	"This task is about reverting [FLINK-6328].

The problem is that you can get incorrect results with exactly-once sinks if there is a failure after taking a savepoint but before taking the next checkpoint because the savepoint will also have manifested side effects to the sink.
"	FLINK	Closed	3	1	3568	pull-request-available
13343946	Some Table examples are not built correctly	"Some examples were moved to the {{org.apache.flink.table.examples.scala.basics}} package but the pom.xml was not updated. This means the example jars are not built correctly and do not contain the classes.

Examples that I noticed:
* org.apache.flink.table.examples.scala.basics.StreamTableExample
* org.apache.flink.table.examples.scala.basics.TPCHQuery3Table

We should update the {{includes}} sections e.g.:

{code}
<execution>
	<id>StreamTableExample</id>
	<phase>package</phase>
	<goals>
		<goal>jar</goal>
	</goals>
	<configuration>
		<classifier>StreamTableExample</classifier>

<!--- The sections below should be updated -->

		<archive>
			<manifestEntries>
				<program-class>org.apache.flink.table.examples.scala.StreamTableExample</program-class>
			</manifestEntries>
		</archive>
		<includes>
			<include>org/apache/flink/table/examples/scala/StreamTableExample*</include>
		</includes>
	</configuration>
</execution>
{code}"	FLINK	Closed	2	1	3568	pull-request-available
13355441	Write savepoints in unified format from HeapStateBackend	The aim is to implement a {{HeapKeyValueStateIterator}} which can be used to produce a unified savepoint out of a HeapKeyedStateBackend	FLINK	Closed	3	7	3568	pull-request-available
13242119	Remove expressionBridge from QueryOperations factories	Expression bridge is used to create a schema of QueryOperation. This is no longer necessary with ResolvedExpressions in place.	FLINK	Closed	3	7	3568	pull-request-available
13415595	Document claim & no-claim mode	We should describe how the different restore modes work. It is important to go through the FLIP and include all {{NOTES}} in the written documentation	FLINK	Closed	3	7	3568	pull-request-available
13238698	Improve expression based TableSchema extraction from DataStream/DataSet	"We should improve the extraction of {{TableSchema}} from {{DataStream/DataSet}}. Currently it is split into a few stages:
# Extract types ignoring time attributes via {{FieldInfoUtils#getFieldInfo}}
# Extract the rowtime and proctime positions via {{StreamTableEnvImpl#validateAndExtractTimeAttributes}}
# Adjust the indices from #1 using information from #2

All that could happen in a single pass. This will also deal with the porting/removing of a few methods from {{StreamTableEnvImpl}}."	FLINK	Closed	3	7	3568	pull-request-available
13314702	Tests RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte & RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort fail locally	"The tests:
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte

fail locally (in IDE or from cmd with {{mvn clean install}}) with
{code}
java.lang.UnsatisfiedLinkError: org.rocksdb.ReadOptions.newReadOptions()J
	at org.rocksdb.ReadOptions.newReadOptions(Native Method)
	at org.rocksdb.ReadOptions.<init>(ReadOptions.java:16)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIterator(RocksKeyGroupsRocksSingleStateIteratorTest.java:78)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIteratorShort(RocksKeyGroupsRocksSingleStateIteratorTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13353481	SQLClientSchemaRegistryITCase unstable with InternalServerErrorException: Status 500	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12253&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2021-01-20T00:10:21.3510385Z Jan 20 00:10:21 
2021-01-20T00:10:21.3516246Z Jan 20 00:10:21 [ERROR] testWriting(org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase)  Time elapsed: 0.001 s  <<< ERROR!
2021-01-20T00:10:21.3517459Z Jan 20 00:10:21 java.lang.RuntimeException: Could not build the flink-dist image
2021-01-20T00:10:21.3518178Z Jan 20 00:10:21 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:281)
2021-01-20T00:10:21.3519176Z Jan 20 00:10:21 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.<init>(SQLClientSchemaRegistryITCase.java:88)
2021-01-20T00:10:21.3519873Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-01-20T00:10:21.3520537Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-01-20T00:10:21.3521390Z Jan 20 00:10:21 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-01-20T00:10:21.3522080Z Jan 20 00:10:21 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2021-01-20T00:10:21.3522730Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
2021-01-20T00:10:21.3523452Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
2021-01-20T00:10:21.3524237Z Jan 20 00:10:21 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-01-20T00:10:21.3524879Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
2021-01-20T00:10:21.3525527Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-01-20T00:10:21.3526157Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-01-20T00:10:21.3526754Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-01-20T00:10:21.3527316Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-01-20T00:10:21.3527884Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-01-20T00:10:21.3528462Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-01-20T00:10:21.3529491Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-01-20T00:10:21.3530220Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-01-20T00:10:21.3530970Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-01-20T00:10:21.3531649Z Jan 20 00:10:21 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-01-20T00:10:21.3532201Z Jan 20 00:10:21 	at java.lang.Thread.run(Thread.java:748)
2021-01-20T00:10:21.3533545Z Jan 20 00:10:21 Caused by: com.github.dockerjava.api.exception.InternalServerErrorException: Status 500: {""message"":""Get https://registry-1.docker.io/v2/testcontainers/ryuk/manifests/0.3.0: received unexpected HTTP status: 502 Bad Gateway""}
2021-01-20T00:10:21.3534353Z Jan 20 00:10:21 
2021-01-20T00:10:21.3534955Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:247)
2021-01-20T00:10:21.3536388Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$executeAndStream$1(DefaultInvocationBuilder.java:269)
2021-01-20T00:10:21.3537066Z Jan 20 00:10:21 	... 1 more
2021-01-20T00:10:21.3541323Z Jan 20 00:10:21 
{code}"	FLINK	Closed	2	1	3568	test-stability
13280523	SELECT 'ABC'; does not work in sql-client	"A query like {{SELECT 'abc';}} fails in sql-client with blink planner enabled with an error:
{code}
org.apache.flink.table.api.ValidationException: Type CHAR(3) of table field 'EXPR$0' does not match with the physical type STRING of the 'EXPR$0' field of the TableSink consumed type.
{code}

The reason is that those sinks do not properly support new type system. There is no good way to define schema and consumed data type so that they match. We should update the in-memory sinks in sql-client to work with the legacy type system for now until the retract and upsert sinks work properly with the new type system."	FLINK	Closed	1	1	3568	pull-request-available
13436369	Introduce parallelism setter for table store	"Support:
 * scan.parallelism
 * sink.parallelism"	FLINK	Closed	3	7	6732	pull-request-available
13473492	BucketSelector is wrong when hashcode is negative	The calculation of bucket should be `Math.abs(hashcode % numBucket)` instead of `hashcode % numBucket`.	FLINK	Closed	1	1	6732	pull-request-available
13243326	Support LocalZonedTimestampType in blink	"Now we just support TimestampType, it is without time zone.

We need support LocalZonedTimestampType, and define time zone in config."	FLINK	Closed	3	2	6732	pull-request-available
13527915	Refactor classes code of full-compaction	Refactor classes code of full-compaction, this is to prepare some shared codes for lookup changelog producer.	FLINK	Closed	3	7	6732	pull-request-available
13506161	Refactor Lock to provide Lock.Factory	For the core, it should not see too many Flink Table concepts, such as database and tableName. It only needs to create a Lock.	FLINK	Closed	3	4	6732	pull-request-available
13300233	Integrate orc to file system connector	"Integrate orc to file system connector, so in the sql world, users can create file system table with orc format by DDL, do some reading, writing and streaming writing. And the {{RowData}} is the sql data format. The works are:
 # Introduce OrcRowDataInputFormat with partition support.
 # Introduce RowDataVectorizer.
 # Introduce OrcFileSystemFormatFactory."	FLINK	Closed	3	7	6732	pull-request-available
13246504	Fix SINGLE_VALUE is not correctly supported in blink planner	"There some problem in SingleValueAggFunction:

1.The comment in the method is ""value = count == 0 ? exception : operand(0)"", but actually it need to be ""value = count > 0 ? exception : operand(0)"" according to the code and logic.

2.The throwException expression should call(THROW_EXCEPTION, literal(msg), typeLiteral(type)).

And there are some bugs to support it in blink-planner:

1.RexNodeConverter lack converter for THROW_EXCEPTION

2.PlannerExpressions lack expr to support type inference for THROW_EXCEPTION"	FLINK	Resolved	1	1	6732	pull-request-available
13222755	Introduce StreamOperatorFactory to help table perform the whole Operator CodeGen	"If we need CodeGen an entire Operator, one possible solution is to introduce an OperatorWrapper, then generate a CodeGen sub-Operator in OperatorWrapper's open, and then proxy all methods to the sub-Operator. But introduce OperatorWrapper results in multiple virtual function calls.

The another way is to introduce a StreamOperatorFactory. In runtime, we get the StreamOperatorFactory and create real operator to invoke. In this way, there is no redundant virtual call, the test results show that the performance improves by about 10% after the introduction of StreamOperatorFactory. (Benchmark for simple query: [https://github.com/JingsongLi/flink/blob/benchmarkop/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/benchmark/batch/CalcBenchmark.java])"	FLINK	Closed	3	2	6732	pull-request-available
13272376	Orc reader should use java.sql.Timestamp to read for respecting time zone	Hive orc use java.sql.Timestamp to read and write orc files... default, timestamp will consider time zone to adjust seconds.	FLINK	Closed	3	1	6732	pull-request-available
13289961	Introduce flink-sql-connector-hive modules to provide hive uber jars	Discussed in: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Introduce-flink-connector-hive-xx-modules-td38440.html]	FLINK	Resolved	3	2	6732	pull-request-available
13480921	Support create table-store table with 'connector'='table-store'	"Support create table-store table with 'connector'='table-store': 

sink to table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'datagen',
    'fields.word.length' = '1'
);
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',  //should rename 'catalog-database'？
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; {code}
source from table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'print'
);
INSERT INTO word_table SELECT word FROM word_count;{code}"	FLINK	Closed	4	4	6732	pull-request-available
13410002	ParquetFileSystemITCase.testLimitableBulkFormat failed on Azure	"The test {{ParquetFileSystemITCase.testLimitableBulkFormat}} fails with 

{code}
2021-11-03T22:10:11.5106075Z Nov 03 22:10:11 [ERROR] testLimitableBulkFormat[false]  Time elapsed: 9.177 s  <<< ERROR!
2021-11-03T22:10:11.5106643Z Nov 03 22:10:11 java.lang.RuntimeException: Failed to fetch next result
2021-11-03T22:10:11.5107213Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-11-03T22:10:11.5111034Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-11-03T22:10:11.5112190Z Nov 03 22:10:11 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:188)
2021-11-03T22:10:11.5112892Z Nov 03 22:10:11 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2021-11-03T22:10:11.5113393Z Nov 03 22:10:11 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:109)
2021-11-03T22:10:11.5114157Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetFileSystemITCase.testLimitableBulkFormat(ParquetFileSystemITCase.java:128)
2021-11-03T22:10:11.5114951Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-03T22:10:11.5115568Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-03T22:10:11.5116115Z Nov 03 22:10:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-03T22:10:11.5116591Z Nov 03 22:10:11 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-03T22:10:11.5117088Z Nov 03 22:10:11 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-03T22:10:11.5117807Z Nov 03 22:10:11 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-03T22:10:11.5118821Z Nov 03 22:10:11 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-03T22:10:11.5119417Z Nov 03 22:10:11 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-03T22:10:11.5119944Z Nov 03 22:10:11 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-11-03T22:10:11.5120427Z Nov 03 22:10:11 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-11-03T22:10:11.5120919Z Nov 03 22:10:11 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-03T22:10:11.5121571Z Nov 03 22:10:11 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-11-03T22:10:11.5122526Z Nov 03 22:10:11 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-11-03T22:10:11.5123245Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-03T22:10:11.5123804Z Nov 03 22:10:11 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-11-03T22:10:11.5124314Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-11-03T22:10:11.5124806Z Nov 03 22:10:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-11-03T22:10:11.5125313Z Nov 03 22:10:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-11-03T22:10:11.5125810Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-03T22:10:11.5126281Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-03T22:10:11.5126739Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-03T22:10:11.5127349Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-03T22:10:11.5128092Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-03T22:10:11.5128984Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-03T22:10:11.5129685Z Nov 03 22:10:11 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-11-03T22:10:11.5130330Z Nov 03 22:10:11 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-11-03T22:10:11.5130771Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-03T22:10:11.5131222Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-03T22:10:11.5131663Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-03T22:10:11.5132139Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-03T22:10:11.5132776Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-03T22:10:11.5133441Z Nov 03 22:10:11 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-03T22:10:11.5134150Z Nov 03 22:10:11 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-03T22:10:11.5134816Z Nov 03 22:10:11 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-11-03T22:10:11.5135741Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-03T22:10:11.5136292Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-03T22:10:11.5136717Z Nov 03 22:10:11 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-11-03T22:10:11.5137140Z Nov 03 22:10:11 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-11-03T22:10:11.5137603Z Nov 03 22:10:11 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
2021-11-03T22:10:11.5138134Z Nov 03 22:10:11 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-11-03T22:10:11.5138766Z Nov 03 22:10:11 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-11-03T22:10:11.5139235Z Nov 03 22:10:11 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2021-11-03T22:10:11.5139733Z Nov 03 22:10:11 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2021-11-03T22:10:11.5140493Z Nov 03 22:10:11 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-11-03T22:10:11.5141265Z Nov 03 22:10:11 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-11-03T22:10:11.5141991Z Nov 03 22:10:11 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-11-03T22:10:11.5142892Z Nov 03 22:10:11 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-11-03T22:10:11.5143712Z Nov 03 22:10:11 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-11-03T22:10:11.5144655Z Nov 03 22:10:11 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-11-03T22:10:11.5145423Z Nov 03 22:10:11 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
2021-11-03T22:10:11.5146236Z Nov 03 22:10:11 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
2021-11-03T22:10:11.5147106Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-11-03T22:10:11.5148061Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-11-03T22:10:11.5149081Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-11-03T22:10:11.5149900Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-11-03T22:10:11.5150722Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-11-03T22:10:11.5151619Z Nov 03 22:10:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-11-03T22:10:11.5152790Z Nov 03 22:10:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2021-11-03T22:10:11.5153810Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-11-03T22:10:11.5154754Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-11-03T22:10:11.5155649Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-11-03T22:10:11.5156235Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-11-03T22:10:11.5156702Z Nov 03 22:10:11 Caused by: java.io.IOException: Failed to fetch job execution result
2021-11-03T22:10:11.5157298Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2021-11-03T22:10:11.5157969Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2021-11-03T22:10:11.5158972Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-11-03T22:10:11.5160158Z Nov 03 22:10:11 	... 67 more
2021-11-03T22:10:11.5160863Z Nov 03 22:10:11 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-11-03T22:10:11.5161734Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-11-03T22:10:11.5162618Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-11-03T22:10:11.5163501Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2021-11-03T22:10:11.5164204Z Nov 03 22:10:11 	... 69 more
2021-11-03T22:10:11.5164752Z Nov 03 22:10:11 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-11-03T22:10:11.5165533Z Nov 03 22:10:11 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-11-03T22:10:11.5166427Z Nov 03 22:10:11 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-11-03T22:10:11.5167353Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-11-03T22:10:11.5168131Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-11-03T22:10:11.5169012Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-11-03T22:10:11.5169614Z Nov 03 22:10:11 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
2021-11-03T22:10:11.5170488Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2021-11-03T22:10:11.5170942Z Nov 03 22:10:11 	... 69 more
2021-11-03T22:10:11.5171362Z Nov 03 22:10:11 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-11-03T22:10:11.5171990Z Nov 03 22:10:11 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-11-03T22:10:11.5172937Z Nov 03 22:10:11 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-11-03T22:10:11.5173707Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
2021-11-03T22:10:11.5174364Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
2021-11-03T22:10:11.5174999Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
2021-11-03T22:10:11.5175621Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:681)
2021-11-03T22:10:11.5176374Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-11-03T22:10:11.5177185Z Nov 03 22:10:11 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:445)
2021-11-03T22:10:11.5177900Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-03T22:10:11.5178571Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-03T22:10:11.5179526Z Nov 03 22:10:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-03T22:10:11.5180271Z Nov 03 22:10:11 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-03T22:10:11.5181030Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-11-03T22:10:11.5182018Z Nov 03 22:10:11 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-11-03T22:10:11.5183204Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-11-03T22:10:11.5183790Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-11-03T22:10:11.5184385Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-11-03T22:10:11.5184939Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-11-03T22:10:11.5185462Z Nov 03 22:10:11 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-11-03T22:10:11.5185923Z Nov 03 22:10:11 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-11-03T22:10:11.5186504Z Nov 03 22:10:11 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-11-03T22:10:11.5186966Z Nov 03 22:10:11 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-11-03T22:10:11.5187419Z Nov 03 22:10:11 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-11-03T22:10:11.5187885Z Nov 03 22:10:11 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-11-03T22:10:11.5188346Z Nov 03 22:10:11 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-03T22:10:11.5188890Z Nov 03 22:10:11 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-03T22:10:11.5189566Z Nov 03 22:10:11 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-11-03T22:10:11.5190267Z Nov 03 22:10:11 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-11-03T22:10:11.5190700Z Nov 03 22:10:11 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-11-03T22:10:11.5191170Z Nov 03 22:10:11 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-11-03T22:10:11.5191586Z Nov 03 22:10:11 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-11-03T22:10:11.5192019Z Nov 03 22:10:11 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-11-03T22:10:11.5192544Z Nov 03 22:10:11 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-11-03T22:10:11.5192937Z Nov 03 22:10:11 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-11-03T22:10:11.5193584Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-11-03T22:10:11.5194359Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-11-03T22:10:11.5195082Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-11-03T22:10:11.5195807Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-11-03T22:10:11.5196524Z Nov 03 22:10:11 Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2021-11-03T22:10:11.5197410Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
2021-11-03T22:10:11.5198368Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
2021-11-03T22:10:11.5199370Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
2021-11-03T22:10:11.5200292Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:350)
2021-11-03T22:10:11.5201255Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
2021-11-03T22:10:11.5202193Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-11-03T22:10:11.5203230Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:474)
2021-11-03T22:10:11.5203819Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2021-11-03T22:10:11.5204599Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:788)
2021-11-03T22:10:11.5205415Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:737)
2021-11-03T22:10:11.5206119Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:947)
2021-11-03T22:10:11.5206635Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:926)
2021-11-03T22:10:11.5207306Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:740)
2021-11-03T22:10:11.5207928Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
2021-11-03T22:10:11.5208474Z Nov 03 22:10:11 	at java.lang.Thread.run(Thread.java:748)
2021-11-03T22:10:11.5209343Z Nov 03 22:10:11 Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
2021-11-03T22:10:11.5209976Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
2021-11-03T22:10:11.5210570Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
2021-11-03T22:10:11.5211092Z Nov 03 22:10:11 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-11-03T22:10:11.5211564Z Nov 03 22:10:11 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-11-03T22:10:11.5212180Z Nov 03 22:10:11 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-11-03T22:10:11.5212969Z Nov 03 22:10:11 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-11-03T22:10:11.5213361Z Nov 03 22:10:11 	... 1 more
2021-11-03T22:10:11.5215786Z Nov 03 22:10:11 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:11.5217523Z Nov 03 22:10:11 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:11.5218577Z Nov 03 22:10:11 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:11.5219513Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:11.5220068Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:11.5220721Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:11.5221505Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:11.5222138Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:11.5222733Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:11.5223230Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1254)
2021-11-03T22:10:11.5223707Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1479)
2021-11-03T22:10:11.5224231Z Nov 03 22:10:11 	at org.apache.parquet.hadoop.codec.SnappyCodec.createInputStream(SnappyCodec.java:75)
2021-11-03T22:10:11.5224772Z Nov 03 22:10:11 	at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:109)
2021-11-03T22:10:11.5225418Z Nov 03 22:10:11 	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readDictionaryPage(ColumnChunkPageReadStore.java:196)
2021-11-03T22:10:11.5226286Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader.<init>(AbstractColumnReader.java:110)
2021-11-03T22:10:11.5226876Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.vector.reader.IntColumnReader.<init>(IntColumnReader.java:33)
2021-11-03T22:10:11.5227492Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader(ParquetSplitReaderUtil.java:280)
2021-11-03T22:10:11.5228185Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readNextRowGroup(ParquetVectorizedInputFormat.java:412)
2021-11-03T22:10:11.5228961Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.nextBatch(ParquetVectorizedInputFormat.java:381)
2021-11-03T22:10:11.5229660Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readBatch(ParquetVectorizedInputFormat.java:358)
2021-11-03T22:10:11.5230333Z Nov 03 22:10:11 	at org.apache.flink.table.filesystem.LimitableBulkFormat$LimitableReader.readBatch(LimitableBulkFormat.java:108)
2021-11-03T22:10:11.5230939Z Nov 03 22:10:11 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
2021-11-03T22:10:11.5231515Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
2021-11-03T22:10:11.5232095Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
2021-11-03T22:10:11.5232614Z Nov 03 22:10:11 	... 6 more
2021-11-03T22:10:11.5232979Z Nov 03 22:10:11 
2021-11-03T22:10:11.5234489Z Exception in thread ""Thread-11"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:11.5235610Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:11.5236345Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:11.5236944Z 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:11.5237383Z 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:11.5237843Z 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:11.5238296Z 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:11.5238798Z 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:11.5239225Z 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:11.5239647Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812)
2021-11-03T22:10:11.5240102Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1789)
2021-11-03T22:10:11.5240583Z 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
2021-11-03T22:10:11.5241070Z 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
2021-11-03T22:10:11.5241561Z 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
2021-11-03T22:10:11.5242032Z 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
2021-11-03T22:10:12.8086663Z Nov 03 22:10:12 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.576 s - in org.apache.flink.formats.parquet.ParquetFileCompactionITCase
2021-11-03T22:10:12.8135630Z Exception in thread ""Thread-10"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:12.8137964Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:12.8139304Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:12.8140308Z 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:12.8141050Z 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:12.8141824Z 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:12.8142673Z 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:12.8143402Z 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:12.8144120Z 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:12.8144880Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812)
2021-11-03T22:10:12.8145632Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1789)
2021-11-03T22:10:12.8146463Z 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
2021-11-03T22:10:12.8147479Z 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
2021-11-03T22:10:12.8148320Z 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
2021-11-03T22:10:12.8149186Z 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
2021-11-03T22:10:13.1599258Z Nov 03 22:10:13 [INFO] Running org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase
2021-11-03T22:10:28.5117719Z Nov 03 22:10:28 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.346 s - in org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase
2021-11-03T22:10:28.5155684Z Exception in thread ""Thread-11"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:28.5157699Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:28.5158964Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:28.5159902Z 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:28.5160644Z 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:28.5161387Z 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:28.5162100Z 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:28.5162937Z 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:28.5163616Z 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:28.5164320Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812)
2021-11-03T22:10:28.5165054Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1789)
2021-11-03T22:10:28.5165827Z 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
2021-11-03T22:10:28.5166962Z 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
2021-11-03T22:10:28.5167777Z 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
2021-11-03T22:10:28.5168523Z 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25896&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13356"	FLINK	Closed	2	1	6732	pull-request-available, test-stability
13251589	Refactor ExpressionConverter(RexNodeConverter) in blink	"For subsequent development, refactor ExpressionConverter in the form of Rules.

Now the RexNodeConverter has 1000 lines, but these logic can do some better classification.

Later we will introduce more converter mode, refactoring can let us better maintain and modify."	FLINK	Resolved	3	7	6732	pull-request-available
13278992	Incompatible types of expression and result type thrown in codegen	"*The sql is:*
 CREATE TABLE `over10k` (
 t tinyint,
 si smallint,
 i int,
 b bigint,
 f float,
 d double,
 bo boolean,
 s varchar,
 ts timestamp,
 deci decimal(4,2),
 bin varchar
 ) WITH (
 'connector.path'='/daily_regression_batch_hive_1.10/test_window_with_specific_behavior/sources/over10k.csv',
 'format.empty-column-as-null'='true',
 'format.field-delimiter'='|',
 'connector.type'='filesystem',
 'format.derive-schema'='true',
 'format.type'='csv'
 );
 select s, rank() over (partition by s order by si), sum(b) over (partition by s order by si) from over10k limit 100;

*The data is :*
 109|277|65620|4294967305|97.25|7.80|true|nick quirinius|2013-03-01 09:11:58.703226|27.72|undecided
 93|263|65725|4294967341|6.06|4.12|false|calvin king|2013-03-01 09:11:58.703299|32.44|values clariffication
 108|383|65629|4294967510|39.55|47.67|false|jessica zipper|2013-03-01 09:11:58.703133|74.23|nap time
 89|463|65537|4294967493|64.82|13.79|true|ethan white|2013-03-01 09:11:58.703243|89.52|nap time
 88|372|65645|4294967358|34.48|11.18|true|quinn thompson|2013-03-01 09:11:58.703168|84.86|forestry
 123|432|65626|4294967435|2.39|16.49|true|david white|2013-03-01 09:11:58.703136|61.24|joggying
 57|486|65551|4294967397|36.11|9.88|true|katie xylophone|2013-03-01 09:11:58.703142|57.10|zync studies
 59|343|65787|4294967312|66.89|6.54|true|mike laertes|2013-03-01 09:11:58.703209|27.56|xylophone band
 74|267|65671|4294967409|21.14|14.64|true|priscilla miller|2013-03-01 09:11:58.703197|89.06|undecided
 25|336|65587|4294967336|71.01|14.90|true|tom ichabod|2013-03-01 09:11:58.703127|74.32|zync studies
 48|346|65712|4294967315|45.01|16.08|true|zach brown|2013-03-01 09:11:58.703108|21.68|zync studies
 84|385|65776|4294967452|35.80|32.13|false|xavier zipper|2013-03-01 09:11:58.703311|99.46|education
 58|389|65766|4294967416|95.55|20.62|false|sarah miller|2013-03-01 09:11:58.703215|70.92|history
 22|403|65565|4294967381|99.65|35.42|false|yuri johnson|2013-03-01 09:11:58.703154|94.47|geology
 55|428|65733|4294967535|99.54|5.35|false|jessica king|2013-03-01 09:11:58.703233|30.30|forestry
 117|410|65706|4294967391|50.15|0.21|false|quinn johnson|2013-03-01 09:11:58.703248|65.99|yard duty
 95|423|65573|4294967378|47.59|17.37|true|alice robinson|2013-03-01 09:11:58.703133|54.57|linguistics
 87|332|65748|4294967320|19.83|41.67|false|fred ellison|2013-03-01 09:11:58.703289|79.02|mathematics
 114|263|65674|4294967405|84.44|33.18|true|victor van buren|2013-03-01 09:11:58.703092|63.74|linguistics
 5|369|65780|4294967488|92.02|38.59|true|zach polk|2013-03-01 09:11:58.703271|67.29|yard duty
 -3|430|65667|4294967469|65.50|40.46|true|yuri xylophone|2013-03-01 09:11:58.703258|30.94|american history
 120|264|65769|4294967486|89.97|41.18|false|xavier hernandez|2013-03-01 09:11:58.703140|66.89|philosophy
 107|317|65634|4294967488|5.68|18.89|false|priscilla ichabod|2013-03-01 09:11:58.703196|39.42|joggying
 29|386|65723|4294967328|71.48|6.13|false|ulysses ichabod|2013-03-01 09:11:58.703215|86.65|xylophone band
 22|434|65768|4294967543|44.25|27.56|false|tom polk|2013-03-01 09:11:58.703306|12.30|kindergarten
 -1|274|65755|4294967300|22.01|35.52|false|oscar king|2013-03-01 09:11:58.703141|33.35|chemistry
 6|365|65603|4294967522|18.51|5.60|false|gabriella king|2013-03-01 09:11:58.703104|34.20|geology
 97|414|65757|4294967325|31.82|22.37|false|rachel nixon|2013-03-01 09:11:58.703127|61.00|nap time
 72|448|65538|4294967524|80.09|7.73|true|luke brown|2013-03-01 09:11:58.703090|95.81|american history
 51|280|65589|4294967486|57.46|23.35|false|zach xylophone|2013-03-01 09:11:58.703299|11.54|education
 12|447|65583|4294967389|0.98|29.79|true|yuri polk|2013-03-01 09:11:58.703305|1.89|wind surfing
 -1|360|65539|4294967464|4.08|39.51|false|oscar davidson|2013-03-01 09:11:58.703144|59.47|nap time
 0|380|65569|4294967425|0.94|28.93|false|sarah robinson|2013-03-01 09:11:58.703176|88.81|xylophone band
 66|478|65669|4294967339|23.66|38.34|true|yuri carson|2013-03-01 09:11:58.703228|64.68|opthamology
 12|322|65771|4294967545|84.87|10.76|false|sarah allen|2013-03-01 09:11:58.703271|0.79|joggying
 79|308|65563|4294967347|4.06|44.84|false|nick underhill|2013-03-01 09:11:58.703097|76.53|industrial engineering
 4|382|65719|4294967329|7.26|39.92|true|fred polk|2013-03-01 09:11:58.703073|73.64|mathematics
 10|448|65675|4294967392|26.20|16.30|true|rachel laertes|2013-03-01 09:11:58.703200|18.01|xylophone band
 45|281|65685|4294967513|81.33|32.22|true|oscar allen|2013-03-01 09:11:58.703285|71.38|religion
 57|288|65599|4294967422|90.33|44.25|false|bob young|2013-03-01 09:11:58.703185|11.16|biology
 77|452|65706|4294967512|22.90|5.35|true|bob van buren|2013-03-01 09:11:58.703290|14.58|debate
 103|492|65773|4294967404|58.29|48.28|false|yuri thompson|2013-03-01 09:11:58.703249|84.38|undecided
 84|411|65737|4294967486|63.13|1.10|true|katie ichabod|2013-03-01 09:11:58.703086|29.57|american history
 28|378|65589|4294967511|26.41|39.79|true|yuri polk|2013-03-01 09:11:58.703267|28.62|values clariffication
 88|478|65752|4294967364|80.59|45.13|true|victor garcia|2013-03-01 09:11:58.703081|34.90|chemistry
 37|388|65608|4294967350|32.94|39.06|false|mike polk|2013-03-01 09:11:58.703273|42.48|quiet hour
 25|264|65648|4294967402|90.83|30.96|false|tom ichabod|2013-03-01 09:11:58.703268|65.58|history
 17|455|65738|4294967508|15.73|27.01|false|david young|2013-03-01 09:11:58.703254|26.24|american history
 62|438|65655|4294967511|91.77|1.90|false|sarah steinbeck|2013-03-01 09:11:58.703150|16.41|chemistry
 65|298|65669|4294967328|68.89|2.75|true|david miller|2013-03-01 09:11:58.703077|51.86|values clariffication
 25|491|65641|4294967387|94.82|10.04|false|ulysses thompson|2013-03-01 09:11:58.703124|63.75|linguistics
 25|497|65708|4294967497|2.45|49.99|false|ethan laertes|2013-03-01 09:11:58.703320|49.72|yard duty
 117|288|65591|4294967530|75.18|2.71|false|fred quirinius|2013-03-01 09:11:58.703221|99.58|geology
 62|404|65706|4294967549|86.06|40.01|true|irene zipper|2013-03-01 09:11:58.703139|13.38|kindergarten
 99|362|65709|4294967399|50.48|26.34|false|jessica white|2013-03-01 09:11:58.703294|83.53|kindergarten
 62|395|65685|4294967446|56.73|14.87|false|victor johnson|2013-03-01 09:11:58.703194|31.42|history
 62|386|65615|4294967359|44.03|43.78|true|luke underhill|2013-03-01 09:11:58.703099|86.73|nap time
 15|302|65698|4294967526|91.38|3.59|true|wendy carson|2013-03-01 09:11:58.703111|9.46|religion
 92|507|65699|4294967512|8.44|34.72|false|calvin xylophone|2013-03-01 09:11:58.703198|66.89|study skills
 3|279|65756|4294967439|87.65|24.72|false|david white|2013-03-01 09:11:58.703233|47.19|study skills
 114|330|65754|4294967500|76.20|39.35|true|rachel quirinius|2013-03-01 09:11:58.703145|76.16|undecided
 24|500|65717|4294967535|60.96|21.51|false|victor falkner|2013-03-01 09:11:58.703318|82.83|nap time
 -2|331|65707|4294967335|67.12|13.51|false|bob ovid|2013-03-01 09:11:58.703285|62.32|joggying
 101|463|65740|4294967425|52.27|11.58|true|priscilla robinson|2013-03-01 09:11:58.703078|13.09|yard duty
 106|269|65577|4294967524|17.11|38.45|true|rachel falkner|2013-03-01 09:11:58.703197|79.89|xylophone band
 121|500|65690|4294967517|49.31|9.85|false|luke robinson|2013-03-01 09:11:58.703074|37.91|topology
 37|351|65587|4294967410|99.66|20.51|false|quinn falkner|2013-03-01 09:11:58.703221|80.69|history
 6|340|65612|4294967345|54.08|3.53|true|oscar white|2013-03-01 09:11:58.703279|68.67|debate
 115|366|65785|4294967330|90.00|25.79|true|jessica carson|2013-03-01 09:11:58.703143|2.72|xylophone band
 124|307|65649|4294967368|81.66|19.35|true|wendy ichabod|2013-03-01 09:11:58.703254|73.76|opthamology
 11|286|65752|4294967355|72.33|20.94|false|xavier carson|2013-03-01 09:11:58.703109|23.28|history
 15|320|65716|4294967505|49.25|27.53|false|fred carson|2013-03-01 09:11:58.703263|18.08|industrial engineering
 76|316|65706|4294967460|12.99|35.53|true|rachel davidson|2013-03-01 09:11:58.703300|85.43|quiet hour
 -2|485|65788|4294967510|9.99|22.75|false|luke carson|2013-03-01 09:11:58.703217|82.56|mathematics
 87|482|65612|4294967327|16.51|22.21|true|katie nixon|2013-03-01 09:11:58.703083|47.09|xylophone band
 21|400|65777|4294967354|4.05|11.10|false|david quirinius|2013-03-01 09:11:58.703205|25.69|geology
 97|343|65764|4294967427|47.79|18.94|true|ethan miller|2013-03-01 09:11:58.703308|39.81|topology
 2|292|65783|4294967420|38.86|12.14|true|wendy robinson|2013-03-01 09:11:58.703239|72.70|wind surfing
 48|440|65570|4294967438|41.44|13.11|true|bob thompson|2013-03-01 09:11:58.703122|57.67|american history
 87|333|65592|4294967296|71.77|8.28|false|yuri nixon|2013-03-01 09:11:58.703302|87.58|quiet hour
 -1|344|65616|4294967444|29.44|19.94|false|oscar falkner|2013-03-01 09:11:58.703203|28.22|geology
 1|425|65625|4294967531|51.83|38.18|false|holly xylophone|2013-03-01 09:11:58.703198|0.31|geology
 108|363|65715|4294967467|99.69|17.10|true|yuri xylophone|2013-03-01 09:11:58.703177|44.91|geology
 93|500|65778|4294967442|82.52|38.24|true|xavier falkner|2013-03-01 09:11:58.703277|25.41|history
 112|260|65612|4294967500|51.90|24.53|false|rachel falkner|2013-03-01 09:11:58.703211|65.45|american history
 89|294|65754|4294967450|94.21|35.55|true|gabriella falkner|2013-03-01 09:11:58.703156|18.36|topology
 32|389|65700|4294967525|42.65|32.59|true|yuri king|2013-03-01 09:11:58.703253|1.70|undecided
 13|395|65715|4294967317|64.24|36.77|false|fred ovid|2013-03-01 09:11:58.703168|74.25|yard duty
 5|262|65726|4294967543|8.85|12.89|true|rachel garcia|2013-03-01 09:11:58.703222|45.65|yard duty
 65|324|65569|4294967315|93.15|41.46|false|alice brown|2013-03-01 09:11:58.703110|77.23|topology
 73|477|65764|4294967542|27.96|44.68|false|bob steinbeck|2013-03-01 09:11:58.703173|90.95|undecided
 6|337|65616|4294967456|38.34|34.04|true|rachel hernandez|2013-03-01 09:11:58.703223|60.63|debate
 51|384|65649|4294967423|14.62|5.33|true|oscar king|2013-03-01 09:11:58.703232|21.96|history
 87|369|65626|4294967403|20.94|26.46|true|ulysses hernandez|2013-03-01 09:11:58.703076|35.79|values clariffication
 48|365|65558|4294967361|66.17|6.28|true|alice xylophone|2013-03-01 09:11:58.703081|51.13|study skills
 12|388|65642|4294967298|58.26|34.09|false|jessica brown|2013-03-01 09:11:58.703081|92.61|linguistics
 12|353|65703|4294967414|54.55|5.92|true|jessica johnson|2013-03-01 09:11:58.703289|91.71|chemistry
 117|499|65566|4294967328|32.18|19.59|true|priscilla king|2013-03-01 09:11:58.703214|66.88|philosophy
 116|363|65719|4294967513|18.59|48.19|false|priscilla johnson|2013-03-01 09:11:58.703237|55.47|history
 21|433|65551|4294967366|84.35|34.09|false|oscar thompson|2013-03-01 09:11:58.703291|7.99|values clariffication
 -2|409|65717|4294967343|39.62|9.79|true|irene ichabod|2013-03-01 09:11:58.703315|64.80|joggying
 23|495|65785|4294967473|30.91|21.95|true|fred robinson|2013-03-01 09:11:58.703240|66.34|nap time
 30|507|65673|4294967453|83.51|40.92|true|oscar thompson|2013-03-01 09:11:58.703281|65.25|values clariffication
 13|365|65594|4294967446|13.41|34.03|true|irene white|2013-03-01 09:11:58.703084|52.53|topology
 92|419|65771|4294967310|64.82|3.01|false|yuri brown|2013-03-01 09:11:58.703271|18.05|undecided
 81|351|65781|4294967473|48.46|15.80|false|bob nixon|2013-03-01 09:11:58.703254|99.35|debate
 105|490|65543|4294967334|32.91|42.91|false|yuri steinbeck|2013-03-01 09:11:58.703233|42.19|xylophone band
 25|402|65619|4294967340|6.28|49.92|true|victor xylophone|2013-03-01 09:11:58.703210|84.32|philosophy
 88|485|65557|4294967391|95.95|46.22|true|irene xylophone|2013-03-01 09:11:58.703141|63.31|mathematics
 81|285|65758|4294967338|37.83|38.23|true|irene ichabod|2013-03-01 09:11:58.703322|43.31|quiet hour
 96|316|65764|4294967442|86.76|32.89|false|wendy miller|2013-03-01 09:11:58.703190|10.35|geology
 43|321|65538|4294967422|81.78|6.07|false|zach van buren|2013-03-01 09:11:58.703273|26.02|topology
 60|496|65614|4294967376|34.40|45.59|true|jessica steinbeck|2013-03-01 09:11:58.703076|81.95|xylophone band
 44|395|65611|4294967443|15.58|1.53|false|gabriella thompson|2013-03-01 09:11:58.703295|11.00|values clariffication
 73|409|65767|4294967371|36.93|36.16|true|quinn ellison|2013-03-01 09:11:58.703105|82.70|religion
 121|330|65772|4294967508|70.46|44.50|true|quinn zipper|2013-03-01 09:11:58.703272|11.31|philosophy
 61|421|65541|4294967410|34.59|27.52|false|calvin johnson|2013-03-01 09:11:58.703299|3.52|history
 65|370|65674|4294967474|6.94|4.38|false|tom falkner|2013-03-01 09:11:58.703142|63.24|wind surfing
 41|462|65699|4294967391|58.03|17.26|false|calvin xylophone|2013-03-01 09:11:58.703322|92.60|study skills
 97|460|65591|4294967515|46.39|2.16|false|mike carson|2013-03-01 09:11:58.703265|97.16|values clariffication
 -1|435|65624|4294967377|73.60|45.63|true|irene hernandez|2013-03-01 09:11:58.703208|31.35|study skills
 22|282|65782|4294967318|75.19|40.78|false|quinn ichabod|2013-03-01 09:11:58.703122|44.85|topology
 46|487|65748|4294967318|67.01|24.13|false|victor zipper|2013-03-01 09:11:58.703273|95.40|linguistics
 18|275|65757|4294967307|80.45|18.92|false|bob hernandez|2013-03-01 09:11:58.703307|38.25|education
 103|264|65587|4294967306|97.65|11.36|false|david ovid|2013-03-01 09:11:58.703265|42.76|wind surfing
 86|466|65642|4294967333|40.96|26.06|true|david young|2013-03-01 09:11:58.703155|2.99|kindergarten
 119|437|65637|4294967494|18.93|31.04|true|calvin brown|2013-03-01 09:11:58.703241|30.45|debate
 62|285|65593|4294967518|83.43|2.05|false|rachel xylophone|2013-03-01 09:11:58.703084|45.21|quiet hour
 1|283|65752|4294967528|95.01|1.76|false|ethan ichabod|2013-03-01 09:11:58.703072|16.68|history
 8|333|65732|4294967503|22.43|21.80|false|mike polk|2013-03-01 09:11:58.703160|71.80|industrial engineering
 90|425|65648|4294967323|50.68|40.41|false|victor allen|2013-03-01 09:11:58.703146|58.75|kindergarten
 110|319|65620|4294967332|32.36|35.17|true|ethan davidson|2013-03-01 09:11:58.703269|73.03|history
 111|313|65711|4294967418|70.04|10.88|true|priscilla nixon|2013-03-01 09:11:58.703206|66.32|mathematics
 96|399|65719|4294967401|52.35|4.01|true|rachel hernandez|2013-03-01 09:11:58.703076|32.45|values clariffication
 83|353|65714|4294967384|10.12|15.81|false|rachel miller|2013-03-01 09:11:58.703110|16.39|philosophy
 11|475|65747|4294967303|98.29|32.30|false|yuri king|2013-03-01 09:11:58.703285|11.06|forestry
 84|295|65682|4294967463|17.75|23.28|true|alice zipper|2013-03-01 09:11:58.703306|79.77|industrial engineering
 8|348|65626|4294967373|52.54|31.29|false|bob underhill|2013-03-01 09:11:58.703189|82.40|undecided
 0|339|65603|4294967356|32.42|31.31|false|katie young|2013-03-01 09:11:58.703238|49.14|forestry
 82|280|65688|4294967427|19.11|0.10|false|holly young|2013-03-01 09:11:58.703256|71.39|chemistry
 119|465|65781|4294967467|23.83|0.95|false|yuri zipper|2013-03-01 09:11:58.703094|96.06|history
 10|356|65586|4294967339|71.96|32.54|true|oscar zipper|2013-03-01 09:11:58.703091|73.01|quiet hour
 25|364|65682|4294967449|50.96|34.46|true|sarah steinbeck|2013-03-01 09:11:58.703139|18.28|philosophy
 47|270|65652|4294967393|85.46|33.87|true|luke zipper|2013-03-01 09:11:58.703173|96.68|philosophy
 89|470|65676|4294967314|39.34|37.35|false|ulysses miller|2013-03-01 09:11:58.703303|69.67|values clariffication
 105|393|65703|4294967359|19.00|45.80|false|oscar johnson|2013-03-01 09:11:58.703086|99.42|linguistics
 120|415|65785|4294967498|54.68|32.92|true|calvin hernandez|2013-03-01 09:11:58.703086|93.09|linguistics
 94|486|65649|4294967549|33.47|35.42|false|jessica carson|2013-03-01 09:11:58.703089|34.30|mathematics
 38|288|65634|4294967304|5.10|44.83|false|ethan white|2013-03-01 09:11:58.703083|0.94|xylophone band
 91|268|65578|4294967501|43.98|2.77|false|jessica white|2013-03-01 09:11:58.703195|51.68|joggying
 123|409|65629|4294967431|29.23|27.30|false|ulysses garcia|2013-03-01 09:11:58.703141|70.01|philosophy
 7|454|65697|4294967394|62.25|3.38|false|tom underhill|2013-03-01 09:11:58.703121|47.97|values clariffication
 13|488|65662|4294967457|25.08|4.01|false|quinn van buren|2013-03-01 09:11:58.703272|35.40|history
 118|388|65642|4294967438|52.78|15.67|true|rachel falkner|2013-03-01 09:11:58.703158|61.13|opthamology
 1|315|65713|4294967509|43.80|24.95|false|nick brown|2013-03-01 09:11:58.703287|83.95|mathematics
 11|416|65658|4294967433|19.94|8.97|false|jessica nixon|2013-03-01 09:11:58.703117|63.58|joggying
 42|457|65669|4294967534|13.45|16.47|true|calvin polk|2013-03-01 09:11:58.703257|59.51|yard duty
 119|467|65639|4294967304|57.17|35.89|false|nick nixon|2013-03-01 09:11:58.703088|0.98|history
 5|383|65629|4294967302|70.92|32.41|false|rachel young|2013-03-01 09:11:58.703314|1.72|opthamology
 108|304|65557|4294967498|26.30|33.01|true|tom nixon|2013-03-01 09:11:58.703189|70.64|opthamology
 60|447|65778|4294967546|65.11|14.36|true|yuri robinson|2013-03-01 09:11:58.703284|45.69|joggying
 65|406|65613|4294967522|93.10|16.27|false|xavier laertes|2013-03-01 09:11:58.703178|25.19|philosophy
 113|482|65739|4294967311|51.17|36.29|true|priscilla steinbeck|2013-03-01 09:11:58.703084|13.07|kindergarten
 58|453|65780|4294967484|25.45|1.99|false|alice ichabod|2013-03-01 09:11:58.703307|25.71|nap time
 24|320|65759|4294967315|23.99|43.22|false|irene robinson|2013-03-01 09:11:58.703095|24.36|chemistry
 112|438|65622|4294967483|62.47|21.21|false|tom laertes|2013-03-01 09:11:58.703257|54.45|nap time
 89|382|65708|4294967459|40.10|45.17|false|luke ovid|2013-03-01 09:11:58.703325|59.38|yard duty
 63|410|65561|4294967330|86.99|24.01|false|fred underhill|2013-03-01 09:11:58.703288|29.48|religion
 103|462|65658|4294967533|48.98|46.63|true|wendy laertes|2013-03-01 09:11:58.703272|85.64|philosophy
 97|279|65563|4294967322|79.42|41.65|false|yuri thompson|2013-03-01 09:11:58.703308|43.37|mathematics
 122|375|65717|4294967513|99.32|27.37|true|rachel falkner|2013-03-01 09:11:58.703095|65.37|philosophy
 25|481|65672|4294967454|98.90|37.58|false|oscar ovid|2013-03-01 09:11:58.703293|73.85|biology
 71|409|65667|4294967420|1.98|44.05|true|alice brown|2013-03-01 09:11:58.703117|38.55|religion
 86|399|65568|4294967404|26.97|34.10|true|priscilla ichabod|2013-03-01 09:11:58.703283|87.92|yard duty
 114|348|65752|4294967368|18.90|42.15|false|irene zipper|2013-03-01 09:11:58.703154|63.92|debate
 31|464|65683|4294967364|20.61|48.84|false|irene garcia|2013-03-01 09:11:58.703219|80.62|american history
 30|302|65688|4294967477|7.75|5.34|false|quinn polk|2013-03-01 09:11:58.703085|80.36|geology
 72|423|65665|4294967353|54.78|15.57|false|fred quirinius|2013-03-01 09:11:58.703219|56.86|philosophy
 78|408|65609|4294967534|83.25|24.25|false|quinn falkner|2013-03-01 09:11:58.703074|29.42|quiet hour
 35|308|65659|4294967371|89.52|45.35|true|luke carson|2013-03-01 09:11:58.703276|78.07|wind surfing
 13|310|65558|4294967399|60.05|38.39|false|priscilla polk|2013-03-01 09:11:58.703194|53.92|mathematics
 80|450|65537|4294967548|74.10|8.87|true|ulysses falkner|2013-03-01 09:11:58.703139|56.48|nap time
 30|295|65743|4294967359|17.51|44.20|true|bob hernandez|2013-03-01 09:11:58.703242|59.71|quiet hour
 25|372|65606|4294967412|99.40|36.98|false|yuri quirinius|2013-03-01 09:11:58.703242|87.18|zync studies
 -3|454|65733|4294967544|73.83|18.42|false|bob ichabod|2013-03-01 09:11:58.703240|95.56|debate
 9|440|65773|4294967362|30.46|44.91|true|xavier falkner|2013-03-01 09:11:58.703098|62.35|religion
 105|289|65576|4294967342|76.65|29.47|false|ulysses garcia|2013-03-01 09:11:58.703282|71.95|chemistry
 116|263|65757|4294967525|94.04|37.06|false|priscilla hernandez|2013-03-01 09:11:58.703072|13.75|linguistics
 124|458|65726|4294967483|7.96|0.29|false|zach laertes|2013-03-01 09:11:58.703281|1.46|study skills
 -3|507|65671|4294967305|60.28|41.50|false|quinn polk|2013-03-01 09:11:58.703244|77.17|industrial engineering
 -3|458|65679|4294967331|64.29|43.80|true|irene young|2013-03-01 09:11:58.703084|2.61|american history
 17|435|65739|4294967438|44.39|9.29|false|alice thompson|2013-03-01 09:11:58.703241|68.01|undecided
 33|390|65564|4294967305|8.20|17.36|false|calvin laertes|2013-03-01 09:11:58.703176|65.07|zync studies
 73|474|65789|4294967421|62.00|40.44|true|alice quirinius|2013-03-01 09:11:58.703101|98.80|geology
 46|313|65692|4294967310|93.40|34.70|true|fred hernandez|2013-03-01 09:11:58.703196|26.80|geology
 50|302|65581|4294967387|2.73|18.54|false|jessica carson|2013-03-01 09:11:58.703282|58.24|study skills
 115|311|65651|4294967423|44.94|33.29|true|ethan laertes|2013-03-01 09:11:58.703116|63.49|biology
 88|368|65556|4294967428|37.79|47.21|true|tom laertes|2013-03-01 09:11:58.703149|7.26|topology
 59|476|65560|4294967341|26.00|21.70|true|irene ovid|2013-03-01 09:11:58.703224|37.32|wind surfing
 33|489|65723|4294967491|52.08|36.13|false|quinn robinson|2013-03-01 09:11:58.703174|29.70|chemistry
 69|329|65580|4294967527|45.37|25.36|true|irene ichabod|2013-03-01 09:11:58.703267|95.34|joggying
 8|342|65542|4294967486|86.51|30.05|true|ulysses johnson|2013-03-01 09:11:58.703164|4.89|kindergarten
 47|327|65660|4294967329|53.96|10.07|false|fred white|2013-03-01 09:11:58.703313|48.34|zync studies
 77|296|65771|4294967420|94.25|12.67|true|ulysses underhill|2013-03-01 09:11:58.703080|45.67|biology
 63|451|65581|4294967493|44.66|40.63|true|alice miller|2013-03-01 09:11:58.703071|97.98|geology
 103|303|65605|4294967540|54.00|47.97|true|fred davidson|2013-03-01 09:11:58.703087|68.42|zync studies
 68|300|65577|4294967395|8.00|27.76|false|quinn quirinius|2013-03-01 09:11:58.703124|14.35|values clariffication
 41|424|65684|4294967396|44.97|44.01|false|calvin polk|2013-03-01 09:11:58.703161|31.72|linguistics
 84|448|65649|4294967425|5.81|28.49|true|ulysses ichabod|2013-03-01 09:11:58.703317|96.87|history
 30|398|65577|4294967306|71.32|39.24|false|katie zipper|2013-03-01 09:11:58.703310|97.22|wind surfing
 70|361|65695|4294967371|6.97|45.29|false|oscar falkner|2013-03-01 09:11:58.703268|79.32|opthamology
 92|371|65702|4294967518|29.30|18.48|false|david ellison|2013-03-01 09:11:58.703192|30.01|topology
 10|298|65666|4294967460|82.71|16.06|true|irene white|2013-03-01 09:11:58.703198|64.62|quiet hour
 109|496|65699|4294967536|36.99|14.91|true|holly hernandez|2013-03-01 09:11:58.703123|66.43|geology
 68|383|65597|4294967334|84.64|1.14|true|holly falkner|2013-03-01 09:11:58.703210|96.35|kindergarten
 95|433|65738|4294967363|95.88|45.88|false|rachel steinbeck|2013-03-01 09:11:58.703308|34.85|history
 37|262|65773|4294967482|26.04|4.86|true|oscar hernandez|2013-03-01 09:11:58.703285|92.63|linguistics
 24|421|65676|4294967355|23.99|14.11|true|ulysses ovid|2013-03-01 09:11:58.703281|19.16|forestry
 91|485|65607|4294967315|55.90|17.62|false|zach nixon|2013-03-01 09:11:58.703305|83.23|joggying
 67|387|65790|4294967318|93.14|31.43|false|irene king|2013-03-01 09:11:58.703188|6.25|industrial engineering
 82|262|65571|4294967465|56.70|30.18|true|irene van buren|2013-03-01 09:11:58.703167|3.00|study skills
 98|505|65582|4294967365|17.40|40.51|false|sarah polk|2013-03-01 09:11:58.703121|56.65|history
 22|268|65612|4294967462|9.69|4.64|false|xavier ichabod|2013-03-01 09:11:58.703304|3.86|linguistics
 10|332|65685|4294967332|76.12|20.13|true|priscilla laertes|2013-03-01 09:11:58.703170|82.71|opthamology
 36|317|65641|4294967471|56.22|36.78|true|tom johnson|2013-03-01 09:11:58.703296|53.38|biology
 60|501|65555|4294967313|13.57|11.68|true|yuri davidson|2013-03-01 09:11:58.703183|10.42|religion
 123|267|65560|4294967438|40.69|11.41|true|ethan allen|2013-03-01 09:11:58.703086|91.03|undecided
 -2|482|65558|4294967487|36.92|49.78|true|nick johnson|2013-03-01 09:11:58.703204|39.91|industrial engineering
 59|270|65726|4294967372|48.94|37.15|false|oscar polk|2013-03-01 09:11:58.703221|12.67|quiet hour
 119|385|65595|4294967373|36.66|15.82|true|jessica nixon|2013-03-01 09:11:58.703127|5.26|zync studies
 122|306|65751|4294967471|56.79|48.37|true|bob hernandez|2013-03-01 09:11:58.703186|50.61|kindergarten
 64|402|65777|4294967481|77.49|13.11|false|nick carson|2013-03-01 09:11:58.703264|66.64|study skills
 48|465|65758|4294967485|75.39|30.96|false|ethan allen|2013-03-01 09:11:58.703076|10.00|joggying
 117|458|65603|4294967342|53.32|32.59|true|ethan garcia|2013-03-01 09:11:58.703204|47.35|yard duty
 23|283|65557|4294967415|24.61|14.57|false|fred white|2013-03-01 09:11:58.703082|12.44|chemistry
 56|507|65538|4294967507|67.82|42.13|false|alice king|2013-03-01 09:11:58.703297|54.64|american history
 96|436|65737|4294967528|81.66|27.09|false|tom zipper|2013-03-01 09:11:58.703199|85.16|debate
 88|292|65578|4294967546|91.57|37.42|false|nick zipper|2013-03-01 09:11:58.703294|96.08|religion
 73|481|65717|4294967391|40.07|27.66|true|yuri xylophone|2013-03-01 09:11:58.703120|18.21|history
 80|280|65620|4294967482|58.09|40.39|false|fred polk|2013-03-01 09:11:58.703136|23.61|xylophone band
 96|464|65659|4294967493|74.22|21.71|true|jessica ichabod|2013-03-01 09:11:58.703226|92.72|undecided
 103|485|65707|4294967436|94.57|21.16|true|zach van buren|2013-03-01 09:11:58.703313|3.93|study skills
 31|410|65566|4294967518|36.11|16.72|true|nick ellison|2013-03-01 09:11:58.703305|61.53|biology
 -3|270|65702|4294967512|38.05|1.07|true|david carson|2013-03-01 09:11:58.703136|28.07|philosophy
 3|404|65709|4294967473|14.86|48.87|true|mike quirinius|2013-03-01 09:11:58.703099|37.99|xylophone band
 124|473|65644|4294967314|65.16|19.33|false|oscar white|2013-03-01 09:11:58.703194|33.17|debate
 103|321|65572|4294967353|64.79|0.22|false|david robinson|2013-03-01 09:11:58.703187|20.31|linguistics
 41|395|65686|4294967428|61.99|11.61|false|sarah steinbeck|2013-03-01 09:11:58.703278|17.45|biology
 -3|469|65752|4294967350|55.41|32.11|true|oscar johnson|2013-03-01 09:11:58.703110|47.32|philosophy
 98|336|65641|4294967519|82.11|7.91|true|tom davidson|2013-03-01 09:11:58.703320|83.43|debate
 54|422|65655|4294967551|15.74|34.11|true|bob garcia|2013-03-01 09:11:58.703086|46.93|yard duty
 70|462|65671|4294967385|82.68|7.94|false|fred white|2013-03-01 09:11:58.703167|45.89|joggying
 62|325|65751|4294967342|36.71|28.42|true|priscilla garcia|2013-03-01 09:11:58.703239|0.56|mathematics
 56|504|65635|4294967318|93.88|34.87|true|holly polk|2013-03-01 09:11:58.703227|89.14|american history
 50|275|65697|4294967322|58.10|27.56|false|priscilla johnson|2013-03-01 09:11:58.703096|6.19|biology
 114|428|65680|4294967498|62.68|3.90|true|yuri nixon|2013-03-01 09:11:58.703086|53.28|xylophone band
 100|277|65739|4294967382|1.61|18.22|true|wendy garcia|2013-03-01 09:11:58.703137|78.35|industrial engineering
 7|494|65601|4294967403|20.76|19.41|false|david underhill|2013-03-01 09:11:58.703164|70.81|topology
 79|448|65744|4294967479|18.18|36.26|true|david xylophone|2013-03-01 09:11:58.703310|76.40|joggying
 19|289|65562|4294967344|56.25|33.81|true|sarah van buren|2013-03-01 09:11:58.703301|64.05|forestry
 10|508|65589|4294967473|96.49|7.56|false|priscilla brown|2013-03-01 09:11:58.703134|2.08|education
 89|451|65686|4294967396|21.20|13.22|true|oscar king|2013-03-01 09:11:58.703127|49.12|undecided
 45|323|65540|4294967436|29.79|5.69|false|tom falkner|2013-03-01 09:11:58.703102|53.85|nap time
 34|319|65780|4294967523|80.40|9.05|true|sarah falkner|2013-03-01 09:11:58.703179|75.06|yard duty
 30|510|65632|4294967373|60.94|21.31|true|gabriella steinbeck|2013-03-01 09:11:58.703146|69.16|undecided
 72|350|65742|4294967491|3.33|30.48|false|katie johnson|2013-03-01 09:11:58.703315|55.83|topology
 96|402|65620|4294967320|19.38|49.45|false|oscar steinbeck|2013-03-01 09:11:58.703303|25.84|yard duty
 95|405|65536|4294967338|18.26|1.46|false|sarah thompson|2013-03-01 09:11:58.703073|29.27|education
 80|396|65675|4294967379|30.21|28.41|false|rachel white|2013-03-01 09:11:58.703316|11.37|topology
 5|507|65715|4294967297|87.39|16.09|true|sarah xylophone|2013-03-01 09:11:58.703321|0.46|nap time
 52|322|65635|4294967296|13.25|10.02|false|wendy falkner|2013-03-01 09:11:58.703094|2.51|industrial engineering
 64|345|65744|4294967316|23.26|29.25|true|sarah brown|2013-03-01 09:11:58.703245|96.45|kindergarten
 97|502|65654|4294967405|0.09|3.10|false|victor robinson|2013-03-01 09:11:58.703141|29.03|religion
 25|424|65599|4294967303|49.92|33.86|true|calvin miller|2013-03-01 09:11:58.703095|76.80|study skills
 115|298|65599|4294967457|78.69|11.89|false|luke steinbeck|2013-03-01 09:11:58.703245|22.81|geology
 49|496|65722|4294967407|17.46|33.62|false|ethan underhill|2013-03-01 09:11:58.703158|7.67|forestry
 77|315|65592|4294967532|28.72|38.15|false|nick robinson|2013-03-01 09:11:58.703296|78.69|debate
 33|258|65780|4294967448|5.78|19.07|true|calvin davidson|2013-03-01 09:11:58.703133|18.12|study skills
 98|390|65592|4294967397|36.40|29.61|false|sarah young|2013-03-01 09:11:58.703314|74.60|wind surfing
 41|415|65618|4294967426|2.23|46.43|true|nick van buren|2013-03-01 09:11:58.703225|14.78|yard duty
 62|427|65671|4294967359|75.01|38.93|false|bob ovid|2013-03-01 09:11:58.703195|17.17|values clariffication
 -2|294|65588|4294967301|8.51|2.16|false|zach zipper|2013-03-01 09:11:58.703208|35.15|debate
 94|309|65653|4294967447|6.14|5.65|false|yuri van buren|2013-03-01 09:11:58.703279|94.47|study skills
 120|377|65615|4294967364|24.99|12.26|true|oscar nixon|2013-03-01 09:11:58.703250|71.62|industrial engineering
 3|500|65756|4294967445|98.38|39.43|true|luke nixon|2013-03-01 09:11:58.703243|29.49|yard duty
 -1|505|65611|4294967338|75.26|22.98|false|mike allen|2013-03-01 09:11:58.703123|95.80|linguistics
 124|466|65612|4294967456|72.76|15.57|false|calvin polk|2013-03-01 09:11:58.703235|37.15|biology
 1|490|65591|4294967329|69.89|40.29|false|luke laertes|2013-03-01 09:11:58.703104|58.27|quiet hour
 70|385|65553|4294967506|69.14|44.05|false|ethan xylophone|2013-03-01 09:11:58.703150|93.69|chemistry
 68|330|65573|4294967506|66.87|17.31|true|jessica hernandez|2013-03-01 09:11:58.703124|30.57|zync studies
 82|421|65699|4294967550|84.77|40.40|false|gabriella white|2013-03-01 09:11:58.703292|29.99|history
 9|346|65646|4294967449|66.32|24.07|false|jessica xylophone|2013-03-01 09:11:58.703084|94.86|undecided
 116|336|65638|4294967327|64.45|11.24|true|jessica falkner|2013-03-01 09:11:58.703087|60.05|study skills
 19|376|65770|4294967536|79.12|20.11|false|victor carson|2013-03-01 09:11:58.703243|72.69|industrial engineering
 27|433|65767|4294967395|22.53|18.81|false|bob polk|2013-03-01 09:11:58.703097|52.68|linguistics
 31|468|65654|4294967361|33.08|29.95|false|bob young|2013-03-01 09:11:58.703210|16.48|philosophy
 84|411|65564|4294967493|49.25|7.84|true|oscar nixon|2013-03-01 09:11:58.703274|47.54|american history
 37|409|65769|4294967384|25.89|42.27|false|katie underhill|2013-03-01 09:11:58.703172|66.93|zync studies
 10|356|65628|4294967475|98.07|13.86|false|david carson|2013-03-01 09:11:58.703222|7.37|nap time
 105|437|65664|4294967535|2.05|17.01|true|holly laertes|2013-03-01 09:11:58.703144|5.69|industrial engineering
 117|508|65788|4294967319|66.86|25.25|false|ulysses davidson|2013-03-01 09:11:58.703283|85.22|industrial engineering
 108|322|65697|4294967529|20.24|40.23|true|mike carson|2013-03-01 09:11:58.703083|6.04|philosophy
 80|426|65735|4294967533|73.85|41.99|false|quinn hernandez|2013-03-01 09:11:58.703098|69.55|mathematics
 49|434|65692|4294967336|89.33|14.24|true|yuri underhill|2013-03-01 09:11:58.703127|3.91|quiet hour
 74|501|65657|4294967451|88.85|11.09|true|bob king|2013-03-01 09:11:58.703175|51.36|quiet hour
 8|380|65734|4294967369|84.11|10.24|false|victor underhill|2013-03-01 09:11:58.703291|78.90|opthamology
 89|364|65735|4294967334|12.41|24.02|false|nick nixon|2013-03-01 09:11:58.703272|34.80|debate
 53|479|65579|4294967303|7.50|43.05|false|rachel ellison|2013-03-01 09:11:58.703148|48.50|yard duty
 67|493|65626|4294967489|98.74|32.74|false|katie thompson|2013-03-01 09:11:58.703263|87.95|geology
 56|390|65676|4294967456|42.59|1.64|true|wendy king|2013-03-01 09:11:58.703307|39.31|joggying
 13|431|65624|4294967330|94.05|30.76|false|quinn ichabod|2013-03-01 09:11:58.703180|1.72|biology
 85|366|65627|4294967356|37.14|35.57|true|alice king|2013-03-01 09:11:58.703170|6.78|yard duty
 -2|286|65549|4294967493|9.20|1.23|true|ulysses king|2013-03-01 09:11:58.703218|93.35|study skills
 51|344|65698|4294967309|83.66|6.12|false|zach ellison|2013-03-01 09:11:58.703158|29.28|yard duty
 89|489|65610|4294967353|64.70|8.13|true|katie polk|2013-03-01 09:11:58.703120|56.34|education
 95|327|65747|4294967522|1.16|12.00|true|bob van buren|2013-03-01 09:11:58.703284|3.45|opthamology
 50|508|65541|4294967451|37.38|46.94|true|quinn steinbeck|2013-03-01 09:11:58.703081|20.90|forestry
 6|301|65693|4294967454|89.07|41.96|true|alice ichabod|2013-03-01 09:11:58.703297|16.13|religion
 7|322|65719|4294967434|1.02|29.24|false|quinn carson|2013-03-01 09:11:58.703293|47.99|forestry
 99|469|65751|4294967356|10.10|42.47|false|wendy young|2013-03-01 09:11:58.703180|63.14|opthamology
 18|269|65751|4294967544|87.84|0.60|true|mike steinbeck|2013-03-01 09:11:58.703167|36.04|religion
 22|361|65729|4294967328|67.51|15.52|false|zach ovid|2013-03-01 09:11:58.703317|26.96|quiet hour
 114|455|65723|4294967481|4.94|33.44|false|alice van buren|2013-03-01 09:11:58.703074|72.22|philosophy
 -3|384|65676|4294967453|71.97|31.52|false|alice davidson|2013-03-01 09:11:58.703226|14.28|xylophone band
 37|334|65775|4294967518|17.88|45.96|false|zach ellison|2013-03-01 09:11:58.703260|9.92|nap time
 28|427|65648|4294967309|45.65|3.90|true|bob robinson|2013-03-01 09:11:58.703308|89.89|chemistry
 86|469|65780|4294967466|64.61|24.76|true|david steinbeck|2013-03-01 09:11:58.703241|0.68|linguistics
 61|455|65567|4294967315|84.80|25.83|false|alice robinson|2013-03-01 09:11:58.703127|26.03|zync studies
 -3|387|65550|4294967355|84.75|22.75|true|holly thompson|2013-03-01 09:11:58.703073|52.01|biology
 14|492|65690|4294967388|98.07|15.98|true|david miller|2013-03-01 09:11:58.703096|15.69|forestry
 8|318|65687|4294967551|44.02|14.70|false|quinn thompson|2013-03-01 09:11:58.703205|23.43|joggying
 117|502|65789|4294967441|55.39|8.22|false|tom allen|2013-03-01 09:11:58.703129|74.48|xylophone band
 20|285|65783|4294967424|99.34|21.19|false|alice thompson|2013-03-01 09:11:58.703223|9.55|opthamology
 4|478|65538|4294967312|21.90|0.85|false|sarah thompson|2013-03-01 09:11:58.703089|79.07|xylophone band

*The conf is:*

execution:
 planner: blink
 type: batch

*After excuse the sql above, there will be the exception :*
 [ERROR] Could not execute SQL statement. Reason:
 org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(((int) 0),false,,INT NOT NULL,Some(0))] type is [INT NOT NULL], result type is [SMALLINT]"	FLINK	Resolved	2	1	6732	pull-request-available
13450354	Adjust table store document to catalog	After quite a bit of development, we needed to adjust the documentation, use the latest model, and tweak some details.	FLINK	Closed	3	4	6732	pull-request-available
13473756	Throw better exception when file not found in reading	"When reading a file, if it is found that the file does not exist, it directly throws a file not found exception, which is often difficult for users to understand.
We can make it more clear in the exception message, e.g.
The file cannot be found, this may be because the read is too slow and the previous snapshot expired, you can configure a larger snapshot.time-retained or speed up your read.


Caused by: java.io.FileNotFoundException: File does not exist: 
at org.apache.flink.table.store.file.utils.FileUtils.getFileSize(FileUtils.java:94) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]
at org.apache.flink.table.store.file.data.DataFileReader$DataFileRecordReader.<init>(DataFileReader.java:86) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]"	FLINK	Closed	1	4	6732	pull-request-available
13509170	Introduce Table.copy from dynamic options	"At present, our processing of dynamic options is relatively independent. In FileStoreTableFactory, this is not conducive to other engines configuring dynamic options.

We should propose an interface on the Table, and dynamic options can be configured at any time."	FLINK	Closed	3	4	6732	pull-request-available
13517570	Introduce DataType for table store	Introduce table store own DataType to decouple Flink SQL LogicalType.	FLINK	Closed	3	7	6732	pull-request-available
13452172	Table Store: Bucket pruning based on primary key filter	"TABLE(a, b, c), pk is a

Query: SELECT * FROM T WHERE a = '...';

If a is a specific value, we can know which bucket is needed. We don't need to read other buckets."	FLINK	Closed	3	4	6732	pull-request-available
13271643	Add option to close shuffle when dynamic partition inserting	"When partition values are rare or have skew, if we shuffle by dynamic partitions, will break the performance.

We can have an option to close shuffle in such cases:

‘connector.sink.shuffle-by-partition.enable’ = ..."	FLINK	Closed	3	7	6732	pull-request-available
13450565	Create flink-table-store-connector-base to shade all flink dependencies	For Hive and other readers, they currently need to shade a bunch of dependencies, which is not very friendly, we can have a common module, and connector depends on this one module.	FLINK	Closed	3	7	6732	pull-request-available
13336016	Refactor table streaming file sink	"Refactor:
 * Extract some classes
 * Make writer more generic
 * Provides StreamingSink util class"	FLINK	Closed	3	7	6732	pull-request-available
13424285	Delete file is not correct in MergeTreeWriter	"The deletion in MergeTreeWriter.updateCompactResult dose not consider upgrade case, the upgrade file is required by previous snapshot and following snapshot, we should ensure:
1. This file is not the output of upgraded.
2. This file is not the input of upgraded.

Otherwise, the file will be deleted incorrectly."	FLINK	Closed	3	1	6732	pull-request-available
13242105	Introduce JDBCLookupFunction	"It is very common to store dimension information in JDBC databases. It is very useful to support JDBCLookupFunction, user can use this to lookup dimension fields in his streaming job.

1.Introduce JDBCLookupFunction

2.Introduce JDBCLookupTableSource

User can use JDBCLookupFunction to table function, or user can use JDBCLookupTableSource to blink planner temporal table join."	FLINK	Closed	3	2	6732	pull-request-available
13459243	Rename Schema to TableSchema	There are some systems that use schema as a concept of database, so the Schema class will be very confuse in this case, it is better to rename it as TableSchema.	FLINK	Closed	3	4	6732	pull-request-available
13225679	Introduce over window operators to blink batch	"Introduce NonBufferOverWindowOperator: Some over windows do not need to buffer data, such as rank, rows between unbounded preceding and 0, etc. We introduce NonBufferOverWindowOperator to reduce the overhead of data copy in buffer.

Introduce BufferDataOverWindowOperator and OverWindowFrame: 1. Minimize duplicate computation in various OverWindowFrame implementations. 2. An OverWindowOperator can compute multiple window frames."	FLINK	Closed	3	2	6732	pull-request-available
13434083	Add FileStore Continuous Reading ITCase	"* Add FileStore Continuous Reading ITCases
 * LOG_SYSTEM default is null, when null, use FileStore Continuous Reading
 * FileStore Continuous Reading supports latest scan"	FLINK	Closed	3	7	6732	pull-request-available
13446520	Change 'path' to 'root-path' in table store	"path is easy to bother the user, he/she will misinterpret it as path of table.
Let's change it to root-path, and it will be clearer."	FLINK	Closed	3	4	6732	pull-request-available
13485655	Let write buffer spillable	Column format and remote DFS may greatly affect the performance of compaction. We can change the writeBuffer to spillable to improve the performance.	FLINK	Closed	3	4	6732	pull-request-available
13473453	Modify changelog-file to changelog-producer	The changelog-file is limited, we can have more changelog producer types.	FLINK	Closed	1	4	6732	pull-request-available
13273797	Move DDL to first tab in table connector page	Since we have a good support for DDL in tableEnv.sqlUpdate and SQL-CLI, I think it is time to highlight DDL in the document.	FLINK	Resolved	3	7	6732	pull-request-available
13226620	Support e2e SortAggregate and HashAggregate operator run in batch mode	"1.Finish BatchExecSortAggregate

2.Finish BatchExecHashAggregate

3.Add AggITCase"	FLINK	Closed	3	2	6732	pull-request-available
13293693	HiveModuleTest failed to compile on release-1.10	"The cron task of release-1.10 failed to compile with the following exception:
{code}
23:36:45.190 [ERROR] /home/travis/build/apache/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java:[158,45] constructor HiveModule in class org.apache.flink.table.module.hive.HiveModule cannot be applied to given types;
 required: java.lang.String
 found: no arguments
 reason: actual and formal argument lists differ in length
{code}

instance: [https://api.travis-ci.org/v3/job/666450476/log.txt]"	FLINK	Resolved	3	1	6732	pull-request-available
13435037	Finish projection push down for table store	"Now, there are two TODO in table store:
 * support projection push down for log store
 * support projection push down for non-primary-key"	FLINK	Closed	3	7	6732	pull-request-available
13241007	Support intersect all and minus all to blink planner	"Now, we just support intersect and minus, See ReplaceIntersectWithSemiJoinRule and ReplaceMinusWithAntiJoinRule, replace intersect with null aware semi-join and distinct aggregate.

We need support intersect all and minus all too.

Presto and Spark already support them:

[https://github.com/prestodb/presto/issues/4918]

https://issues.apache.org/jira/browse/SPARK-21274

I think them have a good rewrite design and we can follow them:

1.For intersect all

Input Query
{code:java}
SELECT c1 FROM ut1 INTERSECT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
  SELECT c1
    FROM (
         SELECT replicate_row(min_count, c1)
         FROM (
              SELECT c1,
                     IF (vcol1_cnt > vcol2_cnt, vcol2_cnt, vcol1_cnt) AS min_count
              FROM (
                   SELECT   c1, count(vcol1) as vcol1_cnt, count(vcol2) as vcol2_cnt
                   FROM (
                        SELECT c1, true as vcol1, null as vcol2 FROM ut1
                        UNION ALL
                        SELECT c1, null as vcol1, true as vcol2 FROM ut2
                        ) AS union_all
                   GROUP BY c1
                   HAVING vcol1_cnt >= 1 AND vcol2_cnt >= 1
                  )
              )
          )
{code}
2.For minus all:

Input Query
{code:java}
SELECT c1 FROM ut1 EXCEPT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
 SELECT c1
    FROM (
     SELECT replicate_rows(sum_val, c1)
       FROM (
         SELECT c1, sum_val
           FROM (
             SELECT c1, sum(vcol) AS sum_val
               FROM (
                 SELECT 1L as vcol, c1 FROM ut1
                 UNION ALL
                 SELECT -1L as vcol, c1 FROM ut2
              ) AS union_all
            GROUP BY union_all.c1
          )
        WHERE sum_val > 0
       )
   )
{code}"	FLINK	Closed	3	2	6732	pull-request-available
13468477	Add extraFiles to DataFileMeta	"See FLINK-28244
 {code:java}
class DataFileMeta {
    String fileName;
    .....
    // store the name of extra files, extra files including changelog_file, primary_key_index_file, secondary_index_file, and etc...
    List<String> extraFiles;
}
{code}
Extra files can help us for many uses, including index files, changelog files and etc..."	FLINK	Closed	3	2	6732	pull-request-available
13482494	Commit delete file failure due to Checkpoint aborted	"After checkpoint abort, the files in cp5 may fall into cp6, because the compaction commit is deleted first and then added, which may lead to:
-Delete a file
-Add the same file again

This causes the deleted file not to be found.

We need to properly process the merge of the compression files."	FLINK	Closed	1	1	6732	pull-request-available
13440824	Optimize async compaction in MergeTreeWriter	"Currently Full Compaction may cause the writer to be blocked, which has an impact on LogStore latency.

We need to decouple compact and write, compact completely asynchronous.
But too many files will lead to unstable reads, when too many files, Compaction processing speed can not keep up with Writer, need to back press Writer.

Stop parameter: num-sorted-run.stop-trigger, default 10"	FLINK	Closed	3	4	6732	pull-request-available
13274682	Fix hive dialect limitation to overwrite and partition syntax	"As [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Overwrite-and-partition-inserting-support-in-1-10-td35829.html#a35885] discussed.

We should:
 * Remove hive dialect limitation for supported ""INSERT OVERWRITE"" and ""INSERT ... PARTITION(...)"".
 * Limit ""CREATE TABLE ... PARTITIONED BY"" to hive dialect."	FLINK	Resolved	2	1	6732	pull-request-available
13481206	Page not enough Exception in SortBufferMemTable	"When there are many partitions, the partition writer will seize memory and may have the following exceptions:
 !screenshot-1.png! 

We need to make sure that the writer has enough memory before it can start.

Actually, there is enough memory, because it can preempt from other writers. The problem is in OwnerMemoryPool.freePages, it should contain preemptable memory."	FLINK	Closed	3	1	6732	pull-request-available
13225216	Introduce HashJoinOperator and LongHashJoinGenerator to blink	"Introduce HashJoinOperator to support all join type.

Introduce LongHashJoinGenerator to do some performance optimization when key is long using LongHybridHashTable.

HashJoinOperator and LongHashJoinOperator can be used in both shuffle hash join and broadcast hash join."	FLINK	Closed	3	2	6732	pull-request-available
13292452	BytesColumnVector should init buffer in Hive 3.x	The failed test is {{TableEnvHiveConnectorTest#testDifferentFormats}} when hive 3.x.	FLINK	Resolved	3	1	6732	pull-request-available
13295928	Integrate parquet to file system connector	Implement ParquetFileSystemFormatFactory	FLINK	Resolved	3	7	6732	pull-request-available
13526470	Improve the read performance for files table	At present, the reading performance of the Files table is very poor. Even every data read will read the schema file. We can optimize the reading performance.	FLINK	Closed	3	4	6732	pull-request-available
13244318	Introduce type inference for hive functions in blink	See some conversation in [https://github.com/apache/flink/pull/8920]	FLINK	Closed	3	4	6732	pull-request-available
13445775	Add bin-pack strategy to split the whole bucket data files into several small splits	"We don't have to assign each task with a whole bucket data files. Instead, we can use some algorithm ( such as bin-packing) to split the whole bucket data files into multiple fragments to improve the job parallelism.

For merge tree table:
Suppose now there are files: [1, 2] [3, 4] [5, 180] [5, 190] [200, 600] [210, 700]
Files without intersection are not related, we do not need to put all files into one split, we can slice into multiple splits, multiple parallelism execution is faster. Nor can we slice too fine, we should make each split as large as possible with 128 MB, so use BinPack to slice, the final result will be:
 * split1: [1, 2] [3, 4]
 * split2: [5, 180] [5, 190]
 * split3: [200, 600] [210, 700]"	FLINK	Closed	3	7	6732	pull-request-available
13328749	Introduce createBucketWriter to BucketsBuilder	"{code:java}
@Internal
public abstract static class BucketsBuilder {
   ...
   
   @Internal
   public abstract BucketWriter<IN, BucketID> createBucketWriter() throws IOException;
}
{code}
FLINK-19345 depends on {{BucketWriter}} to write compacted file."	FLINK	Closed	3	7	6732	pull-request-available
13410771	CsvFilesystemStreamSinkITCase.testPart times out on AZP	"The test {{CsvFilesystemStreamSinkITCase.testPart}} times out on AZP.

{code}
2021-11-08T16:36:28.6542078Z Nov 08 16:36:28 org.junit.runners.model.TestTimedOutException: test timed out after 20 seconds
2021-11-08T16:36:28.6561998Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.putFields(ObjectOutputStream.java:463)
2021-11-08T16:36:28.6581789Z Nov 08 16:36:28 	at java.util.Locale.writeObject(Locale.java:2156)
2021-11-08T16:36:28.6601916Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6621871Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6632222Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6633082Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6633845Z Nov 08 16:36:28 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)
2021-11-08T16:36:28.6634442Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
2021-11-08T16:36:28.6634968Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6637691Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6640766Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6641958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6642763Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6643563Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6644365Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6645138Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6647747Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6648657Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6649439Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6650189Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6650958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6651975Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6652632Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6653314Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6664918Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6665679Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6666409Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6667211Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6667907Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6668585Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6669301Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6669991Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6670706Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6671353Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6672227Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6672878Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6673381Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6673864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6674366Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6674864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6675348Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6675851Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6676340Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6676827Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6677321Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6677797Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6678290Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6678781Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6679262Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6679764Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6680236Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
2021-11-08T16:36:28.6680733Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:632)
2021-11-08T16:36:28.6681669Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:548)
2021-11-08T16:36:28.6682620Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamConfig.setStreamOperatorFactory(StreamConfig.java:308)
2021-11-08T16:36:28.6683633Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setVertexConfig(StreamingJobGraphGenerator.java:713)
2021-11-08T16:36:28.6684729Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:461)
2021-11-08T16:36:28.6685788Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6686964Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6688033Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setChaining(StreamingJobGraphGenerator.java:377)
2021-11-08T16:36:28.6689024Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:178)
2021-11-08T16:36:28.6689973Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:116)
2021-11-08T16:36:28.6690944Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:960)
2021-11-08T16:36:28.6692027Z Nov 08 16:36:28 	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50)
2021-11-08T16:36:28.6692998Z Nov 08 16:36:28 	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39)
2021-11-08T16:36:28.6694130Z Nov 08 16:36:28 	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56)
2021-11-08T16:36:28.6695274Z Nov 08 16:36:28 	at org.apache.flink.test.util.MiniClusterPipelineExecutorServiceLoader$MiniClusterExecutor.execute(MiniClusterPipelineExecutorServiceLoader.java:137)
2021-11-08T16:36:28.6696466Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2095)
2021-11-08T16:36:28.6697500Z Nov 08 16:36:28 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
2021-11-08T16:36:28.6698470Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:772)
2021-11-08T16:36:28.6699494Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:753)
2021-11-08T16:36:28.6700427Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:574)
2021-11-08T16:36:28.6701363Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:556)
2021-11-08T16:36:28.6702418Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:118)
2021-11-08T16:36:28.6703498Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPart(FsStreamingSinkITCaseBase.scala:84)
2021-11-08T16:36:28.6704348Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6705109Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6705993Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6706775Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6707560Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-08T16:36:28.6708439Z Nov 08 16:36:28 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-08T16:36:28.6709327Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-08T16:36:28.6710196Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-08T16:36:28.6711113Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-11-08T16:36:28.6712067Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-11-08T16:36:28.6712971Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2021-11-08T16:36:28.6714031Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2021-11-08T16:36:28.6714883Z Nov 08 16:36:28 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-11-08T16:36:28.6715538Z Nov 08 16:36:28 	at java.lang.Thread.run(Thread.java:748)
2021-11-08T16:36:28.6715983Z Nov 08 16:36:28 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26165&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13306"	FLINK	Closed	2	1	6732	pull-request-available, test-stability
13259419	Don't use ContinuousFileReaderOperator to support multiple paths	"Now, blink planner use ContinuousFileReaderOperator to support InputFormat, but ContinuousFileReaderOperator not support multiple paths.

If read partitioned source, after partition pruning, we need let InputFormat to read multiple partitions which are multiple paths.

We can use InputFormatSourceFunction directly to support InputFormat."	FLINK	Resolved	3	7	6732	pull-request-available
13423043	Introduce Table Store Flink Sink	Introduce Table Store Sink with SinkWriter and GlobalCommitter.	FLINK	Closed	3	7	6732	pull-request-available
13288616	 connector on hive 2.0.1 don't  support type conversion from STRING to VARCHAR	" it threw  exception  when we query hive 2.0.1 by flink 1.10.0

 Exception stack：

org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=50, backoffTimeMS=10000)
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180)
 at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:484)
 at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
 at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
 at akka.actor.ActorCell.invoke(ActorCell.scala:561)
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
 at akka.dispatch.Mailbox.run(Mailbox.scala:225)
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:76)
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:123)
 at org.apache.flink.orc.OrcSplitReader.<init>(OrcSplitReader.java:73)
 at org.apache.flink.orc.OrcColumnarRowSplitReader.<init>(OrcColumnarRowSplitReader.java:55)
 at org.apache.flink.orc.OrcSplitReaderUtil.genPartColumnarRowReader(OrcSplitReaderUtil.java:96)
 at org.apache.flink.connectors.hive.read.HiveVectorizedOrcSplitReader.<init>(HiveVectorizedOrcSplitReader.java:65)
 at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:117)
 at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:56)
 at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:85)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)
Caused by: java.lang.reflect.InvocationTargetException
 at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.commons.lang3.reflect.MethodUtils.invokeExactMethod(MethodUtils.java:204)
 at org.apache.commons.lang3.reflect.MethodUtils.invokeExactMethod(MethodUtils.java:165)
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:74)
 ... 11 more
Caused by: java.io.IOException: ORC does not support type conversion from STRING to VARCHAR
 at org.apache.hadoop.hive.ql.io.orc.SchemaEvolution.validateAndCreate(SchemaEvolution.java:96)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:255)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:79)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$Builder.build(RecordReaderImpl.java:236)
 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:680)
 ... 17 more"	FLINK	Resolved	3	1	6732	pull-request-available
13492155	Introduce options metadata table	"SELECT * FROM T$options;
KEY | VALUE
...     | ..."	FLINK	Closed	3	7	6732	pull-request-available
13486065	Append-only with eventual log.consistency can not work	"{code:java}
                        ""CREATE TABLE T (i INT, j INT) WITH (""
                                + ""'log.system'='kafka', ""
                                + ""'write-mode'='append-only', ""
                                + ""'log.consistency'='eventual', ""
                                + ""'kafka.bootstrap.servers'='%s', ""
                                + ""'kafka.topic'='T')"",
{code}

Above DDL table should work.
"	FLINK	Closed	3	1	6732	pull-request-available
13342092	File Source lost data when reading from directories created by FileSystemTableSink with JSON format	"When testing the compaction functionality of the FileSystemTableSink, I found that when using json format, the produced directories could not be read correctly by the file source, namely only a part of records are read.


By checking the produced directories, the number of the records in it is the same as expected, thus it seems to be the issue of the source side.

 

The issue only exists for JSON format.

The data is produced by [FileCompactionTest|https://github.com/gaoyunhaii/flink1.12test/blob/main/src/main/java/FileCompactionTest.java] and read by  [FileCompactionCheckTest|https://github.com/gaoyunhaii/flink1.12test/blob/main/src/main/java/FileCompactionCheckTest.java] . An example directories tar file of 8000 records are also attached.

 "	FLINK	Closed	1	1	6732	pull-request-available
13310959	flink-orc and flink-parquet have invalid NOTICE file	"flink-orc provides a {{-jar-with-dependencies.jar}} variant which ships binaries.
However, these binaries are not documented in {{META-INF/NOTICE}}.
There are two similar files in that directory (NOTICE from force-shading and NOTICE.txt from Commons Lang). 

There is a NOTICE file that looks valid, but it is in {{META-INF/services}}.


I assume this has been introduced in FLINK-17460."	FLINK	Closed	1	1	6732	pull-request-available
13336670	Introduce File streaming compaction operators	Introduce CompactCoordinator and CompactOperator.	FLINK	Closed	3	7	6732	pull-request-available
13253199	Hive functions can not work in blink planner stream mode	"In flink, specifying the StreamTableEnvironment through the EnvironmentSetting using the blink planner, when using the UDAF in hive in the table API, the error is reported.

The hive function should been make by correct constants and argTypes. Otherwise it will throw an exception. (See HiveAggSqlFunction)
In this isTableAggregate, it just want to check the aggregate function class type, so the better way is get the function instead of make a function.


{code:java}
Caused by: java.lang.NullPointerException
	at java.util.Arrays.stream(Arrays.java:5004)
	at java.util.stream.Stream.of(Stream.java:1000)
	at org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType(TypeConversions.java:67)
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeSetArgs(HiveFunctionUtils.java:59)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:68)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:47)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.isTableAggregate(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.RelExplainUtil$.streamGroupAggregationToString(RelExplainUtil.scala:346)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.explainTerms(StreamExecGroupAggregate.scala:109)
	at org.apache.calcite.rel.AbstractRelNode.explain(AbstractRelNode.java:307)
	at org.apache.calcite.rel.AbstractRelNode.computeDigest(AbstractRelNode.java:388)
	at org.apache.calcite.rel.AbstractRelNode.recomputeDigest(AbstractRelNode.java:351)
	at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:345)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1668)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:846)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:868)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1939)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:129)
	... 60 more
{code}"	FLINK	Resolved	3	1	6732	pull-request-available
13365473	Refactor SlicingWindowAggOperatorBuilder to accept serializer instead of LogicalType	Now SlicingWindowAggOperatorBuilder accept LogicalTypes, it is better to avoid LogicalTypes in runtime operators and functions.	FLINK	Closed	3	4	6732	pull-request-available
13422141	Introduce FileFormat for table-store	Introduce file format class which creates reader and writer factories for specific file format.	FLINK	Closed	3	7	6732	pull-request-available
13326581	StreamingFileWriter should register Listener before the initialization of buckets	"In [http://apache-flink.147419.n8.nabble.com/StreamingFileSink-hive-metadata-td6898.html]

The feedback of User indicates that some partitions have not been committed since the job failed.

This maybe due to FLINK-18110, in FLINK-18110, it has fixed Buckets, but forgot fixing {{StreamingFileWriter}} , it should register Listener before the initialization of buckets, otherwise, will loose listening too."	FLINK	Closed	1	1	6732	pull-request-available
13526455	Improve StaticFileStoreSplitEnumerator to assign batch splits	"The following batch assignment operation is for two things:
1. It can be evenly distributed during batch reading to avoid scheduling problems (for example, the current resource can only schedule part of the tasks) that cause some tasks to fail to read data.
2. Read with limit, if split is assigned one by one, it may cause the task to repeatedly create SplitFetchers. After the task is created, it is found that it is idle and then closed. Then, new split coming, it will create SplitFetcher and repeatedly read the data of the limit number (the limit status is in the SplitFetcher).

"	FLINK	Closed	3	4	6732	pull-request-available
13229516	avoid to call retractExpressions method of max/min function in retractable aggregate code-gen	after {{FlinkRelMdModifiedMonotonicity}} introduced, a max/min function whose result value is modified  increasing/decreasing could ignore retraction message. We could choose regular max/min function instead of retract max/min function in code-gen. Currently, this requires the regular max/min function must implements {{retractExpressions}} method and do not throw any exceptions. A better approach is the retractable aggregate operator does not call {{retractExpressions}} method for max/min function.	FLINK	Closed	3	2	6732	auto-unassigned, pull-request-available
13471191	Improve Spark dependencies and document	"- Include Hive metastore dependency in Spark bundled jar.
- Improve document of Spark."	FLINK	Closed	3	4	6732	pull-request-available
13217071	Introduce an abstract set of data formats	"Blink uses an abstract set of data formats to make internal calculations use the binary format as much as possible. This minimizes the serialization overhead and java object overhead.

It includes:

BaseRow <=> Row

BaseMap <=> Java Map

BaseArray <=> Java array

BaseString  <=> Java String

Decimal <=> BigDecimal  //Scale of this object is specified by the user, not automatically determined(like BigDecimal).

int <=> Date //Flink used to use int in the calculation, but the remaining in Row is still Date, we will change it completely.

int <=> Time

long <=> Timestamp

byte[] <=> byte[]

BaseGeneric <=> T (GenericRelDataType, we don't know it, let user define serializer)

primitive type keep same, but use less boxed type."	FLINK	Closed	3	2	6732	pull-request-available
13335135	Introduce BulkFormatFactory to integrate new FileSource to table	"Introduce BulkFormatFactory: BulkFormat<T> createBulkFormat(context).

Filesystem connector use this factory to create BulkFormat, and use new FileSource to read files."	FLINK	Closed	3	7	6732	pull-request-available
13310875	Add filesystem documentation for Avro	We should add avro.codec documentation to avro page.	FLINK	Closed	2	7	6732	pull-request-available
13461398	Refactor TableStoreCatalog: Introduce a dedicated Catalog for table store	"We currently have developed a Flink's Catalog.
If we expose this Catalog directly to other connector developers, it is not good and there will be many unsupported interfaces and capabilities.

So here we create a tablestore dedicated catalog."	FLINK	Closed	3	4	6732	pull-request-available
13486040	Introduce Spark writer for table store	The main difficulty is that the Spark SourceV2 interface currently does not support custom distribution, and the Table Store must have consistent distribution.	FLINK	Closed	3	2	6732	pull-request-available
13468839	Planner free in flink-table-store-codegen	"We currently have the table-planner bundled into flink-table-store-codegen, which causes:

* bundle jar is too big, 20+MB
* Dependence on planner code will make it difficult to be compatible with multiple versions of Flink"	FLINK	Closed	3	7	6732	pull-request-available
13219188	Move table-planner-blink type to table-runtime-blink	We should put types in runtime because runtime code relies heavily on types.	FLINK	Closed	3	4	6732	pull-request-available
13286366	Use configuration from TableFactory in hive connector	"Now {{HiveOptions}} is used for {{GlobalConfiguration.loadConfiguration()}} .

It is not natural for table, we should use configuration from TableFactory to enable table config."	FLINK	Resolved	3	4	6732	pull-request-available
13220996	Introduce code generated typed sort to blink table	"Introduce SortCodeGenerator (CodeGen efficient computation and comparison of  NormalizedKey, idea based on FLINK-5734 ):

support sort by primitive type, string, decimal...

support sort by ArrayType

support sort by RowType(Nested Struct)

 

 "	FLINK	Closed	3	2	6732	pull-request-available
13435789	Hybrid reading wrong because of disordered commits	"In AbstractCommitterOperator.endInput:

Suppose the last checkpoint before endInput is 5. Flink Streaming Job calling order:
1. Accept elements from upstream prepareSnapshotPreBarrier(5)
2. this.snapshotState(5)
3. Accept elements from upstream endInput
4. this.endInput
5. this.notifyCheckpointComplete(5)
So we should submit all the data in the endInput in order to avoid disordered commits."	FLINK	Closed	3	7	6732	pull-request-available
13342290	Optimize the compaction section for the document of table filesystem sink.	Since if the files are big and the compaction use too much time, it will backpressure the job and also extend the time period of checkpoint, we might explicitly warns users about the possibility of this case to avoid perplexity.	FLINK	Closed	2	4	6732	pull-request-available
13315024	New table source factory omits unrecognized properties silently	"For the following DDL, we just omits the unrecognized property 'records-per-second'.
{code:sql}
CREATE TABLE MyDataGen (
  int_field int,
  double_field double,
  string_field varchar
) WITH (
  'connector' = 'datagen',
  'records-per-second' = '1'  -- should be rows-per-second
)
{code}
IMO, we should throw Exceptions to tell users that they used a wrong property. 
 CC [~jark] [~twalthr]"	FLINK	Closed	3	1	6732	pull-request-available
13505497	Expose BucketComputer from SupportsWrite	When other engines dock with Sink, they need to know the corresponding bucket rules before they can be correctly distributed to each bucket.	FLINK	Closed	3	4	6732	pull-request-available
13422478	Introduce Predicate to table store	"Flink Expression is not serializable. Although it has asSerializableString, the method is not implemented by all Expressions, and the deserialization requires many parameters.

So table store introduces Predicate to be a serializable class to do filter and partition push down."	FLINK	Closed	3	7	6732	pull-request-available
13415503	Introduce CatalogLock	"Currently, only HiveCatalog can provide this catalog lock.
{code:java}
/**
 * An interface that allows source and sink to use global lock to some transaction-related things.
 */
@Internal
public interface CatalogLock extends Closeable {
 
    /** Run with catalog lock. The caller should tell catalog the database and table name. */
    <T> T runWithLock(String database, String table, Callable<T> callable) throws Exception;
 
    /** Factory to create {@link CatalogLock}. */
    interface Factory extends Serializable {
        CatalogLock create();
    }
} {code}
And we need a interface to set lock to source&sink by catalog:
{code:java}
/**
 * Source and sink implement this interface if they require {@link CatalogLock}. This is marked as
 * internal. If we need lock to be more general, we can put lock factory into {@link
 * DynamicTableFactory.Context}.
 */
@Internal
public interface RequireCatalogLock {
 
    void setLockFactory(CatalogLock.Factory lockFactory);
} {code}
{{}}"	FLINK	Closed	3	7	6732	pull-request-available
13431625	Introduce Join Accumulator for Wide table	"Consider a Join Accumulator, It will merge two records, completing the not-null fields.

It is very useful for wide tables, where two source tables join together to form a wide table."	FLINK	Closed	3	2	6732	pull-request-available
13370535	Partition insert with union all will fail	INSERT INTO partitioned_sink (e,a,g,f,c,d) SELECT e,a,456,123,c,d FROM MyTable GROUP BY a,b,c,d,e UNION ALL SELECT e,a,789,456,c,d FROM MyTable GROUP BY a,b,c,d,e	FLINK	Closed	3	1	6732	pull-request-available
13217243	Remove JobMaster#start(JobMasterId) and #suspend	Currently, the {{JobMaster}} contains a lot of mutable state which is necessary because it is used across different leadership sessions by the {{JobManagerRunner}}. For this purpose, we have the methods {{JobMaster#start(JobMasterId)}} and {{#suspend}}. The mutable state management makes things on the {{JobMaster}} side more complicated than they need to be. In order to improve the {{JobMaster's}} maintainability I suggest to remove this logic and instead terminate the {{JobMaster}} if the {{JobManagerRunner}} loses leadership. This entails that for every leadership we will create a new {{JobMaster}} instance.	FLINK	Closed	3	4	10066	pull-request-available
13179389	Add support for resuming from savepoints to StandaloneJobClusterEntrypoint	The {{StandaloneJobClusterEntrypoint}} should support to resume from a savepoint/checkpoint. I suggest to introduce an optional command line parameter for specifying the savepoint/checkpoint path.	FLINK	Closed	3	4	10066	pull-request-available
13348318	Correct error message when calling TaskExecutor.disconnectJobManagerConnection	When calling {{TaskExecutor.disconnectJobManagerConnection}} then we create an exception with a message stating that the leading JobManager lost the leadership. This is not always true and consequently I propose to update this error message.	FLINK	Closed	4	4	10066	pull-request-available
13191288	Remove legacy FlinkMiniCluster	{{FlinkMiniCluster}} is based on legacy cluster mode and should be no longer used.	FLINK	Resolved	3	7	10066	pull-request-available
13140718	Make shut down of ResourceManagerRunner non blocking	Make the shut down of the {{ResourceManagerRunner}} non blocking. This will allow to shut down the {{MiniCluster}} in a non blocking fashion.	FLINK	Closed	3	4	10066	flip-6
13116971	Let ClusterOverviewHandler directly extend from AbstractRestHandler	In order to get rid of the {{LegacyRestHandler}} we should add a proper implementation of {{ClusterOverviewHandler}} which extends from {{AbstractRestHandler}}.	FLINK	Closed	4	7	10066	flip-6
13121680	Remove work arounds in Flip6LocalStreamEnvironment	After adding FLINK-7956, it is no longer necessary that the {{Flip6LocalStreamEnvironment}} waits for the registration of TaskManagers before submitting a job. Moreover, it is also possible to use slot sharing when submitting jobs.	FLINK	Closed	3	4	10066	flip-6
12862445	Add abstract equals, hashCode and toString methods to TypeInformation	"Flink expects that implementations of {{TypeInformation}} have valid implementations of {{hashCode}} and {{equals}}. However, the API does not enforce to implement these methods. Hence, this is a common origin for bugs such as for example FLINK-2633.

This can be avoided by adding abstract {{hashCode}} and {{equals}} methods to TypeInformation. An abstract {{toString}} method could also be added.

This change will brake the API and require to fix a couple of broken {{TypeInformation}} implementations."	FLINK	Closed	3	4	10066	starter
13418849	FLIP-198: Working directory for Flink processes	"This issue implements [FLIP-198|https://cwiki.apache.org/confluence/x/ZZiqCw]. The idea is to introduce a working directory that can be used by Flink processes to store data that can be recovered if the process is restarted with access to the same working directory (e.g. via persistent volumes or restarting the process on the same node).

The working directory will be default created in the temporary directory of the executing node. Moreover, the path will include the resource id of the Flink process. This will ensure that by default (random resource id), a newly started Flink process will see an empty working directory.

This is an optional feature that might not be usable for all Flink deployments."	FLINK	Closed	3	2	10066	pull-request-available
13374537	UnalignedCheckpointITCase failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17052&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9442


{code:java}
Apr 22 14:28:21 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
Apr 22 14:28:21 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
Apr 22 14:28:21 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Apr 22 14:28:21 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Apr 22 14:28:21 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
Apr 22 14:28:21 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
Apr 22 14:28:21 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
Apr 22 14:28:21 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
Apr 22 14:28:21 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
Apr 22 14:28:21 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
Apr 22 14:28:21 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Apr 22 14:28:21 Caused by: org.apache.flink.util.FlinkException: An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: '[NoMoreSplitEvent]', targetTask: Source: source (1/1) - execution #5
Apr 22 14:28:21 	... 26 more
Apr 22 14:28:21 

{code}


As described in the comment https://issues.apache.org/jira/browse/FLINK-21996?focusedCommentId=17326449&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17326449 we might need to adjust the tests  to allow failover."	FLINK	Closed	4	1	10066	auto-deprioritized-major, pull-request-available, test-stability
13139093	Make ClusterEntrypoint shut down non-blocking	Make the {{ClusterEntrypoint}} shut down method non blocking. That way we don't have to use the common Fork-Join-Pool to shutDownAndTerminate the cluster entrypoint when the Dispatcher terminates.	FLINK	Closed	3	4	10066	flip-6
13213485	Make RetryingRegistration timeouts configurable	For more control and better testability I suggest to make the {{RetryingRegistration}} configurable.  Especially the hard coded timeouts make testing in some scenarios hard.	FLINK	Closed	4	4	10066	pull-request-available
13307724	Increase default value of IO pool executor to 4 * #cores	"Currently, the default value of {{ClusterOptions.CLUSTER_IO_EXECUTOR_POOL_SIZE}} is #cores. I propose to increase it to 4 * #cores to support a higher load of blocking IO operations. 

Moreover, I propose to use a cached thread pool instead of a fixed thread pool. That way, only those use cases which have high IO load will actually occupy the required resources to start more threads.

Last but not least, I propose to change the config option name from {{cluster.io-executor.pool-size}} to {{cluster.io-pool.size}} which is a bit shorter."	FLINK	Resolved	3	4	10066	pull-request-available
13367233	Test failures occur due to the test not waiting for the ExecutionGraph to be created in Adaptive Scheduler	"Various tests are failing due the test not waiting for the ExecutionGraph to be created:
* [JobMasterTest.testRestoringFromSavepoint|https://dev.azure.com/mapohl/flink/_build/results?buildId=356&view=logs&j=243b38e1-22e7-598a-c8ae-385dce2c28b5&t=fea482b6-4f61-51f4-2584-f73df532b395&l=8266]
* {{JobMasterTest.testRequestNextInputSplitWithGlobalFailover}}
* {{JobMasterQueryableStateTest.testRequestKvStateOfWrongJob}}
* {{JobMasterQueryableStateTest.testRequestKvStateWithIrrelevantRegistration}}
* {{JobMasterQueryableStateTest.testDuplicatedKvStateRegistrationsFailTask}}
* {{JobMasterQueryableStateTest.testRegisterKvState}}

We might have to double-check whether other tests are affected as well."	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13346884	JobMasterTest.testSlotRequestTimeoutWhenNoSlotOffering test failed	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11055&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6]
{code:java}
[ERROR] Failures: 
[ERROR]   JobMasterTest.testSlotRequestTimeoutWhenNoSlotOffering:972 
Expected: a collection with size <1>
     but: collection size was <0>
{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13173687	SlotPool#failAllocation is called outside of main thread	The {{JobMaster}} calls directly into the {{SlotPool#failAllocation}} in the method {{JobMaster#notifyAllocationFailure}}. This can the {{SlotPool}} to go into an inconsistent state.	FLINK	Closed	1	1	10066	pull-request-available
13260142	NPE with JobMaster.disconnectTaskManager	"There was some connection issue with zookeeper that caused the job to restart.  But shutdown failed with this fatal NPE, which seems to cause JVM to exit

{code}
2019-10-02 16:16:19,134 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Unable to read additional data from server sessionid 0x16d83374c4206f8, likely server has clo
sed socket, closing socket connection and attempting reconnect
2019-10-02 16:16:19,234 INFO  org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager  - State change: SUSPENDED
2019-10-02 16:16:19,235 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,235 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,235 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp://flink@100.122.177.82:42043/u
ser/dispatcher no longer participates in the leader election.
2019-10-02 16:16:19,237 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - http://100.122.177.82:8081 lost leadership
2019-10-02 16:16:19,237 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - ResourceManager akka.tcp://flink@100.122.177.82:42043/user/resourcemanager was revoked leadershi
p. Clearing fencing token.
2019-10-02 16:16:19,237 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Stopping ZooKeeperLeaderRetrievalService /leader/e4e68f2b3fc40c7008cca624b2a2bab0/job_
manager_lock.
2019-10-02 16:16:19,237 WARN  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - ZooKeeper connection SUSPENDING. Changes to the submitted job graphs are not monitored (tem
porarily).
2019-10-02 16:16:19,238 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunner           - JobManager for job ksrouter (e4e68f2b3fc40c7008cca624b2a2bab0) was revoked leadership at akka.tcp:
//flink@100.122.177.82:42043/user/jobmanager_0.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Stopping ZooKeeperLeaderRetrievalService /leader/resource_manager_lock.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender http://100.122.177.82:8081 no longer pa
rticipates in the leader election.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp://flink@100.122.177.82:42043/u
ser/jobmanager_0 no longer participates in the leader election.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job ksrouter (e4e68f2b3fc40c7008cca624b2a2bab0) switched from state RUNNING to SUSPENDED.
org.apache.flink.util.FlinkException: JobManager is no longer the leader.
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.revokeJobMasterLeadership(JobManagerRunner.java:391)
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.lambda$revokeLeadership$5(JobManagerRunner.java:377)
        at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:981)
        at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2124)
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.revokeLeadership(JobManagerRunner.java:374)
        at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService.notLeader(ZooKeeperLeaderElectionService.java:247)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$8.apply(LeaderLatch.java:640)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$8.apply(LeaderLatch.java:636)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93)
        at org.apache.flink.shaded.curator.org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:635)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.handleStateChange(LeaderLatch.java:623)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.access$000(LeaderLatch.java:64)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$1.stateChanged(LeaderLatch.java:82)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:259)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:255)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93)
        at org.apache.flink.shaded.curator.org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:253)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2019-10-02 16:16:19,240 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp://flink@100.122.177.82:42043/u
ser/resourcemanager no longer participates in the leader election.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Dispatcher akka.tcp://flink@100.122.177.82:42043/user/dispatcher was revoked leadership.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl  - Suspending the SlotManager.
2019-10-02 16:16:19,240 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Stopping all currently running jobs of dispatcher akka.tcp://flink@100.122.177.82:42043/user/dispa
tcher.
2019-10-02 16:16:19,258 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Stopping checkpoint coordinator for job e4e68f2b3fc40c7008cca624b2a2bab0.
2019-10-02 16:16:19,258 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore  - Suspending
2019-10-02 16:16:20,076 WARN  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration se
ction named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-6650646464026425406.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeepe
r server allows it.
2019-10-02 16:16:20,076 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 100.66.21.125/100.66.21.125:2181
2019-10-02 16:16:20,076 ERROR org.apache.flink.shaded.curator.org.apache.curator.ConnectionState  - Authentication failed
2019-10-02 16:16:20,077 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Socket connection established to 100.66.21.125/100.66.21.125:2181, initiating session
2019-10-02 16:16:20,080 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 100.66.21.125/100.66.21.125:2181, sessionid = 0x16d8
3374c4206f8, negotiated timeout = 40000
2019-10-02 16:16:20,080 INFO  org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager  - State change: RECONNECTED
2019-10-02 16:16:20,080 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - ZooKeeper connection RECONNECTED. Changes to the submitted job graphs are monitored again.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,322 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - ResourceManager akka.tcp://flink@100.122.177.82:42043/user/resourcemanager was granted leadershi
p with fencing token 94628b472f22083fb4e611d108304613
2019-10-02 16:16:20,322 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl  - Starting the SlotManager.
2019-10-02 16:16:20,326 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - http://100.122.177.82:8081 was granted leadership with leaderSessionID=69531adf-a5eb-46d2-99dc-b85
0f66e1af1
2019-10-02 16:16:20,393 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunner           - JobManagerRunner already shutdown.
2019-10-02 16:16:20,393 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Dispatcher akka.tcp://flink@100.122.177.82:42043/user/dispatcher was granted leadership with fenci
ng token abe4ffde-0a18-412b-bffa-28067ccccbeb
2019-10-02 16:16:20,393 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Recovering all persisted jobs.
2019-10-02 16:16:20,407 INFO  com.facebook.presto.s3fs.PrestoS3FileSystem                   - Opening path: s3://us-east-1.ksrouter.dev/recovery/4fa8-1569989613304/444/submittedJobGraphf4c0027
a08cf
2019-10-02 16:16:20,407 INFO  com.facebook.presto.s3fs.PrestoS3FileSystem                   - Seek with new stream for s3://us-east-1.ksrouter.dev/recovery/4fa8-1569989613304/444/submittedJobG
raphf4c0027a08cf to offset 0
2019-10-02 16:16:20,412 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - Registering TaskManager with ResourceID 015fd9b87fc4ceb7bb1c40318db0d854 (akka.tcp://flink@100.1
22.134.166:43709/user/taskmanager_0) at ResourceManager
2019-10-02 16:16:20,412 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceReconciler  - Task manager 100.122.134.166 re-registered.
2019-10-02 16:16:20,416 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter  - Shutting down.
2019-10-02 16:16:20,416 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job e4e68f2b3fc40c7008cca624b2a2bab0 has been suspended.
2019-10-02 16:16:20,417 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl      - Suspending SlotPool.
2019-10-02 16:16:20,417 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Close ResourceManager connection dda8a9f54ec0239b7e2aa53e0a9b6174: JobManager is no longer the lea
der..
2019-10-02 16:16:20,418 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Stopping the JobMaster for job ksrouter(e4e68f2b3fc40c7008cca624b2a2bab0).
2019-10-02 16:16:20,420 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Stopping ZooKeeperLeaderElectionService ZooKeeperLeaderElectionService{leaderPath='/leader/e4e68f2b3fc40c7008cca624b2a2bab0/job_manager_lock'}.
2019-10-02 16:16:20,431 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - Registering TaskManager with ResourceID 8d41d9e915b5fe378b3d7cd18042fcff (akka.tcp://flink@100.122.50.217:44327/user/taskmanager_0) at ResourceManager
2019-10-02 16:16:20,431 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceReconciler  - Task manager 100.122.50.217 re-registered.
2019-10-02 16:16:20,542 INFO  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - Recovered SubmittedJobGraph(e4e68f2b3fc40c7008cca624b2a2bab0).
2019-10-02 16:16:20,542 INFO  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - Recovered SubmittedJobGraph(e4e68f2b3fc40c7008cca624b2a2bab0).
2019-10-02 16:16:20,542 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Fatal error occurred in the cluster entrypoint.
org.apache.flink.runtime.dispatcher.DispatcherException: Failed to take leadership with session id abe4ffde-0a18-412b-bffa-28067ccccbeb.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$null$30(Dispatcher.java:915)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:88)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.dispatcher.DispatcherException: Termination of previous JobManager for job e4e68f2b3fc40c7008cca624b2a2bab0 failed. Cannot submit job under the same job id.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$waitForTerminatingJobManager$33(Dispatcher.java:949)
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
        at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:884)
        at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2196)
        at org.apache.flink.runtime.dispatcher.Dispatcher.waitForTerminatingJobManager(Dispatcher.java:946)
        at org.apache.flink.runtime.dispatcher.Dispatcher.tryAcceptLeadershipAndRunJobs(Dispatcher.java:933)
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$null$28(Dispatcher.java:892)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
        ... 23 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Could not properly shut down the JobManagerRunner
        at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
        at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
        at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:911)
        at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:899)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.lambda$closeAsync$0(JobManagerRunner.java:207)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.postStop(AkkaRpcActor.java:132)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.postStop(FencedAkkaRpcActor.java:40)
        at akka.actor.Actor$class.aroundPostStop(Actor.scala:536)
        at akka.actor.AbstractActor.aroundPostStop(AbstractActor.scala:225)
        at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
        at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
        at akka.actor.ActorCell.terminate(ActorCell.scala:429)
        at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:533)
        at akka.actor.ActorCell.systemInvoke(ActorCell.scala:549)
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:261)
        ... 6 more
Caused by: org.apache.flink.util.FlinkException: Could not properly shut down the JobManagerRunner
        ... 22 more
Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Failure while stopping RpcEndpoint jobmanager_0.
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:513)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:175)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        ... 6 more
Caused by: java.lang.NullPointerException
        at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:425)
        at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:343)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:509)
        ... 18 more
{code}"	FLINK	Resolved	2	1	10066	pull-request-available
13347588	Allow to register new slots listener at DeclarativeSlotPool	At the moment it is not possible to register a new slots listener at the {{DeclarativeSlotPool}} after its creation. This is problematic because the listener might not be known at the time of instantiating the {{DeclarativeSlotPool}}. I propose to add a {{registerNewSlotListener}} method to the {{DeclarativeSlotPool}}.	FLINK	Closed	3	4	10066	pull-request-available
13305165	"FileUploadHandlerTest.testUploadCleanupOnFailure fails with ""SocketTimeout timeout"""	"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1392&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d ( select 1st attempt)

{code}
2020-05-15T09:01:40.0547958Z [ERROR] testUploadCleanupOnFailure(org.apache.flink.runtime.rest.FileUploadHandlerTest)  Time elapsed: 10.415 s  <<< ERROR!
2020-05-15T09:01:40.0548716Z java.net.SocketTimeoutException: timeout
2020-05-15T09:01:40.0549048Z 	at okio.Okio$4.newTimeoutException(Okio.java:227)
2020-05-15T09:01:40.0549361Z 	at okio.AsyncTimeout.exit(AsyncTimeout.java:284)
2020-05-15T09:01:40.0549688Z 	at okio.AsyncTimeout$2.read(AsyncTimeout.java:240)
2020-05-15T09:01:40.0552454Z 	at okio.RealBufferedSource.indexOf(RealBufferedSource.java:344)
2020-05-15T09:01:40.0554987Z 	at okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:216)
2020-05-15T09:01:40.0555636Z 	at okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:210)
2020-05-15T09:01:40.0556307Z 	at okhttp3.internal.http1.Http1Codec.readResponseHeaders(Http1Codec.java:189)
2020-05-15T09:01:40.0556856Z 	at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.java:75)
2020-05-15T09:01:40.0557505Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0558021Z 	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:45)
2020-05-15T09:01:40.0558498Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0558932Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
2020-05-15T09:01:40.0559381Z 	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
2020-05-15T09:01:40.0559803Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0560262Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
2020-05-15T09:01:40.0561022Z 	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
2020-05-15T09:01:40.0561701Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0562439Z 	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)
2020-05-15T09:01:40.0563170Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
2020-05-15T09:01:40.0565934Z 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
2020-05-15T09:01:40.0566781Z 	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)
2020-05-15T09:01:40.0575046Z 	at okhttp3.RealCall.execute(RealCall.java:69)
2020-05-15T09:01:40.0575858Z 	at org.apache.flink.runtime.rest.FileUploadHandlerTest.testUploadCleanupOnFailure(FileUploadHandlerTest.java:250)
2020-05-15T09:01:40.0576567Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-15T09:01:40.0577242Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-15T09:01:40.0577979Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-15T09:01:40.0578594Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-15T09:01:40.0579234Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-15T09:01:40.0580279Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-15T09:01:40.0581129Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-15T09:01:40.0581862Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-15T09:01:40.0582538Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-15T09:01:40.0583174Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-15T09:01:40.0583934Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-15T09:01:40.0584501Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-15T09:01:40.0585282Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-15T09:01:40.0586005Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-15T09:01:40.0586666Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-15T09:01:40.0587257Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-15T09:01:40.0587879Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-15T09:01:40.0588486Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-15T09:01:40.0589101Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-15T09:01:40.0589724Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-15T09:01:40.0590407Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-15T09:01:40.0591102Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-15T09:01:40.0591621Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-15T09:01:40.0592245Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-15T09:01:40.0592978Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-15T09:01:40.0593784Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-15T09:01:40.0594692Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-15T09:01:40.0595445Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-15T09:01:40.0596220Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-15T09:01:40.0596947Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-15T09:01:40.0597646Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-15T09:01:40.0598221Z Caused by: java.net.SocketException: Socket closed
2020-05-15T09:01:40.0598774Z 	at java.net.SocketInputStream.read(SocketInputStream.java:204)
2020-05-15T09:01:40.0599375Z 	at java.net.SocketInputStream.read(SocketInputStream.java:141)
2020-05-15T09:01:40.0599864Z 	at okio.Okio$2.read(Okio.java:138)
2020-05-15T09:01:40.0600350Z 	at okio.AsyncTimeout$2.read(AsyncTimeout.java:236)
2020-05-15T09:01:40.0600804Z 	... 51 more
2020-05-15T09:01:40.0601098Z 
2020-05-15T09:01:40.4326691Z [INFO] Running org.apache.flink.runtime.metrics.groups.TaskIOMetricGroupTest
{code}"	FLINK	Resolved	2	1	10066	pull-request-available, test-stability
13139088	Make shut down of RestServerEndpoint non blocking	In order to better shut down different components, it would be helpful if the {{RestServerEndpoint}} would not block when being shut down.	FLINK	Closed	4	4	10066	flip-6
13148183	Jobs can be dropped in HA when job submission fails	Jobs can be dropped in HA mode if the job submission step fails. In such a case, we should fail fatally to let the {{Dispatcher}} restart and retry to recover all jobs.	FLINK	Closed	1	1	10066	flip-6
13283034	Kinesis consumer fails due to jackson-dataformat-cbor conflict in 1.10 RC1	There appears to be an issue with incompatible dependencies being shaded in the connector. This only happens when running against the actual Kinesis service (i.e. when CBOR isn't disabled).	FLINK	Resolved	1	1	10066	pull-request-available
13328924	BlobClientTest.testGetFailsDuringStreamingForJobPermanentBlob hangs	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6803&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708

{code}
2020-09-22T21:40:57.5304615Z ""main"" #1 prio=5 os_prio=0 cpu=18407.84ms elapsed=1969.42s tid=0x00007f0730015800 nid=0x79bd waiting for monitor entry  [0x00007f07389fb000]
2020-09-22T21:40:57.5305080Z    java.lang.Thread.State: BLOCKED (on object monitor)
2020-09-22T21:40:57.5305487Z 	at sun.security.ssl.SSLSocketImpl.duplexCloseOutput(java.base@11.0.7/SSLSocketImpl.java:541)
2020-09-22T21:40:57.5306159Z 	- waiting to lock <0x000000008661a560> (a sun.security.ssl.SSLSocketOutputRecord)
2020-09-22T21:40:57.5306545Z 	at sun.security.ssl.SSLSocketImpl.close(java.base@11.0.7/SSLSocketImpl.java:472)
2020-09-22T21:40:57.5307045Z 	at org.apache.flink.runtime.blob.BlobUtils.closeSilently(BlobUtils.java:367)
2020-09-22T21:40:57.5307605Z 	at org.apache.flink.runtime.blob.BlobServerConnection.close(BlobServerConnection.java:141)
2020-09-22T21:40:57.5308337Z 	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreaming(BlobClientTest.java:443)
2020-09-22T21:40:57.5308904Z 	at org.apache.flink.runtime.blob.BlobClientTest.testGetFailsDuringStreamingForJobPermanentBlob(BlobClientTest.java:408)
{code}"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13137661	Implement asynchronous rescaling REST handlers	In order to trigger the job rescaling we should offer REST handlers which expose the rescaling functionality. Since this is an asynchronous operation, the handlers should be implemented by extending {{AbstractAsynchronousOperationHandlers}}.	FLINK	Closed	3	2	10066	flip-6
13144648	Cancel slot requests for otherwisely fulfilled requests	If a slot request is fulfilled with a different allocation id, then we should cancel the other slot request at the {{ResourceManager}}. Otherwise we might have some stale slot requests which first need to time out before slots are available again.	FLINK	Closed	2	1	10066	flip-6
13368637	Remove unnecessary akka.transport.* configuration options	Flink still exposes config options for configuring Akka's [transport failure detector|https://doc.akka.io/docs/akka-enhancements/current/config-checker.html#transport-failure-detector]. This failure detector is only relevant when not using TCP. Hence, I suggest to remove these configuration options to avoid confusion.	FLINK	Closed	4	4	10066	pull-request-available
13418856	Make sure that FileSystemBlobStore flushes all writes to disk to persist stored blobs	In order to avoid that the {{FileSystemBlobStore}} loses blobs, we have to make sure that all written results are flushed from the OS page cache to disk (fsynced).	FLINK	Closed	3	7	10066	pull-request-available
13084596	Implement skeletal structure of dispatcher component	Implement the skeletal structure of the {{Dispatcher}} component. The initial functionality will support job submissions and listing of jobs.	FLINK	Closed	3	7	10066	flip-6
13277212	Drop vendor specific repositories from pom.xml	"Since Flink no longer bundles Hadoop dependencies we also don't need the vendor specific Hadoop repositories in Flink's {{pom.xml}}. Consequently, I suggest to remove them. 

This idea has been discussed on Flink's dev ML: https://lists.apache.org/thread.html/be402a11bc986219eabd9dd8af507f36f49784d5400d0873e9ec0c2e%40%3Cdev.flink.apache.org%3E."	FLINK	Closed	4	4	10066	pull-request-available
13103591	Revisit default non-leader id for FencedRpcEndpoints	Currently, when a {{FencedRpcEndpoint}} loses leadership, we set its leader id to a random value. This can be problematic, even though it's unlikely, because we might set it to a value which is used somewhere else (e.g. the currently valid leader id). I think it would  be better to simply set the leader id to {{null}} in order to properly encode that the {{FencedRpcEndpoint}} is no longer a leader.	FLINK	Closed	4	1	10066	flip-6
13133041	Integrate queryable state with Flip-6	In order to make Flip-6 support queryable state, we have to start and register the {{KvStateServer}} and {{KvStateProxyClient}} in the {{TaskExecutor}}.	FLINK	Closed	3	4	10066	flip-6
13377258	JobMaster cannot be restarted	Since we are reusing the {{DefaultLeaderRetrievalService}} for finding the resource manager in the {{JobMaster}} and since the leader retrieval service cannot be reused, it is not possible to restart the {{JobMaster}}. This causes Flink to fail in case that the {{JobMaster}} regains the leadership in Flink versions <= 1.12. The problem no longer occurs for newer versions because of FLINK-11719.	FLINK	Closed	3	1	10066	pull-request-available
13424956	ExecutionVertex.getLatestPriorAllocation fails if there is an unsuccessful restart attempt	The {{ExecutionVertex.getLatestPriorAllocation}} does not return the latest prior allocation if there was an unsuccessful restart attempt in between. The problem is that we only look at the last {{Execution}}. Due to this, {{ExecutionVertex.getLatestPriorAllocation}} sometimes returns {{null}} even though there is a prior {{AllocationID}}.	FLINK	Closed	3	7	10066	pull-request-available
13173739	Remove superfluous lock from SlotSharingManager	The {{SlotSharingManager}} uses a lock to synchronize access to some of its fields. Since the component is designed to be used only in a single threaded context, the lock are superfluous and confusing. Therefore, I propose to remove it.	FLINK	Closed	3	4	10066	pull-request-available
13118575	Fulfil slot requests with unused offered slots	The {{SlotPool}} adds unused offered slots to the list of available slots without checking whether another pending slot request could be fulfilled with this slot. This should be changed.	FLINK	Closed	3	4	10066	flip-6
13108554	Move CurrentJobsOverviewHandler to jobs/overview	The {{CurrentJobsOverviewHandler}} is currently registered under {{/joboverview}}. I think it would be more idiomatic to register it under {{/jobs/overview}}.	FLINK	Closed	4	7	10066	flip-6
13186823	Remove legacy mode testing profiles from Travis config	Remove the legacy mode testing profiles from Travis config.	FLINK	Closed	3	7	10066	pull-request-available
13110667	Register TaskManagerMetricGroup under ResourceID instead of InstanceID	"Currently, the {{TaskManager}} registers the {{TaskManagerMetricGroup}} under its {{InstanceID}} and thereby binding its metrics effectively to the lifetime of its registration with the {{JobManager}}. This has also implications how the REST handler retrieve the TaskManager metrics, namely by its {{InstanceID}}.

I would actually propose to register the {{TaskManagerMetricGroup}} under the {{TaskManager}}/{{TaskExecutor}} {{ResourceID}} which is valid over the whole lifetime of the {{TaskManager}}/{{TaskExecutor}}. That way we would also be able to query metrics independent of the connection status to the {{JobManager}}."	FLINK	Closed	4	4	10066	flip-6
13197056	Configurable MetricQueryService interval	"The {{MetricQueryService}} is used for transmitting metrics from TaskManagers to the JobManager, in order to expose them via REST and by extension the WebUI.

By default the JM will poll metrics at most every 10 seconds. This has an adverse effect on the duration of our end-to-end tests, which for example query metrics via the REST API to determine whether the cluster has started. If during the first poll no TM is available it will take another 10 second for updated information to be available.

By making this interval configurable we could this reduce the test duration. Additionally this could serve as a switch to disable the {{MetricQueryService}}."	FLINK	Resolved	3	2	10066	pull-request-available
13103544	Port TaskManagersHandler to new REST endpoint	Port existing {{TaskManagersHandler}} to the new REST endpoint	FLINK	Resolved	3	7	10066	flip-6
13344583	Null result values are being swallowed by RPC system	"If an RPC method returns a {{null}} value, then it seems that the request future won't get completed as reported in FLINK-17921.

We should either not allow to return {{null}} values as responses or make sure that a {{null}} value is properly transmitted to the caller."	FLINK	Closed	3	1	10066	pull-request-available
13356541	Remove SlotPool.suspend	Since the completion of FLINK-11719 we no longer need to suspend the {{SlotPool}}. Hence, I suggest to remove this method.	FLINK	Closed	4	1	10066	pull-request-available
13079687	Instable StatefulJobSavepointMigrationITCase.testRestoreSavepoint	"The test case {{StatefulJobSavepointMigrationITCase.testRestoreSavepoint}} seems to be instable on Travis [1].

[1] https://s3.amazonaws.com/archive.travis-ci.org/jobs/242351588/log.txt"	FLINK	Closed	2	1	10066	test-stability
13065147	StreamingOperatorsITCase.testAsyncWaitOperator spuriously failing on Travis	"The {{StreamingOperatorsITCase.testAsyncWaitOperator}} fails sometimes on Travis.

[1] https://s3.amazonaws.com/archive.travis-ci.org/jobs/223541348/log.txt"	FLINK	Closed	3	1	10066	test-stability
13184607	Simplify ZooKeeperSubmittedJobGraphStore constructor	The {{ZooKeeperSubmittedJobGraphStore}} does a lot of initialization logic in its constructor. I propose to move this out of the constructor in order to simplify it.	FLINK	Resolved	4	4	10066	pull-request-available
13133332	Use single BlobCacheService per TaskExecutor	"Currently, the {{TaskExecutor}} creates a new {{BlobCacheService}} for each new {{JobManagerConnection}}. This is wasteful and, moreover, gives only access to the {{BlobService}} after a connection to a {{JobMaster}} has been established. Due to this, it is not possible to upload the {{TaskExecutor}} logs before a {{TaskExecutor}} is used by a {{JobMaster}}.

 

Since the {{BlobServer}} address is something cluster specific and not {{JobMaster}} specific, I propose to propagate this information when the {{TaskExecutor}} registers at the {{ResourceManager}}. Moreover, I propose to make the {{BlobCacheService}} reusable in case of a {{BlobServer}} address change (e.g. failover) by allowing to change the {{BlobServer}} address."	FLINK	Closed	3	4	10066	flip-6
13200570	Define flink-sql-client uber-jar dependencies via artifactSet	"The module {{flink-sql-client}} defines the content of its uber jar via filtering files from the set of all dependencies. I think this is not ideal because it misses for example the {{NOTICE}} files from down stream dependencies. 

A solution could be to define an {{<artifactSet><includes><include></include></includes></artifactSet>}} and exclude files via the filter."	FLINK	Closed	3	4	10066	pull-request-available
13097293	Set JobMaster leader session id in main thread	Currently, the {{JobMaster}} leader id is set via an {{AtomicReferenceUpdater}}. In order to make it work with the new {{FencedRpcEndpoint}} it should be set in the main thread, because the former only allows to modify the leader session id from the main thread.	FLINK	Closed	4	7	10066	flip-6
13370079	Remove console logging for Kafka connector for AZP runs	For the Kafka connector we do log to the console. These logging statements clutter the AZP output considerably. I propose to remove this logic. Moreover, we still have some DEBUG logging for FLINK-16383 which has been fixed.	FLINK	Closed	4	4	10066	pull-request-available
13198336	Submitting a jobs without enough slots times out due to a unspecified timeout	"When submitting a job without enough slots being available the job will stay in a SCHEDULED/CREATED state. After some time (a few minutes) the job execution will fail with the following timeout exception:
{code}
2018-11-14 13:38:26,614 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPool          - Pending slot request [SlotRequestId{d9c0c94b6b81eae406f3d6cb6150fee4}] timed out.
2018-11-14 13:38:26,615 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN DataSource (at getDefaultTextLineDataSet(WordCountData.java:70) (org.apache.flink.api.java.io.CollectionInputFormat)) -> FlatMap (FlatMap at main(WordCount.java:76)) -> Combine (SUM(1), at main(WordCount.java:79) (1/$java.util.concurrent.TimeoutException
        at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:795)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}

That the job submission may time out is not documented, neither is which timeout is responsible in the first place nor how/whether this can be disabled."	FLINK	Resolved	3	4	10066	pull-request-available
13103546	Port JobCancellationHandler to new REST endpoint	Port existing {{JobCancellationHandler}} to new REST endpoint	FLINK	Resolved	3	7	10066	flip-6
13137141	Enable job cancellation from the web UI	In order to enable the job cancellation from the web UI (including YARN) we have to register the {{JobTerminationHandler}} under {{/jobs/:jobId/yarn-cancel}} and {{/jobs/:jobid/yarn-stop}}.	FLINK	Closed	3	4	10066	flip-6
13198697	docker-entrypoint.sh logs credentails during startup	" {{docker-entrypoint.sh}} prints the full flink-configuration file including potentially sensitive configuration entries.

To be consistent we should hide these as in {{GlobalConfiguration}}."	FLINK	Resolved	2	4	10066	pull-request-available
13388414	AkkaRpcActorTest#testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20163&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6023

{code}
Jul 08 11:03:13 java.lang.AssertionError: 
Jul 08 11:03:13 
Jul 08 11:03:13 Expected: is <false>
Jul 08 11:03:13      but: was <true>
Jul 08 11:03:13 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:964)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:930)
Jul 08 11:03:13 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor(AkkaRpcActorTest.java:375)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 08 11:03:13 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 08 11:03:13 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 08 11:03:13 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 08 11:03:13 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 08 11:03:13 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 08 11:03:13 
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.107 s - in org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:13 [INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.823 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest
Jul 08 11:03:14 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [INFO] Results:
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [ERROR] Failures: 
Jul 08 11:03:14 [ERROR]   AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor:375 
Jul 08 11:03:14 Expected: is <false>
Jul 08 11:03:14      but: was <true>

{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13394535	"SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted failed with ""Stop with savepoint operation could not be completed"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21851&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4858

{code}
Aug 10 22:14:12 [ERROR] Tests run: 15, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 65.911 s <<< FAILURE! - in org.apache.flink.test.checkpointing.SavepointITCase
Aug 10 22:14:12 [ERROR] testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted  Time elapsed: 2.389 s  <<< FAILURE!
Aug 10 22:14:12 java.lang.AssertionError: 
Aug 10 22:14:12 
Aug 10 22:14:12 Expected: a string starting with ""org.apache.flink.util.FlinkException: Inconsistent execution state after stopping with savepoint. At least one execution is still in one of the following states: FAILED. A global fail-over is triggered to recover the job""
Aug 10 22:14:12      but: was ""org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.""
Aug 10 22:14:12 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Aug 10 22:14:12 	at org.junit.Assert.assertThat(Assert.java:964)
Aug 10 22:14:12 	at org.junit.Assert.assertThat(Assert.java:930)
Aug 10 22:14:12 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted(SavepointITCase.java:744)
Aug 10 22:14:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 10 22:14:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 10 22:14:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 10 22:14:12 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 10 22:14:12 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 10 22:14:12 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 10 22:14:12 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 10 22:14:12 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 10 22:14:12 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 10 22:14:12 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 10 22:14:12 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 10 22:14:12 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 10 22:14:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 10 22:14:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 10 22:14:12 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Aug 10 22:14:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Aug 10 22:14:12 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Aug 10 22:14:12 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 10 22:14:12 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Aug 10 22:14:12 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Aug 10 22:14:12 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Aug 10 22:14:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Aug 10 22:14:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13346237	"Local recovery and sticky scheduling end-to-end test timeout with ""IOException: Stream Closed"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10905&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610]

It tried to restart many times, and the final error was following:
{code:java}
2020-12-15T23:54:00.5067862Z Dec 15 23:53:42 2020-12-15 23:53:41,538 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.
2020-12-15T23:54:00.5068392Z Dec 15 23:53:42 java.io.IOException: Stream Closed
2020-12-15T23:54:00.5068767Z Dec 15 23:53:42 	at java.io.FileInputStream.readBytes(Native Method) ~[?:?]
2020-12-15T23:54:00.5069223Z Dec 15 23:53:42 	at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]
2020-12-15T23:54:00.5070150Z Dec 15 23:53:42 	at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5071217Z Dec 15 23:53:42 	at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5072295Z Dec 15 23:53:42 	at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5072967Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]
2020-12-15T23:54:00.5073483Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]
2020-12-15T23:54:00.5074535Z Dec 15 23:53:42 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5075847Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5077187Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5078495Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5079802Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5081013Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5082215Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5083500Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5084899Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5086342Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5087601Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5088924Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5090261Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5091459Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5092604Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5093748Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5094866Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5095912Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5096875Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5097814Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5098373Z Dec 15 23:53:42 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-12-15T23:54:00.5099549Z Dec 15 23:53:42 2020-12-15 23:53:41,557 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/4) from alternative (1/1), will retry while more alternatives are available.
2020-12-15T23:54:00.5100556Z Dec 15 23:53:42 org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
2020-12-15T23:54:00.5101480Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5102669Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:565) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5103763Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:94) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5104723Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5105700Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5106630Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5107587Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:316) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5108581Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:155) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5109505Z Dec 15 23:53:42 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:248) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5110456Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5111316Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:507) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5112175Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5113012Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5113787Z Dec 15 23:53:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:531) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5114521Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5115209Z Dec 15 23:53:42 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5115635Z Dec 15 23:53:42 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-12-15T23:54:00.5115949Z Dec 15 23:53:42 Caused by: java.io.IOException: Stream Closed
2020-12-15T23:54:00.5116246Z Dec 15 23:53:42 	at java.io.FileInputStream.readBytes(Native Method) ~[?:?]
2020-12-15T23:54:00.5116589Z Dec 15 23:53:42 	at java.io.FileInputStream.read(FileInputStream.java:279) ~[?:?]
2020-12-15T23:54:00.5117284Z Dec 15 23:53:42 	at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5118080Z Dec 15 23:53:42 	at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:61) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5118894Z Dec 15 23:53:42 	at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:51) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5119392Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:200) ~[?:?]
2020-12-15T23:54:00.5119808Z Dec 15 23:53:42 	at java.io.DataInputStream.readFully(DataInputStream.java:170) ~[?:?]
2020-12-15T23:54:00.5120605Z Dec 15 23:53:42 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5121576Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:222) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5122579Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:169) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5123543Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:152) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5124476Z Dec 15 23:53:42 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:269) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
2020-12-15T23:54:00.5124994Z Dec 15 23:53:42 	... 16 more
{code}
 "	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13116982	Create common WebMonitorEndpoint	In order to reuse the existing the REST handlers, we should create a common {{WebMonitorEndpoint}} which is shared by the {{Dispatcher}} and the {{JobMaster}} component.	FLINK	Closed	3	7	10066	flip-6
12996106	Implement basic RPC abstraction	"As part of refactoring of the cluster management, we can introduce a new RPC abstraction on top of our Akka-based distributed coordination.

It should address the following issues:

  - Add type safety to the sender and receiver of messages. We want proper types methods to be called, rather than haveing generic message types and pattern matching everywhere. This is similar to typed actors.

  - Make the message receivers testable without involving actors, i.e. the methods should be callable directly. When used with other component, the receiver will be wrapped in an actor that calls the methods based on received messages.

  - We want to keep the paradigm of single-threaded execution per ""actor""

There is some basic code layout in the following branch and commit:

https://github.com/apache/flink/tree/flip-6/flink-runtime/src/main/java/org/apache/flink/runtime/rpc"	FLINK	Resolved	3	2	10066	flip-6
13259539	Decouple leader address from LeaderContender	"At the moment, the {{LeaderContender}} need to know the address of the leader before it has gained leadership. This is problematic if one wants to decouple the leader election from the actual leader component which will potentially only be created after the leadership has been gained. If this is the case, then it is not always possible to know the address of the actual leader component before gaining the leadership.

In order to solve this problem, I propose to change the interface of {{LeaderElectionService#confirmLeadership(UUID)}} to {{LeaderElectionService#confirmLeadership(UUID, String)}} where the second parameter is the leader address under which the leader component is reachable."	FLINK	Closed	3	4	10066	pull-request-available
13110649	Add environment logging to cluster entrypoints	Add environment logging to all {{ClusterEntrypoints}}.	FLINK	Closed	4	4	10066	flip-6
13169712	DispatcherTest#testSubmittedJobGraphListener fails on Travis	"https://travis-ci.org/apache/flink/jobs/399331775
{code:java}
testSubmittedJobGraphListener(org.apache.flink.runtime.dispatcher.DispatcherTest)  Time elapsed: 0.103 sec  <<< FAILURE!
java.lang.AssertionError: 
Expected: a collection with size <1>
     but: collection size was <0>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.runtime.dispatcher.DispatcherTest.testSubmittedJobGraphListener(DispatcherTest.java:294)

testSubmittedJobGraphListener(org.apache.flink.runtime.dispatcher.DispatcherTest)  Time elapsed: 0.11 sec  <<< ERROR!
org.apache.flink.runtime.util.TestingFatalErrorHandler$TestingException: org.apache.flink.runtime.dispatcher.DispatcherException: Could not start the added job b8ab3b7fa8a929bf608a5b65896a2b17
	at org.apache.flink.runtime.util.TestingFatalErrorHandler.rethrowError(TestingFatalErrorHandler.java:51)
	at org.apache.flink.runtime.dispatcher.DispatcherTest.tearDown(DispatcherTest.java:219)
Caused by: org.apache.flink.runtime.dispatcher.DispatcherException: Could not start the added job b8ab3b7fa8a929bf608a5b65896a2b17
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$onAddedJobGraph$28(Dispatcher.java:845)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:70)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.util.FlinkException: Failed to submit job b8ab3b7fa8a929bf608a5b65896a2b17.
	at org.apache.flink.runtime.dispatcher.Dispatcher.submitJob(Dispatcher.java:254)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$onAddedJobGraph$27(Dispatcher.java:836)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:70)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
	at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:176)
	at org.apache.flink.runtime.dispatcher.Dispatcher$DefaultJobManagerRunnerFactory.createJobManagerRunner(Dispatcher.java:901)
	at org.apache.flink.runtime.dispatcher.DispatcherTest$ExpectedJobIdJobManagerRunnerFactory.createJobManagerRunner(DispatcherTest.java:603)
	at org.apache.flink.runtime.dispatcher.Dispatcher.createJobManagerRunner(Dispatcher.java:287)
	at org.apache.flink.runtime.dispatcher.Dispatcher.runJob(Dispatcher.java:277)
	at org.apache.flink.runtime.dispatcher.Dispatcher.persistAndRunJob(Dispatcher.java:262)
	at org.apache.flink.runtime.dispatcher.Dispatcher.submitJob(Dispatcher.java:249)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$onAddedJobGraph$27(Dispatcher.java:836)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:70)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.IllegalStateException: No libraries are registered for job b8ab3b7fa8a929bf608a5b65896a2b17
	at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager.getClassLoader(BlobLibraryCacheManager.java:175)
	at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:137)
	at org.apache.flink.runtime.dispatcher.Dispatcher$DefaultJobManagerRunnerFactory.createJobManagerRunner(Dispatcher.java:901)
	at org.apache.flink.runtime.dispatcher.DispatcherTest$ExpectedJobIdJobManagerRunnerFactory.createJobManagerRunner(DispatcherTest.java:603)
	at org.apache.flink.runtime.dispatcher.Dispatcher.createJobManagerRunner(Dispatcher.java:287)
	at org.apache.flink.runtime.dispatcher.Dispatcher.runJob(Dispatcher.java:277)
	at org.apache.flink.runtime.dispatcher.Dispatcher.persistAndRunJob(Dispatcher.java:262)
	at org.apache.flink.runtime.dispatcher.Dispatcher.submitJob(Dispatcher.java:249)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$onAddedJobGraph$27(Dispatcher.java:836)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:70)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107){code}"	FLINK	Resolved	2	1	10066	pull-request-available, test-stability
13097332	Fence ResourceManager	After introducing automatic fencing with FLINK-7078, we have to port the {{ResourceManager}} to use the {{FenceRpcEndpoint}} class. This also includes introducing a proper {{ResourceManagerId}} which replaces the {{UUID}} as leader id/fencing token.	FLINK	Closed	3	7	10066	flip-6
13284798	Improve error reporting when submitting batch job (instead of AskTimeoutException)	"While debugging the {{Shaded Hadoop S3A end-to-end test (minio)}} pre-commit test, I noticed that the JobSubmission is not producing very helpful error messages.

Environment:
- A simple batch wordcount job 
- a unavailable minio s3 filesystem service

What happens from a user's perspective:
- The job submission fails after 10 seconds with a AskTimeoutException:
{code}
2020-02-07T11:38:27.1189393Z akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-939201095]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-02-07T11:38:27.1189538Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-02-07T11:38:27.1189616Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-02-07T11:38:27.1189713Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-02-07T11:38:27.1189789Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-02-07T11:38:27.1189883Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2020-02-07T11:38:27.1189973Z 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2020-02-07T11:38:27.1190067Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2020-02-07T11:38:27.1190159Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2020-02-07T11:38:27.1190267Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2020-02-07T11:38:27.1190358Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2020-02-07T11:38:27.1190465Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-02-07T11:38:27.1190540Z 	at java.lang.Thread.run(Thread.java:748)
{code}

What a user would expect:
- An error message indicating why the job submission failed.
"	FLINK	Resolved	1	4	10066	pull-request-available
13396936	DefaultDispatcherRunner does not log when the Dispatcher gains and loses leadership	The {{DefaultDispatcherRunner}} does not log when it gains and loses leadership. This can make it hard to understand the behaviour from Flink's logs because {{*DispatcherLeaderProcess}} can suddenly be stopped w/o knowing why. I suggest to improve the logging in the {{DefaultDispatcherRunner}} to make the logs easier to understand.	FLINK	Closed	3	4	10066	pull-request-available
13259536	Simplify Dispatcher factories by removing generics	The {{Dispatcher}} factories can be simplified by removing the generics. For better maintainability of the code base, I propose to do this.	FLINK	Closed	4	4	10066	pull-request-available
13103978	Allow AbstractRestHandler to handle bad requests	The {{AbstractRestHandler}} parses the request and tries to generate a {{HandlerRequest}}. If this fails, then the server answers with an internal server error. Instead we should allow the {{AbstractRestHandler}} to be able to return a BAD_REQUEST status code. In order to do that, I would like to introduce a {{HandlerRequestException}} which can be thrown while creating the {{HandlerRequest}}. If this exception is thrown, then we return an error message with {{BAD_REQUEST}} {{HttpResponseStatus}}.	FLINK	Closed	3	1	10066	flip-6
13282197	Update recommended way to shut down detached Yarn session cluster	When starting a Yarn session cluster in detached mode, then it is printed that one should shut down the cluster via {{yarn application -kill}}. The problem of this approach is, however, that temporary files won't be cleaned up which remain on HDFS. Hence, I think a better way would be to reconnect via the {{yarn-session.sh}} tool and then to stop the cluster gracefully. This won't leave orphaned files behind.	FLINK	Resolved	3	4	10066	pull-request-available, usability
13374658	AdaptiveScheduler does not log failure cause when recovering	The {{AdaptiveScheduler}} does not log the failure cause when recovering.	FLINK	Closed	3	1	10066	pull-request-available
13127988	Make Cluster id typesafe	Currently, the cluster id is of type {{String}}. We should make the id typesafe to avoid mixups between different {{CustomCommandLines}} and {{ClusterDescriptors}}.	FLINK	Closed	3	7	10066	flip-6
13112013	Make MiniClusterConfiguration immutable	The {{MiniClusterConfiguration}} should be made immutable.	FLINK	Closed	4	4	10066	flip-6
13351100	Introduce DeclarativeSlotPool methods to set resource requirements to absolute values	I propose to add {{DeclarativeSlotPool.setResourceRequirements(ResourceCounter resourceRequirements)}} which sets the resource requirements to {{resourceRequirements}}. This will make the operation of this component easier.	FLINK	Closed	3	4	10066	pull-request-available
13110026	Make MetricFetcher work with RestfulGateway	In order to make the {{MetricFetcher}} work together with the new architecture, we have to remove it's dependence on the {{JobManagerGateway}}.	FLINK	Closed	3	7	10066	flip-6
13097307	Generalize leader id of RegisteredRpcConnection	The {{RegisteredRpcConnection}} only accepts {{UUIDs}} as leader ids. In order to introduce type safe ids for the different Flink components ({{JobMaster}}, {{TaskExecutor}}, {{ResourceManager}} and {{Dispatcher}}), we should generalize the {{RegisteredRpcConnection}} such that it supports all different types of leader ids/fencing tokens.	FLINK	Closed	4	1	10066	flip-6
13127930	Make CustomCommandLines non static in CliFrontend	For better testability and maintainability we should make the {{CustomCommandLine}} registration non-static in {{CliFrontend}}.	FLINK	Closed	3	7	10066	flip-6
13173937	ClientTest.testSimpleRequests fails on Travis	"{{ClientTest.testSimpleRequests}} fails on Travis with an {{AssertionError}}: https://api.travis-ci.org/v3/job/405690023/log.txt

"	FLINK	Closed	3	1	10066	auto-deprioritized-critical, pull-request-available, test-stability
13171570	Add Kubernetes deployment files for standalone job cluster	Similar to FLINK-9822, it would be helpful for the user to have example Kubernetes deployment files to start a standalone job cluster.	FLINK	Closed	3	2	10066	pull-request-available
13135455	Respect savepoint settings and recover from latest checkpoint in Flip-6	The {{JobMaster}} should respect savepoints and recover from the latest checkpoint if possible.	FLINK	Closed	3	4	10066	flip-6
13171553	Add cluster component command line parser	In order to parse command line options for the cluster components ({{TaskManagerRunner}}, {{ClusterEntrypoints}}), we should add a {{CommandLineParser}} which supports the common command line options ({{--configDir}}, {{--webui-port}} and dynamic properties which can override configuration values).	FLINK	Closed	3	4	10066	pull-request-available
13304432	LaunchCoordinatorTest fails	"Here is the [instance|https://dev.azure.com/arvidheise0209/arvidheise/_build/results?buildId=234&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=8d823410-c7c7-5a4d-68bb-fa7b08da17b9].

 
{noformat}
[ERROR] Tests run: 24, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 1.828 s <<< FAILURE! - in org.apache.flink.mesos.scheduler.LaunchCoordinatorTest
[ERROR] The LaunchCoordinator when in state GatheringOffers should handle StateTimeout which stays in GatheringOffers when task queue is non-empty(org.apache.flink.mesos.scheduler.LaunchCoordinatorTest)  Time elapsed: 0.021 s  <<< ERROR!
java.lang.IllegalStateException: cannot reserve actor name '$$u': terminating
	at akka.actor.dungeon.ChildrenContainer$TerminatingChildrenContainer.reserve(ChildrenContainer.scala:188)
	at akka.actor.dungeon.Children$class.reserveChild(Children.scala:135)
	at akka.actor.ActorCell.reserveChild(ActorCell.scala:429)
	at akka.testkit.TestActorRef.<init>(TestActorRef.scala:33)
	at akka.testkit.TestFSMRef.<init>(TestFSMRef.scala:40)
	at akka.testkit.TestFSMRef$.apply(TestFSMRef.scala:91)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$Context.<init>(LaunchCoordinatorTest.scala:254)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34$$anon$25.<init>(LaunchCoordinatorTest.scala:459)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34.apply(LaunchCoordinatorTest.scala:459)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34.apply(LaunchCoordinatorTest.scala:459)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078)
	at org.scalatest.TestSuite$class.withFixture(TestSuite.scala:196)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.withFixture(LaunchCoordinatorTest.scala:57)
	at org.scalatest.WordSpecLike$class.invokeWithFixture$1(WordSpecLike.scala:1075)
	at org.scalatest.WordSpecLike$$anonfun$runTest$1.apply(WordSpecLike.scala:1088)
	at org.scalatest.WordSpecLike$$anonfun$runTest$1.apply(WordSpecLike.scala:1088)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.WordSpecLike$class.runTest(WordSpecLike.scala:1088)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.runTest(LaunchCoordinatorTest.scala:57)
	at org.scalatest.WordSpecLike$$anonfun$runTests$1.apply(WordSpecLike.scala:1147)
	at org.scalatest.WordSpecLike$$anonfun$runTests$1.apply(WordSpecLike.scala:1147)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418){noformat}"	FLINK	Resolved	3	1	10066	pull-request-available, test-stability
13368673	Better document the distinction between stop-with-savepoint and stop-with-savepoint-with-drain	The [Flink documentation|https://ci.apache.org/projects/flink/flink-docs-stable/deployment/cli.html#terminating-a-job] only contains very few details about the difference between stop-with-savepoint and stop-with-savepoint-with-drain. We should better explain the semantic differences.	FLINK	Closed	4	4	10066	pull-request-available
13391119	Properties map is not set in DebeziumAvroFormatFactory	FLINK-21229 did not set the properties map correctly in DebeziumAvroFormatFactory.	FLINK	Closed	3	1	10269	pull-request-available
13424657	Use compact DataType serialization for default classes instead of internal ones	It is more likely that default conversion classes spam the plan than internal classes. In most cases when internal classes are used, they usually also use logical type instead of data type. So it should be safer to skip default conversion classes. This also reduces the plan size for serializing `ResolvedSchema`.	FLINK	Closed	3	7	10269	pull-request-available
13394413	Use consistent managed memory weights for StreamNode	"Managed memory that is declared on transformations via {{Transformation#declareManagedMemoryUseCaseAtOperatorScope(ManagedMemoryUseCase managedMemoryUseCase, int weight)}} should be declared using a weight.

Usually, a weight should be some kind of factor, however, in the table planner it is used a kibi byte value. This causes issues on the DataStream API side that sets it to {{1}} in {{org.apache.flink.streaming.runtime.translators.BatchExecutionUtils#applyBatchExecutionSettings}}."	FLINK	Closed	3	7	10269	pull-request-available
13194801	flink-end-to-end-tests can fail silently	"Because they are written in bash and they are not setting

{code:bash}
set -e
{code}

at the beginning, errors can be swallowed silently."	FLINK	Resolved	1	1	10269	pull-request-available
13381453	Drop usages of legacy planner in SQL Client	Drop legacy planner support for SQL Client	FLINK	Closed	3	7	10269	pull-request-available
13315834	Remove RowDataTypeInfo	FLINK-17000 introduced a TypeInformation class that should replace most of the type information in the Blink planner. We start with removing RowDataTypeInfo.	FLINK	Closed	3	7	10269	pull-request-available
13399730	Disable single rowtime column check for collect/print	"As seen in FLINK-23751, the single rowtime column check can occur also during collecting and printing which is not important there as watermarks as not used.

The exception is also misleading as it references a {{DataStream}}:
{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Found more than one rowtime field: [bidtime, window_time] in the query when insert into 'default_catalog.default_database.Unregistered_Collect_Sink_8'.
Please select the rowtime field that should be used as event-time timestamp for the DataStream by casting all other fields to TIMESTAMP.
{code}"	FLINK	Closed	3	1	10269	pull-request-available, stale-assigned
13194331	Make flink-formats Scala-free	{{flink-table}} is the only dependency that pulls in Scala for {{flink-json}}, {{flink-avro}}. We should aim to make {{flink-formats}} Scala-free using only a dependency to {{flink-table-common}}.	FLINK	Resolved	3	7	10269	pull-request-available
13306718	Don't allow self referencing structured type	"Currently, the logical constraint ""A type cannot be defined so that one of its attribute types (transitively) uses itself."" is not enforced during extraction and leads to a stack overflow. We should throw a helpful exception instead."	FLINK	Closed	3	7	10269	pull-request-available
13315579	Scala varargs cause exception for new inference	Scala varargs are supported but cause an exception currently. Because there are two signatures (a valid and invalid one) in the class file.	FLINK	Closed	3	7	10269	pull-request-available
13391510	Simplify BlinkExecutorFactory stack	The {{BlinkExecutorFactory}} stack uses the old table factory stack and is not needed anymore as the old planner has been removed. We should simplify the logic there.	FLINK	Closed	3	7	10269	pull-request-available
13339174	Fix computed column can't be defined on the metadata column	Currenlty it's not allowed to define the computed column on metadata. It's very useful when users extract the metadata from record and use the information as watermark. 	FLINK	Closed	3	7	10269	pull-request-available
13377609	Drop usages of BatchTableEnvironment and old planner in Python	"This is a major cleanup of the Python module that drops support for BatchTableEnvironment and old planner.

Removes usages of:
 - DataSet
 - BatchTableEnvironment
 - Legacy planner
 - ExecutionEnvironment

 

Note: Batch processing is still possible via the unified \{{TableEnvironment}}."	FLINK	Closed	3	7	10269	pull-request-available
13320800	Update internal aggregate functions to new type system	Imperative aggregate functions use the old type system for computation. After FLINK-15803 is completed, we can update those implementations.	FLINK	Closed	3	7	10269	pull-request-available
13316444	Simplify the creation of explicit structured types	Depending on the use case, it might be necessary to explicitly declare a structured type instead of using `DataTypes.of`. This would already help in tests and for the future aggregate function support with `MapView`, `ListView`. 	FLINK	Closed	3	7	10269	pull-request-available
13318705	Fix Scala code examples for UDF type inference annotations	"The Scala code examples for the [UDF type inference annotations|https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#type-inference] are not correct.

For example: the following {{FunctionHint}} annotation 

{code:scala}
@FunctionHint(
  input = Array(@DataTypeHint(""INT""), @DataTypeHint(""INT"")),
  output = @DataTypeHint(""INT"")
)
{code}


needs to be changed to

{code:scala}
@FunctionHint(
  input = Array(new DataTypeHint(""INT""), new DataTypeHint(""INT"")),
  output = new DataTypeHint(""INT"")
)
{code}"	FLINK	Closed	3	1	10269	pull-request-available
12965022	Add a TableSink for Elasticsearch	Add a TableSink that writes data to Elasticsearch	FLINK	Resolved	3	2	10269	pull-request-available
13277698	Expose the new type inference for scalar functions	"After FLINK-15281 is merged, we are able to introduce functions with the new type inference. We start by enabling scalar functions through the stack.

This includes the following changes:

- Introduce a {{UserDefinedFunction.getTypeInference}} method
- Expose {{TableEnvironment.createTemporarySystemFunction}} and {{TableEnvironment.createTemporaryFunction}}
- Update FunctionCatalog
- Update the code generation"	FLINK	Closed	3	7	10269	pull-request-available
13328719	Update API module for FLIP-107	This includes updating the `TableSchema`, `SqlToOperationConverter`, `MergeTableLikeUtil`. Everything until planning.	FLINK	Closed	3	7	10269	pull-request-available
13328018	Allow to read and write metadata in Kafka table source/sink	Add `SupportsReadingMetadata` and `SupportsWritingMetadata` to the Kafka table source and sink.	FLINK	Closed	3	7	10269	pull-request-available
13279778	Implement a DataTypeLookup	Implement the {{DataTypeLookup}} interface and make it available for both in Table API and SQL planner. This is necessary to resolve {{RAW}} types and have access to the catalog for types in the future.	FLINK	Closed	3	7	10269	pull-request-available
13174320	Document unified table sources/sinks/formats	The recent unification of table sources/sinks/formats needs documentation. I propose a new page that explains the built-in sources, sinks, and formats as well as a page for customization of public interfaces.	FLINK	Resolved	3	4	10269	pull-request-available
13341001	FactoryUtil will give an incorrect error message when multiple factories fit the connector identifier	"I was playing with user-defined connectors when I found the following error message:

{code}
Caused by: org.apache.flink.table.api.ValidationException: Multiple factories for identifier 'odps' that implement 'org.apache.flink.table.factories.DynamicTableFactory' found in the classpath.

Ambiguous factory classes are:

java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:258)
	at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:370)
	... 71 more
{code}

This is caused by {{FactoryUtil.java}} line 265, where {{.map(f -> factories.getClass().getName())}} should be {{.map(f -> f.getClass().getName())}}"	FLINK	Closed	3	1	10269	pull-request-available
13316074	Update Table API set operations to the new type system	{{org.apache.flink.table.operations.utils.SetOperationFactory}} is still using the old type system and thus makes e.g. unions difficult to implement.	FLINK	Closed	3	7	10269	pull-request-available
13194067	Fix handling of retractions after clean up	"Our online Flink Job run about a week，job contain sql ：
{code:java}
select  `time`,  
        lower(trim(os_type)) as os_type, 
        count(distinct feed_id) as feed_total_view  
from  my_table 
group by `time`, lower(trim(os_type)){code}
 

  then occur NPE: 

 
{code:java}
java.lang.NullPointerException

at scala.Predef$.Long2long(Predef.scala:363)

at org.apache.flink.table.functions.aggfunctions.DistinctAccumulator.remove(DistinctAccumulator.scala:109)

at NonWindowedAggregationHelper$894.retract(Unknown Source)

at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.processElement(GroupAggProcessFunction.scala:124)

at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.processElement(GroupAggProcessFunction.scala:39)

at org.apache.flink.streaming.api.operators.LegacyKeyedProcessOperator.processElement(LegacyKeyedProcessOperator.java:88)

at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)

at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)

at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)

at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)

at java.lang.Thread.run(Thread.java:745)
{code}
 

 

View DistinctAccumulator.remove
 !image-2018-10-25-14-46-03-373.png!

 

this NPE should currentCnt = null lead to, so we simple handle like :
{code:java}
def remove(params: Row): Boolean = {
  if(!distinctValueMap.contains(params)){
    true
  }else{
    val currentCnt = distinctValueMap.get(params)
    // 
    if (currentCnt == null || currentCnt == 1) {
      distinctValueMap.remove(params)
      true
    } else {
      var value = currentCnt - 1L
      if(value < 0){
        value = 1
      }
      distinctValueMap.put(params, value)
      false
    }
  }
}{code}
 

Update:

Because state clean up happens in processing time, it might be
 the case that retractions are arriving after the state has
 been cleaned up. Before these changes, a new accumulator was
 created and invalid retraction messages were emitted. This
 change drops retraction messages for which no accumulator
 exists. "	FLINK	Closed	4	1	10269	pull-request-available
13296452	Type information in sources should cover all data structures	"In order to ensure that we don't loose any type information in sources, this issue will add a type integrity test for type information converters. 

See {{ScanTableSource#Context#createTypeInformation(DataType)}}."	FLINK	Closed	3	7	10269	pull-request-available
13322483	Update documentation about user-defined aggregate functions	The documentation needs an update because all functions support the new type inference now.	FLINK	Closed	3	7	10269	pull-request-available
13338857	Add StreamTableEnvironment.to/fromChangelogStream	"The more powerful API exposes the same features as a DynamicTableSource/Sink in FLIP-95. It enables ingesting a changelog stream with primary key, computed columns, and watermarks.

{code}
StreamTableEnvironment.fromChangelogStream(DataStream<Row>): Table
{code}

Goal: Create a table from a Changelog as easy as possible. Derive schema entirely.

{code}
StreamTableEnvironment.fromChangelogStream(DataStream<Row>, Schema): Table
{code}

Goal: Create a table from a Changelog with Schema similar to a source.

And similar:

{code}
StreamTableEnvironment.toChangelogStream(Table): DataStream<Row>
StreamTableEnvironment.toChangelogStream(Table, Schema): DataStream<Row>
{code}"	FLINK	Closed	3	7	10269	pull-request-available
13393687	Use pipeline name consistently across DataStream API and Table API	Currently, the pipeline name configured in {{StreamExecutionEnvironment}} is not always considered in a table environment. E.g. when using {{executeSql}}. In general, the job name code can be simplified by relying on the {{StreamGraphGenerator}}.	FLINK	Closed	3	7	10269	pull-request-available
13307343	Implement type inference for AS	Type information gets lost due to the legacy planner expressions. The user might experience unexpected exceptions.	FLINK	Closed	3	7	10269	pull-request-available
13321301	Add StreamStatementSet.attachAsDataStream	"StatementSet solves use cases for pure SQL & Table API pipelines. However, currently there is no way of creating StatementSet for a DataStream API job.

We propose the following API:
{code}
StreamTableEnvironment.createStatementSet(): StreamStatementSet // return a stream-specific set

StreamStatementSet extends StatementSet {
  /**
   * Attaches the optimized statement set to the DataStream pipeline.
   */
  attachAsDataStream(): Unit
}
{code}

An example could look like:
{code}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

tEnv
  .createStatementSet()
  .addInsert(tEnv.from(""FromTable""))
  .attachAsDataStream();

tEnv.from(""OtherTable"").toDataStream(...) // continue with further operations
{code}"	FLINK	Closed	3	7	10269	pull-request-available, usability
13003279	Fix flaky test ScalarFunctionsTest.testCurrentTimePoint	"It seems that the test is still non deterministic.

{code}
org.apache.flink.api.table.expressions.ScalarFunctionsTest
testCurrentTimePoint(org.apache.flink.api.table.expressions.ScalarFunctionsTest)  Time elapsed: 0.083 sec  <<< FAILURE!
org.junit.ComparisonFailure: Wrong result for: AS(>=(CHAR_LENGTH(CAST(CURRENT_TIMESTAMP):VARCHAR(1) CHARACTER SET ""ISO-8859-1"" COLLATE ""ISO-8859-1$en_US$primary"" NOT NULL), 22), '_c0') expected:<[tru]e> but was:<[fals]e>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.apache.flink.api.table.expressions.utils.ExpressionTestBase$$anonfun$evaluateExprs$1.apply(ExpressionTestBase.scala:126)
	at org.apache.flink.api.table.expressions.utils.ExpressionTestBase$$anonfun$evaluateExprs$1.apply(ExpressionTestBase.scala:123)
	at scala.collection.mutable.LinkedHashSet.foreach(LinkedHashSet.scala:87)
	at org.apache.flink.api.table.expressions.utils.ExpressionTestBase.evaluateExprs(ExpressionTestBase.scala:123)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
{code}"	FLINK	Resolved	3	1	10269	starter
13389391	Support object reuse disabled in OperatorChain	"Currently, object reuse must be enabled in order to use chained sources.

Tests such as `HiveDialectQueryITCase` will fail with an exception:
{code}
2021-07-12T14:47:55.8233741Z Jul 12 14:47:55 [ERROR] testQueries(org.apache.flink.connectors.hive.HiveDialectQueryITCase)  Time elapsed: 12.283 s  <<< ERROR!
2021-07-12T14:47:55.8234433Z Jul 12 14:47:55 java.lang.RuntimeException: Failed to fetch next result
2021-07-12T14:47:55.8235133Z Jul 12 14:47:55 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-07-12T14:47:55.8235958Z Jul 12 14:47:55 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-07-12T14:47:55.8236774Z Jul 12 14:47:55 	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
....
2021-07-12T14:47:55.8313594Z Jul 12 14:47:55 Caused by: java.lang.UnsupportedOperationException: Currently chained sources are supported only with objectReuse enabled
2021-07-12T14:47:55.8314356Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedSourceOutput(OperatorChain.java:355)
2021-07-12T14:47:55.8315109Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedSources(OperatorChain.java:322)
2021-07-12T14:47:55.8315820Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:220)
2021-07-12T14:47:55.8316506Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:558)
2021-07-12T14:47:55.8317209Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:661)
2021-07-12T14:47:55.8317948Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:547)
2021-07-12T14:47:55.8318626Z Jul 12 14:47:55 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
2021-07-12T14:47:55.8319205Z Jul 12 14:47:55 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
2021-07-12T14:47:55.8319725Z Jul 12 14:47:55 	at java.lang.Thread.run(Thread.java:748)
2021-07-12T14:47:55.8320122Z Jul 12 1
{code}

The fix should looks as follows:

This particular exception should be rather straightforward to fix. The reason it's not implemented is because the chaining sources feature was implemented in the minimal scope required by blink planner and is missing around ~50-100 lines of production code to work with the object reuse disabled.
In the {{OperatorChain#createChainedSourceOutput}} we need to something similar as is done in {{OperatorChain#wrapOperatorIntoOutput}} , so something like:
{code}
        if (containingTask.getExecutionConfig().isObjectReuseEnabled()) {
            return closer.register(new ChainingOutput(input, metricGroup, outputTag));
        } else {
            TypeSerializer<IN> inSerializer =
                    operatorConfig.getTypeSerializerIn1(userCodeClassloader);
            return closer.register(new CopyingChainingOutput(input, inSerializer, metricGroup, outputTag));
        }
{code}
the missing part to do that is to make {{CopyingChainingOutput}} work with an Input instead of an Operator."	FLINK	Closed	3	7	10269	pull-request-available
13441362	SQL CAST(' 1 ' as BIGINT) returns wrong result	"{code:sql}
Flink SQL> select
>                     cast(' 1 ' as tinyint),
>                     cast(' 1 ' as smallint),
>                     cast(' 1 ' as int),
>                     cast(' 1 ' as bigint),
>                     cast(' 1 ' as float),
>                     cast(' 1 ' as double);
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
| op | EXPR$0 | EXPR$1 |      EXPR$2 |               EXPR$3 |                         EXPR$4 |                         EXPR$5 |
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: ' 1 '. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toByte(BinaryStringDataUtil.java:568)
	at StreamExecCalc$392.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
{code}

Setting CAST behavior to legacy but got null result :

{code}
Flink SQL> set table.exec.legacy-cast-behaviour=enabled;
[INFO] Session property has been set.

Flink SQL> select
>                     cast(' 1 ' as tinyint),
>                     cast(' 1 ' as smallint),
>                     cast(' 1 ' as int),
>                     cast(' 1 ' as bigint),
>                     cast(' 1 ' as float),
>                     cast(' 1 ' as double);
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
| op | EXPR$0 | EXPR$1 |      EXPR$2 |               EXPR$3 |                         EXPR$4 |                         EXPR$5 |
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Column 'EXPR$0' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='DROP' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processNotNullConstraint(ConstraintEnforcer.java:261)
	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:241)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at StreamExecCalc$591.processElement_split1(Unknown Source)
	at StreamExecCalc$591.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
{code}


In 1.14 the result should be {{[1, 1, 1, 1, 1.0, 1.0]}}. 

In Postgres:
{code}
postgres=# select cast(' 1 ' as int), cast(' 1 ' as bigint), cast(' 1 ' as float);
 int4 | int8 | float8
------+------+--------
    1 |    1 |      1
(1 row)
{code}"	FLINK	Closed	2	1	10269	pull-request-available
13269870	Relax structured types constraints	"As mentioned in FLIP-65:

In order to allow type extraction of structured types that are not registered in a catalog, we need to relax the structured type concept to ""inline or anonymous structured types"" that are not identified by an object identifier in a catalog but the fully qualified implementation class.

In order to support case classes and immutable types, we relax the constraint of enforcing a default constructor by the alternative of having a constructor that fully assigns all fields (same parameter names and types). Because we are already using code generation, the implementation of creating instances even without a default constructor is relatively easy."	FLINK	Closed	3	7	10269	pull-request-available
13382145	"Remove ""blink"" term in code base"	"Apart from FLINK-22879 and some API parts (such as EnvironmentSettings and old SQL Client YAML), we should not use the term ""blink"" in the code base and documentation anymore. For giving some background information, we should only document that the current planner was called ""blink planner"" in the past."	FLINK	Closed	3	7	10269	pull-request-available
13217485	Deprecate CalciteConfig temporarily	"The \{{CalciteConfig}} and the \{{CalciteConfigBuilder}} are tightly coupled to Calcite. However, once we move \{{TableConfig}} to \{{flink-table-api-java}} this configuration will not be possible anymore as Calcite is located in the planner module. We should deprecate the corresponding methods and classes for now to warn users until an alternative is available.

This shortcoming has not been covered in FLIP-32."	FLINK	Closed	3	4	10269	pull-request-available
13359278	Implement Schema, ResolvedSchema, SchemaResolver	Introduces the main classes and utilities around schema mentioned in FLIP-164.	FLINK	Closed	3	7	10269	pull-request-available
13328263	Support key and value formats in Kafka connector	"Introduce the following options for Kafka:
{code}
key.format, value.format, key.fields, value.fields-include, fields.verify-integrity
{code}

As described in FLIP-107."	FLINK	Closed	3	7	10269	pull-request-available
13304662	Fix shortcomings in new data structures	"There are a couple of shortcomings in the new data structures:
- The some data structures do not provide a {{hashCode/equals}} for testing. 
- `RawValueData` cannot be created from bytes.
- Accessing elements requires dealing with logical types during runtime.
- Null checks are performed multiple times during runtime even for types that are declared as NOT NULL."	FLINK	Closed	3	7	10269	pull-request-available
13381402	Drop usages of legacy planner in JDBC module	Remove references to {{flink-table-planner}} in the JDBC module.	FLINK	Closed	3	7	10269	pull-request-available
13362096	Implement ResolvedExpression.asSerializableString for SQL	SQL expressions are the first ones (and maybe even the only ones in the future) that implement `ResolvedExpression.asSerializableString`.	FLINK	Closed	3	7	10269	pull-request-available
13525719	CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy	"This issue is reported from the [user mail list|https://lists.apache.org/thread/y6fgzyx330omhkr40376knw8k4oczz3s].

The stacktrace is 
{code:java}
Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) {code}
The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is ""Missing""; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.
!screenshot-1.png|width=675,height=295!"	FLINK	Closed	3	1	10269	pull-request-available
13299144	Improve FieldsDataType	"The problem with {{FieldsDataType}} is that the method {{getFieldDataTypes}} does not keep the order of the fields stored in the logical type. Therefore at couple of locations we have to first iterate over the names in logical type and then get the DataTypes.

{code}
final RowType rowType = (RowType) fieldsDataType.getLogicalType();
final String[] fieldNames = rowType.getFields()
	.stream()
	.map(RowType.RowField::getName)
	.toArray(String[]::new);

final TypeInformation<?>[] fieldTypes = Stream.of(fieldNames)
	.map(name -> fieldsDataType.getFieldDataTypes().get(name))
	.map(LegacyTypeInfoDataTypeConverter::toLegacyTypeInfo)
	.toArray(TypeInformation[]::new);
{code}"	FLINK	Closed	4	7	10269	pull-request-available
13247621	Rename Batch/StreamTableSourceFactory methods for avoiding name clashes	FLINK-11067 introduced {{org.apache.flink.table.factories.BatchTableSourceFactory#createTableSource}} and {{org.apache.flink.table.factories.StreamTableSourceFactory#createTableSource}}. Those methods should have more specific names in order to prevent name clashes when implementing both interfaces.	FLINK	Closed	3	1	10269	pull-request-available
13319108	Allow variables for column names in Scala Table API	"User have reported that the Scala API lacks a way to reference columns via a name that is stored in a variable. String interpolation is inconvenient in this case:

We should allow this also in Scala:
{code}
tab.select($(keyVar), $(valueVar))
{code}"	FLINK	Closed	3	4	10269	pull-request-available
13335732	Merge kafka-connector-base into flink-connector-kafka	Nowadays, we only offer one unified Kafka connector, so a base module is not required anymore. The base module also uses Kafka 0.10 at the moment. We should merge those two modules into one.	FLINK	Closed	3	4	10269	pull-request-available
13229322	Table API does not allow non-static inner class as UDF	See details here [https://lists.apache.org/thread.html/9ecec89ba1225dbd6b3ea2466a910ad9685a42a4672b449f6ee13565@%3Cuser.flink.apache.org%3E]	FLINK	Closed	3	1	10269	pull-request-available
13325553	No access to metric group in ScalarFunction when optimizing	"Under some circumstances, I cannot access {{context.getMetricGroup()}} in a {{ScalarFunction}} like this (full job attached):
{code:java}
  public static class MyUDF extends ScalarFunction {
    @Override
    public void open(FunctionContext context) throws Exception {
      super.open(context);
      context.getMetricGroup();
    }

    public Integer eval(Integer id) {
      return id;
    }
  }
{code}
which leads to this exception:
{code:java}
Exception in thread ""main"" java.lang.UnsupportedOperationException: getMetricGroup is not supported when optimizing
	at org.apache.flink.table.planner.codegen.ConstantFunctionContext.getMetricGroup(ExpressionReducer.scala:249)
	at com.ververica.MetricsGroupBug$MyUDF.open(MetricsGroupBug.java:57)
	at ExpressionReducer$2.open(Unknown Source)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:118)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:696)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:618)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:303)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:328)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:562)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:427)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:264)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:223)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:210)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:164)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:84)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:164)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1264)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:700)
	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:565)
	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:549)
	at com.ververica.MetricsGroupBug.main(MetricsGroupBug.java:50)
{code}
I also tried to work around this with a try-catch, assuming that this method is called once during optimisation and another time at runtime. However, it seems as if {{open()}} is actually only called once (during optimization) thus giving me no choice to access the metrics group.

It seems that removing the where condition before my UDF call also fixes it but it shouldn't be that way..."	FLINK	Closed	3	1	10269	pull-request-available
13420559	Add catalog object compile/restore options	A prerequisite for serialization/deserialization of various entities.	FLINK	Closed	3	7	10269	pull-request-available
13182292	Elasticsearch 6 UpdateRequest fail because of binary incompatibility	"When trying to send UpdateRequest(s) to ElasticSearch6, and one gets the following
error:

{code}
Caused by: java.lang.NoSuchMethodError:
org.elasticsearch.action.bulk.BulkProcessor.add(Lorg/elasticsearch/action/ActionRequest;)Lorg/elasticsearch/action/bulk/BulkProcessor;
	at
org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer.add(BulkProcessorIndexer.java:76)
{code}

ElasticsearchSinkFunction:
{code}
	import org.elasticsearch.action.update.UpdateRequest
	def upsertRequest(element: T): UpdateRequest = {
		new UpdateRequest(
			""myIndex"",
			""record"",
			s""${element.id}"")
	        	.doc(element.toMap())
	}
	override def process(element: T, runtimeContext: RuntimeContext,
requestIndexer: RequestIndexer): Unit = {
		requestIndexer.add(upsertRequest(element))
	}
{code}

This is due to a binary compatibility issue between the base module (which is compiled against a very old ES version and the current Elasticsearch version).

As a work around you can simply copy org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer to your project. This should ensure that the class is compiled correctly."	FLINK	Resolved	1	1	10269	pull-request-available
13409386	Validate partition columns for ResolvedCatalogTable	Currently, partition columns are not validated and might not exist in the schema.	FLINK	Closed	3	7	10269	pull-request-available
13175092	Support a custom FlinkKafkaPartitioner for a Kafka table sink factory	Currently, the Kafka table sink factory does not support a custom FlinkKafkaPartitioner. However, this is needed for many use cases.	FLINK	Resolved	3	7	10269	pull-request-available
13287257	Avoid unnecessary casting in TypeInferenceOperandChecker	{{TypeInferenceOperandChecker}} inserts casts that are actually not necessary. It should have the same behavior as in {{ResolveCallByArgumentsRule}}.	FLINK	Resolved	3	7	10269	pull-request-available
13195911	Interval join produces wrong result type in Scala API	"When stream is a Scala case class, the TypeInformation will fall back to GenericType in the process function which result in bad performance when union another DataStream.

In the union method of DataStream, the type is first checked for equality.

Here is an example:
{code:java}
object Test {

    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment

      val orderA: DataStream[Order] = env.fromCollection(Seq(
        Order(1L, ""beer"", 3),
         Order(1L, ""diaper"", 4),
         Order(3L, ""rubber"", 2)))

      val orderB: DataStream[Order] = env.fromCollection(Seq(
        new Order(2L, ""pen"", 3),
        new Order(2L, ""rubber"", 3),
        new Order(4L, ""beer"", 1)))

      val orderC: DataStream[Order] = orderA.keyBy(_.user)
        .intervalJoin(orderB.keyBy(_.user))
        .between(Time.seconds(0), Time.seconds(0))
        .process(new ProcessJoinFunction[Order, Order, Order] {
          override def processElement(left: Order, right: Order, ctx: ProcessJoinFunction[Order, Order, Order]#Context, out: Collector[Order]): Unit = {
            out.collect(left)
          }})

      println(""C: "" + orderC.dataType.toString)
      println(""B: "" + orderB.dataType.toString)

      orderC.union(orderB).print()

      env.execute()
    }

    case class Order(user: Long, product: String, amount: Int)
}{code}
Here is the Exception:
{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot union streams of different types: GenericType<com.manbuyun.awesome.flink.Test.Order> and com.manbuyun.awesome.flink.Test$Order(user: Long, product: String, amount: Integer)
 at org.apache.flink.streaming.api.datastream.DataStream.union(DataStream.java:219)
 at org.apache.flink.streaming.api.scala.DataStream.union(DataStream.scala:357)
 at com.manbuyun.awesome.flink.Test$.main(Test.scala:38)
 at com.manbuyun.awesome.flink.Test.main(Test.scala){code}
 "	FLINK	Closed	3	1	10269	pull-request-available
13544976	Insufficient validation for table.local-time-zone	"There are still cases where timezone information is lost silently due to the interaction between {{java.util.TimeZone}} and {{java.time.ZoneId}}.

This might be theoretical problem, but I would feel safer if we change the check to:
{code}
if (!java.util.TimeZone.getTimeZone(zoneId).toZoneId().equals(ZoneId.of(zoneId))) {
   throw new ValidationException(errorMessage);
}
{code}"	FLINK	Closed	3	1	10269	pull-request-available
13429731	Harden AggregateCall serialization in JSON plan	"Similar to FLINK-25385, we also need to revisit AggregateCall serialization.

- It should not contain Java serialization.
- It should support all kinds of agg functions.
- It should not support legacy stacks."	FLINK	Closed	3	7	10269	pull-request-available
13281554	SQLClientKafkaITCase.testKafka failed on Travis	"The end-to-end test {{SQLClientKafkaITCase.testKafka}} failed with

{code}
22:09:36.957 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 129.244 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
22:09:36.958 [ERROR] testKafka[2: kafka-version:universal kafka-sql-version:.*kafka.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 45.954 s  <<< FAILURE!
org.junit.ComparisonFailure: 
expected:<...-03-12 09:00:00.000,[Bob,This was another warning.,1,Success constant folding.
2018-03-12 09:00:00.000,Steve,This was another info.,2],Success constant fo...> but was:<...-03-12 09:00:00.000,[Steve,This was another info.,2,Success constant folding.
2018-03-12 09:00:00.000,Bob,This was another warning.,1],Success constant fo...>
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.checkCsvResultFile(SQLClientKafkaITCase.java:226)
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:154)

22:09:37.287 [INFO] 
22:09:37.288 [INFO] Results:
22:09:37.288 [INFO] 
22:09:37.288 [ERROR] Failures: 
22:09:37.288 [ERROR]   SQLClientKafkaITCase.testKafka:154->checkCsvResultFile:226 expected:<...-03-12 09:00:00.000,[Bob,This was another warning.,1,Success constant folding.
{code}

https://api.travis-ci.org/v3/job/641346345/log.txt"	FLINK	Closed	2	1	10269	pull-request-available, test-stability
13139454	Supported Data Types - Six or Seven	"See [Supported Data Types|https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#supported-data-types] in the online documentation. The text specifies that there are ""six different categories of data types,"" but the list below contains seven items. I suggest that you omit a specific number in the sentence preceding the list. Here is a suggestion: ""DataSet and DataStream support the following categories of data elements."" See the attached screenshot. Note: Please let me know if these types of issues are too trivial to report. Thx."	FLINK	Closed	4	1	10269	documentation
13247070	SQL Client end-to-end test fails	"The SQL Client test does not work on the current master and hangs when executing CEP SQL. We reproduced this on two machines.

At commit 475c30cd4064a7bc2e32c963b6ca58e7623251c6 it was working."	FLINK	Resolved	1	1	10269	pull-request-available
13219956	Simplify OVER window Table API classes	The OVER windows Table API can be simplified without breaking the backwards compatibility. Especially for FLINK-11449 and FLINK-11448 it makes sense to simplify the logic here.	FLINK	Closed	3	4	10269	pull-request-available
13179545	Dependency problems when executing SQL query in sql-client	"When tried to run query:
{code}
select count(distinct name) from (Values ('a'), ('b')) AS NameTable(name)
{code}
in {{sql-client.sh}} I got:
{code}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 43, Column 10: Unknown variable or type ""org.apache.commons.codec.binary.Base64""
{code}"	FLINK	Resolved	1	1	10269	pull-request-available
13381722	Add possibility to call built-in functions in SpecializedFunction	This is the last missing piece to avoid code generation when developing built-in functions. Core operations such as CAST, EQUALS, etc. will still use code generation but other built-in functions should be able to use these core operations without the need for generating code. It should be possible to call other built-in functions via a SpecializedFunction.	FLINK	Closed	3	7	10269	auto-unassigned, pull-request-available
13370321	Implement type inference for agg functions	Update avg, count, min, max, sum, sum0, stddevPop, stddevSamp, varPop, varSamp.	FLINK	Closed	3	7	10269	pull-request-available
13296442	Add a changeflag to Row type	"In Blink planner, the change flag of records travelling through the pipeline are part of the record itself but not part of the logical schema. This simplifies the architecture and API in many cases.

Which is why we aim adopt the same mechanism for {{org.apache.flink.types.Row}}.

Take {{tableEnv.toRetractStream()}} as an example that returns either Scala or Java {{Tuple2<Boolean, Row>}}. For FLIP-95 we need to support more update kinds than just a binary boolean.

This means:
- Add a changeflag {{RowKind}} to to {{Row}}
- Update the {{Row.toString()}} method
- Update serializers in backwards compatible way"	FLINK	Closed	3	7	10269	pull-request-available
13313189	GenericArrayData cannot convert object arrays to primitive arrays	GenericArrayData.toBooleanArray throws a cast exception if it is backed by an object array.	FLINK	Closed	3	1	10269	pull-request-available
13237242	Separate function implementation and definition	"This issue continues the work that was started in FLINK-11449. It distinguishes between function with implementation (UDFs) and function with no implementation.

The rough design looks as follows:

{noformat}
1. `interface FunctionDefinition`
   --> general interface for describing a function
   --> goal: separation of runtime and planning/optimization
   --> long-term methods: `getKind()` (aggregate, scalar, table), `getTypeInference()`
2. `interface UserDefinedFunctionDefinition extends FunctionDefinition`
   --> interface for describing a function with implementation
   --> methods: `createImplementation(): UserDefinedFunction`
   --> default: getTypeInference() = Util.DEFAULT_INFERENCE // future work
3. `class BuiltInFunctionDefinition implements FunctionDefinition`
   --> class for describing a function where the planner provides an implementation
   --> methods: `getName(): String`
4. Add `getKind` to `AggregateFunction`, `ScalarFunction`, `TableFunction`
{noformat}"	FLINK	Closed	3	7	10269	pull-request-available
13426866	org.apache.flink.table.runtime.generated.CompileUtils might cause class leaks	CompileUtils has two static caches, namely COMPILED_CACHE and COMPILED_EXPRESSION_CACHE. COMPILED_CACHE is check that it might cache the user ClassLoaders with strong references, thus it might need be improved. COMPILED_EXPRESSION_CACHE would need a double check.	FLINK	Closed	3	7	10269	pull-request-available
13566848	Reduce instantiation of ScanRuntimeProvider in streaming mode	This is pure performance optimization by avoiding an additional call to \{{ScanTableSource#getScanRuntimeProvider}} in \{{org.apache.flink.table.planner.connectors.DynamicSourceUtils#validateScanSource}}.	FLINK	Closed	3	4	10269	pull-request-available
13477229	Use setting instead of merging for pipeline.jars in StreamExecutionEnvironment.configure	"See discussion in FLINK-28213. We should restore a ""setting"" behavior instead of a ""merging"" one."	FLINK	Closed	2	7	10269	pull-request-available
13247585	ThreadLocalCache clashes for Blink planner	{{org.apache.flink.table.runtime.functions.ThreadLocalCache}} currently clashes.	FLINK	Closed	1	7	10269	pull-request-available
13182166	User-defined function with LITERAL paramters yields CompileException	"When using a user-defined scalar function only with literal parameters, a {{CompileException}} is thrown. For example

{code}
SELECT myFunc(CAST(40.750444 AS FLOAT), CAST(-73.993475 AS FLOAT))

public class MyFunc extends ScalarFunction {

	public int eval(float lon, float lat) {
		// do something
	}
}
{code}

results in 

{code}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 5, Column 10: Cannot determine simple type name ""com""
{code}

The problem is probably caused by the expression reducer because it disappears if a regular attribute is added to a parameter expression."	FLINK	Closed	3	1	10269	pull-request-available
13217478	JSON row format is not serializable	{\{JsonRowSerializationSchema}} and \{{JsonRowDeserializationSchema}} use reference comparisons for more efficient execution, however, these do not work after serialization of the class when shipping it to a cluster. This error was not immediately visible as the logic in the else clause was also able to partially do the correct logic.	FLINK	Closed	1	1	10269	pull-request-available
13339692	Add documentation for FLIP-107	"Add documentation for FLIP-107:

- Connector/format metadata in general
- Kafka key/value and metadata
- Debezium metadata"	FLINK	Closed	2	7	10269	pull-request-available
13346121	Update TableResult.collect()/TableResult.print() to the new type system	Currently, TableResult.collect()/TableResult.print() use old sink interfaces and old type system. Those methods are very important in the API and should be updated.	FLINK	Closed	3	7	10269	pull-request-available
13172884	Build SQL jars with every build	Currently, the shaded fat jars for SQL are only built in the {{-Prelease}} profile. However, end-to-end tests require those jars and should also be able to test them. E.g. existing {{META-INF}} entry and proper shading. We should build them with every release. If a build should happen quicker one can use the {{-Pfast}} profile.	FLINK	Resolved	3	4	10269	pull-request-available
13329908	Deprecate old source and sink interfaces	Deprecate all interfaces and classes that are not necessary anymore with FLIP-95.	FLINK	Closed	3	7	10269	pull-request-available
13313053	Support List as a conversion class for ARRAY	"Currently we don't support {{List}} as a conversion class in the new type system. However, there are multiple reasons why we should support this conversion:
1) Hive uses {{List}} as the default for representing arrays e.g. in functions.
2) The new Expression DSL supports converting lists to array literals already.
3) The list interface is essential for Java users and part of many structured types.
4) We need to represent lists in {{ListView}} for aggregate functions."	FLINK	Closed	3	7	10269	pull-request-available
13367784	Add a SupportsSourceWatermark ability interface	"FLINK-21899 added a dedicated function that can be used in watermark definitions. Currently, the generated watermark strategy is invalid because of the exception that we throw in the function’s implementation. We should integrate this concept deeper into the interfaces instead of the need to implement some expression analyzing utility for every source.

We propose the following interface:

{code}
SupportsSourceWatermark {
  void applySourceWatermark()
}
{code}

 "	FLINK	Closed	3	7	10269	pull-request-available
13430117	CastFunctionITCase.testFunction failed on azure	"{code:java}
Feb 22 18:39:45 [ERROR] Tests run: 34, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 42.844 s <<< FAILURE! - in org.apache.flink.table.planner.functions.CastFunctionITCase
Feb 22 18:39:45 [ERROR] CastFunctionITCase.testFunction  Time elapsed: 0.214 s  <<< FAILURE!
Feb 22 18:39:45 java.lang.AssertionError: Failing test item: [SQL] CAST(f1 AS TIMESTAMP_LTZ(9))
Feb 22 18:39:45 	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.testFunction(BuiltInFunctionTestBase.java:116)
Feb 22 18:39:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 18:39:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 18:39:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 18:39:45 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 18:39:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 22 18:39:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 22 18:39:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 22 18:39:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 22 18:39:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Feb 22 18:39:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Feb 22 18:39:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Feb 22 18:39:45 	at org.junit.runners.Suite.runChild(Suite.java:128)
Feb 22 18:39:45 	at org.junit.runners.Suite.runChild(Suite.java:27)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Feb 22 18:39:45 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Feb 22 18:39:45 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Feb 22 18:39:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Feb 22 18:39:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Feb 22 18:39:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Feb 22 18:39:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32060&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10315"	FLINK	Closed	1	1	10269	test-stability
13427422	Link to OpenAPI specification is dead	"In the REST API docs, the link to the OpenAPI specification is missing the flink prefix, so it's leading nowhere.

https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/"	FLINK	Closed	3	11500	11245	pull-request-available
13385112	Add flink-rpc-akka module	"Add a new module containing all Akka-reliant classes.

As a first step we will just move classes, dependencies and build stuff to this module.
Loading this module through a separate classloader will be handled in a follow-up."	FLINK	Closed	3	7	11245	pull-request-available
13195788	TaskManagerProcessFailureBatchRecoveryITCase did not finish on time	"{code:java}
Failed tests: 
  TaskManagerProcessFailureBatchRecoveryITCase>AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure:207 The program did not finish in time
{code}

https://travis-ci.org/apache/flink/jobs/449439623"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13472644	Register Java 8 modules in all internal object mappers	"In FLINK-25588 we extended flink-shaded-jackson to also bundle the jackson extensions for handling Java 8 time / optional classes, but barely any of the internal object mappers were adjusted to register said module.

We can improve the user experience by always registering this module (in cases where users can provide a mapper), and solve some incompatibilities in others (like the JsonNodeDeserializationSchema).

 "	FLINK	Closed	3	4	11245	pull-request-available
13190338	Add stability test for the REST API	The the versioning scheme introduced in FLINK-7551 we should add a test that no API breaking changes occur within a given version.	FLINK	Closed	3	4	11245	pull-request-available
13288299	Remove mocking from DatadogHttpClientTest	"The {{DatadogHttpClientTest}} uses some nasty powermock features, like suppressing methods and mocking static methods.

There are simple workarounds for these that we should employ instead."	FLINK	Closed	3	4	11245	pull-request-available
13244297	Hive connector fails hadoop 2.4.1 builds	"The hive connector does not work with hadoop 2.4, but the tests are still run in the corresponding cron profile.

https://travis-ci.org/apache/flink/jobs/555723021

We should add a profile for skipping the hive tests that we enable for these profiles."	FLINK	Closed	3	1	11245	pull-request-available
13271442	Kinesis NOTICE is incorrect	"The entry for amazon-kinesis-producer still lists 0.12.9, when we now bundle 0.13.1; additionally this dependency is now apache licensed.
There also appear to be a few dependencies being bundled without being listed:

apache license:
com.amazonaws:aws-java-sdk-dynamodb:jar:1.11.272
com.amazonaws:aws-java-sdk-s3:jar:1.11.272
com.amazonaws:aws-java-sdk-kms:jar:1.11.272
amazon license:
com.amazonaws:dynamodb-streams-kinesis-adapter:jar:1.4.0 "	FLINK	Closed	1	4	11245	pull-request-available
13458566	Generalize utils around dependency-plugin	"We'll be adding another safeguard against developer mistakes which also parses the output of the dependency plugin, like the scala suffix checker.

We should generalize this parsing such that both checks can use the same code."	FLINK	Closed	3	7	11245	pull-request-available
13323727	Document JDBC drivers as source of Metaspace leaks	"Hi !

Im running a 1.11.1 flink cluster, where I execute batch jobs made with DataSet API.

I submit these jobs every day to calculate daily data.

In every execution, cluster's used metaspace increase by 7MB and its never released.

This ends up with an OutOfMemoryError caused by Metaspace every 15 days and i need to restart the cluster to clean the metaspace

taskmanager.memory.jvm-metaspace.size is set to 512mb

Any idea of what could be causing this metaspace grow and why is it not released ?

 

================================================
=== Summary ======================================
================================================

Case 1, reported by [~gestevez]:
* Flink 1.11.1
* Java 11
* Maximum Metaspace size set to 512mb
* Custom Batch job, submitted daily
* Requires restart every 15 days after an OOM

 Case 2, reported by [~Echo Lee]:
* Flink 1.11.0
* Java 11
* G1GC
* WordCount Batch job, submitted every second / every 5 minutes
* eventually fails TaskExecutor with OOM

Case 3, reported by [~DaDaShen]
* Flink 1.11.0
* Java 11
* WordCount Batch job, submitted every 5 seconds
* growing Metaspace, eventually OOM
 "	FLINK	Closed	3	1	11245	pull-request-available
13038044	Flink-python tests executing cost too long time	"When execute `mvn clean test` in flink-python, it will wait more than half hour after the console output below:
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.flink.python.api.PythonPlanBinderTest
log4j:WARN No appenders could be found for logger (org.apache.flink.python.api.PythonPlanBinderTest).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.



The stack below:
""main"" prio=5 tid=0x00007f8d7780b800 nid=0x1c03 waiting on condition [0x0000700009fd8000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.python.api.streaming.plan.PythonPlanStreamer.startPython(PythonPlanStreamer.java:70)
	at org.apache.flink.python.api.streaming.plan.PythonPlanStreamer.open(PythonPlanStreamer.java:50)
	at org.apache.flink.python.api.PythonPlanBinder.startPython(PythonPlanBinder.java:211)
	at org.apache.flink.python.api.PythonPlanBinder.runPlan(PythonPlanBinder.java:141)
	at org.apache.flink.python.api.PythonPlanBinder.main(PythonPlanBinder.java:114)
	at org.apache.flink.python.api.PythonPlanBinderTest.testProgram(PythonPlanBinderTest.java:83)
	at org.apache.flink.test.util.JavaProgramTestBase.testJobWithoutObjectReuse(JavaProgramTestBase.java:174)

this is the jstack:
https://gist.github.com/shijinkui/af47e8bc6c9f748336bf52efd3df94b0"	FLINK	Closed	2	1	11245	osx
13178793	Update migration tests for 1.6	Similar to FLINK-10084 we have to update the migration tests for 1.6.	FLINK	Closed	3	4	11245	pull-request-available
13244286	Hive connector does not compile on Java 9	"{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-hive_2.11: Compilation failure
[ERROR] /C:/Dev/Repos/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/batch/connectors/hive/FlinkStandaloneHiveRunner.java:[56,15] package sun.net.www is not visible
{code}"	FLINK	Closed	1	1	11245	pull-request-available
13413384	Bump netty to 4.1.70	But Netty to the latest version for general improvements and a proper for FLINK-24197.	FLINK	Closed	3	4	11245	pull-request-available, stale-assigned
13188334	flink-yarn-tests should depend flink-dist	"may be adding
{code:java}
<dependency>
 <groupId>org.apache.flink</groupId>
 <artifactId>flink-dist_${scala.binary.version}</artifactId>
 <version>${project.version}</version>
 <scope>test</scope>
 <type>pom</type>
</dependency>{code}
not really sure but it causes failure on my automate testing process, and by adding this dependency the error disappear. Even I wonder how it works currently on travis.

flink-yarn-test obviously depends on flink-dist since some tests try to find flink uberjar.

Please take a look for this. cc [~Zentol]"	FLINK	Closed	3	1	11245	pull-request-available
13286486	Remove testing logic from FlinkDistribution	"Currently, the {{FlinkDistribution}} is responsible for locating and copying the distribution for usage in tests, and contains hooks for backing up log files.
 This makes the class a bit inflexible to use, as for example a {{FlinkResource}} could not create 2 distributions in separate places.

I suggest to move these responsibilities into the {{FlinkResource}} implementations, and ""demote"" the distribution to a simple wrapper, only providing programmatic access for mutating/interacting with the distribution."	FLINK	Closed	3	4	11245	pull-request-available
13424697	Remove legacy high availability services	After FLINK-24038, we should consider removing the legacy high availability services {{ZooKeeperHaServices}} and {{KubernetesHaServices}} since they are now subsumed by the multiple component leader election service that only uses a single leader election per component.	FLINK	Closed	1	11500	11245	pull-request-available
13069171	Web UI TaskManager view: Rename 'Free Memory' to 'JVM Heap'	In the TaskManager view, the laben 'Free Memory' is wrong / misleading and should be 'JVM Heap Size' instead.	FLINK	Closed	3	7	11245	easyfix, starter
13539886	Ensure surefire baseLine is picked up by IntelliJ	We currently configure JVM arguments exclusively within the surefire executions, which IntelliJ doesn't read. We should also set the baseArgsLine (which in the future will contain module declarations) to the base surefire configuration.	FLINK	Closed	3	7	11245	pull-request-available
13285048	maven-dependency-plugin not fully compatible with Java 11	The maven-dependency-plugin 3.1.1 is not fully compatible with Java 11; dependency analysis and listing of dependencies is currently failing.	FLINK	Closed	3	4	11245	pull-request-available
13171453	JarListHandler does not close opened jars	"{code}
try {
	JarFile jar = new JarFile(f);
	Manifest manifest = jar.getManifest();
	String assemblerClass = null;

	if (manifest != null) {
		assemblerClass = manifest.getMainAttributes().getValue(PackagedProgram.MANIFEST_ATTRIBUTE_ASSEMBLER_CLASS);
		if (assemblerClass == null) {
			assemblerClass = manifest.getMainAttributes().getValue(PackagedProgram.MANIFEST_ATTRIBUTE_MAIN_CLASS);
		}
	}
	if (assemblerClass != null) {
		classes = assemblerClass.split("","");
	}
}
{code}"	FLINK	Closed	3	1	11245	pull-request-available
13358334	Remove RpcService#getExecutor	"{{RpcService#getExecutor}} exposes Akka's underlying dispatcher thread which introduces the risk of the thread being blocking by an IO operation of another component.

We should look into removing this method, and migrate existing usages to either a dedicated executor or explicitly passing the main thread executor if required."	FLINK	Closed	4	4	11245	auto-deprioritized-major
13482218	ResourceManagerTest#testProcessResourceRequirementsWhenRecoveryFinished prone to race condition	"The test incorrectly assumes that the {{declareRequiredResources}} has already been run when calling {{runInMainThread}}, while the RPC could still be in flight.
This can result in the test failing because within runInMainThread the test assumes that completing the readyToServeFuture will immediately result in the processing of resources, due to this workflow having been set up within delcareRequiredResources. Without it it will just fail because the completion of the future has in practice no effect."	FLINK	Closed	4	7	11245	pull-request-available
13116992	JDK 9 support	This is a JIRA to track all issues that found to make Flink compatible with Java 9.	FLINK	Closed	3	4	11245	pull-request-available
13366330	Check for out-dated slot allocation confirmations is insufficient	"The {{DeclarativeSlotManager}} accounts for the possibility of slot allocation confirmations coming in after the slot has already been freed fully allocated (e.g., through a SlotReport) by checking whether there is still a pending request for that slot.
However, this only works properly if the slot was allocated.

If the slot was freed in the mean time then it could've already been re-assigned to another job, which result in an IllegalStateException once the confirmation arrives.
It can also happen that the slot is reassigned to the same job, which can result in a premature configuratiom of the allocation."	FLINK	Closed	3	1	11245	pull-request-available
13348284	Version mismatch between spotless-maven-plugin and google-java-format plugin	"The spotless-maven-plugin uses version 1.7 of the google-java-format, while the IntelliJ google-java-format plugin uses 1.9, resulting in inconsistent formatting.

We cannot bump the version used by the spotless plugin because it requires java 11, so instead we have to downgrade the intellij plugin to 1.7.0.5 ."	FLINK	Closed	3	1	11245	pull-request-available
13433905	Migrate documentation build to Github Actions	"INFRA recently setup the required credentials to rsync content to nightlies.apache.org via github actions. This means we can migrate out buildbot setup to github actions instead.

This should make maintenance a lot easier, as we'd have more control over the environment. It'd also make it way easier to discover."	FLINK	Closed	3	11500	11245	pull-request-available
13563714	TestingCheckpointIDCounter can easily lead to NPEs	The TestingCheckpointIDCounter builder doesn't define safe defaults for all builder parameters. Using it can easily lead to surprising null pointer exceptions in tests when code is being modified to call more methods.	FLINK	In Progress	3	11500	11245	pull-request-available
13204239	FileUploadHandler stops working if the upload directory is removed	"A user has reported on the ML that the FileUploadHandler does not accept any files anymore if the upload directory was deleted after the cluster has been started.
A cursory glance at the code shows that it currently uses {{Files.createDirectory(...)}} to create a temporary directory for the current request to store uploaded files in.
Changing this to use {{Files.createDirectories(...)}} instead should prevent this from happening again."	FLINK	Closed	3	1	11245	pull-request-available
13442745	JDBC metaspace leak fix is misleading	"{code}
To ensure that these classes are only loaded once you should either add the driver jars to Flink’s lib/ folder, or add the driver classes to the list of parent-first loaded class via classloader.parent-first-patterns-additional.
{code}

This reads as if adding the driver to classloader.parent-first-patterns-additional can solve the issue in all cases, but this only works if the driver is already in lib/."	FLINK	Closed	3	1	11245	pull-request-available
13411809	AdaptiveSchedulerTest.testJobStatusListenerNotifiedOfJobStatusChanges unstable	"[https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1225&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20&l=7753]
 {code}
 2021-11-14T20:22:23.1142812Z Nov 14 20:22:23 [ERROR] Failures:
 2021-11-14T20:22:23.1149388Z Nov 14 20:22:23 [ERROR]   AdaptiveSchedulerTest.testJobStatusListenerNotifiedOfJobStatusChanges:684
 2021-11-14T20:22:23.1150058Z Nov 14 20:22:23 Expected: (a collection containing <RUNNING> and a collection containing <FINISHED>)
 2021-11-14T20:22:23.1150581Z Nov 14 20:22:23      but: a collection containing <FINISHED> was <RUNNING>
 2021-11-14T20:22:23.1152966Z Nov 14 20:22:23 [INFO]
 2021-11-14T20:22:23.1156414Z Nov 14 20:22:23 [ERROR] Tests run: 6048, Failures: 1, Errors: 0, Skipped: 97
{code}

Locally, it fails ~14 runs out of 100 (when running only testJobStatusListenerNotifiedOfJobStatusChanges in a loop).
Also on master.


It looks like job termination future is always completed before the jobStatusChangeListener is notified (AdaptiveScheduler.transitionToState, targetState.getState() completes the future).

Sleeping for 1ms before checking the assertion prevents the failure.

 

cc: [~trohrmann] "	FLINK	Closed	3	1	11245	test-stability
13399727	flink-s3-fs-base contains copied codes not listed in NOTICE file	{{com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser}} is copied from {{aws-java-sdk-s3}}.	FLINK	Closed	1	1	11245	legal, pull-request-available
13281165	Support Java 17 (LTS)	Long-term issue for preparing Flink for Java 17.	FLINK	Closed	3	2	11245	auto-deprioritized-major, pull-request-available, stale-assigned
13285090	Drop Elasticsearch 2.x connector	We should drop the ES 2.x connector as discussed here: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Drop-connectors-for-Elasticsearch-2-x-and-5-x-td37471.html	FLINK	Closed	3	4	11245	pull-request-available
13364975	Replace StreamTaskTestHarness#TestTaskMetricGroup	Reduce reliance on MetricGroup constructors by creating the metric groups through the usual API.	FLINK	Closed	3	7	11245	auto-unassigned, pull-request-available
13413158	StreamingWithStateTestBase does not compile on later Scala versions	"After upgrading Scala to 2.12.15 for testing purposes (because 2.12.7 does not work on Java 17), I got a compile error in the StreamingWithStateTestBase.

The {{subSequence}} method is no longer available for arrays."	FLINK	Closed	3	7	11245	pull-request-available
13454543	Improve LicenseChecker output	The license checker output is difficult to parse for people who aren't too familiar with it. They just get bombarded with 200 log lines, that are too long, with too much redundant information, with no quick way to identify whether they are relevant for a particular change (e.g., module) without any guidance on whether something is critical or not.	FLINK	Closed	3	11500	11245	pull-request-available
13253040	Update Execution Plan docs	"The *Execution Plans* section is totally outdated and refers to the old {{tools/planVisalizer.html}} file that has been removed for two years.

https://ci.apache.org/projects/flink/flink-docs-master/dev/execution_plans.html"	FLINK	Closed	4	1	11245	pull-request-available
13348342	Cleanup checkstyle suppressions files	With spotless formatting having landed this is a good opportunity to remove or narrow down various checkstyle suppressions.	FLINK	Closed	3	4	11245	pull-request-available
13334465	JMXReporterFactoryTest fails on Azure	"The following build failed

[https://dev.azure.com/khachatryanroman/810e80cc-0656-4d3c-9d8c-186764456a01/_apis/build/builds/289/logs/106]

{code}
 [ERROR] testPortRangeArgument(org.apache.flink.metrics.jmx.JMXReporterFactoryTest)  Time elapsed: 0.02 s  <<< FAILURE!
 java.lang.AssertionError:
 
 Expected: (a value equal to or greater than <9000> and a value less than or equal to <9010>)
      but: a value less than or equal to <9010> <9040> was greater than <9010>
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:956)
    at org.junit.Assert.assertThat(Assert.java:923)
    at org.apache.flink.metrics.jmx.JMXReporterFactoryTest.testPortRangeArgument(JMXReporterFactoryTest.java:46)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}

 

I see the following problems in the code:

- tests in JMXReporterFactoryTest assumes JMXService.jmxServer wasn't started or was stopped

- JMXService.jmxServer is not volatile

 cc: [~chesnay], [~rongr]"	FLINK	Closed	1	1	11245	test-stability
13180902	Bump mockito to 2.0+	"Mockito only properly supports java 9 with version 2. We have to bump the dependency and fix various API incompatibilities.

Additionally we could investigate whether we still need powermock after bumping the dependency (which we'd also have to bump otherwise)."	FLINK	Closed	3	7	11245	pull-request-available
13162233	Compatibility table not up-to-date	The compatibility table https://ci.apache.org/projects/flink/flink-docs-master/ops/upgrading.html has not been updated since 1.3.x.	FLINK	Closed	3	4	11245	pull-request-available
13202798	NoClassDefFoundError in presto-s3 filesystem	"A user has reporter an issue on the ML where using the presto-s3 filesystem fails with an exception due to a missing class. The missing class is indeed filtered out in the shade-plugin configuration.
{code:java}
java.lang.NoClassDefFoundError: org/apache/flink/fs/s3presto/shaded/com/facebook/presto/hadoop/HadoopFileStatus
	at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.directory(PrestoS3FileSystem.java:446)
	at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.delete(PrestoS3FileSystem.java:423)
	at org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.delete(HadoopFileSystem.java:147)
	at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:80)
	at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.doDiscard(CompletedCheckpoint.java:250)
	at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:219)
	at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:72)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:844)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:756)
	at org.apache.flink.runtime.jobmaster.JobMaster.lambda$acknowledgeCheckpoint$8(JobMaster.java:680)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107){code}"	FLINK	Resolved	1	1	11245	pull-request-available
13304580	Migrate e2e tests to flink-docker	"Some end-to-end tests make use of flink-container/docker.
We need to migrate them to flink-docker."	FLINK	Closed	3	7	11245	pull-request-available
13069173	Web UI Subtasks view for TaskManagers has a misleading name	The register for the subtasks grouped by TaskManager is simply called {{TaskManager}}, which is confusing users. I suggest to rename it to {{Subtasks by TaskManager}}.	FLINK	Closed	3	7	11245	easyfix, starter
13403068	Add a fallback AkkaRpcSystemLoader for tests in the IDE	"The new RPC loading mechanism requires a some steps to be run in maven. This breaks certain workflows, like when doing a full rebuild within IntelliJ.

We can alleviate this issue by adding a testing implementation of the AkkaRpcSystemLoader specifically for this case."	FLINK	Closed	3	11500	11245	pull-request-available
13397546	flink-dist NOTICE not properly checked by NoticeFileChecker	"com.github.scopt:scopt_2.11:3.5.0 is still bundled by flink-dist because of the scala-shell, but is not mentioned in the NOTICE file.

We should add it, and check why the notice check did not catch it."	FLINK	Closed	1	1	11245	pull-request-available
13240405	Remove legacy flink-python APIs	As discussed on the [mailing list|http://mail-archives.apache.org/mod_mbox/flink-user/201906.mbox/%3cCANC1h_uSoBi0nG1wL-4EATBSU_h2t46g=b9i8tEUSMxrMXRoBw@mail.gmail.com%3e], remove the older batch&streaming python API.	FLINK	Closed	3	4	11245	pull-request-available
13174368	Update scm developerConnection	The developer connection must be updated to point to the update remote.	FLINK	Closed	3	7	11245	pull-request-available
13595062	FlinkSecurityManager#checkExit StackOverFlow if haltOnSystemExit is enabled	"The halt() call in checkExit() will again cause checkExit() to be called since we don't null the security manager, in contrast to how forceProcessExit is implemented.
This is only an issue during the process startup, as at later times we typically initiate the shutdown for forceProcessExit."	FLINK	Closed	4	1	11245	pull-request-available
13440941	Link to Apache privacy policy	We've been instructed to drop our custom privacy policy and only link to the Apache one.	FLINK	Closed	3	11500	11245	pull-request-available
13493981	Clear static Jackson TypeFactory cache on CL release	"The Jackson TypeFactory contains a singleton instance that is at times used by Jackson, potentially containing user-classes for longer than necessary.

https://github.com/FasterXML/jackson-databind/issues/1363

We could clear this cache whenever a user code CL is being released similar to what was done in BEAM-6460."	FLINK	Closed	3	4	11245	pull-request-available
13142826	Accumulators not updated for running jobs	"The FLIP-6 {{TaskExecutor}} is never sending the current state of accumulators to the JobMaster. They are only updated if the job is finished.

The legacy TaskManager did this regularly as part of the heartbeat to the JobManager.

This is a regression and blocks the porting of some tests (like the {{SavepointMigrationTestBase}}) that makes use of accumulators to determine when the job shutdown condition is fulfilled."	FLINK	Resolved	1	1	11245	flip-6
13474026	Deprecate host/web-ui-port parameter of jobmanager.sh	"If we fix FLINK-28733 we could while we're at it deprecate these 2 parameters, since you can then also control them via dynamic properties.

This would also subsume FLINK-21038."	FLINK	Closed	4	4	11245	pull-request-available
13248296	AbstractServerTest failed on Travis	"Likely just a port conflict (the range used in the test only covers 3 ports)
{code:java}
09:21:38.371 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.091 s <<< FAILURE! - in org.apache.flink.queryablestate.network.AbstractServerTest
09:21:38.371 [ERROR] testPortRangeSuccess(org.apache.flink.queryablestate.network.AbstractServerTest)  Time elapsed: 0.062 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<1>
	at org.apache.flink.queryablestate.network.AbstractServerTest.testPortRangeSuccess(AbstractServerTest.java:125){code}"	FLINK	Closed	3	1	11245	pull-request-available
13315450	Add release script for creating snapshot branches	The creation of release branches (e.g., release-1.11) is still an entirely manual step, both involving the creation of the actual git branch and updating the documentation/python/japicmp configuration.	FLINK	Closed	3	4	11245	pull-request-available
13427393	Rework loader-bundle into separate module	"The flink-table-planner currently creates 2 artifacts. 1 jar containing the planner and various dependencies for the cases where the planner is used directly, and another jar that additionally bundles scala for cases where the loader is used.

The latter artifact is purely an intermediate build artifact, and as such we usually wouldn't want to publish it. This is particularly important because this jar doesn't have a correct NOTICE, and having different NOTICE files for different artifacts is surprisingly tricky.

We should just rework this into a separate module."	FLINK	Closed	3	11500	11245	pull-request-available
13413155	LookupJoinITCase fails on Java 17	"The UserDefinedFunctionHelper validates that a given function is public.

The {{InMemory[Async]LookupFunction}} classes are not public however. Probably some Java<->Scala interplay that causes this to not be detected at this time."	FLINK	Closed	3	7	11245	pull-request-available
13413408	FlinkSecurityManager can swallow exception	If the security manager cannot be set then FlinkSecurityManager#setFromConfiguration swallows the exception, because it is passed to String.format instead of the IllegalConfigurationException constructor.	FLINK	Closed	3	1	11245	pull-request-available
13410076	Move JobStatus-related metrics out of ExecutionGraph	Setup metrics such as restartingTime outside of the ExecutionGraph. This is a trivla change for the DefaultScheduler; the AdaptiveScheduler needs a new data-structure for storing the state timestamps.	FLINK	Closed	3	7	11245	pull-request-available
13357754	Relax test naming constraints	"Issues like FLINK-21337 or FLINK-21031 show that accidents happen where tests were added with incorrect names, causing them to not be run on CI, potentially hiding regressions.

[~rmetzger] had the neat idea to relax the constrainst such that in the verify phase we just run everything that is not end on {{Test.java}}.

The subtasks are tests that are currently not being run, and potentially broken."	FLINK	Closed	3	4	11245	pull-request-available
13211595	Upgrade Akka to 2.5	"{noformat}
2019-01-24 14:43:52,059 ERROR akka.remote.Remoting                                          - class [B cannot be cast to class [C ([B and [C are in module java.base of loader 'bootstrap')
java.lang.ClassCastException: class [B cannot be cast to class [C ([B and [C are in module java.base of loader 'bootstrap')
        at akka.remote.artery.FastHash$.ofString(LruBoundedCache.scala:18)
        at akka.remote.serialization.ActorRefResolveCache.hash(ActorRefResolveCache.scala:61)
        at akka.remote.serialization.ActorRefResolveCache.hash(ActorRefResolveCache.scala:55)
        at akka.remote.artery.LruBoundedCache.getOrCompute(LruBoundedCache.scala:110)
        at akka.remote.RemoteActorRefProvider.resolveActorRef(RemoteActorRefProvider.scala:403)
        at akka.actor.SerializedActorRef.readResolve(ActorRef.scala:433)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at java.base/java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1250)
        at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2096)
        at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1594)Running a jobmanager with java 11 fail with the following call stack:
{noformat}

Flink master is using akka 2.4.20.

After some investigation, the error in akka comes from the following line:
{code}
def ofString(s: String): Int = {
val chars = Unsafe.instance.getObject(s, EnvelopeBuffer.StringValueFieldOffset).asInstanceOf[Array[Char]]
{code}

from java 9 it is now an array of byte. The akka code in the newer version is:
{code}
    public static int fastHash(String str) {
          ...
        if (isJavaVersion9Plus) {
            final byte[] chars = (byte[]) instance.getObject(str, stringValueFieldOffset);
            ...
        } else {
            final char[] chars = (char[]) instance.getObject(str, stringValueFieldOffset);
 {code}"	FLINK	Closed	4	7	11245	pull-request-available
13196796	Hcatalog modules needs scala suffix	The {{hcatalog}} connector has a compile dependency on {{flink-hadoop-compatibility}} which has a scala suffix, and thus also requires a suffix.	FLINK	Closed	1	1	11245	pull-request-available
13267096	Add total number of partitions to ResultPartitionDeploymentDescriptor	"The ResourceManager requires some way to tell whether it has received all partitions for a given dataset; to ensure that a) no incomplete datasets are shows to the user [as completed] and b) to initiate a timeout-based cleanup of other partitions.

Conversely, since the RM only has access to what the TaskExecutor sends, this information must also be accessible to the TE.

A good candidate for placing this seems to be the {{ResultPartitionDeploymentDescriptor}} or {{PartitionDescriptor}}, as these already contain the DataSetID."	FLINK	Closed	3	7	11245	pull-request-available
13268341	Process ClusterPartitionReport on ResourceManager	Setup a data-structure for storing the cluster partition report that the ResourceManager receives via TaskExecutor heartbeats.	FLINK	Closed	3	7	11245	pull-request-available
13192175	Remove unused deploysettings.xml	With FLINK-9967 resolved the {{deploysettings.xml}} file is unused and can be removed.	FLINK	Closed	3	4	11245	pull-request-available
13201297	Rework creation of sql-client connector/format jars	"For the SQL client we currently have a separate {{sql-jars}} profile in various connectors/formats that create an additional fat jar with a separate classifier.

One of the core maven mantras is ""One artifact per module."", and we see the importance of this mantra as our current packaging strategy makes it impossible to provide different NOTICE files for the created jars (regular and sql-jar).

Currently we would be forced to provide the same file for both jars, which will cause problems for any downstream users that wants to handle NOTICE files properly. We would cause the same issue we had with netty, which categorically claims to be bundling dependencies although it doesn't, forcing us to manually cut out the valid parts.

My proposal is to move custom packaging strategies into their own module that depend on the original module.

 I will use {{flink-connector-elasticsearch6}} as an example, which packages both a regular jar without any included dependencies, and a sql jar bundling everything.
 * create a separate {{flink-sql-connector-elasticsearch6/}}{{flink-connector-elasticsearch6-uber}}{{}} module
 * this module depends on {{flink-connector-elasticsearch6}}, and bundles all dependencies
 * move the current shading logic for the sql jar out of the {{sql-jars}} profile{{}}
 * add a {{sql-jars}} profile to {{flink-connectors}} for skipping the creation of these jars"	FLINK	Closed	3	4	11245	pull-request-available
13533807	Skip ClosureCleaner if object can be serialized	"Given an object the ClosureCleaner currently recursively digs into every non-static/transient field of the given object. This causes a problem on Java 17 because these reflective accesses all need to be explicitly allowed beforehand.

Instead, we could limit the CC to objects that fail serialization, because if something can be serialized there isn't anything for the CC to do.
This should allow us to avoid a lot of unnecessary reflection accesses to immutable JDK classes, like Strings/BigDecimals etc etc."	FLINK	Closed	3	7	11245	pull-request-available
13307733	Test memory configuration of Flink cluster	Make sure that Flink processes (in particular Master processes) fail with a meaningful exception message if they exceed the configured memory budgets.	FLINK	Closed	2	7	11245	release-testing
13409831	Remove CheckpointStatsTracker#getJobCheckpointingConfiguration	"The CheckpointStatsTracker is currently being used by the EG as some form of container for the checkpoint coordinator configuration.
We can just store this configuration in the EG, further decoupling the tracker from the EG."	FLINK	Closed	3	7	11245	pull-request-available
13437327	CheckpointCoordinatorTest#testMinCheckpointPause	"The test triggers checkpoints, waits for the CC to have stored a pending checkpoint, and then sends an acknowledge.
The acknowledge can fail with an NPE because the PendingCheckpoint#checkpointTargetLocation hasn't been set yet. This doesn't happen synchronously with the PendingCheckpoint being added to CheckpointCoordinator#pendingCheckpoints.

{code}
Apr 01 19:57:36 [ERROR] org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTest.testMinCheckpointPause  Time elapsed: 0.012 s  <<< ERROR!
Apr 01 19:57:36 org.apache.flink.runtime.checkpoint.CheckpointException: Could not finalize the pending checkpoint 1. Failure reason: Failure to finalize checkpoint.
Apr 01 19:57:36 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1354)
Apr 01 19:57:36 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1241)
Apr 01 19:57:36 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
...
Apr 01 19:57:36 Caused by: java.lang.NullPointerException
Apr 01 19:57:36 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.finalizeCheckpoint(PendingCheckpoint.java:327)
Apr 01 19:57:36 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1337)
Apr 01 19:57:36 	... 50 more
{code}"	FLINK	Closed	3	11500	11245	pull-request-available
13517436	AdaptiveSchedulerTest.testStatusMetrics is not stable	"We experience a test instability in {{AdaptiveSchedulerTest.testStatusMetrics}}.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44635&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8475"	FLINK	Closed	2	11500	11245	pull-request-available, test-stability
13440658	Remove unnecessary forkCount settings	Several modules configure an explicit forkCount of 1. As far as I can tell this is simply unnecessary.	FLINK	Closed	3	11500	11245	pull-request-available
13167135	Move jar/artifact upload logic out of JobGraph	"The {{JobGraph}} offers utility methods for uploading jars and artifacts to the BlobService.

However, how these files are uploaded isn't a concern of the {{JobGraph}} but the submission-method, like the {{RestClusterClient}}.

These methods should be moved into a utility class."	FLINK	Closed	3	4	11245	pull-request-available
13299891	Disable merge commit button	"Make use of the [.asf.yaml|https://cwiki.apache.org/confluence/display/INFRA/.asf.yaml+features+for+git+repositories] feature to disable the GitHub merge commit button.

Ideally we just drop this into all repos for consistency."	FLINK	Closed	3	4	11245	pull-request-available
13448742	Savepoint status cannot be queried from standby jobmanager	"The savepoint status handler currently doesn't work on standby dispatchers because the OperationResult isn't serializable.

This wasn't caught by the recently added serialization safeguards, as those only covered the caller side (i.e., arguments passed to callee), but not the return value. "	FLINK	Closed	3	1	11245	pull-request-available
13286225	Harden BlobServerRangeTest	"The {{BlobServerRangeTest}} uses the know anti-pattern of allocating a free port, freeing it, and then passing it into another component, with the expectation that it will start on that port.

This can lead to failures if the port is allocated again in the meantime by another process."	FLINK	Closed	3	4	11245	pull-request-available
13171359	flink-dist is missing dependency on flink-examples	"For the assembly of {{flink-dist}} we copy various batch/streaming examples directly from the respective /target directory.
Never mind that this is already a problem as is (see FLINK-9582), {{flink-dist}} defines no dependency on these modules.
If you were to only compile {{flink-dist}} with the {{-am}} flag (to also build all dependencies) it thus _may_ or _may not_ happen that these modules are actually compiled, which could cause these examples to not be included in the final assembly."	FLINK	Closed	3	4	11245	pull-request-available
13438384	Tests leak executor services	A number of tests created executor services but don't shut them down properly. Others could be improved with try-catch blocks or TestExecutorResource.	FLINK	Closed	3	11500	11245	pull-request-available
13376824	Jira bot does not set the status to Open if it unassigns people from tickets	The Jira bot does not set the status to {{Open}} if it unassigns people from a ticket.	FLINK	Closed	3	1	11245	pull-request-available
