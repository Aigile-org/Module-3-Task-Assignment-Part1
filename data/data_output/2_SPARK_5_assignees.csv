id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13573139	Move `o.a.s.variant` to `o.a.s.types.variant`	"{code}
-package org.apache.spark.variant;
+package org.apache.spark.types.variant;
{code}
 
{code:java}
-package org.apache.spark.sql.catalyst.expressions
+package org.apache.spark.sql.catalyst.expressions.variant
{code}
 "	SPARK	Resolved	3	7	3855	pull-request-available
13593433	Increase `spark.test.docker.connectionTimeout` to 10min	"**MASTER** branch
https://github.com/apache/spark/actions/runs/11045311764/job/30682732260

{code}
[info] OracleIntegrationSuite:
[info] org.apache.spark.sql.jdbc.OracleIntegrationSuite *** ABORTED *** (5 minutes, 17 seconds)
[info]   The code passed to eventually never returned normally. Attempted 298 times over 5.0045005511500005 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.41 port 41079. (CONNECTION_ID=n9ZWIh+nQn+G9fkwKyoBQA==)
{code}

**branch-3.5**
https://github.com/apache/spark/actions/runs/10939696926/job/30370552237

{code}
[info] MsSqlServerNamespaceSuite:
[info] org.apache.spark.sql.jdbc.v2.MsSqlServerNamespaceSuite *** ABORTED *** (5 minutes, 42 seconds)
[info]   The code passed to eventually never returned normally. Attempted 11 times over 5.487631282400001 minutes. Last failure message: The TCP/IP connection to the host 10.1.0.56, port 35345 has failed. Error: ""Connection refused (Connection refused). Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall."".. (DockerJDBCIntegrationSuite.scala:166)
{code}

**branch-3.4**
https://github.com/apache/spark/actions/runs/10937842509/job/30364658576

{code}
[info] MsSqlServerNamespaceSuite:
[info] org.apache.spark.sql.jdbc.v2.MsSqlServerNamespaceSuite *** ABORTED *** (5 minutes, 42 seconds)
[info]   The code passed to eventually never returned normally. Attempted 11 times over 5.487555645633333 minutes. Last failure message: The TCP/IP connection to the host 10.1.0.153, port 46153 has failed. Error: ""Connection refused (Connection refused). Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall."".. (DockerJDBCIntegrationSuite.scala:166)
{code}"	SPARK	Resolved	4	7	3855	pull-request-available
13558528	Fix `pyspark.pandas.tests.computation.test_apply_func` in Python 3.11	"https://github.com/apache/spark/actions/runs/6914662405/job/18812759697

{code}
======================================================================
ERROR [0.686s]: test_apply_batch_with_type (pyspark.pandas.tests.computation.test_apply_func.FrameApplyFunctionTests.test_apply_batch_with_type)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/pandas/tests/computation/test_apply_func.py"", line 248, in test_apply_batch_with_type
    def identify3(x) -> ps.DataFrame[float, [int, List[int]]]:
                        ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/pandas/frame.py"", line 13540, in __class_getitem__
    return create_tuple_for_frame_type(params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/pandas/typedef/typehints.py"", line 721, in create_tuple_for_frame_type
    return Tuple[_to_type_holders(params)]
                 ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/pandas/typedef/typehints.py"", line 766, in _to_type_holders
    data_types = _new_type_holders(data_types, NameTypeHolder)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/pandas/typedef/typehints.py"", line 832, in _new_type_holders
    raise TypeError(
TypeError: Type hints should be specified as one of:
  - DataFrame[type, type, ...]
  - DataFrame[name: type, name: type, ...]
  - DataFrame[dtypes instance]
  - DataFrame[zip(names, types)]
  - DataFrame[index_type, [type, ...]]
  - DataFrame[(index_name, index_type), [(name, type), ...]]
  - DataFrame[dtype instance, dtypes instance]
  - DataFrame[(index_name, index_type), zip(names, types)]
  - DataFrame[[index_type, ...], [type, ...]]
  - DataFrame[[(index_name, index_type), ...], [(name, type), ...]]
  - DataFrame[dtypes instance, dtypes instance]
  - DataFrame[zip(index_names, index_types), zip(names, types)]
However, got (<class 'int'>, typing.List[int]).

----------------------------------------------------------------------
Ran 10 tests in 34.327s

FAILED (errors=1)
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13572284	Make `BlockManager` warn before `removeBlockInternal`	"{code}
24/03/18 18:40:46 WARN BlockManager: Putting block broadcast_0 failed due to exception java.nio.file.NoSuchFileException: /data/spark/blockmgr-56a6c418-90be-4d89-9707-ef45f7eaf74c/0e.
24/03/18 18:40:46 WARN BlockManager: Block broadcast_0 was not removed normally.
24/03/18 18:40:46 INFO TaskSchedulerImpl: Cancelling stage 0
24/03/18 18:40:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
24/03/18 18:40:46 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) failed in 0.264 s due to Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: /data/spark/blockmgr-56a6c418-90be-4d89-9707-ef45f7eaf74c/0e
java.nio.file.NoSuchFileException: /data/spark/blockmgr-56a6c418-90be-4d89-9707-ef45f7eaf74c/0e
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13588209	Use the latest PMD 6.x rules instead of the deprecated ones	"There are too many warnings on PMD like the following.
{code:java}
Use Rule name category/java/errorprone.xml/BrokenNullCheck instead of the deprecated Rule name rulesets/java/basic.xml/BrokenNullCheck. PMD 7.0.0 will remove support for this deprecated Rule name usage.
Use Rule name category/java/errorprone.xml/CheckSkipResult instead of the deprecated Rule name rulesets/java/basic.xml/CheckSkipResult. PMD 7.0.0 will remove support for this deprecated Rule name usage. {code}
 
{code:java}
$ ./gradlew clean build | grep PMD | wc -l
     204 {code}
 

We had better fix it before we do the release.

 

 "	SPARK	Resolved	4	7	3855	pull-request-available
13590919	Add `jjwt` profile	This issue aims to add a new profile `jjwt` to provide `jjwt-impl` and `jjwt-jackson` jars files in a Spark distribution	SPARK	Resolved	3	7	3855	pull-request-available
13573772	Use Java 21 instead of 21-jre in K8s Dockerfile	"{code}
$ docker run -it --rm azul/zulu-openjdk:21-jre jmap
docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""jmap"": executable file not found in $PATH: unknown.
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13570075	Fix `core` module to succeed SBT tests	"This happens at branch-3.4 only. branch-3.3/branch-3.5/master are okay.
{code:java}
$ build/sbt ""core/testOnly *.DAGSchedulerSuite""
[info] DAGSchedulerSuite:
[info] - [SPARK-3353] parent stage should have lower stage id *** FAILED *** (439 milliseconds)
[info]   java.lang.IllegalStateException: Could not initialize plugin: interface org.mockito.plugins.MockMaker (alternate: null)
...
[info] *** 1 SUITE ABORTED ***
[info] *** 118 TESTS FAILED ***
[error] Error during tests:
[error] 	org.apache.spark.scheduler.DAGSchedulerSuite
[error] (core / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 48 s, completed Feb 27, 2024, 1:26:27 PM {code}
 

MAVEN
{code:java}
$ build/mvn dependency:tree -pl core | grep byte-buddy
...
[INFO] |  +- net.bytebuddy:byte-buddy:jar:1.12.10:test
[INFO] |  +- net.bytebuddy:byte-buddy-agent:jar:1.12.10:test
{code}
SBT
{code:java}
$ build/sbt ""core/test:dependencyTree"" | grep byte-buddy
[info]   | | | | +-net.bytebuddy:byte-buddy:1.12.10 (evicted by: 1.12.18)
[info]   | | | | +-net.bytebuddy:byte-buddy:1.12.18
{code}"	SPARK	Resolved	3	1	3855	pull-request-available
13560333	Improve `FileSystemPersistenceEngine` to allow non-exist parents	"To prevent the following
{code}
info]   java.io.IOException: No such file or directory
[info]   at java.base/java.io.UnixFileSystem.createFileExclusively(Native Method)
[info]   at java.base/java.io.File.createNewFile(File.java:1043)
[info]   at org.apache.spark.deploy.master.FileSystemPersistenceEngine.serializeIntoFile(FileSystemPersistenceEngine.scala:62)
[info]   at org.apache.spark.deploy.master.FileSystemPersistenceEngine.persist(FileSystemPersistenceEngine.scala:45)
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13558682	Add `AmmoniteTest` test tag and mark `ReplE2ESuite`	- https://github.com/com-lihaoyi/Ammonite/issues/276	SPARK	Resolved	3	6	3855	pull-request-available
13559120	Install `six==1.16.0` explicitly for `pandas` in Python 3.12	"This happens in Python 3.12 CI only.
- https://github.com/apache/spark/actions/runs/6959106836/job/18935673389
{code}
Starting test(python3.12): pyspark.streaming.tests.test_context (temp output: /__w/spark/spark/python/target/73ed28d0-ae18-426e-9760-d03bea982a9b/python3.12__pyspark.streaming.tests.test_context__l4z6a7a2.log)
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""/__w/spark/spark/python/pyspark/streaming/tests/test_context.py"", line 23, in <module>
    from pyspark.testing.streamingutils import PySparkStreamingTestCase
  File ""/__w/spark/spark/python/pyspark/testing/__init__.py"", line 19, in <module>
    from pyspark.testing.pandasutils import assertPandasOnSparkEqual
  File ""/__w/spark/spark/python/pyspark/testing/pandasutils.py"", line 58, in <module>
    import pyspark.pandas as ps
  File ""/__w/spark/spark/python/pyspark/pandas/__init__.py"", line 33, in <module>
    require_minimum_pandas_version()
  File ""/__w/spark/spark/python/pyspark/sql/pandas/utils.py"", line 27, in require_minimum_pandas_version
    import pandas
  File ""/usr/local/lib/python3.12/dist-packages/pandas/__init__.py"", line 46, in <module>
    from pandas.core.api import (
  File ""/usr/local/lib/python3.12/dist-packages/pandas/core/api.py"", line 1, in <module>
    from pandas._libs import (
  File ""/usr/local/lib/python3.12/dist-packages/pandas/_libs/__init__.py"", line 18, in <module>
    from pandas._libs.interval import Interval
  File ""interval.pyx"", line 1, in init pandas._libs.interval
  File ""hashtable.pyx"", line 1, in init pandas._libs.hashtable
  File ""missing.pyx"", line 42, in init pandas._libs.missing
AttributeError: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13543659	Use `org.seleniumhq.selenium.htmlunit3-driver` instead of `net.sourceforge.htmlunit`	[CVE-2023-26119|https://nvd.nist.gov/vuln/detail/CVE-2023-26119]	SPARK	Resolved	3	7	3855	pull-request-available
13586586	Avoid unnecessary task configuration in `spark-operator-api`	[https://docs.gradle.org/current/userguide/task_configuration_avoidance.html]	SPARK	Resolved	4	7	3855	pull-request-available
13540803	Upgrade ORC to 2.0.0	"Apache ORC community has the following release cycles which are synchronized with Apache Spark releases.
 * ORC v2.0.0 (next year) for Apache Spark 4.0.x
 * ORC v1.9.0 (this month) for Apache Spark 3.5.x
 * ORC v1.8.x for Apache Spark 3.4.x
 * ORC v1.7.x for Apache Spark 3.3.x
 * ORC v1.6.x for Apache Spark 3.2.x"	SPARK	Resolved	3	7	3855	pull-request-available
13556416	Upgrade `Volcano` to 1.8.1	"To bring the latest feature and bug fixes in addition to the test coverage for Volcano scheduler 1.8.1.

[https://github.com/volcano-sh/volcano/releases/tag/v1.8.1]

 

[https://github.com/volcano-sh/volcano/pull/3101 |https://github.com/volcano-sh/volcano/pull/3101](volcano adapt k8s v1.27 volcano-sh/volcano#3101)"	SPARK	Resolved	3	7	3855	pull-request-available
13586412	Upgrade Gradle to 8.9	"Gradle 8.9 is released.
- https://github.com/gradle/gradle/releases/tag/v8.9.0"	SPARK	Resolved	4	7	3855	pull-request-available
13139131	Extend test coverage to all ORC readers	"We have five ORC readers. We had better have a test coverage for all ORC readers.

- Hive Serde
- Hive OrcFileFormat
- Apache ORC Vectorized Wrapper
- Apache ORC Vectorized Copy
- Apache ORC MR
"	SPARK	Resolved	4	4	3855	bulk-closed
12968564	Add pivot functionality to SparkR	"R users are very used to transforming data using functions such as dcast (pkg:reshape2). https://github.com/apache/spark/pull/7841 introduces such functionality to Scala and Python APIs. I'd like to suggest adding this functionality into SparkR API to pivot DataFrames.
I'd love to to this, however, my knowledge of Scala is still limited, but with a proper guidance I can give it a try."	SPARK	Resolved	4	4	3855	pivot
13597425	Use Spark 3.4.4 instead of 3.0.1 in `test_install_spark`	"[https://github.com/apache/spark/actions/runs/11623974780/job/32371883850]

 
{code:java}
urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>
ERROR
test_package_name (pyspark.tests.test_install_spark.SparkInstallationTestCase) ... Trying to download Spark spark-3.0.1 from [https://dlcdn.apache.org/, https://archive.apache.org/dist, https://dist.apache.org/repos/dist/release]
Downloading spark-3.0.1 for Hadoop hadoop3.2 from:
- https://dlcdn.apache.org//spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz
Failed to download spark-3.0.1 for Hadoop hadoop3.2 from https://dlcdn.apache.org//spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz:
Downloading spark-3.0.1 for Hadoop hadoop3.2 from:
- https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz
Failed to download spark-3.0.1 for Hadoop hadoop3.2 from https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz:
Downloading spark-3.0.1 for Hadoop hadoop3.2 from:
- https://dist.apache.org/repos/dist/release/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz
Failed to download spark-3.0.1 for Hadoop hadoop3.2 from https://dist.apache.org/repos/dist/release/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz:
ok {code}"	SPARK	Resolved	4	7	3855	pull-request-available
13589184	Use API Group `spark.apache.org`	"K8s convention follows domain name styles instead of package name styles.
{code}
-apiVersion: org.apache.spark/...
+apiVersion: spark.apache.org/...
{code}"	SPARK	Resolved	2	7	3855	pull-request-available
13568804	Fix HistoryServerSuite.`incomplete apps get refreshed` test to start with empty storeDir	"This has been observed multiple times.
{code:java}
[info] - incomplete apps get refreshed *** FAILED *** (15 seconds, 450 milliseconds)
[info]   The code passed to eventually never returned normally. Attempted 43 times over 10.22918722 seconds. Last failure message: 0 did not equal 4. (HistoryServerSuite.scala:564) {code}"	SPARK	Resolved	3	7	3855	pull-request-available
13384045	Support Spark on Apple Silicon on macOS natively on Java 17	This is an umbrella JIRA tracking the progress of supporting Apple Silicon on macOS natively.	SPARK	Resolved	3	2	3855	release-notes
13587934	Fix `docker-image-tool.sh` to be up-to-date	"Apache Spark 4 dropped Java 11 support. So, we should fix the following.
{code}
-  - Build and push Java11-based image with tag ""v3.4.0"" to docker.io/myrepo
+  - Build and push Java17-based image with tag ""v4.0.0"" to docker.io/myrepo
{code}

Apache Spark 4 requires JDK instead of JRE. So, we should fix the following.
{code}
-    $0 -r docker.io/myrepo -t v3.4.0 -b java_image_tag=11-jre build
+    $0 -r docker.io/myrepo -t v4.0.0 -b java_image_tag=17 build
{code}

Lastly, `3.4.0` is too old because it's released on April 13, 2023. We had better use v4.0.0.
{code}
-    $0 -r docker.io/myrepo -t v3.4.0 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
+    $0 -r docker.io/myrepo -t v4.0.0 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13550775	Support `spark.deploy.maxDrivers`	Like `spark.mesos.maxDrivers`, this issue aims to add `spark.deploy.maxDrivers`.	SPARK	Resolved	3	7	3855	pull-request-available
13548259	Upgrade Ivy to 2.5.2	[CVE-2022-46751|https://www.cve.org/CVERecord?id=CVE-2022-46751]	SPARK	Resolved	3	7	3855	pull-request-available
13544252	Use the latest minikube in K8s IT	"*BEFORE*

* Minikube: v1.30.1
* K8s: v1.26.3

*AFTER*

* Minikube: v1.32.0
* K8s: v1.28.3"	SPARK	Resolved	3	7	3855	pull-request-available
13557406	Revisit and Improve Spark Standalone Cluster	"*Spark Standalone Cluster* has been supported for a long time as one of the resource managers.

As a part of Apache Spark 4.0.0, we revisit all layers of `Spark Standalone Cluster` as a long running subsystem inside K8s environment.
 # Spark Master, Worker, History Server Web UI Layer
 # Spark Master HA and Recovery Layer
 # Spark Master REST API Layer (including Cluster Utilization monitoring)
 # Spark Job Scheduling Layer
 # Spark Worker Management by exposing Cluster Utilization monitoring for Elastic Cluster Management
 # Spark Master/Worker dependency and classpath audit
 # Security
 # Documentation"	SPARK	Resolved	2	15	3855	releasenotes
13578107	Support SPARK_SQL_LEGACY_CREATE_HIVE_TABLE env variable	This issue aims to support `SPARK_SQL_LEGACY_CREATE_HIVE_TABLE` env variable to provide more easier migration.	SPARK	Resolved	4	7	3855	pull-request-available
13578217	Disable a flaky `SparkSessionE2ESuite.interrupt tag` test	"- https://github.com/apache/spark/actions/runs/8962353911/job/24611130573 (Master, 5/5)
- https://github.com/apache/spark/actions/runs/8948176536/job/24581022674 (Master, 5/4)"	SPARK	Resolved	3	7	3855	pull-request-available
13569450	Update `SKIP_SPARK_RELEASE_VERSIONS` in Maven CIs	We need to skip newly released Apache Spark 3.5.1 and remove removed 3.3.4.	SPARK	Resolved	3	7	3855	pull-request-available
13550455	Remove System.setSecurityManager usage	"{code}
$ java -version
openjdk version ""21-ea"" 2023-09-19
OpenJDK Runtime Environment (build 21-ea+32-2482)
OpenJDK 64-Bit Server VM (build 21-ea+32-2482, mixed mode, sharing)
max spark-3.5.0-bin-hadoop3:$ bin/spark-sql --help
...
CLI options:
Exception in thread ""main"" java.lang.UnsupportedOperationException: The Security Manager is deprecated and will be removed in a future release
	at java.base/java.lang.System.setSecurityManager(System.java:429)
	at org.apache.spark.deploy.SparkSubmitArguments.getSqlShellOptions(SparkSubmitArguments.scala:623)
	at org.apache.spark.deploy.SparkSubmitArguments.$anonfun$printUsageAndExit$6(SparkSubmitArguments.scala:593)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.logInfo(SparkSubmit.scala:1112)
	at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.logInfo(SparkSubmit.scala:1104)
	at org.apache.spark.deploy.SparkSubmitArguments.printUsageAndExit(SparkSubmitArguments.scala:593)
	at org.apache.spark.deploy.SparkSubmitArguments.handle(SparkSubmitArguments.scala:448)
	at org.apache.spark.launcher.SparkSubmitOptionParser.parse(SparkSubmitOptionParser.java:174)
	at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:111)
	at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1103)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1103)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13558309	Document `spark.master.*` configurations	"Currently, `spark.master.*` configurations are undocumented.
{code:java}
$ git grep 'ConfigBuilder(""spark.master'
core/src/main/scala/org/apache/spark/internal/config/UI.scala:  val MASTER_UI_DECOMMISSION_ALLOW_MODE = ConfigBuilder(""spark.master.ui.decommission.allow.mode"")
core/src/main/scala/org/apache/spark/internal/config/package.scala:  private[spark] val MASTER_REST_SERVER_ENABLED = ConfigBuilder(""spark.master.rest.enabled"")
core/src/main/scala/org/apache/spark/internal/config/package.scala:  private[spark] val MASTER_REST_SERVER_PORT = ConfigBuilder(""spark.master.rest.port"")
core/src/main/scala/org/apache/spark/internal/config/package.scala:  private[spark] val MASTER_UI_PORT = ConfigBuilder(""spark.master.ui.port"")
core/src/main/scala/org/apache/spark/internal/config/package.scala:    ConfigBuilder(""spark.master.ui.historyServerUrl"")
core/src/main/scala/org/apache/spark/internal/config/package.scala:    ConfigBuilder(""spark.master.useAppNameAsAppId.enabled"") {code}"	SPARK	Resolved	3	7	3855	pull-request-available
13568204	Upgrade `aircompressor` to 0.26	"`aircompressor` is a transitive dependency from Apache ORC and Parquet.

`aircompressor` v0.26 reported the following bug fixes recently.
 - [Fix out of bounds read/write in Snappy decompressor]([https://github.com/airlift/aircompressor/commit/b89db180bb97debe025b640dc40ed43816e8c7d2])
 - [Fix ZstdOutputStream corruption on double close]([https://github.com/airlift/aircompressor/commit/b89db180bb97debe025b640dc40ed43816e8c7d2])"	SPARK	Resolved	3	7	3855	pull-request-available
13514259	Dynamic Allocation on K8S GA	In the Kubernetes environments, this issue aims to make `Dynamic Allocation` as GA in Apache Spark 3.4.	SPARK	Resolved	3	14	3855	releasenotes
13548438	Fix `RELEASE` file to have the correct information in Docker images	"{code}
$ docker run -it --rm apache/spark:latest ls -al /opt/spark/RELEASE
-rw-r--r-- 1 spark spark 0 Jun 25 03:13 /opt/spark/RELEASE

$ docker run -it --rm apache/spark:v3.1.3 ls -al /opt/spark/RELEASE | tail -n1
-rw-r--r-- 1 root root 0 Feb 21  2022 /opt/spark/RELEASE
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13589266	Reduce `spark-operator` fat jar size by excluding dependencies	"This issue aims to reduce the operator fat jar size from `227M` to `148M`. It's 35% reduction.

**BEFORE**
{code}
$ ls -alh spark-operator/build/libs/spark-kubernetes-operator-0.1.0-all.jar
-rw-r--r--  1 dongjoon  staff   227M Aug 18 23:09 spark-operator/build/libs/spark-kubernetes-operator-0.1.0-all.jar
{code}

**AFTER**
{code}
$ ls -alh spark-operator/build/libs/spark-kubernetes-operator-0.1.0-all.jar
-rw-r--r--  1 dongjoon  staff   148M Aug 18 23:03 spark-operator/build/libs/spark-kubernetes-operator-0.1.0-all.jar
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13558542	Upgrade `protobuf-java` to 3.25.1 to match with protobuf 4.25.1	"- https://github.com/protocolbuffers/protobuf/pull/14617

{code}
- set(protobuf_VERSION_STRING ""4.24.0"")
+ set(protobuf_VERSION_STRING ""4.25.0"")
{code}

{code}
- <version>3.24.0</version>
+ <version>3.25.0</version>
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13573258	Set spark.hadoop.fs.s3a.connection.establish.timeout to 30s	"To suppress like HADOOP-19097
{code}
24/03/25 14:46:21 WARN ConfigurationHelper: Option fs.s3a.connection.establish.timeout is too low (5,000 ms). Setting to 15,000 ms instead
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13290548	Upgrade netty-all to 4.1.47.Final	Upgrade version of io.netty_netty-all to 4.1.44.Final [CVE-2019-20445|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20445]	SPARK	Resolved	3	1	3855	security
13588222	Increase `Gradle` JVM memory to `4g` like Spark repo	"{code:java}
> Task :spark-operator:compileTestJava
Note: /Users/dongjoon/APACHE/spark-kubernetes-operator/spark-operator/src/test/java/org/apache/spark/k8s/operator/metrics/healthcheck/SentinelManagerTest.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.
[1.372s][warning][codecache] CodeCache is full. Compiler has been disabled.
OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=
[1.372s][warning][codecache] Try increasing the code cache size using -XX:ReservedCodeCacheSize=
CodeCache: size=2944Kb used=2943Kb max_used=2943Kb free=0Kb
 bounds [0x0000000105004000, 0x00000001052e4000, 0x00000001052e4000]
 total_blobs=1102 nmethods=464 adapters=554
 compilation: disabled (not enough contiguous free space left)
              stopped_count=1, restarted_count=0
 full_count=1 {code}"	SPARK	Resolved	3	7	3855	pull-request-available
13590335	Fix `spark.kubernetes.allocation.batch.delay` to prevent small values less than 100	The default value is `1s` (=1000). Usually, a small value like `1` happens when users do mistakes and forget to add the unit, `s`.	SPARK	Resolved	3	7	3855	pull-request-available
13537904	Build and Run Spark on Java 21	"- [https://docs.scala-lang.org/overviews/jdk-compatibility/overview.html]
||JDK version||Minimum Scala versions||
|21 (ea)|3.3.1 (soon), 2.13.11, 2.12.18|
|20|3.3.0, 2.13.11, 2.12.18|
|19|3.2.0, 2.13.9, 2.12.16|
|18|3.1.3, 2.13.7, 2.12.15|
|17 (LTS)|3.0.0, 2.13.6, 2.12.15|
|11 (LTS)|3.0.0, 2.13.0, 2.12.4, 2.11.12|
|8 (LTS)|3.0.0, 2.13.0, 2.12.0, 2.11.0|"	SPARK	Resolved	3	2	3855	releasenotes
13596658	Upgrade PyPy CI to 3.10	"Since v7.3.17, PyPy3.9 is dropped.

- [https://doc.pypy.org/en/latest/release-v7.3.17.html#]"	SPARK	Resolved	3	7	3855	pull-request-available
13572473	Exclude `logback` dependency from SBT like Maven	"{code}
$ build/mvn dependency:tree --pl core | grep logback
Using `mvn` from path: /opt/homebrew/bin/mvn
Using SPARK_LOCAL_IP=localhost
{code}

{code}
$ build/sbt ""core/test:dependencyTree"" | grep logback
Using SPARK_LOCAL_IP=localhost
[info]   |       +-ch.qos.logback:logback-classic:1.2.13
[info]   |       | +-ch.qos.logback:logback-core:1.2.13
[info]   |       +-ch.qos.logback:logback-core:1.2.13
[info]   | | +-ch.qos.logback:logback-classic:1.2.13
[info]   | | | +-ch.qos.logback:logback-core:1.2.13
[info]   | | +-ch.qos.logback:logback-core:1.2.13
[info]   | +-ch.qos.logback:logback-classic:1.2.13
[info]   | | +-ch.qos.logback:logback-core:1.2.13
[info]   | +-ch.qos.logback:logback-core:1.2.13
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
12980698	Vectorized ORC reader	"Currently Orc reader in Spark SQL doesn't support vectorized reading. As Hive Orc already support vectorization, we should add this support to improve Orc reading performance.
"	SPARK	Resolved	3	2	3855	release-notes, releasenotes
13565179	Use default ORC compression in data source benchmarks	"{code}
$ git grep OrcCompressionCodec | grep Benchmark
sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/BuiltInDataSourceWriteBenchmark.scala:import org.apache.spark.sql.execution.datasources.orc.OrcCompressionCodec
sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/BuiltInDataSourceWriteBenchmark.scala:      OrcCompressionCodec.SNAPPY.lowerCaseName())
sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala:import org.apache.spark.sql.execution.datasources.orc.OrcCompressionCodec
sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala:      OrcCompressionCodec.SNAPPY.lowerCaseName()).orc(dir)
sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/FilterPushdownBenchmark.scala:import org.apache.spark.sql.execution.datasources.orc.OrcCompressionCodec
sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/FilterPushdownBenchmark.scala:      .setIfMissing(""orc.compression"", OrcCompressionCodec.SNAPPY.lowerCaseName())
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13449847	Support IPv6-only environment	"Spark doesn't fully work in pure IPV6 environment that doesn't have IPV4 at all. This is an umbrella jira tracking the support of pure IPV6 deployment.

 

The scope of this JIRA doesn't include ancient resource managers like Mesos(deprecated in Spark) or YARN(because Apache Spark still supports Hadoop 2). We are going to focus on only `Spark Standalone` and `K8s`

 

K8s IPv4/IPv6 dual-stack Feature reached `Stable` stage at v1.23.
 - [https://kubernetes.io/docs/concepts/services-networking/dual-stack/]
 -- v1.16 [alpha]
 -- v1.21 [beta]
 -- v1.23 [stable]

 

Currently, EKS supports IPv6 with v1.21.13-eks-84b4fe6.

- https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/"	SPARK	Resolved	3	2	3855	releasenotes
13588540	Redact `Spark Command` output in `launcher` module	"When `launcher` module shows `Spark Command`, there is no redaction. Although Spark Cluster is supposed to be in a secure environment, this could be collected by a centralized log system. We need to do a proper redaction like we do for `Launch Command:` output.
{code:java}
$ SPARK_NO_DAEMONIZE=1 SPARK_MASTER_OPTS=""-Dspark.master.rest.enabled=true -Dspark.master.rest.filters=org.apache.spark.ui.JWSFilter -Dspark.org.apache.spark.ui.JWSFilter.param.secretKey=VmlzaXQgaHR0cHM6Ly9zcGFyay5hcGFjaGUub3JnIHRvIGRvd25sb2FkIEFwYWNoZSBTcGFyay4="" sbin/start-master.sh
starting org.apache.spark.deploy.master.Master, logging to /Users/dongjoon/APACHE/spark-releases/spark-4.0.0-preview1-bin-hadoop3/logs/spark-dongjoon-org.apache.spark.deploy.master.Master-1-M3-Max.local.out
Spark Command: /Users/dongjoon/.jenv/versions/17/bin/java -cp /Users/dongjoon/APACHE/spark-releases/spark-4.0.0-preview1-bin-hadoop3/conf/:/Users/dongjoon/APACHE/spark-releases/spark-4.0.0-preview1-bin-hadoop3/jars/slf4j-api-2.0.13.jar:/Users/dongjoon/APACHE/spark-releases/spark-4.0.0-preview1-bin-hadoop3/jars/* -Dspark.master.rest.enabled=true -Dspark.master.rest.filters=org.apache.spark.ui.JWSFilter -Dspark.org.apache.spark.ui.JWSFilter.param.secretKey=VmlzaXQgaHR0cHM6Ly9zcGFyay5hcGFjaGUub3JnIHRvIGRvd25sb2FkIEFwYWNoZSBTcGFyay4= -Xmx1g org.apache.spark.deploy.master.Master --host M3-Max.local --port 7077 --webui-port 8080
{code}"	SPARK	Resolved	2	7	3855	pull-request-available
12975343	Prevent saving with all-column partitioning	"When saving datasets on storage, `partitionBy` provides an easy way to construct the directory structure. However, if a user choose all columns as partition columns, some exceptions occurs.

- ORC: `AnalysisException` on **future read** due to schema inference failure.
- Parquet: `InvalidSchemaException` on **write execution** due to Parquet limitation.

The followings are the examples.
**ORC with all column partitioning**
{code}
scala> spark.range(10).write.format(""orc"").mode(""overwrite"").partitionBy(""id"").save(""/tmp/data"")
                                                                                
scala> spark.read.format(""orc"").load(""/tmp/data"").collect()
org.apache.spark.sql.AnalysisException: Unable to infer schema for ORC at /tmp/data. It must be specified manually;
{code}

**Parquet with all-column partitioning**
{code}
scala> spark.range(100).write.format(""parquet"").mode(""overwrite"").partitionBy(""id"").save(""/tmp/data"")
[Stage 0:>                                                          (0 + 8) / 8]16/06/02 16:51:17 ERROR Utils: Aborting task
org.apache.parquet.schema.InvalidSchemaException: A group type can not be empty. Parquet does not support empty group without leaves. Empty group: spark_schema
... (lots of error messages)
{code}

Although some formats like JSON support all-column partitioning without any problem, it seems not a good idea to make lots of empty directories. 

This issue prevents this by consistently raising `AnalysisException` before saving. "	SPARK	Resolved	3	1	3855	releasenotes
13577816	`mypy` should have `--python-executable` parameter	"We assumed that `PYTHON_EXECUTABLE` is used for `dev/lint-python` like the following. That's not true. We need to use `mypy`'s parameter to make it sure.

https://github.com/apache/spark/blob/ff401dde50343c9bbc1c49a0294272f2da7d01e2/.github/workflows/build_and_test.yml#L705

{code}
root@18c8eae5791e:/spark# PYTHON_EXECUTABLE=python3.9 mypy --python-executable=python3.11 --namespace-packages --config-file python/mypy.ini python/pyspark | wc -l
3428
root@18c8eae5791e:/spark# PYTHON_EXECUTABLE=python3.9 mypy --namespace-packages --config-file python/mypy.ini python/pyspark | wc -l
1
root@18c8eae5791e:/spark# PYTHON_EXECUTABLE=python3.11 mypy --namespace-packages --config-file python/mypy.ini python/pyspark | wc -l
1
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13543642	Use ANSI SQL mode by default	To avoid data issue.	SPARK	Resolved	3	7	3855	pull-request-available
13589140	Improve `gradlew` to support both `curl` and `wget`	"`curl` doesn't exist in some docker images like the following.
{code}
$ docker run -it --rm -v $PWD:/spark alpine/java:21-jdk -- sh
/ # cd /spark/
/spark # ./gradlew
./gradlew: line 90: curl: not found
Error: Could not find or load main class org.gradle.wrapper.GradleWrapperMain
Caused by: java.lang.ClassNotFoundException: org.gradle.wrapper.GradleWrapperMain
/spark #
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13558474	Improve Python language test coverage	"This umbrella Jira aims to improve *Apache Spark 4* test coverage across various Python language versions .

- PySpark
- Spark Connect Python Client"	SPARK	Resolved	3	14	3855	releasenotes
13561452	Support `readyz` in REST Submission API	"Like https://kubernetes.io/docs/reference/using-api/health-checks/, we need to provide `/readyz` API.

As a workaround, we can use the following.
{code}
readinessProbe:
  exec:
    command: [""sh"", ""-c"", ""! (curl -s http://localhost:6066/v1/submissions/status/none | grep -q STANDBY)""]
{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13568690	Upgrade several Maven plugins to the latest versions	"* {{versions-maven-plugin}} from 2.16.0 to 2.16.2.
 * {{maven-enforcer-plugin}} from 3.3.0 to 3.4.1.
 * {{maven-compiler-plugin}} from 3.11.0 to 3.12.1.
 * {{maven-surefire-plugin}} from 3.1.2 to 3.2.5.
 * {{maven-clean-plugin}} from 3.3.1 to 3.3.2.
 * {{maven-javadoc-plugin}} from 3.5.0 to 3.6.3.
 * {{maven-shade-plugin}} from 3.5.0 to 3.5.1.
 * {{maven-dependency-plugin}} from 3.6.0 to 3.6.1.
 * {{maven-checkstyle-plugin}} from 3.3.0 to 3.3.1."	SPARK	Resolved	4	7	3855	pull-request-available
13582431	Upgrade `google-java-format` to 1.22.0	"This issue aims to upgrade `google-java-format` plugin of Spark Kubernetes Operator repository to bring the latest bug fixes like the following. The latest version is recommended.
{code}
java.lang.Exception: google-java-format 1.17.0 is currently being used, but outdated.
google-java-format 1.19.2 is the recommended version, which may have fixed this problem.
google-java-format 1.19.2 requires JVM 11+.
{code}"	SPARK	Resolved	3	4	3855	pull-request-available
13528567	Customized K8s Scheduler GA	Since Apache Spark 3.3.1, we have been supported the customized K8s scheduler via SPARK-36057. This issue aims to announce GA of the customized K8s schedulers in Apache Spark 3.4.0.	SPARK	Resolved	3	14	3855	releasenotes
13050171	String literals are not escaped while performing Hive metastore level partition pruning	"{{Shim_v0_13.convertFilters()}} doesn't escape string literals while generating Hive style partition predicates.

The following SQL-injection-like test case illustrates this issue:
{code}
  test(""SPARK-19912"") {
    withTable(""spark_19912"") {
      Seq(
        (1, ""p1"", ""q1""),
        (2, ""p1\"" and q=\""q1"", ""q2"")
      ).toDF(""a"", ""p"", ""q"").write.partitionBy(""p"", ""q"").saveAsTable(""spark_19912"")

      checkAnswer(
        spark.table(""foo"").filter($""p"" === ""p1\"" and q = \""q1"").select($""a""),
        Row(2)
      )
    }
  }
{code}
The above test case fails like this:
{noformat}
[info] - spark_19912 *** FAILED *** (13 seconds, 74 milliseconds)
[info]   Results do not match for query:
[info]   Timezone: sun.util.calendar.ZoneInfo[id=""America/Los_Angeles"",offset=-28800000,dstSavings=3600000,useDaylight=true,transitions=185,lastRule=java.util.SimpleTimeZone[id=America/Los_Angeles,offset=-28800000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]
[info]   Timezone Env:
[info]
[info]   == Parsed Logical Plan ==
[info]   'Project [unresolvedalias('a, None)]
[info]   +- Filter (p#27 = p1"" and q = ""q1)
[info]      +- SubqueryAlias spark_19912
[info]         +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Analyzed Logical Plan ==
[info]   a: int
[info]   Project [a#26]
[info]   +- Filter (p#27 = p1"" and q = ""q1)
[info]      +- SubqueryAlias spark_19912
[info]         +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Optimized Logical Plan ==
[info]   Project [a#26]
[info]   +- Filter (isnotnull(p#27) && (p#27 = p1"" and q = ""q1))
[info]      +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Physical Plan ==
[info]   *Project [a#26]
[info]   +- *FileScan parquet default.spark_19912[a#26,p#27,q#28] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionCount: 0, PartitionFilters: [isnotnull(p#27), (p#27 = p1"" and q = ""q1)], PushedFilters: [], ReadSchema: struct<a:int>
[info]   == Results ==
[info]
[info]   == Results ==
[info]   !== Correct Answer - 1 ==   == Spark Answer - 0 ==
[info]    struct<>                   struct<>
[info]   ![2]
{noformat}"	SPARK	Resolved	3	1	3855	correctness
13354105	For parquet table, after changing the precision and scale of decimal type in hive, spark reads incorrect value	"In Hive, 

{code}
create table test_decimal(amt decimal(18,2)) stored as parquet; 
insert into test_decimal select 100;
alter table test_decimal change amt amt decimal(19,3);
{code}


In Spark,

{code}
select * from test_decimal;
{code}

{code}
+--------+
|    amt |
+--------+
| 10.000 |
+--------+
{code}
"	SPARK	Resolved	1	1	3855	correctness
12908636	Replace example code in mllib-linear-methods.md using include_example	This is similar to SPARK-11289 but for the example code in mllib-linear-methods.md.	SPARK	Resolved	3	7	3855	starter
13588912	Add `OpenContainers` Annotations in docker images	"h3. BEFORE
{code:java}
$ docker inspect apache/spark:3.5.2 | jq '.[0].Config.Labels'
{
  ""org.opencontainers.image.ref.name"": ""ubuntu"",
  ""org.opencontainers.image.version"": ""20.04""
} {code}
h3. AFTER
{code:java}
$ NO_MANUAL=1 ./dev/make-distribution.sh -Pkubernetes
$ bin/docker-image-tool.sh -t SPARK-49243 -n build
$ docker inspect spark:SPARK-49243 | jq '.[0].Config.Labels'
{
  ""org.opencontainers.image.authors"": ""Apache Spark project <dev@spark.apache.org>"",
  ""org.opencontainers.image.licenses"": ""Apache-2.0"",
  ""org.opencontainers.image.ref.name"": ""Apache Spark Scala/Java Image"",
  ""org.opencontainers.image.version"": """"
} {code}"	SPARK	Resolved	4	7	3855	pull-request-available
13569711	Fix `kafka-0-10-sql` to use `ResetSystemProperties` if `KafkaTestUtils` is used	"

https://github.com/apache/spark/blob/ee312ecb40ea5b5303fc794a3d494b6f27cda923/connector/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala#L290"	SPARK	Resolved	3	7	3855	pull-request-available
13485448	Make (SQL|ThriftServer)QueryTestSuite pass except Pandas UDF tests in Java 21	"Some case in SQLQueryTestSuite(sql/core) and ThriftServerQueryTestSuite(sql/hive-thriftserver) failed for this reason:

for example:

 
{code:java}
SQLQueryTestSuite- 

try_aggregates.sql *** FAILED ***
  try_aggregates.sql
  Expected ""4.61168601842738[79]E18"", but got ""4.61168601842738[8]E18"" Result did not match for query #20
  SELECT try_avg(col) FROM VALUES (9223372036854775807L), (1L) AS tab(col) (SQLQueryTestSuite.scala:495) 

{code}
{code:java}
ThriftServerQueryTestSuite- try_aggregates.sql *** FAILED ***
  Expected ""4.61168601842738[79]E18"", but got ""4.61168601842738[8]E18"" Result did not match for query #20
  SELECT try_avg(col) FROM VALUES (9223372036854775807L), (1L) AS tab(col) (ThriftServerQueryTestSuite.scala:222)- try_arithmetic.sql *** FAILED ***
  Expected ""-4.65661287307739[26]E-10"", but got ""-4.65661287307739[3]E-10"" Result did not match for query #26
  SELECT try_divide(1, (2147483647 + 1)) (ThriftServerQueryTestSuite.scala:222)- datetime-{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13578004	Run `maven-build` test only on `Java 21 on MacOS 14 (Apple Silicon)`	"`Java 21 on MacOS 14` is the fastest Maven test and covers both Java 17 and Apple Silicon use case.

 !Screenshot 2024-05-02 at 14.59.14.png|width=100%! "	SPARK	Resolved	3	7	3855	pull-request-available
13578021	Use `Python 3.11` in `pyspark` tests of `build_and_test.yml`	"- https://docs.python.org/3/whatsnew/3.11.html#summary-release-highlights

bq. Python 3.11 is between 10-60% faster than Python 3.10. On average, we measured a 1.25x speedup on the standard benchmark suite."	SPARK	Resolved	3	7	3855	pull-request-available
13024315	Spark SQL: Catalyst is scanning undesired columns	"When doing a left-join between two tables, say A and B,  Catalyst has information about the projection required for table B. Only the required columns should be scanned.

Code snippet below explains the scenario:

scala> val dfA = sqlContext.read.parquet(""/home/mohit/ruleA"")
dfA: org.apache.spark.sql.DataFrame = [aid: int, aVal: string]

scala> val dfB = sqlContext.read.parquet(""/home/mohit/ruleB"")
dfB: org.apache.spark.sql.DataFrame = [bid: int, bVal: string]

scala> dfA.registerTempTable(""A"")
scala> dfB.registerTempTable(""B"")

scala> sqlContext.sql(""select A.aid, B.bid from A left join B on A.aid=B.bid where B.bid<2"").explain

== Physical Plan ==
Project [aid#15,bid#17]
+- Filter (bid#17 < 2)
   +- BroadcastHashOuterJoin [aid#15], [bid#17], LeftOuter, None
      :- Scan ParquetRelation[aid#15,aVal#16] InputPaths: file:/home/mohit/ruleA
      +- Scan ParquetRelation[bid#17,bVal#18] InputPaths: file:/home/mohit/ruleB

This is a watered-down example from a production issue which has a huge performance impact.
External reference: http://stackoverflow.com/questions/40783675/spark-sql-catalyst-is-scanning-undesired-columns"	SPARK	Resolved	3	1	3855	performance
13592297	Support `spark.test.master` in `SparkSubmitArguments`	"To allow users to control the default master setting during testing and documentation generation.

BEFORE (`local[*]`)
{code:java}
$ bin/pyspark
Python 3.9.19 (main, Jun 17 2024, 15:39:29)
[Clang 15.0.0 (clang-1500.3.9.4)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-pattern-layout-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/09/16 13:53:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / _/_  ___ ____/ /_
    \ \/ _ \/ _ `/ __/  '/
   /__ / ._/_,// //_\   version 4.0.0-SNAPSHOT
      /_/
Using Python version 3.9.19 (main, Jun 17 2024 15:39:29)
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1726519982935).
SparkSession available as 'spark'.
>>>{code}
 

AFTER (`local[1]`)
{code:java}
 $ JDK_JAVA_OPTIONS=""-Dspark.test.master=local[1]"" bin/pyspark
NOTE: Picked up JDK_JAVA_OPTIONS: -Dspark.test.master=local[1]
Python 3.9.19 (main, Jun 17 2024, 15:39:29)
[Clang 15.0.0 (clang-1500.3.9.4)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
NOTE: Picked up JDK_JAVA_OPTIONS: -Dspark.test.master=local[1]
NOTE: Picked up JDK_JAVA_OPTIONS: -Dspark.test.master=local[1]
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-pattern-layout-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/09/16 13:51:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / _/_  ___ ____/ /_
    \ \/ _ \/ _ `/ __/  '/
   /__ / ._/_,// //_\   version 4.0.0-SNAPSHOT
      /_/
Using Python version 3.9.19 (main, Jun 17 2024 15:39:29)
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[1], app id = local-1726519863363).
SparkSession available as 'spark'.
>>>{code}"	SPARK	Resolved	3	7	3855	pull-request-available
13254022	Maven install fails on JDK11	"{code}
mvn clean install -pl common/network-common -DskipTests

error: fatal error: object scala in compiler mirror not found.
one error found
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
{code}"	SPARK	Resolved	1	7	3855	pull-request-available
13586411	Use `BasePluginExtension` in `spark-operator/build.gradle`	"{code}
$ ./gradlew test --warning-mode all

> Configure project :spark-operator
Build file '/Users/dongjoon/APACHE/spark-kubernetes-operator/spark-operator/build.gradle': line 58
The org.gradle.api.plugins.BasePluginConvention type has been deprecated. This is scheduled to be removed in Gradle 9.0. Consult the upgrading guide for further information: https://docs.gradle.org/8.7/userguide/upgrading_version_8.html#base_convention_deprecation
        at build_1pl46tz9v8uzxd6mxgrewtcjs.run(/Users/dongjoon/APACHE/spark-kubernetes-operator/spark-operator/build.gradle:58)
        (Run with --stacktrace to get the full stack trace of this deprecation warning.)
Build file '/Users/dongjoon/APACHE/spark-kubernetes-operator/spark-operator/build.gradle': line 58
The BasePluginExtension.archivesBaseName property has been deprecated. This is scheduled to be removed in Gradle 9.0. Please use the archivesName property instead. For more information, please refer to https://docs.gradle.org/8.7/dsl/org.gradle.api.plugins.BasePluginExtension.html#org.gradle.api.plugins.BasePluginExtension:archivesName in the Gradle documentation.
        at build_1pl46tz9v8uzxd6mxgrewtcjs.run(/Users/dongjoon/APACHE/spark-kubernetes-operator/spark-operator/build.gradle:58)
        (Run with --stacktrace to get the full stack trace of this deprecation warning.)
{code}"	SPARK	Resolved	4	7	3855	pull-request-available
13589647	Remove `connector/docker` in favor of `Apache Spark Operator`	This PR aims to remove a test resource `connector/docker` in favor of `Apache Spark Operator`.	SPARK	Resolved	3	7	3855	pull-request-available
13270567	Use Apache Hive 2.3 dependency by default	Some correctness issue like SPARK-25193 is fixed by `hive-2.3` only.	SPARK	Resolved	1	14	3855	correctness, release-notes
13539761	Remove threeten-extra exclusion in enforceBytecodeVersion rule	We can remove `threeten-extra` library exclusion rule because Apache Spark 4.0.0's minimum Java is 17.	SPARK	Resolved	4	7	3855	pull-request-available
13540807	Drop K8s v1.25 and lower version support	"*1. Default K8s Version in Public Cloud environments*

The default K8s versions of public cloud providers are already K8s 1.27+.

- EKS: v1.27 (Default)
- GKE: v1.27 (Stable), v1.27 (Regular), v1.27 (Rapid)

*2. End Of Support*

In addition, K8s 1.25 and olders are going to reach EOL when Apache Spark 4.0.0 arrives on June 2024. K8s 1.26 is also going to reach EOL on June.

|| K8s  ||   AKS   ||   GKE   ||   EKS   ||
| 1.27 | 2024-07 | 2024-08 | 2024-07 |
| 1.26 | 2024-03 | 2024-06 | 2024-06 |
| 1.25 | 2023-12 | 2024-02 | 2024-05 |
| 1.24 | 2023-07 | 2023-10 | 2024-01 |

- [AKS EOL Schedule](https://docs.microsoft.com/en-us/azure/aks/supported-kubernetes-versions?tabs=azure-cli#aks-kubernetes-release-calendar)
- [GKE EOL Schedule](https://cloud.google.com/kubernetes-engine/docs/release-schedule)
- [EKS EOL Schedule](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar)"	SPARK	Resolved	3	7	3855	pull-request-available
13160530	Upgrade Apache ORC to 1.4.4	"ORC 1.4.4 includes [nine fixes|https://issues.apache.org/jira/issues/?filter=12342568&jql=project%20%3D%20ORC%20AND%20resolution%20%3D%20Fixed%20AND%20fixVersion%20%3D%201.4.4]. One of the issues is about `Timestamp` bug (ORC-306) which occurs when `native` ORC vectorized reader reads ORC column vector's sub-vector `times` and `nanos`. ORC-306 fixes this according to the [original definition|https://github.com/apache/hive/blob/master/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java#L45-L46] and the linked PR includes the updated interpretation on ORC column vectors. Note that `hive` ORC reader and ORC MR reader is not affected.

{code}
scala> spark.version
res0: String = 2.3.0
scala> spark.sql(""set spark.sql.orc.impl=native"")
scala> Seq(java.sql.Timestamp.valueOf(""1900-05-05 12:34:56.000789"")).toDF().write.orc(""/tmp/orc"")
scala> spark.read.orc(""/tmp/orc"").show(false)
+--------------------------+
|value                     |
+--------------------------+
|1900-05-05 12:34:55.000789|
+--------------------------+
{code}

This issue aims to update Apache Spark to use it.

*FULL LIST*

|| ID || TITLE ||
| ORC-281 | Fix compiler warnings from clang 5.0 | 
| ORC-301 | `extractFileTail` should open a file in `try` statement | 
| ORC-304 | Fix TestRecordReaderImpl to not fail with new storage-api | 
| ORC-306 | Fix incorrect workaround for bug in java.sql.Timestamp | 
| ORC-324 | Add support for ARM and PPC arch | 
| ORC-330 | Remove unnecessary Hive artifacts from root pom | 
| ORC-332 | Add syntax version to orc_proto.proto | 
| ORC-336 | Remove avro and parquet dependency management entries | 
| ORC-360 | Implement error checking on subtype fields in Java | "	SPARK	Resolved	3	1	3855	correctness
13354522	Avro should read decimal values with the file schema	"{code:java}
scala> sql(""SELECT 3.14 a"").write.format(""avro"").save(""/tmp/avro"")
scala> spark.read.schema(""a DECIMAL(4, 3)"").format(""avro"").load(""/tmp/avro"").show
+-----+
|    a|
+-----+
|0.314|
+-----+ {code}"	SPARK	Resolved	1	1	3855	correctness
13410694	Build and test on Python 3.10	"Python 3.10 introduced breaking changes. We need to update PySpark.
- https://docs.python.org/3/whatsnew/3.10.html

For example, the following.
{code}
>>> from collections import Callable
<stdin>:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
{code}

This is targeting Apache Spark 3.3.0 in 2022."	SPARK	Resolved	3	4	3855	releasenotes
13568622	Use Scala 2.13 Spark distribution in HiveExternalCatalogVersionsSuite	"SPARK-45314 makes Scala 2.13 is the default Scala version.
As one of migration paths, the users choose Apache Spark 3.5.0 (Scala 2.13) and Apache Spark 3.4.2 (Scala 2.13).
We had better focus on Scala 2.13 testing."	SPARK	Resolved	3	7	3855	pull-request-available
13534805	Re-enable test_create_dataframe_from_pandas_with_day_time_interval	This test fails with PyPy 3.8.	SPARK	Resolved	3	7	3855	pull-request-available
13016926	GraphX Invalid initial capacity when running triangleCount	"Running GraphX triangle count on large-ish file results in the ""Invalid initial capacity"" error when running on Spark 2.0 (tested on Spark 2.0, 2.0.1, and 2.0.2).  You can see the results at: http://bit.ly/2eQKWDN

Running the same code on Spark 1.6 and the query completes without any problems: http://bit.ly/2fATO1M

As well, running the GraphFrames version of this code runs as well (Spark 2.0, GraphFrames 0.2): http://bit.ly/2fAS8W8

Reference Stackoverflow question:
Spark GraphX: requirement failed: Invalid initial capacity (http://stackoverflow.com/questions/40337366/spark-graphx-requirement-failed-invalid-initial-capacity)"	SPARK	Resolved	3	1	3855	graph, graphx
13586592	Fix `spark-operator` module to define test framework explicitly	"[https://docs.gradle.org/8.9/userguide/upgrading_version_8.html#test_framework_implementation_dependencies]

 
{code:java}
$ ./gradlew clean test --warning-mode=fail

...

> Task :spark-operator:test
The automatic loading of test framework implementation dependencies has been deprecated. This is scheduled to be removed in Gradle 9.0. Declare the desired test framework directly on the test suite or explicitly declare the test framework implementation dependencies on the test's runtime classpath. Consult the upgrading guide for further information: https://docs.gradle.org/8.9/userguide/upgrading_version_8.html#test_framework_implementation_dependencies
OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appendedFAILURE: Build failed with an exception.* What went wrong:
Deprecated Gradle features were used in this build, making it incompatible with Gradle 9.0* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
> Get more help at https://help.gradle.org.BUILD FAILED in 6s
16 actionable tasks: 16 executed {code}"	SPARK	Resolved	4	7	3855	pull-request-available
13580835	Implement DataFrameQueryContext in Spark Connect	Implements the same https://github.com/apache/spark/pull/45377 in Spark Connect	SPARK	Resolved	3	4	4739	pull-request-available
13560563	Remove stale Python 3.8/3.7 version checkings	See PR linked. We dropped Python 3.7 and lowest is Python 3.8 so we can remove all stale checkings.	SPARK	Resolved	3	4	4739	pull-request-available
13551821	Remove PID communication between Python workers when no demon is used	We don't need to send the PID around when JDK 9+ is used because we can get the API directly.	SPARK	Reopened	3	7	4739	pull-request-available
13560379	Test SparkPandasNotImplementedError (pyspark.pandas.exceptions)	https://app.codecov.io/gh/apache/spark/blob/master/python%2Fpyspark%2Fpandas%2Fexceptions.py	SPARK	Resolved	4	7	4739	pull-request-available
13560388	Add applyInArrow to groupBy and cogroup in Spark Connect	Should have the connect version of SPARK-40559	SPARK	Resolved	3	2	4739	pull-request-available
12952965	Python DataFrame CSV load on large file is writing to console in Ipython	"I am using the spark from the master branch and when I run the following command on a large tab separated file then I get the contents of the file being written to the stderr

{code}
df = sqlContext.read.load(""temp.txt"", format=""csv"", header=""false"", inferSchema=""true"", delimiter=""\t"")
{code}

Here is a sample of output:

{code}
^M[Stage 1:>                                                          (0 + 2) / 2]16/03/23 14:01:02 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 2)
com.univocity.parsers.common.TextParsingException: Error processing input: Length of parsed input (1000001) exceeds the maximum number of characters defined in your parser settings (1000000). Identified line separator characters in the parsed content. This may be the cause of the error. The line separator in your parser settings is set to '\n'. Parsed content:
        Privacy-shake"",: a haptic interface for managing privacy settings in mobile location sharing applications       privacy shake a haptic interface for managing privacy settings in mobile location sharing applications  2010    2010/09/07              international conference on human computer interaction  interact                43331058        19371[\n]        3D4F6CA1        Between the Profiles: Another such Bias. Technology Acceptance Studies on Social Network Services       between the profiles another such bias technology acceptance studies on social network services 2015    2015/08/02      10.1007/978-3-319-21383-5_12    international conference on human-computer interaction  interact                43331058        19502[\n]

.......

.........

web snippets    2008    2008/05/04      10.1007/978-3-642-01344-7_13    international conference on web information systems and technologies    webist          44F29802        19489
06FA3FFA        Interactive 3D User Interfaces for Neuroanatomy Exploration     interactive 3d user interfaces for neuroanatomy exploration     2009                    internationa]
        at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:241)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:356)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:137)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.next(CSVParser.scala:120)
        at scala.collection.Iterator$class.foreach(Iterator.scala:742)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foreach(CSVParser.scala:120)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.foldLeft(CSVParser.scala:120)
        at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:212)
        at org.apache.spark.sql.execution.datasources.csv.BulkCsvReader.aggregate(CSVParser.scala:120)
        at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
        at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1058)
        at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
        at org.apache.spark.SparkContext$$anonfun$35.apply(SparkContext.scala:1827)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:69)
        at org.apache.spark.scheduler.Task.run(Task.scala:82)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException
16/03/23 14:01:03 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
^M[Stage 1:>                                                          (0 + 1) / 2]


{code}


For a small sample (<10,000 lines) of the data, I am not getting any error. But as soon as I go above more than 100,000 samples, I start getting the error. 

I don't think the spark platform should output the actual data to stderr ever as it decreases the readability. "	SPARK	Resolved	3	1	4739	csv, csvparser, dataframe, pyspark
13559255	Make SparkNoSuchElementException as a canonical error API	https://github.com/apache/spark/pull/43927 added SparkNoSuchElementException. It should be a canonical error API, documented properly.	SPARK	Resolved	4	4	4739	pull-request-available
13564134	Exclude unittest-xml-reporting in Python 3.12 image	"unittest-xml-reporting seems not supporting, and this seems hiding the real error:

{code}
  File ""/__w/spark/spark/python/pyspark/streaming/tests/test_kinesis.py"", line 118, in <module>
    unittest.main(testRunner=testRunner, verbosity=2)
  File ""/usr/lib/python3.12/unittest/main.py"", line 105, in __init__
    self.runTests()
  File ""/usr/lib/python3.12/unittest/main.py"", line 281, in runTests
    self.result = testRunner.run(self.test)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/xmlrunner/runner.py"", line 67, in run
    test(result)
  File ""/usr/lib/python3.12/unittest/suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/unittest/suite.py"", line 122, in run
    test(result)
  File ""/usr/lib/python3.12/unittest/suite.py"", line 84, in __call__
    return self.run(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/unittest/suite.py"", line 122, in run
    test(result)
  File ""/usr/lib/python3.12/unittest/case.py"", line 692, in __call__
    return self.run(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/unittest/case.py"", line 662, in run
    result.stopTest(self)
  File ""/usr/local/lib/python3.12/dist-packages/xmlrunner/result.py"", line 327, in stopTest
    self.callback()
  File ""/usr/local/lib/python3.12/dist-packages/xmlrunner/result.py"", line 235, in callback
    test_info.test_finished()
  File ""/usr/local/lib/python3.12/dist-packages/xmlrunner/result.py"", line 180, in test_finished
    self.test_result.stop_time - self.test_result.start_time
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: '_XMLTestResult' object has no attribute 'start_time'. Did you mean: 'stop_time'?

{code}

This is optional dependency in testing so we can exclude this."	SPARK	Resolved	3	7	4739	pull-request-available
13417574	Switch default index to distributed-sequence by default in pandas API on Spark	"pandas API on Spark currently sets {{compute.default_index_type}} to {{sequence}} which relies on sending all data to one executor that easily causes OOM.

We should better switch to {{distributed-sequence}} type that truly distributes the data.

With this change, we can now leverage https://issues.apache.org/jira/browse/SPARK-36559 and https://issues.apache.org/jira/browse/SPARK-36338 by default, and end users will benefit a lot of performance improvement."	SPARK	Resolved	2	4	4739	release-notes
13231019	Creating a DataFrame in PySpark with ArrayType produces some Rows with Arrays of None	"This seems to only affect Python 3.

When creating a DataFrame with type {{ArrayType(IntegerType(), True)}} there ends up being rows that are filled with None.

 
{code:java}
In [1]: from pyspark.sql.types import ArrayType, IntegerType                                                                 

In [2]: df = spark.createDataFrame([[1, 2, 3, 4]] * 100, ArrayType(IntegerType(), True))                                     

In [3]: df.distinct().collect()                                                                                              
Out[3]: [Row(value=[None, None, None, None]), Row(value=[1, 2, 3, 4])]
{code}
 

From this example, it is consistently at elements 97, 98:
{code}
In [5]: df.collect()[-5:]                                                                                                    
Out[5]: 
[Row(value=[1, 2, 3, 4]),
 Row(value=[1, 2, 3, 4]),
 Row(value=[None, None, None, None]),
 Row(value=[None, None, None, None]),
 Row(value=[1, 2, 3, 4])]
{code}
This also happens with a type of {{ArrayType(ArrayType(IntegerType(), True))}}"	SPARK	Resolved	1	1	4739	correctness
13564143	Run PyPy 3 and Python 3.10 tests independently	"https://github.com/apache/spark/actions/runs/7462843546/job/20306241275

Seems like it terminates in the middle because of OOM. we should split"	SPARK	Resolved	4	6	4739	pull-request-available
13313420	Use iloc for positional slicing instead of direct slicing in createDataFrame with Arrow	"When you use floats are index of pandas, it produces a wrong results:

{code}
>>> import pandas as pd
>>> spark.createDataFrame(pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])).show()
+---+
|  a|
+---+
|  1|
|  1|
|  2|
+---+
{code}

This is because direct slicing uses the value as index when the index contains floats:

{code}
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:]
     a
2.0  1
3.0  2
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:]
     a
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:]
   a
4  3
{code}"	SPARK	Resolved	2	1	4739	correctness
13555804	Makes entire Binder build fails fast during setting up	"Binder build is currently broken:

https://mybinder.org/v2/gh/apache/spark/ce5ddad9903?filepath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_df.ipynb

Seems like we uploaded PySpark late into PyPI, and the installation steps just slightly ignored the failure."	SPARK	Resolved	3	2	4739	pull-request-available
13243438	Revisiting Python / pandas UDF	"In the past two years, the pandas UDFs are perhaps the most important changes to Spark for Python data science. However, these functionalities have evolved organically, leading to some inconsistencies and confusions among users. This document revisits UDF definition and naming, as a result of discussions among Xiangrui, Li Jin, Hyukjin, and Reynold.

-See document here: [https://docs.google.com/document/d/10Pkl-rqygGao2xQf6sddt0b-4FYK4g8qr_bXLKTL65A/edit#|https://docs.google.com/document/d/10Pkl-rqygGao2xQf6sddt0b-4FYK4g8qr_bXLKTL65A/edit]-

 New proposal: https://docs.google.com/document/d/1-kV0FS_LF2zvaRh_GhkV32Uqksm_Sq8SvnBBmRyxm30/edit?usp=sharing"	SPARK	Resolved	1	4	4739	release-notes
13577665	Backward compatibility test for Spark Connect	Now that we can run the Spark Connect server separately in CI, we can run the Spark Connect server of lower version, and higher version of client, and the opposite as well.	SPARK	Resolved	3	7	4739	pull-request-available
13561085	Test missing cases for SparkSession (pyspark.sql.session)	https://app.codecov.io/gh/apache/spark/blob/master/python%2Fpyspark%2Fsql%2Fsession.py	SPARK	Resolved	4	7	4739	pull-request-available
13568665	Recover -1 and 0 case for spark.sql.execution.arrow.maxRecordsPerBatch	"{code}
import pandas as pd
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""true"")
spark.conf.set(""spark.sql.execution.arrow.maxRecordsPerBatch"", 0)
spark.conf.set(""spark.sql.execution.arrow.pyspark.fallback.enabled"", False)
spark.createDataFrame(pd.DataFrame({'a': [123]})).toPandas()

spark.conf.set(""spark.sql.execution.arrow.maxRecordsPerBatch"", -1)
spark.createDataFrame(pd.DataFrame({'a': [123]})).toPandas()
{code}

{code}
/.../spark/python/pyspark/sql/pandas/conversion.py:371: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and will not continue because automatic fallback with 'spark.sql.execution.arrow.pyspark.fallback.enabled' has been set to false.
  range() arg 3 must not be zero
  warn(msg)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/session.py"", line 1483, in createDataFrame
    return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]
  File ""/.../spark/python/pyspark/sql/pandas/conversion.py"", line 351, in createDataFrame
    return self._create_from_pandas_with_arrow(data, schema, timezone)
  File ""/.../spark/python/pyspark/sql/pandas/conversion.py"", line 633, in _create_from_pandas_with_arrow
    pdf_slices = (pdf.iloc[start : start + step] for start in range(0, len(pdf), step))
ValueError: range() arg 3 must not be zero
{code}

{code}
Empty DataFrame
Columns: [a]
Index: []
{code}"	SPARK	Resolved	2	1	4739	pull-request-available
13561097	Fix the output name of pyspark.sql.functions.now	It returns {{current_timestamp}} now.	SPARK	Resolved	4	1	4739	pull-request-available
13559809	Skip `TorchDistributorLocalUnitTests.test_end_to_end_run_locally` with Python 3.12	"{code}
======================================================================
ERROR [12.635s]: test_end_to_end_run_locally (pyspark.ml.tests.connect.test_parity_torch_distributor.TorchDistributorLocalUnitTestsIIOnConnect.test_end_to_end_run_locally)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/ml/torch/tests/test_distributor.py"", line 403, in test_end_to_end_run_locally
    output = TorchDistributor(num_processes=2, local_mode=True, use_gpu=False).run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 969, in run
    return self._run(
           ^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 985, in _run
    output = self._run_local_training(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 593, in _run_local_training
    output = TorchDistributor._get_output_from_framework_wrapper(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 567, in _get_output_from_framework_wrapper
    return framework_wrapper(
           ^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 908, in _run_training_on_pytorch_function
    raise RuntimeError(
RuntimeError: TorchDistributor failed during training.View stdout logs for detailed error message.

======================================================================
ERROR [14.850s]: test_end_to_end_run_locally (pyspark.ml.tests.connect.test_parity_torch_distributor.TorchDistributorLocalUnitTestsOnConnect.test_end_to_end_run_locally)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/ml/torch/tests/test_distributor.py"", line 403, in test_end_to_end_run_locally
    output = TorchDistributor(num_processes=2, local_mode=True, use_gpu=False).run(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 969, in run
    return self._run(
           ^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 985, in _run
    output = self._run_local_training(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 593, in _run_local_training
    output = TorchDistributor._get_output_from_framework_wrapper(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 567, in _get_output_from_framework_wrapper
    return framework_wrapper(
           ^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 908, in _run_training_on_pytorch_function
    raise RuntimeError(
RuntimeError: TorchDistributor failed during training.View stdout logs for detailed error message.

----------------------------------------------------------------------
{code}

https://github.com/apache/spark/actions/runs/7020654429/job/19100964890"	SPARK	Resolved	4	7	4739	pull-request-available
13559258	Dataset.groupingSets in Scala Spark Connect client	Scala Spark Connect client for SPARK-45929	SPARK	Resolved	3	2	4739	pull-request-available
13584438	Use SparkSession in SQLImplicits	See https://github.com/apache/spark/pull/47173	SPARK	Resolved	4	6	4739	pull-request-available
13571080	Suppress Python exceptions where PySpark is not in the Python path	"{code}
scala> sql(""create table t(i int) using json"")
24/03/06 16:09:44 WARN DataSourceManager: Skipping the lookup of Python Data Sources due to the failure.
org.apache.spark.SparkException:
Error from python worker:
  /opt/homebrew/Caskroom/miniconda/base/bin/python3: Error while finding module specification for 'pyspark.daemon' (ModuleNotFoundError: No module named 'pyspark')
{code}

When PySpark is not in the Python path at all, it always shows this warning message once for every Spark session initialization."	SPARK	Resolved	4	6	4739	pull-request-available
13565742	Refine error classes in Python with automatic sorting function	There are too many inconsistency within error_classes, and there's no way to automatically generate/sort the error classes. We should make the dev easier.	SPARK	Resolved	3	7	4739	pull-request-available
13569980	Increase timeout between actions in KafkaContinuousTest	It fails in MacOS build	SPARK	Resolved	4	6	4739	pull-request-available
13558571	Upgrade R version from 4.3.1 to 4.3.2 in AppVeyor	"
https://cran.r-project.org/doc/manuals/r-release/NEWS.html"	SPARK	Resolved	3	7	4739	pull-request-available
13559628	Flaky `pyspark.tests.test_worker.WorkerSegfaultNonDaemonTest.test_python_segfault` with Python 3.12	"{code}
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/tests/test_worker.py"", line 241, in test_python_segfault
    self.sc.parallelize([1]).map(lambda x: f()).count()
  File ""/__w/spark/spark/python/pyspark/rdd.py"", line 2315, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/rdd.py"", line 2290, in sum
    return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/rdd.py"", line 2043, in fold
    vals = self.mapPartitions(func).collect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/pyspark/rdd.py"", line 1832, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File ""/__w/spark/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (localhost executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:560)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:535)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:863)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:843)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:473)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.mutable.Growable.addAll(Growable.scala:61)
	at scala.collection.mutable.Growable.addAll$(Growable.scala:57)
	at scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:67)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:560)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:535)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:863)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:843)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:473)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.mutable.Growable.addAll(Growable.scala:61)
	at scala.collection.mutable.Growable.addAll$(Growable.scala:57)
	at scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:67)
	at scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1346)
	at scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1339)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1047)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2468)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:628)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:631)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
Caused by: java.io.EOFException
	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:851)
	... 22 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/tests/test_worker.py"", line 243, in test_python_segfault
    self.assertRegex(str(e), ""Segmentation fault"")
AssertionError: Regex didn't match: 'Segmentation fault' not found in 'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (localhost executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:560)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:535)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:863)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:843)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:473)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:67)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1346)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1047)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2468)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:628)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:631)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.EOFException\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:851)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2820)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2817)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2817)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1258)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1258)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3087)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3021)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3010)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:990)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2428)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2449)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2468)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1047)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1046)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:196)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:560)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:535)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:863)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:843)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:473)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:67)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1346)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1047)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2468)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:628)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:631)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.io.EOFException\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)\n\tat org
{code}

This is flaky https://github.com/apache/spark/actions/runs/6996322044/job/19032101353"	SPARK	Resolved	4	7	4739	pull-request-available
13569791	Uses hash key properly for SparkR build on Windows	"The cache is not being used

https://github.com/apache/spark/actions/runs/8039485831/job/21956555533"	SPARK	Resolved	3	4	4739	pull-request-available
13569299	Install proper Python version in SparkR Windows build to avoid warnings	"https://github.com/HyukjinKwon/spark/actions/runs/7985005685/job/21802732830

{code}
Traceback (most recent call last):
  File ""C:\hostedtoolcache\windows\Python\3.7.9\x64\lib\runpy.py"", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""C:\hostedtoolcache\windows\Python\3.7.9\x64\lib\runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""D:\a\spark\spark\python\lib\pyspark.zip\pyspark\__init__.py"", line [53](https://github.com/HyukjinKwon/spark/actions/runs/7985005685/job/21802732830#step:10:54), in <module>
  File ""D:\a\spark\spark\python\lib\pyspark.zip\pyspark\rdd.py"", line [54](https://github.com/HyukjinKwon/spark/actions/runs/7985005685/job/21802732830#step:10:55), in <module>
  File ""D:\a\spark\spark\python\lib\pyspark.zip\pyspark\java_gateway.py"", line 33, in <module>
  File ""D:\a\spark\spark\python\lib\pyspark.zip\pyspark\serializers.py"", line 69, in <module>
  File ""D:\a\spark\spark\python\lib\pyspark.zip\pyspark\cloudpickle\__init__.py"", line 1, in <module>
  File ""D:\a\spark\spark\python\lib\pyspark.zip\pyspark\cloudpickle\cloudpickle.py"", line 80, in <module>
ImportError: cannot import name 'CellType' from 'types' (C:\hostedtoolcache\windows\Python\3.7.9\x64\lib\types.py)
{code}"	SPARK	Resolved	3	6	4739	pull-request-available
13569123	"Deflake ""interrupt tag"" at SparkSessionE2ESuite"	"{code}
- interrupt tag *** FAILED ***
  The code passed to eventually never returned normally. Attempted 30 times over 20.037421464999998 seconds. Last failure message: ListBuffer(""2beba4ac-a994-45f5-bd46-fca3e43fb5ef"") had length 1 instead of expected length 2 Interrupted operations: ListBuffer(2beba4ac-a994-45f5-bd46-fca3e43fb5ef).. (SparkSessionE2ESuite.scala:216)
{code}

https://github.com/apache/spark/actions/runs/7959951623/job/21727929211"	SPARK	Resolved	3	6	4739	pull-request-available
13579939	Checkpoint and localCheckpoint in Scala Spark Connect client	SPARK-48258 implemented checkpoint and localcheckpoint in Python Spark Connect client. We should do it in Scala too.	SPARK	Resolved	3	4	4739	pull-request-available
13559251	Upgrade Cloudpickle to 3.0.0	It includes official support of Python 3.12 (https://github.com/cloudpipe/cloudpickle/pull/517)	SPARK	Resolved	3	7	4739	pull-request-available
13559626	Fix the doctest in pyspark.pandas.frame.DataFrame.to_dict (Python 3.12)	"{code}
File ""/__w/spark/spark/python/pyspark/pandas/frame.py"", line 2515, in pyspark.pandas.frame.DataFrame.to_dict
Failed example:
    df.to_dict(into=OrderedDict)
Expected:
    OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])), ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])
Got:
    OrderedDict({'col1': OrderedDict({'row1': 1, 'row2': 2}), 'col2': OrderedDict({'row1': 0.5, 'row2': 0.75})})
{code}"	SPARK	Resolved	4	7	4739	pull-request-available
13560925	Exclude generated files from the code coverage report	We should exclude https://app.codecov.io/gh/apache/spark/commit/1a651753f4e760643d719add3b16acd311454c76/tree/python/pyspark/sql/connect/proto	SPARK	Resolved	3	7	4739	pull-request-available
13105961	Type conflicts between dates, timestamps and date in partition column	"It looks we have some bugs when resolving type conflicts in partition column. I found few corner cases as below:

Case 1: timestamp should be inferred but date type is inferred.

{code}
val df = Seq((1, ""2015-01-01""), (2, ""2016-01-01 00:00:00"")).toDF(""i"", ""ts"")
df.write.format(""parquet"").partitionBy(""ts"").save(""/tmp/foo"")
spark.read.load(""/tmp/foo"").printSchema()
{code}

{code}
root
 |-- i: integer (nullable = true)
 |-- ts: date (nullable = true)
{code}

Case 2: decimal should be inferred but integer is inferred.

{code}
val df = Seq((1, ""1""), (2, ""1"" * 30)).toDF(""i"", ""decimal"")
df.write.format(""parquet"").partitionBy(""decimal"").save(""/tmp/bar"")
spark.read.load(""/tmp/bar"").printSchema()
{code}

{code}
root
 |-- i: integer (nullable = true)
 |-- decimal: integer (nullable = true)
{code}

Looks we should de-duplicate type resolution logic if possible rather than separate numeric precedence-like comparison alone."	SPARK	Resolved	4	1	4739	release-notes
13313142	PySpark <> Beam pickling issues for collections.namedtuple	"PySpark monkeypatching namedtuple makes it difficult/impossible to depickle collections.namedtuple instances from outside of a pyspark environment.

 

When PySpark has been loaded into the environment, any time that you try to pickle a namedtuple, you are only able to unpickle it from an environment where the [hijack|https://github.com/apache/spark/blob/master/python/pyspark/serializers.py#L385] has been applied. 

This conflicts directly when trying to use Beam from a non-Spark environment (namingly Flink or Dataflow) making it impossible to use the pipeline if it has a namedtuple loaded somewhere. 

 
{code:python}
import collections
import dill
ColumnInfo = collections.namedtuple(
    ""ColumnInfo"",
    [
        ""name"",  # type: ColumnName  # pytype: disable=ignored-type-comment
        ""type"",  # type: Optional[ColumnType]  # pytype: disable=ignored-type-comment
    ])
dill.dumps(ColumnInfo('test', int))
{code}

{{b'\x80\x03cdill._dill\n_create_namedtuple\nq\x00X\n\x00\x00\x00ColumnInfoq\x01X\x04\x00\x00\x00nameq\x02X\x04\x00\x00\x00typeq\x03\x86q\x04X\x08\x00\x00\x00__main__q\x05\x87q\x06Rq\x07X\x04\x00\x00\x00testq\x08cdill._dill\n_load_type\nq\tX\x03\x00\x00\x00intq\n\x85q\x0bRq\x0c\x86q\r\x81q\x0e.'}}
{code:python}
import pyspark
import collections
import dill
ColumnInfo = collections.namedtuple(
    ""ColumnInfo"",
    [
        ""name"",  # type: ColumnName  # pytype: disable=ignored-type-comment
        ""type"",  # type: Optional[ColumnType]  # pytype: disable=ignored-type-comment
    ])
dill.dumps(ColumnInfo('test', int))
{code}
{{b'\x80\x03cpyspark.serializers\n_restore\nq\x00X\n\x00\x00\x00ColumnInfoq\x01X\x04\x00\x00\x00nameq\x02X\x04\x00\x00\x00typeq\x03\x86q\x04X\x04\x00\x00\x00testq\x05cdill._dill\n_load_type\nq\x06X\x03\x00\x00\x00intq\x07\x85q\x08Rq\t\x86q\n\x87q\x0bRq\x0c.'}}


Second pickled object can only be used from an environment with PySpark. "	SPARK	Resolved	3	1	4739	release-notes
13582814	Add Origin to RelationCommon in protobuf defnition	SPARK-48459 adds the new protobuf message for Origin. We should reuse the definition in `RelationCommon` as well.	SPARK	Resolved	3	4	4739	pull-request-available
13577917	Different Arrow versions in client and server 	"{code}
======================================================================
FAIL [1.071s]: test_pandas_udf_arrow_overflow (pyspark.sql.tests.connect.test_parity_pandas_udf.PandasUDFParityTests.test_pandas_udf_arrow_overflow)
----------------------------------------------------------------------
pyspark.errors.exceptions.connect.PythonException: 
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 302, in _create_array
    return pa.Array.from_pandas(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow/array.pxi"", line 1054, in pyarrow.lib.Array.from_pandas
  File ""pyarrow/array.pxi"", line 323, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 83, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Integer value 128 not in range: -128 to 127

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1834, in main
    process()
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1826, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 531, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 104, in dump_stream
    for batch in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 525, in init_stream_yield_batches
    batch = self._create_batch(series)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 511, in _create_batch
    arrs.append(self._create_array(s, t, arrow_cast=self._arrow_cast))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 330, in _create_array
    raise PySparkValueError(error_msg % (series.dtype, series.na...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/pandas/test_pandas_udf.py"", line 299, in test_pandas_udf_arrow_overflow
    with self.assertRaisesRegex(
AssertionError: ""Exception thrown when converting pandas.Series"" does not match ""
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 302, in _create_array
    return pa.Array.from_pandas(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow/array.pxi"", line 1054, in pyarrow.lib.Array.from_pandas
  File ""pyarrow/array.pxi"", line 323, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 83, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Integer value 128 not in range: -128 to 127

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1834, in main
    process()
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1826, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 531, in dump_stream

Traceback (most recent call last):
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/pandas/test_pandas_udf.py"", line 279, in test_pandas_udf_detect_unsafe_type_conversion
    with self.assertRaisesRegex(
AssertionError: ""Exception thrown when converting pandas.Series"" does not match ""
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line [302](https://github.com/HyukjinKwon/spark/actions/runs/8916220872/job/24487232590#step:9:303), in _create_array
    return pa.Array.from_pandas(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""pyarrow/array.pxi"", line 1054, in pyarrow.lib.Array.from_pandas
  File ""pyarrow/array.pxi"", line 323, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 83, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Float value 0.5 was truncated converting to int32

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1834, in main
    process()
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1826, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 531, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 104, in dump_stream
    for batch in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 525, in init_stream_yield_batches
    batch = self._create_batch(series)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 511, in _create_batch
    arrs.append(self._create_array(s, t, arrow_cast=self._arrow_cast))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 330, in _create_array
    raise PySparkValueError(error_msg % (series.dtype, ser...""

----------------------------------------------------------------------
{code}

{code}

======================================================================
FAIL [0.162s]: test_vectorized_udf_exception (pyspark.sql.tests.connect.test_parity_pandas_udf_scalar.PandasUDFScalarParityTests.test_vectorized_udf_exception)
----------------------------------------------------------------------
pyspark.errors.exceptions.connect.PythonException: 
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1834, in main
    process()
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1826, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 531, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 104, in dump_stream
    for batch in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 524, in init_stream_yield_batches
    for series in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1734, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for arg_offsets, f in udfs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1734, in <genexpr>
    result = tuple(f(*[a[o] for o in arg_offsets]) for arg_offsets, f in udfs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 146, in <lambda>
    verify_result_length(verify_result_type(func(*a)), len(a[0])),
                                            ^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/util.py"", line 118, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/pandas/test_pandas_udf_scalar.py"", line 650, in <lambda>
    scalar_raise_exception = pandas_udf(lambda x: x * (1 / 0), LongType())
                                                       ~~^~...
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/connect/test_parity_pandas_udf_scalar.py"", line 35, in test_vectorized_udf_exception
    self.check_vectorized_udf_exception()
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/pandas/test_pandas_udf_scalar.py"", line 658, in check_vectorized_udf_exception
    with self.assertRaisesRegex(Exception, ""division( or modulo)? by zero""):
AssertionError: ""division( or modulo)? by zero"" does not match ""
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1834, in main
    process()
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1826, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 531, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 104, in dump_stream
    for batch in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 524, in init_stream_yield_batches
    for series in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1734, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for arg_offsets, f in udfs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1734, in <genexpr>
    result = tuple(f(*[a[o] for o in arg_offsets]) for arg_offsets, f in udfs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 146, in <lambda>
    verify_result_length(verify_result_type(func(*a)), len(a[0])),
                                            ^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/util.py"", line 118, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/pandas/test_pandas_udf_scalar.py"", line 650, in <lambda>
    scalar_raise_exception = pandas_udf(lambda x: x * (1 / 0), LongType())
                                                       ~~^~...""
----------------------------------------------------------------------
{code}"	SPARK	Resolved	3	7	4739	pull-request-available
13563827	Log full exception when failed to lookup Python Data Sources	See https://github.com/apache/spark/pull/44617	SPARK	Resolved	4	7	4739	pull-request-available
13559991	Move MockDF under PlanOnlyTestFixture	"
When you don't have the dependencies for Spark Connect, and you run the tests:

{code}
 ./python/run-tests --python-executables=python3  --testnames 'pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby_rolling'
{code}

it shows the error as below:

{code}
  File ""/.../spark/python/pyspark/pandas/tests/connect/test_parity_ops_on_diff_frames_groupby_rolling.py"", line 22, in <module>
    from pyspark.testing.connectutils import ReusedConnectTestCase
  File ""/.../spark/python/pyspark/testing/connectutils.py"", line 92, in <module>
    class PlanOnlyTestFixture(unittest.TestCase, PySparkErrorTestUtils):
  File ""/.../spark/python/pyspark/testing/connectutils.py"", line 93, in PlanOnlyTestFixture
    class MockDF(DataFrame):
                 ^^^^^^^^^
NameError: name 'DataFrame' is not defined
{code}

which isn't actionable."	SPARK	Resolved	4	6	4739	pull-request-available
13569416	Skip scheduled SparkR on Windows in fork repositories by default	To be consistent with other scheduled build.	SPARK	Resolved	4	6	4739	pull-request-available
13597313	Use latest Python dependencies for Python client3.5 <> 4.0server job	https://github.com/apache/spark/actions/runs/11543780375/job/32128570166 fails apparently because of different grpc versions. While it is a legitimate failure to fix, that CI does not target to test all different dependency versions. It only targets 3.5cleint <> 4.0server. We should better make the test on its purpose.	SPARK	Resolved	3	7	4739	pull-request-available
13581856	Make StreamingQueryListener.spark settable	Downstream users might already implement StreamingQueryListener.spark.	SPARK	Resolved	3	4	4739	pull-request-available
13559135	Improve error messages related to argument types in cute, rollup, groupby, and pivot	"{code}
>>> spark.range(1).cube(cols=1.2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../python/pyspark/sql/connect/dataframe.py"", line 544, in cube
    raise PySparkTypeError(
pyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `cube` should be a Column or str, got float.
{code}

{code}
>>> help(spark.range(1).cube)
Help on method cube in module pyspark.sql.connect.dataframe:

cube(*cols: 'ColumnOrName') -> 'GroupedData' method of pyspark.sql.connect.dataframe.DataFrame instance
    Create a multi-dimensional cube for the current :class:`DataFrame` using
    the specified columns, allowing aggregations to be performed on them.

    .. versionadded:: 1.4.0

    .. versionchanged:: 3.4.0
{code}

it has to be {cols}"	SPARK	Resolved	4	4	4739	pull-request-available
13174834	Complex type and binary type in in-memory partition pruning does not work due to missing upper/lower bounds cases	"For example, if array is used (where the lower and upper bounds for its column batch are {{null}})), it looks wrongly filtering all data out:

{code}
scala> import org.apache.spark.sql.functions
import org.apache.spark.sql.functions

scala> val df = Seq(Array(""a"", ""b""), Array(""c"", ""d"")).toDF(""arrayCol"")
df: org.apache.spark.sql.DataFrame = [arrayCol: array<string>]

scala> df.filter(df.col(""arrayCol"").eqNullSafe(functions.array(functions.lit(""a""), functions.lit(""b"")))).show()
+--------+
|arrayCol|
+--------+
|  [a, b]|
+--------+


scala> df.cache().filter(df.col(""arrayCol"").eqNullSafe(functions.array(functions.lit(""a""), functions.lit(""b"")))).show()
+--------+
|arrayCol|
+--------+
+--------+
{code}"	SPARK	Resolved	2	1	4739	correctness
13551815	Remove workaround for dateformatter added in SPARK-31827	We dropped JDK 8 at SPARK-44112, and we don't need the workaround for SPARK-31827 anymore.	SPARK	Resolved	3	7	4739	pull-request-available
13551814	Comment out unused JDK 11 related in dev/run-tests.py	"{code}
    # set up java11 env if this is a pull request build with 'test-java11' in the title
    if ""ghprbPullTitle"" in os.environ:
        if ""test-java11"" in os.environ[""ghprbPullTitle""].lower():
            os.environ[""JAVA_HOME""] = ""/usr/java/jdk-11.0.1""
            os.environ[""PATH""] = ""%s/bin:%s"" % (os.environ[""JAVA_HOME""], os.environ[""PATH""])
            test_profiles += [""-Djava.version=11""]
{code}

we don't need this anymore"	SPARK	Resolved	3	7	4739	pull-request-available
13561098	Fix the output name of pyspark.sql.functions.user and session_user	"{code}
scala> spark.range(1).select(user()).show()
+--------------+
|current_user()|
+--------------+
|  hyukjin.kwon|
+--------------+
{code}"	SPARK	Resolved	4	1	4739	pull-request-available
13551813	Remove Utils.isMemberClass workaround for JDK 8	We dropped JDK 8 and 11 at SPARK-44112. We don't need the workaround anymore	SPARK	Resolved	3	7	4739	pull-request-available
13581131	Support interruptTag and interruptAll in streaming queries	Spark Connect's interrupt API does not interrupt streaming queries. We should support them.	SPARK	Resolved	3	4	4739	pull-request-available
13574955	Add pyspark.sql.connect.protobuf into setup.py	We should add them.They are missing in pypi package.	SPARK	Resolved	3	1	4739	pull-request-available
13025234	Test Java 8 unidoc build on Jenkins master builder	[SPARK-3359] fixed the unidoc build for Java 8, but it is easy to break.  It would be great to add this build to the Spark master builder on Jenkins to make it easier to identify PRs which break doc builds.	SPARK	Resolved	3	6	4739	jenkins
13558944	Reenable a `releaseSession` test case in SparkConnectServiceE2ESuite	https://github.com/apache/spark/pull/43942#issuecomment-1821896165	SPARK	Resolved	3	7	4739	pull-request-available
13588534	Embed script level parsing logic into SparkSubmitCommandBuilder	Embed the logics in script to JVM, see https://github.com/apache/spark/pull/47402	SPARK	Resolved	3	7	4739	pull-request-available
13596102	Make pysaprk-connect tests passing without optional dependencies	https://github.com/apache/spark/actions/runs/11420354598/job/31775990587	SPARK	Resolved	3	7	4739	pull-request-available
13578848	Fix 'pyspark.sql.tests.connect.test_connect_session' in Python 3.12 build	https://github.com/apache/spark/actions/runs/9022174253/job/24804919747	SPARK	Resolved	3	6	4739	pull-request-available
13551575	Refine docstring of `count`	Refine the docstring of the function `count` (e.g provide examples with groupby)	SPARK	Resolved	3	7	4739	pull-request-available
13553214	Fix imports according to PEP8: pyspark.pandas and pyspark (core)	https://peps.python.org/pep-0008/#imports	SPARK	Resolved	3	4	4739	pull-request-available
13580092	Remove obsolete workflow cancel_duplicate_workflow_runs	After https://github.com/apache/spark/pull/46689, we don't need this anymore	SPARK	Resolved	3	4	4739	pull-request-available
13562042	Support PythonSQLMetrics.pythonMetrics	We should show the stats such as `pythonDataSent`, `pythonDataReceived` and `pythonNumRowsReceived`.	SPARK	Resolved	4	7	4739	pull-request-available
13560378	Test invalid error class (pyspark.errors.utils)	https://app.codecov.io/gh/apache/spark/blob/master/python%2Fpyspark%2Ferrors%2Futils.py	SPARK	Resolved	4	7	4739	pull-request-available
13562050	Pin the bundler version in CI	"https://github.com/apache/spark/actions/runs/7226413850/job/19691970695

{code}
Requirement already satisfied: docutils<0.18.0 in /usr/local/lib/python3.9/dist-packages (0.17.1)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
ERROR:  Error installing bundler:
	The last version of bundler (>= 0) to support your Ruby & RubyGems was 2.4.22. Try installing it with `gem install bundler -v 2.4.22`
	bundler requires Ruby version >= 3.0.0. The current ruby version is 2.7.0.0.
{code}"	SPARK	Resolved	3	4	4739	pull-request-available
13585611	In spark ML, replace RDD read / write API invocation with Dataframe read / write API	"In spark ML, replace RDD read / write API invocation with Dataframe read / write API.

 

Motivation:

In databricks runtime, RDD read / write API has some issue for certain storage types that requires the account key, but Dataframe read / write API works."	SPARK	Resolved	3	4	4739	pull-request-available
13574216	Separate pure Python packaging	Initial version.	SPARK	Resolved	3	7	4739	pull-request-available
13568532	Remove pinned version of torch for Python 3.12 support	Basically a revert of SPARK-45409	SPARK	Resolved	3	7	4739	pull-request-available
13414942	Uses Python's standard string formatter for SQL API in PySpark	This is similar with SPARK-37436. It aims to add the Python string formatter support for {{SparkSession.sql}} API.	SPARK	Resolved	3	4	4739	release-notes
13375470	SQL DataFrameReader unescapedQuoteHandling parameter is misdocumented	"The unescapedQuoteHandling parameter of DataFrameReader isn't correctly documented. STOP_AT_DELIMITER appears twice, and it looks like that's overwritten an intended option, e.g. [https://github.com/apache/spark/blob/1d550c4e90275ab418b9161925049239227f3dc9/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L744-L749]

To view instances where this error occurs, this is a useful query [https://github.com/apache/spark/search?q=STOP_AT_DELIMITER]

It appears that this bug was introduced here: [https://github.com/apache/spark/pull/30518|https://github.com/apache/spark/pull/30518,]"	SPARK	Resolved	3	20	4739	starter
13279420	Publicly document options under spark.sql.*	"SPARK-20236 added a new option, {{spark.sql.sources.partitionOverwriteMode}}, but it doesn't appear to be documented in [the expected place|http://spark.apache.org/docs/2.4.4/configuration.html]. In fact, none of the options under {{spark.sql.*}} that are intended for users are documented on spark.apache.org/docs.

We should add a new documentation page for these options."	SPARK	Resolved	4	4	4739	release-notes
13588345	Update Spark Shell documentation with Spark Connect	Documentation update by SPARK-48936	SPARK	Resolved	3	7	4739	pull-request-available
13569120	Upgrade Python to 3.11 in Maven builds	"{code}
  Error: dyld[4738]: Library not loaded: /usr/local/opt/gettext/lib/libintl.8.dylib
    Referenced from: <E2FD7085-038F
  Error: -3B8E-94C6-6649527BFDBE> /Users/runner/hostedtoolcache/Python/3.9.18/x64/bin/python3.9
    Reason: tried: '/usr/local/opt/gettext/lib/libintl.8.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/gettext/lib/libintl.8.dylib' (no such file), '/usr/local/opt/gettext/lib/libintl.8.dylib' (no such file), '/usr/local/lib/libintl.8.dylib' (no such file), '/usr/lib/libintl.8.dylib' (no such file, not in dyld cache)
  ./setup.sh: line 52:  4738 Abort trap: 6           ./python -m ensurepip
{code}

https://github.com/apache/spark/actions/runs/7964626045/job/21742574260

https://github.com/actions/runner-images/blob/main/images/macos/macos-14-Readme.md"	SPARK	Resolved	3	7	4739	pull-request-available
13551824	Remove test classloader workaround for SBT build	Revert https://github.com/apache/spark/pull/30198. We don't need it anymore since we dropped JDK 8 and 11.	SPARK	Resolved	4	7	4739	pull-request-available
13579918	Fix lint-scala for scalafmt to detect properly	"{code}
./build/mvn \
    -Pscala-2.13 \
    scalafmt:format \
    -Dscalafmt.skip=false \
    -Dscalafmt.validateOnly=true \
    -Dscalafmt.changedOnly=false \
    -pl connector/connect/common \
    -pl connector/connect/server \
    -pl connector/connect/client/jvm
{code}

fails as below:

{code}
[INFO] Scalafmt results: 1 of 36 were unformatted
[INFO] Details:
[INFO] - Requires formatting: ConnectProtoUtils.scala
[INFO] - Formatted: UdfUtils.scala
[INFO] - Formatted: DataTypeProtoConverter.scala
[INFO] - Formatted: ConnectCommon.scala
[INFO] - Formatted: ProtoUtils.scala
[INFO] - Formatted: Abbreviator.scala
[INFO] - Formatted: ProtoDataTypes.scala
[INFO] - Formatted: LiteralValueProtoConverter.scala
[INFO] - Formatted: InvalidPlanInput.scala
[INFO] - Formatted: ForeachWriterPacket.scala
[INFO] - Formatted: StreamingListenerPacket.scala
[INFO] - Formatted: StorageLevelProtoConverter.scala
[INFO] - Formatted: UdfPacket.scala
[INFO] - Formatted: ClassFinder.scala
[INFO] - Formatted: SparkConnectClient.scala
[INFO] - Formatted: GrpcRetryHandler.scala
[INFO] - Formatted: GrpcExceptionConverter.scala
[INFO] - Formatted: ArrowEncoderUtils.scala
[INFO] - Formatted: ScalaCollectionUtils.scala
[INFO] - Formatted: ArrowDeserializer.scala
[INFO] - Formatted: ArrowVectorReader.scala
[INFO] - Formatted: ArrowSerializer.scala
[INFO] - Formatted: ConcatenatingArrowStreamReader.scala
[INFO] - Formatted: RetryPolicy.scala
[INFO] - Formatted: SparkConnectStubState.scala
[INFO] - Formatted: ArtifactManager.scala
[INFO] - Formatted: SparkResult.scala
[INFO] - Formatted: RetriesExceeded.scala
[INFO] - Formatted: CloseableIterator.scala
[INFO] - Formatted: package.scala
[INFO] - Formatted: ExecutePlanResponseReattachableIterator.scala
[INFO] - Formatted: ResponseValidator.scala
[INFO] - Formatted: SparkConnectClientParser.scala
[INFO] - Formatted: CustomSparkConnectStub.scala
[INFO] - Formatted: CustomSparkConnectBlockingStub.scala
[INFO] - Formatted: TestUDFs.scala
{code}

This is because the output format has changed due to scalafmt version upgrade."	SPARK	Resolved	3	1	4739	pull-request-available
13579001	Fix nested array to respect legacy conf of inferArrayTypeFromFirstElement	"{code}
>>> spark.conf.set(""spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled"", True)
>>> spark.createDataFrame([[[[1, ""a""]]]])
DataFrame[_1: array<array<string>>]
{code}

should infer it as an integer of array"	SPARK	Resolved	3	1	4739	pull-request-available
13558575	Show proper dependency requirement messages for Spark Connect	"
{code}
./bin/pyspark --remote local
{code}

We should improve the error messages below.

{code}
/.../pyspark/shell.py:57: UserWarning: Failed to initialize Spark session.
  warnings.warn(""Failed to initialize Spark session."")
Traceback (most recent call last):
  File ""/.../pyspark/shell.py"", line 52, in <module>
    spark = SparkSession.builder.getOrCreate()
  File ""/.../pyspark/sql/session.py"", line 476, in getOrCreate
    from pyspark.sql.connect.session import SparkSession as RemoteSparkSession
  File ""/.../pyspark/sql/connect/session.py"", line 53, in <module>
    from pyspark.sql.connect.client import SparkConnectClient, ChannelBuilder
  File ""/.../pyspark/sql/connect/client/__init__.py"", line 22, in <module>
    from pyspark.sql.connect.client.core import *  # noqa: F401,F403
  File ""/.../pyspark/sql/connect/client/core.py"", line 51, in <module>
    import google.protobuf.message
ModuleNotFoundError: No module named 'google
{code}

{code}
/.../pyspark/shell.py:57: UserWarning: Failed to initialize Spark session.
  warnings.warn(""Failed to initialize Spark session."")
Traceback (most recent call last):
  File ""/.../pyspark/shell.py"", line 52, in <module>
    spark = SparkSession.builder.getOrCreate()
  File ""/.../pyspark/sql/session.py"", line 476, in getOrCreate
    from pyspark.sql.connect.session import SparkSession as RemoteSparkSession
  File ""/.../pyspark/sql/connect/session.py"", line 53, in <module>
    from pyspark.sql.connect.client import SparkConnectClient, ChannelBuilder
  File ""/.../pyspark/sql/connect/client/__init__.py"", line 22, in <module>
    from pyspark.sql.connect.client.core import *  # noqa: F401,F403
  File ""/.../pyspark/sql/connect/client/core.py"", line 52, in <module>
    from grpc_status import rpc_status
ModuleNotFoundError: No module named 'grpc_status'
{code}


"	SPARK	Resolved	3	4	4739	pull-request-available
13551820	Remove org.scala-lang scala-library added for JDK 11 workaround 	"https://github.com/apache/spark/pull/25800 added 

{code}
    <!-- SPARK-28932 This is required in JDK11 -->
    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-library</artifactId>
    </dependency>
{code}

Now with JDK 17 it works without them"	SPARK	Resolved	4	7	4739	pull-request-available
13553212	Fix imports according to PEP8: pyspark.testing, pyspark.mllib, pyspark.resource and pyspark.streaming	https://peps.python.org/pep-0008/#imports	SPARK	Resolved	3	4	4739	pull-request-available
13562030	Refactor Python Data Source instance loading	we should make the instance in lookupDataSourceV2 instead.	SPARK	Resolved	3	7	4739	pull-request-available
13559994	Split scheduled Python build	"Python 3.12 build fails as below:

{code}
/__w/spark/spark/python/pyspark/pandas/utils.py:1015: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas Series is expected to be small.
  warnings.warn(message, PandasAPIOnSparkAdviceWarning)
/__w/spark/spark/python/pyspark/testing/pandasutils.py:401: FutureWarning: `assertPandasOnSparkEqual` will be removed in Spark 4.0.0. Use `ps.testing.assert_frame_equal`, `ps.testing.assert_series_equal` and `ps.testing.assert_index_equal` instead.
  warnings.warn(
/__w/spark/spark/python/pyspark/pandas/utils.py:1015: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.
  warnings.warn(message, PandasAPIOnSparkAdviceWarning)
ok (15.809s)
  test_groupby_rolling_sum (pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby_rolling.OpsOnDiffFramesGroupByRollingParityTests.test_groupby_rolling_sum) ... ERROR StatusCo
Had test failures in pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby_rolling with python3.12; see logs.
Error:  running /__w/spark/spark/python/run-tests --modules=pyspark-pandas-connect-part2 --parallelism=1 --python-executables=pypy3,python3.10,python3.11,python3.12 ; received return code 255
Error: Process completed with exit code 19.
{code}

https://github.com/apache/spark/actions/runs/7034467411/job/19154767856

I suspect this by OOM. We could split the build."	SPARK	Resolved	3	7	4739	pull-request-available
13204256	Remove 'spark.driver.allowMultipleContexts' to disallow multiple Spark contexts	"Multiple Spark contexts are discouraged and it has been warning from 4 years ago (see SPARK-4180).

It could cause arbitrary and mysterious error cases. (Honestly, I didn't even know Spark allows it). "	SPARK	Resolved	4	4	4739	release-notes
13558315	Restore documentation for DSv2 API	DSv2 documentation is mistakenly gone after https://github.com/apache/spark/pull/38392. It used to exist in 3.3.0: https://spark.apache.org/docs/3.3.0/api/scala/org/apache/spark/sql/connector/catalog/index.html	SPARK	Resolved	3	1	4739	pull-request-available
13577918	Python UDTF incompatibility in 3.5 client <> 4.0 server	"{code}
======================================================================
FAIL [0.103s]: test_udtf_init_with_additional_args (pyspark.sql.tests.connect.test_parity_udtf.ArrowUDTFParityTests.test_udtf_init_with_additional_args)
----------------------------------------------------------------------
pyspark.errors.exceptions.connect.PythonException: 
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1816, in main
    func, profiler, deserializer, serializer = read_udtf(pickleSer, infile, eval_type)
    self._check_result_or_exception(TestUDTF, ret_type, expected)
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/test_udtf.py"", line 598, in _check_result_or_exception
    with self.assertRaisesRegex(err_type, expected):
AssertionError: ""AttributeError"" does not match ""
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1834, in main
    process()
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1826, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 224, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 145, in dump_stream
    for obj in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 213, in _batched
    for item in iterator:
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1391, in mapper
    yield eval(*[a[o] for o in args_kwargs_offsets])
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1371, in evaluate
    return tuple(map(verify_and_convert_result, res))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1340, in verify_and_convert_result
    return toInternal(result)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py"", line 1291, in toInternal
    return tuple(
           ^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py"", line 1292, in <genexpr>
    f.toInternal(v) if c else v
    ^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py"", line 907, in toInternal
    return self.dataType.toInternal(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/sql/types.py"", line 372, in toInternal
    calendar.timegm(dt.utctimetuple()) if dt.tzinfo else time.mktime(dt.timetuple())
            ...""
{code}

{code}

======================================================================
FAIL [0.096s]: test_udtf_init_with_additional_args (pyspark.sql.tests.connect.test_parity_udtf.UDTFParityTests.test_udtf_init_with_additional_args)
----------------------------------------------------------------------
pyspark.errors.exceptions.connect.PythonException: 
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1816, in main
    func, profiler, deserializer, serializer = read_udtf(pickleSer, infile, eval_type)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 946, in read_udtf
    raise PySparkRuntimeError(
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_CONSTRUCTOR_INVALID_NO_ANALYZE_METHOD] Failed to evaluate the user-defined table function 'TestUDTF' because its constructor is invalid: the function does not implement the 'analyze' method, and its constructor has more than one argument (including the 'self' reference). Please update the table function so that its constructor accepts exactly one 'self' argument, and try the query again.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/test_udtf.py"", line 274, in test_udtf_init_with_additional_args
    with self.assertRaisesRegex(
AssertionError: ""__init__\(\) missing 1 required positional argument: 'a'"" does not match ""
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1816, in main
    func, profiler, deserializer, serializer = read_udtf(pickleSer, infile, eval_type)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 946, in read_udtf
    raise PySparkRuntimeError(
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_CONSTRUCTOR_INVALID_NO_ANALYZE_METHOD] Failed to evaluate the user-defined table function 'TestUDTF' because its constructor is invalid: the function does not implement the 'analyze' method, and its constructor has more than one argument (including the 'self' reference). Please update the table function so that its constructor accepts exactly one 'self' argument, and try the query again.
""
{code}



{code}

======================================================================
FAIL [0.087s]: test_udtf_with_wrong_num_input (pyspark.sql.tests.connect.test_parity_udtf.UDTFParityTests.test_udtf_with_wrong_num_input)
----------------------------------------------------------------------
pyspark.errors.exceptions.connect.PythonException: 
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1816, in main
    func, profiler, deserializer, serializer = read_udtf(pickleSer, infile, eval_type)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1082, in read_udtf
    raise PySparkRuntimeError(
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EVAL_METHOD_ARGUMENTS_DO_NOT_MATCH_SIGNATURE] Failed to evaluate the user-defined table function 'TestUDTF' because the function arguments did not match the expected signature of the 'eval' method (missing a required argument: 'a'). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its 'eval' method accepts the provided arguments, and then try the query again.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark-3.5/python/pyspark/sql/tests/test_udtf.py"", line 255, in test_udtf_with_wrong_num_input
    with self.assertRaisesRegex(
AssertionError: ""eval\(\) missing 1 required positional argument: 'a'"" does not match ""
  An exception was thrown from the Python worker. Please see the stack trace below.
Traceback (most recent call last):
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1816, in main
    func, profiler, deserializer, serializer = read_udtf(pickleSer, infile, eval_type)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/runner/work/spark/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 1082, in read_udtf
    raise PySparkRuntimeError(
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EVAL_METHOD_ARGUMENTS_DO_NOT_MATCH_SIGNATURE] Failed to evaluate the user-defined table function 'TestUDTF' because the function arguments did not match the expected signature of the 'eval' method (missing a required argument: 'a'). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its 'eval' method accepts the provided arguments, and then try the query again.
""
----------------------------------------------------------------------
{code}

"	SPARK	Resolved	3	7	4739	pull-request-available
13560596	Enable test for np.left_shift for Pandas-on-Spark object.	Now we support PyArrow>=4.0.0, we can enable the test for `np.left_shift`.	SPARK	Resolved	3	1	5221	pull-request-available
13559476	Add copyright to the PySpark official documentation.	Add copyright to the PySpark official documentation by using Sphinx extension.	SPARK	Resolved	3	1	5221	pull-request-available
13536150	Enable StatsTests.test_axis_on_dataframe for pandas 2.0.0.	Enable StatsTests.test_axis_on_dataframe for pandas 2.0.0.	SPARK	Resolved	3	7	5221	pull-request-available
13535787	Enable SeriesConversionTests.test_to_latex for pandas 2.0.0.	Enable SeriesConversionTests.test_to_latex for pandas 2.0.0.	SPARK	Resolved	3	7	5221	pull-request-available
13574752	Upgrade the minimum version of PyArrow to 10.0.0	For more rich API support	SPARK	Resolved	3	7	5221	pull-request-available
13559152	Refactor `(DataFrame|Series).factorize()` to use `create_map`.	We can accept Column object for Column.__getitem__ on remote Session, so we can optimize the existing factorize implementation.	SPARK	Resolved	3	1	5221	pull-request-available
13560980	Enable `fill_value` tests for `GroupByTests.test_shift`	Enable `fill_value` tests for `GroupByTests.test_shift` since the bug from Pandas is fixed.	SPARK	Resolved	3	1	5221	pull-request-available
13587273	ErrorClassesJsonReader cannot handle null properly	ErrorClassesJsonReader raises INTERNAL_ERROR when getting null so we should make it work properly.	SPARK	Resolved	3	1	5221	pull-request-available
13593953	API compatibility check for Catalog	We should ensure all the existing functionalities from Catalog working in the same way on Spark Connect, and also identifying the missing APIs and broken features, etc.	SPARK	Resolved	3	7	5221	pull-request-available
13575580	Missing warnings for deprecated features	There are some APIs will be removed but missing deprecation warnings	SPARK	Resolved	3	1	5221	pull-request-available
13578536	Update error contribution guide to respect new error class file	We moved error class definition from .py to .json but documentation still shows old behavior. We should update it.	SPARK	Resolved	3	1	5221	pull-request-available
13558800	Remove deprecated functions APIs from documents	We should not expose the deprecated APIs on official documentation.	SPARK	Resolved	3	1	5221	pull-request-available
13564267	Remove assertPandasOnSparkEqual	Remove deprecated API	SPARK	Resolved	3	1	5221	pull-request-available
13587192	Raise proper error for dropDuplicates when wrong subset is given	"Currently dropDuplicates raise unrelated internal error so we should improve it
{code:java}
>>> df.dropDuplicates(None)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/classic/dataframe.py"", line 1249, in dropDuplicates
    jdf = self._jdf.dropDuplicates(self._jseq(subset))
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1322, in __call__
    return_value = get_return_value(
  File ""/.../spark/python/pyspark/errors/exceptions/captured.py"", line 247, in deco
    return f(*a, **kw)
  File ""/.../spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o56.dropDuplicates.
: org.apache.spark.SparkException: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1201', MessageTemplate: Cannot resolve column name ""<colName>"" among (<fieldNames>)., Parameters: Map(colName -> null, fieldNames -> name, age) SQLSTATE: XX000
    at org.apache.spark.SparkException$.internalError(SparkException.scala:107)
    at org.apache.spark.ErrorClassesJsonReader.getErrorMessage(ErrorClassesJSONReader.scala:58)
    at org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:56)
    at org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:43)
    at org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:47)
    at org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:82)
    at org.apache.spark.sql.errors.QueryCompilationErrors$.cannotResolveColumnNameAmongAttributesError(QueryCompilationErrors.scala:2257)
    at org.apache.spark.sql.Dataset.$anonfun$groupColsFromDropDuplicates$1(Dataset.scala:3267)
    at scala.collection.immutable.List.flatMap(List.scala:294)
    at scala.collection.immutable.List.flatMap(List.scala:79)
    at org.apache.spark.sql.Dataset.groupColsFromDropDuplicates(Dataset.scala:3261)
    at org.apache.spark.sql.Dataset.dropDuplicates(Dataset.scala:3126)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.IllegalArgumentException: Cannot resolve variable 'colName' (enableSubstitutionInVariables=false).
    at org.apache.commons.text.StringSubstitutor.substitute(StringSubstitutor.java:1535)
    at org.apache.commons.text.StringSubstitutor.substitute(StringSubstitutor.java:1392)
    at org.apache.commons.text.StringSubstitutor.replace(StringSubstitutor.java:896)
    at org.apache.spark.ErrorClassesJsonReader.getErrorMessage(ErrorClassesJSONReader.scala:53)
    ... 22 more {code}"	SPARK	Resolved	3	1	5221	pull-request-available
13559388	"Refactor ""Supported pandas API"" generation script"	"The current ""python/pyspark/pandas/supported_api_gen.py"" is complex to understand due to lack of docstring and unclear variable naming, etc. We should make it easier for further maintenance."	SPARK	Resolved	3	1	5221	pull-request-available
13583006	Display correct call site from IPython Notebook	Current IPython Notebook does not show proper DataFrameQueryContext	SPARK	Resolved	3	1	5221	pull-request-available
13548949	Reducing the CI time for slow pyspark-pandas-connect tests	pyspark-pandas-connect test takes more than 3 hours in Github Actions, so we might need to reduce the execution time. See https://github.com/apache/spark/actions/runs/5989124806/job/16245001034	SPARK	Resolved	3	6	5221	pull-request-available
13538049	Enable GroupByTests for pandas 2.0.0.	"test list:
 * test_prod
 * test_nth
 * test_mad
 * test_basic_stat_funcs
 * test_groupby_multiindex_columns
 * test_apply_without_shortcut
 * test_mean
 * test_apply"	SPARK	Resolved	3	7	5221	pull-request-available
13535588	Match `GroupBy.nth` behavior with new pandas behavior	Match behavior with https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#dataframegroupby-nth-and-seriesgroupby-nth-now-behave-as-filtrations	SPARK	Resolved	3	7	5221	pull-request-available
13584173	Introduce `pyspark.logging` for improved structured logging for PySpark	"We introduced structured logging from [#45729|https://github.com/apache/spark/pull/45729], but PySpark log is still hard to figure out in the current structured log, because it is hidden and mixed within bunch of complex JVM stacktraces and it's also not very Python-friendly:

 
{code:java}
{
  ""ts"": ""2024-06-28T10:53:48.528Z"",
  ""level"": ""ERROR"",
  ""msg"": ""Exception in task 7.0 in stage 0.0 (TID 7)"",
  ""context"": {
    ""task_name"": ""task 7.0 in stage 0.0 (TID 7)""
  },
  ""exception"": {
    ""class"": ""org.apache.spark.SparkArithmeticException"",
    ""msg"": ""[DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \""spark.sql.ansi.enabled\"" to \""false\"" to bypass this error. SQLSTATE: 22012\n== DataFrame ==\n\""__truediv__\"" was called from\n/.../spark/python/test_error_context.py:17\n"",
    ""stacktrace"": [
      {
        ""class"": ""org.apache.spark.sql.errors.QueryExecutionErrors$"",
        ""method"": ""divideByZeroError"",
        ""file"": ""QueryExecutionErrors.scala"",
        ""line"": 203
      },
      {
        ""class"": ""org.apache.spark.sql.errors.QueryExecutionErrors"",
        ""method"": ""divideByZeroError"",
        ""file"": ""QueryExecutionErrors.scala"",
        ""line"": -1
      },
      {
        ""class"": ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1"",
        ""method"": ""project_doConsume_0$"",
        ""file"": null,
        ""line"": -1
      },
      {
        ""class"": ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1"",
        ""method"": ""processNext"",
        ""file"": null,
        ""line"": -1
      },
      {
        ""class"": ""org.apache.spark.sql.execution.BufferedRowIterator"",
        ""method"": ""hasNext"",
        ""file"": ""BufferedRowIterator.java"",
        ""line"": 43
      },
      {
        ""class"": ""org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1"",
        ""method"": ""hasNext"",
        ""file"": ""WholeStageCodegenEvaluatorFactory.scala"",
        ""line"": 50
      },
      {
        ""class"": ""org.apache.spark.sql.execution.SparkPlan"",
        ""method"": ""$anonfun$getByteArrayRdd$1"",
        ""file"": ""SparkPlan.scala"",
        ""line"": 388
      },
      {
        ""class"": ""org.apache.spark.rdd.RDD"",
        ""method"": ""$anonfun$mapPartitionsInternal$2"",
        ""file"": ""RDD.scala"",
        ""line"": 896
      },
      {
        ""class"": ""org.apache.spark.rdd.RDD"",
        ""method"": ""$anonfun$mapPartitionsInternal$2$adapted"",
        ""file"": ""RDD.scala"",
        ""line"": 896
      },
      {
        ""class"": ""org.apache.spark.rdd.MapPartitionsRDD"",
        ""method"": ""compute"",
        ""file"": ""MapPartitionsRDD.scala"",
        ""line"": 52
      },
      {
        ""class"": ""org.apache.spark.rdd.RDD"",
        ""method"": ""computeOrReadCheckpoint"",
        ""file"": ""RDD.scala"",
        ""line"": 369
      },
      {
        ""class"": ""org.apache.spark.rdd.RDD"",
        ""method"": ""iterator"",
        ""file"": ""RDD.scala"",
        ""line"": 333
      },
      {
        ""class"": ""org.apache.spark.scheduler.ResultTask"",
        ""method"": ""runTask"",
        ""file"": ""ResultTask.scala"",
        ""line"": 93
      },
      {
        ""class"": ""org.apache.spark.TaskContext"",
        ""method"": ""runTaskWithListeners"",
        ""file"": ""TaskContext.scala"",
        ""line"": 171
      },
      {
        ""class"": ""org.apache.spark.scheduler.Task"",
        ""method"": ""run"",
        ""file"": ""Task.scala"",
        ""line"": 146
      },
      {
        ""class"": ""org.apache.spark.executor.Executor$TaskRunner"",
        ""method"": ""$anonfun$run$5"",
        ""file"": ""Executor.scala"",
        ""line"": 644
      },
      {
        ""class"": ""org.apache.spark.util.SparkErrorUtils"",
        ""method"": ""tryWithSafeFinally"",
        ""file"": ""SparkErrorUtils.scala"",
        ""line"": 64
      },
      {
        ""class"": ""org.apache.spark.util.SparkErrorUtils"",
        ""method"": ""tryWithSafeFinally$"",
        ""file"": ""SparkErrorUtils.scala"",
        ""line"": 61
      },
      {
        ""class"": ""org.apache.spark.util.Utils$"",
        ""method"": ""tryWithSafeFinally"",
        ""file"": ""Utils.scala"",
        ""line"": 99
      },
      {
        ""class"": ""org.apache.spark.executor.Executor$TaskRunner"",
        ""method"": ""run"",
        ""file"": ""Executor.scala"",
        ""line"": 647
      },
      {
        ""class"": ""java.util.concurrent.ThreadPoolExecutor"",
        ""method"": ""runWorker"",
        ""file"": ""ThreadPoolExecutor.java"",
        ""line"": 1136
      },
      {
        ""class"": ""java.util.concurrent.ThreadPoolExecutor$Worker"",
        ""method"": ""run"",
        ""file"": ""ThreadPoolExecutor.java"",
        ""line"": 635
      },
      {
        ""class"": ""java.lang.Thread"",
        ""method"": ""run"",
        ""file"": ""Thread.java"",
        ""line"": 840
      }
    ]
  },
  ""logger"": ""Executor""
} {code}"	SPARK	Resolved	3	1	5221	pull-request-available
13537071	Enable InternalFrameParityTests.test_from_pandas	Enable InternalFrameParityTests.test_from_pandas	SPARK	Resolved	3	7	5221	pull-request-available
13537037	Enable SparkContext-related tests with Spark Connect	Enable SparkContext with Spark Connect	SPARK	Resolved	3	7	5221	pull-request-available
13560279	Use a narrower scope exception for SQL processor	Current exception handling in these functions uses the general {{Exception}} type, which can obscure the root cause of issues and make the code harder to maintain and debug	SPARK	Resolved	3	1	5221	pull-request-available
13560282	Adding a link for latest Pandas API specifications.	Use specific supported Pandas version to be more clear.	SPARK	Resolved	3	1	5221	pull-request-available
13559488	"Adding ""Q&A Support"" and ""Mailing Lists"" link into PySpark doc homepage."	"It is aimed at improving user engagement and providing quick access to community support and discussions. This approach is inspired by the [Pandas documentation](https://pandas.pydata.org/docs/index.html), which effectively uses a similar section for community engagement.

The ""Q&A Support"" will lead users to a curated list of StackOverflow questions tagged with `pyspark`, while the mailing lists will offer platforms for deeper discussions and insights within the Spark community."	SPARK	Resolved	3	1	5221	pull-request-available
13555185	Cleanup unused import for PySpark testing	Cleanup unused import for PySpark testing	SPARK	Resolved	3	1	5221	pull-request-available
13558950	Add individual categories for `Options and settings` to API reference	Pandas has an individual Options and settings page ([https://pandas.pydata.org/docs/reference/options.html).] So we might need to follow their behavior for usability.	SPARK	Resolved	3	1	5221	pull-request-available
13547504	Resolve testing timeout issue from Spark Connect	DiffFramesParitySetItemSeriesTests.test_series_iloc_setitem is failing on Spark Connect due to unexpected timeout issue: https://github.com/itholic/spark/actions/runs/5850534247/job/15860127608	SPARK	Resolved	3	1	5221	pull-request-available
13586468	Make the parameter naming of PySparkException consistent with JVM	Parameter naming of PySparkException <> SparkException is different so there are inconsistency when searching error logs.	SPARK	Resolved	3	1	5221	pull-request-available
13559479	Update `pydata_sphinx_theme` version requirement to >=0.13	Updating to `>=0.13` ensures that we can benefit from the latest improvements and bug fixes in `pydata_sphinx_theme` without being restricted to a specific patch version. It helps maintain the documentation with the latest features available in the theme.	SPARK	Resolved	3	1	5221	pull-request-available
13560641	Enable more NumPy compatibility function tests	Should enable more NumPy test for better coverage	SPARK	Resolved	3	1	5221	pull-request-available
13593141	uncaught Java exception from make_timestamp() with bad timezone	"spark-sql (default)> select make_timestamp(1, 2, 28, 23, 1, 1, -100);
24/09/23 19:35:44 ERROR SparkSQLDriver: Failed in [select make_timestamp(1, 2, 28, 23, 1, 1, -100)]
java.time.DateTimeException: Invalid ID for ZoneOffset, invalid format: -100
at java.base/java.time.ZoneOffset.of(ZoneOffset.java:243)
at java.base/java.time.ZoneId.of(ZoneId.java:404)"	SPARK	Resolved	3	1	5221	pull-request-available
13554233	Introduce flexible parameters to assertDataFrameEqual	Add new parameters maxErrors, showOnlyDiff, maxRowsShow, ignoreColumnNames to the assertDataFrameEqual.	SPARK	Resolved	3	7	5221	pull-request-available
13554224	Remove deprecated APIs from Pandas API on Spark	There are some APIs that marked as deprecated from Spark 3.x, so we should remove them from Spark 4.x.	SPARK	Resolved	3	1	5221	pull-request-available
13406993	np.nan series.astype(bool) should be True	"np.nan series.astype(bool) should be True, rather than Fasle:

https://github.com/apache/spark/blob/46bcef7472edd40c23afd9ac74cffe13c6a608ad/python/pyspark/pandas/data_type_ops/base.py#L147

>>> pd.Series([1, 2, np.nan], dtype=float).astype(bool)
>>> pd.Series([1, 2, np.nan], dtype=str).astype(bool)
>>> pd.Series([datetime.date(1994, 1, 31), datetime.date(1994, 2, 1), np.nan])
0     True
1     True
2     True
dtype: bool

But in pyspark, it is:
0     True
1     True
2     False
dtype: bool"	SPARK	Resolved	3	1	5221	pull-request-available
13558577	Remove deprecated APIs from legacy Koalas	We should remove the deprecated Koalas signatures to cleanup the API surface.	SPARK	Resolved	3	1	5221	pull-request-available
13593954	API compatibility check for Structured Streaming Query Management	We should ensure all the existing functionalities from Structured Streaming Query Management working in the same way on Spark Connect, and also identifying the missing APIs and broken features, etc.	SPARK	Resolved	3	7	5221	pull-request-available
13555597	Improve error message for JVM-dependent attributes on Spark Connect.	When trying to access to `sparkContext` on Spark Connect, `[NOT_IMPLEMENTED] sparkContext() is not implemented.` is raised, but it is not clear to describe why sparkContext is not available from Spark Connect.	SPARK	Resolved	3	7	5221	pull-request-available
13537075	Enable NumPy compat tests	"Run `NumPyCompatParityTests.test_np_spark_compat_frame` to repro.

`"	SPARK	Resolved	3	7	5221	pull-request-available
13576872	Upgrade the minimum Pandas version to 2.0.0	Bump up the minimum version of Pandas from 1.4.4 to 2.0.0 to support Pandas API on Spark from Apache Spark 4.0.0.	SPARK	Resolved	3	1	5221	pull-request-available
13550658	Remove deprecated Index APIs	We should remove the deprecated Index APIs to match the behavior with Pandas 2.0.0 and above.	SPARK	Resolved	3	7	5221	pull-request-available
13563302	FutureWarning for interpolate with object dtype	">>> pdf.interpolate()
<stdin>:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.
   A  B
0  a  1
1  b  2
2  c  3"	SPARK	Resolved	3	1	5221	pull-request-available
13559477	Implement lint check for PySpark custom errors	"Currently, in the PySpark codebase, there is an inconsistency in the usage of exceptions. In some instances, PySpark-specific exceptions are utilized, while in others, generic Python built-in exceptions are used. This inconsistency can lead to confusion and difficulty in maintaining and debugging the code. See [https://github.com/apache/spark/pull/44024] related work to fix such a case.

The goal of this ticket is to establish a standardized practice for error handling in PySpark by mandating the use of PySpark-specific exceptions where applicable. This will ensure that all exceptions thrown within PySpark adhere to a consistent format and standard, making them more informative and easier to handle.

 "	SPARK	Resolved	3	7	5221	pull-request-available
13569774	Assign proper name and sqlState to _LEGACY_ERROR_TEMP_2134 & 2231	Assign proper name and sqlState to _LEGACY_ERROR_TEMP_2134 & 2231	SPARK	Resolved	3	7	5221	pull-request-available
13534183	Make DataFrameGroupBy.sum support for string type columns	"From pandas 2.0.0, DataFrameGroupBy.sum also works for string type columns:
{code:java}
>>> psdf
   A    B  C      D
0  1  3.1  a   True
1  2  4.1  b  False
2  1  4.1  b  False
3  2  3.1  a   True
>>> psdf.groupby(""A"").sum().sort_index()
     B  D
A
1  7.2  1
2  7.2  1
>>> psdf.to_pandas().groupby(""A"").sum().sort_index()
     B   C  D
A
1  7.2  ab  1
2  7.2  ba  1 {code}"	SPARK	Resolved	3	7	5221	pull-request-available
13591560	Use `classic` instead of `vanilla` from PySpark code base	We decided to use classic for legacy PySpark	SPARK	Resolved	3	1	5221	pull-request-available
13555180	Remove `get_dtype_counts` from Pandas API on Spark	The internal API get_dtype_counts is no longer used from Pandas API on Spark.	SPARK	Resolved	3	1	5221	pull-request-available
13569923	Improve error message from SparkThrowableSuite for better debuggability	Current error message is not very helpful when error classes documentation is not up-to-date so we better improve it	SPARK	Resolved	3	1	5221	pull-request-available
13326051	Make unionByName optionally fill missing columns with nulls in PySpark	It would be better to expose {{unionByName}} parameter in Python APIs as well. Currently this is only exposed in Scala/Java APIs (at SPARK-29358)	SPARK	Resolved	3	4	5221	starter
13564785	Check pandas installation properly	"Checking minimum_pandas_version currently not working properly.

 

>>> import pyspark.pandas
AttributeError: module 'pandas' has no attribute '__version__'"	SPARK	Resolved	3	1	5221	pull-request-available
13570752	Provide more useful context for PySpark DataFrame API errors	"Errors originating from PySpark operations can be difficult to debug with limited context in the error messages. While improvements on the JVM side have been made to offer detailed error contexts, PySpark errors often lack this level of detail. Adding detailed context about the location within the user's PySpark code where the error occurred will help debuggability for PySpark users.

 

 "	SPARK	Resolved	3	1	5221	pull-request-available
13550666	Remove `inplace` parameter from `Categorical` APIs	`inplace` should be removed from CategoricalIndex APIs to match the pandas behavior	SPARK	Resolved	3	7	5221	pull-request-available
13561318	Enhance error message debugging with new `getMessage` API	introduces a new API, `getMessage`. This API provides a standardized way for users to obtain a concise and clear error message, streamlining the process of error handling and debugging.	SPARK	Resolved	3	1	5221	pull-request-available
13554236	Returning a debuggable object for failed assertion	To facilitate debugging, we should add a functionality to return debuggable object when the assertion is failed from testing util function.	SPARK	Resolved	3	7	5221	pull-request-available
13554335	Introduce Pandas-like testing utils for Pandas API on Spark	Pandas supports separate testing utils for DataFrame, Series, Index respectively. We should follow the behavior.	SPARK	Resolved	3	7	5221	pull-request-available
13549515	Support Pandas 2.1.0	"We should support the Pandas 2.1.0.

See [https://pandas.pydata.org/docs/whatsnew/v2.1.0.html#] more detail"	SPARK	Resolved	3	7	5221	pull-request-available
13367149	SPIP: Support pandas API layer on PySpark	"This is a SPIP for porting [Koalas project|https://github.com/databricks/koalas] to PySpark, that is once discussed on the dev-mailing list with the same title, [[DISCUSS] Support pandas API layer on PySpark|http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Support-pandas-API-layer-on-PySpark-td30945.html]. 

*Q1. What are you trying to do? Articulate your objectives using absolutely no jargon.*

 Porting Koalas into PySpark to support the pandas API layer on PySpark for:
 - Users can easily leverage their existing Spark cluster to scale their pandas workloads.
 - Support plot and drawing a chart in PySpark
 - Users can easily switch between pandas APIs and PySpark APIs


*Q2. What problem is this proposal NOT designed to solve?*

Some APIs of pandas are explicitly unsupported. For example, {{memory_usage}} in pandas will not be supported because DataFrames are not materialized in memory in Spark unlike pandas.

This does not replace the existing PySpark APIs. PySpark API has lots of users and existing code in many projects, and there are still many PySpark users who prefer Spark’s immutable DataFrame API to the pandas API.


*Q3. How is it done today, and what are the limits of current practice?*

The current practice has 2 limits as below.
 # There are many features missing in Apache Spark that are very commonly used in data science. Specifically, plotting and drawing a chart is missing which is one of the most important features that almost every data scientist use in their daily work.
 # Data scientists tend to prefer pandas APIs, but it is very hard to change them into PySpark APIs when they need to scale their workloads. This is because PySpark APIs are difficult to learn compared to pandas' and there are many missing features in PySpark.


*Q4. What is new in your approach and why do you think it will be successful?*

I believe this suggests a new way for both PySpark and pandas users to easily scale their workloads. I think we can be successful because more and more people tend to use Python and pandas. In fact, there are already similar tries such as Dask and Modin which are all growing fast and successfully.


*Q5. Who cares? If you are successful, what difference will it make?*

Anyone who wants to scale their pandas workloads on their Spark cluster. It will also significantly improve the usability of PySpark.


*Q6. What are the risks?*

Technically I don't see many risks yet given that:
- Koalas has grown separately for more than two years, and has greatly improved maturity and stability.
- Koalas will be ported into PySpark as a separate package

It is more about putting documentation and test cases in place properly with properly handling dependencies. For example, Koalas currently uses pytest with various dependencies whereas PySpark uses the plain unittest with fewer dependencies.

In addition, Koalas' default Indexing system could not be much loved because it could potentially cause overhead, so applying it properly to PySpark might be a challenge.


*Q7. How long will it take?*

Before the Spark 3.2 release.


*Q8. What are the mid-term and final “exams” to check for success?*

The first check for success would be to make sure that all the existing Koalas APIs and tests work as they are without any affecting the existing Koalas workloads on PySpark.

The last thing to confirm is to check whether the usability and convenience that we aim for is actually increased through user feedback and PySpark usage statistics.


*Also refer to:*
- [Koalas internals documentation|https://docs.google.com/document/d/1tk24aq6FV5Wu2bX_Ym606doLFnrZsh4FdUd52FqojZU/edit]
- [[VOTE] SPIP: Support pandas API layer on PySpark|http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-SPIP-Support-pandas-API-layer-on-PySpark-td30996.html]"	SPARK	Resolved	1	14	5221	SPIP
13561602	Add information of isocalendar into migration guide	Missing removed API from docs.	SPARK	Resolved	3	1	5221	pull-request-available
13551489	Upgrade Pandas to 2.1.1	https://pandas.pydata.org/pandas-docs/dev/whatsnew/v2.1.1.html	SPARK	Resolved	3	7	5221	pull-request-available
13587626	Remove special casing for Protobuf functions in Connect	"Tasks to perform:
 * Remove the special casing from the connect planner.
 * Register the protobuf functions under system.internal using session extensions.
 * Add the needed constructors to the protobuf expressions.
 * Update protobuf functions and make them use unresolved function path"	SPARK	Resolved	3	2	5221	pull-request-available
13559525	Using brighter color for document title for better visibility	With the increasing popularity of dark mode for its eye comfort and energy-saving benefits, it's important to ensure that our documentation is easily readable in both light and dark settings. The current title font color in dark mode is not optimal for readability, which can hinder user experience. By adjusting the color, we aim to enhance the overall accessibility and readability of the PySpark documentation in dark mode. 	SPARK	Resolved	3	1	5221	pull-request-available
13537028	Support `Column` for SparkConnectColumn.__getitem__	"Repro:
{code:java}
pser = pd.Series([""a"", ""b"", ""c""])
psser = ps.from_pandas(pser)
psser.astype(""category"")  # internally calls `map_scol[self.spark.column]`{code}"	SPARK	Resolved	3	7	5221	pull-request-available
13534308	Migrate `ValueError` from Spark SQL types into error class	Migrate `ValueError` from Spark SQL types into error class	SPARK	Resolved	3	7	5221	pull-request-available
13591908	Add API compatibility check between Classic and Connect	We should ensure every API has same signature between Classic and Connect	SPARK	Resolved	3	7	5221	pull-request-available
13565866	Fix error message regression by restoring new_msg	">>> from pyspark.sql.types import StructType, StructField, StringType, IntegerType
>>> schema = StructType([
...     StructField(""name"", StringType(), nullable=True),
...     StructField(""age"", IntegerType(), nullable=False)
... ])
>>> df = spark.createDataFrame([(""asd"", None])], schema)
pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_BE_NONE] Argument `obj` cannot be None.

 "	SPARK	Resolved	3	7	5221	pull-request-available
13593955	API compatibility check for Avro	We should ensure all the existing functionalities from Avro working in the same way on Spark Connect, and also identifying the missing APIs and broken features, etc.	SPARK	Resolved	3	7	5221	pull-request-available
13556050	Remove remaining deprecated Pandas APIs from Spark 3.4.0	Remove remaining deprecated Pandas APIs from Spark 3.4.0	SPARK	Resolved	3	7	5221	pull-request-available
13558779	Fix broken link for Koalas issues	There is a link broken for Koalas old repo. We should address it.	SPARK	Resolved	3	1	5221	pull-request-available
13559256	Refactor data type casting operation for Categorical type.	Using official API for better performance and readability.	SPARK	Resolved	3	1	5221	pull-request-available
13558781	Fix pandas API support list properly	Currently Supported pandas API is not generated properly, so we should fix it.	SPARK	Resolved	3	1	5221	pull-request-available
13554235	Introduce flexible parameter to assertSchemaEqual	Add new parameter ignoreColumnNames to the assertSchemaEqual.	SPARK	Resolved	3	7	5221	pull-request-available
13560382	Migrate all remaining RuntimeError into PySpark error framework	We should migrate all errors into PySpark error framework	SPARK	Resolved	3	7	5221	pull-request-available
13537073	Enable NamespaceParityTests.test_get_index_map	Enable NamespaceParityTests.test_get_index_map	SPARK	Resolved	3	7	5221	pull-request-available
13589389	Improve error message for assertSchemaEqual	assertSchemaEqual does not provide actionable error message when the given parameter has invalid data type	SPARK	Resolved	3	1	5221	pull-request-available
13550299	Support Series.empty for Spark Connect.	We should remove JVM dependency for Pandas API on Spark.	SPARK	Resolved	3	7	5221	pull-request-available
13557807	Make the internal attributes private from PySpark errors.	There are some APIs from PySparkException are exposed to user space which should have not been. We should hide them.	SPARK	Resolved	3	1	5221	pull-request-available
13559267	Sync PySpark dependencies in docs and dev requirements	There is inconsistency between docs and dev env. We should sync them.	SPARK	Resolved	3	1	5221	pull-request-available
13554234	Deprecate assertPandasOnSparkEqual	We will add new APIs for DataFrame, Series and Index separately, and we should deprecate assertPandasOnSparkEqual.	SPARK	Resolved	3	7	5221	pull-request-available
13560575	Add appropriate link for error class usage documentation.	We don't have appropriate link for error class usage documentation.	SPARK	Resolved	3	1	5221	pull-request-available
13559630	Add GitHub link icon to PySpark documentation header	Add GitHub link icon to PySpark documentation header for better accessibility such as Pandas does.	SPARK	Resolved	3	1	5221	pull-request-available
13556297	Introduce an implicit function for Scala Array to wrap into `immutable.ArraySeq`.	Currently, we need to use `immutable.ArraySeq.unsafeWrapArray(array)` to wrap an Array into an `immutable.ArraySeq`, which makes the code look bloated.	SPARK	Resolved	3	7	6681	pull-request-available
13345763	Build and Run Spark on Java 17	"Apache Spark supports Java 8 and Java 11 (LTS). The next Java LTS version is 17.

||Version||Release Date||
|Java 17 (LTS)|September 2021|


Apache Spark has a release plan and `Spark 3.2 Code freeze` was July along with the release branch cut.
- https://spark.apache.org/versioning-policy.html

Supporting new Java version is considered as a new feature which we cannot allow to backport."	SPARK	Resolved	3	2	6681	releasenotes
13557437	Remove Java version check from `IsolatedClientLoader`	"{code:java}
val rootClassLoader: ClassLoader =
  if (SystemUtils.isJavaVersionAtLeast(JavaVersion.JAVA_9)) {
    // In Java 9, the boot classloader can see few JDK classes. The intended parent
    // classloader for delegation is now the platform classloader.
    // See http://java9.wtf/class-loading/
    val platformCL =
    classOf[ClassLoader].getMethod(""getPlatformClassLoader"").
      invoke(null).asInstanceOf[ClassLoader]
    // Check to make sure that the root classloader does not know about Hive.
    assert(Try(platformCL.loadClass(""org.apache.hadoop.hive.conf.HiveConf"")).isFailure)
    platformCL
  } else {
    // The boot classloader is represented by null (the instance itself isn't accessible)
    // and before Java 9 can see all JDK classes
    null
  } {code}
Spark 4.0.0 has a minimum requirement of Java 17, so the version check for Java 9 is not necessary."	SPARK	Resolved	4	7	6681	pull-request-available
13552176	Maven test `SparkConnectProtoSuite` failed	" 

build/mvn clean install -pl connector/connect/server -am -DskipTests

mvn test -pl connector/connect/server 

 
{code:java}
- Test observe *** FAILED ***
  == FAIL: Plans do not match ===
  !CollectMetrics my_metric, [min(id#0) AS min_val#0, max(id#0) AS max_val#0, sum(id#0) AS sum(id)#0L], 0   CollectMetrics my_metric, [min(id#0) AS min_val#0, max(id#0) AS max_val#0, sum(id#0) AS sum(id)#0L], 53
   +- LocalRelation <empty>, [id#0, name#0]                                                                 +- LocalRelation <empty>, [id#0, name#0] (PlanTest.scala:179) {code}
 

 "	SPARK	Resolved	3	1	6681	pull-request-available
13559731	Remove comments about JVM module options from the test submission options of `HiveExternalCatalogVersionsSuite`	"Complete TODO:
{code:java}
val args = Seq(
  ""--name"", ""prepare testing tables"",
  ""--master"", ""local[2]"",
  ""--conf"", s""${UI_ENABLED.key}=false"",
  ""--conf"", s""${MASTER_REST_SERVER_ENABLED.key}=false"",
  ""--conf"", s""${HiveUtils.HIVE_METASTORE_VERSION.key}=$hiveMetastoreVersion"",
  ""--conf"", s""${HiveUtils.HIVE_METASTORE_JARS.key}=maven"",
  ""--conf"", s""${WAREHOUSE_PATH.key}=${wareHousePath.getCanonicalPath}"",
  ""--conf"", s""spark.sql.test.version.index=$index"",
  ""--driver-java-options"", s""-Dderby.system.home=${wareHousePath.getCanonicalPath} "" +
    // TODO SPARK-37159 Consider to remove the following
    // JVM module options once the Spark 3.2 line is EOL.
    JavaModuleOptions.defaultModuleOptions(),
  tempPyFile.getCanonicalPath) {code}"	SPARK	Resolved	4	3	6681	pull-request-available
13559157	Use the Separators API instead of the String API to construct the DefaultPrettyPrinter	"{code:java}
/**
 * Constructor that specifies separator String to use between root values;
 * if null, no separator is printed.
 *<p>
 * Note: simply constructs a {@link SerializedString} out of parameter,
 * calls {@link #DefaultPrettyPrinter(SerializableString)}
 *
 * @param rootSeparator String to use as root value separator
 * @deprecated in 2.16. Use the Separators API instead.
 */
@Deprecated
public DefaultPrettyPrinter(String rootSeparator) {
    this((rootSeparator == null) ? null : new SerializedString(rootSeparator));
} {code}"	SPARK	Resolved	4	4	6681	pull-request-available
13544244	K8s-it test failed	"* [https://github.com/apache/spark/actions/runs/5607397734/jobs/10258527838]

{code:java}
[info] - PVs with local hostpath storage on statefulsets *** FAILED *** (3 minutes, 11 seconds)
3786[info]   The code passed to eventually never returned normally. Attempted 7921 times over 3.0001059888166663 minutes. Last failure message: ""++ id -u
3787[info]   + myuid=185
3788[info]   ++ id -g
3789[info]   + mygid=0
3790[info]   + set +e
3791[info]   ++ getent passwd 185
3792[info]   + uidentry=
3793[info]   + set -e
3794[info]   + '[' -z '' ']'
3795[info]   + '[' -w /etc/passwd ']'
3796[info]   + echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
3797[info]   + '[' -z /opt/java/openjdk ']'
3798[info]   + SPARK_CLASSPATH=':/opt/spark/jars/*'
3799[info]   + grep SPARK_JAVA_OPT_
3800[info]   + sort -t_ -k4 -n
3801[info]   + sed 's/[^=]*=\(.*\)/\1/g'
3802[info]   + env
3803[info]   ++ command -v readarray
3804[info]   + '[' readarray ']'
3805[info]   + readarray -t SPARK_EXECUTOR_JAVA_OPTS
3806[info]   + '[' -n '' ']'
3807[info]   + '[' -z ']'
3808[info]   + '[' -z ']'
3809[info]   + '[' -n '' ']'
3810[info]   + '[' -z ']'
3811[info]   + '[' -z x ']'
3812[info]   + SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*'
3813[info]   + SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*:/opt/spark/work-dir'
3814[info]   + case ""$1"" in
3815[info]   + shift 1
3816[info]   + CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --conf ""spark.executorEnv.SPARK_DRIVER_POD_IP=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"")
3817[info]   + exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.244.0.45 --conf spark.executorEnv.SPARK_DRIVER_POD_IP=10.244.0.45 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.MiniReadWriteTest local:///opt/spark/examples/jars/spark-examples_2.12-4.0.0-SNAPSHOT.jar /opt/spark/pv-tests/tmp3727659354473892032.txt
3818[info]   Files local:///opt/spark/examples/jars/spark-examples_2.12-4.0.0-SNAPSHOT.jar from /opt/spark/examples/jars/spark-examples_2.12-4.0.0-SNAPSHOT.jar to /opt/spark/work-dir/spark-examples_2.12-4.0.0-SNAPSHOT.jar
3819[info]   23/07/20 06:15:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
3820[info]   Performing local word count from /opt/spark/pv-tests/tmp3727659354473892032.txt
3821[info]   File contents are List(test PVs)
3822[info]   Creating SparkSession
3823[info]   23/07/20 06:15:15 INFO SparkContext: Running Spark version 4.0.0-SNAPSHOT
3824[info]   23/07/20 06:15:15 INFO SparkContext: OS info Linux, 5.15.0-1041-azure, amd64
3825[info]   23/07/20 06:15:15 INFO SparkContext: Java version 17.0.7
3826[info]   23/07/20 06:15:15 INFO ResourceUtils: ==============================================================
3827[info]   23/07/20 06:15:15 INFO ResourceUtils: No custom resources configured for spark.driver.
3828[info]   23/07/20 06:15:15 INFO ResourceUtils: ==============================================================
3829[info]   23/07/20 06:15:15 INFO SparkContext: Submitted application: Mini Read Write Test
3830[info]   23/07/20 06:15:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0) {code}
The tests in the past two days have failed"	SPARK	Resolved	3	1	6681	pull-request-available
13566464	Upgrade slf4j to 2.0.11	 This release reinstates the `renderLevel()` method in {{SimpleLogger}} which was removed by mistake.	SPARK	Resolved	3	4	6681	pull-request-available
13413061	Disable unsupported `ExtendedLevelDBTest` on `MacOS/aarch64`	"After SPARK-37272  and SPARK-37282,  we can manually add
{code:java}
-Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest,org.apache.spark.tags.ExtendedRocksDBTest {code}
when run mvn test or sbt test to disable unsupported UTs on Macos using Apple Silicon.

 

We can add a profile to and  activate this property automatically when run UTs on Macos using Apple Silicon.

 

 

 

 "	SPARK	Resolved	4	7	6681	pull-request-available
13555776	Clean up the deprecated API usage related to `SetOps`	"* method - in trait SetOps is deprecated (since 2.13.0)
 * method -- in trait SetOps is deprecated (since 2.13.0)
 * method + in trait SetOps is deprecated (since 2.13.0)
 * method retain in trait SetOps is deprecated (since 2.13.0)

 
{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala:70:32: method + in trait SetOps is deprecated (since 2.13.0): Consider requiring an immutable Set or fall back to Set.union
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.storage.BlockReplicationUtils.getSampleIds.indices.$anonfun, origin=scala.collection.SetOps.+, version=2.13.0
[warn]       if (set.contains(t)) set + i else set + t
[warn]                                ^ {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13570160	Upgrade slf4j to 2.0.12	https://www.slf4j.org/news.html#2.0.12	SPARK	Resolved	3	7	6681	pull-request-available
13560420	Make `HiveDDLSuite` independently testable	"Run `

build/sbt ""hive/testOnly org.apache.spark.sql.hive.execution.HiveDDLSuite"" -Phive

`
{code:java}
[info] - SPARK-34261: Avoid side effect if create exists temporary function *** FAILED *** (4 milliseconds)
[info]   java.util.NoSuchElementException: key not found: default
[info]   at scala.collection.MapOps.default(Map.scala:274)
[info]   at scala.collection.MapOps.default$(Map.scala:273)
[info]   at scala.collection.AbstractMap.default(Map.scala:405)
[info]   at scala.collection.MapOps.apply(Map.scala:176)
[info]   at scala.collection.MapOps.apply$(Map.scala:175)
[info]   at scala.collection.AbstractMap.apply(Map.scala:405)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite.$anonfun$new$445(HiveDDLSuite.scala:3275)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withUserDefinedFunction(SQLTestUtils.scala:256)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withUserDefinedFunction$(SQLTestUtils.scala:254)
[info]   at org.apache.spark.sql.execution.command.DDLSuite.withUserDefinedFunction(DDLSuite.scala:326)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite.$anonfun$new$444(HiveDDLSuite.scala:3267)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:333)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}"	SPARK	Resolved	4	1	6681	pull-request-available
13573142	Upgrade jmock-junit5 to 2.13.1	https://github.com/jmock-developers/jmock-library/releases/tag/2.13.1	SPARK	Resolved	3	7	6681	pull-request-available
13552503	Replcace `TraversableOnce` with `IterableOnce`	"{code:java}
@deprecated(""Use IterableOnce instead of TraversableOnce"", ""2.13.0"")
type TraversableOnce[+A] = scala.collection.IterableOnce[A]

type IterableOnce[+A] = scala.collection.IterableOnce[A] {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13482317	Upgrade Scala to 2.13.11	"We tested and decided to skip the following releases. This issue aims to use 2.13.11.
- 2022-09-21: v2.13.9 released [https://github.com/scala/scala/releases/tag/v2.13.9]
- 2022-10-13: 2.13.10 released [https://github.com/scala/scala/releases/tag/v2.13.10]
 

Scala 2.13.11 Milestone
- https://github.com/scala/scala/milestone/100"	SPARK	Resolved	3	7	6681	pull-request-available
13554820	"Fix ""Auto-application to `()` is deprecated."""	"For the following case, a compile warning will be issued in Scala 2.13：

 
{code:java}
Welcome to Scala 2.13.12 (OpenJDK 64-Bit Server VM, Java 17.0.8).
Type in expressions for evaluation. Or try :help.


scala> class Foo {
     |     def isEmpty(): Boolean = true
     |     def isTrue(x: Boolean): Boolean = x
     |   }
class Foo


scala> val foo = new Foo
val foo: Foo = Foo@7061622


scala> val ret = foo.isEmpty
                     ^
       warning: Auto-application to `()` is deprecated. Supply the empty argument list `()` explicitly to invoke method isEmpty,
       or remove the empty argument list from its definition (Java-defined methods are exempt).
       In Scala 3, an unapplied method like this will be eta-expanded into a function. [quickfixable]
val ret: Boolean = true {code}
But for Scala 3, it is a compile error:
{code:java}
Welcome to Scala 3.3.1 (17.0.8, Java OpenJDK 64-Bit Server VM).
Type in expressions for evaluation. Or try :help.
                                                                                                                                                                                                                                                     
scala> class Foo {
     |     def isEmpty(): Boolean = true
     |     def isTrue(x: Boolean): Boolean = x
     |   }
// defined class Foo
                                                                                                                                                                                                                                                     
scala> val foo = new Foo
val foo: Foo = Foo@591f6f83
                                                                                                                                                                                                                                                     
scala> val ret = foo.isEmpty
-- [E100] Syntax Error: --------------------------------------------------------
1 |val ret = foo.isEmpty
  |          ^^^^^^^^^^^
  |          method isEmpty in class Foo must be called with () argument
  |
  | longer explanation available when compiling with `-explain`
1 error found {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13585111	Remove the check for the existence of ./dev/free_disk_space_container	`./dev/free_disk_space_container` has already been backported to branch-3.4 and branch-3.5 through https://github.com/apache/spark/pull/45624 and https://github.com/apache/spark/pull/43381, so there is no need to check its existence before execution.	SPARK	Resolved	3	4	6681	pull-request-available
13558863	Upgrade ZSTD-JNI to 1.5.5-10	"[https://github.com/luben/zstd-jni/releases/tag/v1.5.5-8]

[https://github.com/luben/zstd-jni/releases/tag/v1.5.5-9]

 "	SPARK	Resolved	3	4	6681	pull-request-available
13555760	  Clean up the deprecated API usage related to `RightProjection/LeftProjection/Either`	"* method get in class RightProjection is deprecated (since 2.13.0)
 * method get in class LeftProjection is deprecated (since 2.13.0)
 * method right in class Either is deprecated (since 2.13.0)

 
{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala:54:28: method get in class LeftProjection is deprecated (since 2.13.0): use `Either.swap.getOrElse` instead
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.execution.datasources.v2.GroupBasedRowLevelOperationScanPlanning.apply, origin=scala.util.Either.LeftProjection.get, version=2.13.0
[warn]         pushedFilters.left.get.mkString("", "")
[warn]                            ^ {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13559160	Upgrade commons-compress to 1.25.0	https://commons.apache.org/proper/commons-compress/changes-report.html#a1.25.0	SPARK	Resolved	3	7	6681	pull-request-available
13554137	Replace `setSafeMode(HdfsConstants.SafeModeAction, boolean)` with `setSafeMode(SafeModeAction, boolean)`	"{code:java}
/**
 * Enter, leave or get safe mode.
 *
 * @param action
 *          One of SafeModeAction.ENTER, SafeModeAction.LEAVE and
 *          SafeModeAction.GET.
 * @param isChecked
 *          If true check only for Active NNs status, else check first NN's
 *          status.
 *
 * @see org.apache.hadoop.hdfs.protocol.ClientProtocol#setSafeMode(HdfsConstants.SafeModeAction,
 * boolean)
 *
 * @deprecated please instead use
 *               {@link DistributedFileSystem#setSafeMode(SafeModeAction, boolean)}.
 */
@Deprecated
public boolean setSafeMode(HdfsConstants.SafeModeAction action,
    boolean isChecked) throws IOException {
  return dfs.setSafeMode(action, isChecked);
} {code}
 

`setSafeMode(HdfsConstants.SafeModeAction, boolean)` is `Deprecated`"	SPARK	Resolved	4	4	6681	pull-request-available
13554713	   Replace `s.c.MapOps.mapValues` with `s.c.MapOps.view.mapValues`	"{code:java}
@deprecated(""Use .view.mapValues(f). A future version will include a strict version of this method (for now, .view.mapValues(f).toMap)."", ""2.13.0"")
def mapValues[W](f: V => W): MapView[K, W] = new MapView.MapValues(this, f) {code}"	SPARK	Resolved	4	7	6681	pull-request-available
13555773	Fix `The outer reference in this type test cannot be checked at run time`	"{code:java}
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:324:12: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.createScalaTestCase
[error]       case udfTestCase: UDFTest
[error]            ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:506:12: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.runQueries
[error]       case udfTestCase: UDFTest =>
[error]            ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:508:12: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.runQueries
[error]       case udtfTestCase: UDTFSetTest =>
[error]            ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:514:13: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.runQueries
[error]       case _: PgSQLTest =>
[error]             ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:522:13: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.runQueries
[error]       case _: AnsiTest =>
[error]             ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:524:13: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.runQueries
[error]       case _: TimestampNTZTest =>
[error]             ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:584:12: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.runQueries.clue
[error]       case udfTestCase: UDFTest
[error]            ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:596:12: The outer reference in this type test cannot be checked at run time.
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unchecked, site=org.apache.spark.sql.SQLQueryTestSuite.runQueries.clue
[error]       case udtfTestCase: UDTFSetTest
[error]            ^ {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13561495	Simplify the code to generate the Spark tarball `filename` in the `HiveExternalCatalogVersionsSuite`.	"{code:java}
val filename = VersionUtils.majorMinorPatchVersion(version) match {
  case Some((major, _, _)) if major > 3 => s""spark-$version-bin-hadoop3.tgz""
  case Some((3, minor, _)) if minor >= 3 => s""spark-$version-bin-hadoop3.tgz""
  case Some((3, minor, _)) if minor < 3 => s""spark-$version-bin-hadoop3.2.tgz""
  case Some((_, _, _)) => s""spark-$version-bin-hadoop2.7.tgz""
  case None => s""spark-$version-bin-hadoop2.7.tgz""
} {code}
Currently, the minimum tested version is Spark 3.3, so there is no need for complex case matching anymore."	SPARK	Resolved	4	4	6681	pull-request-available
13553807	Fix compilation warnings related to other-nullary-override	"{code:java}
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/connect/common/src/main/scala/org/apache/spark/sql/connect/client/CloseableIterator.scala:36:16: method with a single empty parameter list overrides method hasNext in trait Iterator defined without a parameter list [quickfixable]
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=other-nullary-override, site=org.apache.spark.sql.connect.client.WrappedCloseableIterator
[error]   override def hasNext(): Boolean = innerIterator.hasNext
[error]                ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/connect/common/src/main/scala/org/apache/spark/sql/connect/client/ExecutePlanResponseReattachableIterator.scala:136:16: method without a parameter list overrides method hasNext in class WrappedCloseableIterator defined with a single empty parameter list [quickfixable]
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=other-nullary-override, site=org.apache.spark.sql.connect.client.ExecutePlanResponseReattachableIterator
[error]   override def hasNext: Boolean = synchronized {
[error]                ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/connect/common/src/main/scala/org/apache/spark/sql/connect/client/GrpcExceptionConverter.scala:73:20: method without a parameter list overrides method hasNext in class WrappedCloseableIterator defined with a single empty parameter list [quickfixable]
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=other-nullary-override, site=org.apache.spark.sql.connect.client.GrpcExceptionConverter.convertIterator
[error]       override def hasNext: Boolean = {
[error]                    ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/connect/common/src/main/scala/org/apache/spark/sql/connect/client/GrpcRetryHandler.scala:77:18: method without a parameter list overrides method next in class WrappedCloseableIterator defined with a single empty parameter list [quickfixable]
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=other-nullary-override, site=org.apache.spark.sql.connect.client.GrpcRetryHandler.RetryIterator
[error]     override def next: U = {
[error]                  ^
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/connect/common/src/main/scala/org/apache/spark/sql/connect/client/GrpcRetryHandler.scala:81:18: method without a parameter list overrides method hasNext in class WrappedCloseableIterator defined with a single empty parameter list [quickfixable]
[error] Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=other-nullary-override, site=org.apache.spark.sql.connect.client.GrpcRetryHandler.RetryIterator
[error]     override def hasNext: Boolean = {
[error]                  ^
 {code}"	SPARK	Resolved	4	7	6681	pull-request-available
13555754	Use `LazyList` instead of `Stream`	"* class Stream in package immutable is deprecated (since 2.13.0)
 * object Stream in package immutable is deprecated (since 2.13.0)
 * type Stream in package scala is deprecated (since 2.13.0)
 * value Stream in package scala is deprecated (since 2.13.0)
 * method append in class Stream is deprecated (since 2.13.0)
 * method toStream in trait IterableOnceOps is deprecated (since 2.13.0)

 
{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/test/scala/org/apache/spark/sql/GenTPCDSData.scala:49:20: class Stream in package immutable is deprecated (since 2.13.0): Use LazyList (which is fully lazy) instead of Stream (which has a lazy tail only)
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.BlockingLineStream.BlockingStreamed.stream, origin=scala.collection.immutable.Stream, version=2.13.0
[warn]     val stream: () => Stream[T])
[warn]                    ^ {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13551399	Correct the default value of `spark.history.store.hybridStore.diskBackend` in `monitoring.md`	"SPARK-42277 change to use RocksDB for spark.history.store.hybridStore.diskBackend by default, but in `monitoring.md`, the default value is still set as LEVELDB.
 
 
 
 "	SPARK	Resolved	3	1	6681	pull-request-available
13560573	Upgrade Derby to 10.16.1.1	"[https://db.apache.org/derby/releases/release-10_16_1_1.cgi]

1. Drop Java Security Manager.
{quote}Derby no longer supports the Java SecurityManager. This is because the Open JDK team deprecated the SecurityManager and marked it for removal.
{quote}
2. Compile on Java 17
{quote}Compile 10.16 into Java 17 byte code
{quote}"	SPARK	Resolved	3	7	6681	pull-request-available
13563141	Remove the check for `c>=0` from `ExternalCatalogUtils#needsEscaping`	" 
{code:java}
def needsEscaping(c: Char): Boolean = {
  c >= 0 && c < charToEscape.size() && charToEscape.get(c)
} {code}
 

 

The numerical range of Char in Scala is from 0 to 65,535, so `c>=0` is always true."	SPARK	Resolved	4	4	6681	pull-request-available
13553820	Use enhanced `switch` expressions to replace the regular `switch` statement	"refer to [JEP 361|https://openjdk.org/jeps/361] 

 

Example:
{code:java}
double getPrice(String fruit) {
  switch (fruit) {
    case ""Apple"":
      return 1.0;
    case ""Orange"":
      return 1.5;
    case ""Mango"":
      return 2.0;
    default:
      throw new IllegalArgumentException();
   }
 } {code}

Can be changed to 
{code:java}
double getPrice(String fruit) {
    return switch (fruit) {
      case ""Apple"" -> 1.0;
      case ""Orange"" -> 1.5;
      case ""Mango"" -> 2.0;
      default -> throw new IllegalArgumentException();
    };
} {code}
 "	SPARK	Resolved	3	7	6681	pull-request-available
13562903	Upgrade Jackson to 2.16.1	https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.16.1	SPARK	Resolved	3	4	6681	pull-request-available
13588695	sbt compilation warning: `Regular tasks always evaluate task dependencies (.value) regardless of if expressions`	"{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-sbt/project/SparkBuild.scala:1554:77: value lookup of `/` inside an `if` expression
[warn] 
[warn] problem: `/.value` is inside an `if` expression of a regular task.
[warn]   Regular tasks always evaluate task dependencies (`.value`) regardless of `if` expressions.
[warn] solution:
[warn]   1. Use a conditional task `Def.taskIf(...)` to evaluate it when the `if` predicate is true or false.
[warn]   2. Or turn the task body into a single `if` expression; the task is then auto-converted to a conditional task. 
[warn]   3. Or make the static evaluation explicit by declaring `/.value` outside the `if` expression.
[warn]   4. If you still want to force the static lookup, you may annotate the task lookup with `@sbtUnchecked`, e.g. `(/.value: @sbtUnchecked)`.
[warn]   5. Add `import sbt.dsl.LinterLevel.Ignore` to your build file to disable all task linting.
[warn]     
[warn]         val replClasspathes = (LocalProject(""connect-client-jvm"") / Compile / dependencyClasspath)
 {code}"	SPARK	Resolved	3	4	6681	pull-request-available
13509873	Add `bouncy-castle` test dependencies to `sql/core` module for Hadoop 3.4.0	"on hadoop trunk (but not the 3.3.x line), spark builds fail with a CNFE

{code}
net.alchim31.maven:scala-maven-plugin:4.7.2:testCompile: org/bouncycastle/jce/provider/BouncyCastleProvider

{code}

full stack

{code}
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.7.2:testCompile (scala-test-compile-first) on project spark-sql_2.12: Execution scala-test-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.7.2:testCompile failed: A required class was missing while executing net.alchim31.maven:scala-maven-plugin:4.7.2:testCompile: org/bouncycastle/jce/provider/BouncyCastleProvider
[ERROR] -----------------------------------------------------
[ERROR] realm =    plugin>net.alchim31.maven:scala-maven-plugin:4.7.2
[ERROR] strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy
[ERROR] urls[0] = file:/Users/stevel/.m2/repository/net/alchim31/maven/scala-maven-plugin/4.7.2/scala-maven-plugin-4.7.2.jar
[ERROR] urls[1] = file:/Users/stevel/.m2/repository/org/apache/maven/shared/maven-dependency-tree/3.2.0/maven-dependency-tree-3.2.0.jar
[ERROR] urls[2] = file:/Users/stevel/.m2/repository/org/eclipse/aether/aether-util/1.0.0.v20140518/aether-util-1.0.0.v20140518.jar
[ERROR] urls[3] = file:/Users/stevel/.m2/repository/org/apache/maven/reporting/maven-reporting-api/3.1.1/maven-reporting-api-3.1.1.jar
[ERROR] urls[4] = file:/Users/stevel/.m2/repository/org/apache/maven/doxia/doxia-sink-api/1.11.1/doxia-sink-api-1.11.1.jar
[ERROR] urls[5] = file:/Users/stevel/.m2/repository/org/apache/maven/doxia/doxia-logging-api/1.11.1/doxia-logging-api-1.11.1.jar
[ERROR] urls[6] = file:/Users/stevel/.m2/repository/org/apache/maven/maven-archiver/3.6.0/maven-archiver-3.6.0.jar
[ERROR] urls[7] = file:/Users/stevel/.m2/repository/org/codehaus/plexus/plexus-io/3.4.0/plexus-io-3.4.0.jar
[ERROR] urls[8] = file:/Users/stevel/.m2/repository/org/codehaus/plexus/plexus-interpolation/1.26/plexus-interpolation-1.26.jar
[ERROR] urls[9] = file:/Users/stevel/.m2/repository/org/apache/commons/commons-exec/1.3/commons-exec-1.3.jar
[ERROR] urls[10] = file:/Users/stevel/.m2/repository/org/codehaus/plexus/plexus-utils/3.4.2/plexus-utils-3.4.2.jar
[ERROR] urls[11] = file:/Users/stevel/.m2/repository/org/codehaus/plexus/plexus-archiver/4.5.0/plexus-archiver-4.5.0.jar
[ERROR] urls[12] = file:/Users/stevel/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar
[ERROR] urls[13] = file:/Users/stevel/.m2/repository/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar
[ERROR] urls[14] = file:/Users/stevel/.m2/repository/org/iq80/snappy/snappy/0.4/snappy-0.4.jar
[ERROR] urls[15] = file:/Users/stevel/.m2/repository/org/tukaani/xz/1.9/xz-1.9.jar
[ERROR] urls[16] = file:/Users/stevel/.m2/repository/com/github/luben/zstd-jni/1.5.2-4/zstd-jni-1.5.2-4.jar
[ERROR] urls[17] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc_2.13/1.7.1/zinc_2.13-1.7.1.jar
[ERROR] urls[18] = file:/Users/stevel/.m2/repository/org/scala-lang/scala-library/2.13.8/scala-library-2.13.8.jar
[ERROR] urls[19] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc-core_2.13/1.7.1/zinc-core_2.13-1.7.1.jar
[ERROR] urls[20] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc-apiinfo_2.13/1.7.1/zinc-apiinfo_2.13-1.7.1.jar
[ERROR] urls[21] = file:/Users/stevel/.m2/repository/org/scala-sbt/compiler-bridge_2.13/1.7.1/compiler-bridge_2.13-1.7.1.jar
[ERROR] urls[22] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc-classpath_2.13/1.7.1/zinc-classpath_2.13-1.7.1.jar
[ERROR] urls[23] = file:/Users/stevel/.m2/repository/org/scala-lang/scala-compiler/2.13.8/scala-compiler-2.13.8.jar
[ERROR] urls[24] = file:/Users/stevel/.m2/repository/org/scala-sbt/compiler-interface/1.7.1/compiler-interface-1.7.1.jar
[ERROR] urls[25] = file:/Users/stevel/.m2/repository/org/scala-sbt/util-interface/1.7.0/util-interface-1.7.0.jar
[ERROR] urls[26] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc-persist-core-assembly/1.7.1/zinc-persist-core-assembly-1.7.1.jar
[ERROR] urls[27] = file:/Users/stevel/.m2/repository/org/scala-lang/modules/scala-parallel-collections_2.13/0.2.0/scala-parallel-collections_2.13-0.2.0.jar
[ERROR] urls[28] = file:/Users/stevel/.m2/repository/org/scala-sbt/io_2.13/1.7.0/io_2.13-1.7.0.jar
[ERROR] urls[29] = file:/Users/stevel/.m2/repository/com/swoval/file-tree-views/2.1.9/file-tree-views-2.1.9.jar
[ERROR] urls[30] = file:/Users/stevel/.m2/repository/net/java/dev/jna/jna/5.12.0/jna-5.12.0.jar
[ERROR] urls[31] = file:/Users/stevel/.m2/repository/net/java/dev/jna/jna-platform/5.12.0/jna-platform-5.12.0.jar
[ERROR] urls[32] = file:/Users/stevel/.m2/repository/org/scala-sbt/util-logging_2.13/1.7.0/util-logging_2.13-1.7.0.jar
[ERROR] urls[33] = file:/Users/stevel/.m2/repository/org/scala-sbt/collections_2.13/1.7.0/collections_2.13-1.7.0.jar
[ERROR] urls[34] = file:/Users/stevel/.m2/repository/org/scala-sbt/util-position_2.13/1.7.0/util-position_2.13-1.7.0.jar
[ERROR] urls[35] = file:/Users/stevel/.m2/repository/org/scala-sbt/core-macros_2.13/1.7.0/core-macros_2.13-1.7.0.jar
[ERROR] urls[36] = file:/Users/stevel/.m2/repository/org/jline/jline-terminal/3.19.0/jline-terminal-3.19.0.jar
[ERROR] urls[37] = file:/Users/stevel/.m2/repository/org/scala-lang/scala-reflect/2.13.8/scala-reflect-2.13.8.jar
[ERROR] urls[38] = file:/Users/stevel/.m2/repository/org/scala-sbt/util-relation_2.13/1.7.0/util-relation_2.13-1.7.0.jar
[ERROR] urls[39] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc-persist_2.13/1.7.1/zinc-persist_2.13-1.7.1.jar
[ERROR] urls[40] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc-compile-core_2.13/1.7.1/zinc-compile-core_2.13-1.7.1.jar
[ERROR] urls[41] = file:/Users/stevel/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.13/1.1.2/scala-parser-combinators_2.13-1.1.2.jar
[ERROR] urls[42] = file:/Users/stevel/.m2/repository/net/openhft/zero-allocation-hashing/0.10.1/zero-allocation-hashing-0.10.1.jar
[ERROR] urls[43] = file:/Users/stevel/.m2/repository/org/scala-sbt/util-control_2.13/1.7.0/util-control_2.13-1.7.0.jar
[ERROR] urls[44] = file:/Users/stevel/.m2/repository/org/scala-sbt/zinc-classfile_2.13/1.7.1/zinc-classfile_2.13-1.7.1.jar
[ERROR] Number of foreign imports: 1
[ERROR] import: Entry[import  from realm ClassRealm[project>org.apache.spark:spark-parent_2.12:3.4.0-SNAPSHOT, parent: ClassRealm[maven.api, parent: null]]]
[ERROR] 
[ERROR] -----------------------------------------------------
[ERROR] : org.bouncycastle.jce.provider.BouncyCastleProvider
[ERROR] -> [Help 1]
[ERROR] 

{code}

HADOOP-17563 did upgrade hadoop's import of bouncy castle to 1.68 which did break spark builds earlier (asm, maven shade), but with SPARK-29729 this has gone away. spark will build with the hadoop 3.3.5 RC0 which has that JIRA. Somehow some other hadoop change is triggering this.


"	SPARK	Resolved	3	7	6681	pull-request-available
13562408	The `sbt console` command is not available	"# Unable to define expressions after executing the `build/sbt console` command

{code:java}
scala> val i = 1 // show
package $line3 {
  sealed class $read extends _root_.scala.Serializable {
    def <init>() = {
      super.<init>;
      ()
    };
    sealed class $iw extends _root_.java.io.Serializable {
      def <init>() = {
        super.<init>;
        ()
      };
      val i = 1
    };
    val $iw = new $iw.<init>
  };
  object $read extends scala.AnyRef {
    def <init>() = {
      super.<init>;
      ()
    };
    val INSTANCE = new $read.<init>
  }
}
warning: -target is deprecated: Use -release instead to compile against the correct platform API.
Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation
       ^
       error: expected class or object definition {code}
2.  Due to the default unused imports check, the error ""unused imports"" will be reported after executing the `build/sbt sql/console` command
{code:java}
Welcome to Scala 2.13.12 (OpenJDK 64-Bit Server VM, Java 17.0.9).
Type in expressions for evaluation. Or try :help.
warning: -target is deprecated: Use -release instead to compile against the correct platform API.
Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation
       import org.apache.spark.sql.catalyst.errors._
                                            ^
On line 6: error: object errors is not a member of package org.apache.spark.sql.catalyst
       import org.apache.spark.sql.catalyst.analysis._
                                                     ^
On line 4: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.catalyst.dsl._
                                                ^
On line 5: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.catalyst.errors._
                                                   ^
On line 6: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.catalyst.expressions._
                                                        ^
On line 7: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.catalyst.plans.logical._
                                                          ^
On line 8: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.catalyst.rules._
                                                  ^
On line 9: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.catalyst.util._
                                                 ^
On line 10: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.execution
                                   ^
On line 11: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.functions._
                                             ^
On line 12: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import org.apache.spark.sql.types._
                                         ^
On line 13: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import sqlContext.implicits._
                                   ^
On line 17: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=
       import sqlContext._
                         ^
On line 18: error: Unused import
       Applicable -Wconf / @nowarn filters for this fatal warning: msg=<part of the message>, cat=unused-imports, site=


scala>  {code}
It is necessary to delete `-Wunused:imports` from both SparkBuild. sbt and pom.xml in order to avoid this error"	SPARK	Resolved	4	7	6681	pull-request-available
13584693	Replace calls to bridged APIs based on SparkSession#sqlContext with SparkSession API	"In the internal code of Spark, there are instances where, despite having a SparkSession instance, the bridged APIs based on SparkSession#sqlContext are still used. So we can makes some simplifications:


1. `SparkSession#sqlContext#read` -> `SparkSession#read`


```scala
/**
   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a
   * `DataFrame`.
   * {{{
   *   sqlContext.read.parquet(""/path/to/file.parquet"")
   *   sqlContext.read.schema(schema).json(""/path/to/file.json"")
   * }}}
   *
   * @group genericdata
   * @since 1.4.0
   */
  def read: DataFrameReader = sparkSession.read
```

2. `SparkSession#sqlContext#setConf` -> `SparkSession#conf#set`


```scala
  /**
   * Set the given Spark SQL configuration property.
   *
   * @group config
   * @since 1.0.0
   */
  def setConf(key: String, value: String): Unit = {
    sparkSession.conf.set(key, value)
  }
```


3. `SparkSession#sqlContext#getConf` -> `SparkSession#conf#get`

```scala
/**
   * Return the value of Spark SQL configuration property for the given key.
   *
   * @group config
   * @since 1.0.0
   */
  def getConf(key: String): String = {
    sparkSession.conf.get(key)
  }
```

4. `SparkSession#sqlContext#createDataFrame` -> `SparkSession#createDataFrame`

```scala
/**
   * Creates a DataFrame from an RDD of Product (e.g. case classes, tuples).
   *
   * @group dataframes
   * @since 1.3.0
   */
  def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {
    sparkSession.createDataFrame(rdd)
  }
```

5. `SparkSession#sqlContext#sessionState` -> `SparkSession#sessionState`

```scala
private[sql] def sessionState: SessionState = sparkSession.sessionState
```

6. `SparkSession#sqlContext#sharedState` -> `SparkSession#sharedState`

```scala
private[sql] def sharedState: SharedState = sparkSession.sharedState
```

7. `SparkSession#sqlContext#streams` -> `SparkSession#streams`


```
/**
   * Returns a `StreamingQueryManager` that allows managing all the
   * [[org.apache.spark.sql.streaming.StreamingQuery StreamingQueries]] active on `this` context.
   *
   * @since 2.0.0
   */
  def streams: StreamingQueryManager = sparkSession.streams
```

8. `SparkSession#sqlContext#uncacheTable` -> ``SparkSession#catalog#uncacheTable`

```
/**
   * Removes the specified table from the in-memory cache.
   * @group cachemgmt
   * @since 1.3.0
   */
  def uncacheTable(tableName: String): Unit = {
    sparkSession.catalog.uncacheTable(tableName)
  }
```"	SPARK	Resolved	3	4	6681	pull-request-available
13548734	Upgrade Derby to 10.15.1.3+	"Currently, Derby is fixed at 10.14.2.0 because 10.15.1.3 requires JDK9 at least.

This issue aims to upgrade it after dropping Java 8."	SPARK	Resolved	4	7	6681	pull-request-available
13554101	Upgrade jetty to  9.4.53.v20231009	"fix 
 * [CVE-2023-44487|https://github.com/advisories/GHSA-qppj-fm5r-hxr3]
 * [CVE-2023-36478|https://github.com/advisories/GHSA-wgh7-54f2-x98r]"	SPARK	Resolved	3	7	6681	pull-request-available
13552395	Replace mutable.WrappedArray with mutable.ArraySeq	"{code:java}
@deprecated(""Use ArraySeq instead of WrappedArray; it can represent both, boxed and unboxed arrays"", ""2.13.0"")
type WrappedArray[X] = ArraySeq[X]
@deprecated(""Use ArraySeq instead of WrappedArray; it can represent both, boxed and unboxed arrays"", ""2.13.0"")
val WrappedArray = ArraySeq {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13551834	Make `InMemoryColumnarBenchmark` use AQE-aware utils to collect plans	After SPARK-42768, the default value of `spark.sql.optimizer.canChangeCachedPlanOutputPartitioning` has changed from false to true, so we should use AQE-aware utils to collect plans.	SPARK	Resolved	3	1	6681	pull-request-available
13559651	Replace `df.take(1).toSeq(0)` with `df.first()` in `ProtobufFunctionsSuite`	In `ProtobufFunctionsSuite`, there are some cases where `df.take(1).toSeq(0)` is used to get the first row in the DataFrame. This can be achieved by using the `.first()` API, which looks clearer and more concise.	SPARK	Resolved	4	4	6681	pull-request-available
13561099	Remove unnecessary override functions when constructing `WrappedCloseableIterator` in `ResponseValidator#wrapIterator`	"Should reuse functions defined in {{WrappedCloseableIterator}} instead of overriding them

 
*ResponseValidator#wrapIterator*
 
{code:java}
def wrapIterator[T <: GeneratedMessageV3, V <: CloseableIterator[T]](
    inner: V): WrappedCloseableIterator[T] = {
  new WrappedCloseableIterator[T] {

    override def innerIterator: Iterator[T] = inner

    override def hasNext: Boolean = {
      innerIterator.hasNext
    }

    override def next(): T = {
      verifyResponse {
        innerIterator.next()
      }
    }

    override def close(): Unit = {
      innerIterator match {
        case it: CloseableIterator[T] => it.close()
        case _ => // nothing
      }
    }
  }
} {code}
*WrappedCloseableIterator*
 
{code:java}
private[sql] abstract class WrappedCloseableIterator[E] extends CloseableIterator[E] {

  def innerIterator: Iterator[E]

  override def next(): E = innerIterator.next()

  override def hasNext: Boolean = innerIterator.hasNext

  override def close(): Unit = innerIterator match {
    case it: CloseableIterator[E] => it.close()
    case _ => // nothing
  }
} {code}"	SPARK	Resolved	3	4	6681	pull-request-available
13559710	Fix QueryExecutionErrorsSuite with Java 21	"[https://github.com/apache/spark/actions/runs/7014008773/job/19081075487]
{code:java}
[info] - FAILED_EXECUTE_UDF: execute user defined function with registered UDF *** FAILED *** (42 milliseconds)
15247[info]   java.lang.IllegalArgumentException: For parameter 'reason' value 'java.lang.StringIndexOutOfBoundsException: Range [5, 6) out of bounds for length 5' does not match: java.lang.StringIndexOutOfBoundsException: begin 5, end 6, length 5
15248[info]   at org.apache.spark.SparkFunSuite.$anonfun$checkError$2(SparkFunSuite.scala:357)
15249[info]   at org.apache.spark.SparkFunSuite.$anonfun$checkError$2$adapted(SparkFunSuite.scala:352)
15250[info]   at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)
15251[info]   at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)
15252[info]   at scala.collection.AbstractIterable.foreach(Iterable.scala:933) {code}"	SPARK	Resolved	3	1	6681	pull-request-available
13573797	Make Spark build with -release instead of -target 	https://github.com/scala/scala/pull/9982	SPARK	Resolved	3	7	6681	pull-request-available
13551530	`sbt doc` execution failed.	"run 

 
{code:java}
build/sbt clean doc -Phadoop-3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pvolcano
{code}
 
{code:java}
[info] Main Scala API documentation successful.
[error] sbt.inc.Doc$JavadocGenerationFailed
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cachedJavadoc$1(Doc.scala:51)
[error]         at sbt.inc.Doc$$anonfun$cachedJavadoc$2.run(Doc.scala:41)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$prepare$1(Doc.scala:62)
[error]         at sbt.inc.Doc$$anonfun$prepare$5.run(Doc.scala:57)
[error]         at sbt.inc.Doc$.go$1(Doc.scala:73)
[error]         at sbt.inc.Doc$.$anonfun$cached$5(Doc.scala:82)
[error]         at sbt.inc.Doc$.$anonfun$cached$5$adapted(Doc.scala:81)
[error]         at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:220)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cached$1(Doc.scala:85)
[error]         at sbt.inc.Doc$$anonfun$cached$7.run(Doc.scala:68)
[error]         at sbt.Defaults$.$anonfun$docTaskSettings$4(Defaults.scala:2178)
[error]         at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error]         at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:63)
[error]         at sbt.std.Transform$$anon$4.work(Transform.scala:69)
[error]         at sbt.Execute.$anonfun$submit$2(Execute.scala:283)
[error]         at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:24)
[error]         at sbt.Execute.work(Execute.scala:292)
[error]         at sbt.Execute.$anonfun$submit$1(Execute.scala:283)
[error]         at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
[error]         at sbt.CompletionService$$anon$2.call(CompletionService.scala:65)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[error]         at java.lang.Thread.run(Thread.java:750)
[error] sbt.inc.Doc$JavadocGenerationFailed
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cachedJavadoc$1(Doc.scala:51)
[error]         at sbt.inc.Doc$$anonfun$cachedJavadoc$2.run(Doc.scala:41)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$prepare$1(Doc.scala:62)
[error]         at sbt.inc.Doc$$anonfun$prepare$5.run(Doc.scala:57)
[error]         at sbt.inc.Doc$.go$1(Doc.scala:73)
[error]         at sbt.inc.Doc$.$anonfun$cached$5(Doc.scala:82)
[error]         at sbt.inc.Doc$.$anonfun$cached$5$adapted(Doc.scala:81)
[error]         at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:220)
[error]         at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cached$1(Doc.scala:85)
[error]         at sbt.inc.Doc$$anonfun$cached$7.run(Doc.scala:68)
[error]         at sbt.Defaults$.$anonfun$docTaskSettings$4(Defaults.scala:2178)
[error]         at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error]         at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:63)
[error]         at sbt.std.Transform$$anon$4.work(Transform.scala:69)
[error]         at sbt.Execute.$anonfun$submit$2(Execute.scala:283)
[error]         at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:24)
[error]         at sbt.Execute.work(Execute.scala:292)
[error]         at sbt.Execute.$anonfun$submit$1(Execute.scala:283)
[error]         at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
[error]         at sbt.CompletionService$$anon$2.call(CompletionService.scala:65)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[error]         at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[error]         at java.lang.Thread.run(Thread.java:750)
[error] (network-yarn / Compile / doc) sbt.inc.Doc$JavadocGenerationFailed
[error] (network-shuffle / Compile / doc) sbt.inc.Doc$JavadocGenerationFailed
[error] Total time: 126 s (02:06), completed 2023-9-21 20:51:43
{code}
 

 "	SPARK	Resolved	4	1	6681	pull-request-available
13555213	Upgrade jersey to 2.41	https://github.com/eclipse-ee4j/jersey/releases/tag/2.41	SPARK	Resolved	4	4	6681	pull-request-available
13555089	Move `mllib` and `mllib-local` to separate test groups.	"Improve the stability of GitHub Action tests.
 
 
 
 
 
 
 
 "	SPARK	Resolved	3	4	6681	pull-request-available
13543376	Clean up the compilation warnings related to `it will become a keyword in Scala 3`	"{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/JavaTypeInferenceSuite.scala:74:21: [deprecation @  | origin= | version=2.13.7] Wrap `enum` in backticks to use it as an identifier, it will become a keyword in Scala 3.
[warn]   @BeanProperty var enum: java.time.Month = _ {code}
enum will become a keyword in Scala 3, this also includes {{export}} and {{{}given{}}}.

 

Scala 2.13
{code:java}
Welcome to Scala 2.13.12 (OpenJDK 64-Bit Server VM, Java 17.0.8).
Type in expressions for evaluation. Or try :help.


scala> val enum: Int = 1
           ^
       warning: Wrap `enum` in backticks to use it as an identifier, it will become a keyword in Scala 3. [quickfixable]
val enum: Int = 1


scala> val export: Int = 1
           ^
       warning: Wrap `export` in backticks to use it as an identifier, it will become a keyword in Scala 3. [quickfixable]
val export: Int = 1


scala> val given: Int = 1
           ^
       warning: Wrap `given` in backticks to use it as an identifier, it will become a keyword in Scala 3. [quickfixable]
val given: Int = 1 {code}
 
Scala 3
 
{code:java}
Welcome to Scala 3.3.1 (17.0.8, Java OpenJDK 64-Bit Server VM).
Type in expressions for evaluation. Or try :help.
                                                                                                                                                                                                                                                     
scala> val enum: Int = 1
-- [E032] Syntax Error: --------------------------------------------------------
1 |val enum: Int = 1
  |    ^^^^
  |    pattern expected
  |
  | longer explanation available when compiling with `-explain`
                                                                                                                                                                                                                                                     
scala> val export: Int = 1
-- [E032] Syntax Error: --------------------------------------------------------
1 |val export: Int = 1
  |    ^^^^^^
  |    pattern expected
  |
  | longer explanation available when compiling with `-explain`
                                                                                                                                                                                                                                                     
scala> val given: Int = 1
-- [E040] Syntax Error: --------------------------------------------------------
1 |val given: Int = 1
  |         ^
  |         an identifier expected, but ':' found
  |
  | longer explanation available when compiling with `-explain` {code}
 
 
 "	SPARK	Resolved	4	7	6681	pull-request-available
13556302	Upgrade dropwizard metrics 4.2.21	"[https://github.com/dropwizard/metrics/releases/tag/v4.2.21]

[https://github.com/dropwizard/metrics/releases/tag/v4.2.20]

 "	SPARK	Resolved	3	7	6681	pull-request-available
13580506	Upgrade scala-parser-combinators to 2.4	https://github.com/scala/scala-parser-combinators/releases/tag/v2.4.0	SPARK	Resolved	3	4	6681	pull-request-available
13564342	Upgrade Apache commons-pool2 to 2.12.0	https://github.com/apache/commons-pool/blob/rel/commons-pool-2.12.0/RELEASE-NOTES.txt	SPARK	Resolved	3	4	6681	pull-request-available
13563319	Wrap the `export` in the package name with backticks	`export` will be a keyword in Scala 3, using it directly in the package name will cause a compilation error.	SPARK	Resolved	4	4	6681	pull-request-available
13555442	Replace `IterableOnceOps#aggregate` with `IterableOnceOps#foldLeft`	"{code:java}
@deprecated(""`aggregate` is not relevant for sequential collections. Use `foldLeft(z)(seqop)` instead."", ""2.13.0"")
def aggregate[B](z: => B)(seqop: (B, A) => B, combop: (B, B) => B): B = foldLeft(z)(seqop) {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13555759	Clean up type use of `BufferedIterator/CanBuildFrom/Traversable`	"* type BufferedIterator in package scala is deprecated (since 2.13.0)
 * type CanBuildFrom in package generic is deprecated (since 2.13.0)
 * type Traversable in package scala is deprecated (since 2.13.0)

 
{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala:67:12: type BufferedIterator in package scala is deprecated (since 2.13.0): Use scala.collection.BufferedIterator instead of scala.BufferedIterator
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.execution.GroupedIterator.input, origin=scala.BufferedIterator, version=2.13.0
[warn]     input: BufferedIterator[InternalRow],
[warn]            ^ {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13564344	Refactor `ExecutorFailureTracker#maxNumExecutorFailures` to avoid unnecessary computations when `MAX_EXECUTOR_FAILURES` is configured	"{code:java}
def maxNumExecutorFailures(sparkConf: SparkConf): Int = {
  val effectiveNumExecutors =
    if (Utils.isStreamingDynamicAllocationEnabled(sparkConf)) {
      sparkConf.get(STREAMING_DYN_ALLOCATION_MAX_EXECUTORS)
    } else if (Utils.isDynamicAllocationEnabled(sparkConf)) {
      sparkConf.get(DYN_ALLOCATION_MAX_EXECUTORS)
    } else {
      sparkConf.get(EXECUTOR_INSTANCES).getOrElse(0)
    }
  // By default, effectiveNumExecutors is Int.MaxValue if dynamic allocation is enabled. We need
  // avoid the integer overflow here.
  val defaultMaxNumExecutorFailures = math.max(3,
    if (effectiveNumExecutors > Int.MaxValue / 2) Int.MaxValue else 2 * effectiveNumExecutors)

  sparkConf.get(MAX_EXECUTOR_FAILURES).getOrElse(defaultMaxNumExecutorFailures)
} {code}
The result of defaultMaxNumExecutorFailures is calculated first, even if {{MAX_EXECUTOR_FAILURES}} is configured now
 
 
 
 
 "	SPARK	Resolved	4	4	6681	pull-request-available
13562597	Change Utils.isJavaVersionAtLeast21 to use the utility method from commons-lang3	commons-lang3 added support for checking `JAVA_21` after version 3.13.0, so we can directly use the utility methods provided by commons-lang3.	SPARK	Resolved	3	4	6681	pull-request-available
13541272	Migrating Junit4 to Junit5	"JUnit5 is a powerful and flexible update to the JUnit framework, and it provides a variety of improvements and new features to organize and
describe test cases, as well as help in understanding test results：
 # JUnit 5 leverages features from Java 8 or later, such as lambda functions, making tests more powerful and easier to maintain, but Junit 4 still a Java 7 compatible version
 # JUnit 5 has added some useful new features for describing, organizing, and executing tests. For examples: [Parameterized Tests|https://junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests] and [Conditional Test Execution|https://junit.org/junit5/docs/current/user-guide/#extensions-conditions] may make our test code look simpler, [Parallel Execution|https://junit.org/junit5/docs/current/user-guide/#writing-tests-parallel-execution] may make our test faster

 

More importantly, Junit4 is currently an inactive project, which has not released a new version for more than two years

 "	SPARK	Resolved	3	7	6681	pull-request-available
13576213	Remove unused import `spark/connect/common.proto` from `spark/connect/relations.proto`	"fix compile waring:

 
{code:java}
spark/connect/relations.proto:26:1: warning: Import spark/connect/common.proto is unused. {code}"	SPARK	Resolved	3	4	6681	pull-request-available
13559570	Replace explicit  `ArrayOps#toSeq` with `s.c.immutable.ArraySeq.unsafeWrapArray`	"There is a behavioral difference between Scala 2.13 and 2.12 for explicit `ArrayOps.toSeq` calls, similar to the implicit conversion from `Array` to `Seq`.

In Scala 2.12, it returns a `mutable.WrappedArray`, which does not involve a collection copy.

```scala
Welcome to Scala 2.12.18 (OpenJDK 64-Bit Server VM, Java 17.0.9).
Type in expressions for evaluation. Or try :help.

scala> Array(1,2,3).toSeq
res0: Seq[Int] = WrappedArray(1, 2, 3)
```

However, in Scala 2.13, it returns an `immutable.ArraySeq` that with collection copy.

Since we have always used the non-collection copy behavior for this explicit conversion in the era of Scala 2.12, it is safe to assume that no collection copy is needed for Scala 2.13."	SPARK	Resolved	3	7	6681	pull-request-available
13556165	Upgrade commons-io to 2.15.0	https://commons.apache.org/proper/commons-io/changes-report.html#a2.15.0	SPARK	Resolved	3	7	6681	pull-request-available
13554684	Replace `s.c.MapOps.filterKeys` with `s.c.MapOps.view.filterKeys`	"{code:java}
/** Filters this map by retaining only keys satisfying a predicate.
  *  @param  p   the predicate used to test keys
  *  @return an immutable map consisting only of those key value pairs of this map where the key satisfies
  *          the predicate `p`. The resulting map wraps the original map without copying any elements.
  */
@deprecated(""Use .view.filterKeys(f). A future version will include a strict version of this method (for now, .view.filterKeys(p).toMap)."", ""2.13.0"")
def filterKeys(p: K => Boolean): MapView[K, V] = new MapView.FilterKeys(this, p) {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13555752	Fix `method any2stringadd in object Predef is deprecated`	"{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala:720:17: method any2stringadd in object Predef is deprecated (since 2.13.0): Implicit injection of + is deprecated. Convert to String to call +
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen.nullSafeEval, origin=scala.Predef.any2stringadd, version=2.13.0
[warn]         leftGen.code + ctx.nullSafeExec(left.nullable, leftGen.isNull) {
[warn]                 ^ {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13578416	Clean up the use of deprecated APIs related to `o.rocksdb.Logger`	"{code:java}
/**
 * <p>AbstractLogger constructor.</p>
 *
 * <p><strong>Important:</strong> the log level set within
 * the {@link org.rocksdb.Options} instance will be used as
 * maximum log level of RocksDB.</p>
 *
 * @param options {@link org.rocksdb.Options} instance.
 *
 * @deprecated Use {@link Logger#Logger(InfoLogLevel)} instead, e.g. {@code new
 *     Logger(options.infoLogLevel())}.
 */
@Deprecated
public Logger(final Options options) {
  this(options.infoLogLevel());
} {code}"	SPARK	Resolved	3	4	6681	pull-request-available
13555758	  Clean up the deprecated API usage related to `StringContext/StringOps`	" 
{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/javaCode.scala:258:30: method treatEscapes in object StringContext is deprecated (since 2.13.0): use processEscapes
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalyst.expressions.codegen.Block.foldLiteralArgs, origin=scala.StringContext.treatEscapes, version=2.13.0
[warn]     buf.append(StringContext.treatEscapes(strings.next()))
[warn]                              ^
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/javaCode.scala:270:32: method treatEscapes in object StringContext is deprecated (since 2.13.0): use processEscapes
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.sql.catalyst.expressions.codegen.Block.foldLiteralArgs, origin=scala.StringContext.treatEscapes, version=2.13.0
[warn]       buf.append(StringContext.treatEscapes(strings.next()))
[warn]   {code}
 

 
 * method checkLengths in class StringContext is deprecated (since 2.13.0)
 * method treatEscapes in object StringContext is deprecated (since 2.13.0)
 * method replaceAllLiterally in class StringOps is deprecated (since 2.13.2)"	SPARK	Resolved	3	7	6681	pull-request-available
13570660	Upgrade jetty to 11.0.20	"fix 
 * [CVE-2024-22201|https://github.com/advisories/GHSA-rggv-cv7r-mw98] - HTTP/2 connection not closed after idle timeout when TCP congested"	SPARK	Resolved	3	7	6681	pull-request-available
13572960	Replace Deprecated `JsonParser#getCurrentName` with `JsonParser#currentName`	"[https://github.com/FasterXML/jackson-core/blob/8fba680579885bf9cdae72e93f16de557056d6e3/src/main/java/com/fasterxml/jackson/core/JsonParser.java#L1521-L1551]

 
{code:java}
    /**
     * Deprecated alias of {@link #currentName()}.
     *
     * @return Name of the current field in the parsing context
     *
     * @throws IOException for low-level read issues, or
     *   {@link JsonParseException} for decoding problems
     *
     * @deprecated Since 2.17 use {@link #currentName} instead.
     */
    @Deprecated
    public abstract String getCurrentName() throws IOException;    /**
     * Method that can be called to get the name associated with
     * the current token: for {@link JsonToken#FIELD_NAME}s it will
     * be the same as what {@link #getText} returns;
     * for field values it will be preceding field name;
     * and for others (array values, root-level values) null.
     *
     * @return Name of the current field in the parsing context
     *
     * @throws IOException for low-level read issues, or
     *   {@link JsonParseException} for decoding problems
     *
     * @since 2.10
     */
    public String currentName() throws IOException {
        // !!! TODO: switch direction in 2.18 or later
        return getCurrentName();
    } {code}"	SPARK	Resolved	3	4	6681	pull-request-available
13587515	Remove bouncycastle-related test dependencies from hive-thriftserver module	After SPARK-49066 merged, other than `OrcEncryptionSuite`, the test cases for writing Orc data no longer require the use of `FakeKeyProvider`. As a result, `hive-thriftserver` no longer needs these test dependencies.	SPARK	Resolved	3	4	6681	pull-request-available
13580320	Upgrade commons-cli to 1.8.0	"* [https://commons.apache.org/proper/commons-cli/changes-report.html#a1.7.0]
 * [https://commons.apache.org/proper/commons-cli/changes-report.html#a1.8.0|https://commons.apache.org/proper/commons-cli/changes-report.html#a1.7.8]"	SPARK	Resolved	3	4	6681	pull-request-available
13581484	Upgrade jetty to 11.0.21	* https://github.com/jetty/jetty.project/releases/tag/jetty-11.0.21	SPARK	Resolved	3	7	6681	pull-request-available
13563315	Extract a helper method to eliminate the duplicate code in `GrpcExceptionConverter` that retrieves `MessageParameters` from `ErrorParams`	"{code:java}
params.errorClass match {
  case Some(_) => params.messageParameters
  case None => Map(""message"" -> params.message)
} {code}
The above code pattern appears 7 times in `GrpcExceptionConverter`."	SPARK	Resolved	3	4	6681	pull-request-available
13248010	Ensure executorMemoryHead requested value not less than MEMORY_OFFHEAP_SIZE when MEMORY_OFFHEAP_ENABLED is true	"If MEMORY_OFFHEAP_ENABLED is true, we should ensure executorOverheadMemory not less than MEMORY_OFFHEAP_SIZE, otherwise the memory resource requested for executor may be not enough.

 

 

If MEMORY_OFFHEAP_ENABLED is true, add MEMORY_OFFHEAP_SIZE to resource requested for executor to ensure instance has enough memory to use."	SPARK	Resolved	3	4	6681	release-notes
13555275	Replace `s.c.mutable.MapOps#transform` with `s.c.mutable.MapOps#mapValuesInPlace`	"{code:java}
@deprecated(""Use mapValuesInPlace instead"", ""2.13.0"")
@inline final def transform(f: (K, V) => V): this.type = mapValuesInPlace(f) {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13556623	Remove the defensive null check added in SPARK-39553.	"{code:java}
def unregisterShuffle(shuffleId: Int): Unit = {
    shuffleStatuses.remove(shuffleId).foreach { shuffleStatus =>
      // SPARK-39553: Add protection for Scala 2.13 due to https://github.com/scala/bug/issues/12613
      // We should revert this if Scala 2.13 solves this issue.
      if (shuffleStatus != null) {
        shuffleStatus.invalidateSerializedMapOutputStatusCache()
        shuffleStatus.invalidateSerializedMergeOutputStatusCache()
      }
    }
  } {code}
This issue has been fixed in Scala 2.13.9."	SPARK	Resolved	4	4	6681	pull-request-available
13576356	Upgrade commons-text to 1.12.0	https://github.com/apache/commons-text/blob/rel/commons-text-1.12.0/RELEASE-NOTES.txt	SPARK	Resolved	3	7	6681	pull-request-available
13554920	"Remove redundant""Auto-application to `()` is deprecated"" compile suppression rules."	"Due to the issue https://github.com/scalatest/scalatest/issues/2297, we need to wait until we upgrade a scalatest version before removing these suppression rules.

Maybe 3.2.18"	SPARK	Resolved	4	4	6681	pull-request-available
13552048	Make the sbt doc command execute successfully with Java 17	"{code:java}
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/Picked up JAVA_TOOL_OPTIONS:-Duser.language=en
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBTypeInfo.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/ArrayWrappers.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVIndex.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/RocksDB.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/RocksDBTypeInfo.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/UnsupportedStoreVersionException.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDB.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreIterator.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStore.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreView.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVTypeInfo.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/RocksDBIterator.java...
[error] Loading source file /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java...
[error] Constructing Javadoc information...
[error] Building index for all the packages and classes...
[error] Standard Doclet version 17.0.8+7-LTS
[error] Building tree for all the packages and classes...
[error] /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStore.java:32:1:  error: heading used out of sequence: <H3>, compared to implicit preceding heading: <H1>
[error]  * <h3>Serialization</h3>
[error]    ^Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/InMemoryStore.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/KVIndex.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/KVStore.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/KVStoreIterator.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/KVStoreSerializer.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/KVStoreView.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/KVTypeInfo.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/LevelDB.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/LevelDB.TypeAliases.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/RocksDB.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/RocksDB.TypeAliases.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/UnsupportedStoreVersionException.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/package-summary.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/org/apache/spark/util/kvstore/package-tree.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/constant-values.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/serialized-form.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/overview-tree.html...
[error] Building index for all classes...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/allclasses-index.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/allpackages-index.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/index-all.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/index.html...
[error] Generating /Users/yangjie01/SourceCode/git/spark-mine-sbt/common/kvstore/target/scala-2.13/api/help-doc.html...
[error] 1 error
[warn] javadoc exited with exit code 1 



[error] sbt.inc.Doc$JavadocGenerationFailed
[error] 	at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cachedJavadoc$1(Doc.scala:51)
[error] 	at sbt.inc.Doc$$anonfun$cachedJavadoc$2.run(Doc.scala:41)
[error] 	at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$prepare$1(Doc.scala:62)
[error] 	at sbt.inc.Doc$$anonfun$prepare$5.run(Doc.scala:57)
[error] 	at sbt.inc.Doc$.go$1(Doc.scala:73)
[error] 	at sbt.inc.Doc$.$anonfun$cached$5(Doc.scala:82)
[error] 	at sbt.inc.Doc$.$anonfun$cached$5$adapted(Doc.scala:81)
[error] 	at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:220)
[error] 	at sbt.inc.Doc$.sbt$inc$Doc$$$anonfun$cached$1(Doc.scala:85)
[error] 	at sbt.inc.Doc$$anonfun$cached$7.run(Doc.scala:68)
[error] 	at sbt.Defaults$.$anonfun$docTaskSettings$4(Defaults.scala:2178)
[error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:63)
[error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:69)
[error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:283)
[error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:24)
[error] 	at sbt.Execute.work(Execute.scala:292)
[error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:283)
[error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
[error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:65)
[error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[error] 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[error] 	at java.base/java.lang.Thread.run(Thread.java:833)
[error] (kvstore / Compile / doc) sbt.inc.Doc$JavadocGenerationFailed
[error] Total time: 118 s (01:58), completed Sep 26, 2023, 10:11:02 PM{code}"	SPARK	Resolved	4	7	6681	pull-request-available
13540299	`Logging plan changes for execution` test failed	"run {{build/sbt clean ""sql/test"" -Dtest.exclude.tags=org.apache.spark.tags.ExtendedSQLTest,org.apache.spark.tags.SlowSQLTest}}

{{}}
{code:java}
2023-06-15T19:58:34.4105460Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32mQueryExecutionSuite:�[0m�[0m
2023-06-15T19:58:34.5395268Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file (77 milliseconds)�[0m�[0m
2023-06-15T19:58:34.5856902Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to an existing file (49 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6099849Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to non-existing folder (25 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6136467Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info by invalid path (4 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6425071Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file - explainMode=formatted (28 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7084916Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- limit number of fields by sql config (66 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7432299Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- check maximum fields restriction (34 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7554546Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- toString() exception/error handling (11 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7621424Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- SPARK-28346: clone the query plan between different stages (6 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8001412Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m- Logging plan changes for execution *** FAILED *** (12 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8007977Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m  testAppender.loggingEvents.exists(((x$10: org.apache.logging.log4j.core.LogEvent) => x$10.getMessage().getFormattedMessage().contains(expectedMsg))) was false (QueryExecutionSuite.scala:232)�[0m�[0m 

{code}
 

but run {{build/sbt ""sql/testOnly *QueryExecutionSuite""}} not this issue, need to investigate. "	SPARK	Resolved	3	1	6681	pull-request-available
13553635	Use pattern matching for type checking and conversion	"Refer to [JEP 394|https://openjdk.org/jeps/394]

Example:
{code:java}
if (obj instanceof String) {
    String str = (String) obj;
    System.out.println(str);
} {code}

Can be replaced with

 
{code:java}
if (obj instanceof String str) {
    System.out.println(str);
} {code}
The new code look more compact"	SPARK	Resolved	4	7	6681	pull-request-available
13550448	Downgrade scala-maven-plugin to 4.7.1	"The `scala-maven-plugin` versions 4.7.2 and later will try to automatically append the `-release` option as a Scala compilation argument when it is not specified by the user:

1. 4.7.2 and 4.8.0: Try to add the `-release` option for Scala versions 2.13.9 and higher.
2. 4.8.1: Try to append the `-release` option for Scala versions 2.12.x/2.13.x/3.1.1, and append `-java-output-version` for Scala 3.1.2.

The addition of the `-release` option has caused issues mentioned in SPARK-44376 | https://github.com/apache/spark/pull/41943 and https://github.com/apache/spark/pull/40442#issuecomment-1474161688. This is because the `-release` option has stronger compilation restrictions than `-target`, ensuring not only bytecode format, but also that the API used in the code is compatible with the specified version of Java. However, many APIs in the `sun.*` package are not `exports` in Java 11, 17, and 21, such as `sun.nio.ch.DirectBuffer`, `sun.util.calendar.ZoneInfo`, and `sun.nio.cs.StreamDecoder`, making them invisible when compiling across different versions.

For discussions within the Scala community, see https://github.com/scala/bug/issues/12643, https://github.com/scala/bug/issues/12824, https://github.com/scala/bug/issues/12866.

I have also submitted an issue to the `scala-maven-plugin` community to discuss the possibility of adding additional settings to control the addition of the `-release` option: https://github.com/davidB/scala-maven-plugin/issues/722.

For Apache Spark 4.0, in the short term, I suggest downgrading `scala-maven-plugin` to version 4.7.1 to avoid the automatic adding the `-release` option as a Scala compilation argument. In the long term, we should reduce our use of APIs that are not `exports` for compatibility with the `-release` compilation option. "	SPARK	Resolved	3	4	6681	pull-request-available
13555753	Clean up the deprecated API usage related to `SeqOps`	"* method transform in trait SeqOps is deprecated (since 2.13.0)
 * method reverseMap in trait SeqOps is deprecated (since 2.13.0)
 * method union in trait SeqOps is deprecated (since 2.13.0)

{code:java}
[warn] /Users/yangjie01/SourceCode/git/spark-mine-sbt/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala:675:15: method transform in trait SeqOps is deprecated (since 2.13.0): Use `mapInPlace` on an `IndexedSeq` instead
[warn] Applicable -Wconf / @nowarn filters for this warning: msg=<part of the message>, cat=deprecation, site=org.apache.spark.ml.classification.LogisticRegression.train.$anonfun, origin=scala.collection.mutable.SeqOps.transform, version=2.13.0
[warn]       centers.transform(_ / numCoefficientSets)
[warn]               ^ {code}"	SPARK	Resolved	3	7	6681	pull-request-available
13553325	Replace `Proxy.getProxyClass()` with `Proxy.newProxyInstance().getClass`	"{code:java}
 * @deprecated Proxy classes generated in a named module are encapsulated
 *      and not accessible to code outside its module.
 *      {@link Constructor#newInstance(Object...) Constructor.newInstance}
 *      will throw {@code IllegalAccessException} when it is called on
 *      an inaccessible proxy class.
 *      Use {@link #newProxyInstance(ClassLoader, Class[], InvocationHandler)}
 *      to create a proxy instance instead.
 *
 * @see <a href=""#membership"">Package and Module Membership of Proxy Class</a>
 * @revised 9
 */
@Deprecated
@CallerSensitive
public static Class<?> getProxyClass(ClassLoader loader,
                                     Class<?>... interfaces)
    throws IllegalArgumentException {code}"	SPARK	Resolved	4	7	6681	pull-request-available
13592847	Deprecate getErrorClass in SparkThrowable	"As a follow up of the conclusion of https://issues.apache.org/jira/browse/SPARK-46810:
# Deprecate the getErrorClass() method in SparkThrowable
# Add new method getCondition as the replacement of getErrorClass"	SPARK	Resolved	3	7	6986	pull-request-available
13210240	Use Proleptic Gregorian calendar	"Spark 2.4 and previous versions use a hybrid calendar - Julian + Gregorian in date/timestamp parsing, functions and expressions. The ticket aims to switch Spark on Proleptic Gregorian calendar, and use java.time classes introduced in Java 8 for timestamp/date manipulations. One of the purpose of switching on Proleptic Gregorian calendar is to conform to SQL standard which supposes such calendar.

*Release note:*

Spark 3.0 has switched on Proleptic Gregorian calendar in parsing, formatting, and converting dates and timestamps as well as in extracting sub-components like years, days and etc. It uses Java 8 API classes from the java.time packages that based on [ISO chronology |https://docs.oracle.com/javase/8/docs/api/java/time/chrono/IsoChronology.html]. Previous versions of Spark performed those operations by using [the hybrid calendar|https://docs.oracle.com/javase/7/docs/api/java/util/GregorianCalendar.html] (Julian + Gregorian). The changes might impact on the results for dates and timestamps before October 15, 1582 (Gregorian)."	SPARK	Resolved	3	14	6986	release-notes
13562286	SessionHolder doesn't throw exceptions from internalError()	Need to throw SparkException returned by internalError in SessionHolder otherwise users won't see the internal error	SPARK	Resolved	3	1	6986	pull-request-available
13563541	Handle a non-existent error class	Check existence of error class in Scala connect client while converting errors from proto to Spark exceptions. And when an error class doesn't exists, use some legacy error class. This will allow to avoid internal errors when the client receives non-existent error class from the connect server.	SPARK	Open	3	4	6986	pull-request-available
13568684	Wrong error message for incorrect ANSI intervals	"When Spark SQL cannot recognise ANSI interval, it outputs wrong pattern for particular ANSI interval. For example, it cannot recognise year-month interval, but says about day-time interval:

{code:sql}
spark-sql (default)> select interval '-\t2-2\t' year to month;

Interval string does not match year-month format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval year to month: -	2-2	. (line 1, pos 16)

== SQL ==
select interval '-\t2-2\t' year to month
----------------^^^
{code}
"	SPARK	Resolved	4	1	6986	pull-request-available
13561281	Require an error class in AnalysisException	Modify AnalysisException to accept only an error class instead of an error message while creating the exception.	SPARK	Resolved	3	7	6986	pull-request-available
13213473	Add Benchmark for XORShiftRandom	Currently, benchmark for XORShiftRandom is mixed with implementation: [https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala#L70-L107] . Need to extract the code and create a separate benchmark.	SPARK	Resolved	4	7	6986	low-hanging-fruit
13564722	Add DAYNAME function	"Add function DAYNAME which returns three-letter abbreviation of a given date/timestamp to various spark apis (scala, python, r, connect)

Similar to [46515|https://issues.apache.org/jira/projects/SPARK/issues/SPARK-46515?filter=allissues]


cc: [~mitkedb]"	SPARK	Resolved	3	4	6986	pull-request-available
13591397	Detect unused message parameters	"The passed error message parameters and places holders in message format can not be matched. From the code maintainability perspective, it would be nice to detect such cases while running tests.

For example, the error message format could look like:

{code}
  ""CANNOT_UP_CAST_DATATYPE"" : {
    ""message"" : [
      ""Cannot up cast <expression> from <sourceType> to <targetType>."",
      ""<details>""
    ],
    ""sqlState"" : ""42846""
  },
{code}
 
but the passed message parameters have extra parameter:

{code:scala}
          messageParameters = Map(
            ""expression"" -> ""CAST('aaa' AS LONG)"",
            ""sourceType"" -> ""STRING"",
            ""targetType"" -> ""LONG"",
            ""op"" -> ""CAST"", // unused parameter
            ""details"" -> ""implicit cast""
          ))
{code}
"	SPARK	Resolved	4	7	6986	pull-request-available
13568223	Check SparkUnsupportedOperationException instead of UnsupportedOperationException	Use checkError() to test the SparkUnsupportedOperationException exception instead of UnsupportedOperationException in the SQL project.	SPARK	Resolved	3	7	6986	pull-request-available
13595197	Rename errorClass to condition in errors of the JSON format	"Rename ""errorClass"" to ""condition"" in errors in the JSON formats to follow up the agreement of https://issues.apache.org/jira/browse/SPARK-46810"	SPARK	Open	3	7	6986	pull-request-available
13557505	Align codegen and non-codegen implementation of Encode	Fix implementation of codegen and non-codegen in the Encode expression, and raise the same error for invalid input charset.	SPARK	Resolved	3	1	6986	pull-request-available
13290181	Convert Catalyst's DATE/TIMESTAMP to Java Date/Timestamp via local date-time	"By default, collect() returns java.sql.Timestamp/Date instances with offsets derived from internal values of Catalyst's TIMESTAMP/DATE that store microseconds since the epoch. The conversion from internal values to java.sql.Timestamp/Date based on Proleptic Gregorian calendar but converting the resulted values before 1582 year to strings produces timestamp/date string in Julian calendar. For example:
{code}
scala> sql(""select date '1100-10-10'"").collect()
res1: Array[org.apache.spark.sql.Row] = Array([1100-10-03])
{code} 

This can be fixed if internal Catalyst's values are converted to local date-time in Gregorian calendar,  and construct local date-time from the resulted year, month, ..., seconds in Julian calendar."	SPARK	Resolved	3	7	6986	correctness
13562107	Convert IllegalStateException to internalError in session iterators	Convert `IllegalStateException` in the two classes `UpdatingSessionsIterator` and `MergingSessionsIterator` to `SparkException.internalError`.	SPARK	Resolved	3	7	6986	pull-request-available
13568460	Replace IllegalArgumentException by SparkIllegalArgumentException in sql/api	Replace all IllegalArgumentException by SparkIllegalArgumentException in *sql/api* code base, and introduce new legacy error classes with the _LEGACY_ERROR_TEMP_ prefix.	SPARK	Resolved	3	7	6986	pull-request-available
13563071	Pass message parameters in metadata of ErrorInfo	Put message parameters together with an error class in the `messageParameter` field in metadata of `ErrorInfo`. Now, it is not possible to re-construct an error having only an error class.	SPARK	Resolved	3	4	6986	pull-request-available
13354124	LOAD DATA doesn't refresh v1 table cache	"The example below portraits the issue:
1. Create a source table:
{code:sql}
spark-sql> CREATE TABLE src_tbl (c0 int, part int) USING hive PARTITIONED BY (part);
spark-sql> INSERT INTO src_tbl PARTITION (part=0) SELECT 0;
spark-sql> SHOW TABLE EXTENDED LIKE 'src_tbl' PARTITION (part=0);
default	src_tbl	false	Partition Values: [part=0]
Location: file:/Users/maximgekk/proj/load-data-refresh-cache/spark-warehouse/src_tbl/part=0
...
{code}
2. Load data from the source table to a cached destination table:
{code:sql}
spark-sql> CREATE TABLE dst_tbl (c0 int, part int) USING hive PARTITIONED BY (part);
spark-sql> INSERT INTO dst_tbl PARTITION (part=1) SELECT 1;
spark-sql> CACHE TABLE dst_tbl;
spark-sql> SELECT * FROM dst_tbl;
1	1
spark-sql> LOAD DATA LOCAL INPATH '/Users/maximgekk/proj/load-data-refresh-cache/spark-warehouse/src_tbl/part=0' INTO TABLE dst_tbl PARTITION (part=0);
spark-sql> SELECT * FROM dst_tbl;
1	1
{code}
The last query does not show recently loaded data from the source table."	SPARK	Resolved	3	7	6986	correctness
13564771	Port classifyException() in JDBC dialects on error classes	Port the existing classifyException method which accepts a description to new one w/ an error class.	SPARK	Resolved	3	7	6986	pull-request-available
13437045	Test the error class: INDEX_OUT_OF_BOUNDS	"Add at least one test for the error class *INDEX_OUT_OF_BOUNDS* to QueryExecutionErrorsSuite. The test should cover the exception throw in QueryExecutionErrors:

{code:scala}
  def indexOutOfBoundsOfArrayDataError(idx: Int): Throwable = {
    new SparkIndexOutOfBoundsException(errorClass = ""INDEX_OUT_OF_BOUNDS"", Array(idx.toString))
  }
{code}

For example, here is a test for the error class *UNSUPPORTED_FEATURE*: https://github.com/apache/spark/blob/34e3029a43d2a8241f70f2343be8285cb7f231b9/sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala#L151-L170

+The test must have a check of:+
# the entire error message
# sqlState if it is defined in the error-classes.json file
# the error class"	SPARK	Resolved	4	7	6986	starter
13559487	Restrict charsets in encode()	"Currently the list of supported charsets in encode() is not stable and fully depends on the used JDK version. So, sometimes user code might not work because a devop changed Java version in Spark cluster. The ticket aims to restrict the list of supported charsets by:

{code}
'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'
{code}
"	SPARK	Resolved	3	4	6986	pull-request-available
13567717	Deprecate `spark.sql.legacy.allowZeroIndexInFormatString`	"The SQL config spark.sql.legacy.allowZeroIndexInFormatString doesn't allow to use the zero index. Even set it to true, users get the error:

{code:sql}
> select format_string('%0$s', 'Hello');

Illegal format argument index = 0
java.util.IllegalFormatArgumentIndexException: Illegal format argument index = 0
	at java.base/java.util.Formatter$FormatSpecifier.index(Formatter.java:2808)
	at java.base/java.util.Formatter$FormatSpecifier.<init>(Formatter.java:2879)
{code}
"	SPARK	Resolved	3	7	6986	pull-request-available
13568983	Raise Spark's exception with an error class in config value check	Currently, Spark throws *IllegalArgumentException* in `checkValue` of ConfigBuilder. Need to overload `checkValue` to throw `SparkIllegalArgumentException` with an error class. This should improve user experience with Spark SQL, and impressions of Spark's errors.	SPARK	Resolved	3	7	6986	pull-request-available
13561237	Replace IllegalStateException by SparkException.internalError in sql/core	"Replace IllegalStateException by SparkException.internalError in sql/core as a part of migration onto new error framework and error classes.

"	SPARK	Resolved	4	7	6986	pull-request-available
13551893	Respect `spark.sql.files.ignoreMissingFiles` in HadoopRDD	Currently, the SQL config spark.sql.files.ignoreMissingFiles influences on RDDs created in Spark SQL such as FileScanRDD but doesn't impact on HadoopRDD and NewHadoopRDD. The last RDDs have separate core config spark.files.ignoreMissingFiles. That inconsistency might confuse users if they don't know implementation details. This ticket aims to eliminate the inconsistency.	SPARK	Resolved	3	4	6986	pull-request-available
13351211	ALTER TABLE .. ADD PARTITION doesn't refresh cache	"Here is the example to reproduce the issue:
{code:sql}
spark-sql> create table tbl2 (col int, part int) partitioned by (part);
spark-sql> insert into tbl2 partition (part=0) select 0;
spark-sql> cache table tbl2;
spark-sql> select * from tbl2;
0	0
spark-sql> show table extended like 'tbl2' partition (part = 0);
default	tbl2	false	Partition Values: [part=0]
Location: file:/Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl2/part=0
...
{code}
Add new partition by copying the existing one:
{code}
cp -r /Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl2/part=0 /Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl2/part=1
{code}
 Add new partition and select the table:
{code}
spark-sql> alter table tbl2 add partition (part = 1) location '/Users/maximgekk/proj/add-partition-refresh-cache-2/spark-warehouse/tbl2/part=1';
spark-sql> select * from tbl2;
0	0
{code}
We see only old data."	SPARK	Resolved	3	7	6986	correctness
13595302	Require an error class in SparkException	Modify SparkException to accept only an error class instead of an error message while creating the exception.	SPARK	Open	3	7	6986	pull-request-available
13568578	Check SparkIllegalArgumentException instead of IllegalArgumentException in catalyst	Use checkError() to test the SparkIllegalArgumentException exception instead of IllegalArgumentException in the Catalyst project.	SPARK	Resolved	3	7	6986	pull-request-available
13515717	`min` fails on the minimal timestamp	"The code below demonstrates the issue:

{code:python}
>>> from datetime import datetime, timezone
>>> from pyspark.sql.types import TimestampType
>>> from pyspark.sql import functions as F
>>> ts = spark.createDataFrame([datetime(1, 1, 1, 0, 0, 0, 0, tzinfo=timezone.utc)], TimestampType()).toDF(""test_column"")
>>> ts.select(F.min('test_column')).first()[0]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/dataframe.py"", line 2762, in first
    return self.head()
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/dataframe.py"", line 2738, in head
    rs = self.head(1)
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/dataframe.py"", line 2740, in head
    return self.take(n)
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/dataframe.py"", line 1297, in take
    return self.limit(num).collect()
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/dataframe.py"", line 1198, in collect
    return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/serializers.py"", line 152, in load_stream
    yield self._read_with_length(stream)
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/serializers.py"", line 174, in _read_with_length
    return self.loads(obj)
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/serializers.py"", line 472, in loads
    return cloudpickle.loads(obj, encoding=encoding)
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/types.py"", line 2010, in <lambda>
    return lambda *a: dataType.fromInternal(a)
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/types.py"", line 1018, in fromInternal
    values = [
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/types.py"", line 1019, in <listcomp>
    f.fromInternal(v) if c else v
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/types.py"", line 667, in fromInternal
    return self.dataType.fromInternal(obj)
  File ""/Users/maximgekk/proj/apache-spark/python/pyspark/sql/types.py"", line 279, in fromInternal
    return datetime.datetime.fromtimestamp(ts // 1000000).replace(microsecond=ts % 1000000)
ValueError: year 0 is out of range
{code}
"	SPARK	In Progress	3	1	6986	pull-request-available
13563270	Replace UnsupportedOperationException by SparkUnsupportedOperationException in catalyst	Replace all UnsupportedOperationException by SparkUnsupportedOperationException in Catalyst code base, and introduce new legacy error classes with the _LEGACY_ERROR_TEMP_ prefix.	SPARK	Resolved	3	7	6986	pull-request-available
13382768	Support fields by year-month interval type	"Extend YearMonthIntervalType to support:
* INTERVAL YEAR
* INTERVAL MONTH
* INTERVAL YEAR TO MONTH
 "	SPARK	Resolved	3	7	6986	pull-request-available
13560348	Restrict charsets in decode()	"Currently the list of supported charsets in decode() is not stable and fully depends on the used JDK version. So, sometimes user code might not work because a devop changed Java version in Spark cluster. The ticket aims to restrict the list of supported charsets by:

{code}
'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'
{code}
"	SPARK	Resolved	3	4	6986	pull-request-available
13364523	Difference in results of casting float to timestamp	"The code below portraits the issue:
{code:sql}
spark-sql> CREATE TEMP VIEW v1 AS SELECT 16777215.0f AS f;
spark-sql> SELECT * FROM v1;
1.6777215E7
spark-sql> SELECT CAST(f AS TIMESTAMP) FROM v1;
1970-07-14 07:20:15
spark-sql> CACHE TABLE v1;
spark-sql> SELECT * FROM v1;
1.6777215E7
spark-sql> SELECT CAST(f AS TIMESTAMP) FROM v1;
1970-07-14 07:20:14.951424
{code}

The result from the cached view *1970-07-14 07:20:14.951424* is different from un-cached view *1970-07-14 07:20:15*."	SPARK	Resolved	3	1	6986	correctness
13564156	Skip query context catching in DataFrame methods	"To improve user experience with Spark DataFrame/Dataset APIs, and provide more precise context of errors, catching of Dataframe query context can be skipped in Dataframe/Dataset  methods.

"	SPARK	Resolved	3	4	6986	pull-request-available
13560109	Align codegen and non-codegen implementation of StringDecode	Fix implementation of codegen and non-codegen in the StringDecode expression, and raise the same error for invalid input charset.	SPARK	Resolved	3	1	6986	pull-request-available
13562237	Remove the legacy datetime rebasing SQL configs	"Remove already deprecated SQL configs (alternatives to other configs):
- spark.sql.legacy.parquet.int96RebaseModeInWrite
- spark.sql.legacy.parquet.datetimeRebaseModeInWrite
- spark.sql.legacy.parquet.int96RebaseModeInRead
- spark.sql.legacy.parquet.datetimeRebaseModeInRead
- spark.sql.legacy.avro.datetimeRebaseModeInWrite
- spark.sql.legacy.avro.datetimeRebaseModeInRead
"	SPARK	Resolved	4	4	6986	pull-request-available
13561188	Replace IllegalStateException by SparkException.internalError in catalyst	"Replace IllegalStateException by SparkException.internalError in catalyst as a part of migration onto new error framework and error classes.

"	SPARK	Resolved	4	7	6986	pull-request-available
13550393	Unsupported map and array constructors by `sql()` in connect clients	"The code below demonstrates the issue:

{code:scala}
spark.sql(""select element_at(?, 1)"", Array(array(lit(1)))).collect()
{code}

It fails with the error:

{code:java}
[info]   java.lang.UnsupportedOperationException: literal unresolved_function {
[info]   function_name: ""array""
[info]   arguments {
[info]     literal {
[info]       integer: 1
[info]     }
[info]   }
[info] }
[info]  not supported (yet).
{code}

"	SPARK	Resolved	3	1	6986	pull-request-available
13287647	Potential data loss for legacy applications after switch to proleptic Gregorian calendar	"tl;dr: We recently discovered some Spark 2.x sites that have lots of data containing dates before October 15, 1582. This could be an issue when such sites try to upgrade to Spark 3.0.

From SPARK-26651:
{quote}""The changes might impact on the results for dates and timestamps before October 15, 1582 (Gregorian)
{quote}
We recently discovered that some large scale Spark 2.x applications rely on dates before October 15, 1582.

Two cases came up recently:
 * An application that uses a commercial third-party library to encode sensitive dates. On insert, the library encodes the actual date as some other date. On select, the library decodes the date back to the original date. The encoded value could be any date, including one before October 15, 1582 (e.g., ""0602-04-04"").
 * An application that uses a specific unlikely date (e.g., ""1200-01-01"") as a marker to indicate ""unknown date"" (in lieu of null)

Both sites ran into problems after another component in their system was upgraded to use the proleptic Gregorian calendar. Spark applications that read files created by the upgraded component were interpreting encoded or marker dates incorrectly, and vice versa. Also, their data now had a mix of calendars (hybrid and proleptic Gregorian) with no metadata to indicate which file used which calendar.

Both sites had enormous amounts of existing data, so re-encoding the dates using some other scheme was not a feasible solution.

This is relevant to Spark 3:

Any Spark 2 application that uses such date-encoding schemes may run into trouble when run on Spark 3. The application may not properly interpret the dates previously written by Spark 2. Also, once the Spark 3 version of the application writes data, the tables will have a mix of calendars (hybrid and proleptic gregorian) with no metadata to indicate which file uses which calendar.

Similarly, sites might run with mixed Spark versions, resulting in data written by one version that cannot be interpreted by the other. And as above, the tables will now have a mix of calendars with no way to detect which file uses which calendar.

As with the two real-life example cases, these applications may have enormous amounts of legacy data, so re-encoding the dates using some other scheme may not be feasible.

We might want to consider a configuration setting to allow the user to specify the calendar for storing and retrieving date and timestamp values (not sure how such a flag would affect other date and timestamp-related functions). I realize the change is far bigger than just adding a configuration setting.

Here's a quick example of where trouble may happen, using the real-life case of the marker date.

In Spark 2.4:
{noformat}
scala> spark.read.orc(s""$home/data/datefile"").filter(""dt == '1200-01-01'"").count
res0: Long = 1
scala>
{noformat}
In Spark 3.0 (reading from the same legacy file):
{noformat}
scala> spark.read.orc(s""$home/data/datefile"").filter(""dt == '1200-01-01'"").count
res0: Long = 0
scala> 
{noformat}
By the way, Hive had a similar problem. Hive switched from hybrid calendar to proleptic Gregorian calendar between 2.x and 3.x. After some upgrade headaches related to dates before 1582, the Hive community made the following changes:
 * When writing date or timestamp data to ORC, Parquet, and Avro files, Hive checks a configuration setting to determine which calendar to use.
 * When writing date or timestamp data to ORC, Parquet, and Avro files, Hive stores the calendar type in the metadata.
 * When reading date or timestamp data from ORC, Parquet, and Avro files, Hive checks the metadata for the calendar type.
 * When reading date or timestamp data from ORC, Parquet, and Avro files that lack calendar metadata, Hive's behavior is determined by a configuration setting. This allows Hive to read legacy data (note: if the data already consists of a mix of calendar types with no metadata, there is no good solution)."	SPARK	Resolved	1	1	6986	release-notes
13557356	Enforce the error classes in sub-classes of AnalysisException	Make the error class in sub-classes of AnalysisException mandatory to enforce callers to always set it. This simplifies migration on error classes.	SPARK	Resolved	3	7	6986	pull-request-available
13268879	Remove SQL configs declared to be removed in Spark 3.0	"Need to remove the following SQL configs:
* spark.sql.fromJsonForceNullableSchema
* spark.sql.legacy.compareDateTimestampInTimestamp"	SPARK	Resolved	4	4	6986	release-notes
13566149	Incorrect count() of a dataframe loaded from CSV datasource	"The example below portraits the issue:
{code:java}
>>> df=spark.read.option(""multiline"", ""true"").option(""header"", ""true"").option(""escape"", '""').csv(""es-939111-data.csv"")
>>> df.count()
4
>>> df.cache()
DataFrame[jobID: string, Name: string, City: string, Active: string]
>>> df.count()
5{code}"	SPARK	Resolved	3	1	6986	correctness, pull-request-available
13570968	Split DataFrameSuite	Split DataFrameSuite to smaller test suite, and/or distribute its tests among other test suites. This should improve maintainability and speed up its execution.	SPARK	Resolved	3	6	6986	pull-request-available
13595253	 Require an error class in SparkOutOfMemoryError	Modify SparkOutOfMemoryError to accept only an error class instead of an error message while creating the exception.	SPARK	Resolved	3	7	6986	pull-request-available
13549931	reflect() fails with an internal error on NULL class and method	"The example below demonstrates the issue:

{code:sql}
spark-sql (default)> select reflect('java.util.UUID', CAST(NULL AS STRING));
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
{code}
"	SPARK	Resolved	3	1	6986	pull-request-available
13561144	Migrate CatalogNotFoundException to an error class	"Migrate the exception CatalogNotFoundException to an error class and
- prohibit creation of CatalogNotFoundException w/o a error class
- introduce new error class
- create CatalogNotFoundException using new error class"	SPARK	Resolved	3	7	6986	pull-request-available
13198740	Do not write empty files by text datasources	Text based datasources like CSV, JSON and Text produces empty files for empty partitions. This introduces additional overhead while opening and reading such files back. In current implementation of OutputWriter, the output stream are created eagerly even no records are written to the stream. So, creation can be postponed up to the first write.	SPARK	Resolved	4	4	6986	release-notes
13563097	Convert NPE and asserts from commands to internal errors	Handle NPE and asserts from eagerly executed commands, and convert them to internal errors.	SPARK	Resolved	3	4	6986	pull-request-available
13549170	Provide context for dataset API errors	"SQL failures already provide nice error context when there is a failure:
{noformat}
org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
== SQL(line 1, position 1) ==
a / b
^^^^^

	at org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:201)
	at org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)
...
{noformat}
We could add a similar user friendly error context to Dataset APIs.

E.g. consider the following Spark app SimpleApp.scala:
{noformat}
   1  import org.apache.spark.sql.SparkSession
   2  import org.apache.spark.sql.functions._
   3
   4  object SimpleApp {
   5    def main(args: Array[String]) {
   6      val spark = SparkSession.builder.appName(""Simple Application"").config(""spark.sql.ansi.enabled"", true).getOrCreate()
   7      import spark.implicits._
   8
   9      val c = col(""a"") / col(""b"")
  10
  11      Seq((1, 0)).toDF(""a"", ""b"").select(c).show()
  12
  13      spark.stop()
  14    }
  15  }
{noformat}
then the error context could be:
{noformat}
Exception in thread ""main"" org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
== Dataset ==
""div"" was called from SimpleApp$.main(SimpleApp.scala:9)

	at org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:201)
	at org.apache.spark.sql.catalyst.expressions.DivModLike.eval(arithmetic.scala:672
...
{noformat}"	SPARK	Resolved	3	4	6986	pull-request-available
13591601	Rename errorClass in checkError()	Rename errorClass to condition in checkError() and related functions to follow up the agreement of https://issues.apache.org/jira/browse/SPARK-46810	SPARK	Resolved	3	7	6986	pull-request-available
13552786	Refer to the unescaping rules from expression descriptions	Update the expression/function description and refer to the unescaping rules in the items where regexp parameters are described. This should less confuse users.	SPARK	Resolved	4	20	6986	pull-request-available
13562769	Require error classes in SparkThrowable sub-classes	Modify SparkThrowable sub-classes to accept only an error class instead of an error message while creating the exception.	SPARK	Resolved	3	7	6986	pull-request-available
13557482	Output full stack trace in callSite of DataFrame context	Include all available items to callSite of DataFrame context. This will simplifies user issues.	SPARK	Resolved	3	4	6986	pull-request-available
13561716	Classify exceptions in the JDBC table catalog	Handle exceptions from JDBC drivers and convert them to AnalysisException with error classes.	SPARK	Resolved	3	4	6986	pull-request-available
13234582	Support ANSI SQL INTERVAL types	"Spark has an INTERVAL data type, but it is “broken”:
# It cannot be persisted
# It is not comparable because it crosses the month day line. That is there is no telling whether “1 Month 1 Day” is equal to “1 Month 1 Day” since not all months have the same number of days.

I propose here to introduce the two flavours of INTERVAL as described in the ANSI SQL Standard and deprecate the Sparks interval type.
* ANSI describes two non overlapping “classes”: 
** YEAR-MONTH, 
** DAY-SECOND ranges
* Members within each class can be compared and sorted.
* Supports datetime arithmetic
* Can be persisted.

The old and new flavors of INTERVAL can coexist until Spark INTERVAL is eventually retired. Also any semantic “breakage” can be controlled via legacy config settings. 

*Milestone 1* --  Spark Interval equivalency (   The new interval types meet or exceed all function of the existing SQL Interval):
* Add two new DataType implementations for interval year-month and day-second. Includes the JSON format and DLL string.
* Infra support: check the caller sides of DateType/TimestampType
* Support the two new interval types in Dataset/UDF.
* Interval literals (with a legacy config to still allow mixed year-month day-seconds fields and return legacy interval values)
* Interval arithmetic(interval * num, interval / num, interval +/- interval)
* Datetime functions/operators: Datetime - Datetime (to days or day second), Datetime +/- interval
* Cast to and from the new two interval types, cast string to interval, cast interval to string (pretty printing), with the SQL syntax to specify the types
* Support sorting intervals.

*Milestone 2* -- Persistence:
* Ability to create tables of type interval
* Ability to write to common file formats such as Parquet and JSON.
* INSERT, SELECT, UPDATE, MERGE
* Discovery

*Milestone 3* --  Client support
* JDBC support
* Hive Thrift server

*Milestone 4* -- PySpark and Spark R integration
* Python UDF can take and return intervals
* DataFrame support

"	SPARK	Resolved	3	4	6986	release-notes
13285830	Wrong truncations of timestamps before the epoch to hours and days	"Truncations to seconds and minutes of timestamps after the epoch are correct:
{code:sql}
spark-sql> select date_trunc('HOUR', '2020-02-11 00:01:02.123'), date_trunc('HOUR', '2020-02-11 00:01:02.789');
2020-02-11 00:00:00	2020-02-11 00:00:00
{code}
but truncations of timestamps before the epoch are incorrect:
{code:sql}
spark-sql> select date_trunc('HOUR', '1960-02-11 00:01:02.123'), date_trunc('HOUR', '1960-02-11 00:01:02.789');
1960-02-11 01:00:00	1960-02-11 01:00:00
{code}
The result must be *1960-02-11 00:00:00 1960-02-11 00:00:00*

The same for the DAY level:
{code:sql}
spark-sql> select date_trunc('DAY', '1960-02-11 00:01:02.123'), date_trunc('DAY', '1960-02-11 00:01:02.789');
1960-02-12 00:00:00	1960-02-12 00:00:00
{code}
The result must be *1960-02-11 00:00:00 1960-02-11 00:00:00*"	SPARK	Resolved	3	1	6986	correctness
13557185	Expose stack trace by DataFrameQueryContext	Modify DataFrameQueryContext and expose stack traces to users. This should allow easily troubleshoot issues. 	SPARK	Resolved	3	4	6986	pull-request-available
13571833	Pass messageParameters by name to require()	Passing *messageParameters* by value independently from requirement might introduce perf regression. Need to pass *messageParameters* by name to avoid eager instantiation	SPARK	Resolved	3	4	6986	pull-request-available
13595205	Rename errorClass to condition in JdbcDialect.classifyException	"Rename ""errorClass"" to ""condition"" in JdbcDialect.classifyException to follow up the agreement of https://issues.apache.org/jira/browse/SPARK-46810"	SPARK	Open	3	7	6986	pull-request-available
13329298	"In OracleDialect, ""RowID"" SQL type should be converted into ""String"" Catalyst type"	"Most JDBC drivers use long SQL type for dataset row ID:

 

(in org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils)
{code:java}
private def getCatalystType(
 sqlType: Int,
 precision: Int,
 scale: Int,
 signed: Boolean): DataType = {
 val answer = sqlType match {
 // scalastyle:off
 ...
 case java.sql.Types.ROWID => LongType
...
 case _ =>
 throw new SQLException(""Unrecognized SQL type "" + sqlType)
 // scalastyle:on
 }
if (answer == null)
{ throw new SQLException(""Unsupported type "" + JDBCType.valueOf(sqlType).getName) }
answer
{code}
 

Oracle JDBC drivers (of all versions) are rare exception, only String value can be extracted:

 

(in oracle.jdbc.driver.RowidAccessor, decompiled bytecode)
{code:java}
...
String getString(int var1) throws SQLException
{ return this.isNull(var1) ? null : this.rowData.getString(this.getOffset(var1), this.getLength(var1), this.statement.connection.conversion.getCharacterSet((short)1)); }
Object getObject(int var1) throws SQLException
{ return this.getROWID(var1); }
...
{code}
 

This caused an exception to be thrown when importing datasets from an Oracle DB, as reported in [https://stackoverflow.com/questions/52244492/spark-jdbc-dataframereader-fails-to-read-oracle-table-with-datatype-as-rowid:]
{code:java}
 
 {{18/09/08 11:38:17 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 23, gbrdsr000002985.intranet.barcapint.com, executor 21): java.sql.SQLException: Invalid column type: getLong not implemented for class oracle.jdbc.driver.T4CRowidAccessor at oracle.jdbc.driver.GeneratedAccessor.getLong(GeneratedAccessor.java:440)
 at oracle.jdbc.driver.GeneratedStatement.getLong(GeneratedStatement.java:228)
 at oracle.jdbc.driver.GeneratedScrollableResultSet.getLong(GeneratedScrollableResultSet.java:620)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$8.apply(JdbcUtils.scala:365)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$8.apply(JdbcUtils.scala:364)}}
 
{code}
 

Therefore, the default SQL type => Catalyst type conversion rule should be overriden in OracleDialect. Specifically, the following rule should be added:
{code:java}
case Types.ROWID => Some(StringType)
{code}
 "	SPARK	Resolved	4	1	6986	jdbc, jdbc_connector
13560453	Describe arguments of decode()	Update the description of the `StringDecode` expression and apparently the `decode()` function by describing the arguments `bin` and `charset`. The ticket aims to improve user experience with Spark SQL by documenting the public function.	SPARK	Resolved	3	20	6986	pull-request-available
13527456	Add the try_aes_decrypt() function	Add new function try_aes_decrypt(). The function aes_decrypt() fails w/ an exception when it faces to a column value which it cannot decrypt. So, if a column contains bad and good input, it is impossible to decrypt even good input.	SPARK	Resolved	3	2	6986	starter
13567264	Replace UnsupportedOperationException by SparkUnsupportedOperationException in sql	Replace all UnsupportedOperationException by SparkUnsupportedOperationException in the *sql* code base, and introduce new legacy error classes with the _LEGACY_ERROR_TEMP_ prefix.	SPARK	Resolved	3	7	6986	pull-request-available
13327984	percentile_approx() returns incorrect results	"Read input data from the attached CSV file:
{code:scala}
      val df = spark.read.option(""header"", ""true"")
        .option(""inferSchema"", ""true"")
        .csv(""/Users/maximgekk/tmp/percentile_approx-input.csv"")
        .repartition(1)
      df.createOrReplaceTempView(table)
{code}
Calculate the 0.77 percentile with accuracy 1e-05:
{code:Scala}
      spark.sql(
        s""""""SELECT
           |  percentile_approx(tr_rat_resampling_score, 0.77, 100000)
           |FROM $table
           """""".stripMargin).show
{code}
{code}
+------------------------------------------------------------------------+
|percentile_approx(tr_rat_resampling_score, CAST(0.77 AS DOUBLE), 100000)|
+------------------------------------------------------------------------+
|                                                                    1000|
+------------------------------------------------------------------------+
{code}
 The same for smaller accuracy 0.001:
{code}
+----------------------------------------------------------------------+
|percentile_approx(tr_rat_resampling_score, CAST(0.77 AS DOUBLE), 1000)|
+----------------------------------------------------------------------+
|                                                                    18|
+----------------------------------------------------------------------+
{code} 
and better accuracy 1e-06:
{code}
+-------------------------------------------------------------------------+
|percentile_approx(tr_rat_resampling_score, CAST(0.77 AS DOUBLE), 1000000)|
+-------------------------------------------------------------------------+
|                                                                       17|
+-------------------------------------------------------------------------+
{code}

For the accuracy 1e-05, the result must be around 17-18 but not 1000.

Here is percentile calculation in Google Sheets for the same input:
https://docs.google.com/spreadsheets/d/1Y1i4Td6s9jZQ-bD4IRTESLXP3UxKpqJSXGtmx0Q5TA0/edit?usp=sharing"	SPARK	Resolved	3	1	6986	correctness
13551383	The `sql()` method doesn't support map and array parameters in Python connect client	At the moment, the Python connect client fails when we pass a map or an array as a parameter to the `sql()` method. This ticket aims to fix the behaviour to reach feature parity with the regular PySpark.	SPARK	Resolved	3	1	6986	pull-request-available
13552040	Remove the SQL config spark.sql.hive.verifyPartitionPath	The SQL config spark.sql.hive.verifyPartitionPath has been deprecated a quite a while in version 3.0. Can be removed in the version 4.0.	SPARK	Resolved	3	4	6986	pull-request-available
13284639	Wrong truncations of timestamps before the epoch to minutes and seconds	"Truncations to seconds and minutes of timestamps after the epoch are correct:
{code:sql}
spark-sql> select date_trunc('SECOND', '2020-02-11 00:01:02.123'), date_trunc('SECOND', '2020-02-11 00:01:02.789');
2020-02-11 00:01:02	2020-02-11 00:01:02
{code}
but truncations of timestamps before the epoch are incorrect:
{code:sql}
spark-sql> select date_trunc('SECOND', '1960-02-11 00:01:02.123'), date_trunc('SECOND', '1960-02-11 00:01:02.789');
1960-02-11 00:01:03	1960-02-11 00:01:03
{code}
The result must be *1960-02-11 00:01:02	1960-02-11 00:01:02*

The same for the MINUTE level:
{code:sql}
spark-sql> select date_trunc('MINUTE', '1960-02-11 00:01:01'), date_trunc('MINUTE', '1960-02-11 00:01:50');
1960-02-11 00:02:00	1960-02-11 00:02:00
{code}
The result must be 1960-02-11 00:01:00	1960-02-11 00:01:00"	SPARK	Resolved	3	1	6986	correctness
13563489	Redact JDBC url in errors and logs	Redact the JDBC url in error message parameters and logs to avoid leaking of user's secrets.	SPARK	Resolved	3	7	6986	pull-request-available
13552517	Deprecate spark.sql.parser.escapedStringLiterals	The config allows to switch to legacy behaviour of Spark 1.6 which is pretty old and not maintained anymore. Deprecation and removing of the config in the future versions should improve code maintenance. Also there is an alternative approach by using RAW string literals in which Spark doesn't especially handle escaped character sequences.	SPARK	Resolved	3	7	6986	pull-request-available
13285260	LIKE returns wrong result from external table using parquet	"# Write a parquet file with a column in upper case:
{code:scala}
Seq(""42"").toDF(""COL"").write.parquet(path)
{code}
# Create an external table on top of the written parquet files with a column in lower case
{code:sql}
CREATE TABLE t1 (col STRING)
USING parquet
OPTIONS (path '$path')
{code}
# Read the table using LIKE
{code:sql}
SELECT * FROM t1 WHERE col LIKE '4%'
{code}
It returns empty set but must be one row with 42.
"	SPARK	Resolved	3	1	6986	correctness
13591617	datetime formatter violates assert of variable seconds fraction	"The datetime pattern ""\nSSSS\r"" cause failure on assert while parsing the pattern by datetime formatter:

{code}
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:264)
	at org.apache.spark.ErrorClassesJsonReader.getMessageTemplate(ErrorClassesJSONReader.scala:91)
	at org.apache.spark.ErrorClassesJsonReader.getErrorMessage(ErrorClassesJSONReader.scala:46)
	at org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:56)
	at org.apache.spark.SparkIllegalArgumentException.<init>(SparkException.scala:405)
	at org.apache.spark.SparkIllegalArgumentException.<init>(SparkException.scala:417)
	at org.apache.spark.SparkIllegalArgumentException.<init>(SparkException.scala:422)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$createBuilderWithVarLengthSecondFraction$1(DateTimeFormatterHelper.scala:247)
{code}
"	SPARK	Resolved	3	1	6986	pull-request-available
13559880	Describe arguments of encode()	Update the description of the `Encode` expression and apparently the `encode()` function by describing the arguments `str` and `charset`. The ticket aims to improve user experience with Spark SQL by documenting the public function.	SPARK	Resolved	3	20	6986	pull-request-available
13550974	Update error messages related to parameterized sql()	Update docs, comments and error formats about map/array/struct parameters of `sql()`.	SPARK	Resolved	3	20	6986	pull-request-available
13556880	Eliminate magic numbers in withOrigin	Refactor `withOrigin`, and make it more generic by eliminating the magic number from which the traverse of stack traces starts.	SPARK	Resolved	3	4	6986	pull-request-available
13596261	Make the InSet expression collation aware	Modify the InSet expression and support the STRING type w/ collations.	SPARK	Resolved	3	7	6986	pull-request-available
13552744	Include `ESCAPE` to `sql()` of `Like`	"Fix the `sql()` method of the `Like` expression and append the `ESCAPE` closure. That should become consistent to `toString` and fix the issue:

{code:sql}
spark-sql (default)> create temp view tbl as (SELECT 'a|_' like 'a||_' escape '|', 'a|_' like 'a||_' escape 'a');
[COLUMN_ALREADY_EXISTS] The column `a|_ like a||_` already exists. Consider to choose another name or rename the existing column.
{code}
"	SPARK	Resolved	3	4	6986	pull-request-available
13562160	Set the rebase configs to the CORRECTED mode by default	"Set all rebase related SQL configs to the `CORRECTED` mode by default. Here are the affected configs:
- spark.sql.parquet.int96RebaseModeInWrite
- spark.sql.parquet.datetimeRebaseModeInWrite
- spark.sql.parquet.int96RebaseModeInRead
- spark.sql.parquet.datetimeRebaseModeInRead
- spark.sql.avro.datetimeRebaseModeInWrite
- spark.sql.avro.datetimeRebaseModeInRead

The configs were set to the `EXCEPTION` mode to give users a choice to select proper mode for compatibility with old Spark versions <= 2.4.5. Those versions are not able to detect the rebase mode from meta information in parquet and avro files. Since the versions are out of broad usage, Spark starting from the version 4.0.0 will write/ read ancient datatime without rebasing and any exceptions. This should be more convenient for users.
"	SPARK	Resolved	3	4	6986	pull-request-available
13552225	Describe characters unescaping in string literals	Update the page https://spark.apache.org/docs/latest/sql-ref-literals.html#string-literal and describe the escaping implemented at https://github.com/apache/spark/blob/9109d7037f44158e72d14019eb33f9c7b8838868/sql/api/src/main/scala/org/apache/spark/sql/catalyst/util/SparkParserUtils.scala#L38	SPARK	Resolved	3	20	6986	pull-request-available
13238956	Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats	"Invalid date formats should throw an exception:
{code:sql}
SELECT date '1999 08 01'
1999-01-01
{code}

Supported date formats:
https://github.com/apache/spark/blob/ab8710b57916a129fcb89464209361120d224535/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L365-L374

Since Spark 1.6.3 ~ 2.4.3, the behavior is the same.
{code}
spark-sql> SELECT CAST('1999 08 01' AS DATE);
1999-01-01
{code}

Hive returns NULL."	SPARK	Resolved	3	1	6986	correctness
13551580	Improve the description for `LIKE`	"The description of `LIKE` says:
{code}
... in order to match ""\abc"", the pattern should be ""\\abc""
{code}
but in Spark SQL shell:
{code:sql}
spark-sql (default)> SELECT c FROM t;
\abc
spark-sql (default)> SELECT c LIKE ""\\abc"" FROM t;
[INVALID_FORMAT.ESC_IN_THE_MIDDLE] The format is invalid: '\\abc'. The escape character is not allowed to precede 'a'.
spark-sql (default)> SELECT c LIKE ""\\\\abc"" FROM t;
true
{code}"	SPARK	Resolved	3	20	6986	pull-request-available
13564986	Add an error class for unsupported method calls	Add new error class for the unsupported method calls. The error class should include method and class names as parameters.	SPARK	Resolved	3	7	6986	pull-request-available
