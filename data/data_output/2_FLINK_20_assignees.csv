id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13308032	It will throw Invalid lambda deserialization Exception when writing to elastic search with new format	"My job follows:
{code:java}
// 
create table csv( pageId VARCHAR, eventId VARCHAR, recvTime VARCHAR) with ( 'connector' = 'filesystem',
 'path' = '/Users/ohmeatball/Work/flink-sql-etl/data-generator/src/main/resources/user3.csv',
 'format' = 'csv'
 )
-----------------------------------------
CREATE TABLE es_table (
  aggId varchar ,
  pageId varchar ,
  ts varchar ,
  expoCnt int ,
  clkCnt int
) WITH (
'connector' = 'elasticsearch',
'hosts' = 'http://localhost:9200',
'index' = 'cli_test',
'document-id.key-delimiter' = '$',
'sink.bulk-flush.interval' = '1000',
'format' = 'json'
)
-----------------------------------------
INSERT INTO es_table
SELECT  pageId,eventId,cast(recvTime as varchar) as ts, 1, 1 from csv;
{code}
The full exception follows:
{code:java}
Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:471) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:459) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:155) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]Caused by: java.io.IOException: unexpected exception type at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1682) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1254) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
Caused by: java.lang.reflect.InvocationTargetExceptionCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreCaused by: java.lang.IllegalArgumentException: Invalid lambda deserialization at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink$Builder.$deserializeLambda$(ElasticsearchSink.java:80) ~[flink-sql-connector-elasticsearch7_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151]
 at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
{code}
Notice: everything works fine with former connector grammer."	FLINK	Closed	1	1	3568	pull-request-available
13187210	Add skip to next strategy	Add skip to next strategy, that should discard all partial matches that started with the same element as found match.	FLINK	Resolved	3	4	3568	pull-request-available
13391141	Add a global flag for enabling/disabling final checkpoints	We should have a feature toggle for the final checkpoint story.	FLINK	Closed	3	7	3568	pull-request-available
13340904	Avro Confluent Registry SQL format does not support adding nullable columns	"The {{AvroSchemaConverter#convertToSchema}} generates a union with ""null"" for nullable logical types, but it does not set the default value to null. In turn it makes it impossible to generate a backwards compatible schema from a DDL statement.

Example:
1. Create a table: {{CREATE TABLE t (id INT NOT NULL) WITH (/* avro confluent format*/)}}
2. Create a new table over the same topic or alter the old table with {{CREATE TABLE newT(id INT NOT NULL, optionalDescription STRING) WITH (/* avro confluent format */)}}
3. When reading from {{newT}} records inserted into {{t}} it will fail, because the {{optionalDescription}} has no default value."	FLINK	Closed	2	1	3568	pull-request-available
13248587	Include table examples in flink-dist	"We want to treat the table api as first-class API. We already included in the lib directory flink.
We should also include some examples of the table api in the distribution.

Before that we should strip all the dependency and just include the classes from  example module."	FLINK	Closed	2	4	3568	pull-request-available
13550488	SupportsReadingMetadata is not applied when loading a CompiledPlan	"If a few conditions are met, we can not apply ReadingMetadata interface:
# source overwrites:
 {code}
    @Override
    public boolean supportsMetadataProjection() {
        return false;
    }
 {code}
# source does not implement {{SupportsProjectionPushDown}}
# table has metadata columns e.g.
{code}
CREATE TABLE src (
  physical_name STRING,
  physical_sum INT,
  timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL
)
{code}
# we query the table {{SELECT * FROM src}}

It fails with:
{code}
Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)
{code}

The reason is {{SupportsReadingMetadataSpec}} is created only in the {{PushProjectIntoTableSourceScanRule}}, but the rule is not applied when 1 & 2"	FLINK	Closed	3	1	3568	pull-request-available
13437148	JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate failed on azure	"
{code:java}
2022-03-31T06:11:52.2333685Z Mar 31 06:11:52 [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 35.288 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase
2022-03-31T06:11:52.2336004Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate  Time elapsed: 15.008 s  <<< FAILURE!
2022-03-31T06:11:52.2336907Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2337353Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2337876Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2338631Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2339436Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2340599Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate(JobMasterStopWithSavepointITCase.java:136)
2022-03-31T06:11:52.2342251Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2342896Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2343608Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2344234Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2344873Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2345590Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2346498Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2347221Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2347922Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2348580Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2349222Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2349860Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2350502Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2351172Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2352095Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2352949Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2353643Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2354298Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2354909Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2355535Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2356505Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2357142Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2357771Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2358400Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2359014Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2359614Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2360221Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2371694Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2372907Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2373992Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2375195Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2376592Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2377778Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2379338Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2380786Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2382151Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2383487Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2384979Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2386341Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2387454Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2389081Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2390447Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2391930Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2393389Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2394759Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2395544Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2396673Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2397347Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2397932Z Mar 31 06:11:52 
2022-03-31T06:11:52.2398639Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend  Time elapsed: 15.004 s  <<< FAILURE!
2022-03-31T06:11:52.2399342Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2399793Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2400311Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2400837Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2401633Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2402751Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend(JobMasterStopWithSavepointITCase.java:130)
2022-03-31T06:11:52.2403623Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2404247Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2404961Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2405936Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2406676Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2407520Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2408242Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2409245Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2409940Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2410604Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2411358Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2412174Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2412786Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2413640Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2414856Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2416140Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2417502Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2418495Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2419110Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2419737Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2420361Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2420986Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2421601Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422359Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422969Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2423569Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2424331Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2424922Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2425464Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2426334Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2427379Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2428432Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2429538Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2430713Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2431900Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2433166Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2434372Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2435500Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2436771Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2437877Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2439206Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2440452Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2441694Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2442881Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2443999Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2445104Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2446367Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2447434Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2448170Z Mar 31 06:11:52 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5183"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13374114	Unnecessary entries in sql hbase-1.4 connector NOTICE file	"The NOTICE file for flink-sql-connector-hbase-1.4 lists dependencies that it does not bundle:

* commons-configuration:commons-configuration:1.7
* org.apache.hbase:hbase-prefix-tree:1.4.3
* org.apache.hbase:hbase-procedure:1.4.3"	FLINK	Closed	2	7	3568	pull-request-available
13296468	Support creating tables using other tables definition	We should be able to create a Table based on properties of other tables. This includes merging the properties and creating a new Table based on that.	FLINK	Closed	3	7	3568	pull-request-available
13364698	NullPointerException on restore in PojoSerializer	"As originally reported in [thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Schema-Evolution-Cannot-restore-from-savepoint-after-deleting-field-from-POJO-td42162.html], after removing a field from a class restore from savepoint fails with the following exception:

{code}
2021-03-10T20:51:30.406Z INFO  org.apache.flink.runtime.taskmanager.Task:960 … (6/8) (d630d5ff0d7ae4fbc428b151abebab52) switched from RUNNING to FAILED. java.lang.Exception: Exception while creating StreamOperatorStateContext.
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:253)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:901)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:415)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedCoProcessOperator_c535ac415eeb524d67c88f4a481077d2_(6/8) from any of the 1 provided restore options.
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:307)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:135)
        ... 6 common frames omitted
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
        at org.apache.flink.runtime.state.memory.MemoryStateBackend.createKeyedStateBackend(MemoryStateBackend.java:347)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:291)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
        ... 8 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.<init>(PojoSerializer.java:123)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:186)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:56)
        at org.apache.flink.api.common.typeutils.CompositeSerializer$PrecomputedParameters.precompute(CompositeSerializer.java:228)
        at org.apache.flink.api.common.typeutils.CompositeSerializer.<init>(CompositeSerializer.java:51)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializer.<init>(TtlStateFactory.java:250)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:359)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:330)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546)
        at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
        at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:505)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.snapshotsToRestoreSerializers(NestedSerializersSnapshotDelegate.java:225)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.getRestoredNestedSerializers(NestedSerializersSnapshotDelegate.java:83)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at org.apache.flink.runtime.state.StateSerializerProvider.previousSchemaSerializer(StateSerializerProvider.java:189)
        at org.apache.flink.runtime.state.StateSerializerProvider.currentSchemaSerializer(StateSerializerProvider.java:164)
        at org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.getStateSerializer(RegisteredKeyValueStateBackendMetaInfo.java:136)
        at org.apache.flink.runtime.state.heap.StateTable.getStateSerializer(StateTable.java:315)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:54)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:36)
        at org.apache.flink.runtime.state.heap.StateTable.<init>(StateTable.java:98)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.<init>(CopyOnWriteStateTable.java:49)
        at org.apache.flink.runtime.state.heap.AsyncSnapshotStrategySynchronicityBehavior.newStateTable(AsyncSnapshotStrategySynchronicityBehavior.java:41)
        at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.newStateTable(HeapSnapshotStrategy.java:243)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.createOrCheckStateForMetaInfo(HeapRestoreOperation.java:185)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:152)
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
        ... 12 common frames omitted
{code}
"	FLINK	Closed	1	1	3568	pull-request-available
13313439	Can not select fields with JdbcCatalog	"A query which selects fields from a table will fail if we set the PostgresCatalog as default.

Steps to reproduce:
 # Create postgres catalog and set it as default
 # Create any table (in any catalog)
 # Query that table with {{SELECT field FROM t}} (Important it must be a field name not '{{*}}'
 #  The query will fail

Stack trace:
{code}
org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement.
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:100) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:91) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:257) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:211) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. null
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
Caused by: java.lang.UnsupportedOperationException
	at org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.getFunction(AbstractJdbcCatalog.java:261) ~[?:?]
	at org.apache.flink.table.catalog.FunctionCatalog.resolvePreciseFunctionReference(FunctionCatalog.java:570) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lambda$resolveAmbiguousFunctionReference$2(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
	at org.apache.flink.table.catalog.FunctionCatalog.resolveAmbiguousFunctionReference(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:370) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:99) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:73) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.makeNullaryCall(SqlValidatorImpl.java:1754) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5987) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6154) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6140) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectExpr(SqlValidatorImpl.java:5574) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:452) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4255) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3523) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
{code}

The problem is that Calcite will try to check first if there is a built-in function with that name that allows calls without parenthesis. Therefore it will query the {{FunctionCatalog}} for that function. The logic in {{org.apache.flink.table.catalog.FunctionCatalog#lookupFunction}} is such that it will call {{JdbcCatalog#getFunction}} in the end, which in case of {{AbstractJdbcCatalog}} throws {{UnsupportedOperationException}}.
"	FLINK	Closed	1	1	3568	pull-request-available
13285020	Introduce a Java Expression DSL	Introduce the basic expressions. The new Java expression DSL should be feature equivalent to string based expression parser. It does not support calls with new inference stack yet.	FLINK	Closed	3	7	3568	pull-request-available
13317135	Docker e2e tests are failing on CI	"{code}
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=host&nocache=1&rm=1&session=z0y5c0io3wt7m3uqfb7zo7uds&shmsize=0&t=test_docker_embedded_job&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied
{code}
Will have to wait for [~rmetzger] to get back."	FLINK	Closed	3	4	3568	pull-request-available
13240421	Convert CatalogView to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	"Similar to [FLINK-12257] we should convert Flink's views to Calcite's views.

The tricky part is that we have to pass around the SqlParser somehow."	FLINK	Closed	3	7	3568	pull-request-available
13341804	Building flink-dist docker image does not work without python2	"The script {{common_docker.sh}} in function {{start_file_server}} tests existence of {{python3}}, but executes command using {{python}}:

{code}
    command -v python3 >/dev/null 2>&1
    if [[ $? -eq 0 ]]; then
      python ${TEST_INFRA_DIR}/python3_fileserver.py &
      return
    fi
{code}

The script {{python3_fileserver.py}} uses python2 {{SocketServer}} which does not exist in python3. It should use {{socketserver}}."	FLINK	Closed	2	1	3568	pull-request-available
13236795	Introduce Table API Planner interface	The planner interface is the bridge between base API and different planner modules. 	FLINK	Closed	3	4	3568	pull-request-available
13363745	DegreesWithExceptionITCase crash	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14422&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b

"	FLINK	Closed	1	7	3568	pull-request-available, test-stability
13141178	Scala shell broken for Flip6	"I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?

I followed the instructions from the Flink master branch so I did the following.
{code:java}
git clone https://github.com/apache/flink.git 
cd flink mvn clean package -DskipTests 
cd build-target
 ./bin/start-scala-shell.sh local{code}
{{And Here is the code I ran}}
{code:java}
val dataStream = senv.fromElements(1, 2, 3, 4)
dataStream.countWindowAll(2).sum(0).print()
senv.execute(""My streaming program""){code}
{{And I finally get this exception}}
{code:java}
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281){code}
 "	FLINK	Resolved	1	1	3568	pull-request-available
13583654	JSON_QUERY should return a well formatted nested objects/arrays for ARRAY<STRING>	"{code}
SELECT JSON_QUERY('{""items"": [{""itemId"":1234, ""count"":10}]}', '$.items' RETURNING ARRAY<STRING>)
{code}

returns

{code}
['{itemId=1234, count=10}']
{code}

but it should:

{code}
['{""itemId"":1234, ""count"":10}']
{code}

We should call jsonize for Collection types here: https://github.com/apache/flink/blob/f6f88135b3a5fa5616fe905346e5ab6dce084555/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/SqlJsonUtils.java#L268"	FLINK	Closed	3	1	3568	pull-request-available
13324973	Remove deprecated methods in ExecutionConfig	"We can remove:

- ExecutionConfig#isLatencyTrackingEnabled (deprecated in 1.7) 

Additionally, we should remove no-ops methods in ExecutionConfig.

    - ExecutionConfig#disable/enableSysoutLogging (deprecated in 1.10)
    - ExecutionConfig#set/isFailTaskOnCheckpointError (deprecated in 1.9) 

They are {{Public}}, however they became no-op operations, which can be argued already broke the stability guarantees."	FLINK	Closed	3	7	3568	pull-request-available
13556216	Implement type inference for reinterpret_cast function	https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/Reinterpret.scala	FLINK	Closed	3	7	3568	pull-request-available
13231976	Port utility methods for extracting fields information from TypeInformation	We need those methods in the api-module in order to create {{Table}} out of {{DataSet/Stream}}.	FLINK	Closed	3	4	3568	pull-request-available
13301012	flink legacy planner should not use CollectionEnvironment any more	"As discussed in https://github.com/apache/flink/pull/11794，{{CollectionEnvironment}} is not a good practice, as it is not going through all the steps that a regular user program would go. We should change the tests to use {{LocalEnvironment}}. 

commit ""Introduce CollectionPipelineExecutor for CollectionEnvironment ([c983ac9|https://github.com/apache/flink/commit/c983ac9c49b7b58394574efdde4f39e8d33a8582])""  should also be reverted at that moment."	FLINK	Closed	3	4	3568	pull-request-available
13380486	Performance regression on 25.05	"Tests such as:
* multiInputMapSink
* multiInputOneIdleMapSink
* readFileSplit

show regressions.

Regression in run for range: 80ad5b3b51-bb597ea-1621977169

It is most probably caused by: https://github.com/apache/flink/commit/ee9f9b227a7703c2688924070c4746a70bff3fd8"	FLINK	Closed	3	4	3568	pull-request-available
13572433	Parsing temporal table join throws cryptic exceptions	"1. Wrong expression type in {{AS OF}}:
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF 'o.rowtime' AS r "" +
      ""ON o.currency = r.currency
{code}

throws: 

{code}
java.lang.AssertionError: cannot convert CHAR literal to class org.apache.calcite.util.TimestampString
{code}

2. Not a simple table reference in {{AS OF}}
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF o.rowtime + INTERVAL '1' SECOND AS r "" +
      ""ON o.currency = r.currency
{code}

throws:
{code}
java.lang.AssertionError: no unique expression found for {id: o.rowtime, prefix: 1}; count is 0
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13434368	TimestampsAndWatermarksOperator should not propagate WatermarkStatus	The lifecycle/scope of WatermarkStatus is tightly coupled with watermarks. Upstream watermarks are cut off in the TimestampsAndWatermarksOperator and therefore watermark statuses should be cut off as well.	FLINK	Closed	3	1	3568	pull-request-available
13269264	Use higher granularity units in generated docs for Duration & MemorySize if possible	"It was mentioned on two occasions (https://github.com/apache/flink/pull/10216#discussion_r346866339, https://github.com/apache/flink/pull/10217/files#r347491314) that it would be better to use a higher granularity units if it is possible.

Right now for a default value of {{Duration.ofMinutes(1)}} the generated documentation will be printed as:

{{60000ms}}

but it would be better readable to print it as:

{{1min}}"	FLINK	Closed	3	4	3568	pull-request-available
13391350	InputStatus should not contain END_OF_RECOVERY	"We added the END_OF_RECOVERY enum value in order to support recovery of unaligned checkpoints with rescaling.

However the InputStatus is expose in a public interface via {{SourceReader}}. At the same time it is not a valid value which the {{SourceReader}} can return.

We should internally replace the InputStatus with an internal equivalent."	FLINK	Closed	3	1	3568	pull-request-available
13300004	Add open method to DeserializationSchema	"Additionally add support for it in connectors:
* Kafka
* PubSub
* RabbitMQ
* Kinesis"	FLINK	Closed	3	7	3568	pull-request-available
13263712	Update documentation regarding Temporary Objects	"* update references to deprecated methods
* describe the concept of temporary tables"	FLINK	Closed	3	7	3568	pull-request-available
13596954	Support LEAD/LAG functions in Table API	We should natively support LAG/LEAD functions in Table API.	FLINK	Open	3	4	3568	pull-request-available
13304842	TableEnvironment fromValues not work with map type and SQL	"{code:java}
Map<Integer, Integer> mapData = new HashMap<>();
      mapData.put(1, 1);
      mapData.put(2, 2);
      Row row = Row.of(mapData);
      tEnv().createTemporaryView(""values_t"", tEnv().fromValues(Collections.singletonList(row)));
      Iterator<Row> iter = tEnv().executeSql(""select * from values_t"").collect();

      List<Row> results = new ArrayList<>();
      while (iter.hasNext()) {
         results.add(iter.next());
      }
      System.out.println(results);
{code}
Not work, will occur exception:
{code:java}
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP f0) NOT NULL
converted type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP NOT NULL f0) NOT NULL
{code}
If change to {{Iterator<Row> iter = tEnv().from(""values_t"").execute().collect();}} will work."	FLINK	Closed	3	1	3568	pull-request-available
13201055	Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution	"If a time interval was specified with {{YEAR TO MONTH}} resolution like e.g.:
{code}
SELECT * 
FROM Mytable
GROUP BY 
    TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)
{code}
it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution."	FLINK	Closed	3	1	3568	pull-request-available
13320624	Create an uber jar when packaging flink-avro for sql client	"Currently users have to provide dependencies such as avro, jackson-core-asl, jackson-mapper-asl and joda-time in the job jar for DataStream jobs, or manually copy them into flink/lib in SQL jobs when using avro formatting.

It's better to generate an uber jar including these dependencies when packaging flink-avro module. "	FLINK	Closed	3	4	3568	pull-request-available
13238695	Port TableEnvironment to flink-api modules	"{{TableEnvironments}} should be a purely API class(es). Current implementation should be split into {{CatalogManager}} and {{Planner}}. The {{Planner}} should be discovered based on configuration. This will allow using either the legacy or the Blink planner.

This applies to the {{StreamTableEnvironment}}. The {{BatchTableEnvironment}} will be left as is. One will be able to use the new {{StreamTableEnvironment}} for stream processing with the legacy planner or stream and batch for the Blink {{Planner}}."	FLINK	Closed	3	4	3568	pull-request-available
13565276	Set ALWAYS ChainingStrategy in TemporalSort	Similarly to FLINK-27992 we should ALWAYS chaining strategy in TemporalSort operator	FLINK	Closed	3	1	3568	pull-request-available
13303573	BatchTableEnvironment#fromValues(Object... values) throws StackOverflowError 	"The Error can be reproduced by following code:
{code:java}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);
tEnv.fromValues(1L, 2L, 3L);
{code}
The Error is as following:
{code:java}
Exception in thread ""main"" java.lang.StackOverflowErrorException in thread ""main"" java.lang.StackOverflowError at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110) at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.expressions.ApiExpressionUtils.convertArray(ApiExpressionUtils.java:142) at org.apache.flink.table.expressions.ApiExpressionUtils.objectToExpression(ApiExpressionUtils.java:100) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1030) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163)
...{code}"	FLINK	Closed	3	1	3568	pull-request-available
13313068	Can not create a catalog from user jar	"The {{CREATE CATALOG}} statement does not work if the catalog implementation comes from the user classloader. The problem is that {{org.apache.flink.table.planner.operations.SqlToOperationConverter#convertCreateCatalog}} uses the {{SqlToOperationConverter}} classloader.

We should use {{Thread.currentThread().getContextClassloader()}} for now.

One of the ways to reproduce it is try to create e.g. a postgres catalog with the {{flink-connector-jdbc}} passed as an additional jar to {{sql--client}}"	FLINK	Closed	2	1	3568	pull-request-available
13383342	Deprecate/Remove StreamOperator#dispose method	"As discussed in the ML thread (e.g. https://lists.apache.org/thread.html/r34a05c77bddb2a7cb550c0b820d2a4aa8e1be882fc81bee501fb74e8%40%3Cdev.flink.apache.org%3E) we want to clean up the contract of {{close}} and {{dispose}} methods. 

We suggest introducing a new method finish(), deprecate or remove the dispose method and extract finish() part out of the close()."	FLINK	Closed	3	7	3568	pull-request-available
13297293	Improve literals conversion in ExpressionConverter	"There are couple of issues with the {{ExpressionResolver}} and literals conversion:
1. There is a lot of code duplication
2. Precision of certain types might get lost e.g. BINARY, CHAR"	FLINK	Closed	3	1	3568	pull-request-available
13370144	Test display last n exceptions/causes for job restarts in Web UI	This is the testing task for FLINK-6042. We should test whether the root causes for multiple restarts are properly displayed in the web UI.	FLINK	Closed	1	4	3568	pull-request-available, release-testing
13378788	Incompatible subtask mappings while resuming from unaligned checkpoints	"A user [reported|https://lists.apache.org/x/list.html?user@flink.apache.org:lte=1M:Flink%201.13.0%20reactive%20mode:%20Job%20stop%20and%20cannot%20restore%20from%20checkpoint] that he encountered an internal error while resuming during reactive mode. There isn't an immediate connection to reactive mode, so it's safe to assume that one rescaling case was not covered.

{noformat}
Caused by: java.lang.IllegalStateException: Incompatible subtask mappings: are multiple operators ingesting/producing intermediate results with varying degrees of parallelism?Found RescaleMappings{mappings=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179], [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]]} and RescaleMappings{mappings=[[0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98, 105, 112, 119, 126, 133, 140, 147, 154, 161, 168, 175, 182, 189, 196, 203], [1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99, 106, 113, 120, 127, 134, 141, 148, 155, 162, 169, 176, 183, 190, 197, 204], [2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86, 93, 100, 107, 114, 121, 128, 135, 142, 149, 156, 163, 170, 177, 184, 191, 198, 205], [3, 10, 17, 24, 31, 38, 45, 52, 59, 66, 73, 80, 87, 94, 101, 108, 115, 122, 129, 136, 143, 150, 157, 164, 171, 178, 185, 192, 199, 206], [4, 11, 18, 25, 32, 39, 46, 53, 60, 67, 74, 81, 88, 95, 102, 109, 116, 123, 130, 137, 144, 151, 158, 165, 172, 179, 186, 193, 200, 207], [5, 12, 19, 26, 33, 40, 47, 54, 61, 68, 75, 82, 89, 96, 103, 110, 117, 124, 131, 138, 145, 152, 159, 166, 173, 180, 187, 194, 201, 208], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97, 104, 111, 118, 125, 132, 139, 146, 153, 160, 167, 174, 181, 188, 195, 202, 209]]}.
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.checkSubtaskMapping(TaskStateAssignment.java:322) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.getInputMapping(TaskStateAssignment.java:306) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.reDistributeInputChannelStates(StateAssignmentOperation.java:409) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignAttemptState(StateAssignmentOperation.java:193) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignStates(StateAssignmentOperation.java:139) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1566) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1646) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphAndRestoreState(AdaptiveScheduler.java:986) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$createExecutionGraphAndRestoreStateAsync$25(AdaptiveScheduler.java:976) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.BackgroundTask.lambda$new$0(BackgroundTask.java:57) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.lang.Thread.run(Thread.java:834) ~[?:?]
{noformat}

Here it seems that the same gate gets input from a range-partitioned and a round-robin partitioned channel at the same time. During the implementation of FLINK-19801, we couldn't find such a case and optimized the implementation accordingly.

We have asked the user to provide his topology."	FLINK	Closed	1	1	3568	pull-request-available
13556214	Implement type inference for Over function	"https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/overOffsets.scala

Functions:
* OVER
* CURRENT_RANGE
* CURRENT_ROW
* UNBOUNDED_ROW
* UNBOUNDED_RANGE"	FLINK	Closed	3	7	3568	pull-request-available
13580266	BlockStatementGrouper uses lots of memory	"For deeply nested {{if else}} statements {{BlockStatementGrouper}} uses loads of memory and fails with OOM quickly.

When running JMs with around 400mb a query like:
{code}
select case when orderid = 0 then 1 when orderid = 1 then 2 when orderid
    = 2 then 3 when orderid = 3 then 4 when orderid = 4 then 5 when orderid = 5 then
    6 when orderid = 6 then 7 when orderid = 7 then 8 when orderid = 8 then 9 when
    orderid = 9 then 10 when orderid = 10 then 11 when orderid = 11 then 12 when orderid
    = 12 then 13 when orderid = 13 then 14 when orderid = 14 then 15 when orderid
    = 15 then 16 when orderid = 16 then 17 when orderid = 17 then 18 when orderid
    = 18 then 19 when orderid = 19 then 20 when orderid = 20 then 21 when orderid
    = 21 then 22 when orderid = 22 then 23 when orderid = 23 then 24 when orderid
    = 24 then 25 when orderid = 25 then 26 when orderid = 26 then 27 when orderid
    = 27 then 28 when orderid = 28 then 29 when orderid = 29 then 30 when orderid
    = 30 then 31 when orderid = 31 then 32 when orderid = 32 then 33 when orderid
    = 33 then 34 when orderid = 34 then 35 when orderid = 35 then 36 when orderid
    = 36 then 37 when orderid = 37 then 38 when orderid = 38 then 39 when orderid
    = 39 then 40 when orderid = 40 then 41 when orderid = 41 then 42 when orderid
    = 42 then 43 when orderid = 43 then 44 when orderid = 44 then 45 when orderid
    = 45 then 46 when orderid = 46 then 47 when orderid = 47 then 48 when orderid
    = 48 then 49 when orderid = 49 then 50 when orderid = 50 then 51 when orderid
    = 51 then 52 when orderid = 52 then 53 when orderid = 53 then 54 when orderid
    = 54 then 55 when orderid = 55 then 56 when orderid = 56 then 57 when orderid
    = 57 then 58 when orderid = 58 then 59 when orderid = 59 then 60 when orderid
    = 60 then 61 when orderid = 61 then 62 when orderid = 62 then 63 when orderid
    = 63 then 64 when orderid = 64 then 65 when orderid = 65 then 66 when orderid
    = 66 then 67 when orderid = 67 then 68 when orderid = 68 then 69 when orderid
    = 69 then 70 when orderid = 70 then 71 when orderid = 71 then 72 when orderid
    = 72 then 73 when orderid = 73 then 74 when orderid = 74 then 75 when orderid
    = 75 then 76 when orderid = 76 then 77 when orderid = 77 then 78 when orderid
    = 78 then 79 when orderid = 79 then 80 when orderid = 80 then 81 when orderid
    = 81 then 82 when orderid = 82 then 83 when orderid = 83 then 84 when orderid
    = 84 then 85 when orderid = 85 then 86 when orderid = 86 then 87 when orderid
    = 87 then 88 when orderid = 88 then 89 when orderid = 89 then 90 when orderid
    = 90 then 91 when orderid = 91 then 92 when orderid = 92 then 93 when orderid
    = 93 then 94 when orderid = 94 then 95 when orderid = 95 then 96 when orderid
    = 96 then 97 when orderid = 97 then 98 when orderid = 98 then 99 when orderid
    = 99 then 100 when orderid = 100 then 101 when orderid = 101 then 102 when orderid
    = 102 then 103 when orderid = 103 then 104 when orderid = 104 then 105 when orderid
    = 105 then 106 when orderid = 106 then 107 when orderid = 107 then 108 when orderid
    = 108 then 109 when orderid = 109 then 110 when orderid = 110 then 111 when orderid
    = 111 then 112 when orderid = 112 then 113 when orderid = 113 then 114 when orderid
    = 114 then 115 when orderid = 115 then 116 when orderid = 116 then 117 when orderid
    = 117 then 118 when orderid = 118 then 119 when orderid = 119 then 120 when orderid
    = 120 then 121 when orderid = 121 then 122 when orderid = 122 then 123 when orderid
    = 123 then 124 when orderid = 124 then 125 when orderid = 125 then 126 when orderid
    = 126 then 127 when orderid = 127 then 128 when orderid = 128 then 129 when orderid
    = 129 then 130 when orderid = 130 then 131 when orderid = 131 then 132 when orderid
    = 132 then 133 when orderid = 133 then 134 when orderid = 134 then 135 when orderid
    = 135 then 136 when orderid = 136 then 137 when orderid = 137 then 138 when orderid
    = 138 then 139 when orderid = 139 then 140 when orderid = 140 then 141 when orderid
    = 141 then 142 when orderid = 142 then 143 when orderid = 143 then 144 when orderid
    = 144 then 145 when orderid = 145 then 146 when orderid = 146 then 147 when orderid
    = 147 then 148 when orderid = 148 then 149 when orderid = 149 then 150 when orderid
    = 150 then 151 when orderid = 151 then 152 when orderid = 152 then 153 when orderid
    = 153 then 154 when orderid = 154 then 155 when orderid = 155 then 156 when orderid
    = 156 then 157 when orderid = 157 then 158 when orderid = 158 then 159 when orderid
    = 159 then 160 when orderid = 160 then 161 when orderid = 161 then 162 when orderid
    = 162 then 163 when orderid = 163 then 164 when orderid = 164 then 165 when orderid
    = 165 then 166 when orderid = 166 then 167 when orderid = 167 then 168 when orderid
    = 168 then 169 when orderid = 169 then 170 when orderid = 170 then 171 when orderid
    = 171 then 172 when orderid = 172 then 173 when orderid = 173 then 174 when orderid
    = 174 then 175 when orderid = 175 then 176 when orderid = 176 then 177 when orderid
    = 177 then 178 when orderid = 178 then 179 when orderid = 179 then 180 when orderid
    = 180 then 181 when orderid = 181 then 182 when orderid = 182 then 183 when orderid
    = 183 then 184 when orderid = 184 then 185 when orderid = 185 then 186 when orderid
    = 186 then 187 when orderid = 187 then 188 when orderid = 188 then 189 when orderid
    = 189 then 190 when orderid = 190 then 191 when orderid = 191 then 192 when orderid
    = 192 then 193 when orderid = 193 then 194 when orderid = 194 then 195 when orderid
    = 195 then 196 when orderid = 196 then 197 when orderid = 197 then 198 when orderid
    = 198 then 199 when orderid = 199 then 200 when orderid = 200 then 201 when orderid
    = 201 then 202 when orderid = 202 then 203 when orderid = 203 then 204 when orderid
    = 204 then 205 when orderid = 205 then 206 when orderid = 206 then 207 when orderid
    = 207 then 208 when orderid = 208 then 209 when orderid = 209 then 210 when orderid
    = 210 then 211 when orderid = 211 then 212 when orderid = 212 then 213 when orderid
    = 213 then 214 when orderid = 214 then 215 when orderid = 215 then 216 when orderid
    = 216 then 217 when orderid = 217 then 218 when orderid = 218 then 219 when orderid
    = 219 then 220 when orderid = 220 then 221 when orderid = 221 then 222 when orderid
    = 222 then 223 when orderid = 223 then 224 when orderid = 224 then 225 when orderid
    = 225 then 226 when orderid = 226 then 227 when orderid = 227 then 228 when orderid
    = 228 then 229 when orderid = 229 then 230 when orderid = 230 then 231 when orderid
    = 231 then 232 when orderid = 232 then 233 when orderid = 233 then 234 when orderid
    = 234 then 235 when orderid = 235 then 236 when orderid = 236 then 237 when orderid
    = 237 then 238 when orderid = 238 then 239 when orderid = 239 then 240 when orderid
    = 240 then 241 when orderid = 241 then 242 when orderid = 242 then 243 when orderid
    = 243 then 244 when orderid = 244 then 245 when orderid = 245 then 246 when orderid
    = 246 then 247 when orderid = 247 then 248 when orderid = 248 then 249 when orderid
    = 249 then 250 else 9999 end case_when_col from sample_data_1;
{code}

fails with an OOM. (Yes, I know the query can be simplified, but it shows the case)."	FLINK	Resolved	3	4	3568	pull-request-available
13387961	All records are processed in the close stage in ContinuousFileReaderOperatorBenchmark	The {{TARGET_COUNT_REACHED_LATCH}} is not correctly reset after the warmup iterations and thus subsequent runs process all records in the {{CLOSE}} stage of the {{ContinuousFileReaderOperator}} testing something different than anticipated.	FLINK	Closed	3	1	3568	pull-request-available
13296470	Support parsing LIKE clause in CREATE TABLE statement	We should support the CREATE TABLE ... LIKE syntax in SqlParser	FLINK	Closed	3	7	3568	pull-request-available
13553409	MATCH_RECOGNIZE AFTER MATCH clause can not be deserialised from a compiled plan	"{code}
        String sql =
                ""insert into MySink""
                        + "" SELECT * FROM\n""
                        + "" MyTable\n""
                        + ""   MATCH_RECOGNIZE(\n""
                        + ""   PARTITION BY vehicle_id\n""
                        + ""   ORDER BY `rowtime`\n""
                        + ""   MEASURES \n""
                        + ""       FIRST(A.`rowtime`) as startTime,\n""
                        + ""       LAST(A.`rowtime`) as endTime,\n""
                        + ""       FIRST(A.engine_temperature) as Initial_Temp,\n""
                        + ""       LAST(A.engine_temperature) as Final_Temp\n""
                        + ""   ONE ROW PER MATCH\n""
                        + ""   AFTER MATCH SKIP TO FIRST B\n""
                        + ""   PATTERN (A+ B)\n""
                        + ""   DEFINE\n""
                        + ""       A as LAST(A.engine_temperature,1) is NULL OR A.engine_temperature > LAST(A.engine_temperature,1),\n""
                        + ""       B as B.engine_temperature < LAST(A.engine_temperature)\n""
                        + ""   )MR;"";
        util.verifyJsonPlan(String.format(sql, afterClause));
{code}

fails with:

{code}
Could not resolve internal system function '$SKIP TO LAST$1'. This is a bug, please file an issue. (through reference chain: org.apache.flink.table.planner.plan.nodes.exec.serde.JsonPlanGraph[""nodes""]->java.util.ArrayList[3]->org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch[""matchSpec""]->org.apache.flink.table.planner.plan.nodes.exec.spec.MatchSpec[""after""])
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13324219	Remove deprecated RuntimeContext#getAllAccumulators	"We could  remove the deprecated:
{code}
RuntimeContext#getAllAcumullators
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13556206	Remove old expression stack leftovers for time functions	"Remove leftovers from https://issues.apache.org/jira/browse/FLINK-13785

There are some parts of the time functions that have not been removed e.g. https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/time.scala and some code in https://github.com/apache/flink/blob/b6000f6e589128ae1fd1e0e7d063a1b6ff1fcc20/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/PlannerExpressionConverter.scala"	FLINK	Closed	3	7	3568	pull-request-available
13360332	Document possible recommended usage of Bounded{One/Multi}Input.endInput and emphasize that they could be called multiple times	"It is too tempting to use these api, especially {{BoundedOneInput.endInput}}, to commit final result before FLIP-147 delivered. And this will cause re-commit after failover as [~gaoyunhaii] has pointed out in FLINK-21132.

I have [pointed|https://github.com/apache/iceberg/issues/2033#issuecomment-784153620] this out in [apache/iceberg#2033|https://github.com/apache/iceberg/issues/2033], please correct me if I was wrong.

cc [~aljoscha] [~pnowojski] [~roman_khachatryan]"	FLINK	Closed	10200	4	3568	auto-deprioritized-major, pull-request-available, stale-minor
13212945	Create CatalogManager to manage multiple catalogs and encapsulate Calcite schema	"Flink allows for more than one registered catalogs. {{CatalogManager}} class is the holding class to manage and encapsulate the catalogs and their interrelations with Calcite.

Following section describes how table resolution should work:

h4. PATH resolution:

First look into DEFAULT PATH: cat or cat.db. Then in the root. This is also the behavior of Calcite. We should mimic this behavior in Flink.
Example:

{noformat}
root:
  |- builtin
      |- default
          |- tab1
          |- tab2
      |- db1
          |- tab1
      |- clashing
          |- tab1
  |- cat1
      |- db1
          |- tab1
          |- tab2
  |- extCat1
      |- tab1
      |- clashing
          |- tab1
      |- extCat2
          |- tab1
          |- tab2
  |- clashing (ExternalCatalog)
      |- tab1
{noformat}
      
There is always a default catalog, initially set to builtin and default database initially set to default.

{noformat}
Assume 
default path = builtin then
  default.tab1 = builtin.default.tab1
  tab1 = error
  cat1.db1.tab1 = cat1.db1.tab1
  clashing.tab1 = builtin.clashing.tab1
default path = extCat1 then
  tab1 = extCat1.tab1
  clashing.tab1 = extCat1.clashing.tab1
default path = extCat1.extCat2 (do not support further nesting) then
  tab1 = extCat1.extCat2.tab1
  clashing.tab1 = clashing.tab1
{noformat}
  

h4. Structure in the Planner(Calcite-specific)

{noformat}
root: CatalogManagerSchema(CatalogManager)
     |- CatalogCalciteSchema(ReadableCatalog)
         |- DatabaseCalciteSchema (ReadableCatalog scoped to DB)
             |- Table
     |- ExternalCatalogSchema
         |- Table
         |- ExternalCatalogSchema
             |- Table
{noformat}

h4. Structure in the API
{noformat}    
CatalogManager:
    |- ReadableCatalog
      |- CatalogTable
    |- ExternalCatalog
      |- ExternalCatalog
      |- ExternalCatalogTable
{noformat}"	FLINK	Resolved	3	7	3568	pull-request-available
13194127	Add a switch to run_test to configure if logs should be checked for errors/excepions	After adding the switch, we should disable log checking for nightly-tests that currently fail (or fix the test).	FLINK	Closed	1	4	3568	pull-request-available
13574746	AggregateQueryOperations produces wrong asSerializableString representation	"A table API query:
{code}
        env.fromValues(1, 2, 3)
                .as(""number"")
                .select(col(""number"").count())
                .insertInto(TEST_TABLE_API)
{code}

produces

{code}
INSERT INTO `default`.`timo_eu_west_1`.`table_api_basic_api` SELECT `EXPR$0` FROM (
    SELECT (COUNT(`number`)) AS `EXPR$0` FROM (
        SELECT `f0` AS `number` FROM (
            SELECT `f0` FROM (VALUES 
                (1),
                (2),
                (3)
            ) VAL$0(`f0`)
        )
    )
    GROUP BY 
)
{code}

which is missing a grouping expression"	FLINK	Closed	3	1	3568	pull-request-available
13382184	Tasks are blocked while broadcasting stream status	"On a cluster I observed symptoms of tasks being blocked for long time, causing long delays with unaligned checkpointing. 99% of those cases were caused by `broadcastEmit` of the stream status

{noformat}
2021-06-04 14:41:44,049 ERROR org.apache.flink.runtime.io.network.buffer.LocalBufferPool   [] - Blocking wait [11059 ms] for an available buffer.
java.lang.Exception: Stracktracegenerator
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:323) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:290) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:338) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:314) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:246) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:142) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:67) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.writeStreamStatus(RecordWriterOutput.java:136) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.streamstatus.AnnouncedStatus.ensureActive(AnnouncedStatus.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:182) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.pollNext(IteratorSourceReader.java:98) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:294) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:69) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:422) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:680) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:635) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:619) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{noformat}

*I have seen this happening both in source and network tasks.* ~80% cases were in the source tasks

{{broadcastEmit}} can easily bypass our non blocking checks. There are two questions:
# why is the stream idling so much? It’s like almost every millisecond it’s broadcasting status
# should we optimise this? Broadcasting CBs and other events is not an issue, as those are events that do not request/require buffers"	FLINK	Closed	2	1	3568	pull-request-available
13316803	Kerberized YARN per-job on Docker test failed to download JDK 8u251	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
{code}
+ mkdir -p /usr/java/default
+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie
+ tar --strip-components=1 -xz -C /usr/java/default/

gzip: stdin: not in gzip format
tar: Child returned status 1
{code}"	FLINK	Closed	1	1	3568	pull-request-available
13192203	YarnConfigurationITCase.testFlinkContainerMemory test instability	"Test appeared to fail (by a narrow margin) without a reason randomly:

{noformat}
Failed tests: 
  YarnConfigurationITCase.testFlinkContainerMemory:182 
Expected: is a numeric value within <0.1> of <1.0>
     but: <0.8913043478260869> differed by <0.008695652173913077>
{noformat}

https://api.travis-ci.org/v3/job/442246057/log.txt"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13398595	Document FLIP-147 capabiliites and limitations	We should document how to enable the checkpointing after tasks finish as well as limitations we are aware of.	FLINK	Closed	2	7	3568	pull-request-available
13362233	 CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107

{code:java}
[ERROR] testAsyncCheckpointFailureTriggerJobFailed(org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase)  Time elapsed: 38.623 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:62)
	at org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed(CheckpointFailureManagerITCase.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
"	FLINK	Closed	3	1	3568	pull-request-available, test-stability
13228910	Convert CatalogTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	In FLINK-11476, we created CatalogManager to hook up planner with unified catalog APIs. What's missing there is, at the very last step, convert CatalogBaseTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs, like how {{ExternalTableUtil.fromExternalCatalogTable()}} works to convert the old {{ExternalCatalogTable}} to a Calcite table	FLINK	Closed	3	7	3568	pull-request-available
13409619	Remove scala suffix from respective benchmarks dependencies	With FLINK-24018 few dependencies lost its scala suffix. We should remove it in benchmark dependencies to test against newest artifacts.	FLINK	Closed	3	1	3568	pull-request-available
13404250	Add Flink 1.14 MigrationVersion	"Currently the largest MigrationVersion is 1.13. We need newer versions to add more serializer compatibility tests.
"	FLINK	Closed	3	4	3568	pull-request-available
13231708	Fix failling e2e test test_streaming_sql.sh	"https://travis-ci.org/apache/flink/jobs/535231108

{code}
==============================================================================
Running 'Streaming SQL end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-37856266807
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.9-SNAPSHOT-bin/flink-1.9-SNAPSHOT
Starting cluster.
Starting standalonesession daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Dispatcher REST endpoint is up.
[INFO] 1 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 2 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 3 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting execution of program
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(INTEGER NOT NULL correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
expression type is RecordType(INTEGER correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
set is rel#150:LogicalWindowAggregate.NONE(input=HepRelVertex#139,group={},correct=SUM($1),w$start=start('w$),w$end=end('w$),w$rowtime=rowtime('w$),w$proctime=proctime('w$))
expression is LogicalWindowAggregate#167
	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:57)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:234)
	at org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.onMatch(ExtendedAggregateExtractProjectRule.java:90)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:559)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:418)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:255)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:214)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:201)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlanner(TableEnvImpl.scala:282)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlannerSequentially(TableEnvImpl.scala:248)
	at org.apache.flink.table.api.TableEnvImpl.optimizeNormalizeLogicalPlan(TableEnvImpl.scala:204)
	at org.apache.flink.table.api.StreamTableEnvImpl.optimize(StreamTableEnvImpl.scala:738)
	at org.apache.flink.table.api.StreamTableEnvImpl.translate(StreamTableEnvImpl.scala:787)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:100)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:83)
	at org.apache.flink.sql.tests.StreamSQLTestProgram.main(StreamSQLTestProgram.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:269)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:742)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:272)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:204)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:983)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1056)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1056)
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13180226	WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow	"{{WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow}} failed on Travis.

https://api.travis-ci.org/v3/job/418629694/log.txt"	FLINK	Closed	2	1	3568	test-stability
13486345	BatchExecutionKeyedStateBackend is using incorrect ExecutionConfig when creating serializer	"{{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend#getOrCreateKeyedState}} is using freshly constructed {{ExecutionConfig}}, instead of the one configured by the user from the environment.


{code:java}
    public <N, S extends State, T> S getOrCreateKeyedState(
            TypeSerializer<N> namespaceSerializer, StateDescriptor<S, T> stateDescriptor)
            throws Exception {
        checkNotNull(namespaceSerializer, ""Namespace serializer"");
        checkNotNull(
                keySerializer,
                ""State key serializer has not been configured in the config. ""
                        + ""This operation cannot use partitioned state."");

        if (!stateDescriptor.isSerializerInitialized()) {
            stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
        }
{code}

The correct one could be obtained from {{env.getExecutionConfig()}} in {{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend#createKeyedStateBackend}} "	FLINK	Closed	4	1	3568	pull-request-available
13208537	KafkaITCase.testConcurrentProducerConsumerTopology times out on Travis	"The {{KafkaITCase.testConcurrentProducerConsumerTopology}} times out on Travis.

{code}
20:26:29.579 [ERROR] Errors: 
20:26:29.579 [ERROR]   KafkaITCase.testConcurrentProducerConsumerTopology:73->KafkaConsumerTestBase.runSimpleConcurrentProducerConsumerTopology:824->KafkaTestBase.deleteTestTopic:206->Object.wait:502->Object.wait:-2 Â» TestTimedOut
{code}

The solution might as simple as increasing the timeout.

https://api.travis-ci.org/v3/job/476975725/log.txt"	FLINK	Closed	2	4	3568	pull-request-available, test-stability
13031245	Fails AkkaRpcServiceTest#testTerminationFuture	"{code}
testTerminationFuture(org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest)  Time elapsed: 1.013 sec  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 1000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at akka.remote.Remoting.start(Remoting.scala:179)
	at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:634)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:142)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:119)
	at akka.actor.ActorSystem$.create(ActorSystem.scala:67)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
	at org.apache.flink.runtime.akka.AkkaUtils$.createDefaultActorSystem(AkkaUtils.scala:114)
	at org.apache.flink.runtime.akka.AkkaUtils.createDefaultActorSystem(AkkaUtils.scala)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.testTerminationFuture(AkkaRpcServiceTest.java:134)
{code} in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest while testing current master 1.2.0 branch "	FLINK	Closed	3	1	3568	test-stability
13371934	JobMasterStopWithSavepointITCase failed due to status is FAILING	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16405&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4472


{code:java}
[ERROR] Failures: 
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheSuspend:133->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate:138->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR] Errors: 
[ERROR]   JobMasterStopWithSavepointITCase.suspendWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished:103->stopWithSavepointNormalExecutionHelper:113->setUpJobGraph:307 » IllegalState
[ERROR]   JobMasterStopWithSavepointITCase.testRestartCheckpointCoordinatorIfStopWithSavepointFails:237 » IllegalState
[INFO] 
[ERROR] Tests run: 1645, Failures: 2, Errors: 2, Skipped: 51

{code}
"	FLINK	Resolved	2	1	3568	test-stability
13326530	Clean up the UnilateralSortMerger	"This is a preparation step for [FLIP-140|https://cwiki.apache.org/confluence/display/FLINK/FLIP-140%3A+Introduce+bounded+style+execution+for+keyed+streams]. The purpose of the task is two-folds:
* break down the implementation into a more composable pieces
* introduce a way to produce records in a push-based manner instead of pull-based with additional reading thread."	FLINK	Closed	3	7	3568	pull-request-available
13278176	Preserve logs from BashJavaUtils and make them part of TM logs	"In FLINK-13983 we introduced BashJavaUtils utility to call in taskmanager.sh before starting TM and calculate memory configuration for the JVM process of TM.

Ideally, it would be nice to preserve BashJavaUtils logs and make them part of the TM logs. Currently, logging for BashJavaUtils is configured from the class path and can differ from TM logging. Moreover TM logging can rewrite BashJavaUtils even if we align their loggings (e.g. log4j.appender.file.append=false in default log4j.properties  for Flink)."	FLINK	Closed	1	4	3568	pull-request-available
13597368	TIMESTAMPDIFF can not be string serialized	TIMESTAMPDIFF can not be properly string serialized, because TIMEPOINTUNIT can not be serialized.	FLINK	In Progress	3	1	3568	pull-request-available
13205141	Exception in code generation when ambiguous columns in MATCH_RECOGNIZE	"Query:
{code}
SELECT *
FROM Ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol, price
  ORDER BY proctime
  MEASURES
    A.symbol AS symbol,
    A.price AS price
  PATTERN (A)
  DEFINE
    A AS symbol = 'a'
) AS T
{code}

throws a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. 

I've also created a calcite ticket to fix it on calcite's side: [CALCITE-2747]"	FLINK	Closed	3	1	3568	pull-request-available
13185500	Savepoints should be counted as retained checkpoints	"This task is about reverting [FLINK-6328].

The problem is that you can get incorrect results with exactly-once sinks if there is a failure after taking a savepoint but before taking the next checkpoint because the savepoint will also have manifested side effects to the sink.
"	FLINK	Closed	3	1	3568	pull-request-available
13343946	Some Table examples are not built correctly	"Some examples were moved to the {{org.apache.flink.table.examples.scala.basics}} package but the pom.xml was not updated. This means the example jars are not built correctly and do not contain the classes.

Examples that I noticed:
* org.apache.flink.table.examples.scala.basics.StreamTableExample
* org.apache.flink.table.examples.scala.basics.TPCHQuery3Table

We should update the {{includes}} sections e.g.:

{code}
<execution>
	<id>StreamTableExample</id>
	<phase>package</phase>
	<goals>
		<goal>jar</goal>
	</goals>
	<configuration>
		<classifier>StreamTableExample</classifier>

<!--- The sections below should be updated -->

		<archive>
			<manifestEntries>
				<program-class>org.apache.flink.table.examples.scala.StreamTableExample</program-class>
			</manifestEntries>
		</archive>
		<includes>
			<include>org/apache/flink/table/examples/scala/StreamTableExample*</include>
		</includes>
	</configuration>
</execution>
{code}"	FLINK	Closed	2	1	3568	pull-request-available
13355441	Write savepoints in unified format from HeapStateBackend	The aim is to implement a {{HeapKeyValueStateIterator}} which can be used to produce a unified savepoint out of a HeapKeyedStateBackend	FLINK	Closed	3	7	3568	pull-request-available
13242119	Remove expressionBridge from QueryOperations factories	Expression bridge is used to create a schema of QueryOperation. This is no longer necessary with ResolvedExpressions in place.	FLINK	Closed	3	7	3568	pull-request-available
13415595	Document claim & no-claim mode	We should describe how the different restore modes work. It is important to go through the FLIP and include all {{NOTES}} in the written documentation	FLINK	Closed	3	7	3568	pull-request-available
13238698	Improve expression based TableSchema extraction from DataStream/DataSet	"We should improve the extraction of {{TableSchema}} from {{DataStream/DataSet}}. Currently it is split into a few stages:
# Extract types ignoring time attributes via {{FieldInfoUtils#getFieldInfo}}
# Extract the rowtime and proctime positions via {{StreamTableEnvImpl#validateAndExtractTimeAttributes}}
# Adjust the indices from #1 using information from #2

All that could happen in a single pass. This will also deal with the porting/removing of a few methods from {{StreamTableEnvImpl}}."	FLINK	Closed	3	7	3568	pull-request-available
13314702	Tests RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte & RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort fail locally	"The tests:
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte

fail locally (in IDE or from cmd with {{mvn clean install}}) with
{code}
java.lang.UnsatisfiedLinkError: org.rocksdb.ReadOptions.newReadOptions()J
	at org.rocksdb.ReadOptions.newReadOptions(Native Method)
	at org.rocksdb.ReadOptions.<init>(ReadOptions.java:16)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIterator(RocksKeyGroupsRocksSingleStateIteratorTest.java:78)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIteratorShort(RocksKeyGroupsRocksSingleStateIteratorTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13353481	SQLClientSchemaRegistryITCase unstable with InternalServerErrorException: Status 500	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12253&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2021-01-20T00:10:21.3510385Z Jan 20 00:10:21 
2021-01-20T00:10:21.3516246Z Jan 20 00:10:21 [ERROR] testWriting(org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase)  Time elapsed: 0.001 s  <<< ERROR!
2021-01-20T00:10:21.3517459Z Jan 20 00:10:21 java.lang.RuntimeException: Could not build the flink-dist image
2021-01-20T00:10:21.3518178Z Jan 20 00:10:21 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:281)
2021-01-20T00:10:21.3519176Z Jan 20 00:10:21 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.<init>(SQLClientSchemaRegistryITCase.java:88)
2021-01-20T00:10:21.3519873Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-01-20T00:10:21.3520537Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-01-20T00:10:21.3521390Z Jan 20 00:10:21 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-01-20T00:10:21.3522080Z Jan 20 00:10:21 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2021-01-20T00:10:21.3522730Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
2021-01-20T00:10:21.3523452Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
2021-01-20T00:10:21.3524237Z Jan 20 00:10:21 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-01-20T00:10:21.3524879Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
2021-01-20T00:10:21.3525527Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-01-20T00:10:21.3526157Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-01-20T00:10:21.3526754Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-01-20T00:10:21.3527316Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-01-20T00:10:21.3527884Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-01-20T00:10:21.3528462Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-01-20T00:10:21.3529491Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-01-20T00:10:21.3530220Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-01-20T00:10:21.3530970Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-01-20T00:10:21.3531649Z Jan 20 00:10:21 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-01-20T00:10:21.3532201Z Jan 20 00:10:21 	at java.lang.Thread.run(Thread.java:748)
2021-01-20T00:10:21.3533545Z Jan 20 00:10:21 Caused by: com.github.dockerjava.api.exception.InternalServerErrorException: Status 500: {""message"":""Get https://registry-1.docker.io/v2/testcontainers/ryuk/manifests/0.3.0: received unexpected HTTP status: 502 Bad Gateway""}
2021-01-20T00:10:21.3534353Z Jan 20 00:10:21 
2021-01-20T00:10:21.3534955Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:247)
2021-01-20T00:10:21.3536388Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$executeAndStream$1(DefaultInvocationBuilder.java:269)
2021-01-20T00:10:21.3537066Z Jan 20 00:10:21 	... 1 more
2021-01-20T00:10:21.3541323Z Jan 20 00:10:21 
{code}"	FLINK	Closed	2	1	3568	test-stability
13280523	SELECT 'ABC'; does not work in sql-client	"A query like {{SELECT 'abc';}} fails in sql-client with blink planner enabled with an error:
{code}
org.apache.flink.table.api.ValidationException: Type CHAR(3) of table field 'EXPR$0' does not match with the physical type STRING of the 'EXPR$0' field of the TableSink consumed type.
{code}

The reason is that those sinks do not properly support new type system. There is no good way to define schema and consumed data type so that they match. We should update the in-memory sinks in sql-client to work with the legacy type system for now until the retract and upsert sinks work properly with the new type system."	FLINK	Closed	1	1	3568	pull-request-available
13321104	Improve the Python documentation about the operations in Table	Currently, there are a few documentation is out of date and should be updated. For example, Python UDTF has been already supported in Python Table API and we could use examples of Python UDTF instead of Java UDTF in the Python doc.	FLINK	Closed	3	4	3708	pull-request-available
13342028	Make invalid managed memory fraction errors of python udf more user friendly	"When managed memory is required for python udf but its ""taskmanager.memory.managed.consumer-weights"" is set to 0, error will happen but the message is hard to understand for users, see [1].

I think we should expose the invalid fraction error to users in this case and guide users to properly configure ""taskmanager.memory.managed.consumer-weights"".

[1]
{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:534)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:126)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:254)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:121)
	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:134)
	at org.apache.flink.table.runtime.operators.python.scalar.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:94)
	at org.apache.flink.table.runtime.operators.python.scalar.AbstractRowPythonScalarFunctionOperator.open(AbstractRowPythonScalarFunctionOperator.java:67)
	at org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperator.open(PythonScalarFunctionOperator.java:64)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:401)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:506)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
{code}



"	FLINK	Closed	2	4	3708	pull-request-available
13245448	flink-python releases 2 jars	"{{flink-python}} uses a classifier to differentiate itseld from the old python API. turns out thsi doesn't work since it still tries to release a normal unshaded flink-python jar.

We should drop the classifier, and either stick to flink-python or rename it as proposed in FLINK-12776."	FLINK	Closed	1	1	3708	pull-request-available
13285793	Introduce ArrowReader and ArrowWriter for Arrow format data read and write	As the title described, this aim of this JIRA is to introduce classes such as ArrowReader which is used to read the execution results of vectorized Python UDF and ArrowWriter which is used to convert Flink rows to Arrow format before sending them to the Python worker for vectorized Python UDF execution.	FLINK	Closed	3	7	3708	pull-request-available
13265008	Fix the ClassNotFoundException issue for run python job in standalone mode	java.lang.ClassNotFoundException: org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator will be thrown when running a Python UDF job in a standalone cluster.	FLINK	Closed	3	1	3708	pull-request-available
13385556	The dependencies are not handled properly when mixing use of Python Table API and Python DataStream API	The reason is that when converting from DataStream to Table, the dependencies should be handled and set correctly for the existing DataStream operators.	FLINK	Closed	3	1	3708	pull-request-available
13290278	Add Python building blocks to make sure the basic functionality of vectorized Python UDF could work	The aim of this Jira is to add Python building blocks such as the coders, etc to make sure the basic functionality of vectorized Python UDF could work.	FLINK	Closed	3	7	3708	pull-request-available
13258436	maven-shade-plugin 3.2.1 doesn't work on ARM for Flink	"recently, maven-shade-plugin  is bumped from 3.0.0 to 3.2.1 by the [commit|https://github.com/apache/flink/commit/e7216eebc846a69272c21375af0f4db8009c2e3e]. While with my test locally on ARM, The Flink build process will be jammed. After debugging, I found there is an infinite loop.

Downgrade maven-shade-plugin to 3.1.0 can solve this problem."	FLINK	Resolved	4	7	3708	pull-request-available
13325654	Support Expression in the operations of the Python Table API	"Currently, it only supports string in the operations of the Python Table API. 
For example:
{code}
>>> tab.group_by(""key"").select(""key, value.avg"")
{code}

After introducing the Expression class in FLINK-19114, it's possible to support
to use Expression in the operations in the Python Table API, e.g.
{code}
>>> tab.group_by(col(""key"")).select(col(""key""), col(""value"").avg)
{code}"	FLINK	Closed	3	7	3708	pull-request-available
13175379	FsStorageLocationReferenceTest#testEncodeAndDecode failed in CI	"{code:java}
testEncodeAndDecode(org.apache.flink.runtime.state.filesystem.FsStorageLocationReferenceTest)  Time elapsed: 0.027 sec  <<< ERROR!
java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in hostname at index 5: gl://碪⯶㪴]ឪ嵿⎐䪀筪ᆶ歑ᆂ玚୓䇷ノⳡ೯43575/䡷ᦼ☶⨩䚩筶ࢊණ⣁᳊尯/彡䫼畒伈森削㔞/缳漸⩧勎㓘癐⍖ᾐ䘽㼺䨶/粉掩㤡⪌⎏㆐罠Ꮨㆆ䤱ൎ堉儾
	at java.net.URI$Parser.fail(URI.java:2848)
	at java.net.URI$Parser.parseHostname(URI.java:3387)
	at java.net.URI$Parser.parseServer(URI.java:3236)
	at java.net.URI$Parser.parseAuthority(URI.java:3155)
	at java.net.URI$Parser.parseHierarchical(URI.java:3097)
	at java.net.URI$Parser.parse(URI.java:3053)
	at java.net.URI.<init>(URI.java:746)
	at org.apache.flink.core.fs.Path.initialize(Path.java:247)
	at org.apache.flink.core.fs.Path.<init>(Path.java:217)
	at org.apache.flink.runtime.state.filesystem.FsStorageLocationReferenceTest.randomPath(FsStorageLocationReferenceTest.java:88)
	at org.apache.flink.runtime.state.filesystem.FsStorageLocationReferenceTest.testEncodeAndDecode(FsStorageLocationReferenceTest.java:41)
{code}
log is here : https://travis-ci.org/apache/flink/jobs/409430886

 "	FLINK	Closed	2	1	3708	pull-request-available, test-stability
13373121	NPE exception happens if it throws exception in finishBundle during job shutdown	"Currently, if it throws exceptions in finishBundle during job shutdown, NPE exception may happen if time-based finish bundle is scheduled. It caused the actual exception isn't propagate. This makes users very difficult to trouble shot the problem.

See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-called-already-closed-and-NullPointerException-td42997.html for more details."	FLINK	Closed	3	1	3708	pull-request-available
13199775	Add overload support for user defined function 	"Currently overload is not supported in user defined function and given the following UDF
{code:java}
class Func21 extends ScalarFunction {
  def eval(p: People): String = {
    p.name
  }

  def eval(p: Student): String = {
    ""student#"" + p.name
  }
}

class People(val name: String)

class Student(name: String) extends People(name)

class GraduatedStudent(name: String) extends Student(name)
{code}
Queries such as the following will compile failed with error msg ""Found multiple 'eval' methods which match the signature.""

 
{code:java}
val udf = new Func21
val table = ...
table.select(udf(new GraduatedStudent(""test""))) {code}
That's because overload is not supported in user defined function currently. I think it will make sense to support overload following the java language specification in section [15.2|https://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.12].

 "	FLINK	Resolved	3	4	3708	pull-request-available
13255637	Add Python building blocks to make sure the basic functionality of Python ScalarFunction could work	"We need to add a few Python building blocks such as ScalarFunctionOperation, RowCoder, etc for Python ScalarFunction execution. ScalarFunctionOperation is subclass of Operation in Beam and RowCoder, etc are subclasses of Coder in Beam. These classes will be registered into the Beam’s portability framework to make sure they take effects.

This PR makes sure that a basic end to end Python UDF could be executed."	FLINK	Closed	3	7	3708	pull-request-available
13311719	Python NOTICE issues	"beam-runners-core-java / beam-vendor-bytebuddy bundled but not listed
protobuf-java-util listed but not bundled

The NOTICE file additionally lists various dependencies that are bundled by beam. While this is fine, the lack of separation makes verification difficult.

These would be the entries for directly bundled dependencies:
{code}
This project bundles the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)

- com.fasterxml.jackson.core:jackson-annotations:2.10.1
- com.fasterxml.jackson.core:jackson-core:2.10.1
- com.fasterxml.jackson.core:jackson-databind:2.10.1
- com.google.flatbuffers:flatbuffers-java:1.9.0
- io.netty:netty-buffer:4.1.44.Final
- io.netty:netty-common:4.1.44.Final
- joda-time:joda-time:2.5
- org.apache.arrow:arrow-format:0.16.0
- org.apache.arrow:arrow-memory:0.16.0
- org.apache.arrow:arrow-vector:0.16.0
- org.apache.beam:beam-model-fn-execution:2.19.0
- org.apache.beam:beam-model-job-management:2.19.0
- org.apache.beam:beam-model-pipeline:2.19.0
- org.apache.beam:beam-runners-core-construction-java:2.19.0
- org.apache.beam:beam-runners-java-fn-execution:2.19.0
- org.apache.beam:beam-sdks-java-core:2.19.0
- org.apache.beam:beam-sdks-java-fn-execution:2.19.0
- org.apache.beam:beam-vendor-grpc-1_21_0:0.1
- org.apache.beam:beam-vendor-guava-26_0-jre:0.1
- org.apache.beam:beam-vendor-sdks-java-extensions-protobuf:2.19.0

This project bundles the following dependencies under the BSD license.
See bundled license files for details

- net.sf.py4j:py4j:0.10.8.1
- com.google.protobuf:protobuf-java:3.7.1

This project bundles the following dependencies under the MIT license. (https://opensource.org/licenses/MIT)
See bundled license files for details.

- net.razorvine:pyrolite:4.13
{code}

These are the ones that are (supposedly) bundled by beam, which would need additional verification:
{code}
The bundled Apache Beam dependencies bundle the following dependencies under the Apache Software License 2.0 (http://www.apache.org/licenses/LICENSE-2.0.txt)

- com.google.api.grpc:proto-google-common-protos:1.12.0
- com.google.code.gson:gson:2.7
- com.google.guava:guava:26.0-jre
- io.grpc:grpc-auth:1.21.0
- io.grpc:grpc-core:1.21.0
- io.grpc:grpc-context:1.21.0
- io.grpc:grpc-netty:1.21.0
- io.grpc:grpc-protobuf:1.21.0
- io.grpc:grpc-stub:1.21.0
- io.grpc:grpc-testing:1.21.0
- io.netty:netty-buffer:4.1.34.Final
- io.netty:netty-codec:4.1.34.Final
- io.netty:netty-codec-http:4.1.34.Final
- io.netty:netty-codec-http2:4.1.34.Final
- io.netty:netty-codec-socks:4.1.34.Final
- io.netty:netty-common:4.1.34.Final
- io.netty:netty-handler:4.1.34.Final
- io.netty:netty-handler-proxy:4.1.34.Final
- io.netty:netty-resolver:4.1.34.Final
- io.netty:netty-transport:4.1.34.Final
- io.netty:netty-transport-native-epoll:4.1.34.Final
- io.netty:netty-transport-native-unix-common:4.1.34.Final
- io.netty:netty-tcnative-boringssl-static:2.0.22.Final
- io.opencensus:opencensus-api:0.21.0
- io.opencensus:opencensus-contrib-grpc-metrics:0.21.0

The bundled Apache Beam dependencies bundle the following dependencies under the BSD license.
See bundled license files for details

- com.google.protobuf:protobuf-java-util:3.7.1
- com.google.auth:google-auth-library-credentials:0.13.0
{code}"	FLINK	Closed	2	1	3708	pull-request-available
13258711	Optimize the execution plan for Python Calc when there is a condition	"As discussed in [https://github.com/apache/flink/pull/9748]:

""For the filter, we calculate these condition UDFs together with other UDFs and do the filter later. I think we can optimize it a bit, i.e., calculate the conditions first and then check whether to call the other UDFs. This can be easily achieved in the SplitRule."""	FLINK	Closed	3	7	3708	pull-request-available
13321255	Support JdbcCatalog in Python Table API	We should provide built-in support for JdbcCatalog in Python Table API.	FLINK	Closed	3	4	3708	pull-request-available
13315283	Move get_config implementation to TableEnvironment to eliminate the duplication	Currently, TableEnvironment.get_config is abstract and the implementations in the child classes BatchTableEnvironment/StreamTableEnvironment are duplicate. The implementation could be moved to TableEnvironment to eliminate the duplication.	FLINK	Closed	3	4	3708	pull-request-available
13343547	Fix license documentation mistakes in flink-python.jar	"Issues reported by Chesnay:

-The flink-python jar contains 2 license files in the root directory and another 2 in the META-INF directory. This should be reduced down to 1 under META-INF. I'm inclined to block the release on this because the root license is BSD.
- The flink-python jar appears to bundle lz4 (native libraries under win32/, linux/ and darwin/), but this is neither listed in the NOTICE nor do we have an explicit license file for it.

Other minor things that we should address in the future:
- opt/python contains some LICENSE files that should instead be placed under licenses/
- licenses/ contains a stray ""ASM"" file containing the ASM license. It's not a problem (because it is identical with our intended copy), but it indicates that something is amiss. This seems to originate from the flink-python jar, which bundles some beam stuff, which bundles bytebuddy, which bundles this license file. From what I can tell bytebuddy is not actually bundling ASM though; they just bundle the license for whatever reason. It is not listed as bundled in the flink-python NOTICE though, so I wouldn't block the release on it."	FLINK	Closed	1	1	3708	pull-request-available
13320304	"""compile_cron_scala212"" failed to compile"	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5060&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065

{code}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-avro-confluent-registry ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7
Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21
Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21
Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0
Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21
Found Banned Dependency: org.apache.flink:flink-table-api-java-bridge_2.11:jar:1.12-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-table-runtime-blink_2.11:jar:1.12-SNAPSHOT
Found Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21
Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0
Found Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.12-SNAPSHOT
Found Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1
Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6
Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.12-SNAPSHOT
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
{code}"	FLINK	Closed	1	1	3708	pull-request-available, test-stability
13344743	Publish Dockerfiles for release 1.12.0	"Publish the Dockerfiles for 1.12.0 to finalize the release process.
"	FLINK	Closed	3	4	3708	pull-request-available
13240919	lint-python.sh cannot find flake8	"Hi guys,

I tried to run tests for flink-python with {{./dev/lint-python.sh}} by following README. But it reported it couldn't find flake8, error as

{code:java}
./dev/lint-python.sh: line 490: /.../flink/flink-python/dev/.conda/bin/flake8: No such file or directory
{code}

I've tried {{./dev/lint-python.sh -f}}, also didn't work.

I suspect the reason may be that I already have an anaconda3 installed and it conflicts with the miniconda installed by flink-python somehow. I'm not fully sure about that.

If that's the reason, I think we need to try to resolve the conflict because anaconda is a pretty common package that developers install and use. We shouldn't require devs to uninstall their existing conda environment in order to develop flink-python and run its tests. It's better if flink-python can have a well isolated environment on machines.
"	FLINK	Closed	3	1	3708	pull-request-available
13309314	Test Pandas UDF support	Test that Pandas UDF functionality.	FLINK	Closed	1	4	3708	release-testing
13265007	Clean up the package of py4j	"Currently it contains a directory __MACOSX in the Py4j package. It's useless and should be removed.

 "	FLINK	Closed	3	4	3708	pull-request-available
13283144	Bump Beam to 2.19.0	"Currently PyFlink depends on Beam's portability framework for Python UDF execution. The current dependent version is 2.15.0. We should bump it to 2.19.0(the latest version) as it includes several critical features/fixes, e.g.
1) BEAM-7951: It allows to not serialize the window/timestamp/pane info between the Java operator and the Python worker which could definitely improve the performance a lot
2) BEAM-8935: It allows to fail fast if the Python worker start up failed. Currently it takes 2 minutes to detect the failure if the Python worker is started failed. 
3) BEAM-7948: It supports periodically flush the data between the Java operator and the Python worker. This feature is especially useful for streaming jobs and could improve the latency.
"	FLINK	Closed	3	4	3708	pull-request-available
13239023	Fix the bug that fix time quantifier can not be the last element of a pattern	"Currently, exception ""Greedy quantifiers are not allowed as the last element of a Pattern yet. Finish your pattern with either a simple variable or reluctant quantifier."" will be thrown for patterns such as ""a\{2}"". Actually greedy property is not meaningful for this kind of pattern."	FLINK	Closed	3	7	3708	pull-request-available
13199977	Add Map operator to Table API	"Add Map operator to Table API as described in [Google doc|https://docs.google.com/document/d/1tnpxg31EQz2-MEzSotwFzqatsB4rNLz0I-l_vPa5H4Q/edit#heading=h.q23rny2iglsr]

The usage：
{code:java}
val res = tab
   .map(fun: ScalarFunction)  // output has columns 'a, 'b, 'c
   .select('a, 'c)
{code}"	FLINK	Closed	3	7	3708	pull-request-available
13307455	Improve error reporting of Python CI tests	"While working on some changes, I noticed that the error reporting of the Python tests is sometimes not very detailed.

Example:
{code}
>       self._j_elasticsearch = gateway.jvm.Elasticsearch()
E       TypeError: 'JavaPackage' object is not callable
{code}

https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8081&view=logs&j=584fa981-f71a-5840-1c49-f800c954fe4b&t=532bf1f8-8c75-59c3-eaad-8c773769bc3a

The probably root cause is that required JAR files are missing. It would be nice if the tests would show the underlying (java)Flink error / Flink logs."	FLINK	Closed	3	4	3708	pull-request-available
13321471	table.to_pandas should handle retraction rows properly	Currently, the retraction rows are not handled properly and should be removed.	FLINK	Closed	3	1	3708	pull-request-available
13283243	Improve the Python API doc adding the version they are introduced	Currently it's not possible to know in which version a Python API is added. This information is very useful for users and it should be added.	FLINK	Closed	3	4	3708	pull-request-available
13140041	Optimize runtime support for distinct filter	"Possible optimizaitons:
1. Decouple distinct map and actual accumulator so that they can separately be created in codegen.
2. Reuse same distinct accumulator for filtering, e.g. `SELECT COUNT(DISTINCT(a)), SUM(DISTINCT(a))` should reuse the same distinct map."	FLINK	Closed	3	4	3708	pull-request-available
13290281	Support vectorized Python UDF in the batch mode of old planner	Currently, vectorized Python UDF is only supported in the batch/stream mode for the blink planner and stream mode for the old planner. The aim of this Jira is to add support in the batch mode for the old planner.	FLINK	Closed	3	7	3708	pull-request-available
13338839	"SQLClientKafkaITCase.testKafka times out while creating topic caused by ""PyFlink end-to-end test"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8967&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-11-04T12:00:36.0501135Z Nov 04 12:00:36 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 57.959 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
2020-11-04T12:00:36.0538557Z Nov 04 12:00:36 [ERROR] testKafka[0: kafka-version:2.4.1 kafka-sql-version:universal](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 57.949 s  <<< ERROR!
2020-11-04T12:00:36.0539781Z Nov 04 12:00:36 java.io.IOException: Process failed due to timeout.
2020-11-04T12:00:36.0540659Z Nov 04 12:00:36 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:130)
2020-11-04T12:00:36.0548817Z Nov 04 12:00:36 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)
2020-11-04T12:00:36.0549582Z Nov 04 12:00:36 	at org.apache.flink.tests.util.AutoClosableProcess.runBlocking(AutoClosableProcess.java:70)
2020-11-04T12:00:36.0550231Z Nov 04 12:00:36 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.createTopic(LocalStandaloneKafkaResource.java:261)
2020-11-04T12:00:36.0550909Z Nov 04 12:00:36 	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:136)
{code}
"	FLINK	Closed	2	1	3708	pull-request-available, test-stability
13328464	Improve the package structure of Python DataStream API 	Currently, the classes added for Python DataStream is located in *org.apache.flink.datastream* and there is already a package named *org.apache.flink.streaming**,* it would be great to move the these classes to package *org.apache.flink.streaming* to be consistent with the naming conversion of flink-*streaming-java*. Besides, the class name could also be optimized, e.g. DataStreamPythonReduceFunctionOperator -> PythonReduceOperator.	FLINK	Closed	3	4	3708	pull-request-available
13212476	"Deprecate ""new Table(TableEnvironment, String)"""	"A more detailed description can be found in [FLIP-32|https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions].

Once table is an interface we can easily replace the underlying implementation at any time. The constructor call prevents us from converting it into an interface."	FLINK	Closed	1	2	3708	pull-request-available
13213668	Deprecate ExternalCatalogTable.builder()	As discussed in FLINK-10755, we will deprecate ExternalCatalogTable.builder() in 1.8 and remove it in 1.9 to unblock the Planner interface. This method may be added back in the future(may be 1.9) after the catalog rework.	FLINK	Closed	3	4	3708	pull-request-available
13285269	Remove redundant metrics in PyFlink	We have recorded the metrics about how many elements it has processed in Python UDF. This kind of information is not necessary as there is also this kind of information in the Java operator. I have performed a simple test and find that removing it could improve the performance about 5% - 10%. Besides, as these metrics are still not exposed and it will be safe to remove it.	FLINK	Closed	3	4	3708	pull-request-available
13367126	Support batch mode in Python DataStream API for basic operations	"This ticket is dedicated to support batch mode for basic operations such as map/flatmap/filter, etc in Python DataStream API. 

For the other operations such as reduce, etc, we will support them in separate tickets."	FLINK	Closed	3	7	3708	pull-request-available
13354904	Python dependencies specified via CLI should not override the dependencies specified in configuration	Currently, the python dependencies specified via CLI will override the dependencies specified in configuration. For python.files and python.archives, it makes more sense to merge them together.	FLINK	Closed	3	1	3708	pull-request-available
13200861	Fix distinct aggregates for group window in Table API	"Currently distinct aggregates does not work on group window in Table API.
For the following query:
{code:java}
val table = util.addTable[(Int, Long, String)](
  ""MyTable"",
  'a, 'b, 'c, 'proctime.proctime, 'rowtime.rowtime)

val result = table
  .window(Tumble over 15.minute on 'rowtime as 'w)
  .groupBy('w)
  .select('a.count.distinct, 'a.sum)
{code}
The following exception will be thrown:
{code:java}
org.apache.flink.table.api.ValidationException: It's not allowed to use an aggregate function as input of another aggregate function

at org.apache.flink.table.plan.logical.LogicalNode.failValidation(LogicalNode.scala:156)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2$1$$anonfun$apply$3.apply(operators.scala:643)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2$1$$anonfun$apply$3.apply(operators.scala:641)
at org.apache.flink.table.plan.TreeNode.preOrderVisit(TreeNode.scala:82)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2$1.apply(operators.scala:641)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2$1.apply(operators.scala:640)
at scala.collection.immutable.List.foreach(List.scala:392)
at org.apache.flink.table.plan.logical.WindowAggregate.org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2(operators.scala:640)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2$4.apply(operators.scala:654)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2$4.apply(operators.scala:654)
at scala.collection.immutable.List.foreach(List.scala:392)
at org.apache.flink.table.plan.logical.WindowAggregate.org$apache$flink$table$plan$logical$WindowAggregate$$validateAggregateExpression$2(operators.scala:654)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$validate$12.apply(operators.scala:628)
at org.apache.flink.table.plan.logical.WindowAggregate$$anonfun$validate$12.apply(operators.scala:628)
at scala.collection.immutable.List.foreach(List.scala:392)
at org.apache.flink.table.plan.logical.WindowAggregate.validate(operators.scala:628)
at org.apache.flink.table.api.WindowGroupedTable.select(table.scala:1206)
at org.apache.flink.table.api.stream.table.DistinctAggregateTest.testDistinctAggregateOnTumbleWindow(DistinctAggregateTest.scala:60)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
at org.junit.rules.RunRules.evaluate(RunRules.java:20)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:51)
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:237)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}"	FLINK	Closed	3	1	3708	pull-request-available
13212492	Port and move TableSource and TableSink to flink-table-common	"A more detailed description can be found in [FLIP-32|https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions].

This step only unblockes the TableEnvironment interfaces task. Stream/BatchTableSouce/Sink remain in flink-table-api-java-bridge for now until they have been reworked."	FLINK	Closed	3	4	3708	pull-request-available
13309436	Correct the exception handling of the Python CompletableFuture	The implementation of method `exception` and `set_exception` are not correct. The purpose of the Python CompletableFuture is to hole the results from the Java CompletableFuture. We should expose the exception inside the Java CompletableFuture to users instead of allow Python users set the exception.	FLINK	Closed	3	1	3708	pull-request-available
13354568	Extract zip file dependencies before adding to PYTHONPATH	Not all zip files are importable in Python and so we should expand zip file dependencies and add the root directory to PYTHONPATH.	FLINK	Closed	3	1	3708	pull-request-available
13231698	Adds from_elements in TableEnvironment	"This is a convenient method to create a table from a collection of elements. It works as follows:
1) Serializes the python objects to a local file
2) Loads the file in Java and deserializes the data to Java objects"	FLINK	Closed	3	7	3708	pull-request-available
13246935	Supported java UDFs in python API	It's better to support java UDF in python API.	FLINK	Closed	2	7	3708	pull-request-available
13359492	pyflink DataTypes.DECIMAL is not available	"when i use DataTypes.DECIMAL in udaf
File ""/home/ubuntu/pyflenv/lib/python3.7/site-packages/pyflink/table/types.py"", line 2025, in _to_java_data_type
 _to_java_data_type(data_type._element_type))
 File ""/home/ubuntu/pyflenv/lib/python3.7/site-packages/pyflink/table/types.py"", line 1964, in _to_java_data_type
 j_data_type = JDataTypes.Decimal(data_type.precision, data_type.scale)
 File ""/home/ubuntu/pyflenv/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1516, in __getattr__
 ""\{0}.\{1} does not exist in the JVM"".format(self._fqn, name))
py4j.protocol.Py4JError: org.apache.flink.table.api.DataTypes.Decimal does not exist in the JVM

 

in pyflink\table\types.py

line 1963-1964

elif isinstance(data_type, DecimalType):
    j_data_type = JDataTypes.{color:#FF0000}Decimal{color}(data_type.precision, data_type.scale)

in java should be called ""DECIMAL"""	FLINK	Closed	2	1	3708	pull-request-available
13238745	Correct the package name for python API	"Currently the package name of flink APIs should cantians the language name, such as:
 * flink-java -> org.apache.flink.api.java
 * flink-scala -> org.apache.flink.api.scala

So I think we should follow the pattern of API package name and correct the current python API package name for `flink-python`. But for long-term goal, the flink API package name should be:

 * org.apache.flink.api.common.
 * org.apache.flink.api.common.python.
 * org.apache.flink.api.datastream.
 * org.apache.flink.api.datastream.scala.
 * org.apache.flink.api.datastream.python.
 * org.apache.flink.api.table.
 * org.apache.flink.api.table.scala.
 * org.apache.flink.api.table.python.

 So, in this JIRA, we should correct the package name from `org.apache.flink.python` -------> `org.apache.flink.api.table.python`

What do you think?"	FLINK	Closed	3	7	3708	pull-request-available
13252087	Support converting flink table to pandas dataframe	Pandas dataframe is the de facto standard tableau data format of python community. It would be nice to have the ability to convert flink table to pandas dataframe.	FLINK	Closed	3	7	3708	pull-request-available
13237897	Support user defined connectors/format	Currently, only built-in connectors such as FileSystem/Kafka/ES are supported and only built-in formats such as OldCSV/JSON/Avro/CSV/ are supported. We should also provide a convenient way for the connectors/formats that are not built-in supported.	FLINK	Closed	3	7	3708	pull-request-available
13087508	Refactor build-in agg(MaxWithRetractAccumulator and MinWithRetractAccumulator) using the DataView	Refactor build-in agg(MaxWithRetractAccumulator and MinWithRetractAccumulator) using the DataView.	FLINK	Closed	3	7	3708	pull-request-available
13284318	Support to use the Python UDF directly in the Python Table API	"Currently, a Python UDF has been registered before using in Python Table API, e.g.
{code}
t_env.register_function(""inc"", inc)
table.select(""inc(id)"") \
         .insert_into(""sink"")
{code}

It would be great if we could support to use Python UDF directly in the Python Table API, e.g.
{code}
table.select(inc(""id"")) \
         .insert_into(""sink"")
{code}"	FLINK	Closed	3	7	3708	pull-request-available
13339678	Add a dedicated stage to build the PyFlink wheel packges in azure-pipelines.yml	"Currently, we reuse the nightly pipeline to build the ""PyFlink wheel packages"" during release. There are some drawbacks as following:
- As there are many stages in the nightly pipeline and so it takes quite a long time to build the ""PyFlink wheel packages"" as there is no much resource available in the personal azure account. It takes me about 5 to 6 hours to build the ""PyFlink wheel packages"" during preparing the RC1 of 1.12.0.
- It's wired that there are so many errors [1] for the stages and it confuses release managers who are not familiar with process.

[1]  https://dev.azure.com/dianfu/Flink/_build/results?buildId=203&view=results"	FLINK	Closed	3	4	3708	pull-request-available
13240634	Fix the Python catalog test issue	"self = <pyflink.table.tests.test_catalog.CatalogTestBase testMethod=test_table_exists>

 

   def test_table_exists(self):

     self.catalog.create_database(self.db1, self.create_db(), False)

 

pyflink/table/tests/test_catalog.py:491: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

 

   @staticmethod

   def create_db():

       gateway = get_gateway()

     j_database = gateway.jvm.GenericCatalogDatabase(\{""k1"": ""v1""}, CatalogTestBase.test_comment)

E       TypeError: 'JavaPackage' object is not callable

 

 

pyflink/table/tests/test_catalog.py:78: TypeError"	FLINK	Closed	3	1	3708	pull-request-available
13204832	Improve the harness tests to use the code-generated operator	As a follow up work of FLINK-11074, we need to update all the harness test to use code-generated operator instead of hard-coded ones.	FLINK	Closed	3	4	3708	pull-request-available
13269827	PythonScalarFunctionOperator should be chained with upstream operators by default	Currently the default chaining strategy for PythonScalarFunctionOperator is not set and it's HEAD by default. We should set the default value as ALWAYS.	FLINK	Closed	3	1	3708	pull-request-available
13379633	Type mismatch thrown in DataStream.union if parameter is KeyedStream for Python DataStream API	See [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-DataStream-union-type-mismatch-td43855.html] for more details.	FLINK	Closed	3	1	3708	pull-request-available
13255630	Introduce PythonScalarFunctionRunner to handle the communication with Python worker for Python ScalarFunction execution	"PythonScalarFunctionRunner is responsible for Python ScalarFunction execution and it only handles the Python ScalarFunction execution and nothing else. So its logic should be very simple, forwarding an input element to Python worker and fetching the execution results back:
# Internally, it uses Apache Beam’s portability for Python UDF execution and this is transparent for the caller of PythonScalarFunctionRunner
# By default, each runner will startup a separate Python worker
# The Python worker can run in a docker, a separate process or even an non-managed external service.
# It has the ability to execute multiple Python ScalarFunctions
# It also supports chained Python ScalarFunctions"	FLINK	Closed	3	7	3708	pull-request-available
13366893	The method partition_by in Over doesn't work for expression dsl	"For the following example:
{code}

t = t_env.from_elements([(1, 1, ""Hello"")], ['a', 'b', 'c'])

result = t.over_window(
 Over.partition_by(t.c)
 .order_by(""a"")
 .preceding(expr.row_interval(2))
 .following(expr.CURRENT_ROW)
 .alias(""w""))
{code}

It will throw the following exception:

{code}
org.apache.flink.api.python.shaded.py4j.Py4JException: Method partitionBy([class org.apache.flink.table.api.ApiExpression]) does not existorg.apache.flink.api.python.shaded.py4j.Py4JException: Method partitionBy([class org.apache.flink.table.api.ApiExpression]) does not exist at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318) at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339) at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:276) at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79) at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748)
{code}"	FLINK	Closed	3	1	3708	pull-request-available
13291869	Support primitive data types for vectorized Python UDF	As the title described, the aim of this JIRA is to support the primitive types for vectorized Python UDF.	FLINK	Closed	3	4	3708	pull-request-available
13295439	Add fromValues in Python API	Expose APIs added in FLINK-16379 in python	FLINK	Closed	3	7	3708	pull-request-available
13369797	Revisit the return value of MapState.get when a key doesn't exist	Currently, it will thrown KeyError if the key doesn't exist for MapState in Python DataStream API. However, it returns null in the Java DataStream API. Maybe we should keep the behavior the same across Python DataStream API and Java DataStream API.	FLINK	Closed	3	7	3708	pull-request-available
13195752	Port external catalogs in Table API extension points to flink-table-common	"A more detailed description can be found in [FLIP-32|https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions].

This ticket is for porting external catalog related classes such as ExternalCatalog, ExternalCatalogTable, ExternalCatalogSchema, ExternalTableUtil, TableNotExistException, CatalogNotExistException.

Unblocks TableEnvironment interface task and catalog contribution."	FLINK	Closed	3	4	3708	pull-request-available
13335516	StreamExecutionEnvironment support new Source interface based on FLIP-27	StreamExecutionEnvironment currently supports new Source interface based on FLIP-27 in Java, but doesn't support in Python. PyFlink StreamExecutionEnvironment should add methods related to new Source interface like from_source etc.	FLINK	Closed	3	4	3708	pull-request-available
13297334	ERROR at teardown of TableConfigTests.test_get_set_decimal_context	"CI run: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7243&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9

{code}
2020-04-09T00:34:15.9084299Z ==================================== ERRORS ====================================
2020-04-09T00:34:15.9085728Z ______ ERROR at teardown of TableConfigTests.test_get_set_decimal_context ______
2020-04-09T00:34:15.9086216Z 
2020-04-09T00:34:15.9086725Z self = <contextlib._GeneratorContextManager object at 0x7f8d978989b0>
2020-04-09T00:34:15.9087144Z 
2020-04-09T00:34:15.9087457Z     def __enter__(self):
2020-04-09T00:34:15.9087787Z         try:
2020-04-09T00:34:15.9091929Z >           return next(self.gen)
2020-04-09T00:34:15.9092634Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9092863Z 
2020-04-09T00:34:15.9093134Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9093548Z __ ERROR at setup of TableConfigTests.test_get_set_idle_state_retention_time ___
2020-04-09T00:34:15.9093803Z 
2020-04-09T00:34:15.9094082Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3f3da0>
2020-04-09T00:34:15.9094313Z 
2020-04-09T00:34:15.9094502Z     def __enter__(self):
2020-04-09T00:34:15.9094862Z         try:
2020-04-09T00:34:15.9095088Z >           return next(self.gen)
2020-04-09T00:34:15.9095707Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9095913Z 
2020-04-09T00:34:15.9096203Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9096818Z _ ERROR at teardown of TableConfigTests.test_get_set_idle_state_retention_time _
2020-04-09T00:34:15.9100686Z 
2020-04-09T00:34:15.9101687Z self = <contextlib._GeneratorContextManager object at 0x7f8d978d83c8>
2020-04-09T00:34:15.9102005Z 
2020-04-09T00:34:15.9102193Z     def __enter__(self):
2020-04-09T00:34:15.9102415Z         try:
2020-04-09T00:34:15.9102741Z >           return next(self.gen)
2020-04-09T00:34:15.9103144Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9103367Z 
2020-04-09T00:34:15.9103786Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9104185Z ________ ERROR at setup of TableConfigTests.test_get_set_local_timezone ________
2020-04-09T00:34:15.9104999Z 
2020-04-09T00:34:15.9105287Z self = <contextlib._GeneratorContextManager object at 0x7f8d979345f8>
2020-04-09T00:34:15.9105531Z 
2020-04-09T00:34:15.9105707Z     def __enter__(self):
2020-04-09T00:34:15.9105924Z         try:
2020-04-09T00:34:15.9106138Z >           return next(self.gen)
2020-04-09T00:34:15.9106555Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9106858Z 
2020-04-09T00:34:15.9107159Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9107675Z ______ ERROR at teardown of TableConfigTests.test_get_set_local_timezone _______
2020-04-09T00:34:15.9107983Z 
2020-04-09T00:34:15.9108350Z self = <contextlib._GeneratorContextManager object at 0x7f8d981f8240>
2020-04-09T00:34:15.9108699Z 
2020-04-09T00:34:15.9108983Z     def __enter__(self):
2020-04-09T00:34:15.9109311Z         try:
2020-04-09T00:34:15.9109566Z >           return next(self.gen)
2020-04-09T00:34:15.9109872Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9110082Z 
2020-04-09T00:34:15.9110349Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9111098Z __ ERROR at setup of TableConfigTests.test_get_set_max_generated_code_length ___
2020-04-09T00:34:15.9111479Z 
2020-04-09T00:34:15.9111740Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3380f0>
2020-04-09T00:34:15.9112010Z 
2020-04-09T00:34:15.9112297Z     def __enter__(self):
2020-04-09T00:34:15.9112571Z         try:
2020-04-09T00:34:15.9112803Z >           return next(self.gen)
2020-04-09T00:34:15.9113114Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9113353Z 
2020-04-09T00:34:15.9113737Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9114282Z _ ERROR at teardown of TableConfigTests.test_get_set_max_generated_code_length _
2020-04-09T00:34:15.9114652Z 
2020-04-09T00:34:15.9114929Z self = <contextlib._GeneratorContextManager object at 0x7f8d9783b550>
2020-04-09T00:34:15.9115169Z 
2020-04-09T00:34:15.9115460Z     def __enter__(self):
2020-04-09T00:34:15.9115756Z         try:
2020-04-09T00:34:15.9115989Z >           return next(self.gen)
2020-04-09T00:34:15.9116279Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9116579Z 
2020-04-09T00:34:15.9116944Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9117387Z __________ ERROR at setup of TableConfigTests.test_get_set_null_check __________
2020-04-09T00:34:15.9117640Z 
2020-04-09T00:34:15.9117908Z self = <contextlib._GeneratorContextManager object at 0x7f8d9823a390>
2020-04-09T00:34:15.9118258Z 
2020-04-09T00:34:15.9118519Z     def __enter__(self):
2020-04-09T00:34:15.9118827Z         try:
2020-04-09T00:34:15.9119083Z >           return next(self.gen)
2020-04-09T00:34:15.9119531Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9119819Z 
2020-04-09T00:34:15.9120164Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9120827Z ________ ERROR at teardown of TableConfigTests.test_get_set_null_check _________
2020-04-09T00:34:15.9121505Z 
2020-04-09T00:34:15.9121923Z self = <contextlib._GeneratorContextManager object at 0x7f8d7a6ba978>
2020-04-09T00:34:15.9122165Z 
2020-04-09T00:34:15.9122343Z     def __enter__(self):
2020-04-09T00:34:15.9122554Z         try:
2020-04-09T00:34:15.9122787Z >           return next(self.gen)
2020-04-09T00:34:15.9123242Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9123458Z 
2020-04-09T00:34:15.9123726Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9124304Z _________ ERROR at setup of TableConfigTests.test_get_set_sql_dialect __________
2020-04-09T00:34:15.9124642Z 
2020-04-09T00:34:15.9124899Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3739e8>
2020-04-09T00:34:15.9125262Z 
2020-04-09T00:34:15.9125514Z     def __enter__(self):
2020-04-09T00:34:15.9125854Z         try:
2020-04-09T00:34:15.9126071Z >           return next(self.gen)
2020-04-09T00:34:15.9126572Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9126890Z 
2020-04-09T00:34:15.9127300Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9127871Z ________ ERROR at teardown of TableConfigTests.test_get_set_sql_dialect ________
2020-04-09T00:34:15.9128278Z 
2020-04-09T00:34:15.9128544Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c365748>
2020-04-09T00:34:15.9128851Z 
2020-04-09T00:34:15.9129126Z     def __enter__(self):
2020-04-09T00:34:15.9129350Z         try:
2020-04-09T00:34:15.9129595Z >           return next(self.gen)
2020-04-09T00:34:15.9130021Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9130228Z 
2020-04-09T00:34:15.9130741Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9131424Z _______ ERROR at setup of TableConfigCompletenessTests.test_completeness _______
2020-04-09T00:34:15.9131820Z 
2020-04-09T00:34:15.9132103Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3f3908>
2020-04-09T00:34:15.9132486Z 
2020-04-09T00:34:15.9132681Z     def __enter__(self):
2020-04-09T00:34:15.9132892Z         try:
2020-04-09T00:34:15.9133104Z >           return next(self.gen)
2020-04-09T00:34:15.9133409Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9133617Z 
2020-04-09T00:34:15.9134006Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9134674Z _____ ERROR at teardown of TableConfigCompletenessTests.test_completeness ______
2020-04-09T00:34:15.9135059Z 
2020-04-09T00:34:15.9135426Z self = <contextlib._GeneratorContextManager object at 0x7f8d978d8b00>
2020-04-09T00:34:15.9135726Z 
2020-04-09T00:34:15.9136010Z     def __enter__(self):
2020-04-09T00:34:15.9136212Z         try:
2020-04-09T00:34:15.9136440Z >           return next(self.gen)
2020-04-09T00:34:15.9136871Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9137101Z 
2020-04-09T00:34:15.9137350Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9137907Z _ ERROR at setup of StreamTableEnvironmentTests.test_create_table_environment __
2020-04-09T00:34:15.9138167Z 
2020-04-09T00:34:15.9138570Z self = <contextlib._GeneratorContextManager object at 0x7f8d97907630>
2020-04-09T00:34:15.9138852Z 
2020-04-09T00:34:15.9139045Z     def __enter__(self):
2020-04-09T00:34:15.9139244Z         try:
2020-04-09T00:34:15.9139611Z >           return next(self.gen)
2020-04-09T00:34:15.9140012Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9140236Z 
2020-04-09T00:34:15.9140665Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9141269Z _ ERROR at teardown of StreamTableEnvironmentTests.test_create_table_environment _
2020-04-09T00:34:15.9141527Z 
2020-04-09T00:34:15.9141801Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c63ab70>
2020-04-09T00:34:15.9142029Z 
2020-04-09T00:34:15.9142291Z     def __enter__(self):
2020-04-09T00:34:15.9142610Z         try:
2020-04-09T00:34:15.9142973Z >           return next(self.gen)
2020-04-09T00:34:15.9143456Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9143677Z 
2020-04-09T00:34:15.9143923Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9144502Z _ ERROR at setup of StreamTableEnvironmentTests.test_create_table_environment_with_blink_planner _
2020-04-09T00:34:15.9145034Z 
2020-04-09T00:34:15.9145370Z self = <contextlib._GeneratorContextManager object at 0x7f8d97898828>
2020-04-09T00:34:15.9145662Z 
2020-04-09T00:34:15.9145892Z     def __enter__(self):
2020-04-09T00:34:15.9146159Z         try:
2020-04-09T00:34:15.9146495Z >           return next(self.gen)
2020-04-09T00:34:15.9146948Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9147256Z 
2020-04-09T00:34:15.9147684Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9148149Z _ ERROR at teardown of StreamTableEnvironmentTests.test_create_table_environment_with_blink_planner _
2020-04-09T00:34:15.9148649Z 
2020-04-09T00:34:15.9149059Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c5fada0>
2020-04-09T00:34:15.9149316Z 
2020-04-09T00:34:15.9149536Z     def __enter__(self):
2020-04-09T00:34:15.9149834Z         try:
2020-04-09T00:34:15.9150154Z >           return next(self.gen)
2020-04-09T00:34:15.9150714Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9151092Z 
2020-04-09T00:34:15.9151384Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9151980Z __________ ERROR at setup of StreamTableEnvironmentTests.test_explain __________
2020-04-09T00:34:15.9152334Z 
2020-04-09T00:34:15.9152722Z self = <contextlib._GeneratorContextManager object at 0x7f8d9783b2e8>
2020-04-09T00:34:15.9153030Z 
2020-04-09T00:34:15.9153212Z     def __enter__(self):
2020-04-09T00:34:15.9153430Z         try:
2020-04-09T00:34:15.9153660Z >           return next(self.gen)
2020-04-09T00:34:15.9154078Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9154293Z 
2020-04-09T00:34:15.9154803Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9155329Z ________ ERROR at teardown of StreamTableEnvironmentTests.test_explain _________
2020-04-09T00:34:15.9155614Z 
2020-04-09T00:34:15.9156000Z self = <contextlib._GeneratorContextManager object at 0x7f8d98192cc0>
2020-04-09T00:34:15.9156358Z 
2020-04-09T00:34:15.9156575Z     def __enter__(self):
2020-04-09T00:34:15.9156787Z         try:
2020-04-09T00:34:15.9157013Z >           return next(self.gen)
2020-04-09T00:34:15.9157317Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9157521Z 
2020-04-09T00:34:15.9157891Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9159267Z ___ ERROR at setup of StreamTableEnvironmentTests.test_explain_with_extended ___
2020-04-09T00:34:15.9159634Z 
2020-04-09T00:34:15.9159931Z self = <contextlib._GeneratorContextManager object at 0x7f8d977e1278>
2020-04-09T00:34:15.9160247Z 
2020-04-09T00:34:15.9160605Z     def __enter__(self):
2020-04-09T00:34:15.9160835Z         try:
2020-04-09T00:34:15.9161155Z >           return next(self.gen)
2020-04-09T00:34:15.9161505Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9161758Z 
2020-04-09T00:34:15.9162024Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
2020-04-09T00:34:15.9163041Z _ ERROR at teardown of StreamTableEnvironmentTests.test_explain_with_extended __
2020-04-09T00:34:15.9163619Z 
2020-04-09T00:34:15.9164120Z self = <contextlib._GeneratorContextManager object at 0x7f8d9c3f3e80>
2020-04-09T00:34:15.9164634Z 
2020-04-09T00:34:15.9164967Z     def __enter__(self):
2020-04-09T00:34:15.9165322Z         try:
2020-04-09T00:34:15.9165716Z >           return next(self.gen)
2020-04-09T00:34:15.9166262Z E           OSError: [Errno 9] Bad file descriptor
2020-04-09T00:34:15.9166640Z 
2020-04-09T00:34:15.9167091Z dev/.conda/envs/3.5/lib/python3.5/contextlib.py:59: OSError
{code}"	FLINK	Closed	2	1	3708	pull-request-available, test-stability
13239800	Improves the performance of Python Table API test cases	Most of the Python Table API test cases can be unit test instead of integration test. This can shorten the test time of Python Table API.	FLINK	Closed	3	7	3708	pull-request-available
13307879	ArrowSourceFunctionTestBase.testParallelProcessing is instable	"It failed on cron JDK 11 tests with following error:
{code}
2020-05-27T21:37:21.2840832Z [ERROR] testParallelProcessing(org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTest)  Time elapsed: 0.433 s  <<< FAILURE!
2020-05-27T21:37:21.2841551Z java.lang.AssertionError: expected:<4> but was:<5>
2020-05-27T21:37:21.2842025Z 	at org.junit.Assert.fail(Assert.java:88)
2020-05-27T21:37:21.2842470Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-05-27T21:37:21.2842994Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-05-27T21:37:21.2843484Z 	at org.junit.Assert.assertEquals(Assert.java:631)
2020-05-27T21:37:21.2844779Z 	at org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTestBase.checkElementsEquals(ArrowSourceFunctionTestBase.java:243)
2020-05-27T21:37:21.2845873Z 	at org.apache.flink.table.runtime.arrow.sources.ArrowSourceFunctionTestBase.testParallelProcessing(ArrowSourceFunctionTestBase.java:233)
{code}
instance: [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/2304/logs/535]"	FLINK	Closed	3	1	3708	pull-request-available, test-stability
13386203	ZipUtils doesn't handle properly for softlinks inside the zip file	When extracting softlinks inside the zip file, we should restore the softlinks. However, currently we just create a file which content is the target of the softlink.	FLINK	Closed	3	1	3708	pull-request-available
13238449	Add support to run a Python job-specific cluster on Kubernetes	As discussed in FLINK-12541, we need to support to run a Python job-specific cluster on Kubernetes. To support this, we need to improve the job specific docker image build scripts to support Python Table API jobs.	FLINK	Closed	3	7	3708	pull-request-available
13290282	Add documentation for vectorized Python UDF	As the title described, the aim of this JIRA is to add documentation for vectorized Python UDF.	FLINK	Closed	3	7	3708	pull-request-available
13084108	Support the basic functionality of MATCH_RECOGNIZE	"In this JIRA, we will support the basic functionality of {{MATCH_RECOGNIZE}} in Flink SQL API which includes the support of syntax {{MEASURES}}, {{PATTERN}} and {{DEFINE}}. This would allow users write basic cep use cases with SQL like the following example:
{code}
SELECT T.aid, T.bid, T.cid
FROM MyTable
MATCH_RECOGNIZE (
  MEASURES
    A.id AS aid,
    B.id AS bid,
    C.id AS cid
  PATTERN (A B C)
  DEFINE
    A AS A.name = 'a',
    B AS B.name = 'b',
    C AS C.name = 'c'
) AS T
{code}"	FLINK	Resolved	3	7	3708	pull-request-available
13291017	Restructure Python Table API documentation	"Python Table API documentation is currently spread across a number of pages and it's difficult for a user to find out all the documentations available. Besides, there are also a few documentations which deserves specific page to describe the functionality, such as the environment setup, the Python dependency management, the vectorized Python UDF, etc. 

We want to improve the documentation by adding an item under the Table API as ""Python Table API"" and the structure of ""Python Table API"" is as following:
 * Python Table API
 ** Installation
 ** User-defined Functions
 ** Vectorized Python UDF
 ** Dependency Management
 ** Metrics & Logging
 ** Configuration

 "	FLINK	Closed	3	4	3708	pull-request-available
13237860	Improves the Python word_count example to use the descriptor API	"The aim of this ticket is to improve the word_count example:
1. Uses the from_element API to create a source table
2. Uses the descriptor API to register the sink"	FLINK	Closed	3	7	3708	pull-request-available
13256289	Pyflink building failure in master and 1.9.0 version	"ATTENTION: This is a issue about building pyflink, not development.

During we build pyflink...

After we have built flink from flink source code, a folder named ""target"" is generated.

Then, following the document description, ""cd flink-python; python3 setup.py sdist bdist_wheel"", error happens.

Root cause: in the setup.py file, line 75, ""FLINK_HOME = os.path.abspath(""../build-target"")"", the program can't found folder ""build-target"", however, the building of flink generated a folder named ""target"". So error happens in this way...

 

The right way:

in ../flink-python/setup.py line 75, modify code as following:

FLINK_HOME = os.path.abspath(""../target"")"	FLINK	Closed	1	1	3708	beginner, build, pull-request-available
13322696	StreamTask.invoke should catch Throwable instead of Exception	"In StreamTask.invoke, we should catch Throwable. Otherwise, cleanUpInvoke() will not be called if Error is thrown:

{code}
	@Override
	public final void invoke() throws Exception {
		try {
			beforeInvoke();

			// final check to exit early before starting to run
			if (canceled) {
				throw new CancelTaskException();
			}

			// let the task do its work
			runMailboxLoop();

			// if this left the run() method cleanly despite the fact that this was canceled,
			// make sure the ""clean shutdown"" is not attempted
			if (canceled) {
				throw new CancelTaskException();
			}

			afterInvoke();
		}
		catch (Exception invokeException) {
			failing = !canceled;
			try {
				cleanUpInvoke();
			}
			// TODO: investigate why Throwable instead of Exception is used here.
			catch (Throwable cleanUpException) {
				Throwable throwable = ExceptionUtils.firstOrSuppressed(cleanUpException, invokeException);
				throw (throwable instanceof Exception ? (Exception) throwable : new Exception(throwable));
			}
			throw invokeException;
		}
		cleanUpInvoke();
	}
{code}"	FLINK	Closed	3	1	3708	pull-request-available
13335108	flink-streaming-java module failed to compile	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7474&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-streaming-java_2.11: Compilation failure: Compilation failure:
[ERROR] /__w/1/s/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/StreamMultipleInputProcessorFactory.java:[138,45] method getManagedMemoryFractionOperatorUseCaseOfSlot in class org.apache.flink.streaming.api.graph.StreamConfig cannot be applied to given types;
[ERROR] required: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration,java.lang.ClassLoader
[ERROR] found: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] /__w/1/s/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/StreamTwoInputProcessorFactory.java:[109,45] method getManagedMemoryFractionOperatorUseCaseOfSlot in class org.apache.flink.streaming.api.graph.StreamConfig cannot be applied to given types;
[ERROR] required: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration,java.lang.ClassLoader
[ERROR] found: org.apache.flink.core.memory.ManagedMemoryUseCase,org.apache.flink.configuration.Configuration
[ERROR] reason: actual and formal argument lists differ in length
[ERROR] -> [Help 1]
{code}"	FLINK	Closed	1	1	3708	test-stability
13317354	Add limit method in the Python Table API	Table.limit was introduced in the Java Table API in FLINK-18569. It would be great if we can also support it in the Python Table API.	FLINK	Closed	3	4	3708	pull-request-available
13372776	Perform early check to ensure that the length of the result is the same as the input for Pandas UDF	"For Pandas UDF, the input type for each input argument is Pandas.Series and the result type is also of type Pandas.Series. Besides, the length of the result should be the same as the inputs. If this is not the case, currently the behavior is unclear. We should perform early check for this and provide a clear error message.

See http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-Vectorized-UDF-throws-NullPointerException-td42952.html and http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/PyFlink-called-already-closed-and-NullPointerException-td42997.html for more details."	FLINK	Closed	3	4	3708	pull-request-available
13237401	Adds Python Table API tutorial	We should add a tutorial for Python Table API in the docs to help beginners of Python Table API to get a basic knowledge of how to create a simple Python Table API job.	FLINK	Closed	3	7	3708	pull-request-available
13287366	BaseRowArrowReaderWriterTest/RowArrowReaderWriterTest sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available	"https://travis-ci.org/apache/flink/jobs/654409364

{code}
18:17:45.003 [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.924 s - in org.apache.flink.table.runtime.arrow.ArrowUtilsTest
18:17:45.019 [INFO] Running org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest
sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
	at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399)
	at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257)
	at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247)
	at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248)
	at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228)
	at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)
	at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
18:17:45.128 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.102 s <<< FAILURE! - in org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest
18:17:45.128 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.BaseRowArrowReaderWriterTest)  Time elapsed: 0.097 s  <<< FAILURE!
java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available

18:17:45.143 [INFO] Running org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest
sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
	at io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:399)
	at io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:257)
	at io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:247)
	at io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:248)
	at org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:228)
	at org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:242)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)
	at org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)
	at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:68)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
18:17:45.209 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.057 s <<< FAILURE! - in org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest
18:17:45.209 [ERROR] testBasicFunctionality(org.apache.flink.table.runtime.arrow.RowArrowReaderWriterTest)  Time elapsed: 0.056 s  <<< FAILURE!
java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
{code}"	FLINK	Closed	3	1	3708	pull-request-available, test-stability
13385574	Raise an exception if types other than PickledBytesTypeInfo are specified for state descriptor 	Although it's expected that users could use any type to define the state descriptor, however, only PickledBytesTypeInfo is actually supported for the time being. Meaningful exception should be thrown if types other than PickledBytesTypeInfo are used.	FLINK	Closed	3	4	3708	pull-request-available
13297843	Python UDF doesn't work when the input column is from composite field	"For the following job:
{code}

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import BatchTableEnvironment, StreamTableEnvironment, EnvironmentSettings, CsvTableSink
from pyflink.table.descriptors import Schema, Kafka, Json
from pyflink.table import DataTypes
from pyflink.table.udf import ScalarFunction, udf
import os

@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING()],
 result_type=DataTypes.STRING())
def get_host_ip(source, qr, sip, dip):
    if source == ""NGAF"" and qr == '1':
        return dip
    return sip

@udf(input_types=[DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING()],
 result_type=DataTypes.STRING())
def get_dns_server_ip(source, qr, sip, dip):
    if source == ""NGAF"" and qr == '1':
        return sip
    return dip

def test_case():
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(1)
    t_env = StreamTableEnvironment.create(env)

     from pyflink.table import Row
   table = t_env.from_elements(
      [(""DNS"", Row(source=""source"", devid=""devid"", sip=""sip"", dip=""dip"", qr=""qr"", queries=""queries"", answers=""answers"", qtypes=""qtypes"", atypes=""atypes"", rcode=""rcode"", ts=""ts"",))],
    DataTypes.ROW([DataTypes.FIELD(""stype"", DataTypes.STRING()),
 DataTypes.FIELD(""data"",
 DataTypes.ROW([DataTypes.FIELD('source', DataTypes.STRING()),
 DataTypes.FIELD(""devid"", DataTypes.STRING()),
 DataTypes.FIELD('sip', DataTypes.STRING()),
 DataTypes.FIELD('dip', DataTypes.STRING()),
 DataTypes.FIELD(""qr"", DataTypes.STRING()),
 DataTypes.FIELD(""queries"", DataTypes.STRING()),
 DataTypes.FIELD(""answers"", DataTypes.STRING()),
 DataTypes.FIELD(""qtypes"", DataTypes.STRING()),
 DataTypes.FIELD(""atypes"", DataTypes.STRING()),
 DataTypes.FIELD(""rcode"", DataTypes.STRING()),
 DataTypes.FIELD(""ts"", DataTypes.STRING())]))
 ]
 ))

 result_file = ""/tmp/test.csv""
 if os.path.exists(result_file):
 os.remove(result_file)

 t_env.register_table_sink(""Results"",
 CsvTableSink(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n'],
 [DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING(),
 DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING(), DataTypes.STRING(),
 DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()],
 ""/tmp/test.csv""))

 t_env.register_function(""get_host_ip"", get_host_ip)
 t_env.register_function(""get_dns_server_ip"", get_dns_server_ip)

 t_env.register_table(""source"", table)
 standard_table = t_env.sql_query(""select data.*, stype as dns_type from source"")\
 .where(""dns_type.in('DNSFULL', 'DNS', 'DNSFULL_FROM_LOG', 'DNS_FROM_LOG')"")
 t_env.register_table(""standard_table"", standard_table)

 final_table = t_env.sql_query(""SELECT *, get_host_ip(source, qr, sip, dip) as host_ip,""
 ""get_dns_server_ip(source, qr, sip, dip) as dns_server_ip FROM standard_table"")

 final_table.insert_into(""Results"")

 t_env.execute(""test"")


if __name__ == '__main__':
 test_case()
{code}

The plan is as following which is not correct:
{code}
 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: KafkaTableSource(type, data) -> Map -> where: (IN(type, _UTF-16LE'DNSFULL', _UTF-16LE'DNS', _UTF-16LE'DNSFULL_FROM_LOG', _UTF-16LE'DNS_FROM_LOG')), select: (data, type) -> select: (type, get_host_ip(type.source, type.qr, type.sip, type.dip) AS f0, get_dns_server_ip(type.source, type.qr, type.sip, type.dip) AS f1) -> select: (f0.source AS source, f0.devid AS devid, f0.sip AS sip, f0.dip AS dip, f0.qr AS qr, f0.queries AS queries, f0.answers AS answers, f0.qtypes AS qtypes, f0.atypes AS atypes, f0.rcode AS rcode, f0.ts AS ts, type AS dns_type, f0 AS host_ip, f1 AS dns_server_ip) -> to: Row -> Sink: KafkaTableSink(source, devid, sip, dip, qr, queries, answers, qtypes, atypes, rcode, ts, dns_type, host_ip, dns_server_ip) (1/4) (8d064ab137866a2a9040392a87bcc59d) switched from RUNNING to FAILED.
{code}"	FLINK	Closed	1	1	3708	pull-request-available
13023985	Error result of compareSerialized in RowComparator class	"RowSerializer will write null mask for all fields in a row before serialize row data to  DataOutputView. 

{code:title=RowSerializer.scala|borderStyle=solid}
override def serialize(value: Row, target: DataOutputView) {
    val len = fieldSerializers.length

    if (value.productArity != len) {
      throw new RuntimeException(""Row arity of value does not match serializers."")
    }

    // write a null mask
    writeNullMask(len, value, target)

......
}

{code}

RowComparator will deserialize a row data from DataInputView when call compareSerialized method. However, the first parameter value of readIntoNullMask method is wrong, which should be the count of all fields, rather than the length of serializers (to deserialize the first n fields for comparison).

{code:title=RowComparator.scala|borderStyle=solid}
override def compareSerialized(firstSource: DataInputView, secondSource: DataInputView): Int = {
    val len = serializers.length
    val keyLen = keyPositions.length

    readIntoNullMask(len, firstSource, nullMask1)
    readIntoNullMask(len, secondSource, nullMask2)
......
}
{code}"	FLINK	Closed	3	1	4624	pull-request-available
13290264	SelectivityEstimatorTest logs LinkageErrors	"This is the test run https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6038&view=logs&j=d47ab8d2-10c7-5d9e-8178-ef06a797a0d8&t=9a1abf5f-7cf4-58c3-bb2a-282a64aebb1f

Log output
{code}
2020-03-07T00:35:20.1270791Z [INFO] Running org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest
2020-03-07T00:35:21.6473057Z [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.408 s - in org.apache.flink.table.planner.plan.utils.FlinkRexUtilTest
2020-03-07T00:35:21.6541713Z [INFO] Running org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCostTest
2020-03-07T00:35:21.7294613Z [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.073 s - in org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCostTest
2020-03-07T00:35:21.7309958Z [INFO] Running org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest
2020-03-07T00:35:23.7443246Z ScriptEngineManager providers.next(): javax.script.ScriptEngineFactory: Provider jdk.nashorn.api.scripting.NashornScriptEngineFactory not a subtype
2020-03-07T00:35:23.8260013Z 2020-03-07 00:35:23,819 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
2020-03-07T00:35:23.8262329Z 	at java.lang.ClassLoader.defineClass1(Native Method)
2020-03-07T00:35:23.8263241Z 	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
2020-03-07T00:35:23.8264629Z 	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
2020-03-07T00:35:23.8266241Z 	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
2020-03-07T00:35:23.8267808Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
2020-03-07T00:35:23.8269485Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
2020-03-07T00:35:23.8270900Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
2020-03-07T00:35:23.8272000Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:23.8273779Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:23.8275087Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
2020-03-07T00:35:23.8276515Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
2020-03-07T00:35:23.8278036Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
2020-03-07T00:35:23.8279741Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-03-07T00:35:23.8281190Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-03-07T00:35:23.8282440Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-03-07T00:35:23.8283717Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-03-07T00:35:23.8285186Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-03-07T00:35:23.8286575Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-03-07T00:35:23.8287933Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-03-07T00:35:23.8289393Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-03-07T00:35:23.8290816Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-03-07T00:35:23.8292179Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-03-07T00:35:23.8293304Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-03-07T00:35:23.8294485Z 	at org.apache.calcite.util.trace.CalciteTrace.getParserTracer(CalciteTrace.java:111)
2020-03-07T00:35:23.8295692Z 	at org.apache.calcite.util.trace.CalciteTrace.<clinit>(CalciteTrace.java:56)
2020-03-07T00:35:23.8296925Z 	at org.apache.calcite.sql.parser.SqlParserUtil.<clinit>(SqlParserUtil.java:73)
2020-03-07T00:35:23.8298190Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:86)
2020-03-07T00:35:23.8299369Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:106)
2020-03-07T00:35:23.8300623Z 	at org.apache.calcite.sql.SqlCollation.<clinit>(SqlCollation.java:36)
2020-03-07T00:35:23.8301809Z 	at org.apache.calcite.sql.type.SqlTypeUtil.addCharsetAndCollation(SqlTypeUtil.java:1109)
2020-03-07T00:35:23.8303177Z 	at org.apache.calcite.sql.type.SqlTypeFactoryImpl.createSqlType(SqlTypeFactoryImpl.java:70)
2020-03-07T00:35:23.8304600Z 	at org.apache.flink.table.planner.calcite.FlinkTypeFactory.createSqlType(FlinkTypeFactory.scala:245)
2020-03-07T00:35:23.8305836Z 	at org.apache.calcite.rex.RexBuilder.<init>(RexBuilder.java:120)
2020-03-07T00:35:23.8307197Z 	at org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest.<init>(SelectivityEstimatorTest.scala:69)
2020-03-07T00:35:23.8308550Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-03-07T00:35:23.8309967Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-03-07T00:35:23.8311476Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-03-07T00:35:23.8312736Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-03-07T00:35:23.8314266Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
2020-03-07T00:35:23.8316099Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
2020-03-07T00:35:23.8318030Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
2020-03-07T00:35:23.8320066Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
2020-03-07T00:35:23.8321832Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
2020-03-07T00:35:23.8323341Z 	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
2020-03-07T00:35:23.8324508Z 	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
2020-03-07T00:35:23.8326007Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
2020-03-07T00:35:23.8327909Z 	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
2020-03-07T00:35:23.8329637Z 	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
2020-03-07T00:35:23.8331221Z 	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
2020-03-07T00:35:23.8332426Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-07T00:35:23.8333876Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-07T00:35:23.8335252Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-07T00:35:23.8336523Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-07T00:35:23.8337998Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-07T00:35:23.8339456Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-07T00:35:23.8340858Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-07T00:35:23.8342096Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-07T00:35:23.8342779Z 
2020-03-07T00:35:24.9610362Z ScriptEngineManager providers.next(): javax.script.ScriptEngineFactory: Provider jdk.nashorn.api.scripting.NashornScriptEngineFactory not a subtype
2020-03-07T00:35:25.0466199Z 2020-03-07 00:35:25,039 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
2020-03-07T00:35:25.0469113Z 	at java.lang.ClassLoader.defineClass1(Native Method)
2020-03-07T00:35:25.0470660Z 	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
2020-03-07T00:35:25.0472271Z 	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
2020-03-07T00:35:25.0474050Z 	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
2020-03-07T00:35:25.0475860Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
2020-03-07T00:35:25.0477817Z 	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
2020-03-07T00:35:25.0479429Z 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
2020-03-07T00:35:25.0480937Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:25.0482406Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:25.0484057Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
2020-03-07T00:35:25.0485643Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
2020-03-07T00:35:25.0487266Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
2020-03-07T00:35:25.0488896Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-03-07T00:35:25.0490626Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-03-07T00:35:25.0492120Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-03-07T00:35:25.0493648Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-03-07T00:35:25.0495381Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-03-07T00:35:25.0496936Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-03-07T00:35:25.0498835Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-03-07T00:35:25.0500729Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-03-07T00:35:25.0502293Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-03-07T00:35:25.0504005Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-03-07T00:35:25.0505416Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-03-07T00:35:25.0506942Z 	at org.apache.calcite.util.trace.CalciteTrace.getParserTracer(CalciteTrace.java:111)
2020-03-07T00:35:25.0508534Z 	at org.apache.calcite.util.trace.CalciteTrace.<clinit>(CalciteTrace.java:56)
2020-03-07T00:35:25.0510219Z 	at org.apache.calcite.sql.parser.SqlParserUtil.<clinit>(SqlParserUtil.java:73)
2020-03-07T00:35:25.0511651Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:86)
2020-03-07T00:35:25.0512954Z 	at org.apache.calcite.sql.SqlCollation.<init>(SqlCollation.java:106)
2020-03-07T00:35:25.0514306Z 	at org.apache.calcite.sql.SqlCollation.<clinit>(SqlCollation.java:36)
2020-03-07T00:35:25.0515783Z 	at org.apache.calcite.sql.type.SqlTypeUtil.addCharsetAndCollation(SqlTypeUtil.java:1109)
2020-03-07T00:35:25.0517340Z 	at org.apache.calcite.sql.type.SqlTypeFactoryImpl.createSqlType(SqlTypeFactoryImpl.java:70)
2020-03-07T00:35:25.0519244Z 	at org.apache.flink.table.planner.calcite.FlinkTypeFactory.createSqlType(FlinkTypeFactory.scala:245)
2020-03-07T00:35:25.0520861Z 	at org.apache.calcite.rex.RexBuilder.<init>(RexBuilder.java:120)
2020-03-07T00:35:25.0522555Z 	at org.apache.flink.table.planner.plan.metadata.AggCallSelectivityEstimatorTest.<init>(AggCallSelectivityEstimatorTest.scala:68)
2020-03-07T00:35:25.0524186Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-03-07T00:35:25.0525546Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-03-07T00:35:25.0527263Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-03-07T00:35:25.0528827Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-03-07T00:35:25.0530888Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
2020-03-07T00:35:25.0533089Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
2020-03-07T00:35:25.0535236Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
2020-03-07T00:35:25.0537550Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
2020-03-07T00:35:25.0539721Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
2020-03-07T00:35:25.0541678Z 	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
2020-03-07T00:35:25.0543104Z 	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
2020-03-07T00:35:25.0544778Z 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
2020-03-07T00:35:25.0546859Z 	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
2020-03-07T00:35:25.0548887Z 	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
2020-03-07T00:35:25.0550897Z 	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
2020-03-07T00:35:25.0552395Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-07T00:35:25.0553911Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-07T00:35:25.0555763Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-07T00:35:25.0557261Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-07T00:35:25.0559014Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-07T00:35:25.0561000Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-07T00:35:25.0562494Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-07T00:35:25.0564133Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-07T00:35:25.0564938Z 
2020-03-07T00:35:26.3527172Z [INFO] Tests run: 28, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.223 s - in org.apache.flink.table.planner.plan.metadata.SelectivityEstimatorTest
2020-03-07T00:35:26.3542708Z [INFO] Running org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCountTest
2020-03-07T00:35:26.4374479Z [INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.08 s - in org.apache.flink.table.planner.plan.metadata.FlinkRelMdDistinctRowCountTest
2020-03-07T00:35:26.4385241Z [INFO] Running org.apache.flink.table.planner.plan.metadata.FlinkRelMdModifiedMonotonicityTest
{code}

Also later
{code}
2020-03-07T00:35:42.1007963Z [INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.047 s - in org.apache.flink.table.planner.operations.SqlToOperationConverterTest
2020-03-07T00:35:42.1071931Z 2020-03-07 00:35:42,105 pool-5-thread-1 ERROR Unable to unregister MBeans java.lang.LinkageError: javax/management/MBeanServer
2020-03-07T00:35:42.1073900Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:42.1075391Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:42.1076902Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:249)
2020-03-07T00:35:42.1078469Z 	at org.apache.logging.log4j.core.LoggerContext.stop(LoggerContext.java:362)
2020-03-07T00:35:42.1080437Z 	at org.apache.logging.log4j.core.LoggerContext$1.run(LoggerContext.java:303)
2020-03-07T00:35:42.1082200Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry$RegisteredCancellable.run(DefaultShutdownCallbackRegistry.java:109)
2020-03-07T00:35:42.1084172Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry.run(DefaultShutdownCallbackRegistry.java:74)
2020-03-07T00:35:42.1085579Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-07T00:35:42.1086173Z 
2020-03-07T00:35:53.4173788Z [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.509 s - in org.apache.flink.table.planner.codegen.SortCodeGeneratorTest
2020-03-07T00:35:53.4237819Z 2020-03-07 00:35:53,422 pool-2-thread-1 ERROR Unable to unregister MBeans java.lang.LinkageError: javax/management/MBeanServer
2020-03-07T00:35:53.4239501Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-03-07T00:35:53.4240709Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
2020-03-07T00:35:53.4241982Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:249)
2020-03-07T00:35:53.4243138Z 	at org.apache.logging.log4j.core.LoggerContext.stop(LoggerContext.java:362)
2020-03-07T00:35:53.4244277Z 	at org.apache.logging.log4j.core.LoggerContext$1.run(LoggerContext.java:303)
2020-03-07T00:35:53.4245728Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry$RegisteredCancellable.run(DefaultShutdownCallbackRegistry.java:109)
2020-03-07T00:35:53.4247407Z 	at org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry.run(DefaultShutdownCallbackRegistry.java:74)
2020-03-07T00:35:53.4248633Z 	at java.lang.Thread.run(Thread.java:748)
{code}

These issues were not reported as the root cause for the test failures."	FLINK	Resolved	2	1	4624	pull-request-available, test-stability
13305476	supports all kinds of changes for select result	[FLINK-17252|https://issues.apache.org/jira/browse/FLINK-17252] has supported select query, however only append change is supported. because [FLINK-16998|https://issues.apache.org/jira/browse/FLINK-16998] is not finished. This issue aims to support all kinds of changes based on FLINK-16998.	FLINK	Closed	3	7	4624	pull-request-available
13363482	FlinkRelMdUniqueKeys gets incorrect result on TableScan after project push-down	FlinkRelMdUniqueKeys#getTableUniqueKeys uses the original columns to find the index of primary keys. But after project push-down, the indexes is incorrect based on original columns, and the new columns should be used.	FLINK	Closed	3	1	4624	pull-request-available
13366614	GroupAggregateJsonPlanTest.testDistinctAggCalls fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6364


{code:java}

	at org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.testDistinctAggCalls(GroupAggregateJsonPlanTest.java:148)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)

{code}
"	FLINK	Closed	3	1	4624	pull-request-available, test-stability
13288623	Deprecate the methods in TableEnvironment proposed by FLIP-84	"In [FLIP-84|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=134745878], We propose to deprecate the following methods in TableEnvironment: 
{code:java}
void sqlUpdate(String sql)
void insertInto(String targetPath, Table table)
void execute(String jobName)
String explain(boolean extended)
Table fromTableSource(TableSource<?> source)
{code}
This issue aims to deprecate them.
"	FLINK	Closed	3	7	4624	pull-request-available
13238972	Supports CatalogManager in blink planner	This issue aims to let blink planner support {{CatalogMananger}} which is what FLINK-11476 has done.	FLINK	Closed	3	2	4624	pull-request-available
13343488	Refactor ExecNode and ExecEdge	"Currently, there are many improvements about ExecNode:
1. simplify type parameter of {{ExecNode}}. Currently, {{ExecNode#translateToPlan}} takes {{BatchPlanner}} or {{StreamPlanner}} as a parameter, so {{ExecNode}} has a type parameter {{E <: Planner}}, which indicates the node is a batch node or a streaming node. While in the future, a plan may contain both batch nodes and stream node. The type parameter can be removed, and we will use PlannerBase instead.
2. port the implementation of ExecNodes to Java
3. separate the implementation of {{FlinkPhysicalRel}} and {{ExecNode}}. Currently, an execution node extends both from {{FlinkPhysicalRel}} and {{ExecNode}}. After a physical node is converted to an exec node, many parameters are unnecessary, such as: RelOptCluster, RelTraitSet, etc. With more optimizations on {{ExecNode}}, We need {{ExecNode}} to be cleaner and simpler. So we will separate the implementation of {{FlinkPhysicalRel}} and {{ExecNode}}.

Currently, the ExecEdge represents the properties for the input of an operator, the properties include 
1. required data distribute for an input (the input corresponds to the [Input|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/Input.java])
2. DamBehavior which describes the behaviors how an in record may trigger the output of the target operator.
3. the priority of this input read by the target operator

So ExecEdge should be rename to InputProperty, and we will re-introduce ExecEdge which describes how to connect two ExecNodes, and how to shuffle the data between two ExecNodes. Its role should be similar to StreamEdge.


This is an umbrella issue, we will create more related sub-tasks."	FLINK	Closed	3	4	4624	auto-unassigned
13230463	Add support for generating optimized logical plan for stream window aggregate	"This issue aims to add support  for generating optimized logical plan for stream window aggregate queries, e.g.

{code:sql}
SELECT COUNT(*),
    TUMBLE_END(rowtime, INTERVAL '15' MINUTE) + INTERVAL '1' MINUTE
FROM MyTable
    GROUP BY TUMBLE(rowtime, INTERVAL '15' MINUTE)
{code}

the above query will be optimized to following plan

{code:sql}
Calc(select=[EXPR$0, +(CAST(w$end), 60000:INTERVAL MINUTE) AS EXPR$1])
+- GroupWindowAggregate(window=[TumblingGroupWindow], properties=[w$start, w$end, w$rowtime], select=[COUNT(*) AS EXPR$0, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime])
   +- Exchange(distribution=[single])
      +- Calc(select=[rowtime])
         +- DataStreamScan(table=[[_DataStreamTable_0]], fields=[a, b, c, proctime, rowtime])
{code}
"	FLINK	Closed	3	2	4624	pull-request-available
13241400	Remove dependencies on RelNode from TableImpl in blink planner	This issue aims to remove dependencies on RelNode from TableImpl in blink planner, just as [FLINK-12737|https://issues.apache.org/jira/browse/FLINK-12737] does.	FLINK	Closed	3	4	4624	pull-request-available
13299554	supports INSERT statement in TableEnvironment#executeSql and Table#executeInsert api	[FLINK-16366|https://issues.apache.org/jira/browse/FLINK-16366] has introduced executeSql method in TableEnvironment, but INSERT statement is not supported because [FLINK-17126|https://issues.apache.org/jira/browse/FLINK-17126] is not finished. This issue aims to support INSERT statement and introduce Table#executeInsert api after FLINK-17126 finished.	FLINK	Closed	3	7	4624	pull-request-available
13308876	Can not execute create/drop catalog statement in sql client	"when executing create catalog statement (e.g. {{create CATALOG c1 with('type'='generic_in_memory'}}) in sql client, the following exception will occur:

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unsupported command: CREATE CATALOG
	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:355)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:213)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201)

Similar case for {{drop catalog}}.

The reason is CliClient class does not handle CREATE_CATALOG command and DROP_CATALOG command."	FLINK	Closed	2	1	4624	pull-request-available
13439989	The plan for query with local sort is incorrect if adaptive batch scheduler is enabled	"Add the following test case in ForwardHashExchangeTest


{code:java}
  @Test
    public void testRankWithHashShuffle() {
        util.verifyExecPlan(
                ""SELECT * FROM (SELECT a, b, RANK() OVER(PARTITION BY a ORDER BY b) rk FROM T) WHERE rk <= 10"");
    }
{code}

The result plan is:

{code:java}
Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[true], select=[a, b, w0$o0])
+- Exchange(distribution=[forward])
   +- Sort(orderBy=[a ASC, b ASC])
      +- Exchange(distribution=[hash[a]])
         +- Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[false], select=[a, b])
            +- Sort(orderBy=[a ASC, b ASC])
                +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a, b], metadata=[]]], fields=[a, b])
{code}

There should be an additional {{Exchange(distribution=[forward])}} node between local {{Rank}} and {{Sort}}, other wise if adaptive batch scheduler is enabled but operator chain is disabled, the result may be wrong. Because the parallelism for local {{Rank}} and {{Sort}} should be same, otherwise the adaptive batch scheduler may change their parallelism.

 Local sort agg has the similar problem.
"	FLINK	Closed	3	1	4624	pull-request-available
13308861	Catalog does not exist in SQL Client	"Flink SQL> show catalogs;
default_catalog
hive

Flink SQL> use  catalog hive;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.catalog.exceptions.CatalogException: A catalog with name [`hive`] does not exist.


The reason is {{SqlCommandParser}} adds {{``}} for catalog name, which is unnecessary. "	FLINK	Closed	1	1	4624	pull-request-available
13227868	Add support for generating optimized logical plan for join on stream	"This issue aims to supports generating optimized plan for join on stream. 
The query will be converted to window join if join condition contains window bounds, otherwise will be converted to normal join.

e.g.
Queries similar to the following should be window join:
{code:sql}
SELECT t1.a, t2.b FROM MyTable t1 JOIN MyTable2 t2 ON
    t1.a = t2.a AND
    t1.proctime BETWEEN t2.proctime - INTERVAL '1' HOUR AND t2.proctime + INTERVAL '1' HOUR
{code}
"	FLINK	Closed	3	2	4624	pull-request-available
13247978	CatalogTableStatisticsConverter & TreeNode should be in correct package	currently, {{CatalogTableStatisticsConverter}} is in {{org.apache.flink.table.util}}, {{TreeNode}} is in {{org.apache.flink.table.planner.plan}}. {{CatalogTableStatisticsConverter}} should be in {{org.apache.flink.table.planner.utils}}, {{TreeNode}} should be in {{org.apache.flink.table.planner.expressions}}.	FLINK	Closed	3	1	4624	pull-request-available
13311037	Hidden files should be ignored when the filesystem table searches for partitions	"If there are some hidden files in the path of filesystem partitioned table, query this table will occur:
{code:java}
Caused by: org.apache.flink.table.api.TableException: Partition keys are: [j], incomplete partition spec: {}
        at org.apache.flink.table.filesystem.FileSystemTableSource.toFullLinkedPartSpec(FileSystemTableSource.java:209) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.filesystem.FileSystemTableSource.access$800(FileSystemTableSource.java:62) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.filesystem.FileSystemTableSource$1.lambda$getPaths$0(FileSystemTableSource.java:174) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_152]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545) ~[?:1.8.0_152]
        at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) ~[?:1.8.0_152]
        at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438) ~[?:1.8.0_152]
        at org.apache.flink.table.filesystem.FileSystemTableSource$1.getPaths(FileSystemTableSource.java:177) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{code}
Hidden files should be ignored when the filesystem table searches for partitions. This is not correct partition."	FLINK	Closed	1	1	4624	pull-request-available
13335329	Introduce multi-input operator for batch	After the planner is ready for multi-input, we should introduce multi-input operator for batch.	FLINK	Closed	3	7	4624	pull-request-available
13223971	Add support for generating optimized logical plan for Sort and Rank	"Add support for generating optimized logical plan for Sort and Rank.

There is a summary of Sort and Rank converters:
for batch, 
1. Sort without sort fields will be converted to BatchExecLimit
2. Sort with sort fields, null-fetch and null-offset will be converted to BatchExecSort
3. otherwise, Sort will be converted to BatchExecSortLimit
4. RANK function on over with filter will be converted to BatchExecRank

for stream,
1. Sort with time-ascending-order and non-limit will be converted to StreamExecTemporalSort
2. Sort without sort fields will be converted to StreamExecLimit
3. Sort with sort fields, null-fetch and null-offset will be converted to StreamExecSort
4. otherwise, Sort will be converted to StreamExecSortLimit
5. Rank with ROW_NUMBER function which is sorted on proc-time attribute and fetches only one record start from 0 will be converted to StreamExecDeduplicate
5. otherwise, RANK function on over with filter will be converted to StreamExecRank
"	FLINK	Closed	3	2	4624	pull-request-available
13269857	can not be translated to StreamExecDeduplicate when PROCTIME() is defined in query	"CREATE TABLE user_log (
    user_id VARCHAR,
    item_id VARCHAR,
    category_id VARCHAR,
    behavior VARCHAR,
    ts TIMESTAMP
) WITH (
    'connector.type' = 'kafka',
    'connector.version' = 'universal',
    'connector.topic' = 'user_behavior',
    'connector.startup-mode' = 'earliest-offset',
    'connector.properties.0.key' = 'zookeeper.connect',
    'connector.properties.0.value' = 'localhost:2181',
    'connector.properties.1.key' = 'bootstrap.servers',
    'connector.properties.1.value' = 'localhost:9092',
    'update-mode' = 'append',
    'format.type' = 'json',
    'format.derive-schema' = 'true'
);

CREATE TABLE user_dist (
    dt VARCHAR,
    user_id VARCHAR,
    behavior VARCHAR
) WITH (
    'connector.type' = 'jdbc',
    'connector.url' = 'jdbc:mysql://localhost:3306/flink-test',
    'connector.table' = 'user_behavior_dup',
    'connector.username' = 'root',
    'connector.password' = ‘******',
    'connector.write.flush.max-rows' = '1'
);

INSERT INTO user_dist
SELECT
  dt,
  user_id,
  behavior
FROM (
   SELECT
      dt,
      user_id,
      behavior,
     ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum
   FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc
            from user_log) )
WHERE rownum = 1;

Exception in thread ""main"" org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has a full primary keys if it is updated.
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:114)
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)
at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54)
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)
at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:61)
at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.Iterator$class.foreach(Iterator.scala:891)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:60)
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:149)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:348)"	FLINK	Resolved	3	1	4624	pull-request-available
13246776	Correct package name after relocation	some scala classes's package name is not updated after [FLINK-13266|https://issues.apache.org/jira/browse/FLINK-13267], this issue aims to correct the package names	FLINK	Resolved	1	7	4624	pull-request-available
13305367	watermark defined in ddl does not work in Table api	"the following code will get {{org.apache.flink.table.api.ValidationException: A group window expects a time attribute for grouping in a stream environment.}}

{code:java}
@Test
  def testRowTimeTableSourceGroupWindow(): Unit = {
    val ddl =
      s""""""
         |CREATE TABLE rowTimeT (
         |  id int,
         |  rowtime timestamp(3),
         |  val bigint,
         |  name varchar(32),
         |  watermark for rowtime as rowtime
         |) WITH (
         |  'connector' = 'projectable-values',
         |  'bounded' = 'false'
         |)
       """""".stripMargin
    util.tableEnv.executeSql(ddl)

    val t = util.tableEnv.from(""rowTimeT"")
      .where($""val"" > 100)
      .window(Tumble over 10.minutes on 'rowtime as 'w)
      .groupBy('name, 'w)
      .select('name, 'w.end, 'val.avg)
    util.verifyPlan(t)
  }
{code}

The reason is planner does not convert {{watermarkSpecs}} in {{TableSchema}} to correct type when calling {{tableEnv.from}}
"	FLINK	Closed	1	1	4624	pull-request-available
13227391	 Supports partial-final optimization for stream group aggregate	"To resolve data-skew for distinct aggregates on stream, we introduce a rule named {{SplitAggregateRule}} which rewrites an aggregate query with distinct aggregations into an expanded double aggregations. The first aggregation compute the results in sub-partition(with bucket) and the results are combined by the second aggregation.

if two-stage aggregation is also enabled, we find that many plans have common pattern, looks like:

{code}
... (output)
StreamExecGlobalGroupAggregate (final global agg)
+- StreamExecExchange
     +- StreamExecLocalGroupAggregate (final local agg)
          +- StreamExecGlobalGroupAggregate (partial global agg)
               +- .... (input)
{code}

There is no exchange between the final local aggregate and the partial global aggregate, so they will be executed in a same JobVertex, and could share state. We introduce a rule named {{IncrementalAggregateRule}} to do that optimization."	FLINK	Closed	3	2	4624	pull-request-available
13390825	Inconsistent rowtypes occur in IncrementalAggregateRule	"add the following test in IncrementalAggregateTest, and will get the following exception

{code:java}
  @Test
  def testSumCountWithSingleDistinctAndRetraction(): Unit = {
    val sqlQuery =
      s""""""
         |SELECT
         |  b, SUM(b1), COUNT(DISTINCT b1), COUNT(1)
         |FROM(
         |   SELECT
         |     a, COUNT(b) as b, MAX(b) as b1
         |   FROM MyTable
         |   GROUP BY a
         |) GROUP BY b
       """""".stripMargin
    util.verifyRelPlan(sqlQuery, ExplainDetail.CHANGELOG_MODE)
  }
{code}

{code:java}
java.lang.IllegalStateException
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177)
	at org.apache.flink.table.planner.plan.rules.physical.stream.IncrementalAggregateRule.onMatch(IncrementalAggregateRule.scala:127)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
{code}


The reason is the global agg on incremental agg does not handle retraction message in IncrementalAggregateRule when the query has one distinct agg function and count star agg function
.
"	FLINK	Closed	3	1	4624	pull-request-available
13299555	supports SELECT statement in TableEnvironment#executeSql and Table#execute api	[FLINK-16366|https://issues.apache.org/jira/browse/FLINK-16366] has introduced executeSql method in TableEnvironment, but INSERT statement is not supported because [FLINK-17126|https://issues.apache.org/jira/browse/FLINK-17126] is not finished and the design of [FLINK-14807|https://issues.apache.org/jira/browse/FLINK-14807] is discussing (the whole design of Table#collect and Table#execute are similar, although Table does not introduce collect method based on the conclusion of FLIP-84). This issue aims to support SELECT statement in TableEnvironment#executeSql and Table#execute api.	FLINK	Closed	3	7	4624	pull-request-available
13317346	Simplify the methods of Executor interface in sql client	"After {{TableEnvironment#executeSql}} is introduced, many methods in {{Executor}} interface can be replaced with {{TableEnvironment#executeSql}}. Those methods include:
listCatalogs, listDatabases, createTable, dropTable, listTables, listFunctions, useCatalog, useDatabase, getTableSchema (use {{DESCRIBE xx}})
"	FLINK	Closed	3	7	4624	pull-request-available
13288620	Remove deprecated method in StreamTableSink	"[FLIP-84|https://cwiki.apache.org/confluence/display/FLINK/FLIP-84%3A+Improve+%26+Refactor+API+of+TableEnvironment] proposes to unify the behavior of {{TableEnvironment}} and {{StreamTableEnvironment}}, and requires the {{StreamTableSink}} always returns {{DataStream}}. However
{{StreamTableSink.emitDataStream}} returns nothing and is deprecated since Flink 1.9, So we will remove it."	FLINK	Closed	3	7	4624	pull-request-available
13344563	Separate the implementation of BatchExecExchange and StreamExecExchange	The issue will separate the implementation of {{Batch(/Stream)ExecExchange}}, we will introduce Batch(/Stream)PhysicalExchange which only extends from {{FlinkPhysicalRel}} , and describes the physical   info of {{Exchange}}. Meanwhile, {{BatchExecExchange}} will be moved into `nodes.exec.batch` package and will implement ExecNode for Exchange,  so do it for {{StreamExecExchange}}. 	FLINK	Closed	3	7	4624	pull-request-available
13344556	Introduce getDescription, getOutputType, replaceInput methods for ExecNode	"We will introduce {{getDescription()}} method which returns the description information of the ExecNode,
introduce {{getOutputType()}} method which returns the output {{RowType}} of the ExecNode,
introduce {{replaceInputEdge(int ordinalInParent, ExecEdge newInputEdge)}} method which can update the input edge at the given index.

Note: {{RelNode}} also has {{getDescription()}} method, so before we finish all separation work, we will introduce {{getDesc()}} instead of {{getDescription()}} to avoid compile error."	FLINK	Closed	3	7	4624	pull-request-available
13294358	The config set by SET command does not work	Users can add or change the properties for execution behavior through SET command in SQL client CLI, e.g. {{SET execution.parallelism=10}}, {{SET table.optimizer.join-reorder-enabled=true}}. But the {{table.xx}} config can't change the TableEnvironment behavior, because the property set from CLI does not be set into TableEnvironment's table config.	FLINK	Closed	2	1	4624	pull-request-available
13290702	CheckpointCoordinatorFailureTest logs LinkageErrors	"This issue is in https://travis-ci.org/apache/flink/jobs/660152153?utm_medium=notification&utm_source=slack

Log output

{code:java}
2020-03-09 15:52:14,550 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:349)
	at org.apache.flink.util.TestLogger.<init>(TestLogger.java:36)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.<init>(CheckpointCoordinatorFailureTest.java:55)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
"	FLINK	Closed	4	1	4624	pull-request-available
13311786	Introduce TableResult#await method to wait for data ready	"Currently, {{TableEnvironment.executeSql()}}  method for INSERT statement returns TableResult once the job is submitted. Users must use {{tableResult.getJobClient.get().getJobExecutionResult(Thread.currentThread().getContextClassLoader).get()}} to wait the job finish. This API looks very ugly.
So this issue aims to introduce {{TableResult#await}} method, the code snippet looks like:

{code:java}
val tEnv = ...
// submit the job and wait job finish
tEnv.executeSql(""insert into ..."").await()
{code}

the suggested new methods are:

{code:java}
	/**
	 * Wait if necessary until the data is ready.
	 *
	 * <p>For select operation, this method will wait unit the first row can be accessed in local.
	 * For insert operation, this method will wait for the job to finish, because the result contains only one row.
	 * For other operations, this method will return immediately, because the result is ready in local.
	 *
	 * @throws ExecutionException if this future completed exceptionally
	 * @throws InterruptedException if the current thread was interrupted while waiting
	 */
	void await() throws InterruptedException, ExecutionException;

	/**
	 * Wait if necessary for at most the given time for the data to be ready.
	 *
	 * <p>For select operation, this method will wait unit the first row can be accessed in local.
	 * For insert operation, this method will wait for the job to finish, because the result contains only one row.
	 * For other operations, this method will return immediately, because the result is ready in local.
	 *
	 * @param timeout the maximum time to wait
	 * @param unit the time unit of the timeout argument
	 * @throws ExecutionException if this future completed exceptionally
	 * @throws InterruptedException if the current thread was interrupted while waiting
	 * @throws TimeoutException if the wait timed out
	 */
	void await(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;

{code}

"	FLINK	Closed	3	4	4624	pull-request-available
13285509	Blink planner can not extract correct unique key for UpsertStreamTableSink 	"I reproduce an Elasticsearch6UpsertTableSink issue which user reported in mail list[1] that Blink planner can not extract correct unique key for following query, but legacy planner works well. 
{code:java}
// user code
INSERT INTO ES6_ZHANGLE_OUTPUT  
 SELECT aggId, pageId, ts_min as ts,  
       count(case when eventId = 'exposure' then 1 else null end) as expoCnt,  
       count(case when eventId = 'click' then 1 else null end) as clkCnt  
 FROM  (    
     SELECT        
       'ZL_001' as aggId,
        pageId,        
        eventId,        
        recvTime,        
        ts2Date(recvTime) as ts_min    
     from kafka_zl_etrack_event_stream    
     where eventId in ('exposure', 'click')  
 ) as t1  
 group by aggId, pageId, ts_min
{code}
I  found that blink planner can not extract correct unique key in `*FlinkRelMetadataQuery.getUniqueKeys(relNode)*`, legacy planner works well in  `*org.apache.flink.table.plan.util.UpdatingPlanChecker.getUniqueKeyFields(...)* `. A simple ETL job to reproduce this issue can refers[2]

 

[1][http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-1-10-es-sink-exception-td32773.html]

[2][https://github.com/leonardBang/flink-sql-etl/blob/master/etl-job/src/main/java/kafka2es/Kafka2UpsertEs.java]

 

 "	FLINK	Resolved	2	1	4624	pull-request-available
13243014	Supports explain DAG plan	"in flink planner, a query will be optimized while calling {{TableEnvironment#insertInto}} or {{TableEnvironment#sqlUpdate}}. however, if a job has multiple sinks (means {{insertInto}} or {{sqlUpdate}} will be called multiple times), the final job contains several independent sub-graphs. In most cases, there is duplicate computing in a multiple sinks job. so in blink planner, multiple sinks queries will be optimized together to avoid duplicate computing. a query will not be optimized in {{insertInto}} and {{sqlUpdate}}. instead, queries will be optimized before executing.
this issue aims to support above case.

this issue only introduces {{explain}} method into {{TableEnvironment}}, {{execute}} method will be introduced in [FLINK-13088|https://issues.apache.org/jira/browse/FLINK-13088]

{code:java}
// explain multiple-sinks plan
String explain(boolean extended);
{code}

to make sure the behavior of flink planner is same as before, a {{isLazyOptMode}} filed is added into {{EnvironmentSettings}}, which tells the table environment should optimize the query immediately in {{insertInto}}/{{sqlUpdate}} methods for flink planner({{isLazyOptMode}} is always false) or in execute method for blink planner({{isLazyOptMode}} is always true) .

"	FLINK	Closed	3	2	4624	pull-request-available
13267340	Most tests from package o.a.f.table.planner.functions.aggfunctions are not executed during mvn test	"Only `ListAggWsWithRetractAggFunctionTest` and `ListAggWithRetractAggFunctionTest` are executed. 

And if we run the ignored tests from IDE, some of them will fail. 

 "	FLINK	Closed	1	1	4624	pull-request-available
13225249	Add support for generating optimized logical plan for join on batch	"Currently, there are 3 types of batch physical join nodes: {{BatchExecHashJoin}}, {{BatchExecSortMergeJoin}} and {{BatchExecNestedLoopJoin}}. This issue aims to add rules to convert logical join to appropriate physical join. 

a logical join
* can be converted to {{BatchExecHashJoin}} if there exists at least one equal-join condition and ShuffleHashJoin or BroadcastHashJoin are enabled, 
* can be converted to {{BatchExecSortMergeJoin}} if there exists at least one equal-join condition and SortMergeJoin is enabled,
* can be converted to {{BatchExecNestedLoopJoin}} if NestedLoopJoin is enabled or one of join input sides returns at most a single row.
"	FLINK	Closed	3	2	4624	pull-request-available
13305364	proctime defined in ddl can't work with over window in Table api	"the following test will get {{org.apache.flink.table.api.ValidationException: Ordering must be defined on a time attribute.}}
{code:scala}
  @Test
  def testProcTimeTableSourceOverWindow(): Unit = {
    val ddl =
      s""""""
         |CREATE TABLE procTimeT (
         |  id int,
         |  val bigint,
         |  name varchar(32),
         |  proctime as PROCTIME()
         |) WITH (
         |  'connector' = 'projectable-values',
         |  'bounded' = 'false'
         |)
       """""".stripMargin
    util.tableEnv.executeSql(ddl)

    val t = util.tableEnv.from(""procTimeT"")
      .window(Over partitionBy 'id orderBy 'proctime preceding 2.hours as 'w)
      .select('id, 'name, 'val.sum over 'w as 'valSum)
      .filter('valSum > 100)
    util.verifyPlan(t)
  }
{code}

The reason is: the type of proctime is {{TIMESTAMP(3) NOT null}}, while {{LegacyTypeInfoDataTypeConverter}} does not handle the mapping between {{Types.LOCAL_DATE_TIME}} and {{DataTypes.TIMESTAMP(3)}} with not null. 
"	FLINK	Closed	1	1	4624	pull-request-available
13248707	blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows 	just as [FLINK-11017|https://issues.apache.org/jira/browse/FLINK-11017], blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows 	FLINK	Resolved	3	1	4624	pull-request-available
13232650	Introduce planner rules to rewrite expression and merge calc	"This issue aims to introduce planner rules to rewrite expression and merge calc, rules include:
1. {{ConvertToNotInOrInRule}}, that converts a cascade of predicates to {{IN}} or {{NOT_IN}},
e.g. 
 converts predicate {{(x = 1 OR x = 2 OR x = 3 OR x = 4) AND y = 5}} to predicate {{x IN (1, 2, 3, 4) AND y = 5}}
 converts predicate {{(x <> 1 AND x <> 2 AND x <> 3 AND x <> 4) AND y = 5}} to predicate {{x NOT IN (1, 2, 3, 4) AND y = 5}}
2. {{RewriteCoalesceRule}}, that rewrites {{Coalesce}} to {{Case When}}
3. {{FlinkCalcMergeRule}}, that is copied from Calcite {{CalcMergeRule}}, and it will simplify the merged program"	FLINK	Closed	3	2	4624	pull-request-available
13280393	SQL client can't cancel flink job	"in sql client, CLI client do cancel query operation through {{void cancelQuery(String sessionId, String resultId)}} method in {{Executor}}. However, the {{resultId}} is a random UUID, is not the job id. So CLI client can't cancel a running job.


related code in {{LocalExecutor}}:
{code:java}
private <C> ResultDescriptor executeQueryInternal(String sessionId, ExecutionContext<C> context, String query) {
	 ......

	// store the result with a unique id
	final String resultId = UUID.randomUUID().toString();
	resultStore.storeResult(resultId, result);

	......

	// create execution
	final ProgramDeployer deployer = new ProgramDeployer(
		configuration, jobName, pipeline);

	// start result retrieval
	result.startRetrieval(deployer);

	return new ResultDescriptor(
			resultId,
			removeTimeAttributes(table.getSchema()),
			result.isMaterialized());
}



private <T> void cancelQueryInternal(ExecutionContext<T> context, String resultId) {
	......

	// stop Flink job
	try (final ClusterDescriptor<T> clusterDescriptor = context.createClusterDescriptor()) {
		ClusterClient<T> clusterClient = null;
		try {
			// retrieve existing cluster
			clusterClient = clusterDescriptor.retrieve(context.getClusterId()).getClusterClient();
			try {
				// ======== cancel job through resultId =======
				clusterClient.cancel(new JobID(StringUtils.hexStringToByte(resultId))).get();
			} catch (Throwable t) {
				// the job might has finished earlier
			}
		} catch (Exception e) {
			throw new SqlExecutionException(""Could not retrieve or create a cluster."", e);
		} finally {
			try {
				if (clusterClient != null) {
					clusterClient.close();
				}
			} catch (Exception e) {
				// ignore
			}
		}
	} catch (SqlExecutionException e) {
		throw e;
	} catch (Exception e) {
		throw new SqlExecutionException(""Could not locate a cluster."", e);
	}
}
{code}



"	FLINK	Closed	2	1	4624	pull-request-available
13243112	Support lazy query transformation & execution on TableEnvironment	"in [FLINK-13081|https://issues.apache.org/jira/browse/FLINK-13081], {{explain}} method will be introduced to support explain multiple-sinks plan, this issue aims to introduce {{execute}} method into {{TableEnvironment}} to trigger the program execution. and in blink planner, queries will not be optimized immediately in {{insertInto}}/{{sqlUpdate}} methods, and will be optimized together in this method.

{code:java}
// Triggers the program execution
JobExecutionResult execute(String jobName) throws Exception;
{code}

there are two concerns about this method: 
1. which {{execute}} methods ({{TableEnvironment#execute}} or {{StreamExecutionEnvironment#execute}}) users should use?
2.  how to make sure users only use {{TableEnvironment#execute}} method if their code only depends on planner module instead of bridge module？"	FLINK	Closed	3	2	4624	pull-request-available
13268318	extends max/min type in ColumnStats from Number to Comparable	Many tpc-ds queries have predicates on date, like {{d_date between '1999-02-01' and (cast('1999-02-01' as date) + INTERVAL '60' day)}}, It's very useful to find a better plan if the planner knows the max/min values of date. However, max/min in {{ColumnStats}} only support {{Number}} type currently. This issue aims to extend max/min type from {{Number}} to {{Comparable}}, and then {{Date}}, {{Time}}, {{Timestamp}} even {{String}} could be supported.	FLINK	Closed	3	7	4624	pull-request-available
13240102	Add travis profile for flink-table-planner-blink/flink-table-runtime-blink	The flink-table-planner-blink module and flink-table-runtime-blink module take almost 30 minutes, and that may cause libraries profile frequently hits timeouts; we can resolve this by moving flink-table-planner-blink and flink-table-runtime-blink into a separate profile.	FLINK	Closed	3	4	4624	pull-request-available
13236173	 Introduce MiniBatchIntervalInferRule and watermark assigner operators	"This issue aims to introduce MiniBatchIntervalInferRule to infer the mini-batch interval of watermark assigner, This rule could handle the following two kinds of operator:
1. supports operators which supports mini-batch and does not require watermark, e.g. group aggregate. In this case, {{StreamExecWatermarkAssigner}} with Protime mode will be created if not exist, and the interval value will be set as {{SQL_EXEC_MINIBATCH_ALLOW_LATENCY}}.
2. supports operators which requires watermark, e.g. window join, window aggregate. In this case, {{StreamExecWatermarkAssigner}} already exists, and its MiniBatchIntervalTrait will be updated as the merged intervals from its outputs.
Currently, mini-batched window aggregate is not supported, and will be introduced later.

this issue also introduces watermark assigner operators, including:
1. {{WatermarkAssignerOperator}}, that extracts timestamps from stream elements and generates periodic watermarks.
2. {{MiniBatchedWatermarkAssignerOperator}}, that extracts timestamps from stream elements and generates watermarks with specified emit latency.
3. {{MiniBatchAssignerOperator}}, that emits mini-batch marker in a given period."	FLINK	Closed	3	2	4624	pull-request-available
13244016	Bump Calcite dependency to 1.20.0 in sql parser & flink planner	"blink planner had upgraded calcite version to 1.20.0 (before version is 1.19.0), and blink planner will support DDL in FLINK-1.9 which depends on flink-sql-parser. so calcite version in flink-sql-parser should also be upgrade to 1.20.0.

[~walterddr], [FLINK-11935|https://issues.apache.org/jira/browse/FLINK-11935] will not be fixed in this issue, because supporting DDL in blink planner is blocked by this."	FLINK	Closed	3	4	4624	pull-request-available
13291466	Exception will be thrown when computing columnInterval relmetadata in some case	"Consider the following SQL

 
{code:java}
// a: INT, c: LONG
SELECT 
    c, SUM(a) 
FROM T 
WHERE a > 0.1 AND a < 1 
GROUP BY c{code}
 

Here the sql type of 0.1 is Decimal and 1 is Integer, and they are both in NUMERIC type family, and do not trigger type coercion, so the plan is:
{code:java}
FlinkLogicalAggregate(group=[{0}], EXPR$1=[SUM($1)])
+- FlinkLogicalCalc(select=[c, a], where=[AND(>(a, 0.1:DECIMAL(2, 1)), <(a, 1))])
   +- FlinkLogicalTableSourceScan(table=[[...]], fields=[a, b, c])
{code}
When we calculate the filtered column interval of calc, it'll lead to validation exception of `FiniteValueInterval`:

!image-2020-03-13-10-32-35-375.png!"	FLINK	Closed	2	1	4624	pull-request-available
13248474	JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin	"run tpcds 14.a on blink planner, an exception will thrown

java.lang.ArrayIndexOutOfBoundsException: 84

	at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:564)
	at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:555)
	at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112)
	at org.apache.calcite.rex.RexVisitorImpl.visitCall(RexVisitorImpl.java:80)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:191)
	at org.apache.calcite.rel.rules.JoinToMultiJoinRule.addOnJoinFieldRefCounts(JoinToMultiJoinRule.java:481)
	at org.apache.calcite.rel.rules.JoinToMultiJoinRule.onMatch(JoinToMultiJoinRule.java:166)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:284)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)


the reason is {{JoinToMultiJoinRule}} should match SEMI/ANTI LogicalJoin. before calcite-1.20, SEMI join is represented by {{SemiJoin}} which is not matched {{JoinToMultiJoinRule}}."	FLINK	Resolved	3	1	4624	pull-request-available
13374969	IllegalArgumentException is thrown in WindowAttachedWindowingStrategy when two phase is enabled for distinct agg	"Caused by: java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.flink.table.planner.plan.logical.WindowAttachedWindowingStrategy.<init>(WindowAttachedWindowingStrategy.java:51)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)


The reason is the {{windowStart}} may be {{-1}} when two phase is enabled for distinct agg, see [TwoStageOptimizedWindowAggregateRule.java#L143|https://github.com/apache/flink/blob/a3363b91b144edfbae5ab114984ded622d3f8fbc/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/physical/stream/TwoStageOptimizedWindowAggregateRule.java#L143]"	FLINK	Closed	3	1	4624	pull-request-available
13419823	Idea Scala plugin can not compile RexExplainUtil 	"Idea version: 2021.2.3
Scala version: 2.11.12

There are some errors in {{RexExplainUtil}} and many classes which use the methods in {{RexExplainUtil}}. 
NOTES: those class can be compiled and executed successfully

 !image-2021-12-29-15-16-50-513.png! 
 !image-2021-12-29-15-20-18-794.png! "	FLINK	Closed	4	4	4624	pull-request-available
13312618	TableResult#print can not print the result of unbounded stream query	In current implementation of PrintUtils, all result will be collected to local memory to compute column width first. this can works fine with batch query and bounded stream query. but for unbounded stream query, the result will be endless, so the result will be never printed. To solve this, we can use fix-length strategy, and print a row immediately once the row is accessed.	FLINK	Closed	1	1	4624	pull-request-available
13428607	Explicitly set the partitioner for the sql operators whose shuffle and sort are removed	"After FLINK-25995 is finished, we have add an exchange (which will be converted to ForwardForConsecutiveHashPartitioner) for the nodes which do not need explicitly hash shuffle (which input has already hashed)

e.g.
{code:sql}
WITH r AS (SELECT * FROM T1, T2 WHERE a1 = a2 AND c1 LIKE 'He%') SELECT sum(b1) FROM r group by a1
{code}

the plan after FLINK-25995 is finished:
{code:java}
Calc(select=[EXPR$0])
+- HashAggregate(isMerge=[false], groupBy=[a1], select=[a1, SUM(b1) AS EXPR$0])
   +- Exchange(distribution=[keep_input_as_is[hash[a1]])
      +- Calc(select=[a1, b1])
         +- HashJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2], build=[left])
            :- Exchange(distribution=[hash[a1]])
            :  +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')])
            :     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1])
            +- Exchange(distribution=[hash[a2]])
               +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
{code}

but data between {{Calc}} and {{HashJoin}} may be out of order once their parallelism is different, so an {{Exchange(distribution=[keep_input_as_is[hash[a1]])}} should be added between them."	FLINK	Closed	3	1	4624	pull-request-available
13344533	Refactor verifyPlan methods in TableTestBase	" Currently, we use {{verifyPlan}} method to verify the plan result for both {{RelNode}} plan and {{ExecNode}} plan, because their instances are the same. But once the implementation of {{RelNode}} and {{ExecNode}} are separated, we can't get {{ESTIMATED_COST}} and {{CHANGELOG_MODE}} on {{ExecNode}} plan. So in order to make those methods more clear, we will do the following refactoring:
1. replace {{planBefore}} with {{ast}} in xml file. {{ast}} is ""Abstract Syntax Tree"", corresponding to ""Abstract Syntax Tree"" item in the explain result; 
2. remove {{planAfter}}, introduce  {{optimized rel plan}} and {{optimized exec plan}}. {{optimized rel plan}}  is the optimized rel plan, and is similar to ""Optimized Physical Plan"" item in the explain result. but different from ""Optimized Physical Plan"", {{optimized rel plan}} can represent either optimized logical rel plan (for rule testing) or optimized physical rel plan (for changelog validation, etc). {{optimized exec plan}} is the optimized execution plan, corresponding to ""Optimized Execution Plan"" item in the explain result. see https://issues.apache.org/jira/browse/FLINK-20478 for more details about explain refactor
3. keep {{verifyPlan}} method, which will print {{ast}}, {{optimized rel plan}} and {{optimized exec plan}}. 
4. add {{verifyRelPlan}} method, which will print {{ast}}, {{optimized rel plan}}
5. add {{verifyExecPlan}} method, which will print {{ast}} and {{optimized exec plan}}. "	FLINK	Closed	3	7	4624	pull-request-available
13350566	Separate the implementation of stream window aggregate nodes	"Stream window aggregate nodes include:
StreamExecGroupWindowAggregate
StreamExecGroupWindowTableAggregate
StreamExecPythonGroupWindowAggregate"	FLINK	Closed	3	7	4624	pull-request-available
13305182	sql client supports parser statements via sql parser	"Current, all statements in sql client are parsed via regex matching, which has many limitations, such as it can't handle comments. To avoid that limitations, we should try best to use sql parser to parse a statement. There are many statement can't be handle by sql parser, such as: set, reset. so they are still handle through regex matching.

statements handled through regex matching:
quit, exit, clear, help, desc, explain, set, reset source, show modules

statements handled through sql parser:
show catalogs, show databases, show tables, show functions, use catalog, use, describe, explain plan for, select, insert, DDLs

note: we keep `explain xx`, and also support `explain plan for xx`"	FLINK	Closed	3	4	4624	pull-request-available
13233460	Introduce planner rules about semi/anti join	"This issue aims to introduce planner rules about semi/anti join, rules include:
1. {{FlinkSemiAntiJoinFilterTransposeRule}} that pushes semi/anti join down in a tree past a filter
2. {{FlinkSemiAntiJoinJoinTransposeRule}} that pushes semi/anti join down in a tree past a non semi/anti join
3. {{FlinkSemiAntiJoinProjectTransposeRule}} that push semi/anti join down in a tree past a project
4. {{ProjectSemiAntiJoinTransposeRule}} that pushes a project down in a tree past a semi/anti join

planner rules about non semi/anti join will be introduced in [FLINK-12509|https://issues.apache.org/jira/browse/FLINK-12509].
"	FLINK	Closed	3	2	4624	pull-request-available
13344139	Adjust the explain result	"Currently, the explain result includes ""Abstract Syntax Tree"", ""Optimized Logical Plan"" and ""Physical Execution Plan"". While the ""Optimized Logical Plan"" is an {{ExecNode}} graph, and the ""[ExplainDetail|https://github.com/apache/flink/blob/master/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/ExplainDetail.java]"" represents the expected explain details, including {{ESTIMATED_COST}} and {{CHANGELOG_MODE}} now. Those types can only used for Calicte {{RelNode}}s instead of {{ExecNode}}. So I suggest to make the following adjustments:
1. Keep ""Abstract Syntax Tree"" as it, which represents the original (un-optimized) {{RelNode}} graph converted from {{SqlNode}}.
2. Rename ""Optimized Logical Plan"" to ""Optimized Physical Plan"", which represents the optimized physical {{RelNode}} graph composed of {{FlinkPhysicalRel}}. {{ESTIMATED_COST}} and {{CHANGELOG_MODE}} describe the expected explain details for ""Optimized Physical Plan"".
3.Replace ""Physical Execution Plan"" with ""Optimized Execution Plan"", which represents the optimized {{ExecNode}} graph. Currently, many optimizations are based on {{ExecNode}} graph, such as sub-plan reuse, multiple input rewrite. We may introduce more optimizations in the future. So there are more and more difference between ""Optimized Physical Plan"" and ""Optimized Execution Plan"". We do not want to show tow execution plans, and ""Physical Execution Plan"" for {{StreamGraph}} is less important than ""Optimized Execution Plan"". If we want to introduce ""Physical Execution Plan"" in the future, we can add a type named ""PHYSICAL_EXECUTION_PLAN"" in {{ExplainDetail}} to support it. There is already an issue to do the similar things, [FLINK-19687|https://issues.apache.org/jira/browse/FLINK-19687]

The following example show the explain result after adjustment:


{code}
== Abstract Syntax Tree ==
LogicalLegacySink(name=[`default_catalog`.`default_database`.`upsertSink1`], fields=[a, cnt])
+- LogicalProject(a=[$0], cnt=[$1])
   +- LogicalFilter(condition=[>($1, 10)])
      +- LogicalAggregate(group=[{0}], cnt=[COUNT()])
         +- LogicalProject(a=[$0])
            +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1]])

LogicalLegacySink(name=[`default_catalog`.`default_database`.`upsertSink2`], fields=[a, cnt])
+- LogicalProject(a=[$0], cnt=[$1])
   +- LogicalFilter(condition=[<($1, 10)])
      +- LogicalAggregate(group=[{0}], cnt=[COUNT()])
         +- LogicalProject(a=[$0])
            +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1]])

== Optimized Physical Plan ==
LegacySink(name=[`default_catalog`.`default_database`.`upsertSink1`], fields=[a, cnt])
+- Calc(select=[a, cnt], where=[>(cnt, 10)])
   +- GroupAggregate(groupBy=[a], select=[a, COUNT(*) AS cnt])
      +- Exchange(distribution=[hash[a]])
         +- Calc(select=[a])
            +- DataStreamScan(table=[[default_catalog, default_database, MyTable1]], fields=[a, b, c])

LegacySink(name=[`default_catalog`.`default_database`.`upsertSink2`], fields=[a, cnt])
+- Calc(select=[a, cnt], where=[<(cnt, 10)])
   +- GroupAggregate(groupBy=[a], select=[a, COUNT(*) AS cnt])
      +- Exchange(distribution=[hash[a]])
         +- Calc(select=[a])
            +- DataStreamScan(table=[[default_catalog, default_database, MyTable1]], fields=[a, b, c])

== Optimized Execution Plan ==
GroupAggregate(groupBy=[a], select=[a, COUNT(*) AS cnt], reuse_id=[1])
+- Exchange(distribution=[hash[a]])
   +- Calc(select=[a])
      +- DataStreamScan(table=[[default_catalog, default_database, MyTable1]], fields=[a, b, c])

LegacySink(name=[`default_catalog`.`default_database`.`upsertSink1`], fields=[a, cnt])
+- Calc(select=[a, cnt], where=[>(cnt, 10)])
   +- Reused(reference_id=[1])

LegacySink(name=[`default_catalog`.`default_database`.`upsertSink2`], fields=[a, cnt])
+- Calc(select=[a, cnt], where=[<(cnt, 10)])
   +- Reused(reference_id=[1])
{code}

"	FLINK	Closed	3	7	4624	pull-request-available
12835664	Add ParquetTableSource	Add a {{ParquetTableSource}} to read data from a Apache Parquet file. The {{ParquetTableSource}} should implement the {{ProjectableTableSource}} (FLINK-3848) and {{FilterableTableSource}} (FLINK-3849) interfaces.	FLINK	Closed	4	2	4624	starter
13266841	blink planner should also fetch catalog statistics for permanent CatalogTableImpl	currently, blink planner will fetch the catalog statistics and convert to {{TableStats}} for permanent catalog table in {{DatabaseCalciteSchema}}. However, only {{ConnectorCatalogTable}} is handled in {{convertPermanentTable}} method. This issue aims to support {{CatalogTableImpl}}	FLINK	Closed	3	7	4624	pull-request-available
13346761	ExecNode#getOutputType method should return LogicalType instead of RowType	Currently, {{ExecNode#getOutputType}} always returns {{RowType}}. But some nodes do not return {{RowData}}, such as {{BatchExecSink}} and {{StreamExecSink}}. The output type of {{ExecNode}} should be consistent with the type parameter {{T}}. So {{ExecNode#getOutputType}} should return {{LogicalType}} instead of {{RowType}}, each subclass cast the {{LogicalType}} to specific type when using it.	FLINK	Closed	3	7	4624	pull-request-available
13303954	TableEnvironmentITCase.testStatementSet fails on travis	"{code}
[ERROR] Failures: 
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=912&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129"	FLINK	Closed	1	1	4624	test-stability
13235089	Introduce planner rules to do deterministic rewriting on RelNode 	"This issue aims to introduce planner rules to to do deterministic rewriting on RelNode , rules include:
1. {{FlinkLimit0RemoveRule}} that rewrites `limit 0` to empty {{Values}}
2. {{FlinkRewriteSubQueryRule}} that rewrites a {{Filter}} with condition: `(select count(*) from T) > 0` to a {{Filter}} with condition: `exists(select * from T)` which could be converted to SEMI {{Join}} by {{FlinkSubQueryRemoveRule}}
3. {{ReplaceIntersectWithSemiJoinRule}} that rewrites distinct {{Intersect}} to a distinct {{Aggregate}} on a SEMI {{Join}}.
4. {{ReplaceMinusWithAntiJoinRule}} that rewrites distinct {{Minus}} to a distinct {{Aggregate}} on an ANTI {{Join}}."	FLINK	Closed	3	2	4624	pull-request-available
13250714	Transformations should be cleared because a table environment could execute multiple job	currently, if a table environment execute more than one sql jobs, the following job contains transformations about the previous job. the reason is the transformations is not cleared after execution	FLINK	Resolved	2	1	4624	pull-request-available
13242933	Bump Calcite dependency to 1.20.0 in blink planner	"Bump Calcite dependency to 1.20.0 in flink-table-planner-blink module.

"	FLINK	Closed	3	4	4624	pull-request-available
13246761	RelOptCluster's constructor can not be accessed when connector dependent both flink-planner and blink-planner	"If connector dependent both flink and blink planner. There is an exception shows RelOptCluster's constructor can not be accessed.
{code:java}
java.lang.IllegalAccessError: tried to access method org.apache.calcite.plan.RelOptCluster.<init>(Lorg/apache/calcite/plan/RelOptPlanner;Lorg/apache/calcite/rel/type/RelDataTypeFactory;Lorg/apache/calcite/rex/RexBuilder;Ljava/util/concurrent/atomic/AtomicInteger;Ljava/util/Map;)V from class org.apache.flink.table.planner.calcite.FlinkRelOptCluster

at org.apache.flink.table.planner.calcite.FlinkRelOptCluster.<init>(FlinkRelOptCluster.scala:42)
at org.apache.flink.table.planner.calcite.FlinkRelOptCluster$.create(FlinkRelOptCluster.scala:71)
at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$.create(FlinkRelOptClusterFactory.scala:34)
at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory.create(FlinkRelOptClusterFactory.scala)
at org.apache.flink.table.planner.delegation.PlannerContext.<init>(PlannerContext.java:92)
at org.apache.flink.table.planner.delegation.PlannerBase.<init>(PlannerBase.scala:86)
at org.apache.flink.table.planner.delegation.StreamPlanner.<init>(StreamPlanner.scala:47)
at org.apache.flink.table.planner.delegation.BlinkPlannerFactory.create(BlinkPlannerFactory.java:50)
at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:116)
at org.apache.flink.table.api.java.StreamTableEnvironment.create(StreamTableEnvironment.java:112)
at org.apache.flink.api.java.io.jdbc.JDBCUpsertTableSinkITCase.testUpsert(JDBCUpsertTableSinkITCase.java:124)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
{code}
Maybe we should copy RelOptCluster to flink-planner."	FLINK	Resolved	1	7	4624	pull-request-available
13342758	ColumnIntervalUtil#getColumnIntervalWithFilter does not consider the case when the predicate is a false constant	"To reproduce this bug, add the following test case to {{DeadlockBreakupTest.scala}}

{code:scala}
@Test
def testSubplanReuse_DeadlockCausedByReusingExchangeInAncestor(): Unit = {
  util.tableEnv.getConfig.getConfiguration.setBoolean(
    OptimizerConfigOptions.TABLE_OPTIMIZER_REUSE_SUB_PLAN_ENABLED, true)
  util.tableEnv.getConfig.getConfiguration.setBoolean(
    OptimizerConfigOptions.TABLE_OPTIMIZER_MULTIPLE_INPUT_ENABLED, false)
  util.tableEnv.getConfig.getConfiguration.setString(
    ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, ""NestedLoopJoin,SortMergeJoin"")
  val sqlQuery =
    """"""
      |WITH T1 AS (SELECT x1.*, x2.a AS k, x2.b AS v FROM x x1 LEFT JOIN x x2 ON x1.a = x2.a WHERE x2.b > 0)
      |SELECT x.a, T1.* FROM x LEFT JOIN T1 ON x.a = T1.k WHERE x.b > 0 AND T1.v = 0
      |"""""".stripMargin
  util.verifyPlan(sqlQuery)
}
{code}

And we'll get the exception stack
{code}
java.lang.RuntimeException: Error while applying rule FlinkLogicalJoinConverter(in:NONE,out:LOGICAL), args [rel#414:LogicalJoin.NONE.any.[](left=RelSubset#406,right=RelSubset#413,condition==($0, $4),joinType=inner)]

	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:86)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.getOptimizedPlan(TableTestBase.scala:431)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:348)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)
	at org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.testSubplanReuse_DeadlockCausedByReusingExchangeInAncestor(DeadlockBreakupTest.scala:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.lang.RuntimeException: Error occurred while applying rule FlinkLogicalJoinConverter(in:NONE,out:LOGICAL)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:169)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
	... 54 more
Caused by: java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayBuffer.scala:48)
	at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:74)
	at scala.collection.mutable.ArrayBuffer.reduceLeft(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$.getColumnIntervalWithFilter(ColumnIntervalUtil.scala:219)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:181)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:801)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:156)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:801)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:309)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:306)
	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38)
	at scala.collection.IndexedSeqOptimized$class.exists(IndexedSeqOptimized.scala:46)
	at scala.collection.mutable.ArrayBuffer.exists(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getEquiInnerJoinRowCount(FlinkRelMdRowCount.scala:306)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:268)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:410)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalJoin.computeSelfCost(FlinkLogicalJoin.scala:64)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCost.getNonCumulativeCost(FlinkRelMdNonCumulativeCost.scala:41)
	at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost_$(Unknown Source)
	at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getNonCumulativeCost(RelMetadataQuery.java:288)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.getCost(VolcanoPlanner.java:705)
	at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements0(RelSubset.java:415)
	at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements(RelSubset.java:398)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1268)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1227)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
	... 58 more

{code}"	FLINK	Closed	3	1	4624	pull-request-available
13219831	Introduce flink logical relational nodes	"* _Adds nodes extended from Calcite, including {{FlinkLogicalAggregate}}, {{FlinkLogicalCalc}},{{FlinkLogicalCorrelate}}, {{FlinkLogicalIntersect}}, {{FlinkLogicalJoin}}, {{FlinkLogicalMinus}}, {{FlinkLogicalOverWindow}}, {{FlinkLogicalSort}}, {{FlinkLogicalUnion}}, {{FlinkLogicalValues}}, {{FlinkLogicalTableSourceScan}}, {{FlinkLogicalTableFunctionScan}}_
 * _Adds new RelNode for Flink, including {{Expand}}, {{Rank}}, {{Sink}}, {{WatermarkAssigner}}_"	FLINK	Closed	3	2	4624	pull-request-available
13351925	Separate the implementation of sink nodes	including StreamExecSink, BatchExecSink, StreamExecLegacySink, BatchExecLegacySink	FLINK	Closed	3	7	4624	pull-request-available
13366370	JoinJsonPlanITCase fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15031&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29&l=6693

{code}
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:130)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:81)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:701)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	... 4 more
Caused by: java.io.IOException: Insufficient number of network buffers: required 33, but only 2 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalCreateBufferPool(NetworkBufferPool.java:372)
	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:350)
	at org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.lambda$createBufferPoolFactory$0(ResultPartitionFactory.java:269)
	at org.apache.flink.runtime.io.network.partition.ResultPartition.setup(ResultPartition.java:150)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.setup(BufferWritingResultPartition.java:95)
	at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:926)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:651)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

{code}"	FLINK	Closed	3	1	4624	test-stability
13363765	Introduce ExecNodeTranslator	An ExecNodeTranslator is responsible for translating an ExecNode to Flink operator, it has two sub-classes now: SingleTransformationTranslator and MultipleTransformationTranslator. For SingleTransformationTranslator, we can set some info (such as parallelism) to the Transformation in the base class and will reduce many duplicate code	FLINK	Closed	3	7	4624	pull-request-available
13243330	adapt blink planner to new unified TableEnvironment	This issue aims to adapt blink planner to new unified {{TableEnvironment}}, remove all legacy {{TableEnvironment}}s in blink planner,  and make sure all blink tests could run successfully.	FLINK	Closed	3	2	4624	pull-request-available
13319541	The monotonicity of UNIX_TIMESTAMP function is not correct	Currently, the monotonicity of {{UNIX_TIMESTAMP}} function is always {{INCREASING}}, actually, when it has empty function arguments ({{UNIX_TIMESTAMP()}}, is equivalent to {{NOW()}}), its monotonicity is INCREASING. otherwise its monotonicity should be NOT_MONOTONIC. (e.g. UNIX_TIMESTAMP(string))	FLINK	Closed	3	1	4624	pull-request-available
13366613	IncrementalAggregateJsonPlanTest.testIncrementalAggregate fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6303


{code:java}
org.junit.ComparisonFailure: expected:<...C5TdHJ1Y3R1cmVkVHlwZ[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')""}]},""description"":""LocalGroupAggregate(groupBy=[a, $f2], partialFinalType=[PARTIAL], select=[a, $f2, COUNT(distinct$0 c) AS count$0, DISTINCT(c) AS distinct$0])""},{""class"":""org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecExchange"",""id"": 0,""inputProperties"":[{""requiredDistribution"":{""type"":""HASH"",""keys"":[0,1]},""damBehavior"":""PIPELINED"",""priority"":0}],""outputType"":{""type"":""ROW"",""nullable"":true,""fields"":[{""a"":""BIGINT""},{""$f2"":""INT""},{""count$0"":""BIGINT""},{""distinct$0"":""RAW('org.apache.flink.table.api.dataview.MapView', 'AFZvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkV4dGVybmFsU2VyaWFsaXplciRFeHRlcm5hbFNlcmlhbGl6ZXJTbmFwc2hvdAAAAAMADecEAAAAAaztAAVzcgArb3JnLmFwYWNoZS5mbGluay50YWJsZS50eXBlcy5GaWVsZHNEYXRhVHlwZfSwrBytgZ9fAgABTAAOZmllbGREYXRhVHlwZXN0ABBMamF2YS91dGlsL0xpc3Q7eHIAJW9yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMuRGF0YVR5cGV5y2rIj5/EeAIAAkwAD2NvbnZlcnNpb25DbGFzc3QAEUxqYXZhL2xhbmcvQ2xhc3M7TAALbG9naWNhbFR5cGV0ADJMb3JnL2FwYWNoZS9mbGluay90YWJsZS90eXBlcy9sb2dpY2FsL0xvZ2ljYWxUeXBlO3hwdnIAK29yZy5hcGFjaGUuZmxpbmsudGFibGUuYXBpLmRhdGF2aWV3Lk1hcFZpZXcAAAAAAAAAAAAAAHhwc3IAM29yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMubG9naWNhbC5TdHJ1Y3R1cmVkVHlwZS35a8jjr/r3]AgAFWgAOaXNJbnN0YW50...> but was:<...C5TdHJ1Y3R1cmVkVHlwZ[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')""}]},""description"":""LocalGroupAggregate(groupBy=[a, $f2], partialFinalType=[PARTIAL], select=[a, $f2, COUNT(distinct$0 c) AS count$0, DISTINCT(c) AS distinct$0])""},{""class"":""org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecExchange"",""id"": 0,""inputProperties"":[{""requiredDistribution"":{""type"":""HASH"",""keys"":[0,1]},""damBehavior"":""PIPELINED"",""priority"":0}],""outputType"":{""type"":""ROW"",""nullable"":true,""fields"":[{""a"":""BIGINT""},{""$f2"":""INT""},{""count$0"":""BIGINT""},{""distinct$0"":""RAW('org.apache.flink.table.api.dataview.MapView', 'AFZvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkV4dGVybmFsU2VyaWFsaXplciRFeHRlcm5hbFNlcmlhbGl6ZXJTbmFwc2hvdAAAAAMADecEAAAAAaztAAVzcgArb3JnLmFwYWNoZS5mbGluay50YWJsZS50eXBlcy5GaWVsZHNEYXRhVHlwZfSwrBytgZ9fAgABTAAOZmllbGREYXRhVHlwZXN0ABBMamF2YS91dGlsL0xpc3Q7eHIAJW9yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMuRGF0YVR5cGV5y2rIj5/EeAIAAkwAD2NvbnZlcnNpb25DbGFzc3QAEUxqYXZhL2xhbmcvQ2xhc3M7TAALbG9naWNhbFR5cGV0ADJMb3JnL2FwYWNoZS9mbGluay90YWJsZS90eXBlcy9sb2dpY2FsL0xvZ2ljYWxUeXBlO3hwdnIAK29yZy5hcGFjaGUuZmxpbmsudGFibGUuYXBpLmRhdGF2aWV3Lk1hcFZpZXcAAAAAAAAAAAAAAHhwc3IAM29yZy5hcGFjaGUuZmxpbmsudGFibGUudHlwZXMubG9naWNhbC5TdHJ1Y3R1cmVkVHlwZZb7iqTOB5bC]AgAFWgAOaXNJbnN0YW50...>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyJsonPlan(TableTestBase.scala:750)
	at org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.testIncrementalAggregate(IncrementalAggregateJsonPlanTest.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)

{code}
"	FLINK	Closed	3	1	4624	test-stability
13441367	COALESCE('1', CAST(NULL as varchar)) throws expression type mismatch	"{code}
Flink SQL> SELECT
>     COALESCE('1', cast(NULL as varchar)),
>     COALESCE('4', cast(NULL as varchar), cast(NULL as varchar), cast(NULL as varchar));


Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
Caused by: java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$0, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$1) NOT NULL
expression type is RecordType(CHAR(1) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$0, CHAR(1) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$1) NOT NULL
set is rel#910:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#909,exprs=[COALESCE(_UTF-16LE'1', null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), COALESCE(_UTF-16LE'4', null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")])
expression is LogicalProject(EXPR$0=[_UTF-16LE'1'], EXPR$1=[_UTF-16LE'4'])
  LogicalValues(tuples=[[{ 0 }]])

	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:58)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.flink.table.planner.plan.rules.logical.RemoveUnreachableCoalesceArgumentsRule.onMatch(RemoveUnreachableCoalesceArgumentsRule.java:71)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:164)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:82)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:303)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:179)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:828)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:231)
	at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:561)
	at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:446)
	at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:373)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:330)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:281)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:229)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
{code}"	FLINK	Closed	2	1	4624	pull-request-available
13368519	Reduce the ExecNode scan scope to improve performance when converting json plan to ExecNodeGraph	Change the scan package from {{org.apache.flink}} to {{org.apache.flink.table.planner.plan.nodes.exec}}, which can reduce a lot of useless scan	FLINK	Closed	3	7	4624	pull-request-available
13350567	Separate the implementation of batch window aggregate nodes	"Batch window aggregate nodes include:
BatchExecHashWindowAggregate
BatchExecLocalHashWindowAggregate
BatchExecSortWindowAggregate
BatchExecLocalSortWindowAggregate
BatchExecPythonGroupWindowAggregate"	FLINK	Closed	3	7	4624	pull-request-available
13248697	TumblingGroupWindow should implement toString method	"{code:scala}
  @Test
  def testAllEventTimeTumblingGroupWindowOverTime(): Unit = {
    val util = streamTestUtil()
    val table = util.addDataStream[(Long, Int, String)](
      ""T1"", 'long, 'int, 'string, 'rowtime.rowtime)

    val windowedTable = table
      .window(Tumble over 5.millis on 'rowtime as 'w)
      .groupBy('w)
      .select('int.count)
    util.verifyPlan(windowedTable)
  }
{code}

currently, it's physical plan is 

{code:java}
HashWindowAggregate(window=[TumblingGroupWindow], select=[Final_COUNT(count$0) AS EXPR$0])
+- Exchange(distribution=[single])
   +- LocalHashWindowAggregate(window=[TumblingGroupWindow], select=[Partial_COUNT(int) AS count$0])
      +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])
{code}

we know nothing about the TumblingGroupWindow except its name. the expected plan is

{code:java}
HashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Final_COUNT(count$0) AS EXPR$0])
+- Exchange(distribution=[single])
   +- LocalHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Partial_COUNT(int) AS count$0])
      +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])

{code}

"	FLINK	Resolved	3	1	4624	pull-request-available
13238497	Extracted creation & configuration of FrameworkConfig & RelBuilder to separate class in blink planner	just as commit ([e682395a|https://github.com/apache/flink/commit/e682395ae4e13caba0e2fdd98868f69ede9f3b3e]) in flink planner, do similar things in blink planner	FLINK	Closed	3	4	4624	pull-request-available
13219506	Introduce Flink metadata handlers	"Calcite has defined various metadata handlers(e.g. {{RowCoun}}, {{Selectivity}} and provided default implementation(e.g. {{RelMdRowCount}}, {{RelMdSelectivity}}). However, the default implementation can't completely meet our requirements, e.g. some of its logic is incomplete，and some RelNodes  are not considered.
There are two options to meet our requirements:
option 1. Extends from default implementation, overrides method to improve its logic, add new methods for new {{RelNode}}. The advantage of this option is we just need to focus on the additions and modifications. However, its shortcomings are also obvious: we have no control over the code of non-override methods in default implementation classes especially when upgrading the Calcite version.
option 2. Extends from metadata handler interfaces, reimplement all the logic. Its shortcomings are very obvious, however we can control all the code logic that's what we want.

so we choose option 2!

In this jira, all Flink metadata handles will be introduced, 

including calcite builtin metadata handlers:
{{FlinkRelMdPercentageOriginalRow}},
{{FlinkRelMdNonCumulativeCost}},
{{FlinkRelMdCumulativeCost}},
{{FlinkRelMdRowCount}},
{{FlinkRelMdSize}},
{{FlinkRelMdSelectivity}},
{{FlinkRelMdDistinctRowCoun}},
{{FlinkRelMdPopulationSize}},
{{FlinkRelMdColumnUniqueness}},
{{FlinkRelMdUniqueKeys}},
{{FlinkRelMdDistribution}},

and flink extented metadata handlers:
{{FlinkRelMdColumnInterval}},
{{FlinkRelMdFilteredColumnInterval}},
{{FlinkRelMdColumnNullCount}},
{{FlinkRelMdColumnOriginNullCount}},
{{FlinkRelMdUniqueGroups}},
{{FlinkRelMdModifiedMonotonicity}}
"	FLINK	Closed	3	2	4624	pull-request-available
13362522	When you insert multiple inserts with statementSet, you modify multiple inserts with OPTIONS('table-name '=' XXX '), but only the first one takes effect	"{code:java}
//代码占位符
StatementSet statementSet = tableEnvironment.createStatementSet();
String sql1 = ""insert into test select a,b,c from test_a_12342 /*+
OPTIONS('table-name'='test_a_1')*/"";
String sql2 = ""insert into test select a,b,c from test_a_12342 /*+
OPTIONS('table-name'='test_a_2')*/"";
statementSet.addInsertSql(sql1);
statementSet.addInsertSql(sql2);
statementSet.execute();
{code}
Sql code as above, in the final after the insert is put test_a_1 table data into the two times, and test_a_2 data did not insert, is excuse me this bug"	FLINK	Closed	2	1	4624	pull-request-available
13342364	Rename table.optimizer.union-all-as-breakpoint-disabled to table.optimizer.union-all-as-breakpoint-enabled	{{table.optimizer.union-all-as-breakpoint-disabled}} is defined in {{RelNodeBlockPlanBuilder}}  and is an internal experimental config. While {{disabled}} and {{false}} (as default value) is very obscure.  I suggest to change {{table.optimizer.union-all-as-breakpoint-disabled}} to {{table.optimizer.union-all-as-breakpoint-enabled}} and use {{true}} as default value, which is easier to understand. As this config is an internal experimental config, we don't mark it as deprecated.	FLINK	Closed	3	4	4624	pull-request-available
13394174	Make JaasModule.getAppConfigurationEntries public	Kerberos authentication handler (external project) needs it.	FLINK	Resolved	3	7	4757	pull-request-available
13428736	Support periodic savepointing in the operator	Automatic triggering of savepoints is a commonly requested feature.	FLINK	Closed	3	2	4757	pull-request-available
13536868	Autoscaler min/max parallelism configs should respect the current job parallelism	The autoscaler should never scale the job purely due to max/min parallelism configs. We should adjust these limits to the current parallelism	FLINK	Closed	3	1	4757	pull-request-available
13540595	Support Java records	Reportedly Java records are not supported, because they are neither detected by our Pojo serializer nor supported by Kryo 2.x	FLINK	Closed	3	4	4757	pull-request-available
13543439	Carry over parallelism overrides to prevent users from clearing them on updates	The autoscaler currently sets the parallelism overrides via the Flink config {{pipeline.jobvertex-parallelism-overrides}}. Whenever the user posts specs updates, special care needs to be taken in order to carry over existing overrides. Otherwise the job will reset to the default parallelism configuration. Users shouldn't have to deal with this. Instead, whenever a new spec is posted which does not contain the overrides, the operator should automatically apply the last-used overrides (if autoscaling is enabled).	FLINK	Closed	3	4	4757	pull-request-available
13445707	Add FlinkOperatorEventListener interface	"Currently the [EventUtils|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java] utility that we use to publish events for the operator has an implementation that is tightly coupled with the [Kubernetes Events|https://www.containiq.com/post/kubernetes-events] mechanism.

I suggest to enhance this with a pluggable event interface, which could be implemented by our users to support their event messaging system of choice. "	FLINK	Closed	3	2	4757	pull-request-available
13445640	Add scale subresource	"We should define a scale subresource for the deployment/sessionjob resources that allows us to use the `scale` command or even hook in the HPA.

I suggest to use parallelism as the ""replicas""."	FLINK	Closed	3	2	4757	pull-request-available
13554775	Create TimeMsPerSec metrics for each collector and add metrics across all collectors	"Add the new TimeMsPerSec GC metrics per collector.

Add a new `All` group and aggregate GC metrics across collectors"	FLINK	Closed	4	7	4757	pull-request-available
13526639	JM Deployment recovery logic inconsistent with health related restart 	"The current JM Deployment logic that restarts missing deployments strictly requires HA metadata event for stateless deployments.

This is inconsistent with how the cluster health check related restarts work which can cause the operator to delete an unhealthy deployment and potentially leave it missing if the first deploy attempt fails."	FLINK	Closed	3	1	4757	pull-request-available
13443903	Consider implementing our own status update logic	"If a custom resource version is applied while in the middle of a reconcile loop (for the same resource but previous version) the status update will throw an error and re-trigger reconciliation.

In our case this might be problematic as it would mean we would retry operations that are not necessarily retriable and might require manual user intervention.

Please see: [https://github.com/java-operator-sdk/java-operator-sdk/issues/1198]

I think we should consider implementing our own status update logic that is independent of the current resource version to make the flow more robust."	FLINK	Closed	2	4	4757	pull-request-available
13474305	Add resource type to KubernetesResourceNamespaceMetricGroup scope	"Currently the KubernetesResourceMetricGroup and KubernetesResourceNamespaceMetricGroup doesn't have information about the resource type and different metrics add this differently.

We should unify this and always have the managed resource type FlinkDeployment/FlinkSessionJob in the scope."	FLINK	Closed	3	4	4757	pull-request-available
13556370	Mark new relocated autoscaler configs IGNORE in the operator	"The operator currently only ignores ""kubrernetes.operator"" prefixed configs to not trigger upgrades. Autoscaler configs should also fall in this category."	FLINK	Closed	1	1	4757	pull-request-available
13447227	Handle Upgrade/Deployment errors gracefully	"The operator currently cannot gracefully handle the cases when there is a failure during (or directly after & and before updating the status) job submission.

This applies to both initial cluster submissions when a Flink CR was created but more importantly during upgrades.

This is slightly related to https://issues.apache.org/jira/browse/FLINK-27804 where mid-upgrade observe was disabled to workaround some issues, this logic should also be improved to only skip observing last-state info for already finished jobs (that were observed before).

During upgrades, the observer should be able to recognize when the job/cluster was actually submitted already even if the status update subsequently failed and move the status into a healthy DEPLOYED state."	FLINK	Closed	3	4	4757	pull-request-available
13440311	Document regular CRD upgrade process	"The CRD upgrade documentation ([https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/upgrade/)]. only deals with the case of breaking CRD changes.

It would be important to add a section detailing the regular CRD install/upgrade process using kubectl create/replace as apply simply does not work with a large CRD like this as seen for example here: https://issues.apache.org/jira/browse/FLINK-27288"	FLINK	Closed	3	4	4757	Starter
13543955	Rest api doesn't return minimum resource requirements correctly	The resource requirements returned by the rest api always return a hardcoded 1 lower bound for each vertex.	FLINK	Closed	3	1	4757	pull-request-available
13481685	Some config overrides are ignored when set under spec.flinkConfiguration	"Some [configs|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#resourceuser-configuration] that can be specified under spec.flinkConfiguration won't take affect without an upgrade, e.g.:
 * {{kubernetes.operator.periodic.savepoint.interval}}
 * {{kubernetes.operator.savepoint.format.type}}

These properties are used mainly from the so called 'observeConfig', and won't be available in the operator until the job is restarted. Ideally these should be changed without an upgrade, but at the moment they won't take affect at all.

 

 "	FLINK	Closed	1	1	4757	pull-request-available
13443340	Observer should update last savepoint information directly from cluster too	"The observer should fetch the list checkpoints from the observed job and store the last savepoint into the status directly.

This is especially useful for terminal job states in Flink 1.15 as it allows us to avoid cornercases such as the operator failing after calling cancel-with-savepoint but before updating the status."	FLINK	Closed	1	2	4757	pull-request-available
13527286	Trim autoscaler configMap to not exceed 1mb size limit	"When the {{autoscaler-<deployment_name>}} ConfigMap which is used to persist scaling decisions and metrics becomes too large, the following error is thrown consistently:

{noformat}
io.fabric8.kubernetes.client.KubernetesClientException: Operation: [replace]  for kind: [ConfigMap]  with name: [deployment]  in namespace: [namespace]  failed.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:159)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:169)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:172)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:113)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:41)
    at io.fabric8.kubernetes.client.extension.ResourceAdapter.replace(ResourceAdapter.java:252)
    at org.apache.flink.kubernetes.operator.autoscaler.AutoScalerInfo.replaceInKubernetes(AutoScalerInfo.java:167)
    at org.apache.flink.kubernetes.operator.autoscaler.JobAutoScalerImpl.scale(JobAutoScalerImpl.java:113)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:178)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:130)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:145)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:103)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:102)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: stream was reset: NO_ERROR
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:514)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:551)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleUpdate(OperationSupport.java:347)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleUpdate(BaseOperation.java:680)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:167)
    ... 21 more
Caused by: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
    at okhttp3.internal.http2.Http2Stream.checkOutNotClosed$okhttp(Http2Stream.kt:646)
    at okhttp3.internal.http2.Http2Stream$FramingSink.emitFrame(Http2Stream.kt:557)
    at okhttp3.internal.http2.Http2Stream$FramingSink.write(Http2Stream.kt:532)
    at okio.ForwardingSink.write(ForwardingSink.kt:29)
    at okhttp3.internal.connection.Exchange$RequestBodySink.write(Exchange.kt:218)
    at okio.RealBufferedSink.emitCompleteSegments(RealBufferedSink.kt:255)
    at okio.RealBufferedSink.write(RealBufferedSink.kt:185)
    at okhttp3.RequestBody$Companion$toRequestBody$2.writeTo(RequestBody.kt:152)
    at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.kt:59)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:34)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at org.apache.flink.kubernetes.operator.metrics.KubernetesClientMetrics.intercept(KubernetesClientMetrics.java:130)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)
    at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)
    ... 3 more
    Suppressed: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
        ... 31 more
 {noformat}

We should trim the ConfigMap to not exceed the size limit."	FLINK	Closed	3	1	4757	pull-request-available
13341337	DelimitedInputFormat does not restore compressed filesplits correctly leading to dataloss	"It seems that the delimited input format cannot correctly restore input splits if they belong to compressed files. Basically when a compressed filesplit is restored in the middle, it won't read it anymore leading to dataloss.

The cause of the problem is that for compressed splits that use an inflater stream, the splitlength is set to the magic number -1 which is ignored in the reopen method and causes the split to go to `end` state immediately.

The problem and the fix is shown in this commit:
[https://github.com/gyfora/flink/commit/4adc8ba8d1989fff2db43881c9cb3799848c6e0d]"	FLINK	Closed	2	1	4757	pull-request-available
13526085	Add support for canary resources	While the current health probe mechanism is able to detect different types of errors like startup / informer issues it can be generally beneficial to allow a simply canary mechanism that can verify that the operator recieives updates and reconciles resources in a timely manner.	FLINK	Closed	3	2	4757	pull-request-available
13463009	Successful observe doesn't clear errors due to patching	We set the error to `null` when trying to clear it but due to the patching mechanism this does nothing. It should be set to an empty string instead.	FLINK	Closed	2	1	4757	pull-request-available
13450613	Document periodic savepointing	"We should add a new section to the job management doc page about periodic savepoint triggering [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/#savepoint-management]

 "	FLINK	Closed	3	4	4757	Starter
13458645	Disabling webhook should also disable mutator	"The configuration for the mutating webhook suggests that it is nested inside the (validating) webhook:
https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/values.yaml#L73-L76

Based on this I would expect that if I disable the top level webhook it also disables the mutator, however this is not the case:
https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/templates/webhook.yaml#L19-L79
https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/templates/webhook.yaml#L115-L148

I do not see a use case currently where we would want the mutating webhook without having the validating one, so I suggest following the hierarchy that the helm configs imply. 
"	FLINK	Closed	3	4	4757	pull-request-available
13437447	Merge default flink and operator configuration settings for the operator	"Based on the mailing list discussion : [https://lists.apache.org/thread/pnf2gk9dgqv3qrtszqbfcdxf32t2gr3x]

As a first step we can combine the operators default flink and operator config.

This includes the following changes:
 # Get rid of the DefaultConfig class and replace with a single Configuration object containing the settings for both.
 # Rename OperatorConfigOptions -> KubernetesOperatorConfigOptions
 # Prefix all options with `kubernetes` to get kubernetes.operator.....
 # In the helm chart combine the operatorConfiguration and flinkDefaultConfiguration into a common defaultConfigurationSection. We should still keep the logging settings separately for the two somehow"	FLINK	Closed	3	2	4757	pull-request-available
13304109	Only set offset commit if group id is configured for Kafka Table source	"Currently the KafkaTableSourceBase always creates the KafkaConsumer with the default offset commiting behavior that will try to commit on checkpoints.

However this will fail if group.id is not specified which is an optional parameter.

We should disable offset commiting if group.id wasnt specified."	FLINK	Closed	3	1	4757	pull-request-available
13447033	Do not observe cluster/job mid upgrade	"Seems like in some weird cornercases when we observe the FINISHED job (stopped with savepoint) during an upgrade the recorded last snapshot is incorrect (still need to investigate if this is due to a Flink problem or what) This can lead to upgrade errors.

This can be avoided by simply skipping the observe step when the reconciliation status is UPGRADING because at that point we actually know that the job was already shut down and state recorded correctly in the savepoint info."	FLINK	Closed	2	4	4757	pull-request-available
13429555	Support watching specific namespace for FlinkDeployments	"Currently the operator is watching all the namespaces for FlinkDeployments. We should support configuring it to watch a specific namespace instead.

cc [~mbalassi] "	FLINK	Closed	3	7	4757	pull-request-available
13319928	Yarn ship logic should support files	"The --yarnship / -yt CLI parameter only supports shipping directories. In many cases it would be practical to support shipping single files as well.

Is there any good reason why only directories are supported at the moment?"	FLINK	Closed	3	4	4757	pull-request-available, usability
13477409	Add timestamp to operator resource listener context	In order to make deterministic listener logic easier to implement we should expose a timestamp through the context for both events and status updates.	FLINK	Closed	3	4	4757	pull-request-available
13557608	Improve default autoscaler configs	"There are a few config defaults that should be improved based on prod usage:
 * Metric window : 10 -> 15m
 * Catch up duration: 15 -> 30m
 * Restart time: 3 -> 5m
 * Utilisation boundary: 0.4 -> 0.3

These configs help make the default autoscaler behaviour smoother and less aggressive."	FLINK	Resolved	3	4	4757	pull-request-available
13483779	JobID generation logic could lead to state loss	"The recently added job id override logic (https://issues.apache.org/jira/browse/FLINK-29109) can under certain cases lead to state loss.

State loss scenario:
1. Either first deployment / Stateless upgrade mode used -> new jobId will be generated and set in jobStatus
2. Operator/deployment fails during or directly after successful submission -> status is not persisted with the generated jobId
3. User submits a spec update with last-state upgrade
4.  If the job was never observed (due to a failure or early spec update) a last-state upgrade would be performed, deleting the Deployment and simply submitting the job.
5. The current logic would then generate a new jobid (because it's still empty) leading to a failure to recover the state from HA -> data loss


There are multiple ways to solve this issue:
 a ) Record status after generating a jobid
 b ) Only ever set the status during stateless deployment
 c ) Verify no HA data is present before setting the jobid when empty

Probably the most robust solution is a)."	FLINK	Closed	1	4	4757	pull-request-available
13571144	Do not wait for scaling completion in UPGRADE state with in-place scaling	"The operator currently puts the resource into upgrading state after triggering in-place scaling and keeps observing until the desired parallelism is reached before moving to deployed / stable. 

However this means that due to how the adaptive scheduler works this parallelism may never be reached and this is expected.

We should simplify the logic to consider scaling ""done"" once the resource requirements have been set correctly and then leave the rest to the adaptive scheduler"	FLINK	Resolved	3	4	4757	pull-request-available
13430226	Deletion should remove HA related configmaps also	"At the moment the deletion logic simply deletes the Flink job deployment.
As [~wangyang0918] correctly pointed out in another ticket, deployment deletion does not remove HA related configmaps (this is intentional from Flink side)

This means if the user were to resubmit a new FlinkDeployment under the same name, the job would pick up the HA state and the last checkpoint from the delete resource. This is probably unexpected and we should delete the configmaps after deleting the deployment."	FLINK	Closed	2	7	4757	pull-request-available
13434067	Set ClusterIP service type when watching specific namespaces	"As noted in this PR
[https://github.com/apache/flink-kubernetes-operator/pull/42#issue-1159776739]

Users get service account related error messages unless we set:
{noformat}
kubernetes.rest-service.exposed.type: ClusterIP{noformat}
In cases where we are watching specific namespaces.

We should configure this automatically unless override by the user in the flinkConfiguration for these cases."	FLINK	Closed	3	7	4757	pull-request-available
13533409	test_multi_sessionjob.sh gets stuck very frequently	The test_multi_sessionjob.sh gets stuck almost all the time on recent builds.	FLINK	Closed	1	1	4757	pull-request-available
13543777	Support namespace and flink version specific default configs	"The main goal is to support setting default configuration on a per-namespace and per-flink version level.

This would allow us for example to set config defaults differently for Flink 1.18 (enable adaptive scheduler by default)

Or to use different reconciliation/operator settings for different namespaces.

This can be a very important feature in large scale heterogeneous environments."	FLINK	Closed	3	2	4757	pull-request-available
13527029	Do not scale down operators while processing backlog	"Currently the autoscaler may try to scale down some operators even when the job is struggling to catch up. 

This can lead to a vicious cycle where the backlog keeps increasing. It makes sense to hold off scale down decisions until the job has caught up."	FLINK	Closed	3	4	4757	pull-request-available
13428378	Add basic handling mechanism to deal with job upgrade errors	"There are various different ways how a stateful job upgrade can fail.
For example:
- Failure/timeout during savepoint
- Incompatible state
- Corrupted / not-found checkpoint
- Error after restart

We should allow some strategies for the user to declare how to handle the different error scenarios (such as roll back to earlier state) and what should be treated as a fatal error."	FLINK	Closed	3	2	4757	pull-request-available
12828032	Group by fails on iterative data streams	"Hello!

When I try to run a `groupBy` on an IterativeDataStream I get a NullPointerException. Here is the code that reproduces the issue:

{code}
public Test() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();

    DataStream<Tuple2<Long, Long>> edges = env
            .generateSequence(0, 7)
            .map(new MapFunction<Long, Tuple2<Long, Long>>() {
                @Override
                public Tuple2<Long, Long> map(Long v) throws Exception {
                    return new Tuple2<>(v, (v + 1));
                }
            });

    IterativeDataStream<Tuple2<Long, Long>> iteration = edges.iterate();

    SplitDataStream<Tuple2<Long, Long>> step = iteration.groupBy(1)
            .map(new MapFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>() {
                @Override
                public Tuple2<Long, Long> map(Tuple2<Long, Long> tuple) throws Exception {
                    return tuple;
                }
            })
            .split(new OutputSelector<Tuple2<Long, Long>>() {
                @Override
                public Iterable<String> select(Tuple2<Long, Long> tuple) {
                    List<String> output = new ArrayList<>();
                    output.add(""iterate"");
                    return output;
                }
            });

    iteration.closeWith(step.select(""iterate""));

    env.execute(""Sandbox"");
}
{code}

Moving the groupBy before the iteration solves the issue. e.g. this works:

{code}
... iteration = edges.groupBy(1).iterate();
iteration.map(...)
{code}

Here is the stack trace:

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.flink.streaming.api.graph.StreamGraph.addIterationTail(StreamGraph.java:207)
	at org.apache.flink.streaming.api.datastream.IterativeDataStream.closeWith(IterativeDataStream.java:72)
	at org.apache.flink.graph.streaming.example.Test.<init>(Test.java:73)
	at org.apache.flink.graph.streaming.example.Test.main(Test.java:79)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{code}"	FLINK	Resolved	3	1	4757	iteration, streaming
13433036	Allow definining Operator configuration in Helm chart Values	"Currently the Helm chart hardodes the flink-operator-config and flink-default-config configmap that contains the configurations (logging and yaml).

We should allow the users to define the contents of these config files directly in the helm Values yaml.

Also we should probably rename the flink-conf.yaml config key to operator-conf.yaml in flink-operator-config to not confuse it with flink accidentally."	FLINK	Closed	3	7	4757	pull-request-available
13584916	Simplify job observe logic	There is a fairly complicated listing / observe logic for jobs currently that is no longer necessary as we have a stable logic to always record the jobID in the status before submission.	FLINK	Closed	3	4	4757	pull-request-available
13498682	Support operator leader election	"We should add configurable support to JOSDK leader election:
https://javaoperatorsdk.io/docs/features#leader-election"	FLINK	Closed	3	2	4757	pull-request-available
12915813	DbStateBackendTest	"https://s3.amazonaws.com/archive.travis-ci.org/jobs/92924255/log.txt

{noformat}
Tests run: 5, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 4.505 sec <<< FAILURE! - in org.apache.flink.contrib.streaming.state.DbStateBackendTest
testCompaction(org.apache.flink.contrib.streaming.state.DbStateBackendTest)  Time elapsed: 0.008 sec  <<< ERROR!
java.sql.SQLNonTransientConnectionException: java.net.ConnectException : Error connecting to server localhost on port 1,527 with message Connection refused.
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:208)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:271)
	at org.apache.derby.client.net.OpenSocketAction.run(Unknown Source)
	at org.apache.derby.client.net.OpenSocketAction.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at org.apache.derby.client.net.NetAgent.<init>(Unknown Source)
	at org.apache.derby.client.net.NetConnection.newAgent_(Unknown Source)
	at org.apache.derby.client.am.ClientConnection.<init>(Unknown Source)
	at org.apache.derby.client.net.NetConnection.<init>(Unknown Source)
	at org.apache.derby.client.net.ClientJDBCObjectFactoryImpl.newNetConnection(Unknown Source)
	at org.apache.derby.jdbc.ClientDriver.connect(Unknown Source)
	at java.sql.DriverManager.getConnection(DriverManager.java:571)
	at java.sql.DriverManager.getConnection(DriverManager.java:215)
	at org.apache.flink.contrib.streaming.state.ShardedConnection.<init>(ShardedConnection.java:45)
	at org.apache.flink.contrib.streaming.state.ShardedConnection.<init>(ShardedConnection.java:51)
	at org.apache.flink.contrib.streaming.state.DbBackendConfig.createShardedConnection(DbBackendConfig.java:325)
	at org.apache.flink.contrib.streaming.state.DbStateBackend.initializeForJob(DbStateBackend.java:197)
	at org.apache.flink.contrib.streaming.state.DbStateBackendTest.testCompaction(DbStateBackendTest.java:242)

testSetupAndSerialization(org.apache.flink.contrib.streaming.state.DbStateBackendTest)  Time elapsed: 0.004 sec  <<< ERROR!
java.sql.SQLNonTransientConnectionException: java.net.ConnectException : Error connecting to server localhost on port 1,527 with message Connection refused.
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:208)
	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:271)
	at org.apache.derby.client.net.OpenSocketAction.run(Unknown Source)
	at org.apache.derby.client.net.OpenSocketAction.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at org.apache.derby.client.net.NetAgent.<init>(Unknown Source)
	at org.apache.derby.client.net.NetConnection.newAgent_(Unknown Source)
	at org.apache.derby.client.am.ClientConnection.<init>(Unknown Source)
	at org.apache.derby.client.net.NetConnection.<init>(Unknown Source)
	at org.apache.derby.client.net.ClientJDBCObjectFactoryImpl.newNetConnection(Unknown Source)
	at org.apache.derby.jdbc.ClientDriver.connect(Unknown Source)
	at java.sql.DriverManager.getConnection(DriverManager.java:571)
	at java.sql.DriverManager.getConnection(DriverManager.java:215)
	at org.apache.flink.contrib.streaming.state.ShardedConnection.<init>(ShardedConnection.java:45)
	at org.apache.flink.contrib.streaming.state.ShardedConnection.<init>(ShardedConnection.java:51)
	at org.apache.flink.contrib.streaming.state.DbBackendConfig.createShardedConnection(DbBackendConfig.java:325)
	at org.apache.flink.contrib.streaming.state.DbStateBackend.initializeForJob(DbStateBackend.java:197)
	at org.apache.flink.contrib.streaming.state.DbStateBackendTest.testSetupAndSerialization(DbStateBackendTest.java:105)


Results :

Tests in error: 
  DbStateBackendTest.testCompaction:242 Â» SQLNonTransientConnection java.net.Con...
  DbStateBackendTest.testSetupAndSerialization:105 Â» SQLNonTransientConnection j...
{noformat}"	FLINK	Closed	2	1	4757	test-stability
13517859	Use max busytime instead of average to compute true processing rate	"Currently we use the some of busyTimes and processed records to estimate the true processing rate.

This computation however is not correct when any data skew is present as TPR is not fully additive. The first task to reach 100% utilization will set a limit to the pipeline processing rate through backpressure.

To avoid this we should use the max busyTime and compute the TPR from that."	FLINK	Closed	2	4	4757	pull-request-available
13533679	Upgrade to fabric8 6.5.1 and JOSDK 4.3.0	Update the Kubernetes (fabric8 & josdk) dependencies to the latest versions which contain important fixes.	FLINK	Closed	1	4	4757	pull-request-available
13483853	Name collision: Group already contains a Metric with the name	"k create -f examples/basic-session-deployment-and-job.yaml

results in warnings:
{quote} flink-kubernetes-operator 2022-09-29 13:30:00,001 o.a.f.m.MetricGroup            [WARN ][default/basic-session-job-example] Name collision: Group already contains a Metric with the name  │
│ 'TimeSeconds'. Metric will not be reported.[flink-kubernetes-operator-6f9bbfd557-ljp6w, k8soperator, default, flink-kubernetes-operator, system, Lifecycle, Transition, Resume]            │
│ flink-kubernetes-operator 2022-09-29 13:30:00,001 o.a.f.m.MetricGroup            [WARN ][default/basic-session-job-example] Name collision: Group already contains a Metric with the name  │
│ 'TimeSeconds'. Metric will not be reported.[flink-kubernetes-operator-6f9bbfd557-ljp6w, k8soperator, default, flink-kubernetes-operator, system, Lifecycle, Transition, Upgrade]
{quote}"	FLINK	Closed	3	1	4757	pull-request-available
13579995	Cancel jobs through rest api for last-state upgrades	"The kubernetes operator currently always deletes the JM deployment directly during last-state upgrades instead of attempting any type of graceful shutdown.

We could improve the last-state upgrade logic to cancel the job in cases where the JM is healthy and then simply extract the last checkpoint info through the rest api like we already do for terminal job states.

This would allow the last-state upgrade mode to work even for session jobs and this may even eliminate a few corner cases that can result from the current forceful upgrade mechanism. "	FLINK	Closed	3	4	4757	pull-request-available
13257880	getLatestCheckpoint(true) returns wrong checkpoint	"The flag to prefer checkpoints for recovery introduced in FLINK-11159 returns the wrong checkpoint if:
* checkpoints are preferred  ({{getLatestCheckpoint(true)}}),
* the latest checkpoint is *not* a savepoint,
* more than a single checkpoint is retained.

The current implementation assumes that the latest checkpoint is a savepoint and skips over it. I attached a patch for {{StandaloneCompletedCheckpointStoreTest}} that demonstrates the issue.

You can apply the patch via {{git am -3 < *.patch}}.

 "	FLINK	Resolved	3	1	4757	pull-request-available
13534929	Update to Fabric8 6.5.1+ in flink-kubernetes	"We should update the fabric8 version in flink-kubernetes to at least 6.5.1. 

Flink currently uses a very old fabric8 version. The fabric8 library dependencies have since been revised and greately improved to make them more moduler and allow eliminating securitiy vulnerabilities more easily like: https://issues.apache.org/jira/browse/FLINK-31815

The newer versions especially 6.5.1 + also add some improvement stability fixes for watches and other parts."	FLINK	Closed	3	4	4757	pull-request-available
13535943	When no pod template configured, an invalid null pod template is configured 	"https://issues.apache.org/jira/browse/FLINK-30609 introduced a bug in the podtemplate logic that breaks deployments when no podtemplates are configured.

The basic example doesnt work anymore for example. The reason is that an invalid null object is set as podtemplate when nothing is configured."	FLINK	Closed	1	1	4757	pull-request-available
13569942	Scale down JobManager deployment to 0 before deletion	"We recently improved the JM deployment deletion mechanism, however it seems like task manager pod deletion can get stuck sometimes for a couple of minutes in native mode if we simply try to delete everything at once.

It speeds up the process and leads to cleaner shutdown if we scale down the JM deployment to 0 (shutting down the JM pods first) and then perform the deletion."	FLINK	Closed	3	4	4757	pull-request-available
13585746	Rotate job id on savepoint upgrade as well	Job Ids can and should be rotated on savepoint redeployments as well. It is only important to keep the jobId when we are relying on recovery from HA metadata during last-state restore.	FLINK	Closed	3	4	4757	pull-request-available
13436887	Improve quickstart to use the release helm repo	Once the release is out we can greately improve the quickstart experience by using the stable helm chart from the release repo.	FLINK	Closed	3	7	4757	pull-request-available
13483810	Generate noop.jar instead of committing into source	Previously we decided to simply commit an empty jar file into the project to serve a feature. This might be problematic from a release policy perspective, lets replace with a simple maven plugin	FLINK	Closed	1	4	4757	pull-request-available
13560837	Incorporate GC / Heap metrics in autoscaler decisions	"The autoscaler currently doesn't use any GC/HEAP metrics as part of the scaling decisions. 

While the long term goal may be to support vertical scaling (increasing TM sizes) currently this is out of scope for the autoscaler.

However it is very important to detect cases where the throughput of certain vertices or the entire pipeline is critically affected by long GC pauses. In these cases the current autoscaler logic would wrongly assume a low true processing rate and scale the pipeline too high, ramping up costs and causing further issues.

Using the improved GC metrics introduced in https://issues.apache.org/jira/browse/FLINK-33318 we should measure the GC pauses and simply block scaling decisions if the pipeline spends too much time garbage collecting and notify the user about the required action to increase memory."	FLINK	Closed	3	2	4757	pull-request-available
13469069	Incorrect metric group order for namespaced operator metrics	"The metric group for the resource namespace should follow the main metric group for the metric itself.

So instead of 
{noformat}
flink-kubernetes-operator-64d8cc77c4-w49nj.k8soperator.default.flink-kubernetes-operator.resourcens.default.FlinkDeployment.READY.Count: 1
{noformat}
we should have
{noformat}
flink-kubernetes-operator-64d8cc77c4-w49nj.k8soperator.default.flink-kubernetes-operator.FlinkDeployment.READY.resourcens.default.Count: 1{noformat}"	FLINK	Closed	3	4	4757	pull-request-available
13521255	Support merge by name for podTemplate array fields	"The operator currently merges the hierarchical pod template array fields (containers, volumes, volume mounts etc) by index.

In many cases these array fields already have a name attribute that could be used to merge by name. We should allow this configurable option."	FLINK	Closed	3	2	4757	pull-request-available
13470825	Session Cluster will lost if it failed between status recorded and deploy	"When I test case with https://issues.apache.org/jira/browse/FLINK-28187 
I hit that the session cluster deploy can not be deployed if it fails between status recorded and deploy. Because, in the next reconcile loop, the spec is not detected changed by {{checkNewSpecAlreadyDeployed}}, so it will not try to start the session cluster again. 

The application mode have no problem, because the deployed spec SUSPEND state of the job is not equal to the desired state, so it will try to reconcile the spec change."	FLINK	Closed	2	1	4757	pull-request-available
13482761	SessionJobs are lost when Session FlinkDeployment is upgraded without HA	"Currently SessionJobs are completely lost if the session FlinkDeployment was upgraded and HA wasn't enabled. This is related to FLINK-27979 but its a quite critical manifestation of it.

After that the session job is never restarted and the observer thinks it's ""fine"" and keeps it in the RECONCILING state for some reason."	FLINK	Closed	2	1	4757	pull-request-available
13532910	Incorrect estimation of the target data rate of a vertex when only a subset of its upstream vertex's output is consumed	"Currently, the target data rate of a vertex = SUM(target data rate * input/output ratio) for all of its upstream vertices. This assumes that all output records of an upstream vertex is consumed by the downstream vertex. However, it does not always hold. Consider the following job plan generated by a Flink SQL job. The middle vertex contains multiple chained Calc(select xx) operators, each connecting to a separate downstream sink tasks. As a result, each sink task only consumes a sub-portion of the middle vertex's output.

To fix it, we need operator level edge info to infer the upstream-downstream relationship as well as operator level output metrics. The metrics part is easy but AFAIK, there's no way to get the operator level edge info from the Flink REST API yet.

!image-2023-04-17-23-37-35-280.png!"	FLINK	Closed	3	1	4757	pull-request-available, stale-assigned
13435360	Move jobs to suspended state before upgrading	"We should not upgrade jobs in one step as that might cause us to lose track of what part of the upgrade has succeeded and not.

For example when upgrading with savepoint strategy we need to record the savepoint info in status before trying the deployment because if the deployment fails we lose the savepoint info."	FLINK	Closed	3	7	4757	pull-request-available
13442763	Observing JobManager deployment. Previous status: MISSING	The operator keeps looping if the JM Deployment gets deleted ( and probably when the job is in terminal Flink state such as FAILED). We need to agree on how to handle such cases and fix it.	FLINK	Closed	3	1	4757	pull-request-available
13554594	Use observed true processing rate when source metrics are incorrect	"The aim is to address the cases when Flink incorrectly reports low busy time (high idleness) for sources that are in fact cannot keep up due to the slowness of the reader/fetchers. As the metrics cannot be generally fixed on the Flink - connector side we have to detect this and handle it when collecting the metrics.

The main symptom of this problem is overestimation of the true processing rate and not triggering scaling even if lag is building up as the autoscaler thinks it will be able to keep up.

To tackle this we differentiate two different methods of TPR measurement:
 # *Busy-time based TPR* (this is the current approach in the autoscaler) : computed from incoming records and busy time
 # *Observed TPR* : computed from incoming records and back pressure, measurable only when we assume full processing throughput (i.e during catch-up)

h3. Current behaviour

The operator currently always uses a busy-time based TPR calculation which is very flexible and allows for scaling up / down but is susceptible to overestimation due to the broken metrics.
h3. Suggested new behaviour

Instead of using the busy-time based TPR we detect when TPR is overestimated (busy-time too low) and switch to observed TPR.

To do this, whenever we there is lag for a source (during catchup, or lag-buildup) we measure both busy-time and observed TPR.

If the avg busy-time based TPR is off by a configured amount we switch to observed TPR for this source during metric evaluation.

*Why not use observed TPR all the time?*
Observed TPR can only be measured when we are catching up (during stabilization) or when cannot keep up. This makes it harder to scale down or to detect changes in source throughput over time (before lag starts to build up). Instead of using observed TPR we switch to it only when we detect a problem with the busy-time (this is a rare case overall), to hopefully get the best of both worlds."	FLINK	Closed	2	2	4757	pull-request-available
13448561	Integrate JOSDK metrics with Flink Metrics reporter	"The Java Operator SDK comes with an internal metric interface that could be implemented to forward metrics/measurements to the Flink metric registries. 

We should investigate and implement this if possible."	FLINK	Closed	4	2	4757	Starter, pull-request-available
13542390	Optional startup probe for JM deployment	"There are certain cases where the JM enters a startup crash loop for example due to incorrect HA config setup. With the current operator logic these cases require manual user intervention as we don't have HA metadata available for the last checkpoint and it also seems like the JM actually started already.

To solve this properly we suggest adding a default JM startup probe that queries the rest api (/config) endpoint. "	FLINK	Closed	3	2	4757	pull-request-available
13575831	Deployment recovery is triggered on terminal jobs after jm shutdown ttl	"The deployment recovery mechanism is incorrectly triggered for terminal jobs once the JM deployment is deleted after the TTL period. 

This causes jobs to be resubmitted. This affects only batch jobs.

The workaround is to set 
kubernetes.operator.jm-deployment-recovery.enabled: false

 for batch jobs."	FLINK	Closed	2	1	4757	pull-request-available
13436857	Add CRD compatibility check	"Now with a preview release (almost) out. We should make sure that we do not introduce non-backward compatible changes to the CRD as it can cause serious issues on Kubernetes clusters.

There are 2 aspects to this:
 1. If we introduce a breaking change to already released CRDs we need to bump the version of the CRD (this is not the scope of the ticket)
 2. We should add a verification step in the build ensuring that the CRD changes are compatible

We should try to find a tool that can compare the openAPIV3Schema of the two CRDs and verify compatibility and integrate this into the maven build."	FLINK	Closed	1	4	4757	pull-request-available
13428373	Separate job and deployment errors in FlinkDeployment status	"At the moment the controller does not validate or tolerate any deployment errors such as incorrect configurations etc. Those will lead to an exception loop in the reconcile logic.

There are cases where the job deployment cannot be executed due to incorrect configuration or other causes. In these cases the job can still be running correctly so the job status should be OK but we should signal a deployment error to the user that requires action.

There should be a shared validation logic between the controller and the webhook that should be applied whenever a new FlinkDeployment update is received by the controller. If an error is detected in the controller, set the deployment status to error with a useful message and leave the current job running."	FLINK	Closed	3	7	4757	pull-request-available
13431552	Improve operator logging	"At the moment the way information is logged throughout the operator is very inconsistent. Some parts log the name of the deployment, some the name + namespace, some neither of these.

We should try to clean this up and unify it across the operator.

I see basically 2 possible ways:
 1. Add a log formatter utility to always attach name + namespace information to each logged line
 2. Remove namespace + name from everywhere and extract this as part of the logger setting from MDC information the operator sdk already provides ([https://javaoperatorsdk.io/docs/features)]

We should discuss this on the mailing list as part of this work"	FLINK	Closed	3	7	4757	pull-request-available
13439748	Flink kubernetes operator triggers savepoint failed because of not all tasks running	"{code:java}
2022-04-15 02:38:56,551 o.a.f.k.o.s.FlinkService       [INFO ][default/flink-example-statemachine] Fetching savepoint result with triggerId: 182d7f176496856d7b33fe2f3767da18
2022-04-15 02:38:56,690 o.a.f.k.o.s.FlinkService       [ERROR][default/flink-example-statemachine] Savepoint error
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Source: Custom Source (1/2) of job 00000000000000000000000000000000 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
    at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:143)
    at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:105)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-04-15 02:38:56,693 o.a.f.k.o.o.SavepointObserver  [ERROR][default/flink-example-statemachine] Checkpoint triggering task Source: Custom Source (1/2) of job 00000000000000000000000000000000 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running. {code}
How to reproduce?

Update arbitrary fields(e.g. parallelism) along with {{{}savepointTriggerNonce{}}}.

 

The root cause might be the running state return by {{ClusterClient#listJobs()}} does not mean all the tasks are running."	FLINK	Closed	3	1	4757	pull-request-available
13437403	Bump CRD version to v1beta1	"We should upgrade the CRD version to v1beta1 for both FlinkDeployment and FlinkSessionJob to avoid any conflicts with the preview release.

We should also upgrade the version in all examples and documentation that references it.

We can also consider introducing some tooling to make this easier as we will have to repeate this step at least a few more times."	FLINK	Closed	1	7	4757	pull-request-available
12719805	Add support for null values to the java api	"Currently, many runtime operations fail when encountering a null value. Tuple serialization should allow null fields.

I suggest to add a method to the tuples called `getFieldNotNull()` which throws a meaningful exception when the accessed field is null. That way, we simplify the logic of operators that should not dead with null fields, like key grouping or aggregations.

Even though SQL allows grouping and aggregating of null values, I suggest to exclude this from the java api, because the SQL semantics of aggregating null fields are messy.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/629
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: enhancement, java api, 
Milestone: Release 0.5.1
Created at: Wed Mar 26 00:27:49 CET 2014
State: open
"	FLINK	Resolved	2	4	4757	github-import
13573474	Only scale down JM in Foreground deletion propagation and reduce timeout	"We introduced a logic to scale down the JobManager before the task managers are killed to have a more graceful shutdown sequence.

Currently this is always done in native mode, but it makes more sense to restrict this functionality to foreground deletion. Foreground deletion is the default and recommended propagation but in some cases users may want to use background deletion to speed up the process. In these cases we should not force a synchronous, potentially slow step.

Furthermore instead of using the entire deletion timeout we should reduce this to only use a portion of it as it's an optional step and we should always have keep enough time for the TM shutdown."	FLINK	Closed	3	4	4757	pull-request-available
13494338	Upgrade to Fabric8 6.x.x and JOSDK 4.x.x	In order to get the latest developments from fabric8 and the josdk we should upgrade to the latest version.	FLINK	Closed	3	2	4757	pull-request-available
13482936	Improve observer structure	"The AbstractDeploymentObserver and SessionJobObserver at this point share a lot of common logic due to the unification of other parts.

We should factor out the common parts into an abstract base observer class.

Furthermore we should move the logic of the SavepointObserver into the JobStatusObserver where it logically belongs."	FLINK	Closed	3	4	4757	pull-request-available
13433723	Consider changing flinkVersion to enum type or removing it completely	"Currently the flinkVersion is a string field that we do not use anywhere. There might be some cases in the future where knowing the flink version might be valuable but an optional string field will not work.

We should either make this a required enum of the supported flink versions (I suggest only major.minor -> 1.14, 1.15...) or remove it completely for now.

 "	FLINK	Closed	3	7	4757	pull-request-available
13546399	Reconciliation for autoscaling overrides gets stuck after cancel-with-savepoint	"Since https://issues.apache.org/jira/browse/FLINK-32589 the operator does not rely on the Flink configuration anymore to store the parallelism overrides. Instead, it stores them internally in the autoscaler config map. Upon scalings without the rescaling API, the spec is changed on the fly during reconciliation and the parallelism overrides are added.

Unfortunately, this yields to the cluster getting stuck with the job in FINISHED state after taking a savepoint for upgrade. The operator assumes that the new cluster got deployed successfully and goes into DEPLOYED state again.

Log flow (from oldest to newest):
 # Rescheduling new reconciliation immediately to execute scaling operation.
 # Upgrading/Restarting running job, suspending first...
 # Job is in running state, ready for upgrade with SAVEPOINT
 # Suspending existing deployment.
 # Suspending job with savepoint.
 # Job successfully suspended with savepoint
 # The resource is being upgraded
 # Pending upgrade is already deployed, updating status.
 # Observing JobManager deployment. Previous status: DEPLOYING
 # JobManager deployment port is ready, waiting for the Flink REST API...
 # DEPLOYED The resource is deployed/submitted to Kubernetes, but it’s not yet considered to be stable and might be rolled back in the future

It appears the issue might be in (8): [https://github.com/apache/flink-kubernetes-operator/blob/c09671c5c51277c266b8c45d493317d3be1324c0/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/deployment/AbstractFlinkDeploymentObserver.java#L260] because the generation id hasn't been changed by the mere parallelism override change."	FLINK	Closed	2	1	4757	pull-request-available
13469626	Do not allow last-state upgrades across Flink minor versions	"Since the operator relies on the HA metadata for last-state upgrades we should make sure that it is possible to upgrade between Flink versions using that.

In some cases due to the HA format changes it might not be possible, if so we should force SAVEPOINT upgrade in those cases

cc [~wangyang0918] "	FLINK	Closed	2	4	4757	pull-request-available
13445180	Document new deployment lifecycle features for the operator	"We should document the changes and new features to the core lifecycle management logic, including:
 * JM Deployment Recovery
 * Rollbacks
 * Any changed upgrade behavior
 * 
 "	FLINK	Closed	3	4	4757	pull-request-available
13499732	Make operator more robust to namespaces access errors	With some new features in the JOSDK ([https://javaoperatorsdk.io/docs/patterns-best-practices#stopping-or-not-operator-in-case-of-informer-errors-and-cache-sync-timeouts)]  we can now allow the operator to start even if not all namespace permissions are present.	FLINK	Closed	3	4	4757	pull-request-available
13355181	Support setting namespace in RemoteKeyedStateBackend	Currently, RemoteKeyedStateBackend only support VoidNamespace. We need to add the support for setting namespace so that we can support window state.	FLINK	Resolved	3	7	5059	pull-request-available
13362524	Support Python UDAF in Tumbling Window	Support Python UDAF in Tumbling Window	FLINK	Resolved	3	7	5059	pull-request-available
13427404	PyFlinkEmbeddedSubInterpreterTests. test_udf_without_arguments failed on azure	"
{code:java}
2022-02-08T02:55:16.0701246Z Feb 08 02:55:16 =================================== FAILURES ===================================
2022-02-08T02:55:16.0702483Z Feb 08 02:55:16 ________ PyFlinkEmbeddedSubInterpreterTests.test_udf_without_arguments _________
2022-02-08T02:55:16.0703190Z Feb 08 02:55:16 
2022-02-08T02:55:16.0703959Z Feb 08 02:55:16 self = <pyflink.table.tests.test_udf.PyFlinkEmbeddedSubInterpreterTests testMethod=test_udf_without_arguments>
2022-02-08T02:55:16.0704967Z Feb 08 02:55:16 
2022-02-08T02:55:16.0705639Z Feb 08 02:55:16     def test_udf_without_arguments(self):
2022-02-08T02:55:16.0706641Z Feb 08 02:55:16         one = udf(lambda: 1, result_type=DataTypes.BIGINT(), deterministic=True)
2022-02-08T02:55:16.0707595Z Feb 08 02:55:16         two = udf(lambda: 2, result_type=DataTypes.BIGINT(), deterministic=False)
2022-02-08T02:55:16.0713079Z Feb 08 02:55:16     
2022-02-08T02:55:16.0714866Z Feb 08 02:55:16         table_sink = source_sink_utils.TestAppendSink(['a', 'b'],
2022-02-08T02:55:16.0716495Z Feb 08 02:55:16                                                       [DataTypes.BIGINT(), DataTypes.BIGINT()])
2022-02-08T02:55:16.0717411Z Feb 08 02:55:16         self.t_env.register_table_sink(""Results"", table_sink)
2022-02-08T02:55:16.0718059Z Feb 08 02:55:16     
2022-02-08T02:55:16.0719148Z Feb 08 02:55:16         t = self.t_env.from_elements([(1, 2), (2, 5), (3, 1)], ['a', 'b'])
2022-02-08T02:55:16.0719974Z Feb 08 02:55:16 >       t.select(one(), two()).execute_insert(""Results"").wait()
2022-02-08T02:55:16.0720697Z Feb 08 02:55:16 
2022-02-08T02:55:16.0721294Z Feb 08 02:55:16 pyflink/table/tests/test_udf.py:252: 
2022-02-08T02:55:16.0722119Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0722943Z Feb 08 02:55:16 pyflink/table/table_result.py:76: in wait
2022-02-08T02:55:16.0723686Z Feb 08 02:55:16     get_method(self._j_table_result, ""await"")()
2022-02-08T02:55:16.0725024Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1322: in __call__
2022-02-08T02:55:16.0726044Z Feb 08 02:55:16     answer, self.gateway_client, self.target_id, self.name)
2022-02-08T02:55:16.0726824Z Feb 08 02:55:16 pyflink/util/exceptions.py:146: in deco
2022-02-08T02:55:16.0727569Z Feb 08 02:55:16     return f(*a, **kw)
2022-02-08T02:55:16.0728326Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0728995Z Feb 08 02:55:16 
2022-02-08T02:55:16.0729717Z Feb 08 02:55:16 answer = 'x'
2022-02-08T02:55:16.0730447Z Feb 08 02:55:16 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7fadb5dc97f0>
2022-02-08T02:55:16.0731465Z Feb 08 02:55:16 target_id = 'o26503', name = 'await'
2022-02-08T02:55:16.0732045Z Feb 08 02:55:16 
2022-02-08T02:55:16.0732763Z Feb 08 02:55:16     def get_return_value(answer, gateway_client, target_id=None, name=None):
2022-02-08T02:55:16.0733699Z Feb 08 02:55:16         """"""Converts an answer received from the Java gateway into a Python object.
2022-02-08T02:55:16.0734508Z Feb 08 02:55:16     
2022-02-08T02:55:16.0735205Z Feb 08 02:55:16         For example, string representation of integers are converted to Python
2022-02-08T02:55:16.0736228Z Feb 08 02:55:16         integer, string representation of objects are converted to JavaObject
2022-02-08T02:55:16.0736974Z Feb 08 02:55:16         instances, etc.
2022-02-08T02:55:16.0737508Z Feb 08 02:55:16     
2022-02-08T02:55:16.0738185Z Feb 08 02:55:16         :param answer: the string returned by the Java gateway
2022-02-08T02:55:16.0739074Z Feb 08 02:55:16         :param gateway_client: the gateway client used to communicate with the Java
2022-02-08T02:55:16.0739994Z Feb 08 02:55:16             Gateway. Only necessary if the answer is a reference (e.g., object,
2022-02-08T02:55:16.0740723Z Feb 08 02:55:16             list, map)
2022-02-08T02:55:16.0741491Z Feb 08 02:55:16         :param target_id: the name of the object from which the answer comes from
2022-02-08T02:55:16.0742350Z Feb 08 02:55:16             (e.g., *object1* in `object1.hello()`). Optional.
2022-02-08T02:55:16.0743175Z Feb 08 02:55:16         :param name: the name of the member from which the answer comes from
2022-02-08T02:55:16.0744315Z Feb 08 02:55:16             (e.g., *hello* in `object1.hello()`). Optional.
2022-02-08T02:55:16.0744973Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0745608Z Feb 08 02:55:16         if is_error(answer)[0]:
2022-02-08T02:55:16.0746484Z Feb 08 02:55:16             if len(answer) > 1:
2022-02-08T02:55:16.0747162Z Feb 08 02:55:16                 type = answer[1]
2022-02-08T02:55:16.0747943Z Feb 08 02:55:16                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2022-02-08T02:55:16.0748809Z Feb 08 02:55:16                 if answer[1] == REFERENCE_TYPE:
2022-02-08T02:55:16.0749555Z Feb 08 02:55:16                     raise Py4JJavaError(
2022-02-08T02:55:16.0750313Z Feb 08 02:55:16                         ""An error occurred while calling {0}{1}{2}.\n"".
2022-02-08T02:55:16.0751291Z Feb 08 02:55:16                         format(target_id, ""."", name), value)
2022-02-08T02:55:16.0752004Z Feb 08 02:55:16                 else:
2022-02-08T02:55:16.0752642Z Feb 08 02:55:16                     raise Py4JError(
2022-02-08T02:55:16.0753484Z Feb 08 02:55:16                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
2022-02-08T02:55:16.0754479Z Feb 08 02:55:16                         format(target_id, ""."", name, value))
2022-02-08T02:55:16.0755171Z Feb 08 02:55:16             else:
2022-02-08T02:55:16.0755914Z Feb 08 02:55:16                 raise Py4JError(
2022-02-08T02:55:16.0756575Z Feb 08 02:55:16                     ""An error occurred while calling {0}{1}{2}"".
2022-02-08T02:55:16.0757325Z Feb 08 02:55:16 >                   format(target_id, ""."", name))
2022-02-08T02:55:16.0758178Z Feb 08 02:55:16 E               py4j.protocol.Py4JError: An error occurred while calling o26503.await
2022-02-08T02:55:16.0758918Z Feb 08 02:55:16 
2022-02-08T02:55:16.0760185Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/protocol.py:336: Py4JError
2022-02-08T02:55:16.0761380Z Feb 08 02:55:16 ----------------------------- Captured stdout call -----------------------------
2022-02-08T02:55:16.0762060Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0762752Z Feb 08 02:55:16 # A fatal error has been detected by the Java Runtime Environment:
2022-02-08T02:55:16.0763425Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0764075Z Feb 08 02:55:16 #  SIGSEGV (0xb) at pc=0x00007f417f38bc06, pid=19997, tid=0x00007f4159d73700
2022-02-08T02:55:16.0764867Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0766133Z Feb 08 02:55:16 # JRE version: OpenJDK Runtime Environment (8.0_292-b10) (build 1.8.0_292-8u292-b10-0ubuntu1~16.04.1-b10)
2022-02-08T02:55:16.0767491Z Feb 08 02:55:16 # Java VM: OpenJDK 64-Bit Server VM (25.292-b10 mixed mode linux-amd64 compressed oops)
2022-02-08T02:55:16.0768305Z Feb 08 02:55:16 # Problematic frame:
2022-02-08T02:55:16.0769026Z Feb 08 02:55:16 # C  [libpython3.7m.so.1.0+0x1f5c06]  _PyObject_Malloc+0x76
2022-02-08T02:55:16.0769662Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0770699Z Feb 08 02:55:16 # Core dump written. Default location: /__w/2/s/flink-python/core or core.19997
2022-02-08T02:55:16.0771460Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0772156Z Feb 08 02:55:16 # An error report file with more information is saved as:
2022-02-08T02:55:16.0773210Z Feb 08 02:55:16 # /__w/2/s/flink-python/hs_err_pid19997.log
2022-02-08T02:55:16.0773840Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0774570Z Feb 08 02:55:16 # If you would like to submit a bug report, please visit:
2022-02-08T02:55:16.0775367Z Feb 08 02:55:16 #   http://bugreport.java.com/bugreport/crash.jsp
2022-02-08T02:55:16.0776264Z Feb 08 02:55:16 # The crash happened outside the Java Virtual Machine in native code.
2022-02-08T02:55:16.0777074Z Feb 08 02:55:16 # See problematic frame for where to report the bug.
2022-02-08T02:55:16.0777677Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0778665Z Feb 08 02:55:16 ------------------------------ Captured log call -------------------------------
2022-02-08T02:55:16.0779519Z Feb 08 02:55:16 ERROR    root:java_gateway.py:1056 Exception while sending command.
2022-02-08T02:55:16.0780512Z Feb 08 02:55:16 Traceback (most recent call last):
2022-02-08T02:55:16.0781811Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1224, in send_command
2022-02-08T02:55:16.0782801Z Feb 08 02:55:16     raise Py4JNetworkError(""Answer from Java side is empty"")
2022-02-08T02:55:16.0783648Z Feb 08 02:55:16 py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2022-02-08T02:55:16.0784455Z Feb 08 02:55:16 
2022-02-08T02:55:16.0785200Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0786054Z Feb 08 02:55:16 
2022-02-08T02:55:16.0786654Z Feb 08 02:55:16 Traceback (most recent call last):
2022-02-08T02:55:16.0787982Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1038, in send_command
2022-02-08T02:55:16.0789321Z Feb 08 02:55:16     response = connection.send_command(command)
2022-02-08T02:55:16.0790649Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1229, in send_command
2022-02-08T02:55:16.0791618Z Feb 08 02:55:16     ""Error while receiving"", e, proto.ERROR_ON_RECEIVE)
2022-02-08T02:55:16.0792405Z Feb 08 02:55:16 py4j.protocol.Py4JNetworkError: Error while receiving
2022-02-08T02:55:16.0793242Z Feb 08 02:55:16 ______ PyFlinkStreamUserDefinedFunctionTests.test_execute_from_json_plan _______
2022-02-08T02:55:16.0793915Z Feb 08 02:55:16 
2022-02-08T02:55:16.0794707Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayClient object at 0x7fadb5dc97f0>
2022-02-08T02:55:16.0795354Z Feb 08 02:55:16 
2022-02-08T02:55:16.0796067Z Feb 08 02:55:16     def _get_connection(self):
2022-02-08T02:55:16.0796782Z Feb 08 02:55:16         if not self.is_connected:
2022-02-08T02:55:16.0797537Z Feb 08 02:55:16             raise Py4JNetworkError(""Gateway is not connected."")
2022-02-08T02:55:16.0798233Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0798882Z Feb 08 02:55:16 >           connection = self.deque.pop()
2022-02-08T02:55:16.0799589Z Feb 08 02:55:16 E           IndexError: pop from an empty deque
2022-02-08T02:55:16.0800212Z Feb 08 02:55:16 
2022-02-08T02:55:16.0801304Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:982: IndexError
2022-02-08T02:55:16.0802023Z Feb 08 02:55:16 
2022-02-08T02:55:16.0802720Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0803443Z Feb 08 02:55:16 
2022-02-08T02:55:16.0804235Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayConnection object at 0x7faecec6ecc0>
2022-02-08T02:55:16.0804911Z Feb 08 02:55:16 
2022-02-08T02:55:16.0805463Z Feb 08 02:55:16     def start(self):
2022-02-08T02:55:16.0806319Z Feb 08 02:55:16         """"""Starts the connection by connecting to the `address` and the `port`
2022-02-08T02:55:16.0807034Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0807593Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0808269Z Feb 08 02:55:16 >           self.socket.connect((self.address, self.port))
2022-02-08T02:55:16.0809116Z Feb 08 02:55:16 E           ConnectionRefusedError: [Errno 111] Connection refused
2022-02-08T02:55:16.0809804Z Feb 08 02:55:16 
2022-02-08T02:55:16.0810912Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1132: ConnectionRefusedError
2022-02-08T02:55:16.0811653Z Feb 08 02:55:16 
2022-02-08T02:55:16.0812378Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0813232Z Feb 08 02:55:16 pyflink/testing/test_case_utils.py:137: in setUp
2022-02-08T02:55:16.0814075Z Feb 08 02:55:16     self.t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
2022-02-08T02:55:16.0815068Z Feb 08 02:55:16 pyflink/table/environment_settings.py:267: in in_streaming_mode
2022-02-08T02:55:16.0816018Z Feb 08 02:55:16     get_gateway().jvm.EnvironmentSettings.inStreamingMode())
2022-02-08T02:55:16.0826059Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1712: in __getattr__
2022-02-08T02:55:16.0826592Z Feb 08 02:55:16     ""\n"" + proto.END_COMMAND_PART)
2022-02-08T02:55:16.0827339Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1036: in send_command
2022-02-08T02:55:16.0827853Z Feb 08 02:55:16     connection = self._get_connection()
2022-02-08T02:55:16.0828568Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:984: in _get_connection
2022-02-08T02:55:16.0829089Z Feb 08 02:55:16     connection = self._create_connection()
2022-02-08T02:55:16.0830171Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:990: in _create_connection
2022-02-08T02:55:16.0830673Z Feb 08 02:55:16     connection.start()
2022-02-08T02:55:16.0831220Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0831828Z Feb 08 02:55:16 
2022-02-08T02:55:16.0832255Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayConnection object at 0x7faecec6ecc0>
2022-02-08T02:55:16.0832661Z Feb 08 02:55:16 
2022-02-08T02:55:16.0833006Z Feb 08 02:55:16     def start(self):
2022-02-08T02:55:16.0833469Z Feb 08 02:55:16         """"""Starts the connection by connecting to the `address` and the `port`
2022-02-08T02:55:16.0833921Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0834369Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0834798Z Feb 08 02:55:16             self.socket.connect((self.address, self.port))
2022-02-08T02:55:16.0835281Z Feb 08 02:55:16             self.stream = self.socket.makefile(""rb"")
2022-02-08T02:55:16.0835988Z Feb 08 02:55:16             self.is_connected = True
2022-02-08T02:55:16.0836367Z Feb 08 02:55:16     
2022-02-08T02:55:16.0836734Z Feb 08 02:55:16             self._authenticate_connection()
2022-02-08T02:55:16.0837200Z Feb 08 02:55:16         except Py4JAuthenticationError:
2022-02-08T02:55:16.0837722Z Feb 08 02:55:16             logger.exception(""Cannot authenticate with gateway server."")
2022-02-08T02:55:16.0838161Z Feb 08 02:55:16             raise
2022-02-08T02:55:16.0838559Z Feb 08 02:55:16         except Exception as e:
2022-02-08T02:55:16.0839045Z Feb 08 02:55:16             msg = ""An error occurred while trying to connect to the Java ""\
2022-02-08T02:55:16.0839682Z Feb 08 02:55:16                 ""server ({0}:{1})"".format(self.address, self.port)
2022-02-08T02:55:16.0840168Z Feb 08 02:55:16             logger.exception(msg)
2022-02-08T02:55:16.0840581Z Feb 08 02:55:16 >           raise Py4JNetworkError(msg, e)
2022-02-08T02:55:16.0841178Z Feb 08 02:55:16 E           py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:41141)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30878&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24930"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13284305	Adjust the default value of bundle size and bundle time	"Currently the default value for ""python.fn-execution.bundle.size"" is 1000 and the default value for ""python.fn-execution.bundle.time"" is 1000ms. We should try to find out a meaningful default value which works best in most scenarios."	FLINK	Closed	3	4	5059	pull-request-available
13344175	Support General Python UDF for Map Operation in Python Table API	"Support General Python UDF for Map Operation in Python Table API

The usage:
{code:java}
t = ...  # type: Table, table schema: [a: String, b: Int, c: Int]

# map General Python UDF
map_func = udf(lambda x: Row(x + 1, x * x), 
          result_type=DataTypes.ROW([DataTypes.FIELD(""a"", DataTypes.INT()),
                                     DataTypes.FIELD(""b"", DataTypes.INT())]))
t.map(map_func(t.b)).alias(""a"", ""b"")

{code}"	FLINK	Closed	3	7	5059	pull-request-available
13285540	Introduce FlattenRowCoder to solve the performance issue of __get_item__ in Row	Optimizing the cost of the get item of the Row will gain tremendous improvements in python udf performance	FLINK	Closed	3	7	5059	pull-request-available
13347673	PythonCalcSplitConditionRule is not working as expected	"Currently if users write such a SQL:

`SELECT pyFunc5(f0, f1) FROM (SELECT e.f0, e.f1 FROM (SELECT pyFunc5(a) as e FROM MyTable) where e.f0 is NULL)`

It will be optimized to:

`FlinkLogicalCalc(select=[pyFunc5(pyFunc5(a)) AS f0])
+- FlinkLogicalCalc(select=[a], where=[IS NULL(pyFunc5(a).f0)])
 +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, d)]]], fields=[a, b, c, d])`

The optimized plan is not runnable, we need to fix this."	FLINK	Closed	3	1	5059	pull-request-available
13382721	Exception on JobClient.get_job_status().result()	"Following code finish with exception
{code:java}
table_env.execute_sql(""""""
    CREATE TABLE IF NOT EXISTS datagen (
        id INT,
        data STRING
    ) WITH (
        'connector' = 'datagen'
    )
table_env.execute_sql(""""""
    CREATE TABLE IF NOT EXISTS print (
        id INT,
        data STRING
    ) WITH (
        'connector' = 'print'
    )
""""""){code}
{code:java}
table_result = table_env.execute_sql(""INSERT INTO print SELECT * FROM datagen"")
table_result.get_job_client().get_job_status().result()

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
ValueError: JavaObject id=o125 is not a valid JobStatus

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-16-ee785b26d044> in <module>
----> 1 result.get_job_client().get_job_status().result()

/usr/local/lib/python3.8/dist-packages/pyflink/common/completable_future.py in result(self)
     76             return self._j_completable_future.get()
     77         else:
---> 78             return self._py_class(self._j_completable_future.get())
     79 
     80     def exception(self):

/usr/lib/python3.8/enum.py in __call__(cls, value, names, module, qualname, type, start)
    307         """"""
    308         if names is None:  # simple value lookup
--> 309             return cls.__new__(cls, value)
    310         # otherwise, functional API: we're creating a new Enum type
    311         return cls._create_(value, names, module=module, qualname=qualname, type=type, start=start)

/usr/lib/python3.8/enum.py in __new__(cls, value)
    598                         )
    599             exc.__context__ = ve_exc
--> 600             raise exc
    601 
    602     def _generate_next_value_(name, start, count, last_values):

/usr/lib/python3.8/enum.py in __new__(cls, value)
    582         try:
    583             exc = None
--> 584             result = cls._missing_(value)
    585         except Exception as e:
    586             exc = e

/usr/lib/python3.8/enum.py in _missing_(cls, value)
    611     @classmethod
    612     def _missing_(cls, value):
--> 613         raise ValueError(""%r is not a valid %s"" % (value, cls.__name__))
    614 
    615     def __repr__(self):

ValueError: JavaObject id=o125 is not a valid JobStatus
{code}"	FLINK	Resolved	3	1	5059	pull-request-available
13401492	BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23011

{code}
Sep 15 20:40:43 cls = <class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'>
Sep 15 20:40:43 actual = JavaObject id=o8666
Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']
Sep 15 20:40:43 
Sep 15 20:40:43     @classmethod
Sep 15 20:40:43     def assert_equals(cls, actual, expected):
Sep 15 20:40:43         if isinstance(actual, JavaObject):
Sep 15 20:40:43             actual_py_list = cls.to_py_list(actual)
Sep 15 20:40:43         else:
Sep 15 20:40:43             actual_py_list = actual
Sep 15 20:40:43         actual_py_list.sort()
Sep 15 20:40:43         expected.sort()
Sep 15 20:40:43         assert len(actual_py_list) == len(expected)
Sep 15 20:40:43 >       assert all(x == y for x, y in zip(actual_py_list, expected))
Sep 15 20:40:43 E       AssertionError: assert False
Sep 15 20:40:43 E        +  where False = all(<generator object PyFlinkTestCase.assert_equals.<locals>.<genexpr> at 0x7f792d98b900>)
{code}"	FLINK	Resolved	1	1	5059	pull-request-available, test-stability
13500812	Replace and redesign the Python api documentation base	The doc of the existing python api is difficult to read and use. I have a demo site for redesigning the Python api documentation base. See https://pyflink-api-docs-test.readthedocs.io/en/latest/ as an example.	FLINK	Closed	3	4	5059	pull-request-available
13326411	Add building py38 wheel package of PyFlink in Azure CI	Add building py38 wheel package of PyFlink in Azure CI	FLINK	Closed	3	4	5059	pull-request-available
13404411	PyFlink YARN per-job on Docker test fails on Azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24669&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=23186
{code}
Sep 30 18:20:22 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
Sep 30 18:20:22 	at java.security.AccessController.doPrivileged(Native Method)
Sep 30 18:20:22 	at javax.security.auth.Subject.doAs(Subject.java:422)
Sep 30 18:20:22 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
Sep 30 18:20:22 
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
Sep 30 18:20:22 	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)
Sep 30 18:20:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 30 18:20:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 30 18:20:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 30 18:20:22 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
Sep 30 18:20:22 	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)
Sep 30 18:20:22 	... 17 more

{code}"	FLINK	Resolved	2	1	5059	stale-major, test-stability
13285667	Move the Python scalar operators and table operators to separate package	"Currently both the Python scalar operators and table operators are under the same package org.apache.flink.table.runtime.operators.python. There are already many operators under this package. After introducing the aggregate function support and Vectorized Python function support in the future, there will be more and more operators under the same package. 

We could improve it by the following package structure: org.apache.flink.table.runtime.operators.python.scalar
 org.apache.flink.table.runtime.operators.python.table
org.apache.flink.table.runtime.operators.python.aggregate (in the future)
org.apache.flink.table.runtime.operators.python.scalar.arrow (in the future)

As these classes are internal, it's safe to do so and there are no backwards compatibility issues."	FLINK	Closed	3	4	5059	pull-request-available
13395380	Python test_udf.py fail on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22196&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=1ec6382b-bafe-5817-63ae-eda7d4be718e&l=24542

{code}
Aug 14 23:18:04 =================================== FAILURES ===================================
Aug 14 23:18:04 _______________ PyFlinkStreamUserDefinedFunctionTests.test_open ________________
Aug 14 23:18:04 
Aug 14 23:18:04 self = <pyflink.table.tests.test_udf.PyFlinkStreamUserDefinedFunctionTests testMethod=test_open>
Aug 14 23:18:04 
Aug 14 23:18:04     def test_open(self):
Aug 14 23:18:04         self.t_env.get_config().get_configuration().set_string('python.metric.enabled', 'true')
Aug 14 23:18:04         subtract = udf(Subtract(), result_type=DataTypes.BIGINT())
Aug 14 23:18:04         table_sink = source_sink_utils.TestAppendSink(
Aug 14 23:18:04             ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])
Aug 14 23:18:04         self.t_env.register_table_sink(""Results"", table_sink)
Aug 14 23:18:04     
Aug 14 23:18:04         t = self.t_env.from_elements([(1, 2), (2, 5), (3, 4)], ['a', 'b'])
Aug 14 23:18:04 >       t.select(t.a, subtract(t.b)).execute_insert(""Results"").wait()
Aug 14 23:18:04 
Aug 14 23:18:04 pyflink/table/tests/test_udf.py:226: 
Aug 14 23:18:04 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Aug 14 23:18:04 pyflink/table/table_result.py:76: in wait
Aug 14 23:18:04     get_method(self._j_table_result, ""await"")()
Aug 14 23:18:04 .tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py:1285: in __call__
Aug 14 23:18:04     return_value = get_return_value(
Aug 14 23:18:04 pyflink/util/exceptions.py:146: in deco
Aug 14 23:18:04     return f(*a, **kw)
Aug 14 23:18:04 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
{code}"	FLINK	Closed	3	1	5059	test-stability
13411043	Fix the wrong position mappings in the Python UDTF	"The failed example:
{code:python}
        @udtf(result_types=[DataTypes.STRING(), DataTypes.STRING()])
        def StoTraceMqSourcePlugUDTF(s: str):
            import json
            try:
                data = json.loads(s)
            except Exception as e:
                return None
            source_code = ""trace""
            try:
                shipment_no = data['shipMentNo']
            except Exception as e:
                return None
            yield source_code, shipment_no

        class StoTraceFindNameUDTF(TableFunction):
            def eval(self, shipment_no):
                yield shipment_no, shipment_no

        sto_trace_find_name = udtf(StoTraceFindNameUDTF(),
                                   result_types=[DataTypes.STRING(), DataTypes.STRING()])

        # self.env.set_parallelism(1)
        self.t_env.create_temporary_system_function(
            ""StoTraceMqSourcePlugUDTF"", StoTraceMqSourcePlugUDTF)
        self.t_env.create_temporary_system_function(
            ""sto_trace_find_name"", sto_trace_find_name
        )
        source_table = self.t_env.from_elements([(
            '{""shipMentNo"":""84210186879""}',)],
            ['biz_context'])
        # self.t_env.execute_sql(source_table)
        self.t_env.register_table(""source_table"", source_table)

        t = self.t_env.sql_query(
            ""SELECT biz_context, source_code, shipment_no FROM source_table LEFT JOIN LATERAL TABLE(StoTraceMqSourcePlugUDTF(biz_context)) as T(source_code, shipment_no)""
            "" ON TRUE"")
        self.t_env.register_table(""Table2"", t)
        t = self.t_env.sql_query(
            ""SELECT source_code, shipment_no, shipment_name, shipment_type FROM Table2 LEFT JOIN LATERAL TABLE(sto_trace_find_name(shipment_no)) as T(shipment_name, shipment_type)""
            "" ON TRUE""
        )
        print(t.to_pandas())
{code}
In the failed example, the input arguments of the second Python Table Function has the wrong positions mapping.
"	FLINK	Resolved	3	1	5059	pull-request-available
13296167	Python UDF doesn't work with protobuf 3.6.1	PyFlink UDF execution module is not compatible with protobuf 3.6.1 because it uses a newer interface to access the enum value defined in proto model. We need to fix this.	FLINK	Closed	3	1	5059	pull-request-available
13474822	Cannot run PyFlink 1.16 on MacOS with M1 chip	"I have tested it with 2 m1 machines. i will reproduce the bug 100%.

1.m1 machine
macos bigsur 11.5.1 & jdk8 * & jdk11 & python 3.8 & python 3.9
1.m1 machine
macos monterey 12.1 & jdk8 * & jdk11 & python 3.8 & python 3.9

reproduce step:
1.python -m pip install -r flink-python/dev/dev-requirements.txt
2.cd flink-python; python setup.py sdist bdist_wheel; cd apache-flink-libraries; python setup.py sdist; cd ..;
3.python -m pip install apache-flink-libraries/dist/*.tar.gz
4.python -m pip install dist/*.whl

when run [word_count.py|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/table_api_tutorial/] it will cause


{code:java}
<frozen importlib._bootstrap>:219: RuntimeWarning: apache_beam.coders.coder_impl.StreamCoderImpl size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
Traceback (most recent call last):
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 129, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 49, in word_count
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1637, in _open
    startup_loopback_server()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1628, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 44, in <module>
    from pyflink.fn_execution.beam import beam_sdk_worker_main  # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_sdk_worker_main.py"", line 21, in <module>
    import pyflink.fn_execution.beam.beam_operations # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_operations.py"", line 27, in <module>
    from pyflink.fn_execution.state_impl import RemoteKeyedStateBackend, RemoteOperatorStateBackend
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/state_impl.py"", line 33, in <module>
    from pyflink.fn_execution.beam.beam_coders import FlinkCoder
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_coders.py"", line 27, in <module>
    from pyflink.fn_execution.beam import beam_coder_impl_fast as beam_coder_impl
  File ""pyflink/fn_execution/beam/beam_coder_impl_fast.pyx"", line 1, in init pyflink.fn_execution.beam.beam_coder_impl_fast
KeyError: '__pyx_vtable__'
{code}


"	FLINK	Closed	3	1	5059	pull-request-available
13344564	Introduce StreamExecPythonGroupTableAggregateRule and StreamExecPythonGroupTableAggregate	Introduce StreamExecPythonGroupTableAggregateRule and StreamExecPythonGroupTableAggregate for supporting Stream Python TableAggregate Function.	FLINK	Closed	3	7	5059	pull-request-available
13368986	"Python Test failed with ""OSError: [Errno 12] Cannot allocate memory"""	"https://dev.azure.com/sewen0794/Flink/_build/results?buildId=249&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=443dc6bf-b240-56df-6acf-c882d4b238da&l=21533

Python Test failed with ""OSError: [Errno 12] Cannot allocate memory"" in Azure Pipeline. I am not sure if it is caused by insufficient machine memory on Azure.
"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13414660	Add many kinds of checks in ML Python API	Add many kinds of checks in ML Python API. These checks include pytest, flask8 and mypy.	FLINK	Resolved	3	7	5059	pull-request-available
13344716	StreamArrowPythonGroupWindowAggregateFunctionOperator doesn't handle rowtime and proctime properly	Fix StreamArrowPythonGroupWindowAggregateFunctionOperator's incorrect handling of rowtime and proctime fields	FLINK	Closed	3	1	5059	pull-request-available
13290824	Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls	Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.	FLINK	Closed	3	7	5059	pull-request-available
13329317	Support Pandas Stream Over Window Aggregation	We will add Stream Physical Pandas Over Window RelNode and StreamArrowPythonOverWindowAggregateFunctionOperator to support Pandas Stream Over Window Aggregation	FLINK	Closed	3	7	5059	pull-request-available
13476513	Optimize the Python Execution Mode Documentation	https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/python_execution_mode/	FLINK	Closed	3	4	5059	pull-request-available
13355071	Add internal state hierarchy in PyFlink	!InternalKvState.png!	FLINK	Resolved	3	7	5059	pull-request-available
13355836	Support to specify the input/output types of Python UDFs via string	"Currently, users need to specify the input/output types as following:
{code}
{{@udf(result_type=DataTypes.BIGINT())
def add(i, j):
   return i + j
}}{code}

[FLIP-65|https://cwiki.apache.org/confluence/display/FLINK/FLIP-65%3A+New+type+inference+for+Table+API+UDFs] makes it possible to support syntaxes as following:
{code}
{{@udf(result_type=""BIGINT"")
def add(i, j):
   return i + j
}}{code}"	FLINK	Closed	10200	4	5059	auto-deprioritized-major, auto-deprioritized-minor, auto-unassigned, pull-request-available
13362534	Add NamespacedStateView and PerWindowStateDataViewStore	Currently we only support KeyedStateView and PerKeyStateDataViewStore. We need to Add NamespacedStateView and PerWindowStateDataViewStore for support setting namespace.	FLINK	Resolved	3	7	5059	pull-request-available
13344500	Support FlatMap Operation in Python Table API	"Support Python UDTF for FlatMap Operation in Python Table API

The usage:
{code:java}
t = ...  # type: Table, table schema: [a: String, b: Int, c: Int]

# flat_map Python UDTF
flat_map_func = udtf(lambda x: for i in range(x): yield Row(i + 1, i * i), 
           result_types=[DataTypes.INT(), DataTypes.INT()]
t.flat_map(flat_map_func(t.b)).alias(""a"", ""b""){code}"	FLINK	Closed	3	7	5059	pull-request-available
13369196	Creating python release binaries does not work	"The {{create_binary_release.sh}} fails for creating python binary release:

{code}
[7/7] Cythonizing pyflink/fn_execution/table/window_aggregate_fast.pyx
/home/dwysakowicz/projects/flink/flink-python/dev/.conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/dwysakowicz/projects/flink/flink-python/pyflink/fn_execution/table/window_aggregate_fast.pxd
  tree = Parsing.p_module(s, pxd, full_module_name)
find: 'deps/tmp': No such file or directory
Traceback (most recent call last):
  File ""setup.py"", line 264, in <module>
    subprocess.check_output([collect_licenses_file_sh, TEMP_PATH, TEMP_PATH])
  File ""/home/dwysakowicz/projects/flink/flink-python/dev/.conda/lib/python3.7/subprocess.py"", line 395, in check_output
    **kwargs).stdout
  File ""/home/dwysakowicz/projects/flink/flink-python/dev/.conda/lib/python3.7/subprocess.py"", line 487, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['/home/dwysakowicz/projects/flink/tools/releasing/collect_license_files.sh', 'deps', 'deps']' returned non-zero exit status 1.
{code}

One problem that I was able to fix myself is a wrong path in {{apache-flink-libraries/setup.py}}:

{code}
            collect_licenses_file_sh = os.path.abspath(os.path.join(
                this_directory, "".."", ""tools"", ""releasing"", ""collect_license_files.sh""))
{code}

should be:

{code}
            collect_licenses_file_sh = os.path.abspath(os.path.join(
                this_directory, "".."", "".."", ""tools"", ""releasing"", ""collect_license_files.sh""))
{code}"	FLINK	Resolved	1	1	5059	pull-request-available
13429602	Support Vector and Matrix in ML Python API	We will add class of DenseVector, SparseVector, DenseMatrix and SparseMatrix	FLINK	Closed	3	7	5059	pull-request-available
13470341	Reduce pyflink tests time	Currently, it costs about 1 hour 30mins in pyflink tests. We need to optimize it.	FLINK	Resolved	2	4	5059	pull-request-available
13398850	test_dependency.py fails due to 'Failed to close remote bundle'	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23343&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=23922

{code}
Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:361)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByCount(AbstractPythonFunctionOperator.java:321)
Sep 02 01:34:47 E                   	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.processElement(AbstractStatelessFunctionOperator.java:119)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
Sep 02 01:34:47 E                   	at SourceConversion$38.processElement(Unknown Source)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323)
Sep 02 01:34:47 E                   Caused by: java.lang.RuntimeException: Failed to close remote bundle
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:377)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:361)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$invokeFinishBundle$2(AbstractPythonFunctionOperator.java:340)
Sep 02 01:34:47 E                   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Sep 02 01:34:47 E                   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Sep 02 01:34:47 E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Sep 02 01:34:47 E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Sep 02 01:34:47 E                   	at java.lang.Thread.run(Thread.java:748)
Sep 02 01:34:47 E                   Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
Sep 02 01:34:47 E                       response = task()
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
Sep 02 01:34:47 E                       lambda: self.create_worker().do_instruction(request), request)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 606, in do_instruction
Sep 02 01:34:47 E                       return getattr(self, request_type)(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
Sep 02 01:34:47 E                       bundle_processor.process_bundle(instruction_id))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 999, in process_bundle
Sep 02 01:34:47 E                       input_op_by_transform_id[element.transform_id].process_encoded(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
Sep 02 01:34:47 E                       self.output(decoded_value)
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
Sep 02 01:34:47 E                       self._output_processor.process_outputs(o, self.process_element(value))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/table/operations.py"", line 84, in process_element
Sep 02 01:34:47 E                       return self.func(value)
Sep 02 01:34:47 E                     File ""<string>"", line 1, in <lambda>
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/table/tests/test_dependency.py"", line 53, in plus_two
Sep 02 01:34:47 E                       from test_dependency_manage_lib import add_two
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 161, in exec_module
Sep 02 01:34:47 E                       source_stat, co = _rewrite_test(fn, self.config)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 351, in _rewrite_test
Sep 02 01:34:47 E                       stat = os.stat(fn_)
Sep 02 01:34:47 E                   FileNotFoundError: [Errno 2] No such file or directory: '/tmp/python-dist-266a4f9f-c350-41b8-b437-69b3c67435de/python-files/blob_p-0f11eb68b0611db5e1812d04788ca9c96d8e519c-4ad7d15b4215b93fd53b7ca4ab4bef4e/test_dependency_manage_lib.py'
Sep 02 01:34:47 E                   
Sep 02 01:34:47 E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Sep 02 01:34:47 E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Sep 02 01:34:47 E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:60)
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:375)
Sep 02 01:34:47 E                   	... 7 more
Sep 02 01:34:47 E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
Sep 02 01:34:47 E                       response = task()
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
Sep 02 01:34:47 E                       lambda: self.create_worker().do_instruction(request), request)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 606, in do_instruction
Sep 02 01:34:47 E                       return getattr(self, request_type)(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
Sep 02 01:34:47 E                       bundle_processor.process_bundle(instruction_id))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 999, in process_bundle
Sep 02 01:34:47 E                       input_op_by_transform_id[element.transform_id].process_encoded(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
Sep 02 01:34:47 E                       self.output(decoded_value)
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
Sep 02 01:34:47 E                       self._output_processor.process_outputs(o, self.process_element(value))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/table/operations.py"", line 84, in process_element
Sep 02 01:34:47 E                       return self.func(value)
Sep 02 01:34:47 E                     File ""<string>"", line 1, in <lambda>
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/table/tests/test_dependency.py"", line 53, in plus_two
Sep 02 01:34:47 E                       from test_dependency_manage_lib import add_two
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 161, in exec_module
Sep 02 01:34:47 E                       source_stat, co = _rewrite_test(fn, self.config)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 351, in _rewrite_test
Sep 02 01:34:47 E                       stat = os.stat(fn_)
Sep 02 01:34:47 E                   FileNotFoundError: [Errno 2] No such file or directory: '/tmp/python-dist-266a4f9f-c350-41b8-b437-69b3c67435de/python-files/blob_p-0f11eb68b0611db5e1812d04788ca9c96d8e519c-4ad7d15b4215b93fd53b7ca4ab4bef4e/test_dependency_manage_lib.py'
Sep 02 01:34:47 E                   
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
Sep 02 01:34:47 E                   	... 3 more
{code}"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13375204	Support `print` to print logs in PyFlink	"Currently, if users want to print logs, they need to use logging module.

{code:python}
@udf(result_type=DataTypes.BIGINT())
def add(i, j):
    import logging
    logging.info(""debug"")
    return i + j
{code}
It will be more convenient to use `print` to print logs.

{code: python}
@udf(result_type=DataTypes.BIGINT())
def add(i, j):
    print(""debug"")
    return i + j
{code}
"	FLINK	Resolved	3	2	5059	pull-request-available
13385379	Fix the issue that the InternalRow as arguments in Python UDAF	"The problem is reported from
https://stackoverflow.com/questions/68026832/pyflink-udaf-internalrow-vs-row

In release-1.14, we have reconstructed the coders and fixed this problem. So this problem only appeared in 1.13
"	FLINK	Closed	3	1	5059	pull-request-available
13355063	Add support for general python group window aggregate function in Physical Rule and Node	Add support for general python group window aggregate function in Physical Rule and Node	FLINK	Resolved	3	7	5059	pull-request-available
13419660	Python tests hangs on install dependencies	"{code:java}
Dec 25 04:35:54 py38-cython create: /__w/1/s/flink-python/.tox/py38-cython
Dec 25 04:35:58 py38-cython installdeps: -rdev/dev-requirements.txt, pytest, apache-flink-libraries
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 Process produced no output for 900 seconds.
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 The following Java processes are running (JPS)
Dec 25 04:51:00 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Dec 25 04:51:00 137834 Jps
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 Printing stack trace of Java process 137834
Dec 25 04:51:00 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
137834: No such process
Dec 25 04:51:00 Killing process with pid=725 and all descendants
./flink-python/dev/lint-python.sh: line 580:  2770 Terminated              $TOX_PATH -c $FLINK_PYTHON_DIR/tox.ini --recreate 2>&1
      2771                       | tee -a $LOG_FILE
/__w/1/s/tools/ci/watchdog.sh: line 113:   725 Terminated              $cmd
Dec 25 04:51:00 Process exited with EXIT CODE: 143.
Dec 25 04:51:00 Trying to KILL watchdog (720).
Dec 25 04:51:00 Searching for .dump, .dumpstream and related files in '/__w/1/s'
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '143'.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23407"	FLINK	Resolved	2	1	5059	pull-request-available, test-stability
13258762	Optimize Python UDFs with parameters of constant values	We need support Python UDFs with parameters of constant values. It should be noticed that the constant parameters are not needed to be transferred between the Java operator and the Python worker.	FLINK	Closed	3	7	5059	pull-request-available
13352003	Introduce PythonStreamGroupWindowAggregateOperator	Adds PythonStreamGroupWindowAggregateOperator to support running General Python Stream Group Window Aggregate Function	FLINK	Resolved	3	7	5059	pull-request-available
13351642	Config Python Operator Use Managed Memory In Python DataStream	"Now the way to set `Python DataStream Operator` to use managed memory is to set a hook in the `execute` method of `Python StreamExecutionEnvironment` to traverse the `StreamGraph` and set the `Python Operator` to use managed memory.
But when the user’s job uses `from_data_stream` to convert the `DataStream` to a `Table`, the `TableEnvironment.execute` method is used at the end rather than `StreamExecutionEnvironment.execute`, so the `Python DataStream` related operators will not have `Managed Memory` set."	FLINK	Closed	3	1	5059	pull-request-available
13280640	Improve Python API Tutorial doc	Adds the content of preparing input data in the Python API Tutorial doc	FLINK	Closed	3	4	5059	pull-request-available
13329357	Re-layer Python Operation Make it Possible to Provide only Python implementation	Now whenever we introduce a Python Operation, we need to provide the implementation of python version and cython version, but in many cases, we only need to provide the implementation of Python. We need to re-layer Python Operation make it possible to provide only Python implementation.	FLINK	Closed	3	4	5059	pull-request-available
13532614	Python py37-cython: commands failed, error at setup of ProcessWindowTests.test_count_sliding_window	"{noformat}
Apr 06 05:55:13 ___________________________________ summary ____________________________________
Apr 06 05:55:13 ERROR:   py37-cython: commands failed
Apr 06 05:55:13   py38-cython: commands succeeded
Apr 06 05:55:13   py39-cython: commands succeeded
Apr 06 05:55:13   py310-cython: commands succeeded
{noformat}
in logs there is such error for 37 not sure if it is related
{noformat}
Apr 06 04:26:30 ________ ERROR at setup of ProcessWindowTests.test_count_sliding_window ________
Apr 06 04:26:30 
Apr 06 04:26:30 cls = <class 'pyflink.datastream.tests.test_window.ProcessWindowTests'>
Apr 06 04:26:30 
Apr 06 04:26:30     @classmethod
Apr 06 04:26:30     def setUpClass(cls):
Apr 06 04:26:30         super(PyFlinkStreamingTestCase, cls).setUpClass()
Apr 06 04:26:30         cls.env.set_parallelism(2)
Apr 06 04:26:30 >       cls.env.set_runtime_mode(RuntimeExecutionMode.STREAMING)
Apr 06 04:26:30 
Apr 06 04:26:30 pyflink/testing/test_case_utils.py:193: 
Apr 06 04:26:30 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 {noformat}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47960&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24593]"	FLINK	Open	3	1	5059	stale-assigned, test-stability
13353179	Bump Beam to 2.27.0	"The current Beam version (2.23.0) depends on Netty 4.1.42.Final which suffers the following security issues:
https://nvd.nist.gov/vuln/detail/CVE-2020-11612, fixed in 4.1.46 
https://nvd.nist.gov/vuln/detail/CVE-2019-20445, fixed in 4.1.44 
https://nvd.nist.gov/vuln/detail/CVE-2019-20444, fixed in 4.1.44

The latest Beam (2.27.0) depends on depends Netty 4.1.51.Final. Bump Beam should address this issue."	FLINK	Closed	2	4	5059	pull-request-available
13321230	Python UDTF doesn't work well when the return type isn't generator	"For the following Python UDTF which return type is not a generator:
{code}
# test specify the input_types
@udtf(input_types=[DataTypes.BIGINT()],
           result_types=[DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()])
def split(x):
    return Row(10, 10, 10)
{code}

When used in a job, the operator containing the UDTF will not emit data to the downstream operator and there is also no exception thrown. The job just finished without any result.

We should properly handle this case: either support this use case or throw a proper exception if we don't want to support this case."	FLINK	Closed	3	1	5059	pull-request-available
13348210	Fix NamesTest due to code style refactor	"Due to the [FLINK-20651|https://issues.apache.org/jira/browse/FLINK-20651], the NameTest failed

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11403&view=results]

I will fix it asap"	FLINK	Closed	3	1	5059	pull-request-available, test-stability
13351376	Fix Date/Time/Timestamp in Python DataStream	Currently the Date/Time/Timestamp type doesn't works in Python DataStream.	FLINK	Closed	3	1	5059	pull-request-available
13425074	test_keyed_process_function_with_state of BatchModeDataStreamTests  faild in PyFlink	https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30264&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901	FLINK	Resolved	2	1	5059	pull-request-available, test-stability
13335414	Add Pandas UDAF Doc	Add Pandas UDAF Doc	FLINK	Closed	3	7	5059	pull-request-available
13346477	test_configuration.test_add_all test failed in py35	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10946&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490]
{code:java}
2020-12-17T01:07:04.9062839Z _______________________ ConfigurationTests.test_add_all ________________________
2020-12-17T01:07:04.9063107Z 
2020-12-17T01:07:04.9063436Z self = <pyflink.common.tests.test_configuration.ConfigurationTests testMethod=test_add_all>
2020-12-17T01:07:04.9063719Z 
2020-12-17T01:07:04.9063951Z     def test_add_all(self):
2020-12-17T01:07:04.9064224Z >       conf = Configuration()
2020-12-17T01:07:04.9064411Z 
2020-12-17T01:07:04.9064665Z pyflink/common/tests/test_configuration.py:85: 
2020-12-17T01:07:04.9065074Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-12-17T01:07:04.9065474Z pyflink/common/configuration.py:43: in __init__
2020-12-17T01:07:04.9065765Z     gateway = get_gateway()
2020-12-17T01:07:04.9066065Z pyflink/java_gateway.py:62: in get_gateway
2020-12-17T01:07:04.9066352Z     _gateway = launch_gateway()
2020-12-17T01:07:04.9066671Z pyflink/java_gateway.py:104: in launch_gateway
2020-12-17T01:07:04.9076442Z     p = launch_gateway_server_process(env, args)
2020-12-17T01:07:04.9076987Z pyflink/pyflink_gateway_server.py:197: in launch_gateway_server_process
2020-12-17T01:07:04.9079207Z     download_apache_avro()
2020-12-17T01:07:04.9079558Z pyflink/pyflink_gateway_server.py:129: in download_apache_avro
2020-12-17T01:07:04.9163662Z     cwd=flink_source_root).decode(""utf-8"")
2020-12-17T01:07:04.9164205Z dev/.conda/envs/3.5/lib/python3.5/subprocess.py:316: in check_output
2020-12-17T01:07:04.9164558Z     **kwargs).stdout
2020-12-17T01:07:04.9164887Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-12-17T01:07:04.9165168Z 
2020-12-17T01:07:04.9165396Z input = None, timeout = None, check = True
2020-12-17T01:07:04.9166036Z popenargs = (['mvn', 'help:evaluate', '-Dexpression=avro.version'],)
2020-12-17T01:07:04.9166615Z kwargs = {'cwd': '/__w/1/s', 'stdout': -1}
2020-12-17T01:07:04.9166959Z process = <subprocess.Popen object at 0x7f0ff8a7a320>
2020-12-17T01:07:04.9168169Z stdout = b""[INFO] Scanning for projects...\nDownloading: https://repo.maven.apache.org/maven2/org/apache/apache/20/apache-20.po...R] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :flink-parent\n""
2020-12-17T01:07:04.9169079Z stderr = None, retcode = 1
2020-12-17T01:07:04.9169259Z 
2020-12-17T01:07:04.9169600Z     def run(*popenargs, input=None, timeout=None, check=False, **kwargs):
2020-12-17T01:07:04.9170061Z         """"""Run command with arguments and return a CompletedProcess instance.
2020-12-17T01:07:04.9170373Z     
2020-12-17T01:07:04.9170683Z         The returned instance will have attributes args, returncode, stdout and
2020-12-17T01:07:04.9171117Z         stderr. By default, stdout and stderr are not captured, and those attributes
2020-12-17T01:07:04.9171577Z         will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them.
2020-12-17T01:07:04.9171895Z     
2020-12-17T01:07:04.9172409Z         If check is True and the exit code was non-zero, it raises a
2020-12-17T01:07:04.9172852Z         CalledProcessError. The CalledProcessError object will have the return code
2020-12-17T01:07:04.9173305Z         in the returncode attribute, and output & stderr attributes if those streams
2020-12-17T01:07:04.9173662Z         were captured.
2020-12-17T01:07:04.9173879Z     
2020-12-17T01:07:04.9174175Z         If timeout is given, and the process takes too long, a TimeoutExpired
2020-12-17T01:07:04.9174537Z         exception will be raised.
2020-12-17T01:07:04.9174773Z     
2020-12-17T01:07:04.9175040Z         There is an optional argument ""input"", allowing you to
2020-12-17T01:07:04.9175663Z         pass a string to the subprocess's stdin.  If you use this argument
2020-12-17T01:07:04.9176301Z         you may not also use the Popen constructor's ""stdin"" argument, as
2020-12-17T01:07:04.9176645Z         it will be used internally.
2020-12-17T01:07:04.9176884Z     
2020-12-17T01:07:04.9177163Z         The other arguments are the same as for the Popen constructor.
2020-12-17T01:07:04.9177469Z     
2020-12-17T01:07:04.9177777Z         If universal_newlines=True is passed, the ""input"" argument must be a
2020-12-17T01:07:04.9178337Z         string and stdout/stderr in the returned object will be strings rather than
2020-12-17T01:07:04.9178779Z         bytes.
2020-12-17T01:07:04.9178981Z         """"""
2020-12-17T01:07:04.9179206Z         if input is not None:
2020-12-17T01:07:04.9179677Z             if 'stdin' in kwargs:
2020-12-17T01:07:04.9180236Z                 raise ValueError('stdin and input arguments may not both be used.')
2020-12-17T01:07:04.9180719Z             kwargs['stdin'] = PIPE
2020-12-17T01:07:04.9180942Z     
2020-12-17T01:07:04.9181203Z         with Popen(*popenargs, **kwargs) as process:
2020-12-17T01:07:04.9181505Z             try:
2020-12-17T01:07:04.9181820Z                 stdout, stderr = process.communicate(input, timeout=timeout)
2020-12-17T01:07:04.9182159Z             except TimeoutExpired:
2020-12-17T01:07:04.9182636Z                 process.kill()
2020-12-17T01:07:04.9182936Z                 stdout, stderr = process.communicate()
2020-12-17T01:07:04.9183280Z                 raise TimeoutExpired(process.args, timeout, output=stdout,
2020-12-17T01:07:04.9183718Z                                      stderr=stderr)
2020-12-17T01:07:04.9183997Z             except:
2020-12-17T01:07:04.9184232Z                 process.kill()
2020-12-17T01:07:04.9184491Z                 process.wait()
2020-12-17T01:07:04.9184716Z                 raise
2020-12-17T01:07:04.9184967Z             retcode = process.poll()
2020-12-17T01:07:04.9185256Z             if check and retcode:
2020-12-17T01:07:04.9185573Z                 raise CalledProcessError(retcode, process.args,
2020-12-17T01:07:04.9185943Z >                                        output=stdout, stderr=stderr)
2020-12-17T01:07:04.9186749Z E               subprocess.CalledProcessError: Command '['mvn', 'help:evaluate', '-Dexpression=avro.version']' returned non-zero exit status 1
{code}
 "	FLINK	Resolved	2	1	5059	pull-request-available, stale-assigned, test-stability
13341316	AttributeError: module 'urllib' has no attribute 'parse'	"When executing python udf related examples, it will throw the following exception in constructing python environment:
{code:java}
AttributeError: module 'urllib' has no attribute 'parse'{code}"	FLINK	Closed	3	1	5059	pull-request-available
13321213	Improve the Python documentation about sql	"Now, there are a few documentations are written for java/scala users, which is not convenient for python users to read.
We need to use a more pythonic description in docs."	FLINK	Closed	3	4	5059	pull-request-available
13342239	Liquid Exception: Could not find document 'dev/table/streaming/time_attributes.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/sql/create.zh.md	"When executing the script build_docs.sh, i will throw the following exception:
{code:java}
Liquid Exception: Could not find document 'dev/table/streaming/time_attributes.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/sql/create.zh.md Could not find document 'dev/table/streaming/time_attributes.md' in tag 'link'.
{code}"	FLINK	Closed	3	1	5059	pull-request-available
13326810	Add Python building blocks to make sure the basic functionality of Pandas Batch Group Aggregation could work	"We need to add a few Python building blocks such as PandasAggregateFunctionOperation, PythonAggregateFunction, etc for Pandas Aggregation execution. 

This PR makes sure that a basic end to end Pandas Batch Group Aggregation could be executed."	FLINK	Closed	3	7	5059	pull-request-available
13290730	Use type hints to declare the signature of the methods	[Type Hints|https://www.python.org/dev/peps/pep-0484/] was introduced in Python 3.5 and it would be great if we can declare the signature of the methods using type hints and introduce [type check|https://realpython.com/python-type-checking/] in the python APIs	FLINK	Closed	3	4	5059	pull-request-available
13524963	Chained WindowOperator throws NPE in PyFlink ThreadMode	"Test case
{code:python}
config = Configuration()
config.set_string(""python.execution-mode"", ""process"")
env = StreamExecutionEnvironment.get_execution_environment(config)

class MyTimestampAssigner(TimestampAssigner, ABC):
    def extract_timestamp(self, value: tuple, record_timestamp: int) -> int:
        return value[0]

ds = env.from_collection(
    [(1676461680000, ""a1"", ""b1"", 1), (1676461680000, ""a1"", ""b1"", 1),
     (1676461680000, ""a2"", ""b2"", 1), (1676461680000, ""a1"", ""b2"", 1),
     (1676461740000, ""a1"", ""b1"", 1), (1676461740000, ""a2"", ""b2"", 1)]
).assign_timestamps_and_watermarks(
    WatermarkStrategy.for_monotonous_timestamps().with_timestamp_assigner(MyTimestampAssigner())
)
ds.key_by(
    lambda x: (x[0], x[1], x[2])
).window(
    TumblingEventTimeWindows.of(Time.minutes(1))
).reduce(
    lambda x, y: (x[0], x[1], x[2], x[3] + y[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING(), Types.INT()])
# ).filter(
#     lambda x: x[1] == ""a1""
).map(
    lambda x: (x[0], x[1], x[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.INT()])
).print()
env.execute()
{code}"	FLINK	Resolved	3	1	5059	pull-request-available
13305133	"""Failed to find the file"" in ""build_wheels"" stage"	"CI https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&view=logs&j=fe7ebddc-3e2f-5c50-79ee-226c8653f218&t=b2830442-93c7-50ff-36f4-5b3e2dca8c83

{code}
Successfully built dill crcmod httplib2 hdfs oauth2client future avro-python3
Installing collected packages: six, pbr, mock, dill, typing, crcmod, numpy, pyarrow, python-dateutil, typing-extensions, fastavro, httplib2, protobuf, pymongo, docopt, idna, chardet, urllib3, requests, hdfs, pyparsing, pydot, pyasn1, pyasn1-modules, rsa, oauth2client, grpcio, future, avro-python3, pytz, apache-beam, cython
Successfully installed apache-beam-2.19.0 avro-python3-1.9.2.1 chardet-3.0.4 crcmod-1.7 cython-0.29.16 dill-0.3.1.1 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.29.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.9 mock-2.0.0 numpy-1.18.4 oauth2client-3.0.0 pbr-5.4.5 protobuf-3.11.3 pyarrow-0.15.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.1 pymongo-3.10.1 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 requests-2.23.0 rsa-4.0 six-1.14.0 typing-3.7.4.1 typing-extensions-3.7.4.2 urllib3-1.25.9
+ (( i++ ))
+ (( i<3 ))
+ (( i=0 ))
+ (( i<3 ))
+ /home/vsts/work/1/s/flink-python/dev/.conda/envs/3.5/bin/python setup.py bdist_wheel
Compiling pyflink/fn_execution/fast_coder_impl.pyx because it changed.
Compiling pyflink/fn_execution/fast_operations.pyx because it changed.
[1/2] Cythonizing pyflink/fn_execution/fast_coder_impl.pyx
[2/2] Cythonizing pyflink/fn_execution/fast_operations.pyx
Failed to find the file /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/flink-sql-client_*.jar.
{code}"	FLINK	Closed	3	1	5059	pull-request-available, test-stability
13476330	Optimize the Python DataStream Window Documentation	"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/datastream/operators/windows/
"	FLINK	Resolved	3	4	5059	pull-request-available
13440119	Fix the bug of wrong positions mapping in RowCoder	" !image-2022-04-18-15-12-42-795.png! 
 !image-2022-04-18-15-13-15-045.png! 
 !image-2022-04-18-15-12-58-695.png! "	FLINK	Closed	3	1	5059	pull-request-available
13470075	Python py36-cython: InvocationError for command install_command.sh fails with exit code 1	"{code:java}
Jul 05 03:47:22 Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Jul 05 03:47:32 Using Python version 3.8.13 (default, Mar 28 2022 11:38:47)
Jul 05 03:47:32 pip_test_code.py success!
Jul 05 03:47:32 py38-cython finish: run-test  after 1658.14 seconds
Jul 05 03:47:32 py38-cython start: run-test-post 
Jul 05 03:47:32 py38-cython finish: run-test-post  after 0.00 seconds
Jul 05 03:47:32 ___________________________________ summary ____________________________________
Jul 05 03:47:32 ERROR:   py36-cython: InvocationError for command /__w/3/s/flink-python/dev/install_command.sh --exists-action w .tox/.tmp/package/1/apache-flink-1.15.dev0.zip (exited with code 1)
Jul 05 03:47:32   py37-cython: commands succeeded
Jul 05 03:47:32   py38-cython: commands succeeded
Jul 05 03:47:32 cleanup /__w/3/s/flink-python/.tox/.tmp/package/1/apache-flink-1.15.dev0.zip
Jul 05 03:47:33 ============tox checks... [FAILED]============
Jul 05 03:47:33 Process exited with EXIT CODE: 1.
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37604&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27789"	FLINK	Closed	3	1	5059	stale-assigned, test-stability
13243576	Remove legacy python docs	 Batch (DataSet API)/Transformations and Batch (DataSet API)/Zipping Elements have legacy flink python related documentation	FLINK	Closed	3	4	5059	pull-request-available
13377539	Restructure the coders in PyFlink	Now, PyFlink has introduced many top-level coder. The top-level coder is defined as used by `Python Operation` directly. Currently, some top-level coder comes with the semantics of function, such as `TableFunctionCoder`. In fact, we need to remove this semantic and remove many coders such as `TableFunctionCoder`, `AggregateFunctionCoder` and so on. For the data structure type to be processed, there will only be `FlattenRowCoder`(used in udf, udtf.., the type of data will be List), `TopRowCoder`(used in udaf, udtagg, the type of data will be Row or InternalRow), `RawCoder`(used in datastream, the type of data will be raw data type), `ArrowCoder`(used in pandas udf, the type of data will be pandas data structure).	FLINK	Resolved	3	4	5059	pull-request-available
13462753	'Run kubernetes pyflink application test' fails while pulling image	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37103&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=6592

{code:java}
Jun 23 10:40:35 Flink logs:
Error from server (BadRequest): container ""flink-main-container"" in pod ""flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm"" is waiting to start: image can't be pulled
Jun 23 10:40:35 deployment.apps ""flink-native-k8s-pyflink-application-1"" deleted
Jun 23 10:40:35 clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
Jun 23 10:40:36 pod/flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm condition met
Jun 23 10:40:36 Stopping minikube ...
Jun 23 10:40:36 * Stopping node ""minikube""  ...
Jun 23 10:40:46 * 1 node stopped.
Jun 23 10:40:46 [FAIL] Test script contains errors.
Jun 23 10:40:46 Checking for errors...
Jun 23 10:40:46 No errors in log files.
Jun 23 10:40:46 Checking for exceptions...
Jun 23 10:40:46 No exceptions in log files.
Jun 23 10:40:46 Checking for non-empty .out files...
grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
Jun 23 10:40:46 No non-empty .out files.
Jun 23 10:40:46 

{code}"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13230004	Add simplicity support for submitting Python Table API job in CliFrontend, i.e. `flink run -py wordcount.py` can be work(with simple test).	"Add simplicity support for submitting Python Table API job in CliFrontend, i.e. `flink run -py wordcount.py` can be work(with simple test).   

Support for submitting Python Table API job in CliFrontend，And using `flink run` submit Python Table API job. The current `flink` command command line syntax is as follows:

flink <ACTION> [OPTIONS] [ARGUMENTS]

On the basis of the current `run` ACTION, we add to Python Table API support, specific OPTIONS are as follows:

-py --python  <python-file-name>
Python script with the program entry point. We can configure dependent resources with the `--py-files` option.

* -pyfs --py-files <python-files>   
Attach custom python files for job. Comma can be used as the separator to specify multiple files. The standard python resource file suffixes such as .py/.egg/.zip all also supported.

* -pym --py-module <python-module>  Python module with the program entry point. This option must be used in conjunction with ` --py-files`.

For more details, please refer to [FLIP-38|https://cwiki.apache.org/confluence/display/FLINK/FLIP-38%3A+Python+Table+API]

NOTE: In this JIRA we only need to implement the basic options, without fully implementing the parameters related to UDFs in FLIP-38."	FLINK	Closed	3	7	5059	pull-request-available
13298016	Support Cython Optimizing Python Operations	Support Cython Optimizing Python Operations	FLINK	Closed	3	7	5059	pull-request-available
13316130	Build manylinux1 with better compatibility instead of manylinux2014 Python Wheel Packages	Build manylinux1 with better compatibility instead of manylinux2014 Python Wheel Packages	FLINK	Closed	3	4	5059	pull-request-available
13347223	Support map operation chained together in Python Table API	"{code:java}
tab.map(func1).map(func2){code}
func1 and func2 can be merged into one python function executed in the same Python Operation."	FLINK	Closed	3	7	5059	pull-request-available
13363797	Add apache-flink-libraries module	Since release-1.11, pyflink has introduced cython and provided corresponding wheel packages for different platforms and multiple Python versions. Due to the large size of each wheel package, the entire project space in PyPI has grown very fast, and we need to frequently apply to PyPI for more project space. Please refer to [https://github.com/pypa/pypi-support/issues/831] for more details. The reason why each wheel package is so big is that each wheel package packs several jar packages such as `flink-dist` into it. We'd like to extract these the jar packages into another python project [apache-flink-libraries|https://pypi.org/project/apache-flink-libraries], and then let [apache-flink|https://pypi.org/project/apache-flink/] depends on [apache-flink-libraries|https://pypi.org/project/apache-flink-libraries]. As apache-flink-libraries only contains jar files and so there is only one package for each version. We still need to release multiple wheel packages of  [apache-flink|https://pypi.org/project/apache-flink/]. However, the size will be very small as it doesn't contain jar files any more.	FLINK	Resolved	3	2	5059	pull-request-available
13237397	Add the Python Table API Sphinx docs	"As the Python Table API is added, we should add the Python Table API Sphinx docs. This includes the following work:
1) Add scripts to build the Sphinx docs
2) Add a link in the main page to the generated doc"	FLINK	Closed	3	7	5059	pull-request-available
13284310	Add Python building blocks to make sure the basic functionality of Python TableFunction could work	"We need to add a few Python building blocks such as TableFunctionOperation, TableFunctionRowCoder, etc for Python TableFunction execution. TableFunctionOperation is subclass of Operation in Beam and TableFunctionRowCoder, etc are subclasses of Coder in Beam. These classes will be registered into the Beam’s portability framework to make sure they take effects.

This PR makes sure that a basic end to end Python UDTF could be executed."	FLINK	Closed	3	7	5059	pull-request-available
13315070	Extract the implementation logic of Beam in Operations	Extract the implementation logic of Beam in Operations, so that the implementation of general operations and Beam operations can be decoupled	FLINK	Closed	3	4	5059	pull-request-available
13405842	Python installdeps hangs	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24922&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23587

{code}
Oct 10 02:30:01 py38-cython create: /__w/1/s/flink-python/.tox/py38-cython
Oct 10 02:30:04 py38-cython installdeps: pytest, apache-beam==2.27.0, cython==0.29.16, grpcio>=1.29.0,<2, grpcio-tools>=1.3.5,<=1.14.2, apache-flink-libraries
Oct 10 02:45:22 ==============================================================================
Oct 10 02:45:22 Process produced no output for 900 seconds.
Oct 10 02:45:22 ==============================================================================
{code}"	FLINK	Resolved	2	1	5059	pull-request-available, test-stability
13340949	 Liquid Exception: Could not find document 'dev/table/connectors/kinesis.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/connectors/index.zh.md	"When executing the script build_docs.sh, it will throw the following exception:
{code:java}
Liquid Exception: Could not find document 'dev/table/connectors/kinesis.md' in tag 'link'. Make sure the document exists and the path is correct. in dev/table/connectors/index.zh.md
{code}"	FLINK	Closed	3	1	5059	pull-request-available
13304823	Fix the bug of encoding bytes in cython coder	The python bytes b'x\x00\x00\x00' will be transposed to b'\x'. If we use strlen() of c function to compute the length of char*, we will get wrong length. So we need to use the Python function of len() to compute the length. 	FLINK	Closed	3	1	5059	pull-request-available
13435766	Fix the potential failure of loading library in Thread Mode	The failure occurs in session mode.	FLINK	Resolved	2	1	5059	pull-request-available
13413447	BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount failed on AZP	"The test {{BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount}} fails on AZP with:

{code}
Nov 23 04:20:13 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.167 s <<< FAILURE! - in org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest
Nov 23 04:20:13 [ERROR] testFinishBundleTriggeredByCount  Time elapsed: 0.068 s  <<< FAILURE!
Nov 23 04:20:13 java.lang.AssertionError: expected:<6> but was:<4>
Nov 23 04:20:13 	at org.junit.Assert.fail(Assert.java:89)
Nov 23 04:20:13 	at org.junit.Assert.failNotEquals(Assert.java:835)
Nov 23 04:20:13 	at org.junit.Assert.assertEquals(Assert.java:647)
Nov 23 04:20:13 	at org.junit.Assert.assertEquals(Assert.java:633)
Nov 23 04:20:13 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:80)
Nov 23 04:20:13 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:60)
Nov 23 04:20:13 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.assertOutputEquals(ArrowPythonAggregateFunctionOperatorTestBase.java:62)
Nov 23 04:20:13 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount(BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java:206)
Nov 23 04:20:13 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 23 04:20:13 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 23 04:20:13 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 23 04:20:13 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 23 04:20:13 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Nov 23 04:20:13 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Nov 23 04:20:13 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Nov 23 04:20:13 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Nov 23 04:20:13 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Nov 23 04:20:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Nov 23 04:20:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Nov 23 04:20:13 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Nov 23 04:20:13 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Nov 23 04:20:13 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Nov 23 04:20:13 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Nov 23 04:20:13 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Nov 23 04:20:13 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Nov 23 04:20:13 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Nov 23 04:20:13 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Nov 23 04:20:13 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Nov 23 04:20:13 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Nov 23 04:20:13 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Nov 23 04:20:13 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Nov 23 04:20:13 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Nov 23 04:20:13 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Nov 23 04:20:13 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Nov 23 04:20:13 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Nov 23 04:20:13 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Nov 23 04:20:13 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Nov 23 04:20:13 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Nov 23 04:20:13 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Nov 23 04:20:13 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Nov 23 04:20:13 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Nov 23 04:20:13 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:116)
Nov 23 04:20:13 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Nov 23 04:20:13 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Nov 23 04:20:13 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Nov 23 04:20:13 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26873&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=29294"	FLINK	Closed	1	1	5059	test-stability
13437142	Python EmbeddedThreadDependencyTests.test_add_python_file failed on azure	"
{code:java}
Mar 31 10:49:17 =================================== FAILURES ===================================
Mar 31 10:49:17 ______________ EmbeddedThreadDependencyTests.test_add_python_file ______________
Mar 31 10:49:17 
Mar 31 10:49:17 self = <pyflink.table.tests.test_dependency.EmbeddedThreadDependencyTests testMethod=test_add_python_file>
Mar 31 10:49:17 
Mar 31 10:49:17     def test_add_python_file(self):
Mar 31 10:49:17         python_file_dir = os.path.join(self.tempdir, ""python_file_dir_"" + str(uuid.uuid4()))
Mar 31 10:49:17         os.mkdir(python_file_dir)
Mar 31 10:49:17         python_file_path = os.path.join(python_file_dir, ""test_dependency_manage_lib.py"")
Mar 31 10:49:17         with open(python_file_path, 'w') as f:
Mar 31 10:49:17             f.write(""def add_two(a):\n    raise Exception('This function should not be called!')"")
Mar 31 10:49:17         self.t_env.add_python_file(python_file_path)
Mar 31 10:49:17     
Mar 31 10:49:17         python_file_dir_with_higher_priority = os.path.join(
Mar 31 10:49:17             self.tempdir, ""python_file_dir_"" + str(uuid.uuid4()))
Mar 31 10:49:17         os.mkdir(python_file_dir_with_higher_priority)
Mar 31 10:49:17         python_file_path_higher_priority = os.path.join(python_file_dir_with_higher_priority,
Mar 31 10:49:17                                                         ""test_dependency_manage_lib.py"")
Mar 31 10:49:17         with open(python_file_path_higher_priority, 'w') as f:
Mar 31 10:49:17             f.write(""def add_two(a):\n    return a + 2"")
Mar 31 10:49:17         self.t_env.add_python_file(python_file_path_higher_priority)
Mar 31 10:49:17     
Mar 31 10:49:17         def plus_two(i):
Mar 31 10:49:17             from test_dependency_manage_lib import add_two
Mar 31 10:49:17             return add_two(i)
Mar 31 10:49:17     
Mar 31 10:49:17         self.t_env.create_temporary_system_function(
Mar 31 10:49:17             ""add_two"", udf(plus_two, DataTypes.BIGINT(), DataTypes.BIGINT()))
Mar 31 10:49:17         table_sink = source_sink_utils.TestAppendSink(
Mar 31 10:49:17             ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])
Mar 31 10:49:17         self.t_env.register_table_sink(""Results"", table_sink)
Mar 31 10:49:17         t = self.t_env.from_elements([(1, 2), (2, 5), (3, 1)], ['a', 'b'])
Mar 31 10:49:17 >       t.select(expr.call(""add_two"", t.a), t.a).execute_insert(""Results"").wait()
Mar 31 10:49:17 
Mar 31 10:49:17 pyflink/table/tests/test_dependency.py:63: 
Mar 31 10:49:17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 31 10:49:17 pyflink/table/table_result.py:76: in wait
Mar 31 10:49:17     get_method(self._j_table_result, ""await"")()
Mar 31 10:49:17 .tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py:1321: in __call__

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=27239"	FLINK	Open	2	1	5059	auto-deprioritized-major, stale-assigned, test-stability
13454551	test_es_sink_dynamic failed in jdk11	"
{code:java}
2022-06-21T03:01:35.4707985Z Jun 21 03:01:35 _________________ FlinkElasticsearch7Test.test_es_sink_dynamic _________________
2022-06-21T03:01:35.4709206Z Jun 21 03:01:35 
2022-06-21T03:01:35.4710708Z Jun 21 03:01:35 self = <pyflink.datastream.tests.test_connectors.FlinkElasticsearch7Test testMethod=test_es_sink_dynamic>
2022-06-21T03:01:35.4711754Z Jun 21 03:01:35 
2022-06-21T03:01:35.4712481Z Jun 21 03:01:35     def test_es_sink_dynamic(self):
2022-06-21T03:01:35.4715653Z Jun 21 03:01:35         ds = self.env.from_collection(
2022-06-21T03:01:35.4718082Z Jun 21 03:01:35             [{'name': 'ada', 'id': '1'}, {'name': 'luna', 'id': '2'}],
2022-06-21T03:01:35.4719972Z Jun 21 03:01:35             type_info=Types.MAP(Types.STRING(), Types.STRING()))
2022-06-21T03:01:35.4721209Z Jun 21 03:01:35     
2022-06-21T03:01:35.4722120Z Jun 21 03:01:35 >       es_dynamic_index_sink = Elasticsearch7SinkBuilder() \
2022-06-21T03:01:35.4723876Z Jun 21 03:01:35             .set_emitter(ElasticsearchEmitter.dynamic_index('name', 'id')) \
2022-06-21T03:01:35.4725448Z Jun 21 03:01:35             .set_hosts(['localhost:9200']) \
2022-06-21T03:01:35.4726419Z Jun 21 03:01:35             .build()
2022-06-21T03:01:35.4727430Z Jun 21 03:01:35 
2022-06-21T03:01:35.4877335Z Jun 21 03:01:35 pyflink/datastream/tests/test_connectors.py:132: 
2022-06-21T03:01:35.4882723Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4884972Z Jun 21 03:01:35 pyflink/datastream/connectors/elasticsearch.py:130: in set_hosts
2022-06-21T03:01:35.4886124Z Jun 21 03:01:35     j_http_hosts_array = to_jarray(JHttpHost, j_http_hosts_list)
2022-06-21T03:01:35.4887527Z Jun 21 03:01:35 pyflink/util/java_utils.py:37: in to_jarray
2022-06-21T03:01:35.4888600Z Jun 21 03:01:35     j_arr[i] = arr[i]
2022-06-21T03:01:35.4890812Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:238: in __setitem__
2022-06-21T03:01:35.4892201Z Jun 21 03:01:35     return self.__set_item(key, value)
2022-06-21T03:01:35.4893842Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:221: in __set_item
2022-06-21T03:01:35.4895153Z Jun 21 03:01:35     return get_return_value(answer, self._gateway_client)
2022-06-21T03:01:35.4896282Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4897191Z Jun 21 03:01:35 
2022-06-21T03:01:35.4900656Z Jun 21 03:01:35 answer = 'zsorg.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.ap...haded.py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n'
2022-06-21T03:01:35.4903369Z Jun 21 03:01:35 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f7dd5b8b580>
2022-06-21T03:01:35.4904543Z Jun 21 03:01:35 target_id = None, name = None
2022-06-21T03:01:35.4905404Z Jun 21 03:01:35 
2022-06-21T03:01:35.4906381Z Jun 21 03:01:35     def get_return_value(answer, gateway_client, target_id=None, name=None):
2022-06-21T03:01:35.4908583Z Jun 21 03:01:35         """"""Converts an answer received from the Java gateway into a Python object.
2022-06-21T03:01:35.4909687Z Jun 21 03:01:35     
2022-06-21T03:01:35.4910838Z Jun 21 03:01:35         For example, string representation of integers are converted to Python
2022-06-21T03:01:35.4912061Z Jun 21 03:01:35         integer, string representation of objects are converted to JavaObject
2022-06-21T03:01:35.4913137Z Jun 21 03:01:35         instances, etc.
2022-06-21T03:01:35.4913921Z Jun 21 03:01:35     
2022-06-21T03:01:35.4914859Z Jun 21 03:01:35         :param answer: the string returned by the Java gateway
2022-06-21T03:01:35.4916648Z Jun 21 03:01:35         :param gateway_client: the gateway client used to communicate with the Java
2022-06-21T03:01:35.4918294Z Jun 21 03:01:35             Gateway. Only necessary if the answer is a reference (e.g., object,
2022-06-21T03:01:35.4919591Z Jun 21 03:01:35             list, map)
2022-06-21T03:01:35.4920758Z Jun 21 03:01:35         :param target_id: the name of the object from which the answer comes from
2022-06-21T03:01:35.4921963Z Jun 21 03:01:35             (e.g., *object1* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4923122Z Jun 21 03:01:35         :param name: the name of the member from which the answer comes from
2022-06-21T03:01:35.4924246Z Jun 21 03:01:35             (e.g., *hello* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4925140Z Jun 21 03:01:35         """"""
2022-06-21T03:01:35.4925981Z Jun 21 03:01:35         if is_error(answer)[0]:
2022-06-21T03:01:35.4926846Z Jun 21 03:01:35             if len(answer) > 1:
2022-06-21T03:01:35.4927828Z Jun 21 03:01:35                 type = answer[1]
2022-06-21T03:01:35.4928784Z Jun 21 03:01:35                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2022-06-21T03:01:35.4929792Z Jun 21 03:01:35                 if answer[1] == REFERENCE_TYPE:
2022-06-21T03:01:35.4930858Z Jun 21 03:01:35                     raise Py4JJavaError(
2022-06-21T03:01:35.4931806Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}.\n"".
2022-06-21T03:01:35.4932792Z Jun 21 03:01:35                         format(target_id, ""."", name), value)
2022-06-21T03:01:35.4933346Z Jun 21 03:01:35                 else:
2022-06-21T03:01:35.4933841Z Jun 21 03:01:35 >                   raise Py4JError(
2022-06-21T03:01:35.4934829Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
2022-06-21T03:01:35.4935466Z Jun 21 03:01:35                         format(target_id, ""."", name, value))
2022-06-21T03:01:35.4936110Z Jun 21 03:01:35 E                   py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:
2022-06-21T03:01:35.4937114Z Jun 21 03:01:35 E                   org.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost to org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost
2022-06-21T03:01:35.4938983Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166)
2022-06-21T03:01:35.4940139Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144)
2022-06-21T03:01:35.4941251Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.execute(ArrayCommand.java:97)
2022-06-21T03:01:35.4942313Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2022-06-21T03:01:35.4943334Z Jun 21 03:01:35 E                   	at java.base/java.lang.Thread.run(Thread.java:829)
2022-06-21T03:01:35.4943905Z Jun 21 03:01:35 
2022-06-21T03:01:35.4945225Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/protocol.py:330: Py4JError
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2

"	FLINK	Closed	2	1	5059	pull-request-available, test-stability
13298017	Adds Pipeline of Building Wheel Packages in Azure CI	Adds Pipeline of Building Wheel Packages in Azure CI.	FLINK	Resolved	3	7	5059	pull-request-available
13344509	Introduce PythonStreamGroupTableAggregateOperator	Adds PythonStreamGroupTableAggregateOperator to support running Python TableAggregateFunction	FLINK	Closed	3	7	5059	pull-request-available
13493780	Published metadata for apache-flink in pypi are inconsistent and causes poetry to fail	"Hi, 

Following the debug steps described in [this github thread|https://github.com/python-poetry/poetry/issues/3011] I got to the conclusion that the metadata of the apache-flink 1.16.0 package is wrong. And because of that I cannot properly manage my dependencies using poetry.

I can successfully install it with pip (runs with no errors), as stated in the docs:
{code:java}
python -m pip install apache-flink {code}
But when I try to include the dependency in my poetry project I got the following error:

 
{code:java}
❯ poetry add apache-flink@1.16.0Updating dependencies
Resolving dependencies... (2.0s)Because pemja (0.2.6) depends on numpy (1.21.4)
 and apache-flink (1.16.0) depends on numpy (>=1.14.3,<1.20), pemja (0.2.6) is incompatible with apache-flink (1.16.0).
So, because cv-features depends on apache-flink (1.16.0) which depends on pemja (0.2.6), version solving failed. {code}
 

I've followed the same debug steps as in [this github thread|https://github.com/python-poetry/poetry/issues/3011] and can confirm that apache-flink has exactly the same problem as described in the thread: the wheel package in pypi has correct dependency metadata but the pypi published don't.

 
{code:java}
❯ pkginfo -f requires_dist /Users/andre/Downloads/apache_flink-1.16.0-cp39-cp39-macosx_11_0_arm64.whl
requires_dist: ['py4j (==0.10.9.3)', 'python-dateutil (==2.8.0)', 'apache-beam (==2.38.0)', 'cloudpickle (==2.1.0)', 'avro-python3 (!=1.9.2,<1.10.0,>=1.8.1)', 'pytz (>=2018.3)', 'fastavro (<1.4.8,>=1.1.0)', 'requests (>=2.26.0)', 'protobuf (<3.18)', 'httplib2 (<=0.20.4,>=0.19.0)', 'apache-flink-libraries (<1.16.1,>=1.16.0)', 'numpy (<1.22.0,>=1.21.4)', 'pandas (<1.4.0,>=1.3.0)', 'pyarrow (<9.0.0,>=5.0.0)', 'pemja (==0.2.6) ; python_full_version >= ""3.7"" and platform_system != ""Windows""'] {code}
but the pipy json metadata is wrong:

 

 
{code:java}
❯ curl -sL https://pypi.org/pypi/apache-flink/json | jq '.info.requires_dist'[
  ""py4j (==0.10.9.3)"",
  ""python-dateutil (==2.8.0)"",
  ""apache-beam (==2.38.0)"",
  ""cloudpickle (==2.1.0)"",
  ""avro-python3 (!=1.9.2,<1.10.0,>=1.8.1)"",
  ""pytz (>=2018.3)"",
  ""fastavro (<1.4.8,>=1.1.0)"",
  ""requests (>=2.26.0)"",
  ""protobuf (<3.18)"",
  ""httplib2 (<=0.20.4,>=0.19.0)"",
  ""apache-flink-libraries (<1.16.1,>=1.16.0)"",
  ""numpy (<1.20,>=1.14.3)"",
  ""pandas (<1.2.0,>=1.0)"",
  ""pyarrow (<7.0.0,>=0.15.1)"",
  ""pemja (==0.2.6) ; python_full_version >= \""3.7\"" and platform_system != \""Windows\""""
]{code}
 

As per [this comment|https://github.com/python-poetry/poetry/issues/3011#issuecomment-702826616], could you please republish the package correcting this metadata information, please? This [other comment|https://github.com/apple/turicreate/issues/3342#issuecomment-702957550] can help gain more context.

 

Thanks

 

 "	FLINK	Closed	2	1	5059	pull-request-available
13266725	Data types defined in DDL will lose precision and nullability when converting to properties	"Currently, data types defined in DDL will be converted to {{TypeInformation}} and use {{TypeStringUtils}} to serialize/deserialize which will lose the precision and nullablitiy information. 

We can use {{LogicalType#asSerializableString}} and {{LogicalTypeParser}} to serialize/deserialize data types which keeps all the information. But we need to figure out how to keep compability with previous versions."	FLINK	Resolved	2	7	5351	pull-request-available
13296427	Support LookupTableSource in planner	Support the {{LookupTableSource}} interface in planner. Utility methods for the data structure converters might not be implemented yet.	FLINK	Closed	3	7	5351	pull-request-available
13529111	Move query SqlNode conversion logic to SqlQueryConverter	"Introduce {{SqlQueryConverter}} and move the conversion logic of query {{SqlNode}} -> {{PlannerQueryOption}} to it.

Note that the conversion is complex for query SqlNodes because it's unclear what the specific query SqlNode classes are. But they should all belong to the {{SqlKind.QUERY}}. Therefore, we can introduce a new mapping path for the SqlKind. 

{code:java}
public interface SqlNodeConverter<S extends SqlNode> {
    /**
     * Returns the {@link SqlKind SqlKinds} of {@link SqlNode SqlNodes} that the {@link
     * SqlNodeConverter} supports to convert.
     *
     * <p>If a {@link SqlNodeConverter} return s a non-empty SqlKinds, the conversion framework
     * prefer to match SqlKind of SqlNode instead of matching class of SqlNode.
     *
     * @see SqlQueryConverter
     */
    default Optional<EnumSet<SqlKind>> supportedSqlKinds() {
        return Optional.empty();
    }

   ...

}
{code}



"	FLINK	Closed	3	7	5351	pull-request-available
13304938	Add createTypeInformation to DynamicTableSink#Context	"Currently, we have {{createTypeInformation}} on {{DynamicTableSource#Context}}, but not on {{DynamicTableSink#Context}}.
In some sink connectors, we need to buffer the {{RowData}} and flush at some interval. So we need to copy the {{RowData}} if object reuse is enabled, then the internal TypeInformation is needed to get the RowData TypeSerializer#copy. "	FLINK	Closed	3	7	5351	pull-request-available
13362551	ALTER TABLE statement enhancement	"We already introduced ALTER TABLE statement in FLIP-69 [1], but only support to rename table name and change table options. One useful feature of ALTER TABLE statement is modifying schema. This is also heavily required by integration with data lakes (e.g. iceberg).

Therefore, I propose to support following ALTER TABLE statements (except {{SET}} and {{{}RENAME TO{}}}, others are all new introduced syntax):
{code:sql}
ALTER TABLE table_name {
    ADD { <schema_component> | (<schema_component> [, ...]) }
  | MODIFY { <schema_component> | (<schema_component> [, ...]) }
  | DROP {column_name | (column_name, column_name, ....) | PRIMARY KEY | CONSTRAINT constraint_name | WATERMARK}
  | RENAME old_column_name TO new_column_name
  | RENAME TO new_table_name
  | SET (key1=val1, ...)
  | RESET (key1, ...)
}

<schema_component>::
  { <column_component> | <constraint_component> | <watermark_component> }

<column_component>::
  column_name <column_definition> [FIRST | AFTER column_name]

<constraint_component>::
  [CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED

<watermark_component>::
  WATERMARK FOR rowtime_column_name AS watermark_strategy_expression

<column_definition>::
  { <physical_column_definition> | <metadata_column_definition> | <computed_column_definition> } [COMMENT column_comment]

<physical_column_definition>::
  column_type

<metadata_column_definition>::
  column_type METADATA [ FROM metadata_key ] [ VIRTUAL ]

<computed_column_definition>::
  AS computed_column_expression
{code}
And some examples:
{code:sql}
-- add a new column 
ALTER TABLE mytable ADD new_column STRING COMMENT 'new_column docs';

-- add columns, constraint, and watermark
ALTER TABLE mytable ADD (
    log_ts STRING COMMENT 'log timestamp string' FIRST,
    ts AS TO_TIMESTAMP(log_ts) AFTER log_ts,
    PRIMARY KEY (id) NOT ENFORCED,
    WATERMARK FOR ts AS ts - INTERVAL '3' SECOND
);

-- modify a column type
ALTER TABLE prod.db.sample MODIFY measurement double COMMENT 'unit is bytes per second' AFTER `id`;

-- modify definition of column log_ts and ts, primary key, watermark. they must exist in table schema
ALTER TABLE mytable ADD (
    log_ts STRING COMMENT 'log timestamp string' AFTER `id`,  -- reoder columns
    ts AS TO_TIMESTAMP(log_ts) AFTER log_ts,
    PRIMARY KEY (id) NOT ENFORCED,
    WATERMARK FOR ts AS ts - INTERVAL '3' SECOND
);

-- drop an old column
ALTER TABLE prod.db.sample DROP measurement;

-- drop columns
ALTER TABLE prod.db.sample DROP (col1, col2, col3);

-- drop a watermark
ALTER TABLE prod.db.sample DROP WATERMARK;

-- rename column name
ALTER TABLE prod.db.sample RENAME `data` TO payload;

-- rename table name
ALTER TABLE mytable RENAME TO mytable2;

-- set options
ALTER TABLE kafka_table SET (
    'scan.startup.mode' = 'specific-offsets', 
    'scan.startup.specific-offsets' = 'partition:0,offset:42'
);

-- reset options
ALTER TABLE kafka_table RESET ('scan.startup.mode', 'scan.startup.specific-offsets');
{code}
Note: we don't need to introduce new interfaces, because all the alter table operation will be forward to catalog through {{{}Catalog#alterTable(tablePath, newTable, ignoreIfNotExists){}}}.

[1]: [https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/alter/#alter-table]
[2]: [http://iceberg.apache.org/spark-ddl/#alter-table-alter-column]
[3]: [https://trino.io/docs/current/sql/alter-table.html]
[4]: [https://dev.mysql.com/doc/refman/8.0/en/alter-table.html]
[5]: [https://www.postgresql.org/docs/9.1/sql-altertable.html]"	FLINK	In Progress	3	2	5351	auto-unassigned, stale-assigned
13528910	Introduce SqlNodeConverter for SqlToOperationConverter	"Introduce {{SqlNodeConverter}} for {{SqlToOperationConverter}}, following Timo's idea in FLINK-31368
class like:

{code:java}
public interface SqlNodeConverter<S extends SqlNode> {

    Operation convertSqlNode(S node, ConvertContext context);

}


/** Registry of SqlNode converters. */
public class SqlNodeConverters {

    private static final Map<Class<?>, SqlNodeConverter<?>> CONVERTERS = new HashMap<>();

    static {
        // register all the converters here
        register(new SqlCreateCatalogConverter());
    }
}
{code}"	FLINK	Closed	3	7	5351	pull-request-available
13375757	TUMBLE TVF should throw helpful exception when specifying second interval parameter	"Currently, the following query can run and no exception is thrown. 

However, the second interval parameter (i.e. the offset parameter) is not supported yet. We should throw a exception for this. 


{code:sql}
select 
  date_format(window_end, 'yyyy-MM-dd') as date_str,
  date_format(window_end, 'HH:mm') as time_str,
  count(distinct user_id) as uv
from table(tumble(table user_behavior, descriptor(ts), interval '10' minute, interval '1' day))
group by window_start, window_end;
{code}"	FLINK	Closed	3	1	5351	pull-request-available
13233143	Fix AsyncLookupJoin doesn't close all generated ResultFutures	"There is a fragile test in AsyncLookupJoinITCase, that not all the udfs are closed at the end.

{code:java}
02:40:48.787 [ERROR] Tests run: 22, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 47.098 s <<< FAILURE! - in org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase
02:40:48.791 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=HEAP](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase)  Time elapsed: 1.266 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<2>
	at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)

02:40:48.794 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=ROCKSDB](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase)  Time elapsed: 1.033 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<2>
	at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)
{code}
"	FLINK	Closed	3	1	5351	pull-request-available
13307160	LIMIT queries are failed when adding sleeping time of async checkpoint	When we change the timing of operations (sleep after emit first record and sleep for async operation of checkpoint) with this [commit|https://github.com/apache/flink/commit/c05a0d865989c9959047cebcf2cd68b3838cc699], the test {{org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase#testDifferentTypesSumWithRetract}} in flink-table-planner-blink is failed. 	FLINK	Closed	1	1	5351	pull-request-available
13315848	StreamExecutionEnvironment#addSource(SourceFunction, TypeInformation) doesn't use the user defined type information	"
{code:java}
class MySource<T> implements SourceFunction<T>, ResultTypeQueryable<T> {
 TypeInformation getProducedType() {
   return TypeExtractor.createTypeInfo(SourceFunction.class, this.getClass(), 0, null, null);
 } 
}

DataStream ds = tEnv.addSource(new MySource(), Types.ROW(Types.STRING))
{code}

The returned {{TypeInformation}} of {{MySource}} is {{GenericTypeInfo}}, not the user given {{RowTypeInfo}}.


It seems that {{StreamExecutionEnvironment#getTypeInfo}} doesn't use the user given {{typeInfo}} in the highest priority. "	FLINK	Closed	2	1	5351	pull-request-available
13287407	Generic type can not be matched when convert table to stream.	"The query result schema printed by table.printSchema():
{noformat}
 |-- deviceId: BIGINT
 |-- channel: STRING
 |-- schemaId: BIGINT
 |-- productId: BIGINT
 |-- schema: LEGACY('RAW', 'ANY<com.yunmo.iot.schema.Schema>')
{noformat}
then excuting table.toRetractStream[DeviceSchema].print(), exception throwed:
{noformat}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink do not match.
 Query schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: RAW('com.yunmo.iot.schema.Schema', ?)]
 Sink schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: LEGACY('RAW', 'ANY<com.yunmo.iot.schema.Schema>')]{noformat}
The com.yunmo.iot.schema.Schema is a generic type.

The schema field of Query schema change from LEGACY('RAW' to RAW, but the Sink schema still a LEGACY('RAW'"	FLINK	Resolved	3	1	5351	pull-request-available
13329522	Fix ArrayIndexOutOfBoundsException when executing DELETE statement in JDBC upsert sink	"We found  that the primary key position can cause  ArrayIndexOutOfBoundsException

the sink like that( the primary key select the position of 1, 3):
{code:java}
CREATE TABLE `test`(
  col1 STRING, 
  col2 STRING, 
  col3 STRING, 
  PRIMARY KEY (col1, col3) NOT ENFORCED ) WITH (
  'connector' = 'jdbc',
  ...
){code}
when the DELETE (cdc message) come , it will raise ArrayIndexOutOfBoundsException:
{code:java}
Caused by: java.lang.RuntimeException: Writing records to JDBC failed.    ... 10 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: 2    at org.apache.flink.table.data.GenericRowData.getString(GenericRowData.java:169)    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$245ca7d1$1(RowData.java:310)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.getPrimaryKey(JdbcDynamicOutputFormatBuilder.java:216)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.lambda$createRowKeyExtractor$7(JdbcDynamicOutputFormatBuilder.java:193)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.lambda$createKeyedRowExecutor$3fd497bb$1(JdbcDynamicOutputFormatBuilder.java:128)    at org.apache.flink.connector.jdbc.internal.executor.KeyedBatchStatementExecutor.executeBatch(KeyedBatchStatementExecutor.java:71)    at org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor.executeBatch(BufferReduceStatementExecutor.java:99)    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:200)    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:171)    ... 8 more
{code}
 "	FLINK	Closed	2	1	5351	pull-request-available
13223246	Introduce TableImpl and remove Table in flink-table-planner-blink	"After FLINK-11068 is merged, the {{Table}} interfaced is added into {{flink-table-api-java}}. The classpath is conflicted with {{Table}} in {{flink-table-planner-blink}} which result in IDE errors and some tests fail (only in my local, looks good in mvn verify). 

This issue make {{Table}} in {{flink-table-planner-blink}} to extends {{Table}} in {{flink-table-api-java}} and rename to {{TableImpl}}. We still left the methods implementation to be empty until the {{LogicalNode}} is refactored."	FLINK	Closed	3	4	5351	pull-request-available
13013833	Shade Calcite dependency in flink-table	"The Table API has a dependency on Apache Calcite.
A user reported to have version conflicts when having a own Calcite dependency in the classpath.

The solution would be to shade away the Calcite dependency (Calcite's transitive dependencies are already shaded)."	FLINK	Closed	4	4	5351	auto-deprioritized-major
13505217	Push projection through ChangelogNormalize	"Currently, the ChangelogNormalize node is generated during the physical optimization phase. That means the projection is not pushed through ChangelogNormalize if the {{TableSource}} doesn't support {{SupportsProjectionPushDown}}. We can implement such optimization to reduce the state size (fewer columns in state value) and better throughput (only changes on the selected columns will be emitted). 
"	FLINK	Closed	3	2	5351	pull-request-available
13260688	Add watermark information in TableSchema	"As discussed in FLIP-66, the watermark information should be part of TableSchema, and expose to connectors vis CatalogTable#getTableSchema. 

We may need to introduce a {{WatermarkSpec}} class to describe watermark information. "	FLINK	Resolved	3	7	5351	pull-request-available
13380454	KafkaChangelogTableITCase.testKafkaDebeziumChangelogSource fail due to ConcurrentModificationException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18330&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6608

"	FLINK	Closed	3	1	5351	test-stability
13237721	Introduce unbounded streaming anti/semi join operator	"This operator is responsible for unbounded streaming semi/anti join, and will be optimized in following cases:

1. If the join keys (with equality condition) are also primary key, we will have a more efficient state layout
2. If the inputs have primary keys, but join keys are not primary key, we can also come up with an efficient state layout
3. Inputs don't have primary keys, this will go to default implementation"	FLINK	Closed	3	2	5351	pull-request-available
13290873	Fix exception when computed column expression references a keyword column name	"{code:sql}
json_row          ROW<`timestamp` BIGINT>,
`timestamp`   AS `json_row`.`timestamp`
{code}
It translate to ""SELECT json_row.timestamp FROM __temp_table__""
Throws exception ""Encountered "". timestamp"" at line 1, column 157. Was expecting one of:..."""	FLINK	Resolved	2	1	5351	pull-request-available
13007621	Add possibility to get column names	"For debugging and maybe for visualization in future (e.g. in a shell) it would be good to have the possibilty to get the names of {{Table}} columns. At the moment the user has no idea how the table columns are named; if they need to be matched with POJO fields for example.

My suggestion:

{code}
Schema s = table.schema();
TypeInformation<?> type = s.getType(1);
TypeInformation<?> type = s.getType(""col"");
String s = s.getColumnName(1);
String[] s = s.getColumnNames();
{code}"	FLINK	Resolved	3	2	5351	starter
13352233	USE DATABASE & USE CATALOG fails with quoted identifiers containing characters to be escaped in Flink SQL client	I have a database which name is mod, when I use `use mod` to switch to the db,the system throw an exception, I surround it with backticks ,it is still not well	FLINK	Closed	3	1	5351	pull-request-available
13273411	Introduce TypeTransformation interface and basic transformations	"Currently, the default DataType derived from properties is the default conversion now. However, some connectors/formats are still using sql Timestamp. But bridging DataType into sql Timestamp  is not simple, because DataType is nested. So we propose to introduce a TypeTransformation which transform one data type to another, this is also a useful tool for FLIP-65. 

The proposal including:
- Remove {{CallContext}} from the exiting {{TypeTransformation}}.
- add a package {{o.a.f.table.types.inference.transforms}}
- add commonly used transform classes there, e.g. {{timeToSqlTypes}}
- add a class `o.a.f.table.types.inference.TypeTransforms` for listing all available transforms"	FLINK	Resolved	3	4	5351	pull-request-available
13243032	Support MatchRecognize in blink planner	Support  MATCH RECOGNIZE  in blink planner. This will port the functionality of MATCH RECOGNIZE in flink-planner to blink-planner. 	FLINK	Closed	3	2	5351	pull-request-available
13246275	Add documentation for streaming aggregate performance tunning.	"Add documentation for streaming aggregate performance tuning
- “Performance Tuning / Streaming Aggregation”: Explain how to solve data-skew problem in streaming aggregation (non-windowed aggregate), the internals, and the configurations.
"	FLINK	Resolved	3	7	5351	pull-request-available
13571580	"Add ""Special Thanks"" Page on the Flink Website"	"This issue aims to add a ""Special Thanks"" page on the Flink website (https://flink.apache.org/) to honor and appreciate the companies and organizations that have sponsored machines or services for our project.

Discussion thread: https://lists.apache.org/thread/y5g0nd5t8h2ql4gq7d0kb9tkwv1wkm1j"	FLINK	Open	3	2	5351	pull-request-available
13337838	Fix WatermarkAssigner shouldn't be after ChangelogNormalize	"Cutrrently, for a upsertSource like upsert-kafka, the WatermarkAssigner is followed after ChangelogNormalize Node,  it may returns Long.MaxValue as watermark if some parallelism doesn't have data. 

 
{code:java}
   +- Exchange(distribution=[hash[currency]], changelogMode=[I,UA,D])
      +- WatermarkAssigner(rowtime=[rowtime], watermark=[rowtime], changelogMode=[I,UA,D])
         +- ChangelogNormalize(key=[currency], changelogMode=[I,UA,D])
            +- Exchange(distribution=[hash[currency]], changelogMode=[UA,D])
               +- TableSourceScan(table=[[default_catalog, default_database, rates_history]], fields=[currency, rate, rowtime], changelogMode=[UA,D])
{code}
 

As an improvement, we can move the WatermarkAssigner to be after the SourceCan Node and thus the watermark will produce like general Source.

 "	FLINK	Closed	3	7	5351	pull-request-available
13247092	Configure sending travis build notifications to builds@flink.apache.org	"As discussed in the mailing list[1], the community want to send travis build notifications to builds@flink.apache.org mailing list. 



[1]: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Setup-a-builds-flink-apache-org-mailing-list-for-travis-builds-tt30778.html"	FLINK	Resolved	3	4	5351	pull-request-available
13356497	Improve digest of physical Expand node	"Currently, the digest of {{StreamPhysicalExpand}} only geneartes field names, this loses many useful information, e.g. null fields, expand id, expand times. 

{code}
Expand(projects=[a, b, c, $f3, $f4, $e])
{code}

The digest of {{BatchPhysicalExpand}} generates additional projects list, but the first {{projects}} is reduandent information, we can remove it. 

{code}
Expand(projects=[a, c, $f2, d, $e, $f2_0], projects=[{a, c, $f2, d, 0 AS $e, $f2 AS $f2_0}, {a, c, null AS $f2, null AS d, 3 AS $e, $f2 AS $f2_0}])
{code}

The proposed digest of expand node would be:

{code}
Expand(projects=[{a, c, $f2, d, 0 AS $e, $f2 AS $f2_0}, {a, c, null AS $f2, null AS d, 3 AS $e, $f2 AS $f2_0}])
{code}"	FLINK	Closed	3	4	5351	pull-request-available
13246267	Add documentation for Temporal Table Join in blink planner	"Add documentation for Temporal Table Join in blink planner
- “Streaming Concepts / Temporal Tables”: introduce concepts of temporal table in blink planner and the difference and sameness to flink planner temporal table
- “Joins in Continuous Queries”: how to use temporal join in bink planner
- “SQL”: join with temporal table in SQL
"	FLINK	Resolved	2	7	5351	pull-request-available
13302891	Remove RowData#get() and ArrayData#get() and use FieldGetter and ElementGetter instead	Currently, we are using utility {{RowData#get(RowData, int, LogicalType)}} and {{ArrayData#get(ArrayData, int, LogicalType)}} to get the field/element objects. They have been deprecated since 1.11, we should replace them by {{RowData#FieldGetter}} and {{ArrayData#ElementGetter}}.	FLINK	Closed	3	4	5351	pull-request-available
13225682	Introduce unbounded streaming inner/left/right/full join operator	"This operator is responsible for unbounded streaming inner/left/right/full join, and will be optimized in following cases:
# If the join keys (with equality condition) are also primary key, we will have a more efficient state layout
# If the inputs have primary keys, but join keys are not primary key, we can also come up with an efficient state layout
# Inputs don't have primary keys, this will go to default implementation
"	FLINK	Closed	3	4	5351	pull-request-available
13397772	Fix FLIP-XXX link can't be recognized correctly by IDEA	In FLINK-24013, we support link for FLINK-XXX to JIRA issue in IDEA git log display. However, the FLIP-XXX is also processed in the same way which leads to a wrong link, e.g. FLIP-33 is linked to https://issues.apache.org/jira/browse/FLIP-33. 	FLINK	Closed	3	4	5351	pull-request-available
12987796	Add a basic streaming Table API example	Although the Table API does not offer much streaming features yet, there should be a runnable example showing how to convert, union, filter and project streams with the Table API.	FLINK	Resolved	3	2	5351	starter
13251434	All builds for master branch are failed during compile stage	"Here is an instance: https://api.travis-ci.org/v3/job/572950228/log.txt

There is an error in the log.


{code}
==============================================================================
find: ‘flink-connectors/flink-connector-elasticsearch/target/flink-connector-elasticsearch*.jar’: No such file or directory
==============================================================================
Previous build failure detected, skipping cache setup.
==============================================================================
{code}

The {{flink-connector-elasticsearch}} is not exist. But recent commits didn't modify this."	FLINK	Resolved	1	1	5351	pull-request-available
13285095	Blink planner produces wrong aggregate results with state clean up	"It seems that FLINK-10674 has not been ported to the Blink planner.

Because state clean up happens in processing time, it might be the case that retractions are arriving after the state has been cleaned up. Before these changes, a new accumulator was created and invalid retraction messages were emitted. This change drops retraction messages for which no accumulator exists.

These lines are missing in {{org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction}}:
{code}
if (null == accumulators) {
      // Don't create a new accumulator for a retraction message. This
      // might happen if the retraction message is the first message for the
      // key or after a state clean up.
      if (!inputC.change) {
        return
      }
      // first accumulate message
      firstRow = true
      accumulators = function.createAccumulators()
    } else {
      firstRow = false
    }
{code}

The bug has not been verified. I spotted it only by looking at the code."	FLINK	Resolved	2	1	5351	pull-request-available
13528990	Move execution logic of CreateOperation out from TableEnvironmentImpl	This should implement {{ExecutableOperation}} for all the {{CreateOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.	FLINK	Closed	3	7	5351	pull-request-available
13245220	HBaseITCase bug using blink-runner: SinkCodeGenerator should not compare row type field names	"Running {{org.apache.flink.addons.hbase.HBaseSinkITCase}} using blink planner will encountered this error:


{code}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 28.042 sec
<<< FAILURE! - in org.apache.flink.addons.hbase.HBaseSinkITCase

testTableSink(org.apache.flink.addons.hbase.HBaseSinkITCase)  Time elapsed:
2.431 sec  <<< ERROR!

org.apache.flink.table.api.TableException: Result field 'family1' does not
match requested type. Requested: Row(col1: Integer); Actual: Row(EXPR$0:
Integer)

at
org.apache.flink.addons.hbase.HBaseSinkITCase.testTableSink(HBaseSinkITCase.java:140)
{code}

This might because [SinkCodeGenerator.scala#L243|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/codegen/SinkCodeGenerator.scala#L243] compare RowType with field names.

"	FLINK	Resolved	1	7	5351	pull-request-available
13283636	Unable to use watermark when depends both on flink planner and blink planner	"Run the following code in module `flink-examples-table` (must be under this module)
{code:java}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.flink.table.examples.java;


import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.java.StreamTableEnvironment;

/**
 * javadoc.
 */
public class TableApiExample {

   /**
    *
    * @param args
    * @throws Exception
    */
   public static void main(String[] args) throws Exception {

      StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
      bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
      EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
      StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);
      bsTableEnv.sqlUpdate( ""CREATE TABLE sink_kafka (\n"" +
         ""    status  STRING,\n"" +
         ""    direction STRING,\n"" +
         ""    event_ts TIMESTAMP(3),\n"" +
         ""    WATERMARK FOR event_ts AS event_ts - INTERVAL '5' SECOND\n"" +
         "") WITH (\n"" +
         ""  'connector.type' = 'kafka',       \n"" +
         ""  'connector.version' = 'universal',    \n"" +
         ""  'connector.topic' = 'generated.events2',\n"" +
         ""  'connector.properties.zookeeper.connect' = 'localhost:2181',\n"" +
         ""  'connector.properties.bootstrap.servers' = 'localhost:9092',\n"" +
         ""  'connector.properties.group.id' = 'testGroup',\n"" +
         ""  'format.type'='json',\n"" +
         ""  'update-mode' = 'append'\n"" +
         "")\n"");

   }
}
 {code}
 

And hit the following error:
{code:java}
Exception in thread ""main"" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts'Exception in thread ""main"" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:834) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:819) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4841) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5667) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.SqlOperator.deriveType(SqlOperator.java:501) at org.apache.calcite.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:144) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:947) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateParameterizedExpression(SqlValidatorImpl.java:930) at org.apache.flink.table.planner.operations.SqlToOperationConverter.lambda$createTableSchema$8(SqlToOperationConverter.java:509) at java.util.Optional.ifPresent(Optional.java:159) at org.apache.flink.table.planner.operations.SqlToOperationConverter.createTableSchema(SqlToOperationConverter.java:505) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:177) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:130) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:484) at org.apache.flink.table.examples.java.TableApiExample.main(TableApiExample.java:43)Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572) ... 25 more {code}"	FLINK	Resolved	1	1	5351	pull-request-available
13293298	Fix cast exception when having time point literal as parameters	"I defined as ScalarFunction as follow:

 
{code:java}
public class DateFunc extends ScalarFunction {


    public String eval(Date date) {
      return date.toString();
    }

    @Override
    public TypeInformation<?> getResultType(Class<?>[] signature) {
        return Types.STRING;
    }

   @Override
   public TypeInformation<?>[] getParameterTypes(Class<?>[] signature) {
      return new TypeInformation[]{Types.INT};
   }
}
{code}
I ues it in sql: `select func(DATE '2020-11-12') as a from source` , Flink throws 'cannot cast 2020-11-12 as class java.time.LocalDate '

 

The full code is in the [^Flinktest.zip] Main class is com.lorinda.template.TestDateFunction"	FLINK	Resolved	3	1	5351	pull-request-available
13323550	Support to only read changelogs of specific database and table for canal-json format	"Usually, users use Canal to synchronize binlog data from various MySQL databases and tables into a single Kafka topic. However, currently, canal-json can't support this case, because it requires the canal data in the topic should be in the same data format. 

This issue propose to introduce a new option ""canal-json.database"" and ""canal-json.table"" to filter out the specific data. It would be great to support table list or table pattern in case of all the tables have the same schema. "	FLINK	Closed	3	2	5351	pull-request-available
13250410	Fix TableFactory doesn't work with DDL when containing TIMESTAMP/DATE/TIME types	"Currently, in blink planner, we will convert DDL to {{TableSchema}} with new type system, i.e. DataTypes.TIMESTAMP()/DATE()/TIME() whose underlying TypeInformation are  Types.LOCAL_DATETIME/LOCAL_DATE/LOCAL_TIME. 

However, this makes the existing connector implementations (Kafka, ES, CSV, etc..) don't work because they only accept the old TypeInformations (Types.SQL_TIMESTAMP/SQL_DATE/SQL_TIME).

A simple solution is encode DataTypes.TIMESTAMP() as ""TIMESTAMP"" when translating to properties. And will be converted back to the old TypeInformation: Types.SQL_TIMESTAMP. This would fix all factories at once.
"	FLINK	Resolved	2	1	5351	pull-request-available
13232568	Fix broken links in documentation to make CRON travis job work	"The CRON travis job is failing because of documentation link checks. 

https://travis-ci.org/apache/flink/jobs/530213609

Following are the broken links:


{code:java}
[2019-05-09 14:05:44] ERROR `/zh/dev/stream/side_output.html' not found.
[2019-05-09 14:05:45] ERROR `/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.8.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.7.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.6.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.5.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/levels_of_abstraction.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/table_api.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/program_dataflow.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/parallel_dataflow.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/windows.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/event_ingestion_processing_time.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/state_partitioning.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/tasks_chains.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/processes.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/tasks_slots.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/slot_sharing.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/checkpoints.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/linking_with_flink.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/linking.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/apis/streaming/event_timestamps_watermarks.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/apis/streaming/event_timestamp_extractors.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/apis/streaming/event_time.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-09 14:05:49] ERROR `/zh/fig/checkpoint_tuning.svg' not found.
[2019-05-09 14:05:49] ERROR `/zh/fig/local_recovery.png' not found.
{code}
"	FLINK	Closed	3	1	5351	pull-request-available
13307388	"Translate ""Python Table API Installation"" page into Chinese"	"The page url is https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/python/installation.html

The markdown file is located in flink/docs/dev/table/python/installation.zh.md"	FLINK	Closed	3	7	5351	pull-request-available
13280817	JDBCUpsertOutputFormat does not set bind parameter keyFields in updateStatement	"When using JDBCUpsertOutputFormat custom dialect e.g. H2/Oracle which uses UpsertWriterUsingInsertUpdateStatement, code fails with below error.
{code:java}
Caused by: org.h2.jdbc.JdbcSQLDataException: Parameter ""#6"" is not set [90012-200] 
at org.h2.message.DbException.getJdbcSQLException(DbException.java:590) 
at org.h2.message.DbException.getJdbcSQLException(DbException.java:429) 
at org.h2.message.DbException.get(DbException.java:205) 
at org.h2.message.DbException.get(DbException.java:181) 
at org.h2.expression.Parameter.checkSet(Parameter.java:83) 
at org.h2.jdbc.JdbcPreparedStatement.addBatch(JdbcPreparedStatement.java:1275) 
at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter$UpsertWriterUsingInsertUpdateStatement.processOneRowInBatch(UpsertWriter.java:233) 
at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.executeBatch(UpsertWriter.java:111) {code}
This is due to UpsertWriterUsingInsertUpdateStatement#processOneRowInBatch does not set all bind paramters in case of Update.

This bug does get surfaced while using Derby DB. 
 In JDBCUpsertOutputFormatTest if we replace Derby with H2 we can reproduce the bug.

The fix is trivial. Happy to raise PR.
{code:java}
//for update case replace below
setRecordToStatement(updateStatement, fieldTypes, row); 
//with
setRecordToStatement(updateStatement, fieldTypes + pkTypes, row  + pkRow);
//NOTE:  as prepared updateStatement contains additional where clause we need pass additional bind values and its sql Types



{code}"	FLINK	Closed	2	1	5351	pull-request-available
13247477	Change default planner to legacy planner instead of any one	As discussed in FLINK-13399, we will change the default behavior of the {{EnvironmentSettings}} to use old planner instead of any planner. This will enable us to have both planner in the classpath. This will also enable users/connectors to have both planner in dependency and without  using {{EnvironmentSettings}}.	FLINK	Resolved	1	4	5351	pull-request-available
13298447	Introduce Debezium format to support reading debezium changelogs	"Introduce {{DebeziumFormatFactory}} and {{DebeziumRowDeserializationSchema}} to read debezium changelogs.


{code:sql}
CREATE TABLE my_table (
  ...
) WITH (
  'connector'='...',  -- e.g. 'kafka'
  'format'='debezium-json',
  'debezium-json.schema-include'='true' -- default false, Debeizum can be configured to include or exclude the message schema
  'debezium-json.ignore-parse-errors'='true' -- default false
);
{code}

"	FLINK	Closed	3	7	5351	pull-request-available
13296436	Refactor planner and connectors to use new data structures	"Refactors existing code to use the new data structures interfaces.

This issue might be split into smaller subtasks if necessary."	FLINK	Closed	3	7	5351	pull-request-available
13290446	Improve default flush strategy for JDBC sink to make it work out-of-box	"Currently, old JDBC sink provides 2 flush options:

{code}
'connector.write.flush.max-rows' = '5000', -- default is 5000
'connector.write.flush.interval' = '2s', -- no default value
{code}

That means if flush interval is not set, the buffered output rows may not be flushed to database for a long time. That is a surprising behavior because no results are outputed by default. 

So we propose to have a default flush '1s' interval and '100' rows for JDBC sink flush. This only applies to new JDBC sink options:

{code}
'sink.buffer-flush.max-rows' = '100'
'sink.buffer-flush.interval' = '1s'
{code}



"	FLINK	Closed	2	4	5351	pull-request-available
12987794	Create a batch SQL example	"Currently there is no runnable code example in `flink-table` showing a working batch SQL query with the Table API.

A Scala and Java example should be added."	FLINK	Resolved	3	2	5351	starter
13336263	Replace 'collection' connector by 'values' connector for temporal join plan tests	Currently, both COLLECTION and VALUES connectors are `LookupTableSoure`, we can add a non lookup table source connector to cover  scan-only source.	FLINK	Closed	3	7	5351	pull-request-available
13298583	Refactor BaseRow to use RowKind instead of byte header	"This is a pre-step of FLINK-16996. We will refactor BaseRow#get/setHeader to get/setRowKind first. And update all the existing code to send ""insert/delete/update_before/update_after"" messages instead of ""+/-"" messages. This is possible now since FLINK-16887 is supported. "	FLINK	Closed	3	7	5351	pull-request-available
13275663	"Correct the terminology of ""Time-windowed Join"" to ""Interval Join"" in Table API & SQL"	"Currently, in the docuementation, we call the joins with time conditions as ""Time-windowed Join"". However, it is called ""Interval Join"" in DataStream. We should align the terminology in Flink project. 

From my point of view, ""Interval Join"" is more suitable, because it joins a time interval range of right stream[1]. And ""Windowed Join"" should be joins data in the same window, this is also described in DataStream API. 

For Table API & SQL, the ""Time-windowed Join"" is the ""Interval Join"" in DataStream. And we miss the new feature ""Windowed Join"" in Table API & SQL.

I propose to correct the terminology in docs before 1.10 is release. 


[1]: https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/joining.html#interval-join
[2]: https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/joining.html#window-join

Discussion thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Correct-the-terminology-of-quot-Time-windowed-Join-quot-to-quot-Interval-Join-quot-in-Table-L-td36202.html#a36208
Voting thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Rename-terminology-quot-Time-windowed-Join-quot-to-quot-Interval-Join-quot-in-Table-API-amp-SQL-td36370.html"	FLINK	Closed	1	4	5351	pull-request-available
13237194	Introduce new Interfaces for source and sink to make Blink runner work	"In order to support Blink batch and temporal table join, we need some new source&sink interfaces.

1. Introduce {{InputFormatTableSource}}
 - add {{isBounded}} interface to {{StreamTableSource}}
 - {{InputFormatTableSource}} extends {{StreamTableSource}} and expose {{getInputFormat}}
 - removes {{BatchTableSource}} and {{StreamTableSource}} in blink planner
 - support it in blink and flink planner

2. Introduce {{OutputFormatTableSink}}
 - {{OutputFormatTableSink}} extends {{StreamTableSink}} expose {{getOutputFormat}}
 - removes {{BatchTableSink}} in blink planner
 - support it in blink and flink planner

3. Introduce {{LookupableTableSource}}
 - removes {{LookupableTableSource}} and {{LookupConfig}} in blink planner
 - support it only in blink planner

4. Expose {{getTableStats}} in {{TableSource}}
 - support it in blink and flink planner"	FLINK	Closed	3	2	5351	pull-request-available
13217573	Add a language switch to the sidebar for Flink documents	"Add a language switch similar to the project webpage. 

We didn't add the switch in the first version of supporting Chinese language, because we want to expose the switch when we satisfied the translation coverage."	FLINK	Closed	3	7	5351	pull-request-available
13337074	Fix Flink SQL throws exception when changelog source contains duplicate change events	We are using Canal to synchornize MySQL data into Kafka, the synchornization delivery is not exactly-once, so there might be dupcliate INSERT/UPDATE/DELETE messages for the same primary key. We are using {{'connecotr' = 'kafka', 'format' = 'canal-json'}} to consume such topic. However, when appling TopN query on this created source table, the TopN operator will thrown exception: {{Caused by: java.lang.RuntimeException: Can not retract a non-existent record. This should never happen.}}	FLINK	Closed	3	7	5351	pull-request-available
13335940	Writing Table with RowTime Column of type TIMESTAMP(3) to Kafka fails with ClassCastException	"When I try to write a table to Kafka (JSON format) that has a rowtime attribute of type TIMESTAMP(3) the job fails with 

{noformat}
2020-10-18 18:02:08
java.lang.ClassCastException: org.apache.flink.table.data.TimestampData cannot be cast to java.lang.Long
	at org.apache.flink.table.data.GenericRowData.getLong(GenericRowData.java:154)
	at org.apache.flink.table.runtime.operators.sink.SinkOperator$SimpleContext.timestamp(SinkOperator.java:144)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:866)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)
	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:86)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.processElement(WatermarkAssignerOperator.java:123)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collectWithTimestamp(StreamSourceContexts.java:111)
	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:352)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:185)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:141)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
{noformat}

From looking at the relevant code in SinkOperator$SimpleContext#timestamp it seems that we can only deal with long type timestamps in SinkOperator?!
"	FLINK	Closed	2	1	5351	pull-request-available
13248892	Fix some transformation names are not set in blink planner	"Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion."	FLINK	Resolved	3	1	5351	pull-request-available
13350803	Refactor BytesHashMap and BytesMultiMap to support window key	Currently, the {{BytesHashMap}} and {{BytesMultiMap}} only support {{BinaryRowData}} as the key. However, for window operators, the key should be {{groupKey + window}}. We should make them decouple with {{BinaryRowData}} key type and support window key. 	FLINK	Closed	3	7	5351	pull-request-available
13231910	Synchronize the latest documentation changes into Chinese documents	There are several commits to documentations have not been synchronized to Chinese documents, i.e. `xx.zh.md`. This pull request will synchronize the latest changes into Chinese documents.	FLINK	Closed	4	7	5351	pull-request-available
13295528	DecimalData.toUnscaledBytes should be consistent with BigDecimla.unscaledValue.toByteArray	"In Decimal:
{code:java}
public byte[] toUnscaledBytes() {
   if (!isCompact()) {
      return toBigDecimal().unscaledValue().toByteArray();
   }

   // big endian; consistent with BigInteger.toByteArray()
   byte[] bytes = new byte[8];
   long l = longVal;
   for (int i = 0; i < 8; i++) {
      bytes[7 - i] = (byte) l;
      l >>>= 8;
   }
   return bytes;
}
{code}
When is compact, it will return fix 8 length byte array.

This should not happen, it brings an incompatible byte array."	FLINK	Closed	2	1	5351	pull-request-available
13341006	FactoryUtil#createTableSource will be confused by a table source and a table sink factory with same identifier	"When creating a table source I'm faced with the following exception:
{code:java}
 Caused by: org.apache.flink.table.api.ValidationException: Multiple factories for identifier 'odps' that implement 'org.apache.flink.table.factories.DynamicTableFactory' found in the classpath.

Ambiguous factory classes are:
{code}
However there is only one table source factory with this identifier, and another table sink factory with this identifier. {{FactoryUtil#createTableSource}} shouldn't be confused.

This is caused by {{FactoryUtil.java}} line 370, where {{factory = discoverFactory(context.getClassLoader(), DynamicTableFactory.class, connectorOption);}} should be {{factory = discoverFactory(context.getClassLoader(), factoryClass, connectorOption);}}.

I understand that this change aims to display a better exception message. We might need to change the logic of selecting a proper exception message a little bit."	FLINK	Closed	1	1	5351	pull-request-available
13296762	Introduce a new HBase connector with new property keys	"This new hbase connector should use new interfaces proposed by FLIP-95, e.g. DynamicTableSource, DynamicTableSink, and Factory.

The new proposed keys :
||Old key||New key||Note||
|connector.type|connector| |
|connector.version|N/A|merged into 'connector' key|
|connector.table-name|table-name| |
|connector.zookeeper.quorum|zookeeper.quorum| |
|connector.zookeeper.znode.parent|zookeeper.znode-parent| |
|connector.write.buffer-flush.max-size|sink.buffer-flush.max-size| |
|connector.write.buffer-flush.max-rows|sink.buffer-flush.max-rows| |
|connector.write.buffer-flush.interval|sink.buffer-flush.interval| |
 
 "	FLINK	Closed	3	7	5351	pull-request-available
13221348	Support code generation for all Blink built-in functions and operators	"Support code generation for built-in functions and operators. 

FLINK-11788 has supported some of the operators. This issue is aiming to complement the functions and operators supported in Flink SQL.

This should inlclude: CONCAT, LIKE, SUBSTRING, UPPER, LOWER, and so on."	FLINK	Closed	3	2	5351	pull-request-available
13246268	Add documentation for TopN and Deduplication in blink planner	"Add documentation for TopN in blink planner
“SQL”: how to write TopN in SQL and some tips
"	FLINK	Resolved	2	7	5351	pull-request-available
13338581	Calling NOW() function throws compile exception	"The following test code in {{ScalarOperatorsTest}} will fail with a compile exception

{code:scala}
testSqlApi(""CAST(NOW() AS BIGINT)"", ""??"")
{code}

{code}
java.lang.RuntimeException: Could not instantiate generated class 'TestFunction$24'

	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:57)
	at org.apache.flink.table.planner.expressions.utils.ExpressionTestBase.evaluateExprs(ExpressionTestBase.scala:143)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:52)
	... 25 more
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
	... 27 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
	... 30 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 64, Column 22: Assignment conversion not possible from type ""long"" to type ""org.apache.flink.table.data.TimestampData""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:78)
	... 36 more
{code}

This should be a bug in the code generation for {{NOW()}} function."	FLINK	Closed	3	1	5351	pull-request-available
13371172	KafkaChangelogTableITCase.testKafkaCanalChangelogSource fail due to ConcurrentModificationException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16332&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7425

{code:java}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
	at java.util.HashMap$ValueIterator.next(HashMap.java:1474)
	at java.util.AbstractCollection.toArray(AbstractCollection.java:141)
	at java.util.ArrayList.addAll(ArrayList.java:583)
	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.lambda$getResults$0(TestValuesRuntimeFunctions.java:108)
	at java.util.HashMap$Values.forEach(HashMap.java:981)
	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.getResults(TestValuesRuntimeFunctions.java:108)
	at org.apache.flink.table.planner.factories.TestValuesTableFactory.getResults(TestValuesTableFactory.java:164)
	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.waitingExpectedResults(KafkaTableTestUtils.java:82)
	at org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.testKafkaCanalChangelogSource(KafkaChangelogTableITCase.java:348)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)

{code}
"	FLINK	Closed	2	1	5351	pull-request-available, test-stability
13337366	Refactor and merge SupportsComputedColumnPushDown and SupportsWatermarkPushDown interfaces	"As discussed in mailing list [1], the existing SupportsComputedColumnPushDown and SupportsWatermarkPushDown are confusing and hard to implement for connectors. The 
{{SupportsComputedColumnPushDown}} only used for watermark push down. Therefore, 
 combining them into a single interface {{SupportsWatermarkPushDown}} would be better and also work. The proposed interface looks like this:


{code:java}
public interface SupportsWatermarkPushDown {
    
    void applyWatermark(org.apache.flink.table.sources.wmstrategies.WatermarkStrategy<RowData> watermarkStrategy);
    
}
{code}



[1]: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Merge-SupportsComputedColumnPushDown-and-SupportsWatermarkPushDown-td44387.html"	FLINK	Closed	3	7	5351	pull-request-available
13342076	Optimize the exception message of FileSystemTableSink when missing format dependencies	"Current when the format factory failed to load, the following exception would be thrown:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.sink'.

Table options are:

'auto-compaction'='true'
'connector'='filesystem'
'format'='csv'
'path'='file:///tmp/compaction'
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:166)
  at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:362)
  at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:220)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.Iterator$class.foreach(Iterator.scala:891)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:164)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:674)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:757)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664)
  at FileCompactionTest.main(FileCompactionTest.java:147)
Caused by: org.apache.flink.table.api.ValidationException: Please implement at least one of the following formats: BulkWriter.Factory, SerializationSchema, FileSystemFormatFactory.
  at org.apache.flink.table.filesystem.FileSystemTableSink.<init>(FileSystemTableSink.java:124)
  at org.apache.flink.table.filesystem.FileSystemTableFactory.createDynamicTableSink(FileSystemTableFactory.java:83)
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:163)
  ... 18 more
{code}
 

We might directly advice users to check if the format dependency is added or if there are package conflicts so that users would fix this issue faster.

 "	FLINK	Closed	2	1	5351	pull-request-available
13248596	Verify and correct time function's semantic for Blink planner	"- Drop CONVERT_TZ(timestamp, format, from_tz, to_tz) function support
   - Drop CONVERT_TZ(timestamp, format, from_tz, to_tz) function support in blink planner to align with other databases. We only support CONVERT_TZ(timestamp, from_tz, to_tz) in this version.

- Drop DATE_FORMAT(timestamp, from_format, to_format) function support 
  - Drop DATE_FORMAT(timestamp, from_format, to_format) function support in blink planner to align with other databases. We only support DATE_FORMAT(timestamp, to_format) and DATE_FORMAT(string, to_format) in this version.

- Drop TO_DATE(int) function support
  - Drop TO_DATE(int) function support in blink planner to align with other databases. We only support TO_DATE(string [,format]) in this version.

- Drop TO_TIMESTAMP(bigint) function support
  - Drop TO_TIMESTAMP(bigint) function support in blink planner to align with other systems. We only support TO_TIMESTAMP(string [,format]) in this version.

- Remove some builtin datetime functions which can be covered by existing functions
  - Removes DATE_FORMAT_TZ, DATE_ADD,DATE_SUB, DATEDIFF, FROM_TIMESTAMP, TO_TIMESTAMP_TZ

- Fix FROM_UNIXTIME(bigint [,format]) should work in session time zone
  - This aligns the behavior to other systems (MySQL, Spark).

- Fix UNIX_TIMESTAMP(string [,format]) should work in session time zone
  - This aligns the behavior to other systems (MySQL, Spark). UNIX_TIMESTAMP(string [,format]) is an inverse of FROM_UNIXTIME(bigint [,format]). We also remove the support of UNIX_TIMESTAMP(timestamp) in this commit.

- Fix NOW() should return TIMESTAMP instead of BIGINT.
  -  This aligns the behavior to other systems (MySQL, Spark). Because NOW() is Synonyms for CURRENT_TIMESTAMP.

"	FLINK	Resolved	3	7	5351	pull-request-available
13306796	Improve interface of ScanFormatFactory and SinkFormatFactory	"There is some problem with current ScanForamtFactory and SinkFormatFactory interfaces:
1) {{ScanFormat#createScanFormat}} only accepts {{ScanTableSource.Context}}, which means it can’t work in lookup source.
2) The naming of {{ScanFormat}} also indicates it is only used in scan source. But a lookup source should be able to work with format too.
3) It’s confusing that {{ScanFormatFactory#createScanFormat}} and {{ScanFormat#createScanFormat}} (create itself?)

The proposed changes:

1. Have a common interface DynamicTableSource.Context, and make Context of ScanTableSource and LookupTableSource extend it, and rename them to LookupContext and ScanContext
2. Change parameter of ScanFormat.createScanFormat from ScanTableSource.Context to DynamicTableSource.Context
3. Rename ScanFormat.createScanFormat to DecodingFormat#createRuntimeDecoder()
4. Rename SinkFormat.createSinkFormat to EncodingFormat#createRuntimeEncoder()
5. Rename ScanFormatFactory to DecodingFormatFactory
6. Rename SinkFormatFactory to EncodingFormatFactory
"	FLINK	Closed	1	7	5351	pull-request-available
13427786	Support operators send request to Coordinator and return a response	"Currently, the communitcation between Operator and Coordiator is sigle-way. That means, after Operator sending a message to Coordiator, it can't wait to get the response message. In some senarios, the Operator may need to retrieve some information stored in the Coordinator. Thus, it would be great if we can have a two-way communication. 
"	FLINK	Closed	3	2	5351	pull-request-available
13305641	Align the behavior between the new and legacy HBase table source	The legacy HBase table source, i.e. {{HBaseTableSource}}, supports projection push down. In order to make the user experience consistent. We should align the behavior and add tests. 	FLINK	Closed	3	7	5351	pull-request-available
13310359	Init lookup join failed when use udf on lookup table	"Throw exception 

{code}
Caused by: scala.MatchError: (CONCAT(_UTF-16LE'Hello', $2),_UTF-16LE'Hello,Jark':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") (of class scala.Tuple2)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.org$apache$flink$table$planner$plan$nodes$common$CommonLookupJoin$$extractConstantField(CommonLookupJoin.scala:617)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.extractConstantFieldsFromEquiCondition(CommonLookupJoin.scala:607)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.analyzeLookupKeys(CommonLookupJoin.scala:567)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.<init>(CommonLookupJoin.scala:129)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.<init>(StreamExecLookupJoin.scala:49)
{code}

SQL:

{code:sql}
SELECT
  T.id, T.len, T.content, D.name 
FROM 
  T JOIN userTable for system_time as of T.proctime AS D 
ON T.id = D.id 
WHERE 
  add(T.id, D.id) > 3 AND add(T.id, 2) > 3 AND CONCAT('Hello', D.name) = 'Hello,Jark'
{code}

When use function a RexCall can't match RexInputRef and cause this error, myabe shoud add condition""{{case _ => return}}"" to skip this.
"	FLINK	Closed	3	1	5351	pull-request-available
13272974	types with precision can't be executed in sql client with blink planner	"I created a table in sql client with blink planner:  
{noformat}
create table t (
    a int,
    b varchar,
    c decimal(10, 5))
with (
    'connector.type' = 'filesystem',
    'format.type' = 'csv',
    'format.derive-schema' = 'true',
    'connector.path' = 'xxxxxxx'
);
{noformat}
The table description looks good:
{noformat}
Flink SQL> describe t; 
root 
  |-- a: INT 
  |-- b: STRING 
  |-- c: DECIMAL(10, 5){noformat}
But the select query failed:
{noformat}
Flink SQL> select * from t;
[ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(field$3,isNull$3,,DECIMAL(38, 18),None)] type is [DECIMAL(38, 18)], result type is [DECIMAL(10, 5)]
{noformat}
 "	FLINK	Resolved	3	1	5351	pull-request-available
13316393	Remove deprecated classes in flink-connector-jdbc	We have refactored the class structure of flink-connector-jdbc module and kept the old API classes in {{org.apache.flink.api.java.io.jdbc}} compatible purpose. Now, it's safe to remove these classes. 	FLINK	Closed	3	4	5351	pull-request-available
13312940	JdbcFullTest failed to compile on JDK11	"master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3928&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=946871de-358d-5815-3994-8175615bc253
release-1.11: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=946871de-358d-5815-3994-8175615bc253
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6

{code}
2020-06-22T20:19:50.2157534Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2158031Z [ERROR] COMPILATION ERROR : 
2020-06-22T20:19:50.2158826Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2159987Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[137,51] cannot find symbol
2020-06-22T20:19:50.2160676Z   symbol:   variable f1
2020-06-22T20:19:50.2161236Z   location: variable tuple2 of type java.lang.Object
2020-06-22T20:19:50.2163372Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[136,33] incompatible types: cannot infer functional interface descriptor for org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.StatementExecutorFactory<org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor>
2020-06-22T20:19:50.2164788Z [INFO] 2 errors 
2020-06-22T20:19:50.2165569Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2166430Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2167374Z [INFO] Reactor Summary:
2020-06-22T20:19:50.2167713Z [INFO] 
2020-06-22T20:19:50.2168486Z [INFO] force-shading ...................................... SUCCESS [  5.905 s]
2020-06-22T20:19:50.2169067Z [INFO] flink .............................................. SUCCESS [ 10.173 s]
2020-06-22T20:19:50.2169978Z [INFO] flink-annotations .................................. SUCCESS [  1.637 s]
2020-06-22T20:19:50.2170980Z [INFO] flink-test-utils-parent ............................ SUCCESS [  0.117 s]
2020-06-22T20:19:50.2171877Z [INFO] flink-test-utils-junit ............................. SUCCESS [  1.224 s]
2020-06-22T20:19:50.2172896Z [INFO] flink-metrics ...................................... SUCCESS [  0.101 s]
2020-06-22T20:19:50.2173788Z [INFO] flink-metrics-core ................................. SUCCESS [  1.726 s]
2020-06-22T20:19:50.2175058Z [INFO] flink-core ......................................... SUCCESS [ 29.372 s]
2020-06-22T20:19:50.2175982Z [INFO] flink-java ......................................... SUCCESS [  5.577 s]
2020-06-22T20:19:50.2176868Z [INFO] flink-queryable-state .............................. SUCCESS [  0.085 s]
2020-06-22T20:19:50.2177760Z [INFO] flink-queryable-state-client-java .................. SUCCESS [  1.619 s]
2020-06-22T20:19:50.2178600Z [INFO] flink-filesystems .................................. SUCCESS [  0.105 s]
2020-06-22T20:19:50.2179500Z [INFO] flink-hadoop-fs .................................... SUCCESS [ 20.792 s]
2020-06-22T20:19:50.2180402Z [INFO] flink-runtime ...................................... SUCCESS [01:51 min]
2020-06-22T20:19:50.2181462Z [INFO] flink-scala ........................................ SUCCESS [ 36.797 s]
2020-06-22T20:19:50.2182326Z [INFO] flink-mapr-fs ...................................... SUCCESS [  0.848 s]
2020-06-22T20:19:50.2183372Z [INFO] flink-filesystems :: flink-fs-hadoop-shaded ........ SUCCESS [  4.422 s]
2020-06-22T20:19:50.2184407Z [INFO] flink-s3-fs-base ................................... SUCCESS [  2.085 s]
2020-06-22T20:19:50.2185259Z [INFO] flink-s3-fs-hadoop ................................. SUCCESS [  6.051 s]
2020-06-22T20:19:50.2186131Z [INFO] flink-s3-fs-presto ................................. SUCCESS [ 10.325 s]
2020-06-22T20:19:50.2186990Z [INFO] flink-swift-fs-hadoop .............................. SUCCESS [ 22.021 s]
2020-06-22T20:19:50.2187820Z [INFO] flink-oss-fs-hadoop ................................ SUCCESS [  6.407 s]
2020-06-22T20:19:50.2188686Z [INFO] flink-azure-fs-hadoop .............................. SUCCESS [  8.868 s]
2020-06-22T20:19:50.2189526Z [INFO] flink-optimizer .................................... SUCCESS [ 10.922 s]
2020-06-22T20:19:50.2190385Z [INFO] flink-streaming-java ............................... SUCCESS [ 14.119 s]
2020-06-22T20:19:50.2191563Z [INFO] flink-clients ...................................... SUCCESS [  2.558 s]
2020-06-22T20:19:50.2192425Z [INFO] flink-test-utils ................................... SUCCESS [  1.837 s]
2020-06-22T20:19:50.2193609Z [INFO] flink-runtime-web .................................. SUCCESS [02:01 min]
2020-06-22T20:19:50.2194615Z [INFO] flink-examples ..................................... SUCCESS [  0.174 s]
2020-06-22T20:19:50.2195284Z [INFO] flink-examples-batch ............................... SUCCESS [ 12.889 s]
2020-06-22T20:19:50.2195902Z [INFO] flink-connectors ................................... SUCCESS [  0.109 s]
2020-06-22T20:19:50.2196513Z [INFO] flink-hadoop-compatibility ......................... SUCCESS [  6.164 s]
2020-06-22T20:19:50.2197043Z [INFO] flink-state-backends ............................... SUCCESS [  0.075 s]
2020-06-22T20:19:50.2197591Z [INFO] flink-statebackend-rocksdb ......................... SUCCESS [  4.125 s]
2020-06-22T20:19:50.2198116Z [INFO] flink-tests ........................................ SUCCESS [ 36.488 s]
2020-06-22T20:19:50.2198658Z [INFO] flink-streaming-scala .............................. SUCCESS [ 33.694 s]
2020-06-22T20:19:50.2199392Z [INFO] flink-hcatalog ..................................... SUCCESS [  7.401 s]
2020-06-22T20:19:50.2199957Z [INFO] flink-table ........................................ SUCCESS [  0.073 s]
2020-06-22T20:19:50.2200496Z [INFO] flink-table-common ................................. SUCCESS [  5.219 s]
2020-06-22T20:19:50.2201146Z [INFO] flink-table-api-java ............................... SUCCESS [  3.072 s]
2020-06-22T20:19:50.2201699Z [INFO] flink-table-api-java-bridge ........................ SUCCESS [  1.363 s]
2020-06-22T20:19:50.2202221Z [INFO] flink-table-api-scala .............................. SUCCESS [ 11.035 s]
2020-06-22T20:19:50.2202978Z [INFO] flink-table-api-scala-bridge ....................... SUCCESS [  9.510 s]
2020-06-22T20:19:50.2203548Z [INFO] flink-sql-parser ................................... SUCCESS [  6.408 s]
2020-06-22T20:19:50.2204152Z [INFO] flink-libraries .................................... SUCCESS [  0.080 s]
2020-06-22T20:19:50.2204832Z [INFO] flink-cep .......................................... SUCCESS [  3.582 s]
2020-06-22T20:19:50.2205395Z [INFO] flink-table-planner ................................ SUCCESS [02:02 min]
2020-06-22T20:19:50.2205980Z [INFO] flink-sql-parser-hive .............................. SUCCESS [  2.555 s]
2020-06-22T20:19:50.2206691Z [INFO] flink-table-runtime-blink .......................... SUCCESS [  6.373 s]
2020-06-22T20:19:50.2207523Z [INFO] flink-table-planner-blink .......................... SUCCESS [02:42 min]
2020-06-22T20:19:50.2208335Z [INFO] flink-metrics-jmx .................................. SUCCESS [  0.559 s]
2020-06-22T20:19:50.2209071Z [INFO] flink-formats ...................................... SUCCESS [  0.069 s]
2020-06-22T20:19:50.2209852Z [INFO] flink-json ......................................... SUCCESS [  1.230 s]
2020-06-22T20:19:50.2210584Z [INFO] flink-connector-kafka-base ......................... SUCCESS [  4.715 s]
2020-06-22T20:19:50.2211540Z [INFO] flink-avro ......................................... SUCCESS [  4.357 s]
2020-06-22T20:19:50.2212347Z [INFO] flink-csv .......................................... SUCCESS [  1.086 s]
2020-06-22T20:19:50.2213216Z [INFO] flink-connector-kafka-0.10 ......................... SUCCESS [  4.828 s]
2020-06-22T20:19:50.2214121Z [INFO] flink-connector-kafka-0.11 ......................... SUCCESS [  5.160 s]
2020-06-22T20:19:50.2214850Z [INFO] flink-connector-elasticsearch-base ................. SUCCESS [ 10.191 s]
2020-06-22T20:19:50.2215579Z [INFO] flink-connector-elasticsearch5 ..................... SUCCESS [ 16.121 s]
2020-06-22T20:19:50.2216348Z [INFO] flink-connector-elasticsearch6 ..................... SUCCESS [ 13.925 s]
2020-06-22T20:19:50.2217097Z [INFO] flink-connector-elasticsearch7 ..................... SUCCESS [ 14.855 s]
2020-06-22T20:19:50.2217870Z [INFO] flink-connector-hbase .............................. SUCCESS [ 26.486 s]
2020-06-22T20:19:50.2218627Z [INFO] flink-hadoop-bulk .................................. SUCCESS [  0.804 s]
2020-06-22T20:19:50.2219401Z [INFO] flink-orc .......................................... SUCCESS [  1.600 s]
2020-06-22T20:19:50.2220154Z [INFO] flink-orc-nohive ................................... SUCCESS [  1.258 s]
2020-06-22T20:19:50.2221047Z [INFO] flink-parquet ...................................... SUCCESS [  5.085 s]
2020-06-22T20:19:50.2221830Z [INFO] flink-connector-hive ............................... SUCCESS [ 29.718 s]
2020-06-22T20:19:50.2222595Z [INFO] flink-connector-jdbc ............................... FAILURE [ 42.673 s]
2020-06-22T20:19:50.2223455Z [INFO] flink-connector-rabbitmq ........................... SKIPPED
2020-06-22T20:19:50.2224297Z [INFO] flink-connector-twitter ............................ SKIPPED
2020-06-22T20:19:50.2225011Z [INFO] flink-connector-nifi ............................... SKIPPED
2020-06-22T20:19:50.2225705Z [INFO] flink-connector-cassandra .......................... SKIPPED
2020-06-22T20:19:50.2226427Z [INFO] flink-connector-filesystem ......................... SKIPPED
2020-06-22T20:19:50.2227146Z [INFO] flink-connector-kafka .............................. SKIPPED
2020-06-22T20:19:50.2228045Z [INFO] flink-connector-gcp-pubsub ......................... SKIPPED
2020-06-22T20:19:50.2228760Z [INFO] flink-connector-kinesis ............................ SKIPPED
2020-06-22T20:19:50.2229459Z [INFO] flink-sql-connector-elasticsearch7 ................. SKIPPED
2020-06-22T20:19:50.2230163Z [INFO] flink-connector-base ............................... SKIPPED
2020-06-22T20:19:50.2230956Z [INFO] flink-sql-connector-elasticsearch6 ................. SKIPPED
2020-06-22T20:19:50.2231671Z [INFO] flink-sql-connector-kafka-0.10 ..................... SKIPPED
2020-06-22T20:19:50.2232353Z [INFO] flink-sql-connector-kafka-0.11 ..................... SKIPPED
2020-06-22T20:19:50.2233168Z [INFO] flink-sql-connector-kafka .......................... SKIPPED
2020-06-22T20:19:50.2234041Z [INFO] flink-sql-connector-hive-1.2.2 ..................... SKIPPED
2020-06-22T20:19:50.2234926Z [INFO] flink-sql-connector-hive-2.2.0 ..................... SKIPPED
2020-06-22T20:19:50.2235647Z [INFO] flink-sql-connector-hive-2.3.6 ..................... SKIPPED
2020-06-22T20:19:50.2236328Z [INFO] flink-sql-connector-hive-3.1.2 ..................... SKIPPED
2020-06-22T20:19:50.2236998Z [INFO] flink-avro-confluent-registry ...................... SKIPPED
2020-06-22T20:19:50.2237717Z [INFO] flink-sequence-file ................................ SKIPPED
2020-06-22T20:19:50.2238397Z [INFO] flink-compress ..................................... SKIPPED
2020-06-22T20:19:50.2239109Z [INFO] flink-sql-orc ...................................... SKIPPED
2020-06-22T20:19:50.2239793Z [INFO] flink-sql-parquet .................................. SKIPPED
2020-06-22T20:19:50.2240474Z [INFO] flink-examples-streaming ........................... SKIPPED
2020-06-22T20:19:50.2241300Z [INFO] flink-examples-table ............................... SKIPPED
2020-06-22T20:19:50.2241996Z [INFO] flink-examples-build-helper ........................ SKIPPED
2020-06-22T20:19:50.2242836Z [INFO] flink-examples-streaming-twitter ................... SKIPPED
2020-06-22T20:19:50.2243548Z [INFO] flink-examples-streaming-state-machine ............. SKIPPED
2020-06-22T20:19:50.2244395Z [INFO] flink-examples-streaming-gcp-pubsub ................ SKIPPED
2020-06-22T20:19:50.2245069Z [INFO] flink-container .................................... SKIPPED
2020-06-22T20:19:50.2245787Z [INFO] flink-queryable-state-runtime ...................... SKIPPED
2020-06-22T20:19:50.2246509Z [INFO] flink-mesos ........................................ SKIPPED
2020-06-22T20:19:50.2247208Z [INFO] flink-kubernetes ................................... SKIPPED
2020-06-22T20:19:50.2247921Z [INFO] flink-yarn ......................................... SKIPPED
2020-06-22T20:19:50.2248641Z [INFO] flink-gelly ........................................ SKIPPED
2020-06-22T20:19:50.2249329Z [INFO] flink-gelly-scala .................................. SKIPPED
2020-06-22T20:19:50.2250039Z [INFO] flink-gelly-examples ............................... SKIPPED
2020-06-22T20:19:50.2250869Z [INFO] flink-external-resources ........................... SKIPPED
2020-06-22T20:19:50.2251554Z [INFO] flink-external-resource-gpu ........................ SKIPPED
2020-06-22T20:19:50.2252277Z [INFO] flink-metrics-dropwizard ........................... SKIPPED
2020-06-22T20:19:50.2253101Z [INFO] flink-metrics-graphite ............................. SKIPPED
2020-06-22T20:19:50.2253833Z [INFO] flink-metrics-influxdb ............................. SKIPPED
2020-06-22T20:19:50.2254742Z [INFO] flink-metrics-prometheus ........................... SKIPPED
2020-06-22T20:19:50.2255401Z [INFO] flink-metrics-statsd ............................... SKIPPED
2020-06-22T20:19:50.2256077Z [INFO] flink-metrics-datadog .............................. SKIPPED
2020-06-22T20:19:50.2256755Z [INFO] flink-metrics-slf4j ................................ SKIPPED
2020-06-22T20:19:50.2257446Z [INFO] flink-cep-scala .................................... SKIPPED
2020-06-22T20:19:50.2258126Z [INFO] flink-table-uber ................................... SKIPPED
2020-06-22T20:19:50.2259021Z [INFO] flink-table-uber-blink ............................. SKIPPED
2020-06-22T20:19:50.2259740Z [INFO] flink-python ....................................... SKIPPED
2020-06-22T20:19:50.2260417Z [INFO] flink-sql-client ................................... SKIPPED
2020-06-22T20:19:50.2261262Z [INFO] flink-state-processor-api .......................... SKIPPED
2020-06-22T20:19:50.2261940Z [INFO] flink-ml-parent .................................... SKIPPED
2020-06-22T20:19:50.2262773Z [INFO] flink-ml-api ....................................... SKIPPED
2020-06-22T20:19:50.2263475Z [INFO] flink-ml-lib ....................................... SKIPPED
2020-06-22T20:19:50.2264295Z [INFO] flink-ml-uber ...................................... SKIPPED
2020-06-22T20:19:50.2264976Z [INFO] flink-scala-shell .................................. SKIPPED
2020-06-22T20:19:50.2265828Z [INFO] flink-dist ......................................... SKIPPED
2020-06-22T20:19:50.2266560Z [INFO] flink-yarn-tests ................................... SKIPPED
2020-06-22T20:19:50.2267234Z [INFO] flink-end-to-end-tests ............................. SKIPPED
2020-06-22T20:19:50.2267933Z [INFO] flink-cli-test ..................................... SKIPPED
2020-06-22T20:19:50.2268619Z [INFO] flink-parent-child-classloading-test-program ....... SKIPPED
2020-06-22T20:19:50.2269313Z [INFO] flink-parent-child-classloading-test-lib-package ... SKIPPED
2020-06-22T20:19:50.2269984Z [INFO] flink-dataset-allround-test ........................ SKIPPED
2020-06-22T20:19:50.2270699Z [INFO] flink-dataset-fine-grained-recovery-test ........... SKIPPED
2020-06-22T20:19:50.2271519Z [INFO] flink-datastream-allround-test ..................... SKIPPED
2020-06-22T20:19:50.2272199Z [INFO] flink-batch-sql-test ............................... SKIPPED
2020-06-22T20:19:50.2273062Z [INFO] flink-stream-sql-test .............................. SKIPPED
2020-06-22T20:19:50.2273762Z [INFO] flink-bucketing-sink-test .......................... SKIPPED
2020-06-22T20:19:50.2274588Z [INFO] flink-distributed-cache-via-blob ................... SKIPPED
2020-06-22T20:19:50.2275278Z [INFO] flink-high-parallelism-iterations-test ............. SKIPPED
2020-06-22T20:19:50.2275976Z [INFO] flink-stream-stateful-job-upgrade-test ............. SKIPPED
2020-06-22T20:19:50.2276675Z [INFO] flink-queryable-state-test ......................... SKIPPED
2020-06-22T20:19:50.2277352Z [INFO] flink-local-recovery-and-allocation-test ........... SKIPPED
2020-06-22T20:19:50.2278076Z [INFO] flink-elasticsearch5-test .......................... SKIPPED
2020-06-22T20:19:50.2278778Z [INFO] flink-elasticsearch6-test .......................... SKIPPED
2020-06-22T20:19:50.2279483Z [INFO] flink-quickstart ................................... SKIPPED
2020-06-22T20:19:50.2280173Z [INFO] flink-quickstart-java .............................. SKIPPED
2020-06-22T20:19:50.2280982Z [INFO] flink-quickstart-scala ............................. SKIPPED
2020-06-22T20:19:50.2281719Z [INFO] flink-quickstart-test .............................. SKIPPED
2020-06-22T20:19:50.2282416Z [INFO] flink-confluent-schema-registry .................... SKIPPED
2020-06-22T20:19:50.2283272Z [INFO] flink-stream-state-ttl-test ........................ SKIPPED
2020-06-22T20:19:50.2284102Z [INFO] flink-sql-client-test .............................. SKIPPED
2020-06-22T20:19:50.2284793Z [INFO] flink-streaming-file-sink-test ..................... SKIPPED
2020-06-22T20:19:50.2285536Z [INFO] flink-state-evolution-test ......................... SKIPPED
2020-06-22T20:19:50.2286243Z [INFO] flink-rocksdb-state-memory-control-test ............ SKIPPED
2020-06-22T20:19:50.2287011Z [INFO] flink-end-to-end-tests-common ...................... SKIPPED
2020-06-22T20:19:50.2287749Z [INFO] flink-metrics-availability-test .................... SKIPPED
2020-06-22T20:19:50.2288440Z [INFO] flink-metrics-reporter-prometheus-test ............. SKIPPED
2020-06-22T20:19:50.2289231Z [INFO] flink-heavy-deployment-stress-test ................. SKIPPED
2020-06-22T20:19:50.2290158Z [INFO] flink-connector-gcp-pubsub-emulator-tests .......... SKIPPED
2020-06-22T20:19:50.2291011Z [INFO] flink-streaming-kafka-test-base .................... SKIPPED
2020-06-22T20:19:50.2291740Z [INFO] flink-streaming-kafka-test ......................... SKIPPED
2020-06-22T20:19:50.2292476Z [INFO] flink-streaming-kafka011-test ...................... SKIPPED
2020-06-22T20:19:50.2293323Z [INFO] flink-streaming-kafka010-test ...................... SKIPPED
2020-06-22T20:19:50.2294177Z [INFO] flink-plugins-test ................................. SKIPPED
2020-06-22T20:19:50.2294858Z [INFO] dummy-fs ........................................... SKIPPED
2020-06-22T20:19:50.2295612Z [INFO] another-dummy-fs ................................... SKIPPED
2020-06-22T20:19:50.2296338Z [INFO] flink-tpch-test .................................... SKIPPED
2020-06-22T20:19:50.2297112Z [INFO] flink-streaming-kinesis-test ....................... SKIPPED
2020-06-22T20:19:50.2298047Z [INFO] flink-elasticsearch7-test .......................... SKIPPED
2020-06-22T20:19:50.2298793Z [INFO] flink-end-to-end-tests-common-kafka ................ SKIPPED
2020-06-22T20:19:50.2299551Z [INFO] flink-tpcds-test ................................... SKIPPED
2020-06-22T20:19:50.2300251Z [INFO] flink-netty-shuffle-memory-control-test ............ SKIPPED
2020-06-22T20:19:50.2301104Z [INFO] flink-python-test .................................. SKIPPED
2020-06-22T20:19:50.2301876Z [INFO] flink-statebackend-heap-spillable .................. SKIPPED
2020-06-22T20:19:50.2302551Z [INFO] flink-contrib ...................................... SKIPPED
2020-06-22T20:19:50.2303405Z [INFO] flink-connector-wikiedits .......................... SKIPPED
2020-06-22T20:19:50.2304247Z [INFO] flink-fs-tests ..................................... SKIPPED
2020-06-22T20:19:50.2305061Z [INFO] flink-docs ......................................... SKIPPED
2020-06-22T20:19:50.2305813Z [INFO] flink-walkthroughs ................................. SKIPPED
2020-06-22T20:19:50.2306505Z [INFO] flink-walkthrough-common ........................... SKIPPED
2020-06-22T20:19:50.2307228Z [INFO] flink-walkthrough-table-java ....................... SKIPPED
2020-06-22T20:19:50.2307920Z [INFO] flink-walkthrough-table-scala ...................... SKIPPED
2020-06-22T20:19:50.2308641Z [INFO] flink-walkthrough-datastream-java .................. SKIPPED
2020-06-22T20:19:50.2309341Z [INFO] flink-walkthrough-datastream-scala ................. SKIPPED
2020-06-22T20:19:50.2310060Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2310483Z [INFO] BUILD FAILURE
2020-06-22T20:19:50.2311235Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2311680Z [INFO] Total time: 17:47 min
2020-06-22T20:19:50.2312305Z [INFO] Finished at: 2020-06-22T20:19:50+00:00
2020-06-22T20:19:50.3937346Z [INFO] Final Memory: 326M/1212M
2020-06-22T20:19:50.3938874Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.3953129Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-jdbc_2.11: Compilation failure: Compilation failure:
2020-06-22T20:19:50.3954979Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[137,51] cannot find symbol
2020-06-22T20:19:50.3955665Z [ERROR] symbol:   variable f1
2020-06-22T20:19:50.3956090Z [ERROR] location: variable tuple2 of type java.lang.Object
2020-06-22T20:19:50.3958074Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[136,33] incompatible types: cannot infer functional interface descriptor for org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.StatementExecutorFactory<org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor>
2020-06-22T20:19:50.3959865Z [ERROR] -> [Help 1]
2020-06-22T20:19:50.3960153Z [ERROR] 
2020-06-22T20:19:50.3961449Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-22T20:19:50.3962383Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-22T20:19:50.3962995Z [ERROR] 
2020-06-22T20:19:50.3963580Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-22T20:19:50.3964490Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
2020-06-22T20:19:50.3964969Z [ERROR] 
2020-06-22T20:19:50.3965374Z [ERROR] After correcting the problems, you can resume the build with the command
2020-06-22T20:19:50.3966310Z [ERROR]   mvn <goals> -rf :flink-connector-jdbc_2.11
{code}"	FLINK	Closed	1	1	5351	test-stability
13285715	ExpressionReducer shouldn't escape the reduced string value	"ExpressionReducer shouldn't escape the reduced string value, the escaping should only happen in code generation, otherwise the output result is inccorect. 

The problem is this line I guess: https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala#L142

Here is a simple example to reproduce the problem:

{code:java}
  val smallTupleData3: Seq[(Int, Long, String)] = {
    val data = new mutable.MutableList[(Int, Long, String)]
    data.+=((1, 1L, ""你好""))
    data.+=((2, 2L, ""你好""))
    data.+=((3, 2L, ""你好世界""))
    data
  }

  @Test
  def test(): Unit = {
    val t = env.fromCollection(smallTupleData3)
      .toTable(tEnv, 'a, 'b, 'c)
    tEnv.createTemporaryView(""MyTable"", t)
    val sqlQuery = s""select * from MyTable where c = '你好'""

    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
    val sink = new TestingAppendSink
    result.addSink(sink)
    env.execute()
    println(sink.getAppendResults.mkString(""\n""))
  }
{code}

The output:

{code:java}
1,1,\u4F60\u597D
2,2,\u4F60\u597D
{code}

This is also mentioned in user mailing list: http://apache-flink.147419.n8.nabble.com/ParquetTableSource-blink-table-planner-tp1696p1720.html
"	FLINK	Resolved	2	1	5351	pull-request-available
13296434	Add new data structure interfaces in table-common	"This add the new data structure interfaces to {{table-common}}.

The planner and connector refactoring happens in a separate issue."	FLINK	Closed	3	7	5351	pull-request-available
13324639	KafkaTableITCase.testKafkaSourceSink hangs	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5844&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20]

{code}
2020-08-25T09:04:57.3569768Z ""Kafka Fetcher for Source: KafkaTableSource(price, currency, log_date, log_time, log_ts) -> SourceConversion(table=[default_catalog.default_database.kafka, source: [KafkaTableSource(price, currency, log_date, log_time, log_ts)]], fields=[price, currency, log_date, log_time, log_ts]) -> Calc(select=[(price + 1.0:DECIMAL(2, 1)) AS computed-price, price, currency, log_date, log_time, log_ts, (log_ts + 1000:INTERVAL SECOND) AS ts]) -> WatermarkAssigner(rowtime=[ts], watermark=[ts]) -> Calc(select=[ts, log_date, log_time, CAST(ts) AS ts0, price]) (1/1)"" #1501 daemon prio=5 os_prio=0 tid=0x00007f250000b800 nid=0x22b8 runnable [0x00007f2127efd000]
2020-08-25T09:04:57.3571373Z    java.lang.Thread.State: RUNNABLE
2020-08-25T09:04:57.3571672Z 	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
2020-08-25T09:04:57.3572191Z 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
2020-08-25T09:04:57.3572921Z 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
2020-08-25T09:04:57.3573419Z 	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
2020-08-25T09:04:57.3573957Z 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:377)
2020-08-25T09:04:57.3574809Z 	- locked <0x00000000fde5a308> (a java.lang.Object)
2020-08-25T09:04:57.3575448Z 	at org.apache.kafka.common.network.PlaintextTransportLayer.read(PlaintextTransportLayer.java:103)
2020-08-25T09:04:57.3576309Z 	at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:117)
2020-08-25T09:04:57.3577086Z 	at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:424)
2020-08-25T09:04:57.3577727Z 	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:385)
2020-08-25T09:04:57.3578403Z 	at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:651)
2020-08-25T09:04:57.3579486Z 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:572)
2020-08-25T09:04:57.3580240Z 	at org.apache.kafka.common.network.Selector.poll(Selector.java:483)
2020-08-25T09:04:57.3580880Z 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547)
2020-08-25T09:04:57.3581756Z 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
2020-08-25T09:04:57.3583015Z 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
2020-08-25T09:04:57.3583847Z 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1300)
2020-08-25T09:04:57.3584555Z 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)
2020-08-25T09:04:57.3585197Z 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1168)
2020-08-25T09:04:57.3585961Z 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:253)
{code}"	FLINK	Closed	2	1	5351	test-stability
13239460	Carry primary key and unique key information in TableSchema	"The primary key and unique key is a standard meta information in SQL. And they are important information for optimization, for example, AggregateRemove, AggregateReduceGrouping and state layout optimization for TopN and Join.

So in this issue, we want to extend {{TableSchema}} to carry more information about primary key and unique keys. So that the TableSource can declare this meta information."	FLINK	Closed	3	2	5351	pull-request-available
13471959	Supports predicate testing for new columns	"The currently added column, if there is a filter on it, will cause an error in the RowDataToObjectArrayConverter because the number of columns is not correct
We can make BinaryTableStats supports evolution from shorter rowData."	FLINK	Closed	3	4	6732	pull-request-available
13468839	Planner free in flink-table-store-codegen	"We currently have the table-planner bundled into flink-table-store-codegen, which causes:

* bundle jar is too big, 20+MB
* Dependence on planner code will make it difficult to be compatible with multiple versions of Flink"	FLINK	Closed	3	7	6732	pull-request-available
13238666	Introduce BaseArray and BaseMap to reduce conversion overhead to blink	"Currently, in internal data format of flink, the array is only BinaryArray, and the map is only BinaryMap. If the user writes a UDAF with arrays as parameters and return values, it will lead to frequent conversion between Java arrays and BinaryArrays (each conversion is equivalent to the entire array of copys), which is very time-consuming.

In order to avoid copy in conversion, BaseArray and BaseMap are introduced as internal formats.

BaseArray is the parent of GenericArray and BinaryArray, providing various read and write operations on an array.

GenericArray is a wrapper class for Java arrays, which internally wraps a Java array. This array stores some elements of internal data format.

Conversion can be avoided when the element type is a primitive type or a type that is consistent internally format and externally format. (Detail see: DataFormatConverters)

After our benchmark, the performance of UDAF using primitive Array has been improved by 10 times."	FLINK	Closed	3	1	6732	pull-request-available
13432559	Introduce TableStore API and refactor ITCases	"We need to refactor the FileStoreITCase, and even the Sink interface itself, which is a DataStream layer class that is more complex to build than a simple SQL can accomplish.
We need to think through a problem, StoreSink exposed API should be what kind of, currently about keyed is rather confusing."	FLINK	Closed	3	7	6732	pull-request-available
13222755	Introduce StreamOperatorFactory to help table perform the whole Operator CodeGen	"If we need CodeGen an entire Operator, one possible solution is to introduce an OperatorWrapper, then generate a CodeGen sub-Operator in OperatorWrapper's open, and then proxy all methods to the sub-Operator. But introduce OperatorWrapper results in multiple virtual function calls.

The another way is to introduce a StreamOperatorFactory. In runtime, we get the StreamOperatorFactory and create real operator to invoke. In this way, there is no redundant virtual call, the test results show that the performance improves by about 10% after the introduction of StreamOperatorFactory. (Benchmark for simple query: [https://github.com/JingsongLi/flink/blob/benchmarkop/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/benchmark/batch/CalcBenchmark.java])"	FLINK	Closed	3	2	6732	pull-request-available
13450143	Set Hadoop FileSystem for Orc reader	"Now orc reader uses Hadoop FileSystem, but orc writer uses Flink FileSystem.

This can lead to some inconsistencies and possibly classloader loading issues.
We can unify this by setting the Hadoop FileSystem that we get from Flink for the orc reader."	FLINK	Closed	3	4	6732	pull-request-available
13509173	Introduce a Predicate Visitor	At present, predicate is traversed in many places. We need a visitor mode, which can better traverse Predicate.	FLINK	Closed	3	4	6732	pull-request-available
13410771	CsvFilesystemStreamSinkITCase.testPart times out on AZP	"The test {{CsvFilesystemStreamSinkITCase.testPart}} times out on AZP.

{code}
2021-11-08T16:36:28.6542078Z Nov 08 16:36:28 org.junit.runners.model.TestTimedOutException: test timed out after 20 seconds
2021-11-08T16:36:28.6561998Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.putFields(ObjectOutputStream.java:463)
2021-11-08T16:36:28.6581789Z Nov 08 16:36:28 	at java.util.Locale.writeObject(Locale.java:2156)
2021-11-08T16:36:28.6601916Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6621871Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6632222Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6633082Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6633845Z Nov 08 16:36:28 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)
2021-11-08T16:36:28.6634442Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
2021-11-08T16:36:28.6634968Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6637691Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6640766Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6641958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6642763Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6643563Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6644365Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6645138Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6647747Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6648657Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6649439Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6650189Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6650958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6651975Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6652632Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6653314Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6664918Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6665679Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6666409Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6667211Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6667907Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6668585Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6669301Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6669991Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6670706Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6671353Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6672227Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6672878Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6673381Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6673864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6674366Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6674864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6675348Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6675851Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6676340Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6676827Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6677321Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6677797Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6678290Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6678781Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6679262Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6679764Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6680236Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
2021-11-08T16:36:28.6680733Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:632)
2021-11-08T16:36:28.6681669Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:548)
2021-11-08T16:36:28.6682620Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamConfig.setStreamOperatorFactory(StreamConfig.java:308)
2021-11-08T16:36:28.6683633Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setVertexConfig(StreamingJobGraphGenerator.java:713)
2021-11-08T16:36:28.6684729Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:461)
2021-11-08T16:36:28.6685788Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6686964Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6688033Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setChaining(StreamingJobGraphGenerator.java:377)
2021-11-08T16:36:28.6689024Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:178)
2021-11-08T16:36:28.6689973Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:116)
2021-11-08T16:36:28.6690944Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:960)
2021-11-08T16:36:28.6692027Z Nov 08 16:36:28 	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50)
2021-11-08T16:36:28.6692998Z Nov 08 16:36:28 	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39)
2021-11-08T16:36:28.6694130Z Nov 08 16:36:28 	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56)
2021-11-08T16:36:28.6695274Z Nov 08 16:36:28 	at org.apache.flink.test.util.MiniClusterPipelineExecutorServiceLoader$MiniClusterExecutor.execute(MiniClusterPipelineExecutorServiceLoader.java:137)
2021-11-08T16:36:28.6696466Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2095)
2021-11-08T16:36:28.6697500Z Nov 08 16:36:28 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
2021-11-08T16:36:28.6698470Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:772)
2021-11-08T16:36:28.6699494Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:753)
2021-11-08T16:36:28.6700427Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:574)
2021-11-08T16:36:28.6701363Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:556)
2021-11-08T16:36:28.6702418Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:118)
2021-11-08T16:36:28.6703498Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPart(FsStreamingSinkITCaseBase.scala:84)
2021-11-08T16:36:28.6704348Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6705109Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6705993Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6706775Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6707560Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-08T16:36:28.6708439Z Nov 08 16:36:28 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-08T16:36:28.6709327Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-08T16:36:28.6710196Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-08T16:36:28.6711113Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-11-08T16:36:28.6712067Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-11-08T16:36:28.6712971Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2021-11-08T16:36:28.6714031Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2021-11-08T16:36:28.6714883Z Nov 08 16:36:28 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-11-08T16:36:28.6715538Z Nov 08 16:36:28 	at java.lang.Thread.run(Thread.java:748)
2021-11-08T16:36:28.6715983Z Nov 08 16:36:28 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26165&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13306"	FLINK	Closed	2	1	6732	pull-request-available, test-stability
13480780	Can not overwrite from empty input	There is currently no data, which will not trigger an overwrite, which causes the semantics of the sql to not be correct	FLINK	Closed	3	1	6732	pull-request-available
13473756	Throw better exception when file not found in reading	"When reading a file, if it is found that the file does not exist, it directly throws a file not found exception, which is often difficult for users to understand.
We can make it more clear in the exception message, e.g.
The file cannot be found, this may be because the read is too slow and the previous snapshot expired, you can configure a larger snapshot.time-retained or speed up your read.


Caused by: java.io.FileNotFoundException: File does not exist: 
at org.apache.flink.table.store.file.utils.FileUtils.getFileSize(FileUtils.java:94) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]
at org.apache.flink.table.store.file.data.DataFileReader$DataFileRecordReader.<init>(DataFileReader.java:86) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]"	FLINK	Closed	1	4	6732	pull-request-available
13468477	Add extraFiles to DataFileMeta	"See FLINK-28244
 {code:java}
class DataFileMeta {
    String fileName;
    .....
    // store the name of extra files, extra files including changelog_file, primary_key_index_file, secondary_index_file, and etc...
    List<String> extraFiles;
}
{code}
Extra files can help us for many uses, including index files, changelog files and etc..."	FLINK	Closed	3	2	6732	pull-request-available
13365473	Refactor SlicingWindowAggOperatorBuilder to accept serializer instead of LogicalType	Now SlicingWindowAggOperatorBuilder accept LogicalTypes, it is better to avoid LogicalTypes in runtime operators and functions.	FLINK	Closed	3	4	6732	pull-request-available
13259291	Add hash distribution and sort grouping only when dynamic partition insert	"Now in BatchExecSinkRule, we don't have static partitions, if it is a partitioned table, will add hash distribution and sort grouping. It is wrong:
 # Group only when dynamic partition insert (not all partition fields are static partitions)
 # We can just hash and sort dynamic partition fields instead of all partition fields."	FLINK	Closed	3	7	6732	pull-request-available
13241007	Support intersect all and minus all to blink planner	"Now, we just support intersect and minus, See ReplaceIntersectWithSemiJoinRule and ReplaceMinusWithAntiJoinRule, replace intersect with null aware semi-join and distinct aggregate.

We need support intersect all and minus all too.

Presto and Spark already support them:

[https://github.com/prestodb/presto/issues/4918]

https://issues.apache.org/jira/browse/SPARK-21274

I think them have a good rewrite design and we can follow them:

1.For intersect all

Input Query
{code:java}
SELECT c1 FROM ut1 INTERSECT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
  SELECT c1
    FROM (
         SELECT replicate_row(min_count, c1)
         FROM (
              SELECT c1,
                     IF (vcol1_cnt > vcol2_cnt, vcol2_cnt, vcol1_cnt) AS min_count
              FROM (
                   SELECT   c1, count(vcol1) as vcol1_cnt, count(vcol2) as vcol2_cnt
                   FROM (
                        SELECT c1, true as vcol1, null as vcol2 FROM ut1
                        UNION ALL
                        SELECT c1, null as vcol1, true as vcol2 FROM ut2
                        ) AS union_all
                   GROUP BY c1
                   HAVING vcol1_cnt >= 1 AND vcol2_cnt >= 1
                  )
              )
          )
{code}
2.For minus all:

Input Query
{code:java}
SELECT c1 FROM ut1 EXCEPT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
 SELECT c1
    FROM (
     SELECT replicate_rows(sum_val, c1)
       FROM (
         SELECT c1, sum_val
           FROM (
             SELECT c1, sum(vcol) AS sum_val
               FROM (
                 SELECT 1L as vcol, c1 FROM ut1
                 UNION ALL
                 SELECT -1L as vcol, c1 FROM ut2
              ) AS union_all
            GROUP BY union_all.c1
          )
        WHERE sum_val > 0
       )
   )
{code}"	FLINK	Closed	3	2	6732	pull-request-available
13446520	Change 'path' to 'root-path' in table store	"path is easy to bother the user, he/she will misinterpret it as path of table.
Let's change it to root-path, and it will be clearer."	FLINK	Closed	3	4	6732	pull-request-available
13311655	flink-sql-connector-hive modules should merge hive-exec dependencies	Since hive-exec is a bundle jar, we should merge the bundle dependencies from hive-exec.	FLINK	Closed	1	1	6732	pull-request-available
13256249	Operators use fractions to decide how many managed memory to allocate	"* Operators allocate memory segments with the amount returned by {{MemoryManager#computeNumberOfPages}}.
 * Operators reserve memory with the amount returned by {{MemoryManager#computeMemorySize}}. 

This step activates the new fraction based managed memory."	FLINK	Closed	3	7	6732	pull-request-available
13422119	Init flink-table-store repository	"Create:
 * README.md
 * NOTICE LICENSE CODE_OF_CONDUCT
 * .gitignore
 * maven tools
 * releasing tools
 * github build workflow
 * pom.xml"	FLINK	Closed	3	7	6732	pull-request-available
13328749	Introduce createBucketWriter to BucketsBuilder	"{code:java}
@Internal
public abstract static class BucketsBuilder {
   ...
   
   @Internal
   public abstract BucketWriter<IN, BucketID> createBucketWriter() throws IOException;
}
{code}
FLINK-19345 depends on {{BucketWriter}} to write compacted file."	FLINK	Closed	3	7	6732	pull-request-available
13336670	Introduce File streaming compaction operators	Introduce CompactCoordinator and CompactOperator.	FLINK	Closed	3	7	6732	pull-request-available
13219884	Improve internal data format	"In FLINK-11701 , we introduce an abstract set of internal data formats to table-runtime-blink. This JIRA is a complement to it.

Introduce Decimal: Scale of this object is specified by the user, not automatically determined(like BigDecimal).

Introduce BinaryGeneric(GenericType): We don't care about specific types, we work through serializers.

Introduce LazyBinaryFormat: It can exist in two formats: binary or java object"	FLINK	Closed	3	2	6732	pull-request-available
13271909	Introduce unknown memory setting to table in blink planner	"After https://jira.apache.org/jira/browse/FLINK-14566

We can just set unknown resources with setting whether managed memory is used."	FLINK	Closed	3	7	6732	pull-request-available
13481206	Page not enough Exception in SortBufferMemTable	"When there are many partitions, the partition writer will seize memory and may have the following exceptions:
 !screenshot-1.png! 

We need to make sure that the writer has enough memory before it can start.

Actually, there is enough memory, because it can preempt from other writers. The problem is in OwnerMemoryPool.freePages, it should contain preemptable memory."	FLINK	Closed	3	1	6732	pull-request-available
13246709	Blink-planner not support generic TableSource	"Now there is a exception when user use table source like:
{code:java}
class MyTableSource[T] extend StreamTableSource[T]
{code}
The reason is that blink-planner use TypeExtractor to extract class from TableSource, and use this class to DataFormatConverter.

Now, table source has DataType return type, so we don't need extract class from TableSource, we can just use conversionClass of DataType."	FLINK	Resolved	3	7	6732	pull-request-available
13300233	Integrate orc to file system connector	"Integrate orc to file system connector, so in the sql world, users can create file system table with orc format by DDL, do some reading, writing and streaming writing. And the {{RowData}} is the sql data format. The works are:
 # Introduce OrcRowDataInputFormat with partition support.
 # Introduce RowDataVectorizer.
 # Introduce OrcFileSystemFormatFactory."	FLINK	Closed	3	7	6732	pull-request-available
13226620	Support e2e SortAggregate and HashAggregate operator run in batch mode	"1.Finish BatchExecSortAggregate

2.Finish BatchExecHashAggregate

3.Add AggITCase"	FLINK	Closed	3	2	6732	pull-request-available
13293216	Parquet columnar row reader read footer from wrong end	"{code:java}
readFooter(conf, path, range(splitStart, splitLength))
{code}
Should be:
{code:java}
readFooter(conf, path, range(splitStart, splitStart + splitLength))
{code}
 "	FLINK	Resolved	3	1	6732	pull-request-available
13370533	 Empty values with sort willl fail	SELECT * FROM (VALUES 1, 2, 3) AS T (a) WHERE a = 1 and a = 2 ORDER BY a	FLINK	Closed	3	1	6732	pull-request-available
13527173	Fix Parquet stats extractor	"Some bugs in Parquet stats extractor:
 # Decimal Supports
 # Timestamp Supports
 # Null nullCounts supports"	FLINK	Closed	3	1	6732	pull-request-available
13222817	Introduce ProjectionCodeGenerator and HashCodeGenerator for BaseRow	"Introduce efficient BaseRow operations:

1. Project Code Generator: Take out some fields of BaseRow to generate a new BaseRow.

2. HashCodeGenerator: Calculate a hash value based on some fields of BaseRow."	FLINK	Closed	3	2	6732	pull-request-available
13524757	Introduce streaming-read-atomic to ensure UB and UA cannot be split	"Currently, streaming source will be checkpoint in any time, this means UPDATE_BEFORE and UPDATE_AFTER can be split into two checkpoint.
Downstream can see intermediate state. This is weird in some cases.
So in this ticket, add streaming-read-atomic:
The option to enable return per iterator instead of per record in streaming read. This can ensure that there will be no checkpoint segmentation in iterator consumption."	FLINK	Closed	3	4	6732	pull-request-available
13280673	LastValueAggFunctionWithOrderTest compilation error due to incompatible types	"{{LastValueAggFunctionWithOrderTest}} failed to compile in latest release-1.10 nightly build with below error:
{code}
03:02:41.792 [INFO] -------------------------------------------------------------
03:02:41.792 [ERROR] COMPILATION ERROR : 
03:02:41.792 [INFO] -------------------------------------------------------------
03:02:41.792 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunctionWithOrderTest.java:[78,37] incompatible types: inference variable T has incompatible bounds
    equality constraints: N,java.lang.Byte,N,T,T,T,T,org.apache.flink.table.dataformat.BinaryString,T,T
    lower bounds: java.lang.Byte,org.apache.flink.table.dataformat.BinaryString
{code}

https://api.travis-ci.org/v3/job/639573134/log.txt"	FLINK	Closed	2	1	6732	pull-request-available
13244887	blink runner should avoid stream operator implementing BoundedOneInput	According to https://issues.apache.org/jira/browse/FLINK-11879 , BoundedOneInput should not coexist with checkpoint, so we can not use BoundedOneInput in streaming mode.	FLINK	Resolved	2	1	6732	pull-request-available
13239483	Introduce TypeInfo for LocalDate/LocalTime/LocalDateTime	"Now in the new type system of table, the default class of time type is LocalDate and so on.

There are some situations that need to be converted to TypeInformation, such as toDataStream, so we need to provide TypeInformation support such as LocalDate.

Introduce LocalTimeTypeInfo

Introduce LocalDateSerializer, LocalTimeSerializer, LocalDateTimeSerializer

Introduce LocalDateComparator, LocalTimeComparator, LocalDateTimeComparator"	FLINK	Closed	3	2	6732	pull-request-available
13480921	Support create table-store table with 'connector'='table-store'	"Support create table-store table with 'connector'='table-store': 

sink to table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'datagen',
    'fields.word.length' = '1'
);
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',  //should rename 'catalog-database'？
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; {code}
source from table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'print'
);
INSERT INTO word_table SELECT word FROM word_count;{code}"	FLINK	Closed	4	4	6732	pull-request-available
13351647	CsvFileCompactionITCase.testSingleParallelism test failure	"Happened on a development branch, but I can not see how my changes could be related.

https://dev.azure.com/pnowojski/Flink/_build/results?buildId=252&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=777327ab-6d4e-582e-3e76-4a9391c57e59


{noformat}
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 15.349 s <<< FAILURE! - in org.apache.flink.formats.csv.CsvFileCompactionITCase
[ERROR] testSingleParallelism(org.apache.flink.formats.csv.CsvFileCompactionITCase)  Time elapsed: 1.249 s  <<< FAILURE!
java.lang.AssertionError: expected:<[+I[0, 0, 0], +I[0, 0, 0], +I[1, 1, 1], +I[1, 1, 1], +I[2, 2, 2], +I[2, 2, 2], +I[3, 3, 3], +I[3, 3, 3], +I[4, 4, 4], +I[4, 4, 4], +I[5, 5, 5], +I[5, 5, 5], +I[6, 6, 6], +I[6, 6, 6], +I[7, 7, 7], +I[7, 7, 7], +I[8, 8, 8], +I[8, 8, 8], +I[9, 9, 9], +I[9, 9, 9], +I[10, 0, 0], +I[10, 0, 0], +I[11, 1, 1], +I[11, 1, 1], +I[12, 2, 2], +I[12, 2, 2], +I[13, 3, 3], +I[13, 3, 3], +I[14, 4, 4], +I[14, 4, 4], +I[15, 5, 5], +I[15, 5, 5], +I[16, 6, 6], +I[16, 6, 6], +I[17, 7, 7], +I[17, 7, 7], +I[18, 8, 8], +I[18, 8, 8], +I[19, 9, 9], +I[19, 9, 9], +I[20, 0, 0], +I[20, 0, 0], +I[21, 1, 1], +I[21, 1, 1], +I[22, 2, 2], +I[22, 2, 2], +I[23, 3, 3], +I[23, 3, 3], +I[24, 4, 4], +I[24, 4, 4], +I[25, 5, 5], +I[25, 5, 5], +I[26, 6, 6], +I[26, 6, 6], +I[27, 7, 7], +I[27, 7, 7], +I[28, 8, 8], +I[28, 8, 8], +I[29, 9, 9], +I[29, 9, 9], +I[30, 0, 0], +I[30, 0, 0], +I[31, 1, 1], +I[31, 1, 1], +I[32, 2, 2], +I[32, 2, 2], +I[33, 3, 3], +I[33, 3, 3], +I[34, 4, 4], +I[34, 4, 4], +I[35, 5, 5], +I[35, 5, 5], +I[36, 6, 6], +I[36, 6, 6], +I[37, 7, 7], +I[37, 7, 7], +I[38, 8, 8], +I[38, 8, 8], +I[39, 9, 9], +I[39, 9, 9], +I[40, 0, 0], +I[40, 0, 0], +I[41, 1, 1], +I[41, 1, 1], +I[42, 2, 2], +I[42, 2, 2], +I[43, 3, 3], +I[43, 3, 3], +I[44, 4, 4], +I[44, 4, 4], +I[45, 5, 5], +I[45, 5, 5], +I[46, 6, 6], +I[46, 6, 6], +I[47, 7, 7], +I[47, 7, 7], +I[48, 8, 8], +I[48, 8, 8], +I[49, 9, 9], +I[49, 9, 9], +I[50, 0, 0], +I[50, 0, 0], +I[51, 1, 1], +I[51, 1, 1], +I[52, 2, 2], +I[52, 2, 2], +I[53, 3, 3], +I[53, 3, 3], +I[54, 4, 4], +I[54, 4, 4], +I[55, 5, 5], +I[55, 5, 5], +I[56, 6, 6], +I[56, 6, 6], +I[57, 7, 7], +I[57, 7, 7], +I[58, 8, 8], +I[58, 8, 8], +I[59, 9, 9], +I[59, 9, 9], +I[60, 0, 0], +I[60, 0, 0], +I[61, 1, 1], +I[61, 1, 1], +I[62, 2, 2], +I[62, 2, 2], +I[63, 3, 3], +I[63, 3, 3], +I[64, 4, 4], +I[64, 4, 4], +I[65, 5, 5], +I[65, 5, 5], +I[66, 6, 6], +I[66, 6, 6], +I[67, 7, 7], +I[67, 7, 7], +I[68, 8, 8], +I[68, 8, 8], +I[69, 9, 9], +I[69, 9, 9], +I[70, 0, 0], +I[70, 0, 0], +I[71, 1, 1], +I[71, 1, 1], +I[72, 2, 2], +I[72, 2, 2], +I[73, 3, 3], +I[73, 3, 3], +I[74, 4, 4], +I[74, 4, 4], +I[75, 5, 5], +I[75, 5, 5], +I[76, 6, 6], +I[76, 6, 6], +I[77, 7, 7], +I[77, 7, 7], +I[78, 8, 8], +I[78, 8, 8], +I[79, 9, 9], +I[79, 9, 9], +I[80, 0, 0], +I[80, 0, 0], +I[81, 1, 1], +I[81, 1, 1], +I[82, 2, 2], +I[82, 2, 2], +I[83, 3, 3], +I[83, 3, 3], +I[84, 4, 4], +I[84, 4, 4], +I[85, 5, 5], +I[85, 5, 5], +I[86, 6, 6], +I[86, 6, 6], +I[87, 7, 7], +I[87, 7, 7], +I[88, 8, 8], +I[88, 8, 8], +I[89, 9, 9], +I[89, 9, 9], +I[90, 0, 0], +I[90, 0, 0], +I[91, 1, 1], +I[91, 1, 1], +I[92, 2, 2], +I[92, 2, 2], +I[93, 3, 3], +I[93, 3, 3], +I[94, 4, 4], +I[94, 4, 4], +I[95, 5, 5], +I[95, 5, 5], +I[96, 6, 6], +I[96, 6, 6], +I[97, 7, 7], +I[97, 7, 7], +I[98, 8, 8], +I[98, 8, 8], +I[99, 9, 9], +I[99, 9, 9]]> but was:<[+I[0, 0, 0], +I[1, 1, 1], +I[2, 2, 2], +I[3, 3, 3], +I[4, 4, 4], +I[5, 5, 5], +I[6, 6, 6], +I[7, 7, 7], +I[8, 8, 8], +I[9, 9, 9], +I[10, 0, 0], +I[11, 1, 1], +I[12, 2, 2], +I[13, 3, 3], +I[14, 4, 4], +I[15, 5, 5], +I[16, 6, 6], +I[17, 7, 7], +I[18, 8, 8], +I[19, 9, 9], +I[20, 0, 0], +I[21, 1, 1], +I[22, 2, 2], +I[23, 3, 3], +I[24, 4, 4], +I[25, 5, 5], +I[26, 6, 6], +I[27, 7, 7], +I[28, 8, 8], +I[29, 9, 9], +I[30, 0, 0], +I[31, 1, 1], +I[32, 2, 2], +I[33, 3, 3], +I[34, 4, 4], +I[35, 5, 5], +I[36, 6, 6], +I[37, 7, 7], +I[38, 8, 8], +I[39, 9, 9], +I[40, 0, 0], +I[41, 1, 1], +I[42, 2, 2], +I[43, 3, 3], +I[44, 4, 4], +I[45, 5, 5], +I[46, 6, 6], +I[47, 7, 7], +I[48, 8, 8], +I[49, 9, 9], +I[50, 0, 0], +I[51, 1, 1], +I[52, 2, 2], +I[53, 3, 3], +I[54, 4, 4], +I[55, 5, 5], +I[56, 6, 6], +I[57, 7, 7], +I[58, 8, 8], +I[59, 9, 9], +I[60, 0, 0], +I[61, 1, 1], +I[62, 2, 2], +I[63, 3, 3], +I[64, 4, 4], +I[65, 5, 5], +I[66, 6, 6], +I[67, 7, 7], +I[68, 8, 8], +I[69, 9, 9], +I[70, 0, 0], +I[71, 1, 1], +I[72, 2, 2], +I[73, 3, 3], +I[74, 4, 4], +I[75, 5, 5], +I[76, 6, 6], +I[77, 7, 7], +I[78, 8, 8], +I[79, 9, 9], +I[80, 0, 0], +I[81, 1, 1], +I[82, 2, 2], +I[83, 3, 3], +I[84, 4, 4], +I[85, 5, 5], +I[86, 6, 6], +I[87, 7, 7], +I[88, 8, 8], +I[89, 9, 9], +I[90, 0, 0], +I[91, 1, 1], +I[92, 2, 2], +I[93, 3, 3], +I[94, 4, 4], +I[95, 5, 5], +I[96, 6, 6], +I[97, 7, 7], +I[98, 8, 8], +I[99, 9, 9]]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase.assertIterator(CompactionITCaseBase.java:134)
	at org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase.innerTestNonPartition(CompactionITCaseBase.java:109)
	at org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase.testSingleParallelism(CompactionITCaseBase.java:96)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
"	FLINK	Closed	3	1	6732	test-stability
13222260	Enhance UserDefinedFunction interface to allow more user defined types	"1.Allow UDF & UDTF to access constant parameter values at getReturnType, see similar feature in hive: [https://hive.apache.org/javadocs/r2.2.0/api/org/apache/hadoop/hive/ql/udf/generic/GenericUDF.html#initializeAndFoldConstants-org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector:A-]
{code:java}
/**
 * The input arguments are the input arguments which are passed to the eval() method.
 * Only the literal arguments (constant values) are passed to the [[getResultType()]] method.
 * If non-literal arguments appear, it will pass nulls instead.
 *
 * @param arguments arguments of a function call (only literal arguments
 *                  are passed, nulls for non-literal ones)
 * @param argTypes The classes of the arguments of the called eval() method.
 * @return TypeInformation of result type or null if Flink should determine the type
 */
public TypeInformation<?> getResultType(Object[] arguments, Class<?>[] argTypes){code}
2.Allow AggregateFunction to decide its user define inputs types with argClasses.
{code:java}
/**
 * Returns the result type of the user defined inputs with a given signature.
 */
public TypeInformation[] getUserDefinedInputTypes(Class[] argTypes)
{code}"	FLINK	Closed	3	4	6732	pull-request-available
13423185	DropDelete is incorrect in CompactManager when outputLevel is zero	When output level is zero, there may be have other files in level 0, we can not drop delete.	FLINK	Closed	3	1	6732	pull-request-available
13459243	Rename Schema to TableSchema	There are some systems that use schema as a concept of database, so the Schema class will be very confuse in this case, it is better to rename it as TableSchema.	FLINK	Closed	3	4	6732	pull-request-available
13274302	Message of NoMatchingTableFactoryException should tell users what's wrong	"Currently, all the required properties should exist and match, otherwise, {{NoMatchingTableFactoryException}} will be thrown.

User have no idea to know what is wrong.

We can pick a best candidate to print where is wrong,  print requiredContext and supportedProperties in different formats.

In this way, users can know which property in requiredContext is lack, and which property is not allow in supportedProperties.

 "	FLINK	Resolved	1	1	6732	pull-request-available
13336175	HiveTableSourceITCase.testStreamPartitionRead is not stable on Azure	"Here are some instances:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7845&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7875&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code}
2020-10-19T09:17:36.2004157Z [INFO] Results:
2020-10-19T09:17:36.2004505Z [INFO] 
2020-10-19T09:17:36.2007981Z [ERROR] Failures: 
2020-10-19T09:17:36.2010669Z [ERROR]   HiveTableSourceITCase.testStreamPartitionRead:537 expected:<[+I(0,0,2020-05-06 00:00:00), +I(1,1,2020-05-06 00:10:00), +I(1,1_copy,2020-05-06 00:10:00), +I(2,2,2020-05-06 00:20:00), +I(2,2_copy,2020-05-06 00:20:00), +I(3,3,2020-05-06 00:30:00), +I(3,3_copy,2020-05-06 00:30:00), +I(4,4,2020-05-06 00:40:00), +I(4,4_copy,2020-05-06 00:40:00), +I(5,5,2020-05-06 00:50:00), +I(5,5_copy,2020-05-06 00:50:00)]> but was:<[]>
2020-10-19T09:17:36.2011985Z [INFO] 
2020-10-19T09:17:36.2012582Z [ERROR] Tests run: 80, Failures: 1, Errors: 0, Skipped: 3
2020-10-19T09:17:36.2012976Z [INFO] 
2020-10-19T09:17:36.2137222Z [INFO] ------------------------------------------------------------------------
2020-10-19T09:17:36.2140971Z [INFO] Reactor Summary:
2020-10-19T09:17:36.2141558Z [INFO] 
2020-10-19T09:17:36.2141987Z [INFO] Flink : Tools : Force Shading ...................... SUCCESS [  1.346 s]
2020-10-19T09:17:36.2142534Z [INFO] Flink : Test utils : ............................... SUCCESS [  1.845 s]
2020-10-19T09:17:36.2143098Z [INFO] Flink : Test utils : Junit ......................... SUCCESS [  3.265 s]
2020-10-19T09:17:36.2190677Z [INFO] Flink : Queryable state : .......................... SUCCESS [  0.077 s]
2020-10-19T09:17:36.2191261Z [INFO] Flink : FileSystems : Azure FS Hadoop .............. SUCCESS [ 12.600 s]
2020-10-19T09:17:36.2191821Z [INFO] Flink : Examples : ................................. SUCCESS [  0.249 s]
2020-10-19T09:17:36.2192380Z [INFO] Flink : Examples : Batch ........................... SUCCESS [  1.919 s]
{code}
"	FLINK	Closed	1	1	6732	pull-request-available
13274700	Better error message when insert partition with values	"Now, we not support insert partition with values like:

Insert into mytable partition (date='2019-08-08') values ('jason', 25)

Will throw a exception:

schema not match.

We should improve error message to tell user we not support this pattern."	FLINK	Resolved	3	1	6732	pull-request-available
13483164	Connection leak in orc reader	"1. OrcFileStatsExtractor forget closing reader.
2. HadoopReadOnlyFileSystem forget closing fsDataInputStream.

We need a pocket test to assert all connections are closed. "	FLINK	Closed	1	1	6732	pull-request-available
13219188	Move table-planner-blink type to table-runtime-blink	We should put types in runtime because runtime code relies heavily on types.	FLINK	Closed	3	4	6732	pull-request-available
13310591	IllegalArgumentException when reading filesystem partitioned table with stream mode	"IllegalArgumentException when reading filesystem partitioned table with stream mode.
{code:java}
Caused by: java.lang.IllegalArgumentException: FileInputFormats with multiple paths are not supported yet.
        at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:139) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.<init>(ContinuousFileMonitoringFunction.java:126) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.<init>(ContinuousFileMonitoringFunction.java:110) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createFileInput(StreamExecutionEnvironment.java:1513) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createInput(StreamExecutionEnvironment.java:1480) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.createInput(StreamExecLegacyTableSourceScan.scala:200) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.PhysicalLegacyTableSourceScan.getSourceTransformation(PhysicalLegacyTableSourceScan.scala:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:98) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlan(StreamExecLegacyTableSourceScan.scala:63) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
{code}
When reading filesystem partitioned table, there will maybe many directories to read, but {{ContinuousFileMonitoringFunction}} not support multiple paths, we should not use it."	FLINK	Closed	1	1	6732	pull-request-available
13440824	Optimize async compaction in MergeTreeWriter	"Currently Full Compaction may cause the writer to be blocked, which has an impact on LogStore latency.

We need to decouple compact and write, compact completely asynchronous.
But too many files will lead to unstable reads, when too many files, Compaction processing speed can not keep up with Writer, need to back press Writer.

Stop parameter: num-sorted-run.stop-trigger, default 10"	FLINK	Closed	3	4	6732	pull-request-available
13289467	Hide hive version to avoid user confuse	Version in Yaml/HiveCatalog needs to be consistent with the dependencies version. There are three places: version in metastore, version in dependencies, version in Yaml/HiveCatalog, users are easy to make mistakes.	FLINK	Resolved	3	1	6732	pull-request-available
13305806	Should throw a readable exception when group by Map type	"We use flink 1.10.0 ,  blink planner,  to  submit a batch sql job to read from a hive table which contains map type fields, and then aggregate.   the sql as below:

```
 create view aaa
 as select * from table1 where event_id = '0103002' and `day`='2020-05-13'
 and `hour`='13';
 create view view_1
 as
 select
 `day`,
 a.rtime as itime,
 a.uid as uid,
 trim(BOTH a.event.log_1['scene']) as refer_list,
 T.s as abflags,
 a.hdid as hdid,
 a.country as country
 from aaa as a
 left join LATERAL TABLE(splitByChar(trim(BOTH a.event.log_1['abflag]),
 ',')) as T(s) on true;

{color:#172b4d}CREATE VIEW view_6 as {color}
 {color:#172b4d} SELECT{color}
 {color:#172b4d} `uid`,{color}
 {color:#172b4d} `refer_list`,{color}
 {color:#172b4d} `abflag`,{color}
 {color:#172b4d}        last_value(country){color}
 {color:#172b4d} FROM view_1{color}
 {color:#172b4d} where `refer_list` in ('WELOG_NEARBY', 'WELOG_FOLLOW', 'WELOG_POPULAR'){color}
 {color:#172b4d} GROUP BY  `uid`, `refer_list`, abflag;{color}
 insert into ............
 ``` 

when submit the job, the exception occurs as below:
 org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
         at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
         at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
         at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
         at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
         at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
         at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:422)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)
         at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
         at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
 Caused by: java.lang.RuntimeException: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at sg.bigo.streaming.sql.StreamingSqlRunner.main(StreamingSqlRunner.java:143)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
         at java.lang.reflect.Method.invoke(Method.java:498)
         at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
         ... 11 more
 Caused by: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at org.apache.flink.table.planner.codegen.CodeGenUtils$.hashCodeForType(CodeGenUtils.scala:212)
         at org.apache.flink.table.planner.codegen.HashCodeGenerator$.$anonfun$generateCodeBody$1(HashCodeGenerator.scala:97)
         at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
         at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  
 and then we found the method hashCodeForType  in the CodeGenUtils class do not match MAP type.  and we fix it as below
```
 def hashCodeForType(
 ctx: CodeGeneratorContext, t: LogicalType, term: String): String = t.getTypeRoot match

{ case BOOLEAN => s""$\\{className[JBoolean]}

.hashCode($term)""
 case MAP => s""$\{className[BaseMap]}.getHashCode($term)""  //the code we add
 case TINYINT => s""$\{className[JByte]}.hashCode($term)""
 ```


 then the job can be sumitted, it run for a while, another exception occurs:
 java.lang.RuntimeException: Could not instantiate generated class 'HashAggregateWithKeys$1543'
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67)
 at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:46)
 at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:48)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:156)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:433)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
 at java.lang.Thread.run(Thread.java:745)
 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
 at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
 ... 8 more
 Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
 ... 10 more
 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
 at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
 ... 13 more
 Caused by: org.codehaus.commons.compiler.CompileException: Line 459, Column 57: A method named ""compareTo"" is not declared in any enclosing class nor any supertype, nor through a static import
 at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124)
 at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997)
 at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060)
 at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
 at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421)
 at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394)
 at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062)
 at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)
 at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575)
 at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2580)
 at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:215)
  "	FLINK	Closed	4	1	6732	pull-request-available
13522773	Introduce partition.expiration-time to automatically delete expired partitions	Same to snapshot expiration, we can also introduce partition expiration to automatically delete expired partitions in commit node.	FLINK	Closed	3	4	6732	pull-request-available
13513318	Refactor module name and documentation for filesystems	"* flink-table-store-filesystem => flink-table-store-filesystems
* flink-table-store-fs-oss-hadoop => flink-table-store-oss
* introduce a new page for oss only"	FLINK	Closed	3	7	6732	pull-request-available
13435475	Introduce BlockingIterator to help testing	"BlockingIterator provides the ability to bring timeout to blocking iterators.

It use a static cached \{@link ExecutorService}. We don't limit the number of threads since the work inside is I/O type."	FLINK	Closed	3	7	6732	pull-request-available
13279554	SQL client requires both legacy and blink planner to be on the classpath	"Sql client uses directly some of the internal classes of the legacy planner, thus it does not work with only the blink planner on the classpath.

The internal class that's being used is {{org.apache.flink.table.functions.FunctionService}}

This dependency was introduced in FLINK-13195"	FLINK	Closed	3	1	6732	pull-request-available
13288609	Introduce WritableVectors for abstract writing	"In FLINK-11899 , we need write vectors from parquet input streams.

We need abstract vector writing, in future, we can provide OffHeapVectors."	FLINK	Resolved	3	7	6732	pull-request-available
13505245	CodeGenLoader fails when temporary directory is a symlink	Same to FLINK-28102 	FLINK	Closed	3	1	6732	pull-request-available
13450354	Adjust table store document to catalog	After quite a bit of development, we needed to adjust the documentation, use the latest model, and tweak some details.	FLINK	Closed	3	4	6732	pull-request-available
13473511	Optimize Spark documentation to Catalog and Dataset	"* Introduce Dataset API.
* Unify table_store and tablestore."	FLINK	Closed	1	1	6732	pull-request-available
13347529	Clean useless codes: Never push calcProgram to correlate	projectProgram in StreamPhysicalCorrelateBase never be used.	FLINK	Closed	3	4	6732	pull-request-available
13422226	Introduce RecordReader and related classes for table store	"- Introduce RecordReader interface: The reader that reads the batches of records. 
- Introduce SortMergeReader: This reader is to read a list of `RecordReader`, which is already sorted by key and sequence number, and perform a sort merge algorithm. `KeyValue` with the same key will also be combined during sort merging.
- Introduce ConcatRecordReader: This reader is to concatenate a list of `RecordReader` and read them sequentially. The input list is already sorted by key and sequence number, and the key intervals do not overlap each other.
- Introduce FieldStats: Statistics for each field.
- Introduce SstPathFactory: Factory which produces new Path for sst files.
- Introduce SstFile and SstFileMeta: This SstFile includes several `KeyValue`, representing the changes inserted into the file storage."	FLINK	Closed	3	7	6732	pull-request-available
13434083	Add FileStore Continuous Reading ITCase	"* Add FileStore Continuous Reading ITCases
 * LOG_SYSTEM default is null, when null, use FileStore Continuous Reading
 * FileStore Continuous Reading supports latest scan"	FLINK	Closed	3	7	6732	pull-request-available
13271643	Add option to close shuffle when dynamic partition inserting	"When partition values are rare or have skew, if we shuffle by dynamic partitions, will break the performance.

We can have an option to close shuffle in such cases:

‘connector.sink.shuffle-by-partition.enable’ = ..."	FLINK	Closed	3	7	6732	pull-request-available
13473748	Default Changelog all when changelog producer is input	When changelog producer is input, It is implied that the file already contains all the changelogs	FLINK	Closed	1	4	6732	pull-request-available
13316614	Hive bundle jar URLs are broken	we should use [https://repo.maven.apache.org/maven2/] instead	FLINK	Closed	3	4	6732	pull-request-available
13341284	Partition commit is delayed when records keep coming	"When set partition-commit.delay=0, Users expect partitions to be committed immediately.

However, if the record of this partition continues to flow in, the bucket for the partition will be activated, and no inactive bucket will appear.

We need to consider listening to bucket created."	FLINK	Closed	3	1	6732	pull-request-available
13244456	TemporalTypesTest>ExpressionTestBase.evaluateExprs in blink planner fails on PST Timezone	"TemporalTypesTest>ExpressionTestBase.evaluateExprs in blink planner seems to have a bug on timezone.

When I run it locally at PST timezone, it always fails as:

{code:java}
[ERROR] Failures:
[ERROR]   TemporalTypesTest>ExpressionTestBase.evaluateExprs:154 Wrong result for: [CURRENT_DATE] optimized to: [CURRENT_DATE] expected:<2019-07-1[2]> but was:<2019-07-1[1]>
{code}

I didn't find it fails on travis. Thus I suspect there's a bug w.r.t. timezone."	FLINK	Closed	2	1	6732	pull-request-available
13435731	FileStore continuous bug for delete row kind records	Bug in FileStoreSourceSplitReader, the row is reused, we can not just modify the row kind. We should set back to insert kind before next keyvalue.	FLINK	Closed	3	7	6732	pull-request-available
13302027	Test and correct case insensitive for parquet and orc in hive	"Orc and parquet should be field names case insensitive to compatible with hive.

Both hive mapred reader and vectorization reader."	FLINK	Closed	2	1	6732	pull-request-available
13243118	Implement batch nested loop join in blink	"Nested loop join has two advantages:

1.Nested loop join is quicker when build row size is small.

2.Nested loop join support all kind of joins, include non-key join.

Plan:

Introduce NestedLoopJoinCodeGenerator.

Implement BatchExecNestedLoopJoin."	FLINK	Closed	3	2	6732	pull-request-available
13338963	Integrate new orc to Hive source	"After introducing `OrcColumnarRowFileInputFormat`

We need integrate it to Hive, including Hive 2+ and Hive 1.X"	FLINK	Closed	3	7	6732	pull-request-available
13221354	Introduce parquet ColumnarRow split reader	"Parquet ColumnarRow split reader is introduced to read parquet data in batches.

When returning each row of data, instead of actually retrieving each field, we use BaseRow's abstraction to return a Columnar Row-like view.

This will greatly improve the downstream filtered scenarios, so that there is no need to access redundant fields on the filtered data."	FLINK	Resolved	3	7	6732	pull-request-available
13304020	Introduce DataGen connector in table	"{code:java}
CREATE TABLE user (
    id BIGINT,
    age INT,
    description STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second'='100',

    'fields.id.kind' = 'sequence',
    'fields.id.start' = '1',

    'fields.age.kind' = 'random',
    'fields.age.min' = '0',
    'fields.age.max' = '100',

    'fields.description.kind' = 'random',
    'fields.description.length' = '100'
)

-- Default is random generator.
{code}"	FLINK	Closed	3	7	6732	pull-request-available
13250173	HiveCatalogUseBlinkITCase.testBlinkUdf constantly failed with 1.9.0-rc2	"I tried to build flink 1.9.0-rc2 from source and ran all tests in a linux server, HiveCatalogUseBlinkITCase.testBlinkUdf will be constantly fail. 

 

Fail trace:
{code:java}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 313.228 s <<< FAILURE! - in org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase
[ERROR] testBlinkUdf(org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase) Time elapsed: 305.155 s <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
at org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase.testBlinkUdf(HiveCatalogUseBlinkITCase.java:180)
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.UnfulfillableSlotRequestException: Could not fulfill slot request 35cf6fdc1b525de9b6eed13894e2e31d. Requested resource profile (ResourceProfile{cpuCores=0.0, heapMemoryInMB=0, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0, managedMemoryInMB=128}) is unfulfillable.
{code}
 "	FLINK	Closed	3	1	6732	pull-request-available
13267666	Optimize mapred.HadoopInputSplit to not serialize conf when split is not configurable	"JobConf may very big, contains hundreds of configurations, if it is serialized by every split, that will significantly reduce performance.

Consider thousands of splits, the akka thread of JobMaster will all on the serialization of conf. That may will lead to various akka timeouts too."	FLINK	Closed	3	7	6732	pull-request-available
13527201	Flink 1.16 should implement new LookupFunction	Only implements new LookupFunction, retry lookup join can work.	FLINK	Closed	3	1	6732	pull-request-available
13423409	Introduce metadataConsumer to InitContext in Sink	"In Table Store, we want to get the offsets of kafka writer, only the offset returned by the callback inside the KafkaWriter is accurate, so we need this callback mechanism.

This ticket wants to add metadataConsumer to InitContext in Sink:
{code:java}
/**
 * Returns a metadata consumer, the {@link SinkWriter} can publish metadata events of type
 * {@link MetaT} to the consumer. The consumer can accept metadata events in an asynchronous
 * thread, and the {@link Consumer#accept} method is executed very fast.
 */
default <MetaT> Optional<Consumer<MetaT>> metadataConsumer() {
    return Optional.empty();
}{code}
SinkWriter can get this consumer, and publish metadata to the consumer implemented by table store sink."	FLINK	Closed	3	7	6732	pull-request-available
13217071	Introduce an abstract set of data formats	"Blink uses an abstract set of data formats to make internal calculations use the binary format as much as possible. This minimizes the serialization overhead and java object overhead.

It includes:

BaseRow <=> Row

BaseMap <=> Java Map

BaseArray <=> Java array

BaseString  <=> Java String

Decimal <=> BigDecimal  //Scale of this object is specified by the user, not automatically determined(like BigDecimal).

int <=> Date //Flink used to use int in the calculation, but the remaining in Row is still Date, we will change it completely.

int <=> Time

long <=> Timestamp

byte[] <=> byte[]

BaseGeneric <=> T (GenericRelDataType, we don't know it, let user define serializer)

primitive type keep same, but use less boxed type."	FLINK	Closed	3	2	6732	pull-request-available
13525881	Provides option to sort partition for full stage in streaming read	"The overall order may be out of order due to the writing of the old partition. We can provide an option to sort the full reading stage by partition fields to avoid the disorder.
(Actually, Currently, it is out of order for partitions. Because HashMap is used, we may be able to sort according to the creation time of the first file?)"	FLINK	Closed	1	4	6732	pull-request-available
13293470	[Umbrella] Introduce datagen, print, blackhole connectors	"Discussion: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Introduce-TableFactory-for-StatefulSequenceSource-td39116.html]

Introduce:
 * DataGeneratorSource
 * DataGenTableSourceFactory
 * PrintTableSinkFactory
 * BlackHoleTableSinkFactory"	FLINK	Closed	3	2	6732	pull-request-available
13415504	Introduce ManagedTableFactory	"We need an interface to discover the managed table factory implementation for managed table:
{code:java}
/**
 * Base interface for configuring a managed dynamic table connector. The managed table factory is
 * used when there is no {@link FactoryUtil#CONNECTOR} option.
 */
@Internal
public interface ManagedTableFactory extends DynamicTableFactory {
 
    @Override
    default String factoryIdentifier() {
        return """";
    }
 
    /**
     * Enrich options from catalog and session information.
     *
     * @return new options of this table.
     */
    Map<String, String> enrichOptions(Context context);
 
    /** Notifies the listener that a table creation occurred. */
    void onCreateTable(Context context);
 
    /** Notifies the listener that a table drop occurred. */
    void onDropTable(Context context);
} {code}
A catalog that supports built-in dynamic table needs to implement the method in the Catalog (The GenericInMemoryCatalog and HiveCatalog will implement this method):
{code:java}
/**
 * If return true, the Table without specified connector will be translated to the Flink managed table.
 * See {@link CatalogBaseTable.TableKind#MANAGED}
 */
default boolean supportsManagedTable {
    return false;
} {code}"	FLINK	Closed	3	7	6732	pull-request-available
13510769	Introduce audit_log system table	"In some scenarios, users need to get the changelog to do some auditing work, such as determining the number of updates and inserts.

We can provide audit_log system table, users can get the rowkind information column.


{code:java}
INSERT INTO T VALUES ('1', '2', '3');
INSERT INTO T VALUES ('1', '4', '5');

SELECT * FROM T$audit_log;

users can get:
- ""+I"", ""1"", ""2"", ""3""
- ""-U"", ""1"", ""2"", ""3"";
- ""+U"", ""1"", ""4"", ""5""
{code}
"	FLINK	Closed	3	2	6732	pull-request-available
13234448	Introduce vector data format in blink	"Introduce vector data format to high performance parquet/orc source.

Introduce VectorizedColumnBatch: A VectorizedColumnBatch is a set of rows, organized with each column as a vector.

Introduce ColumnVector with subclasses: IntColumnVector, LongColumnVector and etc..

Introduce ColumnarRow: wrap VectorizedColumnBatch as a BaseRow."	FLINK	Closed	3	4	6732	pull-request-available
13555290	DispatcherResourceCleanupTest.testFatalErrorIfJobCannotBeMarkedDirtyInJobResultStore fails on AZP	"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53905&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=6800

failed with 
{noformat}
Oct 22 00:59:32 Caused by: java.io.IOException: Expected IOException.
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.lambda$testFatalErrorIfJobCannotBeMarkedDirtyInJobResultStore$6(DispatcherResourceCleanupTest.java:558)
Oct 22 00:59:32 	at org.apache.flink.runtime.testutils.TestingJobResultStore.createDirtyResultAsync(TestingJobResultStore.java:81)
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.Dispatcher.createDirtyJobResultEntryAsync(Dispatcher.java:1441)
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createDirtyJobResultEntryIfMissingAsync$45(Dispatcher.java:1422)
Oct 22 00:59:32 	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
Oct 22 00:59:32 	... 39 more

{noformat}"	FLINK	Resolved	2	11500	6847	pull-request-available, test-stability
13569823	Adaptive Scheduler restores from empty state if JM fails during restarting state	"If a JobManager failover occurs while the Job is in a Restarting state, the HA metadata is deleted (as if it was a globally terminal state) and the job restarts from an empty state after the JM comes back up:

Jobmanager killed after killing Taskmanager (restarting phase):
{noformat}
2024-02-26 10:10:12,147 DEBUG org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Ignore TaskManager pod that is already added: autoscaling-example-taskmanager-3-2
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.jobmaster.JobMaster                 [] - Received heartbeat request from 9b7e17b75812ab60ecf028e02368d0c2.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Received heartbeat from 251c25cf794e3c9396fc02306613507b.
2024-02-26 10:10:14,091 DEBUG org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [/10.244.0.120:55647] was disconnected because of [id: 0x4a61a791, /10.244.0.120:55647 :> /10.244.0.118:6123] DISCONNECTED
2024-02-26 10:10:14,091 DEBUG org.apache.pekko.remote.transport.ProtocolStateActor         [] - Association between local [tcp://flink@10.244.0.118:6123] and remote [tcp://flink@10.244.0.120:55647] was disassociated because the ProtocolStateActor failed: Unknown
2024-02-26 10:10:14,092 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2024-02-26 10:10:14,094 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesApplicationClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2024-02-26 10:10:14,095 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
2024-02-26 10:10:14,095 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
2024-02-26 10:10:14,315 DEBUG org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.Watcher [] - Watcher closed
2024-02-26 10:10:14,511 DEBUG org.apache.pekko.actor.CoordinatedShutdown                   [] - Performing task [terminate-system] in CoordinatedShutdown phase [actor-system-terminate]
2024-02-26 10:10:14,595 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
2024-02-26 10:10:14,596 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.{noformat}
Then the new JM comes back it doesn't find any checkpoints as the HA metadata was deleted (we couldn't see this in the logs of the shutting down JM):


{noformat}
2024-02-26 10:10:30,294 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from KubernetesStateHandleStore{configMapName='autoscaling-example-5ddd0b1ba346d3bfd5ef53a63772e43c-config-map'}.2024-02-26 10:10:30,394 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in KubernetesStateHandleStore{configMapName='autoscaling-example-5ddd0b1ba346d3bfd5ef53a63772e43c-config-map'}.{noformat}
Even the main method is re-run and the jobgraph is regenerated (which is expected given the HA metadata was removed incorrectly)"	FLINK	Resolved	2	1	6847	pull-request-available
13359971	AdaptiveSchedulerSlotSharingITCase.testSchedulingOfJobRequiringSlotSharing fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13510&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865
{code:java}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 23.313 s <<< FAILURE! - in org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase [ERROR] testSchedulingOfJobRequiringSlotSharing(org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase) Time elapsed: 20.683 s <<< ERROR! org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) at org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase.runJob(DeclarativeSchedulerSlotSharingITCase.java:83) at org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase.testSchedulingOfJobRequiringSlotSharing(DeclarativeSchedulerSlotSharingITCase.java:71) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
{code}"	FLINK	Closed	2	1	6847	pull-request-available, test-stability
13521721	Migrate LeaderElection-related unit tests to JUnit5	To prepare the merge of the {{MultipleComponentLeaderElectionService}}-related tests with the legacy test, we want to align the JUnit versin they are using.	FLINK	Resolved	3	7	6847	pull-request-available
13565616	Upgrade Flink CI Docker container to Ubuntu 22.04	"The current CI Docker image is based on Ubuntu 16.04. We already use 20.04 for the e2e tests. We can update the Docker image to a newer version to be on par with what we need in GitHub Actions (FLINK-33923).

This issue can cover the following topics:
 * Update to 22.04
 ** OpenSSL 1.0.0 dependency should be added for netty-tcnative support
 ** Use Python3 instead of Python 2.7 (python symlink needs to be added due to FLINK-34195) 
 * Removal of Maven (FLINK-33501 makes us rely on the Maven wrapper)"	FLINK	Reopened	3	1	6847	github-actions, pull-request-available
13426918	ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers failed	"We experienced a [build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30783&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=15997] in {{ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers}}. The test halted when waiting for the next leader in [ZooKeeperMultipleComponentLeaderElectionDriverTest:256|https://github.com/apache/flink/blob/e8742f7f5cac34852d0e621036e1614bbdfe8ec3/flink-runtime/src/test/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriverTest.java#L256]
{code}
Feb 04 18:02:54 ""main"" #1 prio=5 os_prio=0 tid=0x00007fab0800b800 nid=0xe07 waiting on condition [0x00007fab12574000]
Feb 04 18:02:54    java.lang.Thread.State: WAITING (parking)
Feb 04 18:02:54 	at sun.misc.Unsafe.park(Native Method)
Feb 04 18:02:54 	- parking to wait for  <0x000000008065c5c8> (a java.util.concurrent.CompletableFuture$Signaller)
Feb 04 18:02:54 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Feb 04 18:02:54 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Feb 04 18:02:54 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Feb 04 18:02:54 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Feb 04 18:02:54 	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
Feb 04 18:02:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers(ZooKeeperMultipleComponentLeaderElectionDriverTest.java:256)
[...]
{code}

The extended Maven logs indicate that the timeout happened while waiting for the second leader to be selected.
{code}
Test org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers is running.
--------------------------------------------------------------------------------
17:15:10,437 [           Thread-16] INFO  org.apache.curator.test.TestingZooKeeperMain                 [] - Starting server
17:15:10,450 [                main] INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Enforcing default ACL for ZK connections
17:15:10,451 [                main] INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Using '/flink/default' as Zookeeper namespace.
17:15:10,452 [                main] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Starting
17:15:10,455 [                main] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Default schema
17:15:10,462 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: CONNECTED
17:15:10,467 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
17:15:10,482 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
17:15:10,483 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
17:15:10,483 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
17:15:10,484 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
17:15:10,562 [    main-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
17:15:10,600 [                main] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
{code}"	FLINK	Resolved	3	1	6847	test-stability
13262831	Add TaskManageResourceInfo which match the memory compositions of taskmanager	"* information from TaskExecutorResourceSpec in flip-49, add it to TaskExecutorRegistration.

{code:json}
public class TaskManagerResourceInfo {
    private final double cpuCores;
    private final long frameworkHeap;
    private final long frameworkOffHeap;
    private final long taskHeap;
    private final long taskOffHeap;
    private final long shuffleMemory;
    private final long managedMemory;
    private final long jvmMetaSpace;
    private final long jvmOverhead;
    private final long totalProcessMemory;
}{code}
 * url: /taskmanagers/:taskmanagerid
 * response: add

{code:java}
resource: {
  cpuCores: 4,
  frameworkHeap: 134217728,
  frameworkOffHeap: 134217728,
  taskHeap: 181193928,
  taskOffHeap: 0,
  shuffleMemory: 33554432,
  managedMemory: 322122552,
  jvmMetaSpace: 134217728,
  jvmOverhead: 134217728,
  totalProcessMemory: 1073741824
}
{code}"	FLINK	Closed	3	7	6847	pull-request-available
13375384	Avoid discarding checkpoints in case of failure	"Both {{StateHandleStore}} implementations (i.e. [KubernetesStateHandleStore:157|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesStateHandleStore.java#L157] and [ZooKeeperStateHandleStore:170|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L170]) discard checkpoints if the checkpoint metadata wasn't written to the backend. 

This does not cover the cases where the data was actually written to the backend but the call failed anyway (e.g. due to network issues). In such a case, we might end up having a pointer in the backend pointing to a checkpoint that was discarded.

Instead of discarding the checkpoint data in this case, we might want to keep it for this specific use case. Otherwise, we might run into Exceptions when recovering from the Checkpoint later on. We might want to add a warning to the user pointing to the possibly orphaned checkpoint data."	FLINK	Closed	2	1	6847	pull-request-available
13509846	TaskManagerRunnerTest fails with 239 exit code (i.e. FatalExitExceptionHandler was called) NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1	"We're again experiencing 239 exit code being caused by {{FatalExitExceptionHandler}} due class loading issues:
{code}
04:53:03,365 [flink-akka.remote.default-remote-dispatcher-8] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-8' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1
        at akka.remote.transport.netty.NettyFutureBridge$.apply(NettyTransport.scala:65) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport.$anonfun$associate$1(NettyTransport.scala:566) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.remote.transport.netty.NettyFutureBridge$$anon$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:149) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:112) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 16 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43694&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8319

I created this as a follow-up of FLINK-26037 becasue we repurposed it and fixed a bug in FLINK-26037. But it looks like both are being caused by the same issue."	FLINK	Closed	3	7	6847	pull-request-available, test-stability
13562471	Workflow: Add nightly workflow for master and release-1.18	"The nightly builds run on master and the two most-recently released versions of Flink as those are the supported versions. This logic is currently captured in [flink-ci/git-repo-sync:sync_repo.sh|https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh#L28].

In [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure] we decided to go ahead and provide nightly builds for {{master}} and {{{}release-1.18{}}}. Keep in mind that 1.18 has no support for JDK 21"	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
13570336	Align retry mechanisms of FutureUtils	The retry mechanisms of FutureUtils include quite a bit of redundant code which makes it hard to understand and to extend. The logic should be aligned properly.	FLINK	Open	3	11500	6847	pull-request-available
13505229	HadoopModuleFactory creates error if the security module cannot be loaded	"[HadoopModuleFactory|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModuleFactory.java#L51] tries to load the {{{}HadoopModule{}}}. If it fails to load the module, it will log an error an return {{null}} which is going to be handled properly. The resulting error log is, therefore, confusing. We might want to lower the log level to warning since the error doesn't affect the Flink cluster in a fatal way.

We might want to make the cluster fail fatally if we consider this a sever usability problem."	FLINK	Resolved	4	1	6847	pull-request-available, starter
13554044	Reorganize CI stages	{{connect_2}} stage became obsolete due to the externalization of the connectors. We can merge {{connect_1}} and {{connect_2}} again into a single {{connect}} stage (and maybe rename it into something more meaningful?)	FLINK	Resolved	3	11500	6847	pull-request-available
13335243	Add Metaspace metric	We want to expose the currently used Metaspace memory as well that should be provided through the metrics system.	FLINK	Closed	3	7	6847	pull-request-available
13437227	ZooKeeperStateHandleStore.getAllAndLock ends up in a infinite loop if there's an entry marked for deletion that's not cleaned up, yet	"{{ZooKeeperStateHandleStore.getAllAndLock}} is used when recovering {{CompletedCheckpoints}}. It iterates over all childs and retries until it reaches a stable and consistent version (i.e. no entries are subject for deletion and no child nodes were added while accessing the ZK instance).

Additionally, {{ZooKeeperStateHandleStore}} marks entries for deletion internally before actually deleting them. This can lead to a state where an entry is marked for deletion but the discard failed causing the cleanup to fail. The entry will be left marked for deletion and another cleanup will be tried. This works infinitely. But the users has the ability to limit the amount of retries. In that case, the entry might be left marked.

Restarting Flink cluster will now try to access this ZooKeeperStateHandleStore recovering the checkpoints with this entry still being marked for deletion which will cause an error in [ZooKeeperStateHandleStore.getAllAndLock|https://github.com/apache/flink/blob/c3df4c3f1f868d40e1e70404bea41b7a007e8b08/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L413] which results in a retry loop that's not desired.

We actually don't need to retry in that case because the child can be ignored, as far as I can see."	FLINK	Resolved	1	1	6847	pull-request-available
13562466	Custom Action: Enable Java version in Flink's CI Docker image	Flink's CI Docker image comes with multiple Java versions which can be enabled through environment variables. We should have a custom action that sets these variables properly.	FLINK	Closed	3	7	6847	github-actions, pull-request-available
13567000	Enable Apache INFRA ephemeral runners for nightly builds	The nightly CI is currently still utilizing the GitHub runners. We want to switch to Apache INFRA's ephemeral runners (see [docs|https://cwiki.apache.org/confluence/display/INFRA/ASF+Infra+provided+self-hosted+runners]).	FLINK	In Progress	3	7	6847	pull-request-available
13430177	Make max retries configurable	"Right now, the retry mechanism is hard-coded to {{Integer.MAX_VALUE}}. We want to make that configurable as well and keep the default value at {{MAX_VALUE}}. This enables the user to disable the retry mechanism if necessary.

We verify how retries strategies are configured in other places and align with that to have a consistent user experience."	FLINK	Resolved	3	7	6847	pull-request-available
13483982	TypeSerializerUpgradeTestBase still uses 1.15 as the current version on master and release-1.16	"-TypeSerializerUpgradeTestBase still refers to 1.15 as the current version. We could use FlinkVersions.current() instead to avoid running into this issue again for future major updates.-

-I didn't check other occurrences of FlinkVersions. It should be verified as part of this Jira issue that we don't have the same issue in other locations as well.-

{{TypeSerializerUpgradeTestBase.CURRENT_VERSION}} can be a bit misleading. We might want to add some comment explaining that it's referring to the currently actually released major version and that the corresponding test data should reflect the state of {{1.x.0}} as a baseline for any tests"	FLINK	Closed	4	4	6847	pull-request-available, starter
13355145	Extend exception history to collect concurrent task failure that have a common root cause	We want to collect task failures that were caused by the same root cause.	FLINK	Closed	3	7	6847	pull-request-available
13562464	Custom Action: Move files within the Docker image to the root folder to match the user	The way the ci template is setup (right now) is to work in the root user's home folder. For this we're copying the checkout into /root. This copying is done in multiple places which makes it a candidate for a custom action.	FLINK	Closed	3	7	6847	github-actions, pull-request-available
13562472	Adds nightly trigger	The nightly workflows should be trigger for any supported release branch.	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
13310399	Improve exception if jar submission hits OOM	"When using the jar submission from the WebUI, if an OOM occurs in the user-code then the exception is not enriched with additional pointers to the relevant config options.

{code}
2020-06-09 15:12:33,075 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler   [] - Unhandled exception.
java.lang.OutOfMemoryError: Java heap space
	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:65) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$0(JarRunHandler.java:99) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler$$Lambda$306/0x0000000840598c40.get(Unknown Source) ~[?:?]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) [?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:834) [?:?]
{code}"	FLINK	Closed	3	4	6847	pull-request-available, usability
13566896	s3_setup is called in test_file_sink.sh even if the common_s3.sh is not sourced	"See example CI run from the FLINK-34150 PR:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56570&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3191
{code}
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_file_sink.sh: line 38: s3_setup: command not found
{code}"	FLINK	Resolved	3	1	6847	pull-request-available, test-stability
13544182	Run Kubernetes test is unstable on AZP	"This test 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51447&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=6213

fails with

{noformat}
2023-07-19T17:14:49.8144730Z Jul 19 17:14:49 deployment.apps/flink-task-manager created
2023-07-19T17:15:03.7983703Z Jul 19 17:15:03 job.batch/flink-job-cluster condition met
2023-07-19T17:15:04.0937620Z error: Internal error occurred: error executing command in container: http: invalid Host header
2023-07-19T17:15:04.0988752Z sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-11919909188/out/kubernetes_wc_out*': No such file or directory
2023-07-19T17:15:04.1017388Z Jul 19 17:15:04 FAIL WordCount: Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected e682ec6622b5e83f2eb614617d5ab2cf.
{noformat}"	FLINK	Resolved	1	1	6847	pull-request-available, test-stability
13562450	Trial Period: GitHub Actions	This issue is (in contrast to FLINK-27075 which is used for issues that were collected while preparing [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]) collecting all the subtasks that are necessary to initiate the trial phase for GitHub Actions (as discussed in [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]).	FLINK	Open	3	2	6847	github-actions
13562467	Custom Action: Select workflow configuration	"During experiments, we noticed that the GHA UI isn't capable of utilizing a random count of compositions of workflows. If we get into the 3rd level of composite workflow, the job name will be cut off in the left menu which makes navigating the jobs harder (because you have duplicate of the same job, e.g. Compile, belonging to different job profiles).

As a workaround, we came up with Flink CI workflow profiles to configure the CI template yaml that is used in every job. A profile configuration can be specified through a JSON file that lives in the {{.github/workflow}} folder. "	FLINK	Closed	3	7	6847	github-actions, pull-request-available
13364104	JobMasterTest.testReconnectionAfterDisconnect hangs on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14500&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8884

{code}

{code}"	FLINK	Closed	3	1	6847	pull-request-available, test-stability
13563759	Flink Job stuck in suspend state after losing leadership in HA Mode	"The observation is that Job manager goes to suspend state with a failed container not able to register itself to resource manager after timeout.

JM Log, see attached

 "	FLINK	Resolved	1	1	6847	pull-request-available
13515060	DefaultMultipleComponentLeaderElectionService triggers HA backend change even if it's not the leader	{{DefaultMultipleComponentLeaderElectionService}} calls {{LeaderElectionEventHandler#onLeaderInformationChange}} in any case even though the contracts of that method states that it should be only called by the leader to update the HA backend information (see [JavaDoc|https://github.com/apache/flink/blob/5a2f220e31c50306a60aae8281f0ab4073fb85e1/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/LeaderElectionEventHandler.java#L46-L50]).	FLINK	Closed	2	7	6847	pull-request-available
13522716	KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock	"We're seeing a test failure in {{KubernetesHighAvailabilityRecoverFromSavepointITCase}} due to a deadlock:
{code:java}
2023-02-01T18:53:35.5540322Z ""ForkJoinPool-1-worker-1"" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]
2023-02-01T18:53:35.5540900Z    java.lang.Thread.State: TIMED_WAITING (parking)
2023-02-01T18:53:35.5541272Z 	at sun.misc.Unsafe.park(Native Method)
2023-02-01T18:53:35.5541932Z 	- parking to wait for  <0x00000000d14d7b60> (a java.util.concurrent.CompletableFuture$Signaller)
2023-02-01T18:53:35.5542496Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2023-02-01T18:53:35.5543088Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)
2023-02-01T18:53:35.5543672Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2023-02-01T18:53:35.5544240Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)
2023-02-01T18:53:35.5544801Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-01T18:53:35.5545632Z 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)
2023-02-01T18:53:35.5546409Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=61916]

The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474"	FLINK	Resolved	1	1	6847	pull-request-available, test-stability
13438078	Disable tests relying on non-writable directories	"We have a number of tests that rely on {{File#setWritable}} to produce errors.
These currently fail on GHA because we're running the tests as root, who can delete files anyway.
Switching to a non-root user is a follow-up."	FLINK	Resolved	3	7	6847	github-actions, pull-request-available, test-stability
13567766	Increase test coverage for AdaptiveScheduler	"There are still several tests disabled for the {{AdaptiveScheduler}} which we can enable now. All the issues seem to have been fixed.

We can even remove the annotation {{@FailsWithAdaptiveScheduler}} now. It's not needed anymore."	FLINK	Resolved	3	11500	6847	pull-request-available
13581916	Introduce new component RescaleManager	The goal here is to collect the rescaling logic in a single component to improve testability.	FLINK	Resolved	3	7	6847	pull-request-available
13549102	LeaderElectionDriver.toString() is not implemented	"We noticed in FLINK-32678 that the {{toString()}} method of {{LeaderElectionDriver}} wasn't implemented with the FLINK-26522 changes. The legacy implementations actually provided a proper implementation. The {{MultipleComponentLeaderElectionDriver}}  implementations (which we reused in FLINK-26522) didn't provide such a method.

See [ZooKeeperLeaderElectionDriver.toString()|https://github.com/apache/flink/blob/release-1.17/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperLeaderElectionDriver.java#L236] and [KubernetesLeaderElectionDriver.toString()|https://github.com/apache/flink/blob/release-1.17/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L257] for comparison.

I'm marking this as a critical because it's a regression. But I'm not marking it as a blocker because it's only affecting the log output."	FLINK	Resolved	2	1	6847	pull-request-available, starter
13556281	SqlGatewayE2ECase failed due to ConnectException	"The container couldn't be started in [this build|https://github.com/XComp/flink/actions/runs/6696839844/job/18195926497#step:15:11765]:
{code}
Error: 20:18:40 20:18:40.111 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 110.789 s <<< FAILURE! - in org.apache.flink.table.gateway.SqlGatewayE2ECase
Error: 20:18:40 20:18:40.111 [ERROR] org.apache.flink.table.gateway.SqlGatewayE2ECase  Time elapsed: 110.789 s  <<< ERROR!
Oct 30 20:18:40 org.testcontainers.containers.ContainerLaunchException: Container startup failed for image prestodb/hdp2.6-hive:10
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:349)
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.doStart(HiveContainer.java:69)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:322)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1131)
Oct 30 20:18:40 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:28)
Oct 30 20:18:40 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Oct 30 20:18:40 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Oct 30 20:18:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Oct 30 20:18:40 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Oct 30 20:18:40 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Oct 30 20:18:40 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Oct 30 20:18:40 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Oct 30 20:18:40 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Oct 30 20:18:40 Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
Oct 30 20:18:40 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:334)
Oct 30 20:18:40 	... 29 more
Oct 30 20:18:40 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:553)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:344)
Oct 30 20:18:40 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
Oct 30 20:18:40 	... 30 more
Oct 30 20:18:40 Caused by: java.lang.RuntimeException: java.net.ConnectException: Failed to connect to /127.0.0.1:32779
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:93)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.containerIsStarted(GenericContainer.java:712)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:532)
Oct 30 20:18:40 	... 32 more
Oct 30 20:18:40 Caused by: java.net.ConnectException: Failed to connect to /127.0.0.1:32779
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:265)
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:183)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.java:224)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.java:108)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.java:88)
Oct 30 20:18:40 	at okhttp3.internal.connection.Transmitter.newExchange(Transmitter.java:169)
Oct 30 20:18:40 	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:41)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:94)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:88)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:229)
Oct 30 20:18:40 	at okhttp3.RealCall.execute(RealCall.java:81)
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:86)
Oct 30 20:18:40 	... 34 more
Oct 30 20:18:40 Caused by: java.net.ConnectException: Connection refused (Connection refused)
Oct 30 20:18:40 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
Oct 30 20:18:40 	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
Oct 30 20:18:40 	at java.base/java.net.Socket.connect(Socket.java:609)
Oct 30 20:18:40 	at okhttp3.internal.platform.Platform.connectSocket(Platform.java:130)
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:263)
Oct 30 20:18:40 	... 53 more
{code}"	FLINK	Resolved	3	7	6847	github-actions, pull-request-available, test-stability
13529601	Update japicmp configuration	"Update the japicmp reference version and wipe exclusions / enable API compatibility checks for {{@PublicEvolving}} APIs on the corresponding SNAPSHOT branch with the {{update_japicmp_configuration.sh}} script (see below).

For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.

Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.
{code:bash}
tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.sh
tools $ cd ..$ git add *$ git commit -m ""Update japicmp configuration for $RELEASE_VERSION"" {code}"	FLINK	Resolved	3	7	6847	pull-request-available
13405915	checkpoint directory is not configurable through the Flink configuration passed into the StreamExecutionEnvironment	"FLINK-19463 introduced the separation of {{StateBackend}} and {{{}CheckpointStorage{}}}. Before that, both were included in the same interface implementation [AbstractFileStateBackend|https://github.com/apache/flink/blob/0a76daba0a428a322f0273d7dc6a70966f62bf26/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFileStateBackend.java]. {{FsStateBackend}} was used as a default implementation pre-1.13.

pre-{{{}1.13{}}} initialized the checkpoint directory when instantiating the state backend (see [FsStateBackendFactory|https://github.com/apache/flink/blob/release-1.12/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsStateBackendFactory.java#L46]). Starting from {{1.13}} loading the {{CheckpointStorage}} is done by the {{CheckpointStorageLoader.load}} method that is called in various places:
 * Savepoint Disposal (through {{{}Checkpoints.loadCheckpointStorage{}}}) where it only relies on the configuration passed in by the cluster configuration (no application checkpoint storage is passed)
 * {{SchedulerBase}} initialization (through DefaultExecutionGraphBuilder) where it’s based on the cluster’s configuration but also the application configuration (i.e. the {{{}JobGraph{}}}’s setting) that would be considered if {{CheckpointConfig#configure}} would have the checkpoint storage included
 * {{StreamTask}} on the {{{}TaskManager{}}}’s side where it’s based on the configuration passed in by the {{JobVertex}} for the application’s {{CheckpointStorage}} and the {{{}TaskManager{}}}’s configuration (coming from the session cluster) for the fallback {{CheckpointStorage}}

The issue is that we don't set the checkpoint directory in the {{{}CheckpointConfig{}}}. Hence, it's not going to get picked up as a job-related property. Flink always uses the fallback provided by the session cluster configuration."	FLINK	Closed	3	1	6847	pull-request-available
13565253	file sink e2e tests with local setup are not executed if s3 credentials are not provided	"While looking into test parity between Azure Pipelines and GitHub Actions, I noticed that the only OpenSSL-based test is {{flink-end-to-end-tests/test-scripts/test_file_sink.sh}} which comes with a local and a s3 setting.

S3 requires S3 credential and the bucket information to be available through environment variables. That's handled in [flink-end-to-end-tests/test-scripts/common_s3.sh#L25|https://github.com/apache/flink/blob/8a9a08bf408aae8a33438a38614199efeb8f1c63/flink-end-to-end-tests/test-scripts/common_s3.sh#L25]. The problem is that this shell script is also source'd when running the test with local setup (see [flink-end-to-end-tests/test-scripts/test_file_sink.sh#L27|https://github.com/apache/flink/blob/a6bea224ed012e5594ee755526f54ae7f3b0d22f/flink-end-to-end-tests/test-scripts/test_file_sink.sh#L27]).

This means that also the local test is only running in the main repository which is not necessary."	FLINK	Resolved	3	1	6847	pull-request-available, test-stability
13339733	DispatcherTest.testOnRemovedJobGraphDoesNotCleanUpHAFiles does not test the desired functionality	{{DispatcherTest.testOnRemovedJobGraphDoesNotCleanUpHAFiles}} succeeds but due to different reasons: The used {{TestingJobGraphStore}} is not started. An {{IllegalStateException}} prevents the code from reaching the set `removeJobGraphFuture` to get triggered. Hence, the test succeeds but not for the reason the test was implemented for.	FLINK	Closed	4	1	6847	pull-request-available
13581917	Introduces RescaleManager#onTrigger endpoint	The new endpoint would allow use from separating observing change events from actually triggering the rescale operation.	FLINK	Resolved	3	7	6847	pull-request-available
13376730	Running Kerberized YARN application on Docker test (custom fs plugin) unstable	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17553&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=d47e27f5-9721-5d5f-1cf3-62adbf3d115d&l=7448
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17553&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873&l=6764
"	FLINK	Closed	3	1	6847	test-stability
13571898	HA deadlock between JobMasterServiceLeadershipRunner and DefaultLeaderElectionService	"We recently observed a deadlock in the JM within the HA system.
(see below for the thread dump)

[~mapohl] and I looked a bit into it and there appears to be a race condition when leadership is revoked while a JobMaster is being started.
It appears to be caused by {{JobMasterServiceLeadershipRunner#createNewJobMasterServiceProcess}} forwarding futures while holding a lock; depending on whether the forwarded future is already complete the next stage may or may not run while holding that same lock.
We haven't determined yet whether we should be holding that lock or not.

{code}
""DefaultLeaderElectionService-leadershipOperationExecutor-thread-1"" #131 daemon prio=5 os_prio=0 cpu=157.44ms elapsed=78749.65s tid=0x00007f531f43d000 nid=0x19d waiting for monitor entry  [0x00007f53084fd000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.runIfStateRunning(JobMasterServiceLeadershipRunner.java:462)
        - waiting to lock <0x00000000f1c0e088> (a java.lang.Object)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.revokeLeadership(JobMasterServiceLeadershipRunner.java:397)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.notifyLeaderContenderOfLeadershipLoss(DefaultLeaderElectionService.java:484)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1252/0x0000000840ddec40.accept(Unknown Source)
        at java.util.HashMap.forEach(java.base@11.0.22/HashMap.java:1337)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onRevokeLeadershipInternal(DefaultLeaderElectionService.java:452)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1251/0x0000000840dcf840.run(Unknown Source)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.lambda$runInLeaderEventThread$3(DefaultLeaderElectionService.java:549)
        - locked <0x00000000f0e3f4d8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1075/0x0000000840c23040.run(Unknown Source)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(java.base@11.0.22/CompletableFuture.java:1736)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.22/ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.22/ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(java.base@11.0.22/Thread.java:829)
{code}

{code}
""jobmanager-io-thread-1"" #636 daemon prio=5 os_prio=0 cpu=125.56ms elapsed=78699.01s tid=0x00007f5321c6e800 nid=0x396 waiting for monitor entry  [0x00007f530567d000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.hasLeadership(DefaultLeaderElectionService.java:366)
        - waiting to lock <0x00000000f0e3f4d8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElection.hasLeadership(DefaultLeaderElection.java:52)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.isValidLeader(JobMasterServiceLeadershipRunner.java:509)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$forwardIfValidLeader$15(JobMasterServiceLeadershipRunner.java:520)
        - locked <0x00000000f1c0e088> (a java.lang.Object)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner$$Lambda$1320/0x0000000840e1a840.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(java.base@11.0.22/CompletableFuture.java:859)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(java.base@11.0.22/CompletableFuture.java:837)
        at java.util.concurrent.CompletableFuture.postComplete(java.base@11.0.22/CompletableFuture.java:506)
        at java.util.concurrent.CompletableFuture.complete(java.base@11.0.22/CompletableFuture.java:2079)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.registerJobMasterServiceFutures(DefaultJobMasterServiceProcess.java:124)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:114)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess$$Lambda$1319/0x0000000840e1a440.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(java.base@11.0.22/CompletableFuture.java:859)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(java.base@11.0.22/CompletableFuture.java:837)
        at java.util.concurrent.CompletableFuture.postComplete(java.base@11.0.22/CompletableFuture.java:506)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(java.base@11.0.22/CompletableFuture.java:1705)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.22/ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.22/ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(java.base@11.0.22/Thread.java:829)
{code}"	FLINK	In Progress	3	1	6847	pull-request-available
13315521	"ZKCheckpointIDCounterMultiServersTest.testRecoveredAfterConnectionLoss failed with ""Address already in use"""	"[https://travis-ci.org/github/apache/flink/jobs/705770513]

{code}
15:09:34.674 [ERROR] testRecoveredAfterConnectionLoss(org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest)  Time elapsed: 5.74 s  <<< ERROR!
java.net.BindException: Address already in use
{code}"	FLINK	Closed	3	4	6847	test-stability
13544715	JobMasterTest.testRetrievingCheckpointStats fails with NPE on AZP	"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51452&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8654
fails with NPE as
{noformat}
Jul 20 01:01:33 01:01:33.491 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats  Time elapsed: 0.036 s  <<< ERROR!
Jul 20 01:01:33 java.lang.NullPointerException
Jul 20 01:01:33 	at org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats(JobMasterTest.java:2132)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 20 01:01:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 20 01:01:33 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 20 01:01:33 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
...
{noformat}"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13520750	LeaderElectionService.stop should always call revokeLeadership	"The current implementations of {{LeaderElectionService}} do not implement the {{stop()}} call consistently. Some (e.g. [StandaloneLeaderElectionService|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/StandaloneLeaderElectionService.java#L53] call revoke on the {{LeaderContender)}} whereas others don't (e.g. [DefaultLeaderElectionService|https://github.com/apache/flink/blob/6e1caa390882996bf2d602951b54e4bb2d9c90dc/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L96]). The [MultipleComponentLeaderElectionService|https://github.com/apache/flink/blob/0290715a57b8d243586ab747b0cd2416c8081012/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L166] does call revoke on the {{LeaderContender}} instances, though.

We should align this behavior and specify it in the LeaderElectionService contract before going ahead with refactoring the interfaces (FLIP-285)."	FLINK	Resolved	3	7	6847	pull-request-available
13256308	Use Java's Duration instead of Flink's Time	"As discussion in mailing list [here|https://lists.apache.org/x/thread.html/90ad2f1d7856cfe5bdc8f7dd678c626be96eeaeeb736e98f31660039@%3Cdev.flink.apache.org%3E] the community reaches a consensus that we will use Java's Duration for representing ""time interval"" instead of use Flink's Time for it.

Specifically, Flink has two {{Time}} classes, which are

{{org.apache.flink.api.common.time.Time}}
{{org.apache.flink.streaming.api.windowing.time.Time}}

the latter has been already deprecated and superseded by the former. Now we want to also deprecated the former and drop it in 2.0.0(we don't drop it just now because it is part of {{@Public}} interfaces)."	FLINK	Closed	3	7	6847	2.0-related, pull-request-available
13574442	PullRequest template doesn't use the correct format to refer to the testing code convention	The PR template refers to https://flink.apache.org/contributing/code-style-and-quality-common.html#testing rather than https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#7-testing	FLINK	Resolved	4	1	6847	pull-request-available
13523169	ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap fails with assertion	"A build failure in {{ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9831

{code}
Feb 05 01:13:44 [ERROR] Tests run: 30, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.174 s <<< FAILURE! - in org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest
Feb 05 01:13:44 [ERROR] org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap  Time elapsed: 2.026 s  <<< FAILURE!
Feb 05 01:13:44 org.opentest4j.AssertionFailedError: 
Feb 05 01:13:44 
Feb 05 01:13:44 Expecting value to be true but was false
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Feb 05 01:13:44 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Feb 05 01:13:44 	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap(ApplicationDispatcherBootstrapTest.java:361)
[...]
{code}"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13537483	Refactor MultipleComponentLeaderElectionDriver.Listener.notifyAllKnownLeaderInformation(Collection)	We could use {{Map<String, LeaderInformation>}} instead of {{Collection<LeaderInformationWithComponentId>}}	FLINK	Resolved	3	7	6847	pull-request-available
13546105	DistinctAggregateITCaseBase.testMultiDistinctAggOnDifferentColumn got stuck on AZP	"This build hangs https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51955&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=14399

{noformat}
Aug 04 03:03:47 ""ForkJoinPool-1-worker-51"" #28 daemon prio=5 os_prio=0 cpu=49342.66ms elapsed=3079.49s tid=0x00007f67ccdd0000 nid=0x5234 waiting on condition  [0x00007f6791a19000]
Aug 04 03:03:47    java.lang.Thread.State: WAITING (parking)
Aug 04 03:03:47 	at jdk.internal.misc.Unsafe.park(java.base@11.0.19/Native Method)
Aug 04 03:03:47 	- parking to wait for  <0x00000000ad3b1fb8> (a java.util.concurrent.CompletableFuture$Signaller)
Aug 04 03:03:47 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.19/LockSupport.java:194)
Aug 04 03:03:47 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.19/CompletableFuture.java:1796)
Aug 04 03:03:47 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.19/ForkJoinPool.java:3118)
Aug 04 03:03:47 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.19/CompletableFuture.java:1823)
Aug 04 03:03:47 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.19/CompletableFuture.java:1998)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:171)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
Aug 04 03:03:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
Aug 04 03:03:47 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
Aug 04 03:03:47 	at java.util.Iterator.forEachRemaining(java.base@11.0.19/Iterator.java:132)
Aug 04 03:03:47 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:122)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:309)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:145)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:109)
Aug 04 03:03:47 	at org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.testMultiDistinctAggOnDifferentColumn(DistinctAggregateITCaseBase.scala:97)
~~
{noformat}
it is very likely that it is an old issue
the similar case was mentioned for 1.11.0 and closed because of lack of occurrences 
FLINK-16923

and another similar one FLINK-22100 which was marked as a duplicate of FLINK-21996"	FLINK	Resolved	1	1	6847	pull-request-available, test-stability
13429750	ZooKeeperStateHandleStore does not handle not existing nodes properly in getAllAndLock	[c3a6b514595ea3c1bf52126f6f1715b26c871ae9|https://github.com/apache/flink/commit/c3a6b514595ea3c1bf52126f6f1715b26c871ae9] introduces new exceptions that are not properly handled in [ZooKeeperStateHandleStore:378|https://github.com/apache/flink/blob/0cf7c3dedd3575cdfed57727e9712c28c013d7ca/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L378]	FLINK	Resolved	4	1	6847	pull-request-available
13558344	Remove scala-2.12 system variable	We're only relying on Scala 2.12 right now. Now need to have the scala version specified. This makes the build system scripts easier.	FLINK	Resolved	3	11500	6847	pull-request-available
13562470	Workflow: Add basic CI that will run with the default configuration	Runs the Flink CI template with the default configuration (Java 8) and can be enabled in each push to the branch.	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
13433862	Failing to cleanup a job should not fail the Flink Cluster in Session Mode	"We introduced the option to disable the retryable cleanup in FLINK-26331. This should make Flink fall back to the 1.14- functionality with just printing a warning in session mode.

Instead, a {{RetryException}} is thrown which causes Flink to fail fatally. For Job and Application Mode failing fatally is ok because it doesn't affect other builds. But for session mode, we want to print a warning, instead."	FLINK	Resolved	1	1	6847	pull-request-available
13537532	Move error handling into MultipleComponentLeaderElectionDriverFactory	{{LeaderElectionDriverFactory}} allows passing the error handling which can then be used to pass in an error handler that  forwards any error to the contender.	FLINK	Resolved	3	7	6847	pull-request-available
13553837	LicenseChecker fails in GHA but succeeds in Azure	"Both builds are based on [master@011b6b44|https://github.com/apache/flink/commit/011b6b44]:
 * [GitHub Actions|https://github.com/XComp/flink/actions/runs/6487689661/job/17620650207#step:8:41307]
 * [Azure CI|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53624&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=43193]

The GitHub Actions run reports 12 severe issues where it's unclear where they are coming from:
{code:java}
23:30:45,534 WARN  org.apache.flink.tools.ci.licensecheck.LicenseChecker        [] - Found a total of 12 severe license issues {code}"	FLINK	Resolved	3	7	6847	github-actions, test-stability
13434692	CheckpointIDCounter.shutdown should expose errors asynchronously	"{{CheckpointIDCounter.shutdown}} should return a {{CompletableFuture}} exposing errors if anything happens to enable retrying of the cleanup.

All implementations should also work in an idempotent fashion, i.e. nothing should happen if the artifact was already deleted.

This bug is not considered a 1.15 blocker because unit tests rerunning the shutdown worked properly already, i.e. in the worst case (of an error) we would just miss to clean up right now."	FLINK	Resolved	3	1	6847	pull-request-available
13434297	FileSystemJobResultStore#constructDirtyPath might lost the scheme	" 
{code:java}
/**
     * Given a job ID, construct the path for a dirty entry corresponding to it in the job result
     * store.
     *
     * @param jobId The job ID to construct a dirty entry path from.
     * @return A path for a dirty entry for the given the Job ID.
     */
    private Path constructDirtyPath(JobID jobId) {
        return new Path(this.basePath.getPath(), jobId.toString() + DIRTY_FILE_EXTENSION);
    } {code}
 

Just like above piece of code, we are using {{{}this.basePath.getPath(){}}}, not directly use {{this.basePath}} when create a new Path. I am afraid this will cause scheme lost and cause issue when some filesystem implementation tries to stat the path."	FLINK	Resolved	1	1	6847	pull-request-available
13219310	Standby per job mode Dispatchers don't know job's JobSchedulingStatus	"At the moment, it can happen that standby {{Dispatchers}} in per job mode will restart a terminated job after they gained leadership. The problem is that we currently clear the {{RunningJobsRegistry}} once a job has reached a globally terminal state. After the leading {{Dispatcher}} terminates, a standby {{Dispatcher}} will gain leadership. Without having the information from the {{RunningJobsRegistry}} it cannot tell whether the job has been executed or whether the {{Dispatcher}} needs to re-execute the job. At the moment, the {{Dispatcher}} will assume that there was a fault and hence re-execute the job. This can lead to duplicate results.

I think we need some way to tell standby {{Dispatchers}} that a certain job has been successfully executed. One trivial solution could be to not clean up the {{RunningJobsRegistry}} but then we will clutter ZooKeeper."	FLINK	Closed	3	1	6847	pull-request-available
13449803	KeyedStateCheckpointingITCase.KeyedStateCheckpointingITCase ends up in infinite failover loop	"-We observed several situations already where log files reached a file size of over 120G. This caused the worker's disk usage to reach 100% resulting in the worker machine to go ""offline"", i.e. not being available to pick up new tasks.-

The initially observed excessive log spilling is due to a TaskManager failing fatally which results in the requested number of slots never becoming available and the test job ending up in an infinite failover/restart loop. See further details in the comment section."	FLINK	Closed	1	1	6847	test-stability
13561821	Move Slack Invite URL into config.toml	Instead of 4 locations, we want to update only one location with the invite link. Additionally, we should add documentation on how to update the link.	FLINK	Resolved	3	1	6847	pull-request-available
13419212	Introduce JobResultStore	"This issue includes introducing the interface and coming up with a in-memory implementation of it that should be integrated into the {{Dispatcher}}. 
* We’ll introduce the new interface {{JobResultStore}}
* We’ll remove the {{RunningJobsRegistry}} a replace its functionality with {{(Standalone|Embedded)JobResultStore}} (This is basically only about {{RunningJobsRegistry#setJobFinished}} method
* The {{JobResultStore}} should be initialized along the {{JobGraphWriter}} since both components are closely related."	FLINK	Resolved	3	7	6847	pull-request-available
13411256	Dropdown menu is not properly shown in UI	"FLINK-21867 introduced a new dropdown menu to browse through concurrently failed {{Executions}}. This feature is disabled due to ngzorro modules not being imported properly in {{release-1.14}}.

Additionally, the tooltip is not printed correctly.

These two issues are fixed on {{master}} already due to [903185d|https://github.com/apache/flink/commit/903185d72c97dd93c777eeb90cb81a7b1c7465e7]"	FLINK	Closed	4	1	6847	pull-request-available
13553705	.scalafmt.conf cannot be found in Test packaging/licensing job	https://github.com/XComp/flink/actions/runs/6473584177/job/17581941684#step:8:4327	FLINK	Resolved	3	7	6847	github-actions, test-stability
13302625	Make memory configuration logging more user-friendly	"The newly introduced memory configuration logs some output when using the Mini Cluster (or local environment):

{code}
2020-05-04 11:50:05,984 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.cpu.cores' , default: null (fallback keys: []) required for local execution is not set, setting it to its default value 1.7976931348623157E308
2020-05-04 11:50:05,989 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.task.heap.size' , default: null (fallback keys: []) required for local execution is not set, setting it to its default value 9223372036854775807 bytes
2020-05-04 11:50:05,989 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.task.off-heap.size' , default: 0 bytes (fallback keys: []) required for local execution is not set, setting it to its default value 9223372036854775807 bytes
2020-05-04 11:50:05,990 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.network.min' , default: 64 mb (fallback keys: [{key=taskmanager.network.memory.min, isDeprecated=true}]) required for local execution is not set, setting it to its default value 64 mb
2020-05-04 11:50:05,990 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.network.max' , default: 1 gb (fallback keys: [{key=taskmanager.network.memory.max, isDeprecated=true}]) required for local execution is not set, setting it to its default value 64 mb
2020-05-04 11:50:05,991 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.managed.size' , default: null (fallback keys: [{key=taskmanager.memory.size, isDeprecated=true}]) required for local execution is not set, setting it to its default value 128 mb
{code}

This logging output could be made more user-friendly the following way:

* Print only the key string of a {{ConfigOption}}, not the config option object with all the deprecated keys
* Skipping the lines for {{taskmanager.memory.task.heap.size}} and {{taskmanager.memory.task.off-heap.size}} - we don't really set them (they are JVM paramaters) and the printing of long max looks strange (user would have to know these are place holders without effect).
* Maybe similarly skipping the CPU cores value, this looks the strangest (double max)."	FLINK	Closed	3	4	6847	pull-request-available, usability
13345823	Remove SlotProviderStrategy	Once FLINK-19919 has been completed and the {{DefaultExecutionSlotAllocator}} has been removed, we can remove the {{SlotProviderStrategy}} as it is no longer needed.	FLINK	Closed	3	7	6847	pull-request-available
13432061	Missing logs during retry	The {{FutureRetry.retry}} functionality doesn't log the errors but just trigger a retry. This makes it harder for the user to figure out what's wrong.	FLINK	Resolved	2	1	6847	pull-request-available
13520303	Remove JVM asserts from leader election code	{{assert}} is not enabled in the test run. We should using Preconditions	FLINK	Resolved	3	7	6847	pull-request-available
13300487	freeSlot in TaskExecutor.closeJobManagerConnection cause ConcurrentModificationException	"TaskExecutor may freeSlot when closeJobManagerConnection. freeSlot will modify the TaskSlotTable.slotsPerJob. this modify will cause ConcurrentModificationException.
{code:java}
Iterator<AllocationID> activeSlots = taskSlotTable.getActiveSlots(jobId);

final FlinkException freeingCause = new FlinkException(""Slot could not be marked inactive."");

while (activeSlots.hasNext()) {
 AllocationID activeSlot = activeSlots.next();

 try {
 if (!taskSlotTable.markSlotInactive(activeSlot, taskManagerConfiguration.getTimeout())) {
 freeSlotInternal(activeSlot, freeingCause);
 }
 } catch (SlotNotFoundException e) {
 log.debug(""Could not mark the slot {} inactive."", jobId, e);
 }
}
{code}
 error log：
{code:java}
2020-04-21 23:37:11,363 ERROR org.apache.flink.runtime.rpc.akka.AkkaRpcActor                - Caught exception while executing runnable in main thread.
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437)
    at java.util.HashMap$KeyIterator.next(HashMap.java:1461)
    at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable$TaskSlotIterator.hasNext(TaskSlotTable.java:698)
    at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable$AllocationIDIterator.hasNext(TaskSlotTable.java:652)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.closeJobManagerConnection(TaskExecutor.java:1314)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$1300(TaskExecutor.java:149)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobLeaderListenerImpl.lambda$jobManagerLostLeadership$1(TaskExecutor.java:1726)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
    at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
{code}"	FLINK	Closed	3	1	6847	pull-request-available
13505682	We should add a proper error message in case the deprecated reflection-based instantiation of a reporter is triggered	"A [user reported an issue|https://lists.apache.org/thread/yjv0hof5qqnzq22xcjf3y2v61j48gqh4] with the {{PrometheusReporter}} instantiation. The {{NoSuchMethodException}} is caused by the {{PromethusReporter}} not having a default constructor implemented. Instead, the user should switch to the factory class approach.

We might want to -log a warning if the reflection-based approach still works but- wrap the {{NoSuchMethodException}} into a {{IllegalConfigurationException}} with a proper error message pointing to the factory-based approach"	FLINK	Resolved	3	4	6847	pull-request-available, starter
13529598	Publish the Dockerfiles for the new release	"Note: the official Dockerfiles fetch the binary distribution of the target Flink version from an Apache mirror. After publishing the binary release artifacts, mirrors can take some hours to start serving the new artifacts, so you may want to wait to do this step until you are ready to continue with the ""Promote the release"" steps in the follow-up Jira.

Follow the [release instructions in the flink-docker repo|https://github.com/apache/flink-docker#release-workflow] to build the new Dockerfiles and send an updated manifest to Docker Hub so the new images are built and published.

 
----
h3. Expectations
 * Dockerfiles in [flink-docker|https://github.com/apache/flink-docker] updated for the new Flink release and pull request opened on the Docker official-images with an updated manifest"	FLINK	Closed	3	7	6847	pull-request-available
13565966	Job doesn't disconnect from ResourceManager	"https://github.com/XComp/flink/actions/runs/7634987973/job/20800205972#step:10:14557

{code}
[...]
""main"" #1 prio=5 os_prio=0 tid=0x00007fcccc4b7000 nid=0x24ec0 waiting on condition [0x00007fccce1eb000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000bdd52618> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2099)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2077)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:876)
	at org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.testHopWindow_Cube(WindowDistinctAggregateITCase.scala:550)
[...]
{code}"	FLINK	In Progress	2	1	6847	github-actions, pull-request-available, test-stability
13428282	ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification failed on azure	"
{code:java}
2022-02-11T21:43:35.4936452Z Feb 11 21:43:35 java.lang.AssertionError: The TestingFatalErrorHandler caught an exception.
2022-02-11T21:43:35.4940444Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.after(TestingFatalErrorHandlerResource.java:81)
2022-02-11T21:43:35.4941937Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.access$300(TestingFatalErrorHandlerResource.java:36)
2022-02-11T21:43:35.4943249Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:60)
2022-02-11T21:43:35.4944745Z Feb 11 21:43:35 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-02-11T21:43:35.4945682Z Feb 11 21:43:35 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-02-11T21:43:35.4946655Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-11T21:43:35.4947847Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-02-11T21:43:35.4948876Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-02-11T21:43:35.4949842Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-02-11T21:43:35.4951142Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-02-11T21:43:35.4952153Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-02-11T21:43:35.4953115Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-02-11T21:43:35.4954068Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-02-11T21:43:35.4955003Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-02-11T21:43:35.4955981Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-02-11T21:43:35.4956930Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-11T21:43:35.4958008Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-02-11T21:43:35.4958899Z Feb 11 21:43:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-02-11T21:43:35.4959774Z Feb 11 21:43:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-02-11T21:43:35.4960911Z Feb 11 21:43:35 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-02-11T21:43:35.4962095Z Feb 11 21:43:35 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-02-11T21:43:35.4963136Z Feb 11 21:43:35 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-02-11T21:43:35.4964275Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-02-11T21:43:35.4965527Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-02-11T21:43:35.4966787Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-02-11T21:43:35.4968228Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-02-11T21:43:35.4969485Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-02-11T21:43:35.4970753Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-02-11T21:43:35.4971842Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-02-11T21:43:35.4973291Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-02-11T21:43:35.4974538Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-02-11T21:43:35.4975737Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-02-11T21:43:35.4976936Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-02-11T21:43:35.4978291Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-02-11T21:43:35.4979588Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-02-11T21:43:35.4980728Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-02-11T21:43:35.4981753Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-02-11T21:43:35.4982863Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-02-11T21:43:35.4983958Z Feb 11 21:43:35 Caused by: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Could not handle node changed event.
2022-02-11T21:43:35.4985193Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper(ZooKeeperLeaderRetrievalDriver.java:143)
2022-02-11T21:43:35.4986451Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.onReconnectedConnectionState(ZooKeeperLeaderRetrievalDriver.java:181)
2022-02-11T21:43:35.4987902Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.handleStateChange(ZooKeeperLeaderRetrievalDriver.java:164)
2022-02-11T21:43:35.4989235Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.lambda$new$0(ZooKeeperLeaderRetrievalDriver.java:61)
2022-02-11T21:43:35.4990632Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.lambda$processEvents$0(ConnectionStateManager.java:279)
2022-02-11T21:43:35.4991926Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)
2022-02-11T21:43:35.4993137Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)
2022-02-11T21:43:35.4994438Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2022-02-11T21:43:35.4995807Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:279)
2022-02-11T21:43:35.4997130Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
2022-02-11T21:43:35.4999203Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:132)
2022-02-11T21:43:35.4999927Z Feb 11 21:43:35 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-02-11T21:43:35.5000679Z Feb 11 21:43:35 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-02-11T21:43:35.5001306Z Feb 11 21:43:35 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-02-11T21:43:35.5001866Z Feb 11 21:43:35 	at java.lang.Thread.run(Thread.java:748)
2022-02-11T21:43:35.5002381Z Feb 11 21:43:35 Caused by: java.lang.IllegalStateException: java.lang.InterruptedException
2022-02-11T21:43:35.5003374Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest$QueueLeaderElectionListener.notifyLeaderAddress(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:375)
2022-02-11T21:43:35.5004370Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper(ZooKeeperLeaderRetrievalDriver.java:136)
2022-02-11T21:43:35.5005005Z Feb 11 21:43:35 	... 14 more
2022-02-11T21:43:35.5005464Z Feb 11 21:43:35 Caused by: java.lang.InterruptedException
2022-02-11T21:43:35.5006079Z Feb 11 21:43:35 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
2022-02-11T21:43:35.5006973Z Feb 11 21:43:35 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
2022-02-11T21:43:35.5007877Z Feb 11 21:43:35 	at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:353)
2022-02-11T21:43:35.5008787Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest$QueueLeaderElectionListener.notifyLeaderAddress(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:373)
2022-02-11T21:43:35.5009547Z Feb 11 21:43:35 	... 15 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31294&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7581
"	FLINK	Resolved	3	1	6847	pull-request-available, test-stability
13529795	Update reference data for Migration Tests	"# Update {{CURRENT_VERSION in TypeSerializerUpgradeTestBase}}  with the new version. This will likely fail some tests because snapshots are missing for that version. Generate them, for example in {{TypeSerializerUpgradeTestBase.}} 
 # (major/minor only) Update migration tests in master to cover migration from new version: (search for usages of FlinkV{{{}ersion{}}})
 ** AbstractOperatorRestoreTestBase
 ** CEPMigrationTest
 ** BucketingSinkMigrationTest
 ** FlinkKafkaConsumerBaseMigrationTest
 ** ContinuousFileProcessingMigrationTest
 ** WindowOperatorMigrationTest
 ** StatefulJobSavepointMigrationITCase
 ** StatefulJobWBroadcastStateMigrationITCase"	FLINK	Resolved	3	7	6847	pull-request-available
13378918	Root Exception can not be shown on Web UI in Flink 1.13.0	"Hi,
 
We have upgraded our Flink applications to 1.13.0 but we found that Root Exception can not be shown on Web UI with an internal server error message. After opening a browser development console and trace the message, we found that there is an exception in job manager:
 
_{color:#000000}2021-05-12 13:30:45,589 ERROR org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler [] - Unhandled exception.{color}_
_{color:#000000}java.lang.IllegalArgumentException: The location must not be null for a non-global failure.{color}_
    _{color:#000000}at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.assertLocalExceptionInfo(JobExceptionsHandler.java:218) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.createRootExceptionInfo(JobExceptionsHandler.java:191) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.SliceOps$1$1.accept(SliceOps.java:199) ~[?:?]{color}_
    _{color:#000000}at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1632) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:127) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:502) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:488) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?]{color}_
    _{color:#000000}at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ~[?:?]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.createJobExceptionHistory(JobExceptionsHandler.java:169) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.createJobExceptionsInfo(JobExceptionsHandler.java:154) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.handleRequest(JobExceptionsHandler.java:101) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler.handleRequest(JobExceptionsHandler.java:63) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:87) ~[flink-dist_2.12-1.13.0.jar:1.13.0]{color}_
    _{color:#000000}at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]{color}_
    _{color:#000000}at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]{color}_
    _{color:#000000}at java.lang.Thread.run(Thread.java:834) [?:?]{color}_
 
I see there are some exceptions in task managers and I remember the kind of exception can be shown in UI in version 1.12.1 :
 
_2021-05-18 00:50:30,261 WARN org.apache.flink.runtime.taskmanager.Task [] - xxx (23/90)#13 (c345fb009b5d93628b5a6d890c8f4226) switched from RUNNING to FAILED with failure cause: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager '10.194.65.3/10.194.65.3:44273'. This might indicate that the remote task manager was lost._
    _at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelInactive(CreditBasedPartitionRequestClientHandler.java:160)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)_
    _at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelInactive(NettyMessageClientDecoderDelegate.java:94)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)_
    _at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)_
    _at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)_
    _at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)_
    _at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)_
    _at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)_
    _at java.base/java.lang.Thread.run(Thread.java:834)_
 
 
 
The issue has been reported in flink-user mailing list before: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Root-Exception-can-not-be-shown-on-Web-UI-in-Flink-1-13-0-td43673.html"	FLINK	Closed	3	1	6847	pull-request-available
13346687	YARNSessionCapacitySchedulerITCase.perJobYarnClusterOffHeap test failed with NPE	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10989&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa]
{code:java}
2020-12-17T22:57:58.1994352Z Test perJobYarnClusterOffHeap(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase) failed with:
2020-12-17T22:57:58.1994893Z java.lang.NullPointerException: java.lang.NullPointerException
2020-12-17T22:57:58.1995439Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2020-12-17T22:57:58.1996185Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2020-12-17T22:57:58.1996919Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2020-12-17T22:57:58.1997526Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2020-12-17T22:57:58.1998193Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2020-12-17T22:57:58.1998960Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2020-12-17T22:57:58.1999876Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-12-17T22:57:58.2000346Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-12-17T22:57:58.2000744Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2020-12-17T22:57:58.2001532Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2020-12-17T22:57:58.2001915Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-12-17T22:57:58.2002286Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-12-17T22:57:58.2002734Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2020-12-17T22:57:58.2003185Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2020-12-17T22:57:58.2003447Z 
2020-12-17T22:57:58.2003708Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-12-17T22:57:58.2004233Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-12-17T22:57:58.2004810Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-12-17T22:57:58.2005468Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-12-17T22:57:58.2005907Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
2020-12-17T22:57:58.2006387Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateRuntimeException(RPCUtil.java:85)
2020-12-17T22:57:58.2006920Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:122)
2020-12-17T22:57:58.2007515Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:291)
2020-12-17T22:57:58.2008082Z 	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
2020-12-17T22:57:58.2008518Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-12-17T22:57:58.2008964Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-12-17T22:57:58.2009430Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2020-12-17T22:57:58.2010002Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2020-12-17T22:57:58.2010554Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2020-12-17T22:57:58.2011301Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2020-12-17T22:57:58.2011857Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2020-12-17T22:57:58.2012288Z 	at com.sun.proxy.$Proxy111.getApplications(Unknown Source)
2020-12-17T22:57:58.2012739Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:528)
2020-12-17T22:57:58.2013298Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:505)
2020-12-17T22:57:58.2013829Z 	at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:270)
2020-12-17T22:57:58.2014312Z 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:262)
2020-12-17T22:57:58.2014879Z 	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.perJobYarnClusterOffHeap(YARNSessionCapacitySchedulerITCase.java:205)
2020-12-17T22:57:58.2015399Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-12-17T22:57:58.2015825Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-12-17T22:57:58.2016341Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-12-17T22:57:58.2016825Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-12-17T22:57:58.2017268Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-12-17T22:57:58.2017776Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-12-17T22:57:58.2018358Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-12-17T22:57:58.2018857Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-12-17T22:57:58.2019340Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-12-17T22:57:58.2019808Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-12-17T22:57:58.2020275Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-12-17T22:57:58.2020729Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-12-17T22:57:58.2021350Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-12-17T22:57:58.2021755Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-12-17T22:57:58.2022149Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-12-17T22:57:58.2022598Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-12-17T22:57:58.2023100Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-12-17T22:57:58.2023540Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-12-17T22:57:58.2024038Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-12-17T22:57:58.2024514Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-12-17T22:57:58.2024944Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-12-17T22:57:58.2025360Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-12-17T22:57:58.2025815Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-12-17T22:57:58.2026293Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-12-17T22:57:58.2026789Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-12-17T22:57:58.2027249Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-12-17T22:57:58.2027665Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-12-17T22:57:58.2028046Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-12-17T22:57:58.2028499Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-12-17T22:57:58.2029013Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-12-17T22:57:58.2029532Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-12-17T22:57:58.2030050Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-12-17T22:57:58.2030589Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-12-17T22:57:58.2031381Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-12-17T22:57:58.2031904Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-12-17T22:57:58.2032377Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-12-17T22:57:58.2032893Z Caused by: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
2020-12-17T22:57:58.2033563Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2020-12-17T22:57:58.2034291Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2020-12-17T22:57:58.2034964Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2020-12-17T22:57:58.2035575Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2020-12-17T22:57:58.2036257Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2020-12-17T22:57:58.2037129Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2020-12-17T22:57:58.2037801Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2020-12-17T22:57:58.2038286Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2020-12-17T22:57:58.2038667Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2020-12-17T22:57:58.2039072Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2020-12-17T22:57:58.2039456Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-12-17T22:57:58.2039812Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2020-12-17T22:57:58.2040253Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2020-12-17T22:57:58.2040724Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2020-12-17T22:57:58.2041132Z 
2020-12-17T22:57:58.2041435Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2020-12-17T22:57:58.2041824Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2020-12-17T22:57:58.2042282Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2020-12-17T22:57:58.2042721Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2020-12-17T22:57:58.2043216Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2020-12-17T22:57:58.2043651Z 	at com.sun.proxy.$Proxy110.getApplications(Unknown Source)
2020-12-17T22:57:58.2044195Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:288)
2020-12-17T22:57:58.2044672Z 	... 50 more
{code}"	FLINK	Closed	3	1	6847	test-stability
13544960	Renaming AkkaOptions into RpcOptions	"FLINK-32468 introduced Apache Pekko as an replacement for Akka. This involved renaming classes (besides updating comments). {{AkkaOptions}} was the only occurrence that wasn't renamed as it's annotated as {{@PublicEvolving}}.

This issue is about renaming {{AkkaOptions}} into {{PekkoOptions}} (or a more general term considering FLINK-29281)"	FLINK	Resolved	3	11500	6847	2.0-related, pull-request-available
13322761	Added Copyright information to coding style guide	"Add Copyright as a requirement to [https://flink.apache.org/contributing/code-style-and-quality-common.html]

Add Copyright profile instructions to ide_setup.md (including the Chinese version)."	FLINK	Closed	4	4	6847	pull-request-available
13557347	Rely on Maven wrapper instead of having custom Maven installation logic	I noticed that we could use the Maven wrapper instead of having a custom setup logic for Maven in CI.	FLINK	Resolved	3	4	6847	pull-request-available
13484876	Update Pulsar dependency to 2.10.1	Update the Pulsar dependency to 2.10.1 to benefit of the fixes highlights at https://github.com/apache/pulsar/releases/tag/v2.10.1	FLINK	Closed	3	11500	6913	pull-request-available
13579513	[FLIP-453] Promote Unified Sink API V2 to Public and Deprecate SinkFunction	https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=303794871&draftShareId=af4ace88-98b7-4a53-aece-cd67d2f91a15&	FLINK	Closed	3	11500	6913	pull-request-available
13338418	Upgrade Kinesis dependencies to avoid protobuf 2.6.1	"Our current Kinesis dependencies (amazon-kinesis-client, amazon-kinesis-producer) depend on protobuf 2.6.1, which are affected by [CVE-2015-5237|https://nvd.nist.gov/vuln/detail/CVE-2015-5237].

We should look into upgrade the client to 1.14.0, and the producer to 0.14.1 ."	FLINK	Closed	4	4	6913	auto-deprioritized-major, pull-request-available
13470280	E2E tests run out of disk space on Azure	All builds on the {{release-1.14}} branch fail on the E2E step due to running out of disk space. Running out of disk space causes FLINK-28305	FLINK	Closed	1	1	6913	stale-assigned, test-stability
13484663	Bump Minikdc to v3.2.4	Bump Minikdc to v3.2.4 to remove false positive scans on CVEs like CVE-2021-29425 and CVE-2020-15250	FLINK	Closed	3	11500	6913	pull-request-available
13427758	Mark NiFi connector as deprecated	The Flink community has voted to deprecate the NiFi connector in Flink 1.15 and remove it in the version after that one. Details can be found in https://lists.apache.org/thread/gldw588pbdf8hww9jtgfdv5y60v5mt6w	FLINK	Closed	3	11500	6913	pull-request-available
13424395	Update com.h2database:h2 to 2.0.210	"Two security vulnerabilities in H2 Console (CVE-2022-23221 and possible DNS rebinding attack) are fixed in 2.0.120. Flink is currently on 2.0.206 since https://issues.apache.org/jira/browse/FLINK-25576

Note: Flink is using this dependency only for testing, so it's not directly impacted by the CVE. We just want to be good citizens and update our dependencies"	FLINK	Resolved	3	11500	6913	pull-request-available
13531223	Cassandra nightly CI failure	"Investigate and fix the nightly CI failure. Example [https://github.com/apache/flink-connector-cassandra/actions/runs/4585936901]

 
{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-cassandra_2.12: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'junit-jupiter' failed to discover tests: org/junit/jupiter/api/io/CleanupMode: org.junit.jupiter.api.io.CleanupMode -> [Help 1]{code}
 

 "	FLINK	Closed	3	11500	6913	pull-request-available
13505535	Update Parquet version to 1.12.3	Flink currently uses version 1.12.2, which should be upgraded to 1.12.3	FLINK	Closed	3	11500	6913	pull-request-available
13505546	Bump engine.io from 6.2.0 to 6.2.1	"Bump engine.io from 6.2.0 to 6.2.1 to avoid false flag for CVE-2022-41940
"	FLINK	Closed	3	11500	6913	pull-request-available
13468774	Kubernetes test failed with permission denied	"
{code:java}
2022-06-28T00:57:41.2724102Z Jun 28 00:57:41 Preparing to unpack .../conntrack_1%3a1.4.5-2_amd64.deb ...
2022-06-28T00:57:41.2747620Z Jun 28 00:57:41 Unpacking conntrack (1:1.4.5-2) ...
2022-06-28T00:57:41.3287046Z Jun 28 00:57:41 Setting up conntrack (1:1.4.5-2) ...
2022-06-28T00:57:41.3316320Z Jun 28 00:57:41 Processing triggers for man-db (2.9.1-1) ...
2022-06-28T00:57:43.0246060Z Jun 28 00:57:43 fs.protected_regular = 0
2022-06-28T00:57:46.5039126Z Jun 28 00:57:46 * Profile ""minikube"" not found. Run ""minikube profile list"" to view all profiles.
2022-06-28T00:57:46.5047104Z Jun 28 00:57:46   To start a cluster, run: ""minikube start""
2022-06-28T00:57:46.5088285Z Jun 28 00:57:46 Starting minikube ...
2022-06-28T00:57:46.6801624Z Jun 28 00:57:46 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:57:46.7004754Z Jun 28 00:57:46 * Using the none driver based on user configuration
2022-06-28T00:57:46.7128484Z Jun 28 00:57:46 * Starting control plane node minikube in cluster minikube
2022-06-28T00:57:46.7178545Z Jun 28 00:57:46 * Running on localhost (CPUs=2, Memory=6946MB, Disk=85173MB) ...
2022-06-28T00:57:47.0620600Z Jun 28 00:57:47 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:57:49.3027006Z Jun 28 00:57:49 
2022-06-28T00:57:49.3040058Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:57:49.3040497Z stdout:
2022-06-28T00:57:49.3040596Z 
2022-06-28T00:57:49.3040809Z stderr:
2022-06-28T00:57:49.3041270Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:57:49.3041461Z 
2022-06-28T00:57:49.3041663Z * 
2022-06-28T00:57:49.3063195Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:57:49.3063744Z │                                                                                             │
2022-06-28T00:57:49.3064375Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:57:49.3064975Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:57:49.3065484Z │                                                                                             │
2022-06-28T00:57:49.3066019Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:57:49.3066543Z │                                                                                             │
2022-06-28T00:57:49.3067048Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:57:49.3067360Z Jun 28 00:57:49 
2022-06-28T00:57:49.3922883Z E0628 00:57:49.392012  194614 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:57:49.3948489Z Jun 28 00:57:49 
2022-06-28T00:57:49.3957501Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:49.3960093Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:49.3960924Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:49.3966499Z Jun 28 00:57:49 
2022-06-28T00:57:49.4693197Z Jun 28 00:57:49 
2022-06-28T00:57:49.4700763Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:49.4702406Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:49.4703168Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:49.4731042Z Jun 28 00:57:49 
2022-06-28T00:57:49.4733005Z Jun 28 00:57:49 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:57:54.5441686Z Jun 28 00:57:54 
2022-06-28T00:57:54.5443916Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:54.5445961Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:54.5446526Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:54.5450376Z Jun 28 00:57:54 
2022-06-28T00:57:54.5469377Z Jun 28 00:57:54 Starting minikube ...
2022-06-28T00:57:54.6254675Z Jun 28 00:57:54 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:57:54.6299271Z Jun 28 00:57:54 * Using the none driver based on existing profile
2022-06-28T00:57:54.6324036Z Jun 28 00:57:54 * Starting control plane node minikube in cluster minikube
2022-06-28T00:57:54.6867587Z Jun 28 00:57:54 * Restarting existing none bare metal machine for ""minikube"" ...
2022-06-28T00:57:54.6954113Z Jun 28 00:57:54 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:57:55.7715259Z Jun 28 00:57:55 
2022-06-28T00:57:55.7730526Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:57:55.7731022Z stdout:
2022-06-28T00:57:55.7731134Z 
2022-06-28T00:57:55.7731336Z stderr:
2022-06-28T00:57:55.7731822Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:57:55.7732019Z 
2022-06-28T00:57:55.7732211Z * 
2022-06-28T00:57:55.7732693Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:57:55.7733157Z │                                                                                             │
2022-06-28T00:57:55.7733672Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:57:55.7734360Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:57:55.7734861Z │                                                                                             │
2022-06-28T00:57:55.7735418Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:57:55.7735959Z │                                                                                             │
2022-06-28T00:57:55.7736589Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:57:55.7740218Z Jun 28 00:57:55 
2022-06-28T00:57:55.8458528Z E0628 00:57:55.845551  194918 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:57:55.8471612Z Jun 28 00:57:55 
2022-06-28T00:57:55.8477089Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:55.8479071Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:55.8480223Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:55.8482785Z Jun 28 00:57:55 
2022-06-28T00:57:55.9175143Z Jun 28 00:57:55 
2022-06-28T00:57:55.9189731Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:55.9192377Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:55.9194955Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:55.9207163Z Jun 28 00:57:55 
2022-06-28T00:57:55.9233298Z Jun 28 00:57:55 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:58:00.9957446Z Jun 28 00:58:00 
2022-06-28T00:58:00.9963674Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:00.9964966Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:00.9965541Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:00.9973430Z Jun 28 00:58:00 
2022-06-28T00:58:01.0002581Z Jun 28 00:58:00 Starting minikube ...
2022-06-28T00:58:01.0729479Z Jun 28 00:58:01 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:58:01.0790218Z Jun 28 00:58:01 * Using the none driver based on existing profile
2022-06-28T00:58:01.0808505Z Jun 28 00:58:01 * Starting control plane node minikube in cluster minikube
2022-06-28T00:58:01.1067402Z Jun 28 00:58:01 * Restarting existing none bare metal machine for ""minikube"" ...
2022-06-28T00:58:01.1177185Z Jun 28 00:58:01 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:58:02.2118290Z Jun 28 00:58:02 
2022-06-28T00:58:02.2134209Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:58:02.2136970Z stdout:
2022-06-28T00:58:02.2137355Z 
2022-06-28T00:58:02.2137725Z stderr:
2022-06-28T00:58:02.2138686Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:58:02.2139016Z 
2022-06-28T00:58:02.2139342Z * 
2022-06-28T00:58:02.2139953Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:58:02.2140560Z │                                                                                             │
2022-06-28T00:58:02.2141218Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:58:02.2141916Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:58:02.2142571Z │                                                                                             │
2022-06-28T00:58:02.2143272Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:58:02.2143943Z │                                                                                             │
2022-06-28T00:58:02.2144601Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:58:02.2145064Z Jun 28 00:58:02 
2022-06-28T00:58:02.2880448Z E0628 00:58:02.287766  195222 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:02.2892504Z Jun 28 00:58:02 
2022-06-28T00:58:02.2896830Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:02.2899275Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:02.2900544Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:02.2901362Z Jun 28 00:58:02 
2022-06-28T00:58:02.3582770Z Jun 28 00:58:02 
2022-06-28T00:58:02.3587835Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:02.3589055Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:02.3589630Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:02.3599452Z Jun 28 00:58:02 
2022-06-28T00:58:02.3621002Z Jun 28 00:58:02 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:58:07.3638880Z Jun 28 00:58:07 Command: start_kubernetes_if_not_running failed 3 times.
2022-06-28T00:58:07.3639385Z Jun 28 00:58:07 Could not start minikube. Aborting...
2022-06-28T00:58:07.3639759Z Jun 28 00:58:07 Debugging failed Kubernetes test:
2022-06-28T00:58:07.3640148Z Jun 28 00:58:07 Currently existing Kubernetes resources
2022-06-28T00:58:07.5490786Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.5922448Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.5945237Z Jun 28 00:58:07 Flink logs:
2022-06-28T00:58:07.6330601Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.6707241Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7097992Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7506609Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7886315Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7896794Z Jun 28 00:58:07 Stopping minikube ...
2022-06-28T00:58:07.8560167Z E0628 00:58:07.855722  195270 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:07.8564021Z E0628 00:58:07.856252  195270 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:07.8583780Z Jun 28 00:58:07 
2022-06-28T00:58:07.8595235Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:07.8596811Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:07.8597431Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:07.8605881Z Jun 28 00:58:07 
2022-06-28T00:58:07.8632037Z Jun 28 00:58:07 Command: minikube stop failed. Retrying...
2022-06-28T00:58:12.9320992Z E0628 00:58:12.931814  195281 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:12.9323097Z E0628 00:58:12.932141  195281 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:12.9346464Z Jun 28 00:58:12 
2022-06-28T00:58:12.9356598Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:12.9359608Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:12.9361856Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:12.9370436Z Jun 28 00:58:12 
2022-06-28T00:58:12.9401892Z Jun 28 00:58:12 Command: minikube stop failed. Retrying...
2022-06-28T00:58:18.0093241Z E0628 00:58:18.009042  195290 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:18.0094620Z E0628 00:58:18.009278  195290 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:18.0105801Z Jun 28 00:58:18 
2022-06-28T00:58:18.0109189Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:18.0112493Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:18.0114979Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:18.0119419Z Jun 28 00:58:18 
2022-06-28T00:58:18.0139916Z Jun 28 00:58:18 Command: minikube stop failed. Retrying...
2022-06-28T00:58:23.0155447Z Jun 28 00:58:23 Command: minikube stop failed 3 times.
2022-06-28T00:58:23.0156414Z Jun 28 00:58:23 Could not stop minikube. Aborting...
2022-06-28T00:58:23.0159072Z Jun 28 00:58:23 [FAIL] Test script contains errors.
2022-06-28T00:58:23.0166657Z Jun 28 00:58:23 Checking for errors...
2022-06-28T00:58:23.0402412Z Jun 28 00:58:23 No errors in log files.
2022-06-28T00:58:23.0404271Z Jun 28 00:58:23 Checking for exceptions...
2022-06-28T00:58:23.0675070Z Jun 28 00:58:23 No exceptions in log files.
2022-06-28T00:58:23.0677372Z Jun 28 00:58:23 Checking for non-empty .out files...
2022-06-28T00:58:23.0700681Z grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
2022-06-28T00:58:23.0706900Z Jun 28 00:58:23 No non-empty .out files.
2022-06-28T00:58:23.0707516Z Jun 28 00:58:23 
2022-06-28T00:58:23.0708309Z Jun 28 00:58:23 [FAIL] 'Run Kubernetes test' failed after 0 minutes and 44 seconds! Test exited with exit code 1
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4975"	FLINK	Closed	1	1	6913	pull-request-available, test-stability
13410649	Bump Powermock to v2.0.9	We should update Powermock to the latest available version, which is currently v2.0.9	FLINK	Resolved	4	11500	6913	pull-request-available
13410634	Update nifi-site-to-site-client to v1.14.0	We should update org.apache.nifi:nifi-site-to-site-client from 1.6.0 to 1.14.0	FLINK	Resolved	4	11500	6913	pull-request-available
13484857	Bump protoc and protobuf-java dependencies to 3.21.7	Bump protoc and protobuf-java dependencies to at least 3.21.7 to avoid false positive scans on Protobuf vulnerabilities	FLINK	Closed	3	11500	6913	pull-request-available
13480259	Documented SQL Client setup when using Docker Compose doesn't work	The SQL Client Docker Compose setup at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/ is missing required parameter {{rest.address}}	FLINK	Closed	3	1	6913	pull-request-available
13442323	Module flink-sql-avro-confluent-registry does not configure Confluent repo	"flink-sql-avro-confluent-registry depends on org.apache.kafka:kafka-clients, which is not available in Maven Central, but only in the Confluent repo. However, it does not configure this repo. This causes the build to fail for me locally with the following exception:

{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (process-resource-bundles) on project flink-sql-avro-confluent-registry: Error resolving project artifact: Could not transfer artifact org.apache.kafka:kafka-clients:pom:6.2.2-ccs from/to <other repo>: Not authorized , ReasonPhrase: . for project org.apache.kafka:kafka-clients:jar:6.2.2-ccs -> [Help 1]
{code}

This may be build order dependent, but the module should probably configure the repo to be safe, like done elsewhere: https://github.com/apache/flink/blob/dd48d058c6b745f505870836048284a76a23f7cc/flink-end-to-end-tests/flink-confluent-schema-registry/pom.xml#L36-L41

Looks like this is the case since 1.12 at least.
"	FLINK	Closed	3	1	6913	pull-request-available
13566845	Update CI Node Actions from NodeJS 16 to NodeJS 20	"{code:java}
Node.js 16 actions are deprecated. Please update the following actions to use Node.js 20: actions/checkout@v3, actions/setup-java@v3, stCarolas/setup-maven@v4.5, actions/cache/restore@v3, actions/cache/save@v3. 
{code}

For more information see: https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/."	FLINK	Closed	3	11500	6913	pull-request-available
13528760	Connecting to Kafka and Avro Schema Registry fails with ClassNotFoundException	"When running the SQL Client and using flink-sql-connector-kafka, flink-sql-avro and flink-sql-avro-confluent-registry and trying to query Schema Registry, the job will fail with

{code:bash}
[ERROR] Could not execute SQL statement. Reason:
java.lang.ClassNotFoundException: com.google.common.base.Ticker
{code}"	FLINK	Closed	1	1	6913	pull-request-available
13446203	Remove Hugo Modules integration	"Flink provides 2 ways for building the documentation: 1) using a Hugo docker image, and 2) using a local Hugo installation.

Currently, 1) is broken due to the `setup_docs.sh` script requires a local Hugo installation.

This was introduced in FLINK-27394."	FLINK	Resolved	2	11500	6913	pull-request-available, stale-assigned
13395002	Testing Window Join	"The window join requires the join on condition contains window starts equality of input tables and window ends equality of input tables. The semantic of window join is the same to the [DataStream window join|https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/joining.html#window-join].
{code:java}
SELECT ...
FROM L [LEFT|RIGHT|FULL OUTER] JOIN R -- L and R are relations applied windowing TVF
ON L.window_start = R.window_start AND L.window_end = R.window_end AND ...
{code}
In the future, we can also simplify the join on clause to only include the window start equality if the windowing TVF is {{TUMBLE}} or {{HOP}} . Currently, the windowing TVFs must be the same of left and right inputs. This can be extended in the future, for example, tumbling windows join sliding windows with the same window size.



Currently, Flink not only supports Window Join which follows after [Window Aggregation|https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/].  But also supports Window Join which follows after [Windowing TVF|https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/window-tvf/] ."	FLINK	Closed	1	4	6913	release-testing
13566331	Upgrade Confluent Platform to latest compatible version	"Flink uses Confluent Platform for its Confluent Avro Schema Registry implementation, and we can update that to the latest version.

It's also used by the Flink Kafka connector, and we should upgrade it to the latest compatible version of the used Kafka Client (in this case, 7.4.x)"	FLINK	Closed	3	11500	6913	pull-request-available
13417290	Azure failed due to unable to fetch some archives	"{code:java}
/bin/bash --noprofile --norc /__w/_temp/ba0f8961-8595-4ace-b13f-d60e17df8803.sh
Reading package lists...
Building dependency tree...
Reading state information...
The following additional packages will be installed:
  libio-pty-perl libipc-run-perl
Suggested packages:
  libtime-duration-perl libtimedate-perl
The following NEW packages will be installed:
  libio-pty-perl libipc-run-perl moreutils
0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.
Need to get 177 kB of archives.
After this operation, 573 kB of additional disk space will be used.
Err:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 libio-pty-perl amd64 1:1.08-1.1build1
  Could not connect to archive.ubuntu.com:80 (91.189.88.152), connection timed out [IP: 91.189.88.152 80]
Err:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 libipc-run-perl all 0.94-1
  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]
Err:3 http://archive.ubuntu.com/ubuntu xenial/universe amd64 moreutils amd64 0.57-1
  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]
E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libio-pty-perl/libio-pty-perl_1.08-1.1build1_amd64.deb  Could not connect to archive.ubuntu.com:80 (91.189.88.152), connection timed out [IP: 91.189.88.152 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libipc-run-perl/libipc-run-perl_0.94-1_all.deb  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/universe/m/moreutils/moreutils_0.57-1_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]

E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
Running command './tools/ci/test_controller.sh kafka/gelly' with a timeout of 234 minutes.
./tools/azure-pipelines/uploading_watchdog.sh: line 76: ts: command not found
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '141'.
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28064&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=13"	FLINK	Closed	1	1	6913	test-stability
13361553	'SQL Client end-to-end test (Blink planner) Elasticsearch (v6.3.1)' fails during download	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13906&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873

{code}
Downloading Elasticsearch from https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.1.tar.gz ...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0 87.1M    0   997    0     0   3298      0  7:42:02 --:--:--  7:42:02  3290
 17 87.1M   17 15.6M    0     0  11.6M      0  0:00:07  0:00:01  0:00:06 11.6M
 31 87.1M   31 27.8M    0     0  12.1M      0  0:00:07  0:00:02  0:00:05 12.1M
 49 87.1M   49 42.9M    0     0  12.9M      0  0:00:06  0:00:03  0:00:03 12.9M
 67 87.1M   67 58.9M    0     0  13.5M      0  0:00:06  0:00:04  0:00:02 13.5M
 87 87.1M   87 75.8M    0     0  14.3M      0  0:00:06  0:00:05  0:00:01 15.1M
 87 87.1M   87 75.9M    0     0  14.2M      0  0:00:06  0:00:05  0:00:01 15.1M
curl: (56) GnuTLS recv error (-110): The TLS connection was non-properly terminated.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 9200: Connection refused
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.
{code}"	FLINK	Resolved	3	1	6913	auto-deprioritized-major, test-stability
13572484	Switching to the Apache CDN for Dockerfile	"During publishing the official image, we received some comments

for Switching to the Apache CDN

 

See

https://github.com/docker-library/official-images/pull/16114

https://github.com/docker-library/official-images/pull/16430

 

Reason for switching: [https://apache.org/history/mirror-history.html] (also [https://www.apache.org/dyn/closer.cgi] and [https://www.apache.org/mirrors])"	FLINK	Closed	3	4	6913	pull-request-available
13468574	TPC-DS Bash e2e tests don't clean-up after completing	"When debugging the disk space usage for the e2e tests, the top 20 folders with the largest file size are:

{code:java}
2022-06-27T09:32:59.8000587Z Jun 27 09:32:59 List top 20 directories with largest file size
2022-06-27T09:33:00.9811803Z Jun 27 09:33:00 4088524	.
2022-06-27T09:33:00.9813428Z Jun 27 09:33:00 1277080	./flink-end-to-end-tests
2022-06-27T09:33:00.9814324Z Jun 27 09:33:00 624512	./flink-dist
2022-06-27T09:33:00.9815152Z Jun 27 09:33:00 624124	./flink-dist/target
2022-06-27T09:33:00.9816093Z Jun 27 09:33:00 500032	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-06-27T09:33:00.9817429Z Jun 27 09:33:00 500028	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-06-27T09:33:00.9818167Z Jun 27 09:33:00 486412	./.git
2022-06-27T09:33:00.9819096Z Jun 27 09:33:00 479416	./.git/objects
2022-06-27T09:33:00.9819512Z Jun 27 09:33:00 479408	./.git/objects/pack
2022-06-27T09:33:00.9820584Z Jun 27 09:33:00 461456	./flink-connectors
2022-06-27T09:33:00.9821403Z Jun 27 09:33:00 449832	./.git/objects/pack/pack-0bdd9e3186d0cb404910c5843d19b5cb80b84fe0.pack
2022-06-27T09:33:00.9821992Z Jun 27 09:33:00 349236	./flink-table
2022-06-27T09:33:00.9822631Z Jun 27 09:33:00 293008	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/opt
2022-06-27T09:33:00.9823233Z Jun 27 09:33:00 251272	./flink-filesystems
2022-06-27T09:33:00.9823818Z Jun 27 09:33:00 246588	./flink-end-to-end-tests/flink-streaming-kinesis-test
2022-06-27T09:33:00.9824502Z Jun 27 09:33:00 246464	./flink-end-to-end-tests/flink-streaming-kinesis-test/target
2022-06-27T09:33:00.9825210Z Jun 27 09:33:00 196656	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/lib
2022-06-27T09:33:00.9825966Z Jun 27 09:33:00 184364	./flink-end-to-end-tests/flink-streaming-kinesis-test/target/KinesisExample.jar
2022-06-27T09:33:00.9826652Z Jun 27 09:33:00 156136	./flink-end-to-end-tests/flink-tpcds-test
2022-06-27T09:33:00.9827284Z Jun 27 09:33:00 151180	./flink-end-to-end-tests/flink-tpcds-test/target
{code}

See https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2732&view=logs&j=0e31ee24-31a6-528c-a4bf-45cde9b2a14e&t=ff03a8fa-e84e-5199-efb2-5433077ce8e2&l=5093

After running {{TPC-DS end-to-end test}} and after the clean-up, the following directories are listed in the top 20:

{code:java}
2022-06-27T09:49:51.7694429Z Jun 27 09:49:51 List top 20 directories with largest file size AFTER cleaning temorary folders and files
2022-06-27T09:49:52.9617221Z Jun 27 09:49:52 5315996	.
2022-06-27T09:49:52.9618830Z Jun 27 09:49:52 2504556	./flink-end-to-end-tests
2022-06-27T09:49:52.9619848Z Jun 27 09:49:52 1383612	./flink-end-to-end-tests/flink-tpcds-test
2022-06-27T09:49:52.9620796Z Jun 27 09:49:52 1378656	./flink-end-to-end-tests/flink-tpcds-test/target
2022-06-27T09:49:52.9621730Z Jun 27 09:49:52 1223944	./flink-end-to-end-tests/flink-tpcds-test/target/table
2022-06-27T09:49:52.9622844Z Jun 27 09:49:52 624508	./flink-dist
2022-06-27T09:49:52.9623585Z Jun 27 09:49:52 624120	./flink-dist/target
2022-06-27T09:49:52.9624398Z Jun 27 09:49:52 500028	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-06-27T09:49:52.9625366Z Jun 27 09:49:52 500024	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-06-27T09:49:52.9625994Z Jun 27 09:49:52 486412	./.git
2022-06-27T09:49:52.9626514Z Jun 27 09:49:52 479416	./.git/objects
2022-06-27T09:49:52.9631740Z Jun 27 09:49:52 479408	./.git/objects/pack
2022-06-27T09:49:52.9632755Z Jun 27 09:49:52 461456	./flink-connectors
2022-06-27T09:49:52.9633717Z Jun 27 09:49:52 449832	./.git/objects/pack/pack-0bdd9e3186d0cb404910c5843d19b5cb80b84fe0.pack
2022-06-27T09:49:52.9634769Z Jun 27 09:49:52 379348	./flink-end-to-end-tests/flink-tpcds-test/target/table/store_sales.dat
2022-06-27T09:49:52.9635596Z Jun 27 09:49:52 349236	./flink-table
2022-06-27T09:49:52.9636489Z Jun 27 09:49:52 293008	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/opt
2022-06-27T09:49:52.9637526Z Jun 27 09:49:52 288980	./flink-end-to-end-tests/flink-tpcds-test/target/table/catalog_sales.dat
2022-06-27T09:49:52.9638378Z Jun 27 09:49:52 251272	./flink-filesystems
2022-06-27T09:49:52.9639238Z Jun 27 09:49:52 246588	./flink-end-to-end-tests/flink-streaming-kinesis-test
{code}

See https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2732&view=logs&j=0e31ee24-31a6-528c-a4bf-45cde9b2a14e&t=ff03a8fa-e84e-5199-efb2-5433077ce8e2&l=5708

This results in not enough disk space errors during various runs further downstream. This test should also properly clean-up its files"	FLINK	Closed	2	11500	6913	pull-request-available, test-stability
13439021	Deprecate StreamingFileSink	"The StreamingFileSink has been deprecated in favor of the unified FileSink since Flink 1.12.

 

This changed is reflected in the docs, but not yet in the codebase.

 

https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API

https://issues.apache.org/jira/browse/FLINK-19510

https://issues.apache.org/jira/browse/FLINK-20337

 

 "	FLINK	Closed	3	4	6913	pull-request-available
13382695	Migrate flink project website to Hugo	Hugo is working like a charm for the Flink documentation. To reduce the number of software stacks, and massively reduce friction when building the current Flink website, we should migrate the Flink website to hugo as well.	FLINK	Closed	3	4	6913	pull-request-available
13414934	Streaming File Sink s3 end-to-end test failed due to job has not started within a timeout of 10 sec	"{code:java}
Dec 01 19:06:38 Starting taskexecutor daemon on host fv-az26-327.
Dec 01 19:06:38 Submitting job.
Dec 01 19:06:54 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:06:57 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:00 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:03 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:06 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:09 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:12 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:15 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:18 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:21 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:22 Job (62f9a00856309492574699642574071c) has not started within a timeout of 10 sec
Dec 01 19:07:22 Stopping job timeout watchdog (with pid=401626)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27382&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=12560"	FLINK	Resolved	3	1	6913	test-stability
13506892	shading of netty epoll shared library does not account for ARM64 platform	"While evaluating migration of Flink application to Graviton 2 based EC2 instances in a AWS managed Kubernetes service (EKS) using Kubernetes 1.23, found that the shaded Netty library renames the AMD64 version of the shared library as part of relocation of the Netty library but does not rename the matching ARM64 shared library. This results in the following error when `taskmanager.network.netty.transport: epoll` is used:



 

 

{{Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_transport_native_epoll in java.library.path}}
{{at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860) ~[?:1.8.0_352]}}
{{at java.lang.Runtime.loadLibrary0(Runtime.java:843) ~[?:1.8.0_352]}}
{{at java.lang.System.loadLibrary(System.java:1136) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]}}
{{at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]}}
{{at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:335) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:327) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:293) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:309) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.<clinit>(Native.java:85) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:40) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.<clinit>(EpollEventLoop.java:51) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:185) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:36) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:60) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:49) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:113) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:100) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:77) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyClient.initEpollBootstrap(NettyClient.java:164) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyClient.init(NettyClient.java:79) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.start(NettyConnectionManager.java:87) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.start(NettyShuffleEnvironment.java:329) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerServices.fromConfiguration(TaskManagerServices.java:293) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:623) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.createTaskExecutorService(TaskManagerRunner.java:559) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManagerRunnerServices(TaskManagerRunner.java:245) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.start(TaskManagerRunner.java:288) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:481) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.lambda$runTaskManagerProcessSecurely$5(TaskManagerRunner.java:525) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:525) [flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:505) [flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.kubernetes.taskmanager.KubernetesTaskExecutorRunner.main(KubernetesTaskExecutorRunner.java:39) [flink-dist-1.15.2.jar:1.15.2]}}
{{Caused by: java.io.FileNotFoundException: META-INF/native/liborg_apache_flink_shaded_netty4_netty_transport_native_epoll_aarch_64.so}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:170) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:306) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.<clinit>(Native.java:85) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:40) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{... 25 more}}

 

[https://github.com/apache/flink-shaded/blob/3082afc952e68366e9fefe4d1181c4666969ee67/flink-shaded-netty-4/pom.xml#L97] appears to be where the problem is, it only renames the x86_64 shared library, it doesn’t account for aarch_64 shared library."	FLINK	Closed	3	1	6913	pull-request-available
13424853	Add explanation how Kafka Source deals with idleness when parallelism is higher then the number of partitions	Add a section to the Kafka Source documentation to explain what happens with the Kafka Source with regards to idleness when parallelism is higher then the number of partitions	FLINK	Resolved	3	4	6913	pull-request-available
13326295	Revisit java e2e download timeouts	"Consider this failed test case

{code}
Test testHBase(org.apache.flink.tests.util.hbase.SQLClientHBaseITCase) is running.
--------------------------------------------------------------------------------
09:38:38,719 [                main] INFO  org.apache.flink.tests.util.cache.PersistingDownloadCache    [] - Downloading https://archive.apache.org/dist/hbase/1.4.3/hbase-1.4.3-bin.tar.gz.
09:40:38,732 [                main] ERROR org.apache.flink.tests.util.hbase.SQLClientHBaseITCase       [] - 
--------------------------------------------------------------------------------
Test testHBase(org.apache.flink.tests.util.hbase.SQLClientHBaseITCase) failed with:
java.io.IOException: Process ([wget, -q, -P, /home/vsts/work/1/e2e_cache/downloads/1598516010, https://archive.apache.org/dist/hbase/1.4.3/hbase-1.4.3-bin.tar.gz]) exceeded timeout (120000) or number of retries (3).
	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:148)
	at org.apache.flink.tests.util.cache.AbstractDownloadCache.getOrDownload(AbstractDownloadCache.java:127)
	at org.apache.flink.tests.util.cache.PersistingDownloadCache.getOrDownload(PersistingDownloadCache.java:36)
	at org.apache.flink.tests.util.hbase.LocalStandaloneHBaseResource.setupHBaseDist(LocalStandaloneHBaseResource.java:76)
	at org.apache.flink.tests.util.hbase.LocalStandaloneHBaseResource.before(LocalStandaloneHBaseResource.java:70)
	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:46)
{code}

It seems that the download has not been retried. The download might be stuck? I would propose to set a timeout per try and increase the total time from 2 to 5 minutes.


This example is from: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6267&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529"	FLINK	Resolved	4	7	6913	auto-deprioritized-major, pull-request-available, stale-minor, test-stability
13468770	Elasticsearch6SinkE2ECase failed with no space left on device	"
{code:java}
2022-06-27T14:35:05.6850998Z Jun 27 14:35:05 [ERROR] org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase  Time elapsed: 54.613 s  <<< ERROR!
2022-06-27T14:35:05.6851516Z Jun 27 14:35:05 java.lang.RuntimeException: Failed to build JobManager image
2022-06-27T14:35:05.6852121Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.buildJobManagerContainer(FlinkContainersBuilder.java:215)
2022-06-27T14:35:05.6852837Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.build(FlinkContainersBuilder.java:181)
2022-06-27T14:35:05.6853675Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:83)
2022-06-27T14:35:05.6854487Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:62)
2022-06-27T14:35:05.6855176Z Jun 27 14:35:05 	at org.apache.flink.streaming.tests.ElasticsearchSinkE2ECaseBase.<init>(ElasticsearchSinkE2ECaseBase.java:58)
2022-06-27T14:35:05.6855857Z Jun 27 14:35:05 	at org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.<init>(Elasticsearch6SinkE2ECase.java:36)
2022-06-27T14:35:05.6856431Z Jun 27 14:35:05 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-06-27T14:35:05.6856998Z Jun 27 14:35:05 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-06-27T14:35:05.6857653Z Jun 27 14:35:05 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-06-27T14:35:05.6858250Z Jun 27 14:35:05 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2022-06-27T14:35:05.6858817Z Jun 27 14:35:05 	at org.junit.platform.commons.util.ReflectionUtils.newInstance(ReflectionUtils.java:550)
2022-06-27T14:35:05.6859429Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ConstructorInvocation.proceed(ConstructorInvocation.java:56)
2022-06-27T14:35:05.6860144Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-27T14:35:05.6860896Z Jun 27 14:35:05 	at org.junit.jupiter.api.extension.InvocationInterceptor.interceptTestClassConstructor(InvocationInterceptor.java:73)
2022-06-27T14:35:05.6861586Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-27T14:35:05.6862300Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-27T14:35:05.6863038Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-27T14:35:05.6863743Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-27T14:35:05.6864429Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-27T14:35:05.6865061Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-27T14:35:05.6865667Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:77)
2022-06-27T14:35:05.6866345Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestClassConstructor(ClassBasedTestDescriptor.java:355)
2022-06-27T14:35:05.6867072Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateTestClass(ClassBasedTestDescriptor.java:302)
2022-06-27T14:35:05.6867776Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassTestDescriptor.instantiateTestClass(ClassTestDescriptor.java:79)
2022-06-27T14:35:05.6868508Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:280)
2022-06-27T14:35:05.6869344Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:272)
2022-06-27T14:35:05.6869953Z Jun 27 14:35:05 	at java.util.Optional.orElseGet(Optional.java:267)
2022-06-27T14:35:05.6870551Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:271)
2022-06-27T14:35:05.6871271Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
2022-06-27T14:35:05.6871960Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$before$2(ClassBasedTestDescriptor.java:197)
2022-06-27T14:35:05.6872701Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6873374Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:196)
2022-06-27T14:35:05.6874033Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)
2022-06-27T14:35:05.6874720Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
2022-06-27T14:35:05.6875392Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6876068Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-27T14:35:05.6876708Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-27T14:35:05.6877331Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-27T14:35:05.6878016Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6878803Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-27T14:35:05.6879443Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-27T14:35:05.6880209Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-27T14:35:05.6881108Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-27T14:35:05.6881906Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-27T14:35:05.6882578Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6883261Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-27T14:35:05.6883890Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-27T14:35:05.6884511Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-27T14:35:05.6885186Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6885841Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-27T14:35:05.6886477Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-27T14:35:05.6887415Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-27T14:35:05.6888135Z Jun 27 14:35:05 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-27T14:35:05.6888663Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-27T14:35:05.6889201Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-27T14:35:05.6889747Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-27T14:35:05.6890288Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-27T14:35:05.6891296Z Jun 27 14:35:05 Caused by: org.apache.flink.tests.util.flink.container.ImageBuildException: Failed to build image ""flink-dist-configured-jobmanager""
2022-06-27T14:35:05.6891969Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkImageBuilder.build(FlinkImageBuilder.java:204)
2022-06-27T14:35:05.6892675Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.buildJobManagerContainer(FlinkContainersBuilder.java:213)
2022-06-27T14:35:05.6893195Z Jun 27 14:35:05 	... 56 more
2022-06-27T14:35:05.6894128Z Jun 27 14:35:05 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-06-27T14:35:05.6894894Z Jun 27 14:35:05 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
2022-06-27T14:35:05.6895444Z Jun 27 14:35:05 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
2022-06-27T14:35:05.6895980Z Jun 27 14:35:05 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)
2022-06-27T14:35:05.6896566Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:222)
2022-06-27T14:35:05.6897235Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkImageBuilder.build(FlinkImageBuilder.java:179)
2022-06-27T14:35:05.6897697Z Jun 27 14:35:05 	... 57 more
2022-06-27T14:35:05.6898551Z Jun 27 14:35:05 Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-06-27T14:35:05.6899343Z Jun 27 14:35:05 	at com.github.dockerjava.api.command.BuildImageResultCallback.getImageId(BuildImageResultCallback.java:78)
2022-06-27T14:35:05.6900029Z Jun 27 14:35:05 	at com.github.dockerjava.api.command.BuildImageResultCallback.awaitImageId(BuildImageResultCallback.java:50)
2022-06-27T14:35:05.6900686Z Jun 27 14:35:05 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:147)
2022-06-27T14:35:05.6901316Z Jun 27 14:35:05 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)
2022-06-27T14:35:05.6901907Z Jun 27 14:35:05 	at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17)
2022-06-27T14:35:05.6902431Z Jun 27 14:35:05 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39)
2022-06-27T14:35:05.6902929Z Jun 27 14:35:05 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-06-27T14:35:05.6903465Z Jun 27 14:35:05 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-06-27T14:35:05.6904049Z Jun 27 14:35:05 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-06-27T14:35:05.6904549Z Jun 27 14:35:05 	at java.lang.Thread.run(Thread.java:750)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37250&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a"	FLINK	Closed	2	1	6913	test-stability
13399680	Update org.jsoup.jsoup to 1.14.2	"Update org.jsoup.jsoup to at least 1.14.2 to address CVE GHSA-m72m-mhq2-9p6c

Flink itself isn't directly affected, but it's still good to update the dependency to avoid any scanners reporting vulnerabilities in Flink"	FLINK	Closed	4	11500	6913	pull-request-available
13528250	Update external connector workflow to use actions that use Node16 instead of Node12 under the hood	"Currently, running workflows for externalized connectors results in warnings such as:

{code:bash}
Node.js 12 actions are deprecated. Please update the following actions to use Node.js 16: actions/checkout@v2, actions/setup-java@v2, stCarolas/setup-maven@v4.2. For more information see: https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/.
{code}

We should update the used Github actions to their latest versions"	FLINK	Closed	3	11500	6913	pull-request-available
13404410	"""No space left on device"" in Azure e2e tests"	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24668&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=19772

{code}
Sep 30 17:08:42 Job has been submitted with JobID 5594c18e128a328ede39cfa59cb3cb07
Sep 30 17:08:56 2021-09-30 17:08:56,809 main ERROR Recovering from StringBuilderEncoder.encode('2021-09-30 17:08:56,807 WARN  org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
Sep 30 17:08:56 java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
Sep 30 17:08:56 org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (5594c18e128a328ede39cfa59cb3cb07)
Sep 30 17:08:56 	at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:923)
Sep 30 17:08:56 	at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:937)
Sep 30 17:08:56 	at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordina2021-09-30T17:08:57.1584224Z ##[error]No space left on device
{code}"	FLINK	Closed	1	11500	6913	auto-deprioritized-critical, pull-request-available, test-stability
13446617	Elasticsearch connector should not use flink-table-planner but flink-table-planner-loader	Connectors should not rely on {{flink-table-planner}} but on {{flink-table-planner-loader}} by default. We can should change this for the Elasticsearch connector as this is being externalized at the moment	FLINK	Closed	3	11500	6913	pull-request-available
13568267	Re-enable forkReuse for flink-table-planner	With FLINK-18356 resolved, we should re-enable forkReuse for flink-table-planner to speed up the tests	FLINK	Closed	3	11500	6913	pull-request-available
13543739	"Nightly builds for Elasticsearch are failing with ""pull access denied for flink-base"""	"{code:java}
Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: pull access denied for flink-base, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
{code}

https://github.com/apache/flink-connector-elasticsearch/actions/runs/5564892055/jobs/10164792729#step:13:17389"	FLINK	Closed	1	1	6913	test-stability
13555020	Upgrade netty to 4.1.93+	"A number of tests fails like e.g.for 
{{org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOpe}}
{noformat}
[ERROR] ratorTest.testFinishBundleTriggeredByTime  Time elapsed: 0.031 s  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.runtime.arrow.serializers.ArrowSerializer
	at org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.open(ArrowPythonScalarFunctionOperator.java:72)
	at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.open(AbstractStreamOperatorTestHarness.java:681)
	at org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.testFinishBundleTriggeredByTime(PythonScalarFunctionOperatorTestBase.java:156)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
...
Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.RuntimeException: Arrow depends on DirectByteBuffer.<init>(long, int) which is not available. Please set the system property 'io.netty.tryReflectionSetAccessible' to 'true'. [in thread ""ForkJoinPool-3-worker-1""]
    at org.apache.flink.table.runtime.arrow.ArrowUtils.checkArrowUsable(ArrowUtils.java:184)
    at org.apache.flink.table.runtime.arrow.serializers.ArrowSerializer.<clinit>(ArrowSerializer.java:44)
    at org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.<init>(PassThroughPythonAggregateFunctionRunner.java:96)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest$PassThroughBatchArrowPythonGroupWindowAggregateFunctionOperator.createPythonFunctionRunner(BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java:414)
    at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.open(AbstractExternalPythonFunctionOperator.java:56)
    at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:92)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.open(AbstractArrowPythonAggregateFunctionOperator.java:89)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperator.open(AbstractBatchArrowPythonAggregateFunctionOperator.java:82)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperator.open(BatchArrowPythonGroupWindowAggregateFunctionOperator.java:119)
    at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.open(AbstractStreamOperatorTestHarness.java:681)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount(BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java:140)
    ... 57 more
{noformat}


UPDATE
The reason is that since JDK21 there was removed this constructor within https://bugs.openjdk.org/browse/JDK-8303083
 and corresponding changes in Netty are done at https://github.com/netty/netty/pull/13366 which is a part of 4.1.93.Final"	FLINK	Closed	3	7	6913	pull-request-available
13414255	Implement artifact cacher for Bash based Elasticsearch test	The `elasticsearch-common.sh` downloads Elasticsearch for tests and already has a retry feature in it. This should be refactor use the recently introduced common retry function. 	FLINK	Resolved	3	4	6913	pull-request-available
13517434	flink-connector-pulsar not retrievable from Apache's Snapshot Maven repository	"The build failure was caused by {{flink-connector-pulsar}} not being retrievable from the Apache Snapshot Maven repository:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10132

{code}
Jan 10 02:03:24 [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
Jan 10 02:03:24 [ERROR] Failed to execute goal on project flink-python: Could not resolve dependencies for project org.apache.flink:flink-python:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-sql-connector-pulsar:jar:4.0-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -> [Help 1]
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 10 02:03:24 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 10 02:03:24 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] After correcting the problems, you can resume the build with the command
Jan 10 02:03:24 [ERROR]   mvn <goals> -rf :flink-python
{code}"	FLINK	Closed	2	1	6913	pull-request-available, test-stability
13427759	Remove Twitter connector	The Flink community has voted and agreed to remove the Twitter connector from the Flink repository. Details can be found in https://lists.apache.org/thread/b9mdwqwdyfyq38j6z86rn0d8b26k96c2	FLINK	Closed	3	11500	6913	pull-request-available
13568312	Bump org.yaml:snakeyaml from 1.31 to 2.2 for flink-connector-elasticsearch	https://github.com/apache/flink-connector-elasticsearch/pull/90	FLINK	Closed	3	11500	6913	pull-request-available
13486117	Bump aws-java-sdk-s3 to 1.12.319	As reported by Dependabot in https://github.com/apache/flink/pull/20285	FLINK	Closed	3	11500	6913	pull-request-available
13337778	"SQLClientHBaseITCase.testHBase failed with ""java.io.IOException: Process failed due to timeout"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8541&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6

{code}
00:50:02,589 [                main] INFO  org.apache.flink.tests.util.flink.FlinkDistribution          [] - Stopping Flink cluster.
00:50:04,106 [                main] INFO  org.apache.flink.tests.util.flink.FlinkDistribution          [] - Stopping Flink cluster.
00:50:04,741 [                main] INFO  org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource [] - Backed up logs to /home/vsts/work/1/s/flink-end-to-end-tests/artifacts/flink-b3924665-1ac9-4309-8255-20f0dc94e7b9.
00:50:04,788 [                main] INFO  org.apache.flink.tests.util.hbase.LocalStandaloneHBaseResource [] - Stopping HBase Cluster
00:50:16,243 [                main] ERROR org.apache.flink.tests.util.hbase.SQLClientHBaseITCase       [] - 
--------------------------------------------------------------------------------
Test testHBase[0: hbase-version:1.4.3](org.apache.flink.tests.util.hbase.SQLClientHBaseITCase) failed with:
java.io.IOException: Process failed due to timeout.
	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:130)
	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:108)
	at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQLJob(FlinkDistribution.java:221)
	at org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource$StandaloneClusterController.submitSQLJob(LocalStandaloneFlinkResource.java:196)
	at org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.executeSqlStatements(SQLClientHBaseITCase.java:215)
	at org.apache.flink.tests.util.hbase.SQLClientHBaseITCase.testHBase(SQLClientHBaseITCase.java:152)
{code}

"	FLINK	Resolved	3	7	6913	auto-unassigned, pull-request-available, test-stability
13563032	Bump com.google.guava:guava from 31.1-jre to 33.0.0-jre	Resolves two Dependabot reports https://github.com/apache/flink-connector-jdbc/security/dependabot?q=package%3Acom.google.guava%3Aguava+manifest%3Apom.xml+has%3Apatch by upgrading to the now latest available version	FLINK	Closed	3	11500	6913	pull-request-available
13535745	Update the used Pulsar connector in flink-python to 4.0.0	flink-python still references and tests flink-connector-pulsar:3.0.0, while it should be using flink-connector-pulsar:4.0.0. That's because the newer version is the only version compatible with Flink 1.17 and it doesn't rely on flink-shaded. 	FLINK	Closed	2	1	6913	pull-request-available
13413009	Streaming File Sink s3 end-to-end test stalled on azure	"{code:java}
Nov 21 00:04:36 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:41 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:46 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:51 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:56 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:01 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:06 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:11 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:16 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:21 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:26 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:31 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:36 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:41 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:46 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:52 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:57 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:02 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:07 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:12 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:17 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:22 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:27 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:32 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:37 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:42 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:47 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:51 Test (pid: 414853) did not finish after 900 seconds.
Nov 21 00:06:51 Printing Flink logs and killing it: {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26784&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&l=12438"	FLINK	Resolved	3	1	6913	pull-request-available, test-stability
13486632	Remove HCatalog	Remove HCatalog from the codebase as voted on in https://lists.apache.org/thread/w3jfgdk6lq846oh356qnwczydm9oszp9	FLINK	Closed	3	11500	6913	pull-request-available
13416504	Enable Kafka E2E tests on Java 11	The Java Kafka E2E tests are currently not run on Java 11. We should check what the actual issue is and whether it can be resolved (e.g., by a Kafka server version bump):	FLINK	Closed	3	7	6913	pull-request-available
13531224	JDBC nightly CI failure	"Investigate and fix the nightly CI failure. Example [https://github.com/apache/flink-connector-jdbc/actions/runs/4585903259]

 
{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-jdbc: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(Ljava/lang/Class;)Ljava/lang/Object; -> [Help 1]{code}
 

 "	FLINK	Resolved	3	11500	6913	pull-request-available
13507513	Sync Pulsar updates to external Pulsar connector repository	Currently the external Pulsar repository contains the code from the {release-1.16} branch. This should be synced with the changes that are merged into {master} since. 	FLINK	Closed	3	7	6913	pull-request-available
13541612	Connector Shared Utils checks out wrong branch when running CI for PRs	Since FLINK-31923, when a branch is not specified, all CI runs use {{main}} as the default branch when none is specified. This doesn't work when submitting a PR, since it shouldn't use {{main}} but it should use the specific ref that triggered that workflow. 	FLINK	Closed	1	1	6913	pull-request-available
13436885	Add Github Actions build pipeline for flink-connector-elasticsearch	"With connectors being moved to their individual repository, we need to have a pipeline that can run the necessary compile and test steps. 

With Elasticsearch in the process of being moved out (see FLINK-26884) we should add this to make sure that on pushes and pull requests this pipeline is executed. "	FLINK	Closed	3	11500	6913	pull-request-available
13511142	crictl causes long wait in e2e tests	"We observed strange behavior in the e2e test where the e2e test run times out: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43824&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=7446

The issue seems to be related to {{crictl}} again because we see the following error message in multiple tests. No logs are produced afterwards for ~30mins resulting in the overall test run taking too long:
{code}
Dec 09 08:55:39 crictl
fatal: destination path 'cri-dockerd' already exists and is not an empty directory.
fatal: a branch named 'v0.2.3' already exists
mkdir: cannot create directory ‘bin’: File exists
Dec 09 09:26:41 fs.protected_regular = 0
{code}"	FLINK	Closed	3	1	6913	pull-request-available, test-stability
13486088	Update Akka to 2.6.20	Update Akka to the latest 2.6 version that's still under Apache license	FLINK	Closed	3	11500	6913	pull-request-available
13421337	Update com.h2database:h2 to 2.0.206	Flink uses com.h2database:h2 version 1.4.200, we should update this to 2.0.206	FLINK	Resolved	3	11500	6913	pull-request-available
13419815	Update to Log4j 2.17.1	We should update from Log4j 2.17.0 to 2.17.1 to address CVE-2021-44832: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.	FLINK	Closed	3	11500	6913	pull-request-available
13553665	Upgrade org.apache.avro:avro to 1.11.3 to mitigate CVE-2023-39410	"We should update AVRO to 1.11.3 to avoid false-positives on CVE-2023-39410
"	FLINK	Closed	3	11500	6913	pull-request-available
13470135	Update Confluent Platform images to v7.2.2	"We have updated the used Kafka Clients to v3.1.1 via FLINK-28060 and then to v3.2.1 via FLINK-28060 and finally to v3.2.3 via FLINK-29513, but we are using Confluent Platform 6.2.2 which supports up to Kafka 2.8.0.

We should update to Confluent Platform v7.2.2 (latest version of 7.2), which includes support for Kafka 3.2.3. "	FLINK	Closed	3	11500	6913	pull-request-available, stale-assigned
13517503	RabbitMQ connector has dependency convergence error for org.junit.jupiter:junit-jupiter	"https://github.com/apache/flink-connector-rabbitmq/actions/runs/3773487890/jobs/6414966557

{code:java}
Dependency convergence error for org.junit.jupiter:junit-jupiter:5.8.1 paths to dependency are:
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.junit.jupiter:junit-jupiter:5.8.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-test-utils-junit:1.17-20221224.001931-108
    +-org.junit.jupiter:junit-jupiter:5.9.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-architecture-tests-test:1.17-20221224.002742-107
    +-org.junit.jupiter:junit-jupiter:5.9.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-architecture-tests-test:1.17-20221224.002742-107
    +-org.apache.flink:flink-test-utils:1.17-SNAPSHOT
      +-org.junit.jupiter:junit-jupiter:5.9.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-architecture-tests-production:1.17-20221224.003645-107
    +-org.junit.jupiter:junit-jupiter:5.9.1
{code}"	FLINK	Closed	3	1	6913	pull-request-available
13557554	DataType ARRAY<INT NOT NULL> fails to cast into Object[]	"When upgrading Iceberg's Flink version to 1.18, we found the Flink-related unit test case broken due to this issue. The below code used to work fine in Flink 1.17 but failed after upgrading to 1.18. DataType ARRAY<INT NOT NULL> fails to cast into Object[].

*Error:*

{code}
Exception in thread ""main"" java.lang.ClassCastException: [I cannot be cast to [Ljava.lang.Object;
at FlinkArrayIntNotNullTest.main(FlinkArrayIntNotNullTest.java:18)
{code}

*Repro:*

{code}

  import org.apache.flink.table.data.ArrayData;
  import org.apache.flink.table.data.GenericArrayData;
  import org.apache.flink.table.api.EnvironmentSettings;
  import org.apache.flink.table.api.TableEnvironment;
  import org.apache.flink.table.api.TableResult;

  public class FlinkArrayIntNotNullTest {

    public static void main(String[] args) throws Exception {

      EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
      TableEnvironment env = TableEnvironment.create(settings);

      env.executeSql(""CREATE TABLE filesystemtable2 (id INT, data ARRAY<INT NOT NULL>) WITH ('connector' = 'filesystem', 'path' = '/tmp/FLINK/filesystemtable2', 'format'='json')"");
      env.executeSql(""INSERT INTO filesystemtable2 VALUES (4,ARRAY [1,2,3])"");
      TableResult tableResult = env.executeSql(""SELECT * from filesystemtable2"");

      ArrayData actualArrayData = new GenericArrayData((Object[]) tableResult.collect().next().getField(1));
    }
  }

{code}

*Analysis:*

1. The code works fine with ARRAY<INT> datatype. The issue happens when using ARRAY<INT NOT NULL>.
2. The code works fine when casting into int[] instead of Object[].

"	FLINK	Closed	1	1	6913	pull-request-available
13521662	Bump json5 from 1.0.1 to 1.0.2	"Dependabot has created https://github.com/apache/flink/pull/21617

This is the corresponding Jira ticket"	FLINK	Closed	3	11500	6913	pull-request-available
13531252	Remove Conjars	With Conjars no longer being available (only https://conjars.wensel.net/ is there), we should remove all the notices to Conjars in Flink. We've already removed the need for Conjars because we've excluded Pentaho as part of FLINK-27640, which eliminates having any dependency that relies on Conjars. 	FLINK	Closed	3	11500	6913	pull-request-available
13408613	Update Presto Hadoop dependency to latest version	"The Flink Presto Hadoop dependency com.facebook.presto.hadoop:hadoop-apache2 that Flink uses is rather outdated (released in March 2017). 

We should upgrade to the latest released version for Hadoop2 (which was released in March 2021)"	FLINK	Resolved	4	11500	6913	pull-request-available
13409555	Update Cython to the latest version	Update Cython from 0.29.16 to 0.29.24	FLINK	Closed	4	11500	6913	pull-request-available
13451346	Update httplib2 to at least 0.19.0 	We should update httplib2 to at least 0.19.0 to address CVE-2021-21240 and avoid false flags about Flink being vulnerable. 	FLINK	Closed	3	11500	6913	pull-request-available
13530050	ClassNotFoundException when using GCS path as HA directory	"Hi,

When I am trying to run Flink job in HA mode with GCS path as a HA directory (eg: [gs://flame-poc/ha]) or while starting a job from checkpoints in GCS I am getting following exception:
{code:java}
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not found
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2712) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:107) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:102) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:338) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getUgiUserName(GoogleHadoopFileSystemBase.java:1226) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:858) ~[?:?]
	at org.apache.flink.fs.gs.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[?:?] {code}
{*}Observations{*}:

While using File system as a HA path and GCS as checkpointing directory the job is able to write checkpoints to GCS checkpoint path. 

After debugging what I found was all the *org.apache.hadoop* paths are shaded to {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop{*}. Ideally the code should look for  {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback{*} instead of  *org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.*
I think it is not getting shaded over here due to reflection being used here:
[https://github.com/apache/hadoop/blob/branch-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java#L108]

As a workaround I rebuilt *flink-gs-fs-hadoop* plugin removing this relocation and it worked for me.
{code:java}
<relocation>
<pattern>org.apache.hadoop</pattern>
<shadedPattern>org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop</shadedPattern>
</relocation> {code}"	FLINK	Closed	2	1	6913	pull-request-available
13445591	Disable PulsarSinkITCase and PulsarSourceITCase on JDK 11	Since Pulsar doesn't yet support Java 11, we should make sure that the Pulsar tests don't run when testing JDK11. This is the case already for the e2e tests, but not yet for the connector tests. We should disable this too. 	FLINK	Closed	3	11500	6913	pull-request-available
13410247	Bump commons-cli to v1.5.0	Bump commons-cli:commons-cli:1.4 to commons-cli:commons-cli:1.5.0	FLINK	Resolved	4	11500	6913	pull-request-available
13422744	Upgrade com.amazonaws:amazon-kinesis-client dependency from 1.14.1 to 1.14.7	We should update the com.amazonaws:amazon-kinesis-client dependency from 1.14.1 to 1.14.7	FLINK	Resolved	3	11500	6913	pull-request-available
13409696	Update testcontainers dependency to v1.16.2	"We should update our testcontainers dependency to the latest version, which is 1.16.2

Main benefits (based on [https://github.com/testcontainers/testcontainers-java/releases)]

* Better startup performance for all containers
* Faster Cassandra startup
* Host port access for containers (make hosts ports accessible to containers, even after the container has started)
* New Azure Cosmos DB module"	FLINK	Resolved	4	11500	6913	pull-request-available
13419566	Update net.sf.py4j:py4j dependency to 0.10.8.1	Flink uses net.sf.py4j:py4j version 0.10.8.1, while version 0.10.9.3 with multiple improvements has been released. We should upgrade this dependency. 	FLINK	Resolved	3	2	6913	pull-request-available
13589567	Javadocs aren't visible anymore for Flink 1.17 and above	"The JavaDocs for 1.16 correctly open: https://nightlies.apache.org/flink/flink-docs-release-1.16/api/java/

However, for 1.17 and later versions, they aren't visible/rendered properly:
https://nightlies.apache.org/flink/flink-docs-release-1.17/api/java/"	FLINK	Closed	2	1	6913	pull-request-available
13420353	Update and synchronise used versions of Kafka Client and Confluent Platform	"The Flink codebases uses Kafka Client and Confluent Platform in multiple places:

* AVRO Confluent Schema Registry
* Flink end-to-end tests (Bash e2e tests)
* Flink end-to-end tests common (Java e2e tests)
* SQL AVRO Confluent Schema Registry
* Flink Test Utils
* Flink Tests

The used versions are currently not in sync, which could result in unexpected results. For context, these are the currently used versions:

Kafka Client: 2.2.0, 2.2.2, 2.4.1, 2.6.0
Confluent Platform: 5.2.6, 5.5.2, 6.0.4, 6.2.1

Given https://docs.confluent.io/platform/current/installation/versions-interoperability.html it probably makes sense to update to Kafka Client 2.8.1 and Confluent Platform 6.2.2. "	FLINK	Resolved	3	11500	6913	pull-request-available
13550323	Notice files for Statefun are outdated	"{code:java}
- NOTICE files are present
  - Note: The copyright year is out of data (2020)
  - Concern: we bundle AnchorJS (MIT) v3.1.0 and this is not listed in the
NOTICE file
  - Concern: ""statefun-sdk-java"" bundles
""com.google.auto.service:auto-service-annotations:jar:1.0-rc6"" but does not
declare it in the NOTICE
  - Concern: ""statefun-flink-distribution""
    - bundles ""org.apache.kafka:kafka-clients:3.2.3"" but declares
""org.apache.kafka:kafka-clients:2.4.1""
    - bundles ""com.github.luben:zstd-jni:1.5.2-1"" but declares
""com.github.luben:zstd-jni:1.4.3-1""
    - bundles ""com.fasterxml.jackson.core:jackson-core:2.13.4"" but declares
""com.fasterxml.jackson.core:jackson-core:2.12.1""
    - bundles ""com.fasterxml.jackson.core:jackson-annotations:2.13.4"" but
declares ""com.fasterxml.jackson.core:jackson-annotations:2.12.1""
    - bundles ""com.fasterxml.jackson.core:jackson-databind:2.13.4.2"" but
declares ""com.fasterxml.jackson.core:jackson-databind:2.12.1""
    - bundles
""com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.13.4"" but
declares ""com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.12.1""
    - bundles ""commons-io:commons-io:jar:2.11.0"" but declares
""commons-io:commons-io:jar:2.8.0""
    - bundles ""commons-codec:commons-codec:1.15"" but declares
""commons-codec:commons-codec:1.13""
    - bundles ""com.esotericsoftware.minlog:minlog:1.2"" but does not declare
it
    - bundles ""com.ibm.icu:icu4j:jar:67.1"" but does not declare it
    - bundles ""org.objenesis:objenesis:jar:2.1"" but does not declare it
    - bundles ""com.esotericsoftware.kryo:kryo:2.24.0"" but does not declare
it
    - bundles ""commons-collections:commons-collections:3.2.2"" but does not
declare it
    - bundles ""org.apache.commons:commons-compress:1.21"" but does not
declare it
{code}"	FLINK	Closed	1	1	6913	pull-request-available
13410659	Update JUnit5 to v5.8.1	We should update to the latest version of JUnit5, v5.8.1	FLINK	Resolved	4	11500	6913	pull-request-available
13419565	Update slf4j-api dependency to 1.7.32	Flink is using slf4j-api version 1.7.15 (from February 2016), while version 1.7.32 (Jul 2021) has been released. We should upgrade to the latest dependency. 	FLINK	Resolved	3	11500	6913	pull-request-available
13409398	Update multiple Jackson dependencies to v2.13.0	"Flink uses multiple com.fasterxml.jackson components in different Flink modules. We should update these to the latest version

Example (one or more used in ElasticSearch connector, Kinesis, FS Hadoop/Presto, AVRO, Python, Table API etc)
- com.fasterxml.jackson.core:jackson-core:2.12.1
- com.fasterxml.jackson.core:jackson-databind:2.12.1
- com.fasterxml.jackson.core:jackson-annotations:2.12.1
- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.12.1
- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.12.1
- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.12.1

Could all be updated to:
- com.fasterxml.jackson.core:jackson-core:2.13.0
- com.fasterxml.jackson.core:jackson-databind:2.13.0
- com.fasterxml.jackson.core:jackson-annotations:2.13.0
- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.13.0
- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.13.0
- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.13.0
"	FLINK	Resolved	4	11500	6913	pull-request-available
13434131	Remove NiFi connector	The community voted to deprecate NiFi in 1.15 and we should remove it from 1.16 onwards. 	FLINK	Closed	3	11500	6913	pull-request-available
13337781	"YARN tests failed with ""java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8541&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354

{code}
2020-10-28T22:58:39.4927767Z 2020-10-28 22:57:33,866 main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy for element DefaultRolloverStrategy: java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}"" java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}""
2020-10-28T22:58:39.4929252Z 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
2020-10-28T22:58:39.4929823Z 	at java.lang.Integer.parseInt(Integer.java:569)
2020-10-28T22:58:39.4930327Z 	at java.lang.Integer.parseInt(Integer.java:615)
2020-10-28T22:58:39.4931047Z 	at org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy$Builder.build(DefaultRolloverStrategy.java:137)
2020-10-28T22:58:39.4931866Z 	at org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy$Builder.build(DefaultRolloverStrategy.java:90)
2020-10-28T22:58:39.4932720Z 	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:122)
2020-10-28T22:58:39.4933446Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1002)
2020-10-28T22:58:39.4934275Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:942)
2020-10-28T22:58:39.4935029Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:934)
2020-10-28T22:58:39.4935837Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:934)
2020-10-28T22:58:39.4936605Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:552)
2020-10-28T22:58:39.4937573Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:241)
2020-10-28T22:58:39.4938429Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:288)
2020-10-28T22:58:39.4939206Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:579)
2020-10-28T22:58:39.4939885Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-10-28T22:58:39.4940490Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-10-28T22:58:39.4941087Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-10-28T22:58:39.4941733Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-10-28T22:58:39.4942534Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-10-28T22:58:39.4943154Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-10-28T22:58:39.4943820Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-10-28T22:58:39.4944540Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-10-28T22:58:39.4945199Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-10-28T22:58:39.4945858Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-10-28T22:58:39.4946426Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-10-28T22:58:39.4946965Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:349)
2020-10-28T22:58:39.4947698Z 	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.<clinit>(ClusterEntrypoint.java:108)
{code}"	FLINK	Closed	1	1	8669	pull-request-available, test-stability
12945410	Kafka09ITCase.testBigRecordJob fails on Travis	"The test case {{Kafka09ITCase.testBigRecordJob}} failed on Travis.

https://s3.amazonaws.com/archive.travis-ci.org/jobs/112049279/log.txt"	FLINK	Resolved	3	1	8669	test-stability
13240859	YARNSessionCapacitySchedulerITCase failed due to non prohibited exception	"YARNSessionCapacitySchedulerITCase fails due to non prohibited exception.

[https://api.travis-ci.org/v3/job/548491542/log.txt]
{code:java}
2019-06-21 08:22:27,313 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (SUM(1), at main(WordCount.java:79) (2/2) (a1708bb0544633b4e57e8bb84a1a48f3) switched from RUNNING to FAILED.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeNoLongerUsedSlots(TaskExecutor.java:1475)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.syncSlotsWithSnapshotFromJobMaster(TaskExecutor.java:1436)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$3200(TaskExecutor.java:141)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobManagerHeartbeatListener.lambda$reportPayload$1(TaskExecutor.java:1691)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2019-06-21 08:22:27,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Flink Java Job at Fri Jun 21 08:22:16 UTC 2019 (8f8dced4fb89f8e5cb05d9286683ecaf) switched from state RUNNING to FAILING.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeNoLongerUsedSlots(TaskExecutor.java:1475)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.syncSlotsWithSnapshotFromJobMaster(TaskExecutor.java:1436)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$3200(TaskExecutor.java:141)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobManagerHeartbeatListener.lambda$reportPayload$1(TaskExecutor.java:1691)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21){code}"	FLINK	Closed	2	1	8669	test-stability
13368227	Exceptions from JobMaster initialization are not forwarded to the user	"Steps to reproduce:
Set up a streaming job with an invalid parallelism configuration, for example:
{code}
.setParallelism(15).setMaxParallelism(1);
{code}

This should report the following exception to the user:
{code}
Caused by: org.apache.flink.runtime.JobException: Vertex Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)'s parallelism (15) is higher than the max parallelism (1). Please lower the parallelism or increase the max parallelism.
		at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:160)
		at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:781)
		at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:193)
		at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:106)
		at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:252)
		at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:185)
		at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:119)
		at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132)
		at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110)
		at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340)
		at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317)
		at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:94)
		at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:39)
		at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.startJobMasterServiceSafely(JobManagerRunnerImpl.java:363)
		... 13 more
{code}

However, what the user sees is 
{code}
2021-03-28 20:32:33,935 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 419f60eac551619fc1081c670ced3649 reached globally terminal state FAILED.

...

2021-03-28 20:32:33,974 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
2021-03-28 20:32:33,977 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
Exception in thread ""main"" org.apache.flink.util.FlinkException: Failed to execute job 'CarTopSpeedWindowingExample'.
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1975)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1853)
	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:69)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1839)
	at org.apache.flink.streaming.examples.windowing.TopSpeedWindowing.main(TopSpeedWindowing.java:101)
Caused by: java.lang.RuntimeException: Error while waiting for job to be initialized
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:160)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$submitJob$2(PerJobMiniClusterFactory.java:83)
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: JobMaster has been shut down.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$null$0(PerJobMiniClusterFactory.java:89)
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:144)
	... 9 more
Caused by: org.apache.flink.util.FlinkException: JobMaster has been shut down.
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.closeAsync(JobManagerRunnerImpl.java:197)
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137)
	at org.apache.flink.runtime.dispatcher.DispatcherJob.lambda$closeAsync$8(DispatcherJob.java:273)
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848)
	at java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168)
	at org.apache.flink.runtime.dispatcher.DispatcherJob.closeAsync(DispatcherJob.java:268)
	at org.apache.flink.runtime.dispatcher.Dispatcher.removeJob(Dispatcher.java:754)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$4(Dispatcher.java:432)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:814)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-03-28 20:32:34,011 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
{code}

"	FLINK	Closed	1	1	8669	pull-request-available
12736067	Describe how to run the examples from the command line	"The examples page here: http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/java_api_examples.html does not describe how users can run the examples from the command line (or the web interface).

This is the thread where the user suggested this: http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/run_example_quickstart.html#comment-1554345488"	FLINK	Resolved	4	4	8669	starter
13352990	"HBaseTablePlanTest tests failed in haoop 3.1.3 with ""java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12159&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]
{code:java}
2021-01-15T22:48:58.1843544Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
2021-01-15T22:48:58.1844358Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
2021-01-15T22:48:58.1845035Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
2021-01-15T22:48:58.1845805Z 	at org.apache.flink.connector.hbase.options.HBaseOptions.getHBaseConfiguration(HBaseOptions.java:157)
2021-01-15T22:48:58.1846960Z 	at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:73)
2021-01-15T22:48:58.1848020Z 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:119)
2021-01-15T22:48:58.1848574Z 	... 49 more
{code}
The exception seems that the different version of guava caused. 

 "	FLINK	Closed	1	1	8669	pull-request-available, test-stability
12922000	KafkaITCase.testOffsetAutocommitTest	"https://travis-ci.org/apache/flink/jobs/96981797
{noformat}
Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 126.278 sec <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.KafkaITCase
testOffsetAutocommitTest(org.apache.flink.streaming.connectors.kafka.KafkaITCase)  Time elapsed: 12.735 sec  <<< FAILURE!
java.lang.AssertionError: Offset of o1=-915623761776 was not in range
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runOffsetAutocommitTest(KafkaConsumerTestBase.java:333)
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testOffsetAutocommitTest(KafkaITCase.java:56)

Results :

Failed tests: 
KafkaITCase.testOffsetAutocommitTest:56->KafkaConsumerTestBase.runOffsetAutocommitTest:333 Offset of o1=-915623761776 was not in range
{noformat}"	FLINK	Resolved	2	1	8669	test-stability
13337848	Local recovery and sticky scheduling end-to-end test fails to report error properly	"INSTANCE: [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/8563/logs/141]
{code:java}
2020-10-29T09:43:24.0088180Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-end-to-end-tests-hbase: There are test failures.
2020-10-29T09:43:24.0088792Z [ERROR] 
2020-10-29T09:43:24.0089518Z [ERROR] Please refer to /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire-reports for the individual test results.
2020-10-29T09:43:24.0090427Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-10-29T09:43:24.0090914Z [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-29T09:43:24.0093105Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire/surefirebooter6795869883612750001.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire 2020-10-29T09-34-47_778-jvmRun2 surefire2269050977160717631tmp surefire_67897497331523564186tmp
2020-10-29T09:43:24.0094488Z [ERROR] Error occurred in starting fork, check output in log
2020-10-29T09:43:24.0094797Z [ERROR] Process Exit Code: 143
2020-10-29T09:43:24.0095033Z [ERROR] Crashed tests:
2020-10-29T09:43:24.0095321Z [ERROR] org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-10-29T09:43:24.0095828Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-29T09:43:24.0097838Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire/surefirebooter6795869883612750001.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire 2020-10-29T09-34-47_778-jvmRun2 surefire2269050977160717631tmp surefire_67897497331523564186tmp
2020-10-29T09:43:24.0098966Z [ERROR] Error occurred in starting fork, check output in log
2020-10-29T09:43:24.0099266Z [ERROR] Process Exit Code: 143
2020-10-29T09:43:24.0099502Z [ERROR] Crashed tests:
2020-10-29T09:43:24.0099789Z [ERROR] org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-10-29T09:43:24.0100331Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-10-29T09:43:24.0100883Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:282)
2020-10-29T09:43:24.0101774Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)
2020-10-29T09:43:24.0102360Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-10-29T09:43:24.0103004Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-10-29T09:43:24.0103737Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-10-29T09:43:24.0104301Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-10-29T09:43:24.0104828Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-10-29T09:43:24.0105334Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-10-29T09:43:24.0105826Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-10-29T09:43:24.0106384Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-10-29T09:43:24.0106969Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-10-29T09:43:24.0107603Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-10-29T09:43:24.0108201Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-10-29T09:43:24.0108673Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-10-29T09:43:24.0109110Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-10-29T09:43:24.0109517Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-10-29T09:43:24.0110063Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-10-29T09:43:24.0110601Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-10-29T09:43:24.0110998Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-29T09:43:24.0111426Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-29T09:43:24.0112032Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-29T09:43:24.0112487Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-29T09:43:24.0112955Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-10-29T09:43:24.0113563Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-10-29T09:43:24.0114072Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-10-29T09:43:24.0114578Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-10-29T09:43:24.0115188Z [ERROR] -> [Help 1]
{code}"	FLINK	Closed	1	4	8669	pull-request-available, test-stability
13235130	Describe new contribution process	"The community has decided to change the contribution process to seek consensus in Jira first.

Update the website to reflect this change."	FLINK	Resolved	3	4	8669	pull-request-available
13371859	Clarify Reactive Mode documentation wrt to timeouts	In the release testing of Reactive Mode (FLINK-22134) we found that the documentation of the timeouts needs some clarification.	FLINK	Closed	3	4	8669	pull-request-available
12759649	JDBCInputFormat does not implement NonParallelInput interface	"The JDBCInputFormat for the Java API is not implementing the NonParallelInput interface.
There are no methods required for implementing the interface. Its just a maker for the optimizer."	FLINK	Resolved	4	1	8669	starter
13304855	Remove flink-shaded-hadoop-2-parent and submodules	Since Flink does not use flink-shaded-hadoop-2 anymore, we want to remove it from flink-shaded.	FLINK	Closed	3	7	8669	pull-request-available
13037641	Default Flink configuration contains whitespace characters, causing parser WARNings	"{code}
2017-01-25 09:45:30,670 WARN  org.apache.flink.configuration.GlobalConfiguration            - Error while trying to split key and value in configuration file /yarn/nm/usercache/robert/appcache/application_1485249546281_0018/container_1485249546281_0018_01_000001/flink-conf.yaml:  
{code}

The whitespace is currently in line 67:
{code}
#==============================================================================
 
# The address under which the web-based runtime monitor listens.
{code}

I think we should add a test to the {{GlobalConfigurationTest}} that ensures the configuration file we are shipping doesn't produce any WARNings by default."	FLINK	Resolved	3	1	8669	starter
13568720	TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding failed	"https://github.com/XComp/flink/actions/runs/7927275243/job/21643615491#step:10:9880

{code}
Error: 07:48:06 07:48:06.643 [ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.309 s <<< FAILURE! -- in org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest
Error: 07:48:06 07:48:06.646 [ERROR] org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding -- Time elapsed: 0.036 s <<< FAILURE!
Feb 16 07:48:06 Output was not correct.: array lengths differed, expected.length=8 actual.length=7; arrays first differed at element [6]; expected:<Watermark @ 5> but was:<Watermark @ 6>
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:78)
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:28)
Feb 16 07:48:06 	at org.junit.Assert.internalArrayEquals(Assert.java:534)
Feb 16 07:48:06 	at org.junit.Assert.assertArrayEquals(Assert.java:285)
Feb 16 07:48:06 	at org.apache.flink.streaming.util.TestHarnessUtil.assertOutputEquals(TestHarnessUtil.java:59)
Feb 16 07:48:06 	at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding(TwoInputStreamTaskTest.java:248)
Feb 16 07:48:06 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 16 07:48:06 Caused by: java.lang.AssertionError: expected:<Watermark @ 5> but was:<Watermark @ 6>
Feb 16 07:48:06 	at org.junit.Assert.fail(Assert.java:89)
Feb 16 07:48:06 	at org.junit.Assert.failNotEquals(Assert.java:835)
Feb 16 07:48:06 	at org.junit.Assert.assertEquals(Assert.java:120)
Feb 16 07:48:06 	at org.junit.Assert.assertEquals(Assert.java:146)
Feb 16 07:48:06 	at org.junit.internal.ExactComparisonCriteria.assertElementsEqual(ExactComparisonCriteria.java:8)
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:76)
Feb 16 07:48:06 	... 6 more
{code}

I couldn't reproduce it locally with 20000 runs."	FLINK	Open	2	1	8669	github-actions, test-stability
13295451	Various tests failing with: Could not register mbeans javax.management.InstanceAlreadyExistsException	"*Probably it's better to analyse the issue not from this initially reported E2E test, but from some ITCase posted as ""another instance"" in the comment below.*

Watch out, logs are very large and on my machine didn't load in the browser (I had to wget the raw log):

https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6889&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

raw log: https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6889/logs/145

{noformat}
[FAIL] 'TPC-DS end-to-end test (Blink planner)' failed after 80 minutes and 59 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{noformat}

The exception tha probably caused the test to fail:

{noformat}
2020-03-31T18:06:08.7602474Z 2020-03-31 16:47:03,397 HashJoin(joinType=[InnerJoin], where=[(ss_sold_date_sk = d_date_sk)], select=[ss_sold_date_sk, ss_customer_sk, ss_ext_discount_amt, ss_ext_sales_price, ss_ext_wholesale_cost, ss_ex
t_list_price, d_date_sk], isBroadcast=[true], build=[right]) -> Calc(select=[ss_customer_sk, ss_ext_discount_amt, ss_ext_sales_price, ss_ext_wholesale_cost, ss_ext_list_price]) (4/4) ERROR Could not register mbeans javax.management.I
nstanceAlreadyExistsException: org.apache.logging.log4j2:type=791f33ab,component=Loggers,name=
2020-03-31T18:06:08.7604158Z    at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
2020-03-31T18:06:08.7604762Z    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
2020-03-31T18:06:08.7605457Z    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
2020-03-31T18:06:08.7606145Z    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
2020-03-31T18:06:08.7607040Z    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
2020-03-31T18:06:08.7607589Z    at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
2020-03-31T18:06:08.7608064Z    at org.apache.logging.log4j.core.jmx.Server.register(Server.java:393)
2020-03-31T18:06:08.7608523Z    at org.apache.logging.log4j.core.jmx.Server.registerLoggerConfigs(Server.java:362)
2020-03-31T18:06:08.7609052Z    at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:186)
2020-03-31T18:06:08.7609689Z    at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
2020-03-31T18:06:08.7610211Z    at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
2020-03-31T18:06:08.7610726Z    at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-03-31T18:06:08.7611211Z    at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-03-31T18:06:08.7611753Z    at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-03-31T18:06:08.7612279Z    at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-03-31T18:06:08.7612832Z    at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-03-31T18:06:08.7613468Z    at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-03-31T18:06:08.7614156Z    at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-03-31T18:06:08.7614742Z    at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-03-31T18:06:08.7615317Z    at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-03-31T18:06:08.7615872Z    at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-03-31T18:06:08.7616365Z    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-03-31T18:06:08.7616962Z    at LongHashJoinOperator$1634.<clinit>(Unknown Source)
2020-03-31T18:06:08.7617335Z    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-03-31T18:06:08.7617801Z    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-03-31T18:06:08.7618382Z    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-03-31T18:06:08.7618895Z    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-03-31T18:06:08.7619378Z    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
2020-03-31T18:06:08.7619983Z    at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
2020-03-31T18:06:08.7620626Z    at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:61)
2020-03-31T18:06:08.7621220Z    at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:169)
2020-03-31T18:06:08.7621750Z    at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:416)
2020-03-31T18:06:08.7622250Z    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:445)
2020-03-31T18:06:08.7622715Z    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:718)
2020-03-31T18:06:08.7623244Z    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:542)
2020-03-31T18:06:08.7623749Z    at java.lang.Thread.run(Thread.java:748)

{noformat}

"	FLINK	Resolved	1	1	8669	pull-request-available, test-stability
13288860	AZP: Python test fails on jdk11 nightly test (misc profile)	"Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5779&view=logs&j=d5dbfc72-24cf-5a8f-e213-1ae80d4b2df8&t=cb83ed8c-7d59-59ba-b58d-25e43fbaa4b2

{code}
----------------------------- Captured stderr call -----------------------------
Error: A JNI error has occurred, please check your installation and try again
Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/flink/client/python/PythonGatewayServer has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)
___________________ StreamTableWindowTests.test_over_window ____________________

{code}
"	FLINK	Resolved	3	1	8669	pull-request-available
13307735	Test new Flink Docker image	"Test Flink's new Docker image and the corresponding Dockerfile:

* Try to build custom image
* Try to run different Flink processes (Master (session, per-job), TaskManager)
* Try custom configuration and log properties"	FLINK	Closed	2	7	8669	pull-request-available, release-testing
13290270	Improve e2e test failure error reporting	"The purpose of this change is to improve the error reporting for e2e tests:
- The log upload for e2e tests fails if the bash e2e tests fail
- coredumps, dumpstreams etc. are not included into the log upload
- Logs are not scanned for exceptions when exception checking is turned off"	FLINK	Resolved	3	4	8669	pull-request-available
13342573	Add overview / reference architecture documentation page	To properly guide users, we should add some generic overview of the deployment concepts.	FLINK	Closed	3	7	8669	pull-request-available
13343324	Broken links to hadoop.md	"In FLINK-20347 we removed the {{hadoop.md}} page, but there are still links in other pages:
* dev/project-configuration.md
* dev/batch/hadoop-compatibility.md
* connectors/hive/index.md"	FLINK	Closed	2	1	8669	pull-request-available
13324256	Introduce proper IO executor in Dispatcher	"Currently, IO operations in the {{Dispatcher}} are scheduled on the {{rpcService.getExecutor()}}.

We should introduce a separate executor for IO operations."	FLINK	Closed	3	4	8669	pull-request-available
13372167	Harden JobMasterStopWithSavepointITCase	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16451&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=3884


{code:java}
[ERROR] throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase)  Time elapsed: 0.154 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithoutRestartsHelper(JobMasterStopWithSavepointITCase.java:154)
	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(JobMasterStopWithSavepointITCase.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13335617	"Local recovery and sticky scheduling end-to-end test hangs with ""Expected to find info here."""	"The reason for all these e2e test hangs recently seems to be the Local recovery and sticky scheduling end-to-end test.

It is in a restart loop with this error:
{code}
020-10-15T13:01:42.4079891Z 2020-10-15 12:54:06,099 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Unnamed (1/4) (78a56f7797be1d41b0b1b31a75bd90e1_20ba6b65f97481d5570070de90e4e791_0_1) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@65b70d8d.
2020-10-15T13:01:42.4080637Z java.lang.NullPointerException: Expected to find info here.
2020-10-15T13:01:42.4081365Z 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4082067Z 	at org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob$StateCreatingFlatMap.initializeState(StickyAllocationAndLocalRecoveryTestJob.java:343) ~[?:?]
2020-10-15T13:01:42.4083125Z 	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:185) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4103820Z 	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:167) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4104926Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4106020Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:107) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4107084Z 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:262) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4108295Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4109432Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:505) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4110458Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4111428Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4112328Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4113167Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4113962Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4114434Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
{code}"	FLINK	Resolved	1	1	8669	pull-request-available, test-stability
13289286	"Remove ""FAQ"" section from Flink website"	"I propose to remove the [FAQ|https://flink.apache.org/faq.html] page from the Flink website for the following reasons:
- the information on there is not very up to date, nor helpful or extensive (its a small selection of what somebody a few years ago considered frequent questions)
- StackOverflow lists a different set of questions as most frequent: https://stackoverflow.com/questions/tagged/apache-flink?tab=Votes
- The page is only on position 39 in Google Analytics (in the last ~10 months)

I'm happy to hear opinions on this!"	FLINK	Resolved	3	4	8669	pull-request-available
12720090	Add support for POJO objects	"This adds support in the `TypeExtractor` and also one can now use field
expressions for grouping, join, and co-group.

I decided against using ReflectASM for now since it does not work on private fields. We would have to mix ReflectASM and plain java reflection which right now does not seem worth the hassle.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/908
Created by: [aljoscha|https://github.com/aljoscha]
Labels: 
Created at: Thu Jun 05 15:04:50 CEST 2014
State: open
"	FLINK	Resolved	3	2	8669	github-import
12719899	"Forbid catching exceptions only with ""e.printStackTrace()"" using Checkstyle"	"Once https://github.com/stratosphere/stratosphere/issues/596 is merged, I would like to forbid statements like:
```java
} catch (InterruptedException e) {
	e.printStackTrace();
}
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/720
Created by: [rmetzger|https://github.com/rmetzger]
Labels: enhancement, 
Created at: Fri Apr 25 11:28:22 CEST 2014
State: open
"	FLINK	Closed	3	4	8669	github-import
13328411	YARNSessionFIFOITCase.checkForProhibitedLogContents found a log with prohibited string	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6661&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354]

{code}
2020-09-19T22:08:13.5364974Z [ERROR]   YARNSessionFIFOITCase.checkForProhibitedLogContents:83->YarnTestBase.ensureNoProhibitedStringInLogFiles:476 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-1_0/application_1600553154281_0001/container_1600553154281_0001_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13344781	generate-stackbrew-library.sh in flink-docker doesn't properly prune the java11 tag	The output of {generate-stackbrew-library.sh} contains two {java11} tags.	FLINK	Closed	2	1	8669	pull-request-available
12981817	CliFrontendYarnAddressConfigurationTest fails	"The {{CliFrontendYarnAddressConfigurationTest}} failed here:

https://s3.amazonaws.com/archive.travis-ci.org/jobs/139195089/log.txt

https://travis-ci.org/apache/flink/builds/139195083

Another failed run with logs is here: https://travis-ci.org/uce/flink/builds/139262754

"	FLINK	Resolved	3	1	8669	test-stability
12821782	Web interface reports false (the default) jobmanager.rpc.port on YARN	Running Flink as YARN session mode I was completely confused by the web interface reporting a false {{jobmanager.rpc.port}} (the default).	FLINK	Resolved	3	1	8669	web-ui, yarn
13314706	"flink-runtime lists ""org.uncommons.maths:uncommons-maths:1.2.2a"" as a bundled dependency, but it isn't"	"This is the relevant section in the NOTICE file

{code}
This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)

- com.typesafe.akka:akka-remote_2.11:2.5.21
- io.netty:netty:3.10.6.Final
- org.uncommons.maths:uncommons-maths:1.2.2a
{code}.

The uncommons-maths dependency is not declared anywhere, nor is it included in the binary file."	FLINK	Closed	2	1	8669	pull-request-available
13295531	DistinctAggregateITCaseBase.testSingleDistinctAggOnMultiColumnsWithGroupingSets gets stuck	"CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6912&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=294c2388-20e6-57a2-5721-91db544b1e69

Why I believe it is this test:

{code}
2020-04-01T09:46:12.3370955Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f22d800b800 nid=0xe5a waiting on condition [0x00007f22e1b8f000]
2020-04-01T09:46:12.3371365Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3371648Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3372211Z 	- parking to wait for  <0x0000000082d3b7b0> (a java.util.concurrent.CompletableFuture$Signaller)
2020-04-01T09:46:12.3372658Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3373126Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-04-01T09:46:12.3373611Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-04-01T09:46:12.3374068Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-04-01T09:46:12.3374626Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-04-01T09:46:12.3375108Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:653)
2020-04-01T09:46:12.3375650Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77)
2020-04-01T09:46:12.3376163Z 	at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:51)
2020-04-01T09:46:12.3376745Z 	at org.apache.flink.table.planner.utils.TestingTableEnvironment.execute(TableTestBase.scala:1054)
2020-04-01T09:46:12.3377250Z 	at org.apache.flink.table.api.TableUtils.collectToList(TableUtils.java:85)
2020-04-01T09:46:12.3377835Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:288)
2020-04-01T09:46:12.3378381Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:129)
2020-04-01T09:46:12.3378910Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:95)
2020-04-01T09:46:12.3379621Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.testSingleDistinctAggOnMultiColumnsWithGroupingSets(DistinctAggregateITCaseBase.scala:244)
2020-04-01T09:46:12.3380251Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-01T09:46:12.3380721Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-01T09:46:12.3381226Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-01T09:46:12.3381654Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-01T09:46:12.3382099Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-04-01T09:46:12.3382612Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-04-01T09:46:12.3383103Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-04-01T09:46:12.3383706Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-04-01T09:46:12.3384180Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-04-01T09:46:12.3384706Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-04-01T09:46:12.3385168Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-04-01T09:46:12.3385654Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-04-01T09:46:12.3386113Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-04-01T09:46:12.3386561Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-04-01T09:46:12.3386998Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-04-01T09:46:12.3387411Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-04-01T09:46:12.3387842Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-04-01T09:46:12.3388290Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3388724Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3389142Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-04-01T09:46:12.3389513Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-04-01T09:46:12.3389970Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-04-01T09:46:12.3390470Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-04-01T09:46:12.3391052Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-04-01T09:46:12.3391578Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-04-01T09:46:12.3392104Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-04-01T09:46:12.3392665Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-04-01T09:46:12.3393151Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-04-01T09:46:12.3393631Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

Full stacktraces 
{code}
2020-04-01T09:40:58.9129390Z [INFO] Running org.apache.flink.table.planner.catalog.CatalogITCase
2020-04-01T09:40:58.9241845Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in org.apache.flink.table.planner.catalog.CatalogITCase
2020-04-01T09:40:58.9251426Z [INFO] Running org.apache.flink.table.runtime.batch.sql.UnnestITCase
2020-04-01T09:41:00.1602556Z [INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.233 s - in org.apache.flink.table.runtime.batch.sql.UnnestITCase
2020-04-01T09:46:11.2452086Z ==============================================================================
2020-04-01T09:46:11.2454365Z Maven produced no output for 300 seconds.
2020-04-01T09:46:11.2455123Z ==============================================================================
2020-04-01T09:46:11.2472609Z ==============================================================================
2020-04-01T09:46:11.2473050Z The following Java processes are running (JPS)
2020-04-01T09:46:11.2473420Z ==============================================================================
2020-04-01T09:46:11.3898136Z 338 Launcher
2020-04-01T09:46:11.4010823Z 3669 surefirebooter2141596050142400008.jar
2020-04-01T09:46:11.4049294Z 27452 Jps
2020-04-01T09:46:11.5771473Z ==============================================================================
2020-04-01T09:46:11.5772055Z Printing stack trace of Java process 338
2020-04-01T09:46:11.5772468Z ==============================================================================
2020-04-01T09:46:11.9236096Z 2020-04-01 09:46:11
2020-04-01T09:46:11.9238468Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-04-01T09:46:11.9238896Z 
2020-04-01T09:46:11.9239328Z ""Attach Listener"" #81 daemon prio=9 os_prio=0 tid=0x00007f1ff0001000 nid=0x6bc8 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9239781Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9239963Z 
2020-04-01T09:46:11.9240526Z ""Thread-16"" #75 daemon prio=5 os_prio=0 tid=0x00007f1f8800d000 nid=0xe54 runnable [0x00007f2035211000]
2020-04-01T09:46:11.9241169Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9241695Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-04-01T09:46:11.9242092Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-04-01T09:46:11.9242635Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-04-01T09:46:11.9245960Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-04-01T09:46:11.9247627Z 	- locked <0x00000006890f9750> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-04-01T09:46:11.9248394Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-04-01T09:46:11.9249115Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-04-01T09:46:11.9249786Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-04-01T09:46:11.9250597Z 	- locked <0x00000006890fe920> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9251216Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-04-01T09:46:11.9251912Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-04-01T09:46:11.9252604Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-04-01T09:46:11.9253566Z 	- locked <0x00000006890fe920> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9254786Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-04-01T09:46:11.9255679Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-04-01T09:46:11.9256320Z 
2020-04-01T09:46:11.9257446Z ""Thread-15"" #74 daemon prio=5 os_prio=0 tid=0x00007f1f8800b000 nid=0xe53 runnable [0x00007f2035312000]
2020-04-01T09:46:11.9258152Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9258631Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-04-01T09:46:11.9258991Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-04-01T09:46:11.9259554Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-04-01T09:46:11.9260014Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-04-01T09:46:11.9260820Z 	- locked <0x00000006890f7690> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-04-01T09:46:11.9261269Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-04-01T09:46:11.9261685Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-04-01T09:46:11.9262093Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-04-01T09:46:11.9262635Z 	- locked <0x00000006890fbd18> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9263303Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-04-01T09:46:11.9263720Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-04-01T09:46:11.9264115Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-04-01T09:46:11.9264810Z 	- locked <0x00000006890fbd18> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9265195Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-04-01T09:46:11.9265715Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-04-01T09:46:11.9266062Z 
2020-04-01T09:46:11.9266643Z ""Thread-14"" #73 daemon prio=5 os_prio=0 tid=0x00007f1f88002800 nid=0xe52 waiting on condition [0x00007f2035413000]
2020-04-01T09:46:11.9267184Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9267493Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9268051Z 	- parking to wait for  <0x0000000685926988> (a java.util.concurrent.Semaphore$NonfairSync)
2020-04-01T09:46:11.9268525Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9269082Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
2020-04-01T09:46:11.9269743Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
2020-04-01T09:46:11.9270420Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
2020-04-01T09:46:11.9271022Z 	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
2020-04-01T09:46:11.9271616Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestProvidingInputStream.awaitNextTest(TestProvidingInputStream.java:181)
2020-04-01T09:46:11.9272398Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestProvidingInputStream.beforeNextCommand(TestProvidingInputStream.java:144)
2020-04-01T09:46:11.9273124Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.AbstractCommandStream.read(AbstractCommandStream.java:100)
2020-04-01T09:46:11.9273792Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.feed(StreamFeeder.java:123)
2020-04-01T09:46:11.9274414Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.run(StreamFeeder.java:60)
2020-04-01T09:46:11.9274840Z 
2020-04-01T09:46:11.9275223Z ""ThreadedStreamConsumer"" #69 daemon prio=5 os_prio=0 tid=0x00007f1f88009800 nid=0xe4e waiting on condition [0x00007f2035110000]
2020-04-01T09:46:11.9275677Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9275990Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9276634Z 	- parking to wait for  <0x00000006890ccdc8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9277368Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9277914Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9278491Z 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
2020-04-01T09:46:11.9279078Z 	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:83)
2020-04-01T09:46:11.9279565Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9279785Z 
2020-04-01T09:46:11.9280392Z ""surefire-fork-starter"" #68 daemon prio=5 os_prio=0 tid=0x00007f20a6caa000 nid=0xe4d waiting on condition [0x00007f2035a8b000]
2020-04-01T09:46:11.9280913Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9281210Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9281862Z 	- parking to wait for  <0x0000000682797140> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9282362Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9282919Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9283587Z 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
2020-04-01T09:46:11.9284066Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9284653Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9285154Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9285572Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9285771Z 
2020-04-01T09:46:11.9286385Z ""surefire-fork-starter"" #67 daemon prio=5 os_prio=0 tid=0x00007f20a6ca9000 nid=0xe4c in Object.wait() [0x00007f203598a000]
2020-04-01T09:46:11.9286863Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:11.9287279Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:11.9287782Z 	- waiting on <0x00000006890f5418> (a java.lang.UNIXProcess)
2020-04-01T09:46:11.9288125Z 	at java.lang.Object.wait(Object.java:502)
2020-04-01T09:46:11.9288496Z 	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
2020-04-01T09:46:11.9289014Z 	- locked <0x00000006890f5418> (a java.lang.UNIXProcess)
2020-04-01T09:46:11.9289535Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.CommandLineUtils$1.call(CommandLineUtils.java:260)
2020-04-01T09:46:11.9290124Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:614)
2020-04-01T09:46:11.9290730Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-04-01T09:46:11.9291288Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)
2020-04-01T09:46:11.9291820Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)
2020-04-01T09:46:11.9292298Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-04-01T09:46:11.9292741Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:11.9293252Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9293669Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9293872Z 
2020-04-01T09:46:11.9294450Z ""ping-timer-10s"" #65 daemon prio=5 os_prio=0 tid=0x00007f20a6ca8000 nid=0xe4b waiting on condition [0x00007f2037030000]
2020-04-01T09:46:11.9295003Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:11.9295326Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9295956Z 	- parking to wait for  <0x00000006827917a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9296582Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:11.9297218Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:11.9297886Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:11.9298512Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:11.9299062Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9299563Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9300060Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9300479Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9300731Z 
2020-04-01T09:46:11.9301348Z ""timeout-check-timer"" #64 daemon prio=5 os_prio=0 tid=0x00007f20a5e72000 nid=0xe4a waiting on condition [0x00007f2035b8c000]
2020-04-01T09:46:11.9301853Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:11.9302343Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9303147Z 	- parking to wait for  <0x0000000682792088> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9303657Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:11.9304231Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:11.9305077Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:11.9305723Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:11.9306295Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9306789Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9307380Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9307787Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9308000Z 
2020-04-01T09:46:11.9308334Z ""process reaper"" #37 daemon prio=10 os_prio=0 tid=0x00007f1f84006000 nid=0x198 runnable [0x00007f2035687000]
2020-04-01T09:46:11.9308751Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9309068Z 	at java.lang.UNIXProcess.waitForProcessExit(Native Method)
2020-04-01T09:46:11.9309470Z 	at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)
2020-04-01T09:46:11.9309881Z 	at java.lang.UNIXProcess$$Lambda$10/1718188850.run(Unknown Source)
2020-04-01T09:46:11.9310329Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:11.9310899Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9311490Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9311689Z 
2020-04-01T09:46:11.9312309Z ""resolver-5"" #26 daemon prio=5 os_prio=0 tid=0x00007f20a5432800 nid=0x18c waiting on condition [0x00007f2037387000]
2020-04-01T09:46:11.9312754Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9313061Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9313694Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9314210Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9315023Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9315587Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9316082Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9316684Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9317294Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9317703Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9317924Z 
2020-04-01T09:46:11.9318519Z ""resolver-4"" #25 daemon prio=5 os_prio=0 tid=0x00007f20a5431800 nid=0x18b waiting on condition [0x00007f2037488000]
2020-04-01T09:46:11.9318977Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9319285Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9319911Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9320423Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9321026Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9321610Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9322095Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9322598Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9323206Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9323606Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9323823Z 
2020-04-01T09:46:11.9324397Z ""resolver-3"" #24 daemon prio=5 os_prio=0 tid=0x00007f20a5434800 nid=0x18a waiting on condition [0x00007f2037589000]
2020-04-01T09:46:11.9325012Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9325309Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9325964Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9326463Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9327113Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9327688Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9328173Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9328675Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9329170Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9329588Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9329793Z 
2020-04-01T09:46:11.9330375Z ""resolver-2"" #23 daemon prio=5 os_prio=0 tid=0x00007f20a546a800 nid=0x189 waiting on condition [0x00007f203768a000]
2020-04-01T09:46:11.9330878Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9331185Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9331816Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9332330Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9332892Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9333449Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9333944Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9334434Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9335115Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9335537Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9335736Z 
2020-04-01T09:46:11.9336320Z ""resolver-1"" #22 daemon prio=5 os_prio=0 tid=0x00007f20a542e000 nid=0x188 waiting on condition [0x00007f203778b000]
2020-04-01T09:46:11.9336876Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9337272Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9337910Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9338431Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9338974Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9339548Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9340050Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9340539Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9341118Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9341528Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9341739Z 
2020-04-01T09:46:11.9342073Z ""Service Thread"" #20 daemon prio=9 os_prio=0 tid=0x00007f20a4108000 nid=0x186 runnable [0x0000000000000000]
2020-04-01T09:46:11.9342585Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9342768Z 
2020-04-01T09:46:11.9343142Z ""C1 CompilerThread14"" #19 daemon prio=9 os_prio=0 tid=0x00007f20a4105000 nid=0x185 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9343574Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9343772Z 
2020-04-01T09:46:11.9344128Z ""C1 CompilerThread13"" #18 daemon prio=9 os_prio=0 tid=0x00007f20a4103000 nid=0x184 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9344721Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9344932Z 
2020-04-01T09:46:11.9345302Z ""C1 CompilerThread12"" #17 daemon prio=9 os_prio=0 tid=0x00007f20a4101000 nid=0x183 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9345739Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9345936Z 
2020-04-01T09:46:11.9346294Z ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007f20a40fe800 nid=0x182 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9346747Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9347017Z 
2020-04-01T09:46:11.9347375Z ""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007f20a40fd000 nid=0x181 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9347819Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9348001Z 
2020-04-01T09:46:11.9348367Z ""C2 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007f20a40fa800 nid=0x180 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9348806Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9348990Z 
2020-04-01T09:46:11.9349343Z ""C2 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007f20a40f9000 nid=0x17f waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9349781Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9349969Z 
2020-04-01T09:46:11.9350336Z ""C2 CompilerThread7"" #12 daemon prio=9 os_prio=0 tid=0x00007f20a40f7000 nid=0x17e waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9350833Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9351035Z 
2020-04-01T09:46:11.9351386Z ""C2 CompilerThread6"" #11 daemon prio=9 os_prio=0 tid=0x00007f20a40f5000 nid=0x17d waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9351826Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9352009Z 
2020-04-01T09:46:11.9352377Z ""C2 CompilerThread5"" #10 daemon prio=9 os_prio=0 tid=0x00007f20a40f2800 nid=0x17c waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9352800Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9352997Z 
2020-04-01T09:46:11.9353345Z ""C2 CompilerThread4"" #9 daemon prio=9 os_prio=0 tid=0x00007f20a40e8800 nid=0x17b waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9353778Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9354108Z 
2020-04-01T09:46:11.9354476Z ""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007f20a40e6800 nid=0x17a waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9355106Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9355312Z 
2020-04-01T09:46:11.9355661Z ""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007f20a40e4000 nid=0x179 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9356096Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9356278Z 
2020-04-01T09:46:11.9356640Z ""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f20a40e2800 nid=0x178 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9357142Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9357344Z 
2020-04-01T09:46:11.9357691Z ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f20a40df800 nid=0x177 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9358126Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9358308Z 
2020-04-01T09:46:11.9358655Z ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f20a40de000 nid=0x176 runnable [0x0000000000000000]
2020-04-01T09:46:11.9359055Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9359237Z 
2020-04-01T09:46:11.9359581Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f20a40ad800 nid=0x175 in Object.wait() [0x00007f2044d9d000]
2020-04-01T09:46:11.9360129Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:11.9360444Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:11.9361112Z 	- waiting on <0x00000003d9040d08> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:11.9361523Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-04-01T09:46:11.9362111Z 	- locked <0x00000003d9040d08> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:11.9362514Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-04-01T09:46:11.9362952Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-04-01T09:46:11.9363212Z 
2020-04-01T09:46:11.9363573Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f20a40a9000 nid=0x174 in Object.wait() [0x00007f2044e9e000]
2020-04-01T09:46:11.9364046Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:11.9364364Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:11.9365065Z 	- waiting on <0x00000003d9128018> (a java.lang.ref.Reference$Lock)
2020-04-01T09:46:11.9365418Z 	at java.lang.Object.wait(Object.java:502)
2020-04-01T09:46:11.9365803Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-04-01T09:46:11.9366361Z 	- locked <0x00000003d9128018> (a java.lang.ref.Reference$Lock)
2020-04-01T09:46:11.9366778Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-04-01T09:46:11.9367123Z 
2020-04-01T09:46:11.9367461Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f20a400b800 nid=0x15b waiting on condition [0x00007f20abbe2000]
2020-04-01T09:46:11.9367867Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9368182Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9368731Z 	- parking to wait for  <0x0000000685927188> (a java.util.concurrent.FutureTask)
2020-04-01T09:46:11.9369168Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9369622Z 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
2020-04-01T09:46:11.9370035Z 	at java.util.concurrent.FutureTask.get(FutureTask.java:191)
2020-04-01T09:46:11.9370531Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:476)
2020-04-01T09:46:11.9371257Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)
2020-04-01T09:46:11.9371828Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
2020-04-01T09:46:11.9372357Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-04-01T09:46:11.9372907Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-04-01T09:46:11.9373660Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-04-01T09:46:11.9374269Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-04-01T09:46:11.9374944Z 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-04-01T09:46:11.9375490Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-04-01T09:46:11.9375992Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-04-01T09:46:11.9376505Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-04-01T09:46:11.9377126Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-04-01T09:46:11.9377748Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-04-01T09:46:11.9378394Z 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-04-01T09:46:11.9378990Z 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-04-01T09:46:11.9379567Z 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-04-01T09:46:11.9379989Z 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-04-01T09:46:11.9380408Z 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-04-01T09:46:11.9380877Z 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-04-01T09:46:11.9381269Z 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-04-01T09:46:11.9381652Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-01T09:46:11.9382075Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-01T09:46:11.9382592Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-01T09:46:11.9383043Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-01T09:46:11.9383498Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-04-01T09:46:11.9384005Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-04-01T09:46:11.9384500Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-04-01T09:46:11.9385086Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-04-01T09:46:11.9385361Z 
2020-04-01T09:46:11.9385622Z ""VM Thread"" os_prio=0 tid=0x00007f20a409f800 nid=0x173 runnable 
2020-04-01T09:46:11.9385847Z 
2020-04-01T09:46:11.9386131Z ""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f20a4020800 nid=0x15c runnable 
2020-04-01T09:46:11.9386404Z 
2020-04-01T09:46:11.9386689Z ""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f20a4022000 nid=0x15d runnable 
2020-04-01T09:46:11.9387028Z 
2020-04-01T09:46:11.9387310Z ""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x00007f20a4024000 nid=0x15e runnable 
2020-04-01T09:46:11.9387578Z 
2020-04-01T09:46:11.9387857Z ""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x00007f20a4025800 nid=0x15f runnable 
2020-04-01T09:46:11.9388117Z 
2020-04-01T09:46:11.9388409Z ""GC task thread#4 (ParallelGC)"" os_prio=0 tid=0x00007f20a4027800 nid=0x160 runnable 
2020-04-01T09:46:11.9388664Z 
2020-04-01T09:46:11.9388959Z ""GC task thread#5 (ParallelGC)"" os_prio=0 tid=0x00007f20a4029000 nid=0x161 runnable 
2020-04-01T09:46:11.9389214Z 
2020-04-01T09:46:11.9389510Z ""GC task thread#6 (ParallelGC)"" os_prio=0 tid=0x00007f20a402b000 nid=0x162 runnable 
2020-04-01T09:46:11.9389765Z 
2020-04-01T09:46:11.9390056Z ""GC task thread#7 (ParallelGC)"" os_prio=0 tid=0x00007f20a402c800 nid=0x163 runnable 
2020-04-01T09:46:11.9390310Z 
2020-04-01T09:46:11.9390599Z ""GC task thread#8 (ParallelGC)"" os_prio=0 tid=0x00007f20a402e800 nid=0x164 runnable 
2020-04-01T09:46:11.9390996Z 
2020-04-01T09:46:11.9391280Z ""GC task thread#9 (ParallelGC)"" os_prio=0 tid=0x00007f20a4030000 nid=0x165 runnable 
2020-04-01T09:46:11.9391551Z 
2020-04-01T09:46:11.9391836Z ""GC task thread#10 (ParallelGC)"" os_prio=0 tid=0x00007f20a4032000 nid=0x166 runnable 
2020-04-01T09:46:11.9392115Z 
2020-04-01T09:46:11.9392400Z ""GC task thread#11 (ParallelGC)"" os_prio=0 tid=0x00007f20a4033800 nid=0x167 runnable 
2020-04-01T09:46:11.9392671Z 
2020-04-01T09:46:11.9392954Z ""GC task thread#12 (ParallelGC)"" os_prio=0 tid=0x00007f20a4035800 nid=0x168 runnable 
2020-04-01T09:46:11.9393211Z 
2020-04-01T09:46:11.9393507Z ""GC task thread#13 (ParallelGC)"" os_prio=0 tid=0x00007f20a4037000 nid=0x169 runnable 
2020-04-01T09:46:11.9393763Z 
2020-04-01T09:46:11.9394060Z ""GC task thread#14 (ParallelGC)"" os_prio=0 tid=0x00007f20a4039000 nid=0x16a runnable 
2020-04-01T09:46:11.9394314Z 
2020-04-01T09:46:11.9394680Z ""GC task thread#15 (ParallelGC)"" os_prio=0 tid=0x00007f20a403a800 nid=0x16b runnable 
2020-04-01T09:46:11.9394944Z 
2020-04-01T09:46:11.9395237Z ""GC task thread#16 (ParallelGC)"" os_prio=0 tid=0x00007f20a403c800 nid=0x16c runnable 
2020-04-01T09:46:11.9395491Z 
2020-04-01T09:46:11.9395772Z ""GC task thread#17 (ParallelGC)"" os_prio=0 tid=0x00007f20a403e000 nid=0x16d runnable 
2020-04-01T09:46:11.9396157Z 
2020-04-01T09:46:11.9396437Z ""GC task thread#18 (ParallelGC)"" os_prio=0 tid=0x00007f20a4040000 nid=0x16e runnable 
2020-04-01T09:46:11.9396710Z 
2020-04-01T09:46:11.9397053Z ""GC task thread#19 (ParallelGC)"" os_prio=0 tid=0x00007f20a4041800 nid=0x16f runnable 
2020-04-01T09:46:11.9397326Z 
2020-04-01T09:46:11.9397607Z ""GC task thread#20 (ParallelGC)"" os_prio=0 tid=0x00007f20a4043800 nid=0x170 runnable 
2020-04-01T09:46:11.9397861Z 
2020-04-01T09:46:11.9398157Z ""GC task thread#21 (ParallelGC)"" os_prio=0 tid=0x00007f20a4045000 nid=0x171 runnable 
2020-04-01T09:46:11.9398410Z 
2020-04-01T09:46:11.9398709Z ""GC task thread#22 (ParallelGC)"" os_prio=0 tid=0x00007f20a4047000 nid=0x172 runnable 
2020-04-01T09:46:11.9398963Z 
2020-04-01T09:46:11.9399282Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007f20a410a800 nid=0x187 waiting on condition 
2020-04-01T09:46:11.9399553Z 
2020-04-01T09:46:11.9399754Z JNI global references: 238
2020-04-01T09:46:11.9399923Z 
2020-04-01T09:46:11.9432174Z ==============================================================================
2020-04-01T09:46:11.9432645Z Printing stack trace of Java process 3669
2020-04-01T09:46:11.9433013Z ==============================================================================
2020-04-01T09:46:12.2695641Z 2020-04-01 09:46:12
2020-04-01T09:46:12.2696636Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-04-01T09:46:12.2697002Z 
2020-04-01T09:46:12.2697502Z ""Attach Listener"" #43455 daemon prio=9 os_prio=0 tid=0x00007f21cc001000 nid=0x6bf9 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.2697943Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.2698159Z 
2020-04-01T09:46:12.2698944Z ""flink-akka.actor.default-dispatcher-209"" #43454 prio=5 os_prio=0 tid=0x00007f1ff0020800 nid=0x6b31 waiting on condition [0x00007f1f9f9fa000]
2020-04-01T09:46:12.2699565Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2700026Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2701023Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2701633Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-04-01T09:46:12.2702136Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-04-01T09:46:12.2702751Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2703581Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2703919Z 
2020-04-01T09:46:12.2704770Z ""CloseableReaperThread"" #43452 daemon prio=5 os_prio=0 tid=0x00007f1f5c027800 nid=0x6b26 in Object.wait() [0x00007f20984c5000]
2020-04-01T09:46:12.2705509Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:12.2706218Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.2706645Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-04-01T09:46:12.2707523Z 	- locked <0x0000000082d313d0> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:12.2708119Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-04-01T09:46:12.2708915Z 	at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:193)
2020-04-01T09:46:12.2709469Z 
2020-04-01T09:46:12.2711509Z ""Source: Custom Source -> SourceConversion(table=[default_catalog.default_database.SmallTable3], fields=[a, b, c]) -> Expand(projects=[a, b, c, $e], projects=[{a, b, null AS c, 1 AS $e}, {a, null AS b, c, 2 AS $e}]) -> LocalHashAggregate(groupBy=[a, b, c, $e], select=[a, b, c, $e]) (1/1)"" #43451 prio=5 os_prio=0 tid=0x00007f1f7404d800 nid=0x6b25 runnable [0x00007f1f9e4ea000]
2020-04-01T09:46:12.2712603Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.2712955Z 	at java.util.Hashtable.putAll(Hashtable.java:524)
2020-04-01T09:46:12.2713550Z 	- eliminated <0x0000000090fd8ea8> (a java.util.Hashtable)
2020-04-01T09:46:12.2713927Z 	at java.util.Hashtable.<init>(Hashtable.java:226)
2020-04-01T09:46:12.2714659Z 	at javax.management.ObjectName.getKeyPropertyList(ObjectName.java:1624)
2020-04-01T09:46:12.2715190Z 	at com.sun.jmx.mbeanserver.Repository$ObjectNamePattern.matchKeys(Repository.java:179)
2020-04-01T09:46:12.2715703Z 	at com.sun.jmx.mbeanserver.Repository.addAllMatching(Repository.java:233)
2020-04-01T09:46:12.2716317Z 	- locked <0x000000008624e1d8> (a java.util.HashMap)
2020-04-01T09:46:12.2716776Z 	at com.sun.jmx.mbeanserver.Repository.query(Repository.java:569)
2020-04-01T09:46:12.2717334Z 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.queryNamesImpl(DefaultMBeanServerInterceptor.java:562)
2020-04-01T09:46:12.2717990Z 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.queryNames(DefaultMBeanServerInterceptor.java:554)
2020-04-01T09:46:12.2718565Z 	at com.sun.jmx.mbeanserver.JmxMBeanServer.queryNames(JmxMBeanServer.java:619)
2020-04-01T09:46:12.2719083Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-04-01T09:46:12.2719610Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerConfigs(Server.java:301)
2020-04-01T09:46:12.2720344Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:266)
2020-04-01T09:46:12.2721092Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
2020-04-01T09:46:12.2721990Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
2020-04-01T09:46:12.2722859Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
2020-04-01T09:46:12.2723618Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-04-01T09:46:12.2724380Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-04-01T09:46:12.2725053Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-04-01T09:46:12.2725853Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-04-01T09:46:12.2726597Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-04-01T09:46:12.2727125Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-04-01T09:46:12.2727668Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-04-01T09:46:12.2728230Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-04-01T09:46:12.2728777Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-04-01T09:46:12.2729337Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-04-01T09:46:12.2729809Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-04-01T09:46:12.2730367Z 	at LocalHashAggregateWithKeys$74756.<clinit>(Unknown Source)
2020-04-01T09:46:12.2730826Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-04-01T09:46:12.2731349Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-04-01T09:46:12.2731961Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-04-01T09:46:12.2732491Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-04-01T09:46:12.2733010Z 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
2020-04-01T09:46:12.2733747Z 	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
2020-04-01T09:46:12.2734679Z 	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:61)
2020-04-01T09:46:12.2735353Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:473)
2020-04-01T09:46:12.2735961Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:396)
2020-04-01T09:46:12.2737025Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:462)
2020-04-01T09:46:12.2737639Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:396)
2020-04-01T09:46:12.2738260Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:462)
2020-04-01T09:46:12.2738927Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:396)
2020-04-01T09:46:12.2739809Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:157)
2020-04-01T09:46:12.2740469Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:416)
2020-04-01T09:46:12.2741126Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:445)
2020-04-01T09:46:12.2741626Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:718)
2020-04-01T09:46:12.2742092Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:542)
2020-04-01T09:46:12.2742471Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2742682Z 
2020-04-01T09:46:12.2743481Z ""flink-akka.actor.default-dispatcher-208"" #43448 prio=5 os_prio=0 tid=0x00007f1f7404e000 nid=0x6b22 waiting on condition [0x00007f209a9ea000]
2020-04-01T09:46:12.2744087Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2744672Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2745366Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2745868Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2746350Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2746923Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2747224Z 
2020-04-01T09:46:12.2748157Z ""flink-akka.actor.default-dispatcher-207"" #43444 prio=5 os_prio=0 tid=0x00007f1fa801d000 nid=0x6b08 waiting on condition [0x00007f1f9e8e9000]
2020-04-01T09:46:12.2748896Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2749364Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2750203Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2751161Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2751894Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2752413Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2752711Z 
2020-04-01T09:46:12.2753884Z ""flink-akka.actor.default-dispatcher-205"" #43443 prio=5 os_prio=0 tid=0x00007f202801f000 nid=0x6aed waiting on condition [0x00007f21659e2000]
2020-04-01T09:46:12.2754745Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2755184Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2756104Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2756818Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2757570Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2758316Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2758643Z 
2020-04-01T09:46:12.2759629Z ""flink-metrics-akka.remote.default-remote-dispatcher-17"" #43001 prio=1 os_prio=0 tid=0x00007f20f80b4000 nid=0x475e waiting on condition [0x00007f1f9dfe2000]
2020-04-01T09:46:12.2760444Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2761061Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2761967Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2762602Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-04-01T09:46:12.2763451Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-04-01T09:46:12.2764122Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2764984Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2765492Z 
2020-04-01T09:46:12.2766401Z ""mini-cluster-io-thread-32"" #42882 daemon prio=5 os_prio=0 tid=0x00007f1f74001000 nid=0x4115 waiting on condition [0x00007f1f9fcfd000]
2020-04-01T09:46:12.2767213Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2767529Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2768339Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2769112Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2770012Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2771007Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2771740Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2772393Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2773062Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2773697Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2774019Z 
2020-04-01T09:46:12.2775199Z ""mini-cluster-io-thread-31"" #42881 daemon prio=5 os_prio=0 tid=0x00007f1fa8024800 nid=0x4114 waiting on condition [0x00007f1f9fffe000]
2020-04-01T09:46:12.2775881Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2776205Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2777044Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2777585Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2778164Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2778760Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2779262Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2779793Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2780310Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2781010Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2781218Z 
2020-04-01T09:46:12.2781892Z ""mini-cluster-io-thread-30"" #42880 daemon prio=5 os_prio=0 tid=0x00007f1fa8023800 nid=0x4113 waiting on condition [0x00007f209a2e3000]
2020-04-01T09:46:12.2782889Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2783408Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2784459Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2785398Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2786266Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2787148Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2787919Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2788713Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2789431Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2790068Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2790561Z 
2020-04-01T09:46:12.2791727Z ""mini-cluster-io-thread-29"" #42879 daemon prio=5 os_prio=0 tid=0x00007f1fa8022800 nid=0x4112 waiting on condition [0x00007f20993d4000]
2020-04-01T09:46:12.2792457Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2792929Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2793849Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2794782Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2795651Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2796438Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2797268Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2798006Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2798739Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2799315Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2799618Z 
2020-04-01T09:46:12.2800765Z ""mini-cluster-io-thread-28"" #42878 daemon prio=5 os_prio=0 tid=0x00007f1fa8021800 nid=0x4111 waiting on condition [0x00007f2098acd000]
2020-04-01T09:46:12.2801643Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2801970Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2802976Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2803784Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2804713Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2805584Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2806466Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2807091Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2807849Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2808459Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2808790Z 
2020-04-01T09:46:12.2809748Z ""mini-cluster-io-thread-27"" #42877 daemon prio=5 os_prio=0 tid=0x00007f1fa8020800 nid=0x4110 waiting on condition [0x00007f20988cb000]
2020-04-01T09:46:12.2810462Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2811189Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2811854Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2812370Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2812916Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2813487Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2813983Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2814479Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2815081Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2815484Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2815698Z 
2020-04-01T09:46:12.2816324Z ""jobmanager-future-thread-32"" #42867 daemon prio=5 os_prio=0 tid=0x00007f1fa801c000 nid=0x40e1 waiting on condition [0x00007f2098ccf000]
2020-04-01T09:46:12.2816908Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2817215Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2817982Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2818493Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2819080Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2819733Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2820352Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2820975Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2821472Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2821982Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2822407Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2822608Z 
2020-04-01T09:46:12.2823253Z ""jobmanager-future-thread-31"" #42866 daemon prio=5 os_prio=0 tid=0x00007f1fa801b000 nid=0x40e0 waiting on condition [0x00007f20997d8000]
2020-04-01T09:46:12.2823729Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2824035Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2824739Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2825257Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2825797Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2826473Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2827138Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2827698Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2828199Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2828691Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2829107Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2829305Z 
2020-04-01T09:46:12.2829954Z ""jobmanager-future-thread-30"" #42865 daemon prio=5 os_prio=0 tid=0x00007f1fa8014000 nid=0x40df waiting on condition [0x00007f2164bd8000]
2020-04-01T09:46:12.2830425Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2830882Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2831537Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2832037Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2832599Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2833225Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2833856Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2834423Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2834994Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2835510Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2835918Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2836130Z 
2020-04-01T09:46:12.2836807Z ""jobmanager-future-thread-29"" #42861 daemon prio=5 os_prio=0 tid=0x00007f1f0006f800 nid=0x4059 waiting on condition [0x00007f2098dd0000]
2020-04-01T09:46:12.2837408Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2837713Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2838359Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2838883Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2839446Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2840104Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2840789Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2841359Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2841865Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2842362Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2842778Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2842979Z 
2020-04-01T09:46:12.2843617Z ""jobmanager-future-thread-28"" #42860 daemon prio=5 os_prio=0 tid=0x00007f1f00003000 nid=0x4058 waiting on condition [0x00007f20992d3000]
2020-04-01T09:46:12.2844103Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2844431Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2845135Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2845664Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2846240Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2846947Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2847581Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2848127Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2848635Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2849142Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2849544Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2849742Z 
2020-04-01T09:46:12.2850479Z ""jobmanager-future-thread-27"" #42857 daemon prio=5 os_prio=0 tid=0x00007f1f00011000 nid=0x4055 waiting on condition [0x00007f20994d5000]
2020-04-01T09:46:12.2851057Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2851366Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2852015Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2852521Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2853099Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2853752Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2854365Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2855002Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2855499Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2856013Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2856603Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2856805Z 
2020-04-01T09:46:12.2857441Z ""jobmanager-future-thread-26"" #42852 daemon prio=5 os_prio=0 tid=0x00007f1f3c023800 nid=0x4020 waiting on condition [0x00007f20998d9000]
2020-04-01T09:46:12.2857931Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2858242Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2858864Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2859378Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2859918Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2860578Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2861344Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2861897Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2862401Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2862894Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2863308Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2863508Z 
2020-04-01T09:46:12.2864139Z ""mini-cluster-io-thread-26"" #42851 daemon prio=5 os_prio=0 tid=0x00007f1f00014000 nid=0x401f waiting on condition [0x00007f2099adb000]
2020-04-01T09:46:12.2864681Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2864995Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2865623Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2866140Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2866753Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2867316Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2867815Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2868304Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2868810Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2869228Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2869426Z 
2020-04-01T09:46:12.2870149Z ""jobmanager-future-thread-25"" #42850 daemon prio=5 os_prio=0 tid=0x00007f1fa801a000 nid=0x401e waiting on condition [0x00007f2099bdc000]
2020-04-01T09:46:12.2870772Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2871105Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2871737Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2872262Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2872819Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2873475Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2874102Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2874720Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2875233Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2875728Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2876229Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2876444Z 
2020-04-01T09:46:12.2877129Z ""mini-cluster-io-thread-25"" #42849 daemon prio=5 os_prio=0 tid=0x00007f1fa8018800 nid=0x401d waiting on condition [0x00007f2099cdd000]
2020-04-01T09:46:12.2877608Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2877913Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2878548Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2879044Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2879597Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2880161Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2880738Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2881253Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2881751Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2882165Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2882364Z 
2020-04-01T09:46:12.2882997Z ""jobmanager-future-thread-24"" #42848 daemon prio=5 os_prio=0 tid=0x00007f1f3c022800 nid=0x401c waiting on condition [0x00007f2099edf000]
2020-04-01T09:46:12.2883471Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2883781Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2884397Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2885000Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2885555Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2886188Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2886881Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2887432Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2887933Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2888444Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2888847Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2889129Z 
2020-04-01T09:46:12.2889774Z ""mini-cluster-io-thread-24"" #42847 daemon prio=5 os_prio=0 tid=0x00007f1f3c022000 nid=0x401b waiting on condition [0x00007f209b0ef000]
2020-04-01T09:46:12.2890257Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2890553Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2891292Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2891791Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2892346Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2892904Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2893399Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2893901Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2894399Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2894895Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2895094Z 
2020-04-01T09:46:12.2895831Z ""mini-cluster-io-thread-23"" #42846 daemon prio=5 os_prio=0 tid=0x00007f1f3c00b000 nid=0x401a waiting on condition [0x00007f209b3f2000]
2020-04-01T09:46:12.2896301Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2896670Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2897298Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2897812Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2898362Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2898922Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2899421Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2899910Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2900423Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2900922Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2901121Z 
2020-04-01T09:46:12.2901730Z ""mini-cluster-io-thread-22"" #42845 daemon prio=5 os_prio=0 tid=0x00007f1f3c017800 nid=0x4018 waiting on condition [0x00007f209a3e4000]
2020-04-01T09:46:12.2902216Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2902627Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2903225Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2903717Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2904236Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2904866Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2905348Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2905821Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2906313Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2906752Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2906957Z 
2020-04-01T09:46:12.2907558Z ""mini-cluster-io-thread-21"" #42844 daemon prio=5 os_prio=0 tid=0x00007f1f3c019800 nid=0x4017 waiting on condition [0x00007f209a1e2000]
2020-04-01T09:46:12.2908027Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2908309Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2909015Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2909497Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2910033Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2910588Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2911103Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2911586Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2912061Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2912460Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2912652Z 
2020-04-01T09:46:12.2913270Z ""jobmanager-future-thread-23"" #42843 daemon prio=5 os_prio=0 tid=0x00007f1f3c002800 nid=0x4016 waiting on condition [0x00007f2099fe0000]
2020-04-01T09:46:12.2913745Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2914056Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2914734Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2915311Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2915866Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2916532Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2917328Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2917903Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2918383Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2918877Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2919262Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2919478Z 
2020-04-01T09:46:12.2920119Z ""jobmanager-future-thread-22"" #42839 daemon prio=5 os_prio=0 tid=0x00007f1ff000c000 nid=0x4011 waiting on condition [0x00007f209b5f4000]
2020-04-01T09:46:12.2920599Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2920979Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2921603Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2922099Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2922620Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2923249Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2923845Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2924402Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2924970Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2925447Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2925846Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2926039Z 
2020-04-01T09:46:12.2926708Z ""jobmanager-future-thread-21"" #42834 daemon prio=5 os_prio=0 tid=0x00007f1fa8017800 nid=0x3d6e waiting on condition [0x00007f209a5e6000]
2020-04-01T09:46:12.2927170Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2927465Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2928188Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2928686Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2929229Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2929838Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2930444Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2931045Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2931536Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2932021Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2932414Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2932605Z 
2020-04-01T09:46:12.2933222Z ""jobmanager-future-thread-20"" #42833 daemon prio=5 os_prio=0 tid=0x00007f1fa8016800 nid=0x3d6d waiting on condition [0x00007f2164ad7000]
2020-04-01T09:46:12.2933777Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2934062Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2934766Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2935254Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2935783Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2936405Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2937048Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2937597Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2938065Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2938557Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2938943Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2939150Z 
2020-04-01T09:46:12.2939752Z ""jobmanager-future-thread-19"" #42832 daemon prio=5 os_prio=0 tid=0x00007f1fa8015800 nid=0x3d6c waiting on condition [0x00007f209b9f8000]
2020-04-01T09:46:12.2940221Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2940522Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2941192Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2941691Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2942216Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2942833Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2943427Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2943968Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2944454Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2945009Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2945413Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2945604Z 
2020-04-01T09:46:12.2946270Z ""flink-metrics-akka.remote.default-remote-dispatcher-16"" #42829 prio=1 os_prio=0 tid=0x00007f1ee8023000 nid=0x3c9b waiting on condition [0x00007f21668ef000]
2020-04-01T09:46:12.2946902Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2947200Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2947788Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2948263Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2948705Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2949155Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2949447Z 
2020-04-01T09:46:12.2950037Z ""jobmanager-future-thread-18"" #42825 daemon prio=5 os_prio=0 tid=0x00007f1fa8013000 nid=0x3937 waiting on condition [0x00007f209b7f6000]
2020-04-01T09:46:12.2950511Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2950862Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2951476Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2951961Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2952503Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2953218Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2953811Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2954353Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2954904Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2955405Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2955812Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2956007Z 
2020-04-01T09:46:12.2956661Z ""jobmanager-future-thread-17"" #42823 daemon prio=5 os_prio=0 tid=0x00007f1fa8012000 nid=0x3935 waiting on condition [0x00007f209baf9000]
2020-04-01T09:46:12.2957146Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2957462Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2958066Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2958574Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2959109Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2959745Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2960359Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2960950Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2961434Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2961913Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2962311Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2962503Z 
2020-04-01T09:46:12.2963117Z ""jobmanager-future-thread-16"" #42822 daemon prio=5 os_prio=0 tid=0x00007f1fa8011800 nid=0x3934 waiting on condition [0x00007f209bbfa000]
2020-04-01T09:46:12.2963577Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2963877Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2964488Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2965038Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2965659Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2966261Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2966920Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2967459Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2967929Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2968418Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2968804Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2969015Z 
2020-04-01T09:46:12.2969630Z ""jobmanager-future-thread-15"" #42817 daemon prio=5 os_prio=0 tid=0x00007f1fa800f800 nid=0x3910 waiting on condition [0x00007f209bdfc000]
2020-04-01T09:46:12.2970105Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2970391Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2971069Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2971651Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2972172Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2972789Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2973381Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2973926Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2974413Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2974965Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2975371Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2975563Z 
2020-04-01T09:46:12.2976189Z ""mini-cluster-io-thread-20"" #42816 daemon prio=5 os_prio=0 tid=0x00007f1fa800e800 nid=0x390f waiting on condition [0x00007f209befd000]
2020-04-01T09:46:12.2976687Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2976971Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2977573Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2978072Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2978592Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2979143Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2979635Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2980110Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2980655Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2981151Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2981461Z 
2020-04-01T09:46:12.2982183Z ""jobmanager-future-thread-14"" #42815 daemon prio=5 os_prio=0 tid=0x00007f203c014800 nid=0x390e waiting on condition [0x00007f209bffe000]
2020-04-01T09:46:12.2982657Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2982938Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2983553Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2984053Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2984776Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2985407Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2986003Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2986602Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2987095Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2987572Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2987977Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2988171Z 
2020-04-01T09:46:12.2988801Z ""mini-cluster-io-thread-19"" #42814 daemon prio=5 os_prio=0 tid=0x00007f203c00c800 nid=0x390d waiting on condition [0x00007f21643d0000]
2020-04-01T09:46:12.2989260Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2989553Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2990150Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2990842Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2991379Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2991916Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2992391Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2992864Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2993349Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2993752Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2993944Z 
2020-04-01T09:46:12.2994623Z ""jobmanager-future-thread-13"" #42813 daemon prio=5 os_prio=0 tid=0x00007f1f3c020800 nid=0x390c waiting on condition [0x00007f21644d1000]
2020-04-01T09:46:12.2995110Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2995405Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2996012Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2996554Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2997075Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2997695Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2998300Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2998834Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2999315Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2999796Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3000193Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3000385Z 
2020-04-01T09:46:12.3001080Z ""mini-cluster-io-thread-18"" #42812 daemon prio=5 os_prio=0 tid=0x00007f1f3c01b000 nid=0x390b waiting on condition [0x00007f21646d3000]
2020-04-01T09:46:12.3001535Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3001834Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3002432Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3002930Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3003554Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3004092Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3004645Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3005118Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3005608Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3006004Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3006196Z 
2020-04-01T09:46:12.3006849Z ""mini-cluster-io-thread-17"" #42811 daemon prio=5 os_prio=0 tid=0x00007f1f3c005800 nid=0x390a waiting on condition [0x00007f21647d4000]
2020-04-01T09:46:12.3007322Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3007620Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3008226Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3008724Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3009322Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3009872Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3010352Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3010882Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3011373Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3011759Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3011967Z 
2020-04-01T09:46:12.3012567Z ""mini-cluster-io-thread-16"" #42810 daemon prio=5 os_prio=0 tid=0x00007f1fa800e000 nid=0x3909 waiting on condition [0x00007f21667ee000]
2020-04-01T09:46:12.3013043Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3013327Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3014234Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3015093Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3015622Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3016182Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3016710Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3017207Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3017694Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3018091Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3018287Z 
2020-04-01T09:46:12.3018941Z ""mini-cluster-io-thread-15"" #42809 daemon prio=5 os_prio=0 tid=0x00007f1fa8001000 nid=0x3908 waiting on condition [0x00007f2165be4000]
2020-04-01T09:46:12.3019423Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3019709Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3020329Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3020889Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3021424Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3021963Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3022441Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3023075Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3023555Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3023965Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3024157Z 
2020-04-01T09:46:12.3024961Z ""jobmanager-future-thread-12"" #42808 daemon prio=5 os_prio=0 tid=0x00007f1fa8004800 nid=0x3907 waiting on condition [0x00007f209b4f3000]
2020-04-01T09:46:12.3025430Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3025726Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3026343Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3026898Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3027437Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3028049Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3028657Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3029311Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3029806Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3030300Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3030771Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3030980Z 
2020-04-01T09:46:12.3031592Z ""jobmanager-future-thread-11"" #42804 daemon prio=5 os_prio=0 tid=0x00007f2004018800 nid=0x3903 waiting on condition [0x00007f2164edb000]
2020-04-01T09:46:12.3032062Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3032345Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3032963Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3033444Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3033981Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3034736Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3035393Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3035934Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3036404Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3036952Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3037359Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3037551Z 
2020-04-01T09:46:12.3038171Z ""jobmanager-future-thread-10"" #42797 daemon prio=5 os_prio=0 tid=0x00007f203c00a800 nid=0x38d0 waiting on condition [0x00007f2164cd9000]
2020-04-01T09:46:12.3038662Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3038971Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3039570Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3040072Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3040676Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3041307Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3041913Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3042558Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3043049Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3043537Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3043937Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3044130Z 
2020-04-01T09:46:12.3044961Z ""jobmanager-future-thread-9"" #42796 daemon prio=5 os_prio=0 tid=0x00007f203c004800 nid=0x38cf waiting on condition [0x00007f20989cc000]
2020-04-01T09:46:12.3045426Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3045723Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3046356Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3046900Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3047435Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3048040Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3048755Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3049295Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3049768Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3050258Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3050707Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3050916Z 
2020-04-01T09:46:12.3051522Z ""jobmanager-future-thread-8"" #42795 daemon prio=5 os_prio=0 tid=0x00007f203c002800 nid=0x38ce waiting on condition [0x00007f21665ec000]
2020-04-01T09:46:12.3052006Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3052302Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3052918Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3053423Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3053967Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3054740Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3055336Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3055882Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3056372Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3056908Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3057312Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3057504Z 
2020-04-01T09:46:12.3058135Z ""jobmanager-future-thread-7"" #42791 daemon prio=5 os_prio=0 tid=0x00007f1fe401e800 nid=0x387d waiting on condition [0x00007f2165ae3000]
2020-04-01T09:46:12.3058605Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3058914Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3059509Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3060010Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3060547Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3061327Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3061934Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3062468Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3062958Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3063434Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3063841Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3064033Z 
2020-04-01T09:46:12.3064805Z ""jobmanager-future-thread-6"" #42789 daemon prio=5 os_prio=0 tid=0x00007f1fe400d000 nid=0x387b waiting on condition [0x00007f21662eb000]
2020-04-01T09:46:12.3065301Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3065602Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3066229Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3066771Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3067416Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3068030Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3068636Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3069179Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3069654Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3070155Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3070546Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3070814Z 
2020-04-01T09:46:12.3071424Z ""jobmanager-future-thread-5"" #42787 daemon prio=5 os_prio=0 tid=0x00007f1fe400c000 nid=0x3879 waiting on condition [0x00007f21642cf000]
2020-04-01T09:46:12.3071898Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3072179Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3072787Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3073284Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3073803Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3074422Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3075202Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3075756Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3076240Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3076772Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3077175Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3077367Z 
2020-04-01T09:46:12.3078052Z ""flink-metrics-akka.remote.default-remote-dispatcher-15"" #42784 prio=1 os_prio=0 tid=0x00007f1ee8022800 nid=0x385b waiting on condition [0x00007f21885fd000]
2020-04-01T09:46:12.3078540Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3078838Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3079410Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3079983Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3080423Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3080939Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3081236Z 
2020-04-01T09:46:12.3081844Z ""Flink-MetricRegistry-thread-1"" #42780 daemon prio=5 os_prio=0 tid=0x00007f1ebc003000 nid=0x3854 waiting on condition [0x00007f2166ff2000]
2020-04-01T09:46:12.3082341Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3082633Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3083244Z 	- parking to wait for  <0x0000000087ec9df0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3083737Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3084292Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3085702Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3086296Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3087000Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3087472Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3087962Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3088363Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3088553Z 
2020-04-01T09:46:12.3089175Z ""jobmanager-future-thread-4"" #42779 daemon prio=5 os_prio=0 tid=0x00007f1fe400a800 nid=0x3853 waiting on condition [0x00007f21672f5000]
2020-04-01T09:46:12.3089645Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3089951Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3090547Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3091099Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3091627Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3092247Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3092849Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3093380Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3093870Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3094346Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3094836Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3095028Z 
2020-04-01T09:46:12.3095642Z ""mini-cluster-io-thread-14"" #42778 daemon prio=5 os_prio=0 tid=0x00007f1fbc007800 nid=0x3852 waiting on condition [0x00007f21678fb000]
2020-04-01T09:46:12.3096101Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3096407Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3097082Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3097563Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3098100Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3098636Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3099111Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3099686Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3100164Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3100570Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3100834Z 
2020-04-01T09:46:12.3101416Z ""pool-121-thread-1"" #42777 prio=5 os_prio=0 tid=0x00007f1fbc006800 nid=0x3851 waiting on condition [0x00007f2167dfe000]
2020-04-01T09:46:12.3101863Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3102175Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3102765Z 	- parking to wait for  <0x00000000880850b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3103263Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3103816Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3104436Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3105136Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3105749Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3106238Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3106786Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3107176Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3107367Z 
2020-04-01T09:46:12.3107992Z ""jobmanager-future-thread-3"" #42776 daemon prio=5 os_prio=0 tid=0x00007f1fbc003000 nid=0x3850 waiting on condition [0x00007f21881fb000]
2020-04-01T09:46:12.3108478Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3108778Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3109391Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3109885Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3110444Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3111140Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3111747Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3112288Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3112760Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3113248Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3113641Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3113849Z 
2020-04-01T09:46:12.3114443Z ""mini-cluster-io-thread-13"" #42775 daemon prio=5 os_prio=0 tid=0x00007f1fbc001000 nid=0x384f waiting on condition [0x00007f21d0a85000]
2020-04-01T09:46:12.3115005Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3115302Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3115907Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3116399Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3116974Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3117525Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3117987Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3118563Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3119056Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3119445Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3119654Z 
2020-04-01T09:46:12.3120262Z ""jobmanager-future-thread-2"" #42774 daemon prio=5 os_prio=0 tid=0x00007f1fe400a000 nid=0x384e waiting on condition [0x00007f21677fa000]
2020-04-01T09:46:12.3120834Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3121129Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3121748Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3122237Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3122795Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3123427Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3124017Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3124741Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3125213Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3125709Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3126110Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3126304Z 
2020-04-01T09:46:12.3126967Z ""mini-cluster-io-thread-12"" #42773 daemon prio=5 os_prio=0 tid=0x00007f1fe4009000 nid=0x384d waiting on condition [0x00007f1ee598b000]
2020-04-01T09:46:12.3127438Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3127739Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3128338Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3128830Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3129354Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3129903Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3130381Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3130903Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3131394Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3131780Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3131983Z 
2020-04-01T09:46:12.3132579Z ""mini-cluster-io-thread-11"" #42772 daemon prio=5 os_prio=0 tid=0x00007f1fe4008800 nid=0x384c waiting on condition [0x00007f209a4e5000]
2020-04-01T09:46:12.3133044Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3133329Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3133943Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3134440Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3135031Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3135584Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3136046Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3136580Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3137144Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3137549Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3137740Z 
2020-04-01T09:46:12.3138363Z ""mini-cluster-io-thread-10"" #42771 daemon prio=5 os_prio=0 tid=0x00007f2084026800 nid=0x384b waiting on condition [0x00007f209b2f1000]
2020-04-01T09:46:12.3138835Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3139122Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3139741Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3140221Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3140811Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3141347Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3141830Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3142319Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3142793Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3143273Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3143467Z 
2020-04-01T09:46:12.3144077Z ""mini-cluster-io-thread-9"" #42770 daemon prio=5 os_prio=0 tid=0x00007f2084003000 nid=0x384a waiting on condition [0x00007f1f9d4db000]
2020-04-01T09:46:12.3144606Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3144910Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3145515Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3146008Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3146596Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3147137Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3147612Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3148086Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3148573Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3148977Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3149168Z 
2020-04-01T09:46:12.3149765Z ""jobmanager-future-thread-1"" #42769 daemon prio=5 os_prio=0 tid=0x00007f2084002000 nid=0x3849 waiting on condition [0x00007f209a0e1000]
2020-04-01T09:46:12.3150249Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3150556Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3151215Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3151726Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3152263Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3152897Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3153504Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3154033Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3154593Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3155073Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3155475Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3155750Z 
2020-04-01T09:46:12.3156370Z ""mini-cluster-io-thread-8"" #42768 daemon prio=5 os_prio=0 tid=0x00007f1fe4007800 nid=0x3848 waiting on condition [0x00007f1f9ecf5000]
2020-04-01T09:46:12.3156868Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3157173Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3157777Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3158270Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3158808Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3159343Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3159820Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3160289Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3160844Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3161243Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3161510Z 
2020-04-01T09:46:12.3162109Z ""mini-cluster-io-thread-7"" #42767 daemon prio=5 os_prio=0 tid=0x00007f1fe4007000 nid=0x3847 waiting on condition [0x00007f21d0dc7000]
2020-04-01T09:46:12.3162576Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3162875Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3163472Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3163966Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3164487Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3165111Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3165593Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3166062Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3166607Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3166994Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3167201Z 
2020-04-01T09:46:12.3167796Z ""mini-cluster-io-thread-6"" #42766 daemon prio=5 os_prio=0 tid=0x00007f1fe4006000 nid=0x3846 waiting on condition [0x00007f209abec000]
2020-04-01T09:46:12.3168262Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3168546Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3169162Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3169658Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3170184Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3170793Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3171261Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3171745Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3172221Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3172620Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3172812Z 
2020-04-01T09:46:12.3173417Z ""mini-cluster-io-thread-5"" #42765 daemon prio=5 os_prio=0 tid=0x00007f1e87fed000 nid=0x3845 waiting on condition [0x00007f1f9d8df000]
2020-04-01T09:46:12.3173879Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3174164Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3174954Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3175438Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3175976Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3176558Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3177039Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3177526Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3178004Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3178405Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3178596Z 
2020-04-01T09:46:12.3179198Z ""mini-cluster-io-thread-4"" #42764 daemon prio=5 os_prio=0 tid=0x00007f1fd4001800 nid=0x3844 waiting on condition [0x00007f209bcfb000]
2020-04-01T09:46:12.3179655Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3179956Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3180772Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3181267Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3181806Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3182343Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3182825Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3183295Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3183786Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3184188Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3184380Z 
2020-04-01T09:46:12.3185045Z ""mini-cluster-io-thread-3"" #42763 daemon prio=5 os_prio=0 tid=0x00007f1ebc002000 nid=0x3843 waiting on condition [0x00007f209b1f0000]
2020-04-01T09:46:12.3185518Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3185813Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3186416Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3186955Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3187476Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3188025Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3188505Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3188978Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3189473Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3189863Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3190071Z 
2020-04-01T09:46:12.3190724Z ""mini-cluster-io-thread-2"" #42762 daemon prio=5 os_prio=0 tid=0x00007f1e87fec800 nid=0x3842 waiting on condition [0x00007f1f9eff6000]
2020-04-01T09:46:12.3191193Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3191477Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3192095Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3192574Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3193108Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3193747Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3194210Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3194773Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3195259Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3195660Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3195852Z 
2020-04-01T09:46:12.3196509Z ""mini-cluster-io-thread-1"" #42761 daemon prio=5 os_prio=0 tid=0x00007f211ee98000 nid=0x3841 waiting on condition [0x00007f209aeed000]
2020-04-01T09:46:12.3196967Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3197267Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3197888Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3198377Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3198912Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3199545Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3200024Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3200510Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3201053Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3201458Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3201651Z 
2020-04-01T09:46:12.3202276Z ""flink-rest-server-netty-boss-thread-1"" #42760 daemon prio=5 os_prio=0 tid=0x00007f211ee97000 nid=0x3840 runnable [0x00007f209b8f7000]
2020-04-01T09:46:12.3202718Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3203038Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3203404Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3203830Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3204244Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3204990Z 	- locked <0x0000000087e522c0> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2020-04-01T09:46:12.3205617Z 	- locked <0x0000000087e522b0> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3206142Z 	- locked <0x0000000087e522d8> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3206568Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3207099Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
2020-04-01T09:46:12.3207732Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)
2020-04-01T09:46:12.3208284Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)
2020-04-01T09:46:12.3208882Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-04-01T09:46:12.3209523Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-04-01T09:46:12.3209960Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3210163Z 
2020-04-01T09:46:12.3210526Z ""IOManager reader thread #1"" #42755 daemon prio=5 os_prio=0 tid=0x00007f22d8354800 nid=0x383f waiting on condition [0x00007f20987ca000]
2020-04-01T09:46:12.3211056Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3211340Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3211962Z 	- parking to wait for  <0x0000000088072b30> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3212555Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3213073Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3213630Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3214137Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)
2020-04-01T09:46:12.3214472Z 
2020-04-01T09:46:12.3214930Z ""IOManager writer thread #1"" #42754 daemon prio=5 os_prio=0 tid=0x00007f22d8354000 nid=0x383e waiting on condition [0x00007f2165ce5000]
2020-04-01T09:46:12.3215400Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3215683Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3216321Z 	- parking to wait for  <0x0000000087e37990> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3216876Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3217395Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3218033Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3218542Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)
2020-04-01T09:46:12.3218877Z 
2020-04-01T09:46:12.3219429Z ""Timer-830"" #42752 daemon prio=5 os_prio=0 tid=0x00007f22d87eb800 nid=0x383d in Object.wait() [0x00007f20991d2000]
2020-04-01T09:46:12.3219888Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-04-01T09:46:12.3220201Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3220520Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-04-01T09:46:12.3221081Z 	- locked <0x0000000087e45ba8> (a java.util.TaskQueue)
2020-04-01T09:46:12.3221406Z 	at java.util.TimerThread.run(Timer.java:505)
2020-04-01T09:46:12.3221610Z 
2020-04-01T09:46:12.3222159Z ""Timer-829"" #42750 daemon prio=5 os_prio=0 tid=0x00007f22d87eb000 nid=0x383c in Object.wait() [0x00007f2098ed1000]
2020-04-01T09:46:12.3222618Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-04-01T09:46:12.3222935Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3223256Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-04-01T09:46:12.3223736Z 	- locked <0x0000000087e45b68> (a java.util.TaskQueue)
2020-04-01T09:46:12.3224072Z 	at java.util.TimerThread.run(Timer.java:505)
2020-04-01T09:46:12.3224270Z 
2020-04-01T09:46:12.3224710Z ""BLOB Server listener at 41242"" #42746 daemon prio=5 os_prio=0 tid=0x00007f22d94a6000 nid=0x383b runnable [0x00007f2165fea000]
2020-04-01T09:46:12.3225136Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3225457Z 	at java.net.PlainSocketImpl.socketAccept(Native Method)
2020-04-01T09:46:12.3225846Z 	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
2020-04-01T09:46:12.3226286Z 	at java.net.ServerSocket.implAccept(ServerSocket.java:560)
2020-04-01T09:46:12.3226731Z 	at java.net.ServerSocket.accept(ServerSocket.java:528)
2020-04-01T09:46:12.3227121Z 	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:254)
2020-04-01T09:46:12.3227387Z 
2020-04-01T09:46:12.3227940Z ""Timer-828"" #42747 daemon prio=5 os_prio=0 tid=0x00007f22d94a5800 nid=0x383a in Object.wait() [0x00007f1f9dbe0000]
2020-04-01T09:46:12.3228403Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-04-01T09:46:12.3228718Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3229037Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-04-01T09:46:12.3229515Z 	- locked <0x0000000087e45b90> (a java.util.TaskQueue)
2020-04-01T09:46:12.3229851Z 	at java.util.TimerThread.run(Timer.java:505)
2020-04-01T09:46:12.3230049Z 
2020-04-01T09:46:12.3230397Z ""New I/O server boss #408"" #42745 prio=1 os_prio=0 tid=0x00007f1ee8020000 nid=0x3839 runnable [0x00007f1f9dce1000]
2020-04-01T09:46:12.3230941Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3231258Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3231639Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3232052Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3232480Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3233000Z 	- locked <0x0000000087e3b248> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3233523Z 	- locked <0x0000000087e3b238> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3234044Z 	- locked <0x0000000087e3b258> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3234417Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3234874Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-04-01T09:46:12.3235360Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
2020-04-01T09:46:12.3235991Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3236648Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
2020-04-01T09:46:12.3237309Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3237907Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3238452Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3238949Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3239334Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3239540Z 
2020-04-01T09:46:12.3239864Z ""New I/O worker #407"" #42744 prio=1 os_prio=0 tid=0x00007f1ee801f800 nid=0x3838 runnable [0x00007f21653de000]
2020-04-01T09:46:12.3240269Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3240569Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3241002Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3241415Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3241842Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3242393Z 	- locked <0x0000000087e59310> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3242906Z 	- locked <0x0000000087e59300> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3243444Z 	- locked <0x0000000087e59320> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3243803Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3244293Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3244979Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3245638Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3246279Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3246914Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3247507Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3248128Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3248672Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3249165Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3249637Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3249846Z 
2020-04-01T09:46:12.3250170Z ""New I/O worker #406"" #42743 prio=1 os_prio=0 tid=0x00007f1ee800d000 nid=0x3837 runnable [0x00007f21882fc000]
2020-04-01T09:46:12.3250577Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3250932Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3251314Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3251720Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3252147Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3252701Z 	- locked <0x0000000087e335a0> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3253214Z 	- locked <0x0000000087e33590> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3253753Z 	- locked <0x0000000087e335b0> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3254118Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3254684Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3255318Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3256056Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3256744Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3257324Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3257909Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3258534Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3259084Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3259580Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3259968Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3260179Z 
2020-04-01T09:46:12.3260498Z ""New I/O boss #405"" #42742 prio=1 os_prio=0 tid=0x00007f1ee800c800 nid=0x3836 runnable [0x00007f21641ce000]
2020-04-01T09:46:12.3260957Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3261259Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3261637Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3262047Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3262465Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3263016Z 	- locked <0x000000008808f1d0> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3263537Z 	- locked <0x000000008808f1c0> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3264077Z 	- locked <0x000000008805a500> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3264434Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3265000Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3265638Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3266276Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3266942Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
2020-04-01T09:46:12.3267532Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3268250Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3268807Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3269287Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3269687Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3269879Z 
2020-04-01T09:46:12.3270213Z ""New I/O worker #404"" #42740 prio=1 os_prio=0 tid=0x00007f1ee8087000 nid=0x3835 runnable [0x00007f21666ed000]
2020-04-01T09:46:12.3270601Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3270991Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3271355Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3271775Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3272183Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3272739Z 	- locked <0x0000000087e38058> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3273273Z 	- locked <0x0000000087e38048> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3273794Z 	- locked <0x0000000087e38068> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3274240Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3274788Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3275419Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3276077Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3276748Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3277347Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3277921Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3278543Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3279106Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3279580Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3279982Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3280174Z 
2020-04-01T09:46:12.3280509Z ""New I/O worker #403"" #42739 prio=1 os_prio=0 tid=0x00007f1ee8001000 nid=0x3834 runnable [0x00007f2099dde000]
2020-04-01T09:46:12.3280959Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3281279Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3281643Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3282066Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3282474Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3283031Z 	- locked <0x0000000087e595d8> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3283554Z 	- locked <0x0000000087e595c8> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3284075Z 	- locked <0x0000000087e595e8> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3284448Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3284995Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3285624Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3286276Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3287041Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3287641Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3288218Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3288839Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3289393Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3289871Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3290270Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3290462Z 
2020-04-01T09:46:12.3291185Z ""flink-metrics-akka.remote.default-remote-dispatcher-6"" #42737 prio=1 os_prio=0 tid=0x00007f20b8010000 nid=0x3833 waiting on condition [0x00007f21645d2000]
2020-04-01T09:46:12.3291681Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3291978Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3292631Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3293102Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3293544Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3293997Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3294283Z 
2020-04-01T09:46:12.3294998Z ""flink-metrics-akka.remote.default-remote-dispatcher-7"" #42738 prio=1 os_prio=0 tid=0x00007f2110002000 nid=0x3832 waiting on condition [0x00007f209a6e7000]
2020-04-01T09:46:12.3295508Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3295794Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3296388Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3296898Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3297341Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3297806Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3298082Z 
2020-04-01T09:46:12.3298726Z ""flink-metrics-akka.remote.default-remote-dispatcher-5"" #42736 prio=1 os_prio=0 tid=0x00007f1f28038000 nid=0x3831 waiting on condition [0x00007f209aaeb000]
2020-04-01T09:46:12.3299228Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3299523Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3300092Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3300567Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3301064Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3301534Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3301815Z 
2020-04-01T09:46:12.3302472Z ""flink-metrics-akka.remote.default-remote-dispatcher-4"" #42735 prio=1 os_prio=0 tid=0x00007f20b8063800 nid=0x3830 waiting on condition [0x00007f2098bce000]
2020-04-01T09:46:12.3302953Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3303247Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3303829Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3304286Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3304789Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3305240Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3305625Z 
2020-04-01T09:46:12.3306273Z ""flink-metrics-akka.remote.default-remote-dispatcher-3"" #42734 prio=1 os_prio=0 tid=0x00007f20b8065800 nid=0x382f waiting on condition [0x00007f209afee000]
2020-04-01T09:46:12.3306832Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3307115Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3307717Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3308191Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3308615Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3309079Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3309354Z 
2020-04-01T09:46:12.3309908Z ""flink-metrics-2"" #42733 prio=1 os_prio=0 tid=0x00007f22d94ab000 nid=0x382e waiting on condition [0x00007f20999da000]
2020-04-01T09:46:12.3310351Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3310716Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3311319Z 	- parking to wait for  <0x0000000087e42ae0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3311910Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3312465Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3313015Z 	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
2020-04-01T09:46:12.3313495Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-04-01T09:46:12.3313965Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3314800Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3315214Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3315408Z 
2020-04-01T09:46:12.3315997Z ""flink-metrics-scheduler-1"" #42732 prio=5 os_prio=0 tid=0x00007f22d94aa800 nid=0x382d waiting on condition [0x00007f2164dda000]
2020-04-01T09:46:12.3316530Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-04-01T09:46:12.3316848Z 	at java.lang.Thread.sleep(Native Method)
2020-04-01T09:46:12.3317237Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-04-01T09:46:12.3317780Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-04-01T09:46:12.3318307Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-04-01T09:46:12.3318737Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3318928Z 
2020-04-01T09:46:12.3319473Z ""flink-scheduler-1"" #42727 prio=5 os_prio=0 tid=0x00007f1e8bffc800 nid=0x3828 sleeping[0x00007f209b6f5000]
2020-04-01T09:46:12.3319894Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-04-01T09:46:12.3320214Z 	at java.lang.Thread.sleep(Native Method)
2020-04-01T09:46:12.3320600Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-04-01T09:46:12.3321244Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-04-01T09:46:12.3321790Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-04-01T09:46:12.3322196Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3322405Z 
2020-04-01T09:46:12.3323040Z ""FlinkCompletableFutureDelayScheduler-thread-1"" #72 daemon prio=5 os_prio=0 tid=0x00007f2168030000 nid=0xf55 waiting on condition [0x00007f21656df000]
2020-04-01T09:46:12.3323542Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3323823Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3324436Z 	- parking to wait for  <0x0000000088bb0640> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3325133Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3325672Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3326303Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-04-01T09:46:12.3326950Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3327498Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3327970Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3328466Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3328869Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3329061Z 
2020-04-01T09:46:12.3329406Z ""process reaper"" #24 daemon prio=10 os_prio=0 tid=0x00007f2178040000 nid=0xefd waiting on condition [0x00007f21d1e05000]
2020-04-01T09:46:12.3329864Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3330173Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3330883Z 	- parking to wait for  <0x00000000807233f8> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-04-01T09:46:12.3331363Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3331850Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-04-01T09:46:12.3332389Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-04-01T09:46:12.3332885Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-04-01T09:46:12.3333339Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-04-01T09:46:12.3333828Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3334314Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3334795Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3334991Z 
2020-04-01T09:46:12.3335611Z ""surefire-forkedjvm-ping-30s"" #23 daemon prio=5 os_prio=0 tid=0x00007f22d83cb800 nid=0xef7 waiting on condition [0x00007f21d26b6000]
2020-04-01T09:46:12.3336075Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3336384Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3337034Z 	- parking to wait for  <0x0000000080733098> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3337544Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3338100Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3338721Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3339331Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3339885Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3340354Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3340908Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3341315Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3341507Z 
2020-04-01T09:46:12.3342104Z ""surefire-forkedjvm-command-thread"" #22 daemon prio=5 os_prio=0 tid=0x00007f22d83b4000 nid=0xef4 runnable [0x00007f21d29c1000]
2020-04-01T09:46:12.3342529Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3342843Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-04-01T09:46:12.3343284Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-04-01T09:46:12.3343697Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-04-01T09:46:12.3344111Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-04-01T09:46:12.3344760Z 	- locked <0x0000000080725688> (a java.io.BufferedInputStream)
2020-04-01T09:46:12.3345149Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-04-01T09:46:12.3345606Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-04-01T09:46:12.3346149Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-04-01T09:46:12.3346607Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3346816Z 
2020-04-01T09:46:12.3347134Z ""Service Thread"" #21 daemon prio=9 os_prio=0 tid=0x00007f22d82cb800 nid=0xef0 runnable [0x0000000000000000]
2020-04-01T09:46:12.3347532Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3347708Z 
2020-04-01T09:46:12.3348080Z ""C1 CompilerThread14"" #20 daemon prio=9 os_prio=0 tid=0x00007f22d82c8000 nid=0xeed waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3348497Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3348692Z 
2020-04-01T09:46:12.3349115Z ""C1 CompilerThread13"" #19 daemon prio=9 os_prio=0 tid=0x00007f22d82c6000 nid=0xeeb waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3349548Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3349726Z 
2020-04-01T09:46:12.3350094Z ""C1 CompilerThread12"" #18 daemon prio=9 os_prio=0 tid=0x00007f22d82c4800 nid=0xeea waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3350507Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3350735Z 
2020-04-01T09:46:12.3351095Z ""C1 CompilerThread11"" #17 daemon prio=9 os_prio=0 tid=0x00007f22d82c2000 nid=0xee8 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3351529Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3351705Z 
2020-04-01T09:46:12.3352047Z ""C1 CompilerThread10"" #16 daemon prio=9 os_prio=0 tid=0x00007f22d82c0800 nid=0xee6 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3352482Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3352658Z 
2020-04-01T09:46:12.3353017Z ""C2 CompilerThread9"" #15 daemon prio=9 os_prio=0 tid=0x00007f22d82bd800 nid=0xee4 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3353449Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3353624Z 
2020-04-01T09:46:12.3353964Z ""C2 CompilerThread8"" #14 daemon prio=9 os_prio=0 tid=0x00007f22d82bb800 nid=0xee2 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3354391Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3354637Z 
2020-04-01T09:46:12.3354988Z ""C2 CompilerThread7"" #13 daemon prio=9 os_prio=0 tid=0x00007f22d82b9800 nid=0xee1 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3355398Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3355585Z 
2020-04-01T09:46:12.3355923Z ""C2 CompilerThread6"" #12 daemon prio=9 os_prio=0 tid=0x00007f22d82b7800 nid=0xedf waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3356350Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3356572Z 
2020-04-01T09:46:12.3356930Z ""C2 CompilerThread5"" #11 daemon prio=9 os_prio=0 tid=0x00007f22d82b5000 nid=0xedc waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3357340Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3357535Z 
2020-04-01T09:46:12.3357877Z ""C2 CompilerThread4"" #10 daemon prio=9 os_prio=0 tid=0x00007f22d82b3000 nid=0xedb waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3358303Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3358480Z 
2020-04-01T09:46:12.3358837Z ""C2 CompilerThread3"" #9 daemon prio=9 os_prio=0 tid=0x00007f22d82a9000 nid=0xed9 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3359243Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3359433Z 
2020-04-01T09:46:12.3359770Z ""C2 CompilerThread2"" #8 daemon prio=9 os_prio=0 tid=0x00007f22d82a6800 nid=0xed7 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3360272Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3360447Z 
2020-04-01T09:46:12.3360852Z ""C2 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f22d82a4800 nid=0xed5 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3361262Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3361451Z 
2020-04-01T09:46:12.3361788Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f22d82a2800 nid=0xed3 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3362211Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3362387Z 
2020-04-01T09:46:12.3362720Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f22d82a0800 nid=0xed1 runnable [0x0000000000000000]
2020-04-01T09:46:12.3363107Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3363282Z 
2020-04-01T09:46:12.3363673Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f22d829f000 nid=0xecf waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3364143Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3364319Z 
2020-04-01T09:46:12.3364696Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f22d826e800 nid=0xecd in Object.wait() [0x00007f21ec3a1000]
2020-04-01T09:46:12.3365217Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:12.3365534Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3365868Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-04-01T09:46:12.3366525Z 	- locked <0x00000000807456f8> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:12.3366918Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-04-01T09:46:12.3367340Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-04-01T09:46:12.3367587Z 
2020-04-01T09:46:12.3367941Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f22d826a000 nid=0xeca in Object.wait() [0x00007f21ec4a2000]
2020-04-01T09:46:12.3368384Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:12.3368708Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3368991Z 	at java.lang.Object.wait(Object.java:502)
2020-04-01T09:46:12.3369360Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-04-01T09:46:12.3369927Z 	- locked <0x0000000080725b88> (a java.lang.ref.Reference$Lock)
2020-04-01T09:46:12.3370315Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-04-01T09:46:12.3370586Z 
2020-04-01T09:46:12.3370955Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f22d800b800 nid=0xe5a waiting on condition [0x00007f22e1b8f000]
2020-04-01T09:46:12.3371365Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3371648Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3372211Z 	- parking to wait for  <0x0000000082d3b7b0> (a java.util.concurrent.CompletableFuture$Signaller)
2020-04-01T09:46:12.3372658Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3373126Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-04-01T09:46:12.3373611Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-04-01T09:46:12.3374068Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-04-01T09:46:12.3374626Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-04-01T09:46:12.3375108Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:653)
2020-04-01T09:46:12.3375650Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77)
2020-04-01T09:46:12.3376163Z 	at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:51)
2020-04-01T09:46:12.3376745Z 	at org.apache.flink.table.planner.utils.TestingTableEnvironment.execute(TableTestBase.scala:1054)
2020-04-01T09:46:12.3377250Z 	at org.apache.flink.table.api.TableUtils.collectToList(TableUtils.java:85)
2020-04-01T09:46:12.3377835Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:288)
2020-04-01T09:46:12.3378381Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:129)
2020-04-01T09:46:12.3378910Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:95)
2020-04-01T09:46:12.3379621Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.testSingleDistinctAggOnMultiColumnsWithGroupingSets(DistinctAggregateITCaseBase.scala:244)
2020-04-01T09:46:12.3380251Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-01T09:46:12.3380721Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-01T09:46:12.3381226Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-01T09:46:12.3381654Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-01T09:46:12.3382099Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-04-01T09:46:12.3382612Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-04-01T09:46:12.3383103Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-04-01T09:46:12.3383706Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-04-01T09:46:12.3384180Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-04-01T09:46:12.3384706Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-04-01T09:46:12.3385168Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-04-01T09:46:12.3385654Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-04-01T09:46:12.3386113Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-04-01T09:46:12.3386561Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-04-01T09:46:12.3386998Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-04-01T09:46:12.3387411Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-04-01T09:46:12.3387842Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-04-01T09:46:12.3388290Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3388724Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3389142Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-04-01T09:46:12.3389513Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-04-01T09:46:12.3389970Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-04-01T09:46:12.3390470Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-04-01T09:46:12.3391052Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-04-01T09:46:12.3391578Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-04-01T09:46:12.3392104Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-04-01T09:46:12.3392665Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-04-01T09:46:12.3393151Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-04-01T09:46:12.3393631Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-04-01T09:46:12.3393897Z 
2020-04-01T09:46:12.3394154Z ""VM Thread"" os_prio=0 tid=0x00007f22d8260800 nid=0xec9 runnable 
2020-04-01T09:46:12.3394371Z 
2020-04-01T09:46:12.3394742Z ""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8021800 nid=0xe5c runnable 
2020-04-01T09:46:12.3395007Z 
2020-04-01T09:46:12.3395310Z ""Gang worker#1 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8023000 nid=0xe5f runnable 
2020-04-01T09:46:12.3395656Z 
2020-04-01T09:46:12.3395955Z ""Gang worker#2 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8025000 nid=0xe60 runnable 
2020-04-01T09:46:12.3396210Z 
2020-04-01T09:46:12.3396528Z ""Gang worker#3 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8026800 nid=0xe62 runnable 
2020-04-01T09:46:12.3396805Z 
2020-04-01T09:46:12.3397082Z ""Gang worker#4 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8028800 nid=0xe64 runnable 
2020-04-01T09:46:12.3397358Z 
2020-04-01T09:46:12.3397634Z ""Gang worker#5 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802a000 nid=0xe66 runnable 
2020-04-01T09:46:12.3397903Z 
2020-04-01T09:46:12.3398181Z ""Gang worker#6 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802c000 nid=0xe68 runnable 
2020-04-01T09:46:12.3398438Z 
2020-04-01T09:46:12.3398746Z ""Gang worker#7 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802d800 nid=0xe6a runnable 
2020-04-01T09:46:12.3398999Z 
2020-04-01T09:46:12.3399297Z ""Gang worker#8 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802f800 nid=0xe6c runnable 
2020-04-01T09:46:12.3399554Z 
2020-04-01T09:46:12.3399851Z ""Gang worker#9 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8031000 nid=0xe6e runnable 
2020-04-01T09:46:12.3400104Z 
2020-04-01T09:46:12.3400404Z ""Gang worker#10 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8033000 nid=0xe70 runnable 
2020-04-01T09:46:12.3400802Z 
2020-04-01T09:46:12.3401086Z ""Gang worker#11 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8034800 nid=0xe72 runnable 
2020-04-01T09:46:12.3401369Z 
2020-04-01T09:46:12.3401647Z ""Gang worker#12 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8036800 nid=0xe74 runnable 
2020-04-01T09:46:12.3401921Z 
2020-04-01T09:46:12.3402203Z ""Gang worker#13 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8038000 nid=0xe76 runnable 
2020-04-01T09:46:12.3402486Z 
2020-04-01T09:46:12.3402768Z ""Gang worker#14 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803a000 nid=0xe78 runnable 
2020-04-01T09:46:12.3403037Z 
2020-04-01T09:46:12.3403321Z ""Gang worker#15 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803b800 nid=0xe7a runnable 
2020-04-01T09:46:12.3403582Z 
2020-04-01T09:46:12.3403881Z ""Gang worker#16 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803d800 nid=0xe7c runnable 
2020-04-01T09:46:12.3404139Z 
2020-04-01T09:46:12.3404445Z ""Gang worker#17 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803f000 nid=0xe7e runnable 
2020-04-01T09:46:12.3404769Z 
2020-04-01T09:46:12.3405072Z ""Gang worker#18 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8041000 nid=0xe80 runnable 
2020-04-01T09:46:12.3405329Z 
2020-04-01T09:46:12.3405624Z ""Gang worker#19 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8042800 nid=0xe82 runnable 
2020-04-01T09:46:12.3405879Z 
2020-04-01T09:46:12.3406158Z ""Gang worker#20 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8044800 nid=0xe83 runnable 
2020-04-01T09:46:12.3406451Z 
2020-04-01T09:46:12.3406754Z ""Gang worker#21 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8046000 nid=0xe85 runnable 
2020-04-01T09:46:12.3407029Z 
2020-04-01T09:46:12.3407312Z ""Gang worker#22 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8047800 nid=0xe87 runnable 
2020-04-01T09:46:12.3407584Z 
2020-04-01T09:46:12.3407862Z ""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007f22d8092000 nid=0xeba runnable 
2020-04-01T09:46:12.3408120Z 
2020-04-01T09:46:12.3408433Z ""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8093800 nid=0xebc runnable 
2020-04-01T09:46:12.3408705Z 
2020-04-01T09:46:12.3409011Z ""Gang worker#1 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8095800 nid=0xebe runnable 
2020-04-01T09:46:12.3409283Z 
2020-04-01T09:46:12.3409600Z ""Gang worker#2 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8097000 nid=0xec0 runnable 
2020-04-01T09:46:12.3409868Z 
2020-04-01T09:46:12.3410182Z ""Gang worker#3 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8099000 nid=0xec2 runnable 
2020-04-01T09:46:12.3410449Z 
2020-04-01T09:46:12.3410804Z ""Gang worker#4 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d809a800 nid=0xec4 runnable 
2020-04-01T09:46:12.3411152Z 
2020-04-01T09:46:12.3411449Z ""Gang worker#5 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d809c800 nid=0xec6 runnable 
2020-04-01T09:46:12.3411733Z 
2020-04-01T09:46:12.3412015Z ""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007f22d8074000 nid=0xeb8 runnable 
2020-04-01T09:46:12.3412291Z 
2020-04-01T09:46:12.3412571Z ""G1 Concurrent Refinement Thread#1"" os_prio=0 tid=0x00007f22d8072000 nid=0xeb6 runnable 
2020-04-01T09:46:12.3412849Z 
2020-04-01T09:46:12.3413131Z ""G1 Concurrent Refinement Thread#2"" os_prio=0 tid=0x00007f22d8070800 nid=0xeb4 runnable 
2020-04-01T09:46:12.3413386Z 
2020-04-01T09:46:12.3413685Z ""G1 Concurrent Refinement Thread#3"" os_prio=0 tid=0x00007f22d806e800 nid=0xeb2 runnable 
2020-04-01T09:46:12.3413937Z 
2020-04-01T09:46:12.3414234Z ""G1 Concurrent Refinement Thread#4"" os_prio=0 tid=0x00007f22d806d000 nid=0xeb0 runnable 
2020-04-01T09:46:12.3414486Z 
2020-04-01T09:46:12.3414842Z ""G1 Concurrent Refinement Thread#5"" os_prio=0 tid=0x00007f22d806b000 nid=0xeae runnable 
2020-04-01T09:46:12.3415102Z 
2020-04-01T09:46:12.3415404Z ""G1 Concurrent Refinement Thread#6"" os_prio=0 tid=0x00007f22d8069800 nid=0xeab runnable 
2020-04-01T09:46:12.3415656Z 
2020-04-01T09:46:12.3415953Z ""G1 Concurrent Refinement Thread#7"" os_prio=0 tid=0x00007f22d8067800 nid=0xeaa runnable 
2020-04-01T09:46:12.3416278Z 
2020-04-01T09:46:12.3416610Z ""G1 Concurrent Refinement Thread#8"" os_prio=0 tid=0x00007f22d8065800 nid=0xea8 runnable 
2020-04-01T09:46:12.3416884Z 
2020-04-01T09:46:12.3417161Z ""G1 Concurrent Refinement Thread#9"" os_prio=0 tid=0x00007f22d8064000 nid=0xea6 runnable 
2020-04-01T09:46:12.3417437Z 
2020-04-01T09:46:12.3417718Z ""G1 Concurrent Refinement Thread#10"" os_prio=0 tid=0x00007f22d8062000 nid=0xea4 runnable 
2020-04-01T09:46:12.3417993Z 
2020-04-01T09:46:12.3418274Z ""G1 Concurrent Refinement Thread#11"" os_prio=0 tid=0x00007f22d8060800 nid=0xea2 runnable 
2020-04-01T09:46:12.3418530Z 
2020-04-01T09:46:12.3418828Z ""G1 Concurrent Refinement Thread#12"" os_prio=0 tid=0x00007f22d805e800 nid=0xea0 runnable 
2020-04-01T09:46:12.3419088Z 
2020-04-01T09:46:12.3419380Z ""G1 Concurrent Refinement Thread#13"" os_prio=0 tid=0x00007f22d805d000 nid=0xe9e runnable 
2020-04-01T09:46:12.3419636Z 
2020-04-01T09:46:12.3419936Z ""G1 Concurrent Refinement Thread#14"" os_prio=0 tid=0x00007f22d805b000 nid=0xe9c runnable 
2020-04-01T09:46:12.3420190Z 
2020-04-01T09:46:12.3420484Z ""G1 Concurrent Refinement Thread#15"" os_prio=0 tid=0x00007f22d8059800 nid=0xe9a runnable 
2020-04-01T09:46:12.3420790Z 
2020-04-01T09:46:12.3421088Z ""G1 Concurrent Refinement Thread#16"" os_prio=0 tid=0x00007f22d8057800 nid=0xe98 runnable 
2020-04-01T09:46:12.3421343Z 
2020-04-01T09:46:12.3421622Z ""G1 Concurrent Refinement Thread#17"" os_prio=0 tid=0x00007f22d8056000 nid=0xe96 runnable 
2020-04-01T09:46:12.3421895Z 
2020-04-01T09:46:12.3422173Z ""G1 Concurrent Refinement Thread#18"" os_prio=0 tid=0x00007f22d8054000 nid=0xe94 runnable 
2020-04-01T09:46:12.3422442Z 
2020-04-01T09:46:12.3422722Z ""G1 Concurrent Refinement Thread#19"" os_prio=0 tid=0x00007f22d8052800 nid=0xe91 runnable 
2020-04-01T09:46:12.3422993Z 
2020-04-01T09:46:12.3423271Z ""G1 Concurrent Refinement Thread#20"" os_prio=0 tid=0x00007f22d8050800 nid=0xe90 runnable 
2020-04-01T09:46:12.3423525Z 
2020-04-01T09:46:12.3423817Z ""G1 Concurrent Refinement Thread#21"" os_prio=0 tid=0x00007f22d804e800 nid=0xe8e runnable 
2020-04-01T09:46:12.3424069Z 
2020-04-01T09:46:12.3424368Z ""G1 Concurrent Refinement Thread#22"" os_prio=0 tid=0x00007f22d804d000 nid=0xe8c runnable 
2020-04-01T09:46:12.3424683Z 
2020-04-01T09:46:12.3424980Z ""G1 Concurrent Refinement Thread#23"" os_prio=0 tid=0x00007f22d804b000 nid=0xe8a runnable 
2020-04-01T09:46:12.3425234Z 
2020-04-01T09:46:12.3425532Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007f22d82ce000 nid=0xef2 waiting on condition 
2020-04-01T09:46:12.3425792Z 
2020-04-01T09:46:12.3425974Z JNI global references: 2864
2020-04-01T09:46:12.3426156Z 
2020-04-01T09:46:12.3426414Z ==============================================================================
2020-04-01T09:46:12.3426865Z Printing stack trace of Java process 27499
2020-04-01T09:46:12.3427199Z ==============================================================================
2020-04-01T09:46:12.3894127Z 27499: No such process
2020-04-01T09:46:12.3968549Z ./tools/travis_watchdog.sh: line 243:   337 Terminated              $cmd
{code}"	FLINK	Closed	2	1	8669	test-stability
13302513	"Quickstarts Java nightly end-to-end test fails with ""class file has wrong version 55.0, should be 52.0"""	"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=540&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334

{code}

[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
java.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory)
	at java.util.zip.ZipFile.open(Native Method)
	at java.util.zip.ZipFile.<init>(ZipFile.java:230)
	at java.util.zip.ZipFile.<init>(ZipFile.java:160)
	at java.util.zip.ZipFile.<init>(ZipFile.java:131)
	at sun.tools.jar.Main.list(Main.java:1115)
	at sun.tools.jar.Main.run(Main.java:293)
	at sun.tools.jar.Main.main(Main.java:1288)
Success: There are no flink core classes are contained in the jar.
Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. 
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.

[FAIL] 'Quickstarts Java nightly end-to-end test' failed after 0 minutes and 6 seconds! Test exited with exit code 1


{code}
"	FLINK	Resolved	3	1	8669	pull-request-available
13323534	Forward JobStatus.INITIALIZING timestamp to ExecutionGraph	"This is a follow up to FLINK-16866.

Currently, in the ExecutionGraph, the timestamp for JobStatus.INITIALIZING is not set (defaulting to 0), leading to an inconsistent stateTimestamps array.

To resolve this ticket, one needs to forward the timestamp from the Dispatcher (where the initialization is started) to the ExecutionGraph. "	FLINK	Closed	3	4	8669	pull-request-available
13300458	"MVN exited with EXIT CODE: 143. in ""libraries"" test job"	"CI:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=89&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=32b25b6b-f46f-5bca-b5eb-2c6936ee77a4

maven reports ""build success"", but the exit code is 143?

{code}
[INFO] flink-state-processor-api .......................... SUCCESS [  0.273 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 19:24 min
[INFO] Finished at: 2020-04-23T00:46:43+00:00
[INFO] Final Memory: 246M/4214M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile ""e2e-hadoop"" could not be activated because it does not exist.
MVN exited with EXIT CODE: 143.
Trying to KILL watchdog (265).
==============================================================================

{code}"	FLINK	Resolved	2	1	8669	test-stability
13294350	"Run Kubernetes test failed with invalid named ""minikube"""	"This is the test run [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6702&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

Log output
{code:java}
2020-03-27T00:07:38.9666021Z Running 'Run Kubernetes test'
2020-03-27T00:07:38.9666656Z ==============================================================================
2020-03-27T00:07:38.9677101Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-38967103614
2020-03-27T00:07:41.7529865Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-27T00:07:41.7721475Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-27T00:07:41.8208394Z Docker version 19.03.8, build afacb8b7f0
2020-03-27T00:07:42.4793914Z docker-compose version 1.25.4, build 8d51620a
2020-03-27T00:07:42.5359301Z Installing minikube ...
2020-03-27T00:07:42.5494076Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-03-27T00:07:42.5494729Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-03-27T00:07:42.5498136Z 
2020-03-27T00:07:42.6214887Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-03-27T00:07:43.3467750Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-03-27T00:07:43.3469636Z 100 52.0M  100 52.0M    0     0  65.2M      0 --:--:-- --:--:-- --:--:-- 65.2M
2020-03-27T00:07:43.4262625Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.4264438Z   - To fix this, run: minikube start
2020-03-27T00:07:43.4282404Z Starting minikube ...
2020-03-27T00:07:43.7749694Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:43.7761742Z * Using the none driver based on user configuration
2020-03-27T00:07:43.7762229Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:43.8202161Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.8203353Z   - To fix this, run: minikube start
2020-03-27T00:07:43.8568899Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.8570685Z   - To fix this, run: minikube start
2020-03-27T00:07:43.8583793Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:48.9017252Z * There is no local cluster named ""minikube""
2020-03-27T00:07:48.9019347Z   - To fix this, run: minikube start
2020-03-27T00:07:48.9031515Z Starting minikube ...
2020-03-27T00:07:49.0612601Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:49.0616688Z * Using the none driver based on user configuration
2020-03-27T00:07:49.0620173Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:49.1040676Z * There is no local cluster named ""minikube""
2020-03-27T00:07:49.1042353Z   - To fix this, run: minikube start
2020-03-27T00:07:49.1453522Z * There is no local cluster named ""minikube""
2020-03-27T00:07:49.1454594Z   - To fix this, run: minikube start
2020-03-27T00:07:49.1468436Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:54.1907713Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.1909876Z   - To fix this, run: minikube start
2020-03-27T00:07:54.1921479Z Starting minikube ...
2020-03-27T00:07:54.3388738Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:54.3395499Z * Using the none driver based on user configuration
2020-03-27T00:07:54.3396443Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:54.3824399Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.3837652Z   - To fix this, run: minikube start
2020-03-27T00:07:54.4203902Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.4204895Z   - To fix this, run: minikube start
2020-03-27T00:07:54.4217866Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:59.4235917Z Command: start_kubernetes_if_not_running failed 3 times.
2020-03-27T00:07:59.4236459Z Could not start minikube. Aborting...
2020-03-27T00:07:59.8439850Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.8939088Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.9515679Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.9528463Z Stopping minikube ...
2020-03-27T00:07:59.9921558Z * There is no local cluster named ""minikube""
2020-03-27T00:07:59.9922957Z   - To fix this, run: minikube start
2020-03-27T00:07:59.9943342Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:05.0475257Z * There is no local cluster named ""minikube""
2020-03-27T00:08:05.0476544Z   - To fix this, run: minikube start
2020-03-27T00:08:05.0498749Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:10.1843339Z * There is no local cluster named ""minikube""
2020-03-27T00:08:10.1846448Z   - To fix this, run: minikube start
2020-03-27T00:08:10.1890972Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:15.1900926Z Command: sudo minikube stop failed 3 times.
2020-03-27T00:08:15.1906577Z Could not stop minikube. Aborting...
2020-03-27T00:08:15.1907434Z [FAIL] Test script contains errors.
2020-03-27T00:08:15.1915373Z Checking for errors...
2020-03-27T00:08:15.2133082Z No errors in log files.
2020-03-27T00:08:15.2133514Z Checking for exceptions...
2020-03-27T00:08:15.2390795Z No exceptions in log files.
2020-03-27T00:08:15.2392029Z Checking for non-empty .out files...
2020-03-27T00:08:15.2412061Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
2020-03-27T00:08:15.2415311Z No non-empty .out files.
2020-03-27T00:08:15.2415821Z 
2020-03-27T00:08:15.2416806Z [FAIL] 'Run Kubernetes test' failed after 0 minutes and 34 seconds! Test exited with exit code 1
2020-03-27T00:08:15.2417355Z 
2020-03-27T00:08:15.2454057Z cp: cannot stat '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*': No such file or directory
2020-03-27T00:08:15.2459692Z Published e2e logs into debug logs artifact:
2020-03-27T00:08:15.2489410Z COMPRESSING build artifacts.
2020-03-27T00:08:15.2519389Z tar: Removing leading `/' from member names
2020-03-27T00:08:15.2528098Z /home/vsts/work/1/s/flink-end-to-end-tests/artifacts/
2020-03-27T00:08:15.2529353Z /home/vsts/work/1/s/flink-end-to-end-tests/artifacts/e2e-flink-logs/
2020-03-27T00:08:15.5819526Z No taskexecutor daemon to stop on host fv-az678.
2020-03-27T00:08:15.8011570Z No standalonesession daemon to stop on host fv-az678.
2020-03-27T00:08:16.2280113Z 
2020-03-27T00:08:16.2409524Z ##[error]Bash exited with code '1'.
2020-03-27T00:08:16.2456126Z ##[section]Finishing: Run e2e tests{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13370384	Test native Kubernetes pod template	"Flink allows users to define the JobManager and TaskManager pods via template files. This allows to support advanced features(e.g. init-container, sidecar container, volume mount, etc.) that are not supported by Flink Kubernetes config options directly. Use {{kubernetes.pod-template-file}} to specify a local file that contains the pod definition. It will be used to initialize the JobManager and TaskManager.

 

The documentation about how to start a session/application cluster with pod template could be found here[1].

 

[1]. https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/#pod-template"	FLINK	Closed	1	4	8669	release-testing
13292119	Streaming bucketing end-to-end test output hash mismatch	"https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6298/logs/722

Some of the output mismatch failures were reported in another ticket: https://issues.apache.org/jira/browse/FLINK-16227

{code}
2020-03-17T02:04:19.9176915Z Number of produced values 30618/60000
2020-03-17T02:04:19.9202731Z Truncating buckets
2020-03-17T02:04:25.0504959Z Truncating buckets
2020-03-17T02:04:30.1731295Z Truncating buckets
2020-03-17T02:04:35.3190114Z Truncating buckets
2020-03-17T02:04:40.4723887Z Truncating buckets
2020-03-17T02:04:45.5984655Z Truncating buckets
2020-03-17T02:04:50.7185356Z Truncating buckets
2020-03-17T02:04:55.8627129Z Truncating buckets
2020-03-17T02:05:01.0715985Z Number of produced values 74008/60000
2020-03-17T02:05:02.3976850Z Cancelling job dba2fdb79579158295db27d0214fc2ff.
2020-03-17T02:05:03.4633541Z Cancelled job dba2fdb79579158295db27d0214fc2ff.
2020-03-17T02:05:03.4738270Z Waiting for job (dba2fdb79579158295db27d0214fc2ff) to reach terminal state CANCELED ...
2020-03-17T02:05:03.5149228Z Job (dba2fdb79579158295db27d0214fc2ff) reached terminal state CANCELED
2020-03-17T02:05:03.5150587Z Job dba2fdb79579158295db27d0214fc2ff was cancelled, time to verify
2020-03-17T02:05:03.5590118Z FAIL Bucketing Sink: Output hash mismatch.  Got c3787e7a52d913675e620837a7531742, expected 01aba5ff77a0ef5e5cf6a727c248bdc3.
2020-03-17T02:05:03.5591888Z head hexdump of actual:
2020-03-17T02:05:03.5989908Z 0000000   (   7   ,   1   0   ,   0   ,   S   o   m   e       p   a   y
2020-03-17T02:05:03.5991252Z 0000010   l   o   a   d   .   .   .   )  \n   (   7   ,   1   0   ,   1
2020-03-17T02:05:03.5991923Z 0000020   ,   S   o   m   e       p   a   y   l   o   a   d   .   .   .
2020-03-17T02:05:03.5993055Z 0000030   )  \n   (   7   ,   1   0   ,   2   ,   S   o   m   e       p
2020-03-17T02:05:03.5993690Z 0000040   a   y   l   o   a   d   .   .   .   )  \n   (   7   ,   1   0
2020-03-17T02:05:03.5994332Z 0000050   ,   3   ,   S   o   m   e       p   a   y   l   o   a   d   .
2020-03-17T02:05:03.5994967Z 0000060   .   .   )  \n   (   7   ,   1   0   ,   4   ,   S   o   m   e
2020-03-17T02:05:03.5995744Z 0000070       p   a   y   l   o   a   d   .   .   .   )  \n   (   7   ,
2020-03-17T02:05:03.5996359Z 0000080   1   0   ,   5   ,   S   o   m   e       p   a   y   l   o   a
2020-03-17T02:05:03.5997133Z 0000090   d   .   .   .   )  \n   (   7   ,   1   0   ,   6   ,   S   o
2020-03-17T02:05:03.5997704Z 00000a0   m   e       p   a   y   l   o   a   d   .   .   .   )  \n   (
2020-03-17T02:05:03.5998295Z 00000b0   7   ,   1   0   ,   7   ,   S   o   m   e       p   a   y   l
2020-03-17T02:05:03.5999087Z 00000c0   o   a   d   .   .   .   )  \n   (   7   ,   1   0   ,   8   ,
2020-03-17T02:05:03.6000243Z 00000d0   S   o   m   e       p   a   y   l   o   a   d   .   .   .   )
2020-03-17T02:05:03.6000880Z 00000e0  \n   (   7   ,   1   0   ,   9   ,   S   o   m   e       p   a
2020-03-17T02:05:03.6001494Z 00000f0   y   l   o   a   d   .   .   .   )  \n                        
2020-03-17T02:05:03.6001999Z 00000fa
2020-03-17T02:05:03.9875220Z Stopping taskexecutor daemon (pid: 49278) on host fv-az668.
2020-03-17T02:05:04.2569285Z Stopping standalonesession daemon (pid: 46323) on host fv-az668.
2020-03-17T02:05:04.7664418Z Stopping taskexecutor daemon (pid: 46615) on host fv-az668.
2020-03-17T02:05:04.7674722Z Skipping taskexecutor daemon (pid: 47009), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7687383Z Skipping taskexecutor daemon (pid: 47299), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7689091Z Skipping taskexecutor daemon (pid: 47619), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7690289Z Stopping taskexecutor daemon (pid: 48538) on host fv-az668.
2020-03-17T02:05:04.7691796Z Stopping taskexecutor daemon (pid: 48988) on host fv-az668.
2020-03-17T02:05:04.7692365Z [FAIL] Test script contains errors.
2020-03-17T02:05:04.7713750Z Checking of logs skipped.
2020-03-17T02:05:04.7714249Z 
2020-03-17T02:05:04.7715316Z [FAIL] 'Streaming bucketing end-to-end test' failed after 2 minutes and 43 seconds! Test exited with exit code 1
{code}"	FLINK	Resolved	1	1	8669	pull-request-available, test-stability
13342842	Incorrect use of yarn-session.sh -id vs -yid in log statements.	"The Yarn per-job modes log about the recommended shutdown of yarn, which doesn't work.


See: https://github.com/apache/flink/pull/10964#issuecomment-734166399"	FLINK	Closed	2	1	8669	pull-request-available
13343810	Add check to LicenseChecker for top level /LICENSE files in shaded jars	"During the release verification of the 1.12.0 release, we noticed several modules containing LICENSE files in the jar file, which are not Apache licenses.
This could mislead users that the JARs are licensed not according to the ASL, but something else."	FLINK	Closed	2	4	8669	pull-request-available
13375399	Document how to use the reactive mode on K8s	We should extend the existing standalone K8s documentation https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes to contain an example for the reactive mode (including the resource definitions).	FLINK	Closed	2	4	8669	pull-request-available
13357707	Introduce stopping with savepoint state	The declarative scheduler is also affected by the problem described in FLINK-21030. We want to solve this problem by introducing a separate state when are taking a savepoint for stopping Flink.	FLINK	Closed	3	7	8669	pull-request-available
13308774	Fail Maven setup on AZP if download fails	Setup maven task is green even though the install was not a success: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2481&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=7f98ac96-cfb0-5c1a-969b-c2a0e48a2291	FLINK	Closed	3	1	8669	pull-request-available
13381697	Move our Azure pipelines away from Ubuntu 16.04 by September	"Azure won't support Ubuntu 16.04 starting from October, hence we need to migrate to a newer ubuntu version.

We should do this at a time when the builds are relatively stable to be able to clearly identify issues relating to the version upgrade. Also, we shouldn't do this before a feature freeze ;) "	FLINK	Closed	2	1	8669	pull-request-available
13198355	Avro Confluent Schema Registry E2E test failed on Travis	"https://travis-ci.org/zentol/flink/jobs/454943551

{code}
Waiting for schema registry...
[2018-11-14 12:20:59,394] ERROR Server died unexpectedly:  (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain:51)
org.apache.kafka.common.config.ConfigException: No supported Kafka endpoints are configured. Either kafkastore.bootstrap.servers must have at least one endpoint matching kafkastore.security.protocol or broker endpoints loaded from ZooKeeper must have at least one endpoint matching kafkastore.security.protocol.
	at io.confluent.kafka.schemaregistry.storage.KafkaStore.endpointsToBootstrapServers(KafkaStore.java:313)
	at io.confluent.kafka.schemaregistry.storage.KafkaStore.<init>(KafkaStore.java:130)
	at io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry.<init>(KafkaSchemaRegistry.java:144)
	at io.confluent.kafka.schemaregistry.rest.SchemaRegistryRestApplication.setupResources(SchemaRegistryRestApplication.java:53)
	at io.confluent.kafka.schemaregistry.rest.SchemaRegistryRestApplication.setupResources(SchemaRegistryRestApplication.java:37)
	at io.confluent.rest.Application.createServer(Application.java:149)
	at io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain.main(SchemaRegistryMain.java:43)
{code}"	FLINK	Resolved	2	1	8669	pull-request-available, test-stability
13282600	Set up nightly cron jobs on Azure Pipelines build	"FLINK-13978 introduced support for Azure Pipelines, however limited to building pull requests and pushes.

The scope of this issue is to add the cron jobs available in travis also to the Azure setup."	FLINK	Resolved	3	4	8669	pull-request-available
13353801	Document how to use the reactive mode	"We need to document how our users can use the reactive mode. I propose to create a dedicated ""Execution mode"" page under ""Deployment"" describing the active and reactive mode and how to activate the reactive mode."	FLINK	Closed	3	7	8669	pull-request-available
13329854	"Several tests for HBase connector 1.4 failed with ""NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7042&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51

{code}
2020-09-28T21:28:29.4171075Z Running org.apache.flink.connector.hbase1.HBaseTablePlanTest
2020-09-28T21:28:31.0367584Z Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.62 sec <<< FAILURE! - in org.apache.flink.connector.hbase1.HBaseTablePlanTest
2020-09-28T21:28:31.0368925Z testProjectionPushDown(org.apache.flink.connector.hbase1.HBaseTablePlanTest)  Time elapsed: 0.031 sec  <<< ERROR!
2020-09-28T21:28:31.0369805Z org.apache.flink.table.api.ValidationException: 
2020-09-28T21:28:31.0370409Z Unable to create a source for reading table 'default_catalog.default_database.hTable'.
2020-09-28T21:28:31.0370707Z 
2020-09-28T21:28:31.0370976Z Table options are:
2020-09-28T21:28:31.0371204Z 
2020-09-28T21:28:31.0371528Z 'connector'='hbase-1.4'
2020-09-28T21:28:31.0371871Z 'table-name'='my_table'
2020-09-28T21:28:31.0372255Z 'zookeeper.quorum'='localhost:2021'
2020-09-28T21:28:31.0372812Z 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)
2020-09-28T21:28:31.0373359Z 	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)
2020-09-28T21:28:31.0373905Z 	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)
2020-09-28T21:28:31.0374390Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)
2020-09-28T21:28:31.0375224Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)
2020-09-28T21:28:31.0375867Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)
2020-09-28T21:28:31.0376479Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)
2020-09-28T21:28:31.0377077Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)
2020-09-28T21:28:31.0377593Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl$$anon$1.convertFrom(FlinkPlannerImpl.scala:181)
2020-09-28T21:28:31.0378114Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)
2020-09-28T21:28:31.0378622Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)
2020-09-28T21:28:31.0379132Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)
2020-09-28T21:28:31.0379872Z 	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)
2020-09-28T21:28:31.0380477Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:196)
2020-09-28T21:28:31.0381128Z 	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:154)
2020-09-28T21:28:31.0381666Z 	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:823)
2020-09-28T21:28:31.0382264Z 	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:795)
2020-09-28T21:28:31.0382968Z 	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:250)
2020-09-28T21:28:31.0383550Z 	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)
2020-09-28T21:28:31.0384172Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:640)
2020-09-28T21:28:31.0384700Z 	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:346)
2020-09-28T21:28:31.0385201Z 	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)
2020-09-28T21:28:31.0385717Z 	at org.apache.flink.connector.hbase1.HBaseTablePlanTest.testProjectionPushDown(HBaseTablePlanTest.java:124)
2020-09-28T21:28:31.0386166Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-28T21:28:31.0386575Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-28T21:28:31.0387257Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-28T21:28:31.0387822Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-28T21:28:31.0388229Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-28T21:28:31.0388718Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-28T21:28:31.0389198Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-28T21:28:31.0389745Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-28T21:28:31.0390262Z 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
2020-09-28T21:28:31.0390732Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-28T21:28:31.0391179Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-28T21:28:31.0391582Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-28T21:28:31.0391964Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-28T21:28:31.0392382Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-28T21:28:31.0393053Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-28T21:28:31.0393617Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-28T21:28:31.0393997Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-28T21:28:31.0394407Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-28T21:28:31.0394817Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-28T21:28:31.0395211Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-28T21:28:31.0395608Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-28T21:28:31.0396041Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
2020-09-28T21:28:31.0396517Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
2020-09-28T21:28:31.0397026Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-28T21:28:31.0397512Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
2020-09-28T21:28:31.0398245Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
2020-09-28T21:28:31.0398778Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
2020-09-28T21:28:31.0399251Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
2020-09-28T21:28:31.0399838Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
2020-09-28T21:28:31.0400340Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
2020-09-28T21:28:31.0400756Z 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
2020-09-28T21:28:31.0401304Z 	at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113)
2020-09-28T21:28:31.0401869Z 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)
2020-09-28T21:28:31.0402307Z 	... 50 more
2020-09-28T21:28:31.0402624Z 
2020-09-28T21:28:31.0402949Z Running org.apache.flink.connector.hbase1.HBaseDescriptorTest
2020-09-28T21:28:31.0416116Z Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec - in org.apache.flink.connector.hbase1.HBaseDescriptorTest
2020-09-28T21:28:31.4448287Z 
2020-09-28T21:28:31.4448950Z Results :
2020-09-28T21:28:31.4449082Z 
2020-09-28T21:28:31.4449270Z Tests in error: 
2020-09-28T21:28:31.4450556Z   HBaseDynamicTableFactoryTest.testTableSourceFactory:104->createTableSource:332 Â» Validation
2020-09-28T21:28:31.4451232Z   HBaseTableFactoryTest.testTableSourceFactory:101 Â» NoSuchMethod com.google.com...
2020-09-28T21:28:31.4451851Z   HBaseTablePlanTest.testProjectionPushDown:124 Â» Validation Unable to create a ...
{code}"	FLINK	Closed	1	1	8669	pull-request-available, test-stability
13353987	Set up cron job to run CI with declarative scheduler	Once the declarative scheduler has been merged, we should create a Cron job to run all CI profiles with this scheduler in order to find all remaining test failures.	FLINK	Closed	3	7	8669	pull-request-available
13385491	When cancelling any running job of multiple jobs in an application cluster, JobManager shuts down	"I have a jar with two jobs, both executeAsync() from the same main method. I execute the main method in an Application Mode cluster. When I cancel one of the two jobs, both jobs will stop executing.

I would expect that the JobManager shuts down once all jobs submitted from an application are finished.

If this is a known limitation, we should document it.

{code}
2021-06-23 21:29:53,123 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job first job (18181be02da272387354d093519b2359) switched from state RUNNING to CANCELLING.
2021-06-23 21:29:53,124 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from RUNNING to CANCELING.
2021-06-23 21:29:53,141 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from CANCELING to CANCELED.
2021-06-23 21:29:53,144 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b2359
2021-06-23 21:29:53,145 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job first job (18181be02da272387354d093519b2359) switched from state CANCELLING to CANCELED.
2021-06-23 21:29:53,145 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 18181be02da272387354d093519b2359.
2021-06-23 21:29:53,147 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2021-06-23 21:29:53,150 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 18181be02da272387354d093519b2359 reached terminal state CANCELED.
2021-06-23 21:29:53,152 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job first job(18181be02da272387354d093519b2359).
2021-06-23 21:29:53,155 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [c35b64879d6b02d383c825ea735ebba0].
2021-06-23 21:29:53,159 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b2359
2021-06-23 21:29:53,159 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job first job(18181be02da272387354d093519b2359)..
2021-06-23 21:29:53,160 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 18181be02da272387354d093519b2359 from the resource manager.
2021-06-23 21:29:53,225 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application CANCELED:
java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED
	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$4(ApplicationDispatcherBootstrap.java:304) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252]
	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$2(JobStatusPollingUtils.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) [?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) [?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) [?:1.8.0_252]
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1081) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:264) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:261) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED
	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 42 more
Caused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 42 more
2021-06-23 21:29:53,238 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneApplicationClusterEntryPoint down with application status CANCELED. Diagnostics null.
2021-06-23 21:29:53,239 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
2021-06-23 21:29:53,257 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Removing cache directory /var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/flink-web-a0d034d2-da2b-4d72-9ece-ec00c9ae032b/flink-web-ui
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://localhost:8081 lost leadership
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shut down complete.
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics null.
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job second job(e4ff65c30754648cf114232c07ef903e).
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job e4ff65c30754648cf114232c07ef903e reached terminal state SUSPENDED.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job second job (e4ff65c30754648cf114232c07ef903e) switched from state RUNNING to SUSPENDED.
org.apache.flink.util.FlinkException: Scheduler is being stopped.
	at org.apache.flink.runtime.scheduler.SchedulerBase.closeAsync(SchedulerBase.java:604) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopScheduling(JobMaster.java:962) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopJobExecution(JobMaster.java:926) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:398) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2021-06-23 21:29:53,311 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from RUNNING to CANCELING.
2021-06-23 21:29:53,312 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from CANCELING to CANCELED.
2021-06-23 21:29:53,313 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b08fac5184817c72f73a0b3fff0afbd3.
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job e4ff65c30754648cf114232c07ef903e.
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job e4ff65c30754648cf114232c07ef903e has been suspended.
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [30b64fc00bc2c8e83e80567e4f984ae9].
2021-06-23 21:29:53,315 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job second job(e4ff65c30754648cf114232c07ef903e)..
2021-06-23 21:29:53,318 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.
2021-06-23 21:29:53,323 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:61498
2021-06-23 21:29:53,323 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2021-06-23 21:29:53,326 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2021-06-23 21:29:53,331 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2021-06-23 21:29:53,331 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2021-06-23 21:29:53,332 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2021-06-23 21:29:53,332 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2021-06-23 21:29:53,348 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2021-06-23 21:29:53,348 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2021-06-23 21:29:53,359 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2021-06-23 21:29:53,366 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2021-06-23 21:29:53,366 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Terminating cluster entrypoint process StandaloneApplicationClusterEntryPoint with exit code 0.
{code}"	FLINK	Closed	3	1	8669	pull-request-available
13327656	Forward ClusterEntrypoint.ioExecutor to ResourceManager	Based on the discussion in FLINK-19037, we want to forward the ioExecutor from the ClusterEntrypoint to the ResourceManager as well.	FLINK	Closed	3	4	8669	pull-request-available
13304937	"""flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath"" present after building Flink"	"Some changes introduced in FLINK-11086 cause the ""flink-end-to-end-tests/test-scripts/hadoop/yarn.classpath"" file to be generated and present in the source tree after building Flink.

"	FLINK	Closed	3	1	8669	pull-request-available
12748043	Add a QA bot to Flink that is testing pull requests	"We should have a QA bot (similar to Hadoop) that is checking incoming pull requests for a few things:
- Changes to user-facing APIs
- More compiler warnings than before
- more Javadoc warnings than before
- change of the number of files in the lib/ directory.
- unused dependencies
- {{@author}} tag.
- guava (and other shaded jars) in the lib/ directory.

It should be somehow extensible to add new tests."	FLINK	Closed	4	2	8669	starter
13356470	Add DeclarativeScheduler / WaitingForResources state	"This subtask of adding the declarative scheduler is about adding the WaitingForResources state to Flink, including tests.

Waiting for resources: The required resources are declared. The scheduler waits until either the requirements are fulfilled or the set of resources has stabilised.
"	FLINK	Closed	3	7	8669	pull-request-available
13309429	Skip CI execution on documentation pull requests	"In order to save some resources, we can skip the CI execution on documentation-only changes (whole changeset).

"	FLINK	Closed	3	4	8669	pull-request-available
13287779	Execute all end to end tests on AZP	"Ensure that we execute all end to end tests on AZP:
- Make sure that all the e2e tests referenced in the splits are also referenced in the ""run nightly tests"" script
- make sure the java e2e tests are executed"	FLINK	Resolved	3	7	8669	pull-request-available
13357921	Extract interface out of ExecutionGraph for better testability	"This is a follow up to this comment: https://github.com/apache/flink/pull/14879#discussion_r573613450

The ExecutionGraph class is currently not very handy for tests, as it has a lot of dependencies. Extracting an interface for the ExecutionGraph will make testing and future changes easier."	FLINK	Closed	3	4	8669	pull-request-available
13304128	Use default akka.ask.timeout in TPC-DS e2e test	Revert the changes in FLINK-17616	FLINK	Closed	3	4	8669	pull-request-available
13301179	Running HA per-job cluster (rocks, non-incremental) gets stuck killing a non-existing pid in Hadoop 3 build profile	"CI log: https://api.travis-ci.org/v3/job/678609505/log.txt

{code}
Waiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Starting standalonejob daemon on host travis-job-e606668f-b674-49c0-8590-e3508e22b99d.
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Killed TM @ 18864
kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]
Killed TM @ 


No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.
Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received

The build has been terminated
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13285938	YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority() fails	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5259&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=da3c2718-4b76-56bf-ef25-cd33ea381f78
https://transfer.sh/G229r/20200218.1.tar.gz

{code:java}
01:29:45,184 [                main] INFO  org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl  - Notifying ContainerManager to unblock new container-requests
01:29:45,184 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler  - Added node 8b508210238a:37199 clusterResource: <memory:4096, vCores:666>
01:29:45,192 [                main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster                 - All Node Managers connected in MiniYARNCluster
01:29:45,192 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl  - Node 8b508210238a:37199 reported UNHEALTHY with details: 1/1 local-dirs are bad: /__w/1/s/flink-yarn-tests/target/YarnTest_3d15de1b-4c9a-411b-81b1-3ca2788b479a/YarnTest_3d15de1b-4c9a-411b-81b1-3ca2788b479a-localDir-nm-1_0; 1/1 log-dirs are bad: /__w/1/s/flink-yarn-tests/target/YarnTest_3d15de1b-4c9a-411b-81b1-3ca2788b479a/YarnTest_3d15de1b-4c9a-411b-81b1-3ca2788b479a-logDir-nm-1_0
01:29:45,192 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl  - 8b508210238a:37199 Node Transitioned from RUNNING to UNHEALTHY
01:29:45,193 [ResourceManager Event Processor] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler  - Removed node 8b508210238a:37199 clusterResource: <memory:0, vCores:0>

================================================================================
Test yarnApplication_submissionWithPriority_shouldRespectPriority(org.apache.flink.yarn.YarnPrioritySchedulingITCase) is running.
--------------------------------------------------------------------------------
01:29:45,449 [                main] INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at 8b508210238a/172.18.0.2:40977
01:29:45,529 [                main] INFO  org.apache.flink.yarn.YarnTestBase                            - Running with args [-j, /__w/1/s/flink-yarn-tests/../build-target/lib/flink-dist_2.11-1.11-SNAPSHOT.jar, -t, /__w/1/s/flink-yarn-tests/../build-target/lib, -t, /__w/1/s/flink-yarn-tests/target/shaded-hadoop, -jm, 768m, -tm, 1024m, -Dyarn.application.priority=5]
01:29:45,555 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/tmp/junit3457413859979027256/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.
01:29:45,567 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Dynamic Property set: yarn.application.priority=5
01:29:45,591 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at 8b508210238a/172.18.0.2:40977
01:29:45,611 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils  - The derived from fraction jvm overhead memory (102.400mb (107374184 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
01:29:45,753 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.YarnTestBase                            - Runner stopped with exception
org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster
	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:381)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:548)
	at org.apache.flink.yarn.YarnTestBase$Runner.run(YarnTestBase.java:917)
Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:284)
	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:444)
	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:374)
	... 2 more
{code}
"	FLINK	Resolved	3	4	8669	pull-request-available, test-stability
13327432	"""Resuming Savepoint (rocks, scale down, rocks timers) end-to-end test"" failed with ""Dispatcher REST endpoint has not started within a timeout of 20 sec"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6476&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=3e8647c1-5a28-5917-dd93-bf78594ea994

{code}
2020-09-13T21:26:23.3646770Z Running 'Resuming Savepoint (rocks, scale down, rocks timers) end-to-end test'
2020-09-13T21:26:23.3647852Z ==============================================================================
2020-09-13T21:26:23.3689605Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-23367497881
2020-09-13T21:26:23.7122791Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-09-13T21:26:23.9988115Z Starting cluster.
2020-09-13T21:26:27.3702750Z Starting standalonesession daemon on host fv-az655.
2020-09-13T21:26:35.1213853Z Starting taskexecutor daemon on host fv-az655.
2020-09-13T21:26:35.2756714Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:36.4111928Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:37.5358508Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:38.7156039Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:39.8602294Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:41.0399056Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:42.1680966Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:43.2520250Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:44.3833552Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:45.5204296Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:46.6730448Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:47.8274365Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:49.0147447Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:51.5463623Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:52.7366058Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:53.8867521Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:55.0469025Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:56.1901349Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:57.3124935Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:58.4596457Z Waiting for Dispatcher REST endpoint to come up...
2020-09-13T21:26:59.4828675Z Dispatcher REST endpoint has not started within a timeout of 20 sec
2020-09-13T21:26:59.4831446Z [FAIL] Test script contains errors.
{code}"	FLINK	Closed	3	1	8669	test-stability
13384766	cron_snapshot_deployment_maven unstable on maven	"{{cron_snapshot_deployment_maven}}, the cron build on azure that deploys snapshot artifacts to maven central repository, has become unstable recently.

The failures fall into two categories.
- Maven failed to upload/download an artifact
- The stage overall takes too long time.

As far as I can see, the instability starts being observed since June 18th.

Observed instances:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19152&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19154&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19167&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19169&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19176&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19175&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19177&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7"	FLINK	Closed	2	4	8669	pull-request-available, test-stability
12826308	Rework TwitterSource to use a Properties object instead of a file path	"The twitter connector is very hard to use on a cluster because it expects the property file to be present on all nodes.

It would be much easier to ask the user to pass a Properties object immediately.
Also, the javadoc of the class stops in the middle of the sentence.

It was not obvious to me how the two examples TwitterStreaming and TwitterTopology differ. Also, there is a third TwitterStream example in the streaming examples.
The documentation of the Twitter source refers to the non existent TwitterLocal class."	FLINK	Resolved	4	1	8669	starter
13356472	Add DeclarativeScheduler / Executing state	"This subtask of adding the declarative scheduler is about adding the Executing state to Flink, including tests.

Executing: The set of resources is stable and the scheduler could decide on the parallelism with which to execute the job. The ExecutionGraph is created and the execution of the job has started.
"	FLINK	Closed	3	7	8669	pull-request-available
13336879	Upgrade commons_codec to 1.13 or newer	"A user reported a dependency vulnerability which affects {{commons_codec}} [1]. We should try to upgrade this version to 1.13 or newer.

[1] https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E"	FLINK	Closed	2	4	8669	pull-request-available
13310438	"""Avro Confluent Schema Registry nightly end-to-end test"" unstable with ""Kafka cluster did not start after 120 seconds"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3045&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-06-09T15:16:48.1427795Z ==============================================================================
2020-06-09T15:16:48.1428609Z Running 'Avro Confluent Schema Registry nightly end-to-end test'
2020-06-09T15:16:48.1429204Z ==============================================================================
2020-06-09T15:16:48.1438117Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-48143298170
2020-06-09T15:16:48.2985167Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-06-09T15:16:48.3157575Z Downloading Kafka from https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz
2020-06-09T15:16:48.3214487Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-06-09T15:16:48.3215154Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-06-09T15:16:48.3215597Z 
2020-06-09T15:16:48.3528820Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:49.3421526Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:50.3415678Z   8 35.8M    8 2960k    0     0  2896k      0  0:00:12  0:00:01  0:00:11 2896k
2020-06-09T15:16:51.3406836Z  23 35.8M   23 8544k    0     0  4226k      0  0:00:08  0:00:02  0:00:06 4225k
2020-06-09T15:16:51.6553485Z  70 35.8M   70 25.2M    0     0  8550k      0  0:00:04  0:00:03  0:00:01 8548k
2020-06-09T15:16:51.6555606Z 100 35.8M  100 35.8M    0     0  10.7M      0  0:00:03  0:00:03 --:--:-- 10.7M
2020-06-09T15:16:51.9818041Z Downloading confluent from http://packages.confluent.io/archive/3.2/confluent-oss-3.2.0-2.11.tar.gz
2020-06-09T15:16:51.9880242Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-06-09T15:16:51.9880983Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-06-09T15:16:51.9914252Z 
2020-06-09T15:16:52.3398614Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:53.3399552Z   9  398M    9 39.5M    0     0   111M      0  0:00:03 --:--:--  0:00:03  111M
2020-06-09T15:16:53.9149276Z  47  398M   47  188M    0     0   139M      0  0:00:02  0:00:01  0:00:01  138M
2020-06-09T15:16:53.9150980Z 100  398M  100  398M    0     0   206M      0  0:00:01  0:00:01 --:--:--  206M
2020-06-09T15:17:04.3565942Z Waiting for broker...
2020-06-09T15:17:12.4215170Z Waiting for broker...
2020-06-09T15:17:14.3012835Z Waiting for broker...
2020-06-09T15:17:16.1965074Z Waiting for broker...
2020-06-09T15:17:18.1102274Z Waiting for broker...
2020-06-09T15:17:19.9929632Z Waiting for broker...
2020-06-09T15:17:21.8607172Z Waiting for broker...
2020-06-09T15:17:23.7802949Z Waiting for broker...
2020-06-09T15:17:25.6695260Z Waiting for broker...
2020-06-09T15:17:27.5536417Z Waiting for broker...
2020-06-09T15:17:29.4327778Z Waiting for broker...
2020-06-09T15:17:31.3203091Z Waiting for broker...
2020-06-09T15:17:33.1987150Z Waiting for broker...
2020-06-09T15:17:35.0694860Z Waiting for broker...
2020-06-09T15:17:36.9595576Z Waiting for broker...
2020-06-09T15:17:38.9243558Z Waiting for broker...
2020-06-09T15:17:40.7984064Z Waiting for broker...
2020-06-09T15:17:42.6676095Z Waiting for broker...
2020-06-09T15:17:44.5628797Z Waiting for broker...
2020-06-09T15:17:46.4374532Z Waiting for broker...
2020-06-09T15:17:48.3086761Z Waiting for broker...
2020-06-09T15:17:50.1574336Z Waiting for broker...
2020-06-09T15:17:52.0432952Z Waiting for broker...
2020-06-09T15:17:53.9406541Z Waiting for broker...
2020-06-09T15:17:55.8162052Z Waiting for broker...
2020-06-09T15:17:57.7090015Z Waiting for broker...
2020-06-09T15:17:59.5747770Z Waiting for broker...
2020-06-09T15:18:01.4601854Z Waiting for broker...
2020-06-09T15:18:03.3332039Z Waiting for broker...
2020-06-09T15:18:05.2210453Z Waiting for broker...
2020-06-09T15:18:07.1133675Z Waiting for broker...
2020-06-09T15:18:09.0132417Z Waiting for broker...
2020-06-09T15:18:10.8769511Z Waiting for broker...
2020-06-09T15:18:12.7601639Z Waiting for broker...
2020-06-09T15:18:14.6389770Z Waiting for broker...
2020-06-09T15:18:16.5210725Z Waiting for broker...
2020-06-09T15:18:18.4088216Z Waiting for broker...
2020-06-09T15:18:20.2732225Z Waiting for broker...
2020-06-09T15:18:22.1558390Z Waiting for broker...
2020-06-09T15:18:24.0400570Z Waiting for broker...
2020-06-09T15:18:25.9134038Z Waiting for broker...
2020-06-09T15:18:27.7922350Z Waiting for broker...
2020-06-09T15:18:29.6748679Z Waiting for broker...
2020-06-09T15:18:31.5340996Z Waiting for broker...
2020-06-09T15:18:33.3998472Z Waiting for broker...
2020-06-09T15:18:35.2718135Z Waiting for broker...
2020-06-09T15:18:37.1426082Z Waiting for broker...
2020-06-09T15:18:39.1282264Z Waiting for broker...
2020-06-09T15:18:41.0029183Z Waiting for broker...
2020-06-09T15:18:42.8700037Z Waiting for broker...
2020-06-09T15:18:44.7531621Z Waiting for broker...
2020-06-09T15:18:46.6465173Z Waiting for broker...
2020-06-09T15:18:48.9504192Z Waiting for broker...
2020-06-09T15:18:50.4165383Z Waiting for broker...
2020-06-09T15:18:52.2931688Z Waiting for broker...
2020-06-09T15:18:54.1669857Z Waiting for broker...
2020-06-09T15:18:56.0238505Z Waiting for broker...
2020-06-09T15:18:57.8931143Z Waiting for broker...
2020-06-09T15:18:59.7607751Z Kafka cluster did not start after 120 seconds. Printing Kafka logs:
{code}
There's a lot of log output I didn't analyze yet."	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13354471	Reactive mode: Introduce execution mode configuration key and check for supported ClusterEntrypoint type	"According to the FLIP, introduce a ""scheduler-mode"" configuration key, and check in the ClusterEntrypoint if the chosen entry point type is supported by the selected execution mode."	FLINK	Closed	3	7	8669	pull-request-available
13338920	The automatic license check failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9020&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=ffa46b9d-687d-5a4a-3e77-010e0da138d3

{code}

2020-11-04T21:34:32.2175854Z [INFO] ------------------------------------------------------------------------ 2020-11-04T21:34:32.2176981Z [INFO] Building Flink : Tools : CI : Java 1.12-SNAPSHOT 2020-11-04T21:34:32.2177899Z [INFO] ------------------------------------------------------------------------ 2020-11-04T21:34:32.2738277Z [INFO] 2020-11-04T21:34:32.2739975Z [INFO] --- exec-maven-plugin:3.0.0:java (default-cli) @ java-ci-tools --- 2020-11-04T21:34:33.3354165Z 21:34:33,332 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Loaded 20 items from resource modules-skipping-deployment.modulelist 2020-11-04T21:34:33.3377191Z 21:34:33,337 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Loaded 7 items from resource modules-defining-excess-dependencies.modulelist 2020-11-04T21:34:33.3413748Z 21:34:33,341 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - THIS UTILITY IS ONLY CHECKING FOR COMMON LICENSING MISTAKES. A MANUAL CHECK OF THE NOTICE FILES, DEPLOYED ARTIFACTS, ETC. IS STILL NEEDED! 2020-11-04T21:34:37.9134087Z 21:34:37,912 INFO org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Extracted 40 modules with a total of 574 dependencies 2020-11-04T21:34:39.1450119Z 21:34:39,144 INFO org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found 40 NOTICE files to check 2020-11-04T21:34:39.1669092Z 21:34:39,166 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-remote_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-runtime/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.1671308Z 21:34:39,166 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-remote_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-runtime/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.1721569Z 21:34:39,171 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.apache.orc:orc-core:nohive:1.5.6 in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.1724119Z 21:34:39,171 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1727247Z 21:34:39,171 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1730087Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-storage-api:2.3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1732575Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1735187Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:spark-client:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1738561Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-hadoop-bundle:1.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1741197Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:2.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1743602Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1745716Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libfb303:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1747800Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1749957Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-orc:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1752214Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1754327Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1756535Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:minlog:1.3.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1758620Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-metastore:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1760690Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1762758Z 21:34:39,172 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1766619Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1768581Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javax.jdo:jdo-api:3.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1770474Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:14.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1772375Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1774296Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:kryo-shaded:3.0.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1780026Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1782109Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1784013Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1786050Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1787992Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1789940Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-common:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1791842Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.tdunning:json:1.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1793774Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1795709Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.iq80.snappy:snappy:0.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1797616Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-service-rpc:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1799566Z 21:34:39,173 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-client:2.2.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.2.0/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1801981Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1804111Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-storage-api:2.7.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1810514Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:2.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1812619Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1814584Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1921350Z 21:34:39,177 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1928903Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1931355Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1936369Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1938695Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1940907Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1943072Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:reflectasm:1.10.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1945230Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:minlog:1.3.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1947637Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-common:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1951216Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.airlift:aircompressor:0.10 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1953109Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1955504Z 21:34:39,178 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-spark-client:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1957544Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javax.jdo:jdo-api:3.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1959833Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-core:1.5.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1962036Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-standalone-metastore:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1963988Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1966012Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-metastore:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1968158Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-service-rpc:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1970206Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-shims:1.5.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1977414Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-client:3.1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1980778Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:kryo-shaded:3.0.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1986490Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:19.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.1988628Z 21:34:39,179 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.9.9 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2005476Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-tools:1.5.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2009369Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2011337Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2013255Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2015177Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-jackson:1.10.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2017747Z 21:34:39,180 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2019652Z 21:34:39,182 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.tdunning:json:1.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2021577Z 21:34:39,183 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2023752Z 21:34:39,183 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-hadoop-bundle:1.10.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-3.1.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2025783Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2027637Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2029567Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware.minlog:minlog:1.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2031535Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware.reflectasm:reflectasm:1.07 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2033533Z 21:34:39,185 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2035487Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2037441Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.20S:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2039405Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2041359Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2043271Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2045257Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.twitter:parquet-hadoop-bundle:1.6.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2047134Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:14.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2049083Z 21:34:39,186 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2051616Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2053616Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2055720Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2059933Z 21:34:39,187 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2061926Z 21:34:39,188 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:spark-client:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2064924Z 21:34:39,188 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2066900Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2068820Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:1.2.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2070725Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2072836Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2074892Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2076807Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.iq80.snappy:snappy:0.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2078716Z 21:34:39,189 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware.kryo:kryo:2.22 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-1.2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2080694Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-annotations:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2082656Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-logging:commons-logging:1.1.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2084605Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-core:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2086594Z 21:34:39,191 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-databind:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-1.4/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2088548Z 21:34:39,192 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-jackson:1.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2090444Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.jodd:jodd-core:3.5.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2092341Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-storage-api:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2094323Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency joda-time:joda-time:2.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2098788Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.parquet:parquet-hadoop-bundle:1.8.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2101068Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.objenesis:objenesis:2.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2106636Z 21:34:39,193 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:spark-client:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2111497Z 21:34:39,194 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro-mapred:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2115394Z 21:34:39,194 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.slf4j:slf4j-api:1.7.10 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2118884Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.airlift:aircompressor:0.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2120864Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-mapper-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2122837Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.googlecode.javaewah:JavaEWAH:0.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2124804Z 21:34:39,195 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-common:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2126776Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:reflectasm:1.10.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2128858Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:minlog:1.3.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2130795Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-common:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2132825Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive.shims:hive-shims-0.23:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2134776Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-serde:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2136933Z 21:34:39,196 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-core:1.3.4 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2138823Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.avro:avro:1.7.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2140745Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-service-rpc:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2143089Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javax.jdo:jdo-api:3.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2145084Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-metastore:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2146996Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:14.0.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2148883Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency javolution:javolution:5.5.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2152546Z 21:34:39,197 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.esotericsoftware:kryo-shaded:3.0.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2154818Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-common:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2157822Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2159743Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency net.sf.opencsv:opencsv:2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2161674Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hive:hive-llap-client:2.3.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2163608Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.thrift:libthrift:0.9.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2165499Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.orc:orc-tools:1.3.4 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2167420Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:2.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2169889Z 21:34:39,198 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.commons:commons-lang3:3.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2171870Z 21:34:39,199 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.tdunning:json:1.8 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.2173827Z 21:34:39,199 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.codehaus.jackson:jackson-core-asl:1.9.13 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hive-2.3.6/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3020239Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-annotations:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3022559Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-logging:commons-logging:1.1.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3027098Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3031232Z 21:34:39,301 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-core:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3033953Z 21:34:39,302 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hbase:hbase-hadoop-compat:2.2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3042063Z 21:34:39,302 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.apache.hbase:hbase-hadoop2-compat:2.2.3 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.3044366Z 21:34:39,302 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.fasterxml.jackson.core:jackson-databind:2.4.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-connectors/flink-sql-connector-hbase-2.2/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.4959013Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang:scala-reflect:2.12.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4960999Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang:scala-library:2.12.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4962531Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang:scala-compiler:2.12.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4964055Z 21:34:39,492 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang.modules:scala-xml_2.12:1.0.6 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4965574Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-actor_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4967134Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang.modules:scala-java8-compat_2.12:0.8.0 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4968696Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-stream_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4970496Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe:ssl-config-core_2.12:0.3.7 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4972066Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.scala-lang.modules:scala-parser-combinators_2.12:1.1.1 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4973618Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-protobuf_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4975535Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.typesafe.akka:akka-slf4j_2.12:2.5.21 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4977058Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency org.clapper:grizzled-slf4j_2.12:1.3.2 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4978538Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.github.scopt:scopt_2.12:3.5.0 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4979983Z 21:34:39,493 ERROR org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Could not find dependency com.twitter:chill_2.12:0.7.6 in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE 2020-11-04T21:34:39.4981577Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang.modules:scala-java8-compat_2.11:0.7.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4983204Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang:scala-library:2.11.12 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4984775Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.clapper:grizzled-slf4j_2.11:1.3.2 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4988396Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang:scala-compiler:2.11.12 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4990047Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-actor_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4991752Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang.modules:scala-parser-combinators_2.11:1.1.1 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4993404Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-slf4j_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4995012Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang:scala-reflect:2.11.12 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4996813Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe:ssl-config-core_2.11:0.3.7 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.4998368Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.twitter:chill_2.11:0.7.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5000029Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency org.scala-lang.modules:scala-xml_2.11:1.0.5 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5001654Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-protobuf_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5003248Z 21:34:39,493 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.github.scopt:scopt_2.11:3.5.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5004840Z 21:34:39,494 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.typesafe.akka:akka-stream_2.11:2.5.21 is mentioned in NOTICE file /home/vsts/work/1/s/flink-dist/src/main/resources/META-INF/NOTICE, but is not expected there 2020-11-04T21:34:39.5644298Z 21:34:39,561 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-transport-native-epoll:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5646181Z 21:34:39,561 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-context:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5647912Z 21:34:39,561 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.code.gson:gson:2.8.6 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5649691Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec-http:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5651431Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-auth:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5653230Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-transport-native-unix-common:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5655017Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-protobuf:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5656990Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.opencensus:opencensus-contrib-grpc-metrics:0.24.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5659084Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-transport:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5660968Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-handler-proxy:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5662802Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-tcnative-boringssl-static:2.0.26.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5664600Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.opencensus:opencensus-api:0.24.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5666388Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java-util:3.11.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5668152Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-testing:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5669896Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-resolver:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5671698Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.api.grpc:proto-google-common-protos:1.12.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5673464Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-netty:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5675154Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-core:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5676892Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.protobuf:protobuf-java:3.11.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5678657Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-buffer:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5680433Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.grpc:grpc-stub:1.26.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5684365Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec-http2:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5686780Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5688614Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-common:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5690345Z 21:34:39,562 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.guava:guava:26.0-jre is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5692167Z 21:34:39,563 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency com.google.auth:google-auth-library-credentials:0.18.0 is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5693969Z 21:34:39,563 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-codec-socks:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5695934Z 21:34:39,563 DEBUG org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Dependency io.netty:netty-handler:4.1.42.Final is mentioned in NOTICE file /home/vsts/work/1/s/flink-python/src/main/resources/META-INF/NOTICE, but was not mentioned by the build output as a bundled dependency 2020-11-04T21:34:39.5697230Z 21:34:39,563 WARN org.apache.flink.tools.ci.licensecheck.LicenseChecker [] - Found a total of 16 severe license issues 2020-11-04T21:34:39.5840050Z ============================================================================== 2020-11-04T21:34:39.5843291Z License Check failed. See previous output for details. 2020-11-04T21:34:39.5843752Z ============================================================================== 2020-11-04T21:34:39.5873836Z 2020-11-04T21:34:39.6011858Z ##[error]Bash exited with code '1'. 2020-11-04T21:34:39.6028742Z ##[section]Finishing: Build Flink
{code}"	FLINK	Closed	1	1	8669	pull-request-available, test-stability
12724413	Add documentation for POJO expression keys	"The Java API documentation needs to be extended to reflect the extensions added by FLINK-908.
These are basically expression keys for POJO objects, i.e., selecting public fields of POJOs as keys by their names."	FLINK	Resolved	3	7	8669	starter
13342678	Rework standalone deployment documentation page	Similar to FLINK-20347 we need to update the standalone deployment documentation page. Additionally, we need to verify that everything we state on the documentation works.	FLINK	Closed	3	7	8669	pull-request-available
13318752	"Update version of aws to support use of default constructor of ""WebIdentityTokenCredentialsProvider"""	"*Background:*

I am using Flink 1.11.0 on kubernetes platform. To give access of aws services to taskmanager/jobmanager, we are using ""IAM Roles for Service Accounts"" . I have configured below property in flink-conf.yaml to use credential provider.

fs.s3a.aws.credentials.provider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider

 

*Issue:*

When taskmanager/jobmanager is starting up, during this it complains that ""WebIdentityTokenCredentialsProvider"" doesn't have ""public constructor"" and container doesn't come up.

 

*Solution:*

Currently the above credential's class is being used from ""*flink-s3-fs-hadoop""* which gets ""aws-java-sdk-core"" dependency from ""*flink-s3-fs-base*"". In *""flink-s3-fs-base"",*  version of aws is 1.11.754 . The support of default constructor for ""WebIdentityTokenCredentialsProvider"" is provided from aws version 1.11.788 and onward."	FLINK	Closed	4	4	8669	pull-request-available
13492486	Allow using MiniCluster with a PluginManager to use metrics reporters	"Currently, using MiniCluster with a metric reporter loaded as a plugin is not supported, because the {{ReporterSetup.fromConfiguration(config, null));}} gets passed {{null}} for the PluginManager.

I think it generally valuable to allow passing a PluginManager to the MiniCluster.

I'll open a PR for this.
"	FLINK	Closed	3	4	8669	pull-request-available
13286918	Streaming bucketing end-to-end test NoClassDefFoundError	"This nightly cron job has failed: https://travis-ci.org/apache/flink/jobs/653454540

{code}
==============================================================================
Running 'Streaming bucketing end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-05739414867
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
Setting up SSL with: internal JDK dynamic
Using SAN dns:travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7,ip:10.20.0.145,ip:172.17.0.1
Certificate was added to keystore
Certificate was added to keystore
Certificate reply was installed in keystore
MAC verified OK
Setting up SSL with: rest JDK dynamic
Using SAN dns:travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7,ip:10.20.0.145,ip:172.17.0.1
Certificate was added to keystore
Certificate was added to keystore
Certificate reply was installed in keystore
MAC verified OK
Mutual ssl auth: false
Starting cluster.
Starting standalonesession daemon on host travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
Starting taskexecutor daemon on host travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
Waiting for Dispatcher REST endpoint to come up...
Waiting for Dispatcher REST endpoint to come up...
Waiting for Dispatcher REST endpoint to come up...
Waiting for Dispatcher REST endpoint to come up...
Waiting for Dispatcher REST endpoint to come up...
Dispatcher REST endpoint is up.
[INFO] 1 instance(s) of taskexecutor are already running on travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
Starting taskexecutor daemon on host travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
[INFO] 2 instance(s) of taskexecutor are already running on travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
Starting taskexecutor daemon on host travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
[INFO] 3 instance(s) of taskexecutor are already running on travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
Starting taskexecutor daemon on host travis-job-b9e26d64-0a62-42c7-9802-6c49defb4ad7.
Number of running task managers 1 is not yet 4.
Number of running task managers 2 is not yet 4.
Number of running task managers has reached 4.
java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at org.apache.flink.api.java.ClosureCleaner.usesCustomSerialization(ClosureCleaner.java:164)
	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:89)
	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:71)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.clean(StreamExecutionEnvironment.java:1820)
	at org.apache.flink.streaming.api.datastream.DataStream.clean(DataStream.java:188)
	at org.apache.flink.streaming.api.datastream.DataStream.addSink(DataStream.java:1328)
	at org.apache.flink.streaming.tests.BucketingSinkTestProgram.main(BucketingSinkTestProgram.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 23 more
Job () is running.
Waiting for job () to have at least 5 completed checkpoints ...
No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.
Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received
The build has been terminated
{code}"	FLINK	Closed	4	1	8669	test-stability
13336345	Invalid command in flink-end-to-end-tests README	"The [README|https://github.com/apache/flink/tree/master/flink-end-to-end-tests#running-tests] refers to `{{flink-end-to-end-tests/run-pre-commit-tests.sh}}` but that file no longer exists.

 "	FLINK	Closed	4	1	8669	pull-request-available
12731999	Materials section Project Website	"I suggest to add a materials section to the new project website (http://flink.incubator.apache.org/index.html) with an updated logo etc. to allow people to get materials for their projects around flink.

In addition to that I think that the menu entry project -> source code is unnecessary because there is a link to github on the main page... . If you want to keep it it should be renamed to github as well."	FLINK	Resolved	4	4	8669	documentation, easyfix, starter
13299145	System.IO.IOException: No space left on device in misc profile on free Azure builders	"Builds on the free Azure builders are failing with

{code}
##[error]Unhandled exception. System.IO.IOException: No space left on device
   at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source)
   at System.IO.FileStream.FlushWriteBuffer()
   at System.IO.FileStream.Flush(Boolean flushToDisk)
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at System.Diagnostics.TraceSource.Flush()
   at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose(Boolean disposing)
   at Microsoft.VisualStudio.Services.Agent.TraceManager.Dispose()
   at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose(Boolean disposing)
   at Microsoft.VisualStudio.Services.Agent.HostContext.Dispose()
   at Microsoft.VisualStudio.Services.Agent.Worker.Program.Main(String[] args)
Error reported in diagnostic logs. Please examine the log for more details.
    - /home/vsts/agents/2.165.2/_diag/Worker_20200414-093250-utc.log
System.IO.IOException: No space left on device
   at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source)
   at System.IO.FileStream.FlushWriteBuffer()
   at System.IO.FileStream.Flush(Boolean flushToDisk)
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at Microsoft.VisualStudio.Services.Agent.Worker.Worker.RunAsync(String pipeIn, String pipeOut)
   at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args)
System.IO.IOException: No space left on device
   at System.IO.FileStream.WriteNative(ReadOnlySpan`1 source)
   at System.IO.FileStream.FlushWriteBuffer()
   at System.IO.FileStream.Flush(Boolean flushToDisk)
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id)
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message)
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at Microsoft.VisualStudio.Services.Agent.Tracing.Error(Exception exception)
   at Microsoft.VisualStudio.Services.Agent.Worker.Program.MainAsync(IHostContext context, String[] args)
,##[error]The job running on agent Azure Pipelines 9 ran longer than the maximum time of 240 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134
,##[warning]Agent Azure Pipelines 9 did not respond to a cancelation request with 00:01:00.
{code}

CI run: https://dev.azure.com/chesnay/flink/_build/results?buildId=205&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4"	FLINK	Resolved	3	1	8669	pull-request-available
13367302	ReactiveModelITCase.testScaleDownOnTaskManagerLoss failed / hangs	"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=360&view=logs&j=e0582806-6d85-5dc5-7eb4-4289d3d0de6b&t=9fea6cf4-6ce3-5c26-d059-69f4d4cec7d1&l=4442] failed (not exclusively) due to {{ReactiveModelITCase.testScaleDownOnTaskManagerLoss}}.

I was able to reproduce it locally having the {{DefaultScheduler}} enabled. The test seems to get into an infinite loop:

{code}
[...]
76125 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Unnamed (4/4)#8738 92b920a905c55fc85a76c79b3acef161.
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{0896a914cffb9d6631dc061ff4f485b4})
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{0896a914cffb9d6631dc061ff4f485b4})
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot aec00279d7404b26a104ee906695d27a.
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{0896a914cffb9d6631dc061ff4f485b4})
76125 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state Executing to Restarting.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a) switched from state RUNNING to CANCELLING.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (843d0c154f55a15a9bb1e705ae282032) switched from RUNNING to CANCELING.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (ba0dd94db26abc376ee73522410b8094) switched from RUNNING to CANCELING.
76125 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (92b920a905c55fc85a76c79b3acef161) switched from RUNNING to CANCELING.
76126 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (ba0dd94db26abc376ee73522410b8094) switched from CANCELING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (3/4) - execution #8738 to FAILED while being CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{786f89cafa4833afb26d0eb5da265a38})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{786f89cafa4833afb26d0eb5da265a38})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot dde6780a1f8df3d0b1b1b454e28f8566.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{786f89cafa4833afb26d0eb5da265a38})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Cannot run 'newResourcesAvailable' because the actual state is Restarting and not ResourceConsumer.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution ba0dd94db26abc376ee73522410b8094.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (843d0c154f55a15a9bb1e705ae282032) switched from CANCELING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (2/4) - execution #8739 to FAILED while being CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{86ab3cc76a17ec876b12bbaa6efcfa8c})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{86ab3cc76a17ec876b12bbaa6efcfa8c})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot 25dd5bd3007772fe2cc69568cad2d882.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{86ab3cc76a17ec876b12bbaa6efcfa8c})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Cannot run 'newResourcesAvailable' because the actual state is Restarting and not ResourceConsumer.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution 843d0c154f55a15a9bb1e705ae282032.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (92b920a905c55fc85a76c79b3acef161) switched from CANCELING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (4/4) - execution #8738 to FAILED while being CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{1fde7e6e5c69ce0ac831b5bc7de6a90d})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot externally (SlotRequestId{1fde7e6e5c69ce0ac831b5bc7de6a90d})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Free reserved slot 3315697ecf20a1249d7dad268892bcc9.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Release shared slot (SlotRequestId{1fde7e6e5c69ce0ac831b5bc7de6a90d})
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Cannot run 'newResourcesAvailable' because the actual state is Restarting and not ResourceConsumer.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a) switched from state CANCELLING to CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - ExecutionGraph 4b5f437c7c47c8be9f8d8bf08e78910a reached terminal state CANCELED.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Stopping checkpoint coordinator for job 4b5f437c7c47c8be9f8d8bf08e78910a.
76126 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Discarding the results produced by task execution 92b920a905c55fc85a76c79b3acef161.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76126 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Ignoring scheduled action because expected state org.apache.flink.runtime.scheduler.adaptive.Executing@480dd446 is not the actual state org.apache.flink.runtime.scheduler.adaptive.Restarting@64a59f58.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Declare new resource requirements for job 4b5f437c7c47c8be9f8d8bf08e78910a.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=32768}]
	acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=4}}
76126 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76126 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state Restarting to WaitingForResources.
76127 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76127 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76127 [SIGINT handler] WARN  org.apache.flink.util.TestSignalHandler [] - RECEIVED SIGNAL 2: SIGINT. Shutting down as requested.
76127 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state WaitingForResources to CreatingExecutionGraph.
76127 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76127 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Running initialization on master for job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a).
76127 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution ba0dd94db26abc376ee73522410b8094.
76127 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Successfully ran initialization on master in 0 ms.
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Adding 1 vertices from job graph Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a).
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: Custom Source -> Sink: Unnamed) to 0 predecessors.
76127 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 92b920a905c55fc85a76c79b3acef161.
76127 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Cannot find task to stop for execution 843d0c154f55a15a9bb1e705ae282032.
76127 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 4 pipelined regions in 0 ms
76127 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Successfully created execution graph from job graph Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a).
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'totalNumberOfCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (3/4) - execution #8738 to FAILED while being CANCELED.
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numberOfInProgressCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numberOfCompletedCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'numberOfFailedCheckpoints'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointRestoreTimestamp'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointSize'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointDuration'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointProcessedData'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76127 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (4/4) - execution #8738 to FAILED while being CANCELED.
76127 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointPersistedData'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'lastCheckpointExternalPath'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@90aee29
76128 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76128 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Checkpoint storage is set to JobManager
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Ignoring transition of vertex Source: Custom Source -> Sink: Unnamed (2/4) - execution #8739 to FAILED while being CANCELED.
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'restartingTime'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'downtime'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] WARN  org.apache.flink.metrics.MetricGroup [] - Name collision: Group already contains a Metric with the name 'uptime'. Metric will not be reported.[localhost, jobmanager, Flink Streaming Job]
76128 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Status of the shared state registry of job 4b5f437c7c47c8be9f8d8bf08e78910a after restore: SharedStateRegistry{registeredStates={}}.
76128 [jobmanager-future-thread-9] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - No checkpoint found during restore.
76128 [jobmanager-future-thread-9] DEBUG org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Resetting the master hooks.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (4b5f437c7c47c8be9f8d8bf08e78910a) switched from state CREATED to RUNNING.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id aec00279d7404b26a104ee906695d27a.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{88b040e446ee408f792334a2ec437a42})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id 25dd5bd3007772fe2cc69568cad2d882.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{96939e4bb685186000b4001d96082081})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id dde6780a1f8df3d0b1b1b454e28f8566.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{c879c9aa7a7cb6dfbc12502ce7a8ed12})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.slotpool.DefaultAllocatedSlotPool [] - Reserve free slot with allocation id 3315697ecf20a1249d7dad268892bcc9.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Allocating logical slot from shared slot (SlotRequestId{53a25924bcd9fe2db23c22e0bf17effe})
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Successfully reserved and assigned the required slots for the ExecutionGraph.
76128 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state CreatingExecutionGraph to Executing.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (1/4) (9132ba5f6b087654fb351138ce74e710) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (1/4) (attempt #8740) with attempt id 9132ba5f6b087654fb351138ce74e710 to 3ebdf185-dcde-4ad2-b567-2f14c1b86fd1 @ localhost (dataPort=-1) with allocation id aec00279d7404b26a104ee906695d27a
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (7384c32213a4e9cd3aa6ee5875b1e532) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (2/4) (attempt #8740) with attempt id 7384c32213a4e9cd3aa6ee5875b1e532 to 3ebdf185-dcde-4ad2-b567-2f14c1b86fd1 @ localhost (dataPort=-1) with allocation id 25dd5bd3007772fe2cc69568cad2d882
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (497dd733deeb255e05bae82f0e41527d) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (3/4) (attempt #8739) with attempt id 497dd733deeb255e05bae82f0e41527d to 6cf04c09-5378-4bf3-aedd-e9a52076ec99 @ localhost (dataPort=-1) with allocation id dde6780a1f8df3d0b1b1b454e28f8566
76128 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot aec00279d7404b26a104ee906695d27a.
76128 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot dde6780a1f8df3d0b1b1b454e28f8566.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (639cf6da5847c7f4250839aeb2552df9) switched from CREATED to DEPLOYING.
76128 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> Sink: Unnamed (4/4) (attempt #8739) with attempt id 639cf6da5847c7f4250839aeb2552df9 to 6cf04c09-5378-4bf3-aedd-e9a52076ec99 @ localhost (dataPort=-1) with allocation id 3315697ecf20a1249d7dad268892bcc9
76128 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 2 under allocation id dde6780a1f8df3d0b1b1b454e28f8566: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@6d90093b
76128 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id aec00279d7404b26a104ee906695d27a: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@64e96e04
76128 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d), deploy into slot with allocation id dde6780a1f8df3d0b1b1b454e28f8566.
76129 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3315697ecf20a1249d7dad268892bcc9.
76129 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710), deploy into slot with allocation id aec00279d7404b26a104ee906695d27a.
76129 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) switched from CREATED to DEPLOYING.
76129 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) [DEPLOYING]
76129 [flink-akka.actor.default-dispatcher-2] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 3 under allocation id 3315697ecf20a1249d7dad268892bcc9: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@23a8b928
76130 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) switched from CREATED to DEPLOYING.
76130 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [DEPLOYING]
76130 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 25dd5bd3007772fe2cc69568cad2d882.
76130 [TransientBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.TransientBlobCache [] - Shutting down BLOB cache
76130 [TaskExecutorLocalStateStoresManager shutdown hook] INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
76130 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Found existing local state store for 4b5f437c7c47c8be9f8d8bf08e78910a - cbc357ccb763df2852fee8c4fc7d55f2 - 1 under allocation id 25dd5bd3007772fe2cc69568cad2d882: org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@389f2d5c
76130 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532), deploy into slot with allocation id 25dd5bd3007772fe2cc69568cad2d882.
76130 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9), deploy into slot with allocation id 3315697ecf20a1249d7dad268892bcc9.
76131 [PermanentBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.PermanentBlobCache [] - Shutting down BLOB cache
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) switched from CREATED to DEPLOYING.
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) [DEPLOYING]
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 497dd733deeb255e05bae82f0e41527d at library cache manager took 0 milliseconds
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 9132ba5f6b087654fb351138ce74e710 at library cache manager took 0 milliseconds
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 639cf6da5847c7f4250839aeb2552df9 at library cache manager took 0 milliseconds
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7b20c610
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@252878a9
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@30eb707b
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76131 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76131 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) switched from CREATED to DEPLOYING.
76131 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [DEPLOYING]
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [DEPLOYING].
76131 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Getting user code class loader for task 7384c32213a4e9cd3aa6ee5875b1e532 at library cache manager took 0 milliseconds
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [DEPLOYING].
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4c1406b8
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - The configuration state.checkpoint-storage has not be set in the current sessions flink-conf.yaml. Falling back to a default CheckpointStorage type. Users are strongly encouraged explicitly set this configuration so they understand how their applications are checkpointing snapshots for fault-tolerance.
76132 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask [] - Checkpoint storage is set to JobManager
76134 [FileChannelManagerImpl-io shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl [] - FileChannelManager removed spill file directory /var/folders/bd/6xl5m4z90j9438dv5bxg2n180000gn/T/junit2858931828265752695/junit7422691420779266555/flink-io-12fb17cf-712c-45f8-ab94-32fc7f5b5571
76136 [TaskExecutorLocalStateStoresManager shutdown hook] INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
76136 [IOManagerAsync shutdown hook] DEBUG org.apache.flink.runtime.io.disk.iomanager.IOManager [] - Shutting down I/O manager.
76136 [IOManagerAsync shutdown hook] DEBUG org.apache.flink.runtime.io.disk.iomanager.IOManager [] - Shutting down I/O manager.
76136 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (4/4)#8739.
76136 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (2/4)#8740.
76136 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) switched from DEPLOYING to RUNNING.
76136 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (1/4)#8740.
76136 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Initializing Source: Custom Source -> Sink: Unnamed (3/4)#8739.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (2/4) (7384c32213a4e9cd3aa6ee5875b1e532) switched from DEPLOYING to RUNNING.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (4/4) (639cf6da5847c7f4250839aeb2552df9) switched from DEPLOYING to RUNNING.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (1/4) (9132ba5f6b087654fb351138ce74e710) switched from DEPLOYING to RUNNING.
76136 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (3/4) (497dd733deeb255e05bae82f0e41527d) switched from DEPLOYING to RUNNING.
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (3/4)#8739
76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (4/4)#8739
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (1/4)#8740
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(3/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Invoking Source: Custom Source -> Sink: Unnamed (2/4)#8740
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(1/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(4/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for StreamSink_7df19f87deec5680128845fd9a6ca18d_(2/4) with empty state.
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (4/4)#8739 (639cf6da5847c7f4250839aeb2552df9).
76137 [Source: Custom Source -> Sink: Unnamed (4/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (4/4)#8739 network resources (state: FAILED).
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710).
76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (3/4)#8739 (497dd733deeb255e05bae82f0e41527d).
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (1/4)#8740 network resources (state: FAILED).
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] WARN  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228)
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)

76137 [Source: Custom Source -> Sink: Unnamed (3/4)#8739] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (3/4)#8739 network resources (state: FAILED).
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] INFO  org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532).
76137 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Release task Source: Custom Source -> Sink: Unnamed (2/4)#8740 network resources (state: FAILED).
76137 [FileCache shutdown hook] INFO  org.apache.flink.runtime.filecache.FileCache [] - removed file cache directory /var/folders/bd/6xl5m4z90j9438dv5bxg2n180000gn/T/junit2858931828265752695/junit7422691420779266555/flink-dist-cache-13391a9e-181b-4d3c-b373-0ac0203f301e
76137 [Source: Custom Source -> Sink: Unnamed (1/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 (9132ba5f6b087654fb351138ce74e710) [FAILED]
76138 [Source: Custom Source -> Sink: Unnamed (2/4)#8740] DEBUG org.apache.flink.runtime.taskmanager.Task [] - Ensuring all FileSystem streams are closed for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 (7384c32213a4e9cd3aa6ee5875b1e532) [FAILED]
76138 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Unnamed (1/4)#8740 9132ba5f6b087654fb351138ce74e710.
76138 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> Sink: Unnamed (2/4)#8740 7384c32213a4e9cd3aa6ee5875b1e532.
76138 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Sink: Unnamed (1/4) (9132ba5f6b087654fb351138ce74e710) switched from RUNNING to FAILED on 3ebdf185-dcde-4ad2-b567-2f14c1b86fd1 @ localhost (dataPort=-1).
java.lang.RuntimeException: Test error. More instances than expected.
	at org.apache.flink.test.scheduling.ReactiveModeITCase$InstanceTracker.reportNewInstance(ReactiveModeITCase.java:228) ~[test-classes/:?]
	at org.apache.flink.test.scheduling.ReactiveModeITCase$ParallelismTrackingSink.open(ReactiveModeITCase.java:215) ~[test-classes/:?]
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:437) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:550) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:540) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:580) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:760) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[classes/:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
[...]
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13468525	flink-parquet doesn't compile on M1 mac without rosetta	"Compiling Flink 1.16-SNAPSHOT fails on an M1 Mac (apple silicon) without the rosetta translation layer, because the automatically downloaded ""protoc-3.17.3-osx-aarch_64.exe"" file is actually just a copy of ""protoc-3.17.3-osx-x86_64.exe"". (as you can read here: https://github.com/os72/protoc-jar/issues/93)

This is the error:
{code}
[ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:test-compile (default) on project flink-parquet: An error occurred while invoking protoc. Error while executing process. Cannot run program ""/Users/rmetzger/Projects/flink/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.17.3-osx-aarch_64.exe"": error=86, Bad CPU type in executable -> [Help 1]
{code}

"	FLINK	Closed	3	7	8669	pull-request-available
13335614	YARNSessionCapacitySchedulerITCase.checkForProhibitedLogContents: netty failed with java.io.IOException: Broken pipe	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7666&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf
{code}
2020-10-15T10:41:39.3168991Z [ERROR] Failures: 
2020-10-15T10:41:39.3172085Z [ERROR]   YARNSessionCapacitySchedulerITCase.checkForProhibitedLogContents:650->YarnTestBase.ensureNoProhibitedStringInLogFiles:479 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-logDir-nm-1_0/application_1602757819968_0002/container_1602757819968_0002_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2020-10-15T10:41:39.3173934Z [
2020-10-15T10:41:39.3175238Z 2020-10-15 10:31:08,883 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 5a24db6b51d7c697fe30e49aa1a8412c from job leader monitoring.
2020-10-15T10:41:39.3176858Z 2020-10-15 10:31:08,885 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 5a24db6b51d7c697fe30e49aa1a8412c.
2020-10-15T10:41:39.3178614Z 2020-10-15 10:31:09,531 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                 [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2020-10-15T10:41:39.3199037Z 2020-10-15 10:31:09,538 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3202316Z 2020-10-15 10:31:09,573 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-io-fa9b2b85-fbe7-429c-85a9-6fcfb4811b01
2020-10-15T10:41:39.3204499Z 2020-10-15 10:31:09,577 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2020-10-15T10:41:39.3240288Z 2020-10-15 10:31:09,579 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-netty-shuffle-7e95809e-e862-45b1-892e-09835297c82d
2020-10-15T10:41:39.3244086Z 2020-10-15 10:31:09,594 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-dist-cache-ed288b32-5c40-48b9-9bcb-7186f5a7bfc2
2020-10-15T10:41:39.3246636Z 2020-10-15 10:31:09,596 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3248653Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3250746Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3253551Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3255688Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3267975Z ]
2020-10-15T10:41:39.3270637Z [ERROR]   YARNSessionCapacitySchedulerITCase.checkForProhibitedLogContents:650->YarnTestBase.ensureNoProhibitedStringInLogFiles:479 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-logDir-nm-1_0/application_1602757819968_0002/container_1602757819968_0002_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2020-10-15T10:41:39.3272364Z [
2020-10-15T10:41:39.3273848Z 2020-10-15 10:31:08,883 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 5a24db6b51d7c697fe30e49aa1a8412c from job leader monitoring.
2020-10-15T10:41:39.3275694Z 2020-10-15 10:31:08,885 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 5a24db6b51d7c697fe30e49aa1a8412c.
2020-10-15T10:41:39.3277365Z 2020-10-15 10:31:09,531 INFO  org.apache.flink.yarn.YarnTaskExecutorRunner                 [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2020-10-15T10:41:39.3278813Z 2020-10-15 10:31:09,538 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3281136Z 2020-10-15 10:31:09,573 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-io-fa9b2b85-fbe7-429c-85a9-6fcfb4811b01
2020-10-15T10:41:39.3283728Z 2020-10-15 10:31:09,577 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
2020-10-15T10:41:39.3285976Z 2020-10-15 10:31:09,579 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-netty-shuffle-7e95809e-e862-45b1-892e-09835297c82d
2020-10-15T10:41:39.3288937Z 2020-10-15 10:31:09,594 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-1_0/usercache/agent07_azpcontainer/appcache/application_1602757819968_0002/flink-dist-cache-ed288b32-5c40-48b9-9bcb-7186f5a7bfc2
2020-10-15T10:41:39.3291038Z 2020-10-15 10:31:09,596 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
2020-10-15T10:41:39.3293240Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3295639Z 2020-10-15 10:31:09,661 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3297824Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3299775Z 2020-10-15 10:31:09,662 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [61b81e62b514/192.168.128.2:39365] failed with java.io.IOException: Broken pipe
2020-10-15T10:41:39.3300726Z ]
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13217245	ES5: ElasticsearchSinkITCase failing	"On Google Cloud VMs (Ubuntu 18.04), the surefire tests are failing with the following error:
{code:java}
Running org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.972 sec <<< FAILURE! - in org.apache.flin
k.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase
org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase Time elapsed: 14.972 sec <<<
ERROR!
java.lang.IllegalStateException: No match found
at java.util.regex.Matcher.group(Matcher.java:536)
at org.elasticsearch.monitor.os.OsProbe.getControlGroups(OsProbe.java:213)
at org.elasticsearch.monitor.os.OsProbe.getCgroup(OsProbe.java:402)
at org.elasticsearch.monitor.os.OsProbe.osStats(OsProbe.java:454)
at org.elasticsearch.monitor.os.OsService.<init>(OsService.java:45)
at org.elasticsearch.monitor.MonitorService.<init>(MonitorService.java:45)
at org.elasticsearch.node.Node.<init>(Node.java:345)
at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl$Plug
inNode.<init>(EmbeddedElasticsearchNodeEnvironmentImpl.java:78)
at org.apache.flink.streaming.connectors.elasticsearch.EmbeddedElasticsearchNodeEnvironmentImpl.star
t(EmbeddedElasticsearchNodeEnvironmentImpl.java:54)
at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkTestBase.prepare(Elasticsear
chSinkTestBase.java:72)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}
or

{{org.apache.flink.streaming.connectors.elasticsearch5.ElasticsearchSinkITCase Time elapsed: 3.196 s <<< FAILURE!}}
 {{java.lang.AssertionError: 0::/user.slice/user-1001.slice/session-33.scope}}"	FLINK	Resolved	2	1	8669	pull-request-available, test-stability
13429354	SplitAggregateITCase.testAggWithJoin failed on azure	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31850&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10497]

Acknowledge of a checkpoint failed, then the checkpoint expired, then checkpoint failure threshold was reached and job failed.

{code}
Randomly selected true for execution.checkpointing.unaligned
Randomly selected PT2S for execution.checkpointing.alignment-timeout
Randomly selected true for state.backend.changelog.enabled
Randomly selected PT0.1S for state.backend.changelog.periodic-materialize.interval
{code}

{code}
[ERROR] Tests run: 64, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 700.545 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase
[ERROR] SplitAggregateITCase.testAggWithJoin  Time elapsed: 601.77 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
   at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
   at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniCl usterJobClient.java:141)
   at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
   at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
   at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandle r.java:259)
   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
   at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java :93)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadi ngUtils.java:68)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextCla ssLoader$2(ClassLoadingUtils.java:92)
   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
   at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
   at akka.dispatch.OnComplete.internal(Future.scala:300)
   at akka.dispatch.OnComplete.internal(Future.scala:297)
   at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
   at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
   at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
   at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFut ureUtils.java:65)
   at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
   at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
   at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
   at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
...
Caused by: org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
   at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(Checkpo intFailureManager.java:160)
   at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleJobLevelCheckpointException( CheckpointFailureManager.java:123)
   at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(Checkpoi ntFailureManager.java:90)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoor dinator.java:2046)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoor dinator.java:2025)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$600(CheckpointCoordinator.java :98)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoo rdinator.java:2104)
   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
   at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThread PoolExecutor.java:180)
   at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExe cutor.java:293)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
   at java.lang.Thread.run(Thread.java:748)
{code}

{code}
12:18:11,760 [jobmanager-io-thread-5] WARN  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Error while processing AcknowledgeCheckpoint message
java.lang.IllegalStateException: Attempt to reference unknown state: 4a798990-1428-424c-813a-2ec1c4fcee8f-KeyGroupRange{startKeyGroup=0, endKeyGroup=31}-000019.sst
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.SharedStateRegistryImpl.registerReference(SharedStateRegistryImpl.java:82) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.registerSharedStates(IncrementalRemoteKeyedStateHandle.java:317) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.SharedStateRegistryImpl.registerAll(SharedStateRegistryImpl.java:172) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle$ChangelogStateBackendHandleImpl.registerSharedStates(ChangelogStateBackendHandle.java:124) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.registerSharedState(OperatorSubtaskState.java:229) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.registerSharedStates(OperatorSubtaskState.java:219) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.TaskStateSnapshot.registerSharedStates(TaskStateSnapshot.java:189) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1114) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}
"	FLINK	Closed	1	1	8742	test-stability
13436941	Bump CopyOnWriteStateMap entry version before write	"CopyOnWriteStateMap copies the entry before returning it to the client
for update. This also updates its state and entry versions.

However, if the entry is NOT used by any snapshots, the versions
will stay the same despite that state is going to be updated.

With incremental checkpoints, this causes such updated version to
be ignored in the next snapshot."	FLINK	In Progress	3	7	8742	pull-request-available, stale-assigned
13333688	Implement distributed DSTL (DFS-based)	"Sub-tasks:
 # Implement scheduling and deployment (no rescaling)
 # Implement RPC between the tasks and DSTL (including load balancing)
 # Implement re-scaling
 # Implement filtering of records on replay to support up-scaling"	FLINK	Closed	3	2	8742	DSTL
13285302	Performance regression in ContinuousFileReaderOperator	"After switching CFRO to a single-threaded execution model performance regression was expected to be about 15-20% (benchmarked in November).

But after merging to master it turned out to be about 50%.

  

One reason is that the chaining strategy isn't set by default in CFRO factory.

Without that even reading and outputting all records of a split in a single mail action doesn't reverse the regression (only about half).

However,  with strategy set AND batching enabled fixes the regression (starting from batch size 6).

Though batching can't be used in practice because it can significantly delay checkpointing.

 

Another approach would be to process one record and the repeat until defaultMailboxActionAvailable OR haveNewMail.

This reverses regression and even improves the performance by about 50% compared to the old version.

 

The final solution could also be FLIP-27.

 

Other things tried (didn't help):
 * CFRO rework without subsequent commits (removing checkpoint lock)
 * different batch sizes, including the whole split, without chaining strategy fixed - partial improvement only
 * disabling close
 * disabling checkpointing
 * disabling output (serialization)
 * using LinkedList instead of PriorityQueue

 "	FLINK	Closed	1	1	8742	pull-request-available
13357977	Send changes to the state changelog (still proxy everything)	"Subtasks:
 # -Changelog instantiation (including configuration)- (FLINK-21804)
 # Connecting Changelog with ProxyBackend
 # Connecting Proxy-State objects with Changelog
 # Serializing changes in ProxyState objects and sending changes to Changelog
 # no metadata logging (FLINK-22808)
 # Unit test coverage"	FLINK	Closed	3	7	8742	pull-request-available
13423016	TaskExecutorStateChangelogStoragesManager.shutdown is not thread-safe	"[https://github.com/apache/flink/pull/18169#discussion_r785741977]

The method is called from the shutdown hook and therefore should be thread-safe.

cc: [~Zakelly] , [~dmvk] "	FLINK	Resolved	3	1	8742	pull-request-available
13499651	Allow sharing (RocksDB) memory between slots	"h1. Background and motivation

RocksDB is one of the main consumers of off-heap memory, which it uses for BlockCache, MemTables, Indices and Bloom Filters.
Since 1.10 (FLINK-7289), it is possible to:
 - share these objects among RocksDB instances of the same slot
 - bound the total memory usage by all RocksDB instances of a TM

The memory is divided between the slots equally (unless using fine-grained resource control).
This is sub-optimal if some slots contain more memory intensive tasks than the others.

The proposal is to widen the scope of sharing memory to TM, so that it can be shared across all of its RocksDB instances.
That would reduce the overall memory consumption in exchange for resource isolation.
h1. Proposed changes
h2. Configuration
 - introduce ""state.backend.rocksdb.memory.fixed-per-tm"" (memory size, no default)
 -- cluster-level (yaml only)
 -- used by a job only if neither 'state.backend.rocksdb.memory.fixed-per-slot' nor 'state.backend.rocksdb.memory.fixed-per-slot' are not used for the job
 -- use cluster-level or default configuration when creating TM-wise shared RocksDB objects, e.g.  ""state.backend.rocksdb.memory.managed"", ""state.backend.rocksdb.memory.write-buffer-ratio""
 -- doesn't affect Flink memory calculations; user needs to take it into account when planning capacity (similar to fixed-per-slot)

h2. Example
{code:java}
# cluster-level configuration
taskmanager.memory.managed.size: 1gb
state.backend.rocksdb.memory.fixed-per-tm: 1gb
taskmanager.numberOfTaskSlots: 10
cluster.fine-grained-resource-management.enabled: false

# job 1:
state.backend.rocksdb.memory.managed: false # uses shared TM memory

# job 2:
state.backend.rocksdb.memory.managed: false # uses shared TM memory

# job 3:
state.backend.rocksdb.memory.managed: true # uses exclusive managed memory

# job 4:
state.backend.rocksdb.memory.managed: true # gets overriden below
state.backend.rocksdb.memory.fixed-per-slot: 50M # uses exclusive unmanaged memory

{code}
Jobs 1 and2 will use the same 1Gb of shared unmanaged memory and will compete with each other.
Their Python code (or other consumers) will be able to use up to ~100Mb per slot.

Jobs 3 and 4 are not affected as they specify using managed (3) or fixed-per-slot memory (4).
Python code (or other consumers) will be able to use up to ~100Mb per slot but will compete with RocksDB in job (3).

h2. Creating and sharing RocksDB objects

Introduce sharedResources to TaskManager.
Then, similarly to the current slot-wise sharing using MemoryManager:
 - put/get OpaqueMemoryResource
 - Creation of Cache object is done from the backend code on the first call
 - Release it when the last backend that uses it is destroyed
So flink-runtime doesn't have to depend on state backend.

h2. Class loading and resolution

RocksDB state backend is already a part of the distribution.
However, if a job also includes it then classloader.resolve-order should be set to parent-first to prevent conflicts.
h2. Lifecycle

The cache object should be destroyed on TM termnation; job or task completion should NOT close it.
h1. Testing
 * One way to test that the same RocksDB cache is used is via RocksDB metrics.
- -ITCases parameterization-
- manual and unit tests

h1. Limitations
 - classloader.resolve-order=child-first is not supported
 - fine-grained-resource-management is not supported
 - only RocksDB will be able to use TM-wise shared memory; other consumers may be adjusted later

h1. Rejected alternatives
 - set total ""fixed-per-slot"" to a larger value, essentially overcommitting unmanaged memory - doesn't work well in containerized environments (OOMErrors)
 - set numberOfTaskSlots=1 and allow sharing the same slot between any tasks - requires more invasive changes in scheduler and TM
- make part of managed memory shared; it is beleived that managed memory must preserve isolation proprty among other concerns

cc: [~yunta], [~ym], [~liyu]"	FLINK	Closed	3	2	8742	pull-request-available
13323902	"E2E test fails with ""Cannot register Closeable, this subtaskCheckpointCoordinator is already closed. Closing argument."""	"Note: This error occurred in a custom branch with unreviewed changes. I don't believe my changes affect this error, but I would keep this in mind when investigating the error: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8307&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=0d2e35fc-a330-5cf2-a012-7267e2667b1d
 
{code}
2020-08-20T20:55:30.2400645Z 2020-08-20 20:55:22,373 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
2020-08-20T20:55:30.2402392Z 2020-08-20 20:55:22,401 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-08-20T20:55:30.2404297Z 2020-08-20 20:55:22,413 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to RUNNING.
2020-08-20T20:55:30.2405805Z 2020-08-20 20:55:22,786 INFO  org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge [] - Pinging Elasticsearch cluster via hosts [http://127.0.0.1:9200] ...
2020-08-20T20:55:30.2407027Z 2020-08-20 20:55:22,848 INFO  org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge [] - Elasticsearch RestHighLevelClient is connected to [http://127.0.0.1:9200]
2020-08-20T20:55:30.2409277Z 2020-08-20 20:55:29,205 INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) discarding 0 drained requests
2020-08-20T20:55:30.2410690Z 2020-08-20 20:55:29,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FINISHED.
2020-08-20T20:55:30.2412187Z 2020-08-20 20:55:29,218 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) (cbc357ccb763df2852fee8c4fc7d55f2_0_0).
2020-08-20T20:55:30.2414203Z 2020-08-20 20:55:29,224 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) cbc357ccb763df2852fee8c4fc7d55f2_0_0.
2020-08-20T20:55:30.2415602Z 2020-08-20 20:55:29,219 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) - asynchronous part of checkpoint 1 could not be completed.
2020-08-20T20:55:30.2416411Z java.io.UncheckedIOException: java.io.IOException: Cannot register Closeable, this subtaskCheckpointCoordinator is already closed. Closing argument.
2020-08-20T20:55:30.2418956Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.lambda$registerConsumer$2(SubtaskCheckpointCoordinatorImpl.java:468) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2420100Z 	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:91) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2420927Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_265]
2020-08-20T20:55:30.2421455Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_265]
2020-08-20T20:55:30.2421879Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_265]
2020-08-20T20:55:30.2422348Z Caused by: java.io.IOException: Cannot register Closeable, this subtaskCheckpointCoordinator is already closed. Closing argument.
2020-08-20T20:55:30.2423416Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.registerAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:378) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2424635Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.lambda$registerConsumer$2(SubtaskCheckpointCoordinatorImpl.java:466) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-20T20:55:30.2425174Z 	... 4 more
2020-08-20T20:55:30.2426945Z 2020-08-20 20:55:29,339 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: f5938e3baed0f9564aa17169f68947bd, jobId: 333b1654bd93574471bd44ad45847379).
2020-08-20T20:55:30.2428701Z 2020-08-20 20:55:29,354 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 333b1654bd93574471bd44ad45847379 from job leader monitoring.
{code}"	FLINK	Closed	2	1	8742	pull-request-available, test-stability
13436936	Allow reuse of PeriodicMaterializationManager	"The same approach that is used with Changelog can be
used with other state backends too,
in particular IncrementalHeapStateBackend (FLIP-151)."	FLINK	In Progress	3	7	8742	pull-request-available, stale-assigned
13336925	PartitionRequestClientFactoryTest.testInterruptsNotCached fails with NullPointerException	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8517&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374

{code}
2020-10-23T13:25:12.0774554Z [ERROR] testInterruptsNotCached(org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest)  Time elapsed: 0.762 s  <<< ERROR!
2020-10-23T13:25:12.0775695Z java.io.IOException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0776455Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:95)
2020-10-23T13:25:12.0777038Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.testInterruptsNotCached(PartitionRequestClientFactoryTest.java:72)
2020-10-23T13:25:12.0777465Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-23T13:25:12.0777815Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-23T13:25:12.0778221Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-23T13:25:12.0778581Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-23T13:25:12.0778921Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-23T13:25:12.0779331Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-23T13:25:12.0779733Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-23T13:25:12.0780117Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-23T13:25:12.0780484Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-23T13:25:12.0780851Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-23T13:25:12.0781236Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-23T13:25:12.0781600Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-23T13:25:12.0781937Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-23T13:25:12.0782431Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-23T13:25:12.0782877Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-23T13:25:12.0783223Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-23T13:25:12.0783541Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-23T13:25:12.0783905Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-23T13:25:12.0784315Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-23T13:25:12.0784718Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-23T13:25:12.0785125Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-23T13:25:12.0785552Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-23T13:25:12.0785980Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-23T13:25:12.0786379Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-23T13:25:12.0786763Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-23T13:25:12.0787922Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0788575Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-10-23T13:25:12.0788954Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-10-23T13:25:12.0789431Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:88)
2020-10-23T13:25:12.0789808Z 	... 26 more
2020-10-23T13:25:12.0790546Z Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0791396Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:134)
2020-10-23T13:25:12.0791959Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:111)
2020-10-23T13:25:12.0792732Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:77)
2020-10-23T13:25:12.0793118Z 	... 26 more
2020-10-23T13:25:12.0793342Z Caused by: java.lang.NullPointerException
2020-10-23T13:25:12.0793681Z 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:61)
2020-10-23T13:25:12.0794319Z 	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.<init>(NettyPartitionRequestClient.java:73)
2020-10-23T13:25:12.0794854Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:126)
{code}
"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13374533	testScheduleRunAsync fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=6833


{code:java}
Apr 22 22:56:40 [ERROR] testScheduleRunAsync(org.apache.flink.runtime.rpc.RpcEndpointTest)  Time elapsed: 0.404 s  <<< FAILURE!
Apr 22 22:56:40 java.lang.AssertionError
Apr 22 22:56:40 	at org.junit.Assert.fail(Assert.java:86)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:41)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:52)
Apr 22 22:56:40 	at org.apache.flink.runtime.rpc.RpcEndpointTest.testScheduleRunAsync(RpcEndpointTest.java:318)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 22 22:56:40 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 22 22:56:40 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 22 22:56:40 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 22 22:56:40 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 22 22:56:40 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 22 22:56:40 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)

{code}
"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13435060	ChangelogRescalingITCase.test failed on azure due to java.nio.file.NoSuchFileException	"
{code:java}
Mar 21 17:33:56 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.589 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogRescalingITCase
Mar 21 17:33:56 [ERROR] ChangelogRescalingITCase.test  Time elapsed: 8.392 s  <<< ERROR!
Mar 21 17:33:56 java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit4908969673123504454/junit6297505939941694356/d832f597d0b0414695fa746ffc400bb2/chk-43
Mar 21 17:33:56 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
Mar 21 17:33:56 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
Mar 21 17:33:56 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
Mar 21 17:33:56 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Mar 21 17:33:56 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Mar 21 17:33:56 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Mar 21 17:33:56 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
Mar 21 17:33:56 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Mar 21 17:33:56 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
Mar 21 17:33:56 	at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:582)
Mar 21 17:33:56 	at org.apache.flink.test.util.TestUtils.getMostRecentCompletedCheckpointMaybe(TestUtils.java:114)
Mar 21 17:33:56 	at org.apache.flink.test.state.ChangelogRescalingITCase.checkpointAndCancel(ChangelogRescalingITCase.java:333)
Mar 21 17:33:56 	at org.apache.flink.test.state.ChangelogRescalingITCase.test(ChangelogRescalingITCase.java:156)
Mar 21 17:33:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 21 17:33:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 21 17:33:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 21 17:33:56 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 21 17:33:56 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 21 17:33:56 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 21 17:33:56 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 21 17:33:56 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Mar 21 17:33:56 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Mar 21 17:33:56 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Mar 21 17:33:56 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Mar 21 17:33:56 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Mar 21 17:33:56 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 21 17:33:56 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Mar 21 17:33:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Mar 21 17:33:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Mar 21 17:33:56 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)

{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33515&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5643"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13353440	SemanticXidGeneratorTest.testXidsUniqueAmongGenerators test failed	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12245&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20]
{code:java}
2021-01-19T15:21:35.5665063Z [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.1 s <<< FAILURE! - in org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest
2021-01-19T15:21:35.5665936Z [ERROR] testXidsUniqueAmongGenerators(org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest)  Time elapsed: 0.024 s  <<< FAILURE!
2021-01-19T15:21:35.5666770Z junit.framework.AssertionFailedError: expected:<10000> but was:<9999>
{code}"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13438883	PartiallyFinishedSourcesITCase.test hangs on azure	"
{code:java}
Apr 10 08:32:18 ""main"" #1 prio=5 os_prio=0 tid=0x00007f553400b800 nid=0x8345 waiting on condition [0x00007f553be60000]
Apr 10 08:32:18    java.lang.Thread.State: TIMED_WAITING (sleeping)
Apr 10 08:32:18 	at java.lang.Thread.sleep(Native Method)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
Apr 10 08:32:18 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
Apr 10 08:32:18 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
Apr 10 08:32:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 10 08:32:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 10 08:32:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 10 08:32:18 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 10 08:32:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Apr 10 08:32:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 10 08:32:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 10 08:32:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Apr 10 08:32:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 10 08:32:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Apr 10 08:32:18 	at org.junit.runners.Suite.runChild(Suite.java:128)
Apr 10 08:32:18 	at org.junit.runners.Suite.runChild(Suite.java:27)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34484&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=6757"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13571496	JobIDLoggingITCase failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58187&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7897

{code}
Mar 09 01:24:23 01:24:23.498 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.209 s <<< FAILURE! -- in org.apache.flink.test.misc.JobIDLoggingITCase
Mar 09 01:24:23 01:24:23.498 [ERROR] org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(ClusterClient) -- Time elapsed: 1.459 s <<< ERROR!
Mar 09 01:24:23 java.lang.IllegalStateException: Too few log events recorded for org.apache.flink.runtime.jobmaster.JobMaster (12) - this must be a bug in the test code
Mar 09 01:24:23 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
Mar 09 01:24:23 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:148)
Mar 09 01:24:23 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:132)
Mar 09 01:24:23 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 09 01:24:23 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Mar 09 01:24:23 
{code}

The other test failures of this build were also caused by the same test:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58187&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8349
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58187&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8209"	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13337630	Managed memory released check can block IterativeTask	"UnsafeMemoryBudget#reserveMemory, called on TempBarrier, needs time to wait on GC of all allocated/released managed memory at every iteration.

 

stack:

!image-2020-10-28-17-48-48-583.png!

new TempBarrier in BatchTask

!image-2020-10-28-17-48-28-395.png!

 

These will be very slow than before."	FLINK	Resolved	2	1	8742	pull-request-available
13373768	UnalignedCheckpointITCase hangs on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16818&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&l=10144
"	FLINK	Resolved	1	1	8742	pull-request-available, test-stability
13323402	Optimize reading of channel state on recovery	"Curently, channel state is read not sequentially.

Inverting control would make it more efficient.

Current call chain: 
{code:java}
StreamTask.readRecoveredChannelState  
    ResultPartition.readRecoveredState - loop through subpartitions
        PipelinedSubpartition.readRecoveredState - loop while have data; bufferBuilder = parent.getBufferPool().requestBufferBuilderBlocking(subpartitionInfo.getSubPartitionIdx());
            ChannelStateReader.readOutputData   {code}
Proposed call chain:
{code:java}
StreamTask.readRecoveredChannelState
    ChannelStateReader.readOutputData loop through state handles ordererd by handle, offset
        request buffer in the same way: BufferBuilder bufferBuilder = resPart.getBufferPool().requestBufferBuilderBlocking(subpartitionInfo.getSubPartitionIdx());
        pass to resPart.getSubpartition(idx).add(BufferConsumer, boolean, boolean)
{code}
 "	FLINK	Resolved	3	7	8742	pull-request-available
13357345	SavepointITCase.testStopSavepointWithBoundedInputConcurrently is unstable	https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13036&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2	FLINK	Resolved	3	1	8742	pull-request-available
13368839	SavepointWindowReaderITCase.testApplyEvictorWindowStateReader	"The test case {{SavepointWindowReaderITCase.testApplyEvictorWindowStateReader}} failed on AZP with:

{code}

	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:69)
	... 33 more
Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
	at com.sun.proxy.$Proxy32.triggerSavepoint(Unknown Source)
	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:716)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
	at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:714)
	at org.apache.flink.client.program.MiniClusterClient.triggerSavepoint(MiniClusterClient.java:101)
	at org.apache.flink.state.api.utils.SavepointTestBase.triggerSavepoint(SavepointTestBase.java:93)
	at org.apache.flink.state.api.utils.SavepointTestBase.lambda$takeSavepoint$0(SavepointTestBase.java:68)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1646)
	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#-390276455]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15809&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=9197"	FLINK	Resolved	1	1	8742	auto-deprioritized-critical, pull-request-available, test-stability
13248276	Instable KafkaProducerExactlyOnceITCase due to CheckpointFailureManager	"[~banmoy] and I met this instable test below:

[https://api.travis-ci.org/v3/job/565270958/log.txt]
 [https://api.travis-ci.com/v3/job/221237628/log.txt]

The root cause is task {{Source: Custom Source -> Map -> Sink: Unnamed (1/1)}} failed due to expected artificial test failure and then free task resource including closing the registry. However, the async checkpoint thread in {{SourceStreamTask}} would then failed and send decline checkpoint message to JM.
 The key logs is like:
{code:java}
03:36:46,639 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Custom Source -> Map -> Sink: Unnamed (1/1) (f45ff068d2c80da22c2a958739ec0c87) switched from RUNNING to FAILED.
java.lang.Exception: Artificial Test Failure
	at org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.map(FailingIdentityMapper.java:79)
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:637)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:612)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:592)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:727)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:705)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.run(IntegerSource.java:75)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:172)
03:36:46,637 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Decline checkpoint 12 by task f45ff068d2c80da22c2a958739ec0c87 of job d5b629623731c66f1bac89dec3e87b89 at 03cbfd77-0727-4366-83c4-9aa4923fc817 @ localhost (dataPort=-1).
03:36:46,640 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Discarding checkpoint 12 of job d5b629623731c66f1bac89dec3e87b89.
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 12 for operator Source: Custom Source -> Map -> Sink: Unnamed (1/1). Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:431)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1248)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1182)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:853)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:758)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:667)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.triggerCheckpoint(SourceStreamTask.java:147)
	at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot register Closeable, registry is already closed. Closing argument.
	at org.apache.flink.util.AbstractCloseableRegistry.registerCloseable(AbstractCloseableRegistry.java:85)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:122)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:110)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable.toAsyncSnapshotFutureTask(AsyncSnapshotCallable.java:104)
	at org.apache.flink.runtime.state.StateSnapshotContextSynchronousImpl.getKeyedStateStreamFuture(StateSnapshotContextSynchronousImpl.java:127)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:401)
	... 12 more
03:36:46,642 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Exactly once test (d5b629623731c66f1bac89dec3e87b89) switched from state RUNNING to FAILING.
java.lang.Exception: Artificial Test Failure
	at org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.map(FailingIdentityMapper.java:79)
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:637)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:612)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:592)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:727)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:705)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.run(IntegerSource.java:75)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:172)
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Try to restart or fail the job Exactly once test (d5b629623731c66f1bac89dec3e87b89) if no longer possible.
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Exactly once test (d5b629623731c66f1bac89dec3e87b89) switched from state FAILING to RESTARTING.
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Restarting the job Exactly once test (d5b629623731c66f1bac89dec3e87b89).
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Discarding the results produced by task execution f45ff068d2c80da22c2a958739ec0c87.
03:36:46,644 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Exactly once test (d5b629623731c66f1bac89dec3e87b89) switched from state RESTARTING to FAILING.
org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.lambda$null$1(ExecutionGraph.java:586)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}
The failure of {{Source: Custom Source -> Map -> Sink: Unnamed}} would fail the job for the 1st time. However, due to receive declined checkpoint {{CheckpointFailureManager}} would also fail the job again for the 2nd time. Unfortunately, some tests within {{KafkaProducerExactlyOnceITCase}} only allow one restart attempt by {{FixedDelayRestartStrategy}}, that's why the IT case failed at last."	FLINK	Closed	2	1	8742	test-stability
13472897	Remove separate error handling and adjust documentation for CLAIM mode + RocksDB native savepoint	"After FLINK-25872, checkpoint folder deletion is not performed as long as there is some state from that checkpoint used by other checkpoints.
Therefore, the following changes could be reverted/adjusted:
* FLINK-25745 e8bcbfd5a48fd8d3ca48ef7803867569214e0dbc Do not log exception
* FLINK-25745 c1f5c5320150402fc0cb4fbf3a31f9a27b1e4d9a Document incremental savepoints in CLAIM mode limitation

cc: [~Yanfei Lei], [~dwysakowicz]"	FLINK	Closed	3	4	8742	pull-request-available
13272761	InternalTimerServiceImpl references restored state after use, taking up resources unnecessarily	"E.g. org.apache.flink.streaming.api.operators.InternalTimerServiceImpl#restoredTimersSnapshot:
 # written in restoreTimersForKeyGroup()

 # used in startTimerService()

 # and then never used again.

 "	FLINK	Resolved	4	4	8742	pull-request-available
13570453	TVF Window Aggregations might get stuck	"RecordsWindowBuffer flushes buffered records in the following cases:
 * watermark
 * checkpoint barrier
 * buffer overflow

 

In two-phase aggregations, this creates the following problems:

1) Local aggregation: enters hard-backpressure because for flush, it outputs the data downstream and doesn't check network buffer availability

This already disrupts normal checkpointing and watermarks progression

 

2) Global aggregation: 

When the window is large enough and/or the watermark is lagging, lots of data is flushed to state backend (and the state is updated) in checkpoint SYNC phase.

 

All this eventually causes checkpoint timeouts (10 minutes in our env).

 

Example query
{code:java}
INSERT INTO `target_table` 

SELECT window_start, window_end, some, attributes, SUM(view_time) AS total_view_time, COUNT(*) AS num, LISTAGG(DISTINCT page_url) AS pages 

FROM TABLE(TUMBLE(TABLE source_table, DESCRIPTOR($rowtime), INTERVAL '1' HOUR)) 

GROUP BY window_start, window_end, some, attributes;{code}
In our setup, the issue can be reproduced deterministically.

 

As a quick fix, we might want to:
 # limit the amount of data buffered in Global Aggregation nodes
 # disable two-phase aggregations, i.e. Local Aggregations (we can try to limit buffing there two, but network buffer availability can not be easily checked from the operator)"	FLINK	In Progress	3	1	8742	pull-request-available
13387815	FileReadingWatermarkITCase.testWatermarkEmissionWithChaining fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19942&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0&l=5584

{code}
Jul 05 22:19:00 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.334 s <<< FAILURE! - in org.apache.flink.test.streaming.api.FileReadingWatermarkITCase
Jul 05 22:19:00 [ERROR] testWatermarkEmissionWithChaining(org.apache.flink.test.streaming.api.FileReadingWatermarkITCase)  Time elapsed: 4.16 s  <<< FAILURE!
Jul 05 22:19:00 java.lang.AssertionError: too few watermarks emitted: 4
Jul 05 22:19:00 	at org.junit.Assert.fail(Assert.java:89)
Jul 05 22:19:00 	at org.junit.Assert.assertTrue(Assert.java:42)
Jul 05 22:19:00 	at org.apache.flink.test.streaming.api.FileReadingWatermarkITCase.testWatermarkEmissionWithChaining(FileReadingWatermarkITCase.java:65)
Jul 05 22:19:00 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 05 22:19:00 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 05 22:19:00 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 05 22:19:00 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 05 22:19:00 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 05 22:19:00 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 05 22:19:00 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 05 22:19:00 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 05 22:19:00 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 05 22:19:00 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 05 22:19:00 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13509855	TaskExecutorTest.testSharedResourcesLifecycle failed with TaskException	"This seems to be a follow-up of FLINK-30275. Same test but different test failure (2x in the same build):
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7479
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7852
{code}
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
Dec 05 03:59:18 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Dec 05 03:59:18 Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskException: Cannot find task to stop for execution 096b33c46c225fd4af41a9484b64c7fe_010f83ce510d70707aaf04c441173b70_0_0.
Dec 05 03:59:18 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.cancelTask(TaskExecutor.java:864)
Dec 05 03:59:18 	... 53 more
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13387896	Enable changelog backend in tests	FLINK-21448 adds the capability (test randomization), but it can't be turned on as there are some test failures: FLINK-23276, FLINK-23277, FLINK-23278 (should be enabled after those bugs fixed)..	FLINK	Closed	3	7	8742	pull-request-available
13427582	[Changelog] Non-deterministic recovery of PriorityQueue states	"Currently, InternalPriorityQueue.poll() is logged as a separate operation, without specifying the element that has been polled. On recovery, this recorded poll() is replayed.

However, this is not deterministic because the order of PQ elements with equal priorityis not specified. For example, TimerHeapInternalTimer only compares timestamps, which are often equal. This results in polling timers from queue in wrong order => dropping timers => and not firing timers.

 

ProcessingTimeWindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow fails with materialization enabled and using heap state backend (both in-memory and fs-based implementations).

 

Proposed solution is to replace poll with remove operation (which is based on equality).
 
cc: [~masteryhx], [~ym], [~yunta]"	FLINK	Resolved	3	1	8742	pull-request-available
13394871	Hide any configuration, API or docs	"As the feature will not make it to the upcoming 1.14 release,

hide the related config options like CheckpointingOptions.ENABLE_STATE_CHANGE_LOG, API  (e.g. StreamExecutionEnvironment.enableChangelogStateBackend).

Also check Python and Scala APIs and the documentation."	FLINK	Closed	2	7	8742	pull-request-available
13357954	Incremental checkpoint data would be lost once a non-stop savepoint completed	"FLINK-10354 counted savepoint as retained checkpoint so that job could failover from latest position. I think this operation is reasonable, however, current implementation would let incremental checkpoint data lost immediately once a non-stop savepoint completed.

Current general phase of incremental checkpoints: once a newer checkpoint completed, it would be added to checkpoint store. And if the size of completed checkpoints larger than max retained limit, it would subsume the oldest one. This lead to the reference of incremental data decrease one and data would be deleted once reference reached to zero. As we always ensure to register newer checkpoint and then unregister older checkpoint, current phase works fine as expected.

However, if a non-stop savepoint (a median manual trigger savepoint) is completed, it would be also added into checkpoint store and just subsume previous added checkpoint (in default retain one checkpoint case), which would unregister older checkpoint without newer checkpoint registered, leading to data lost.

Thanks for [~banmoy] reporting this problem first."	FLINK	Resolved	1	1	8742	pull-request-available
13587360	Skip distributing maxAllowedWatermark if there are no subtasks	On JM, `SourceCoordinator.announceCombinedWatermark` executes unnecessary if there are no subtasks to distribute maxAllowedWatermark. This involves Heap and ConcurrentHashMap accesses and lots of logging.	FLINK	Closed	3	4	8742	pull-request-available
13398480	Do not re-register SharedStateRegistry to reduce the recovery time of the job	"At present, we only recover the {{CompletedCheckpointStore}} when the {{JobManager}} starts, so it seems that we do not need to re-register the {{SharedStateRegistry}} when the task restarts.

The reason for this issue is that in our production environment, we discard part of the data and state to only restart the failed task, but found that it may take several seconds to register the {{SharedStateRegistry}} (thousands of tasks and dozens of TB states). When there are a large number of task failures at the same time, this may take several minutes (number of tasks * several seconds).

Therefore, if the {{SharedStateRegistry}} can be reused, the time for task recovery can be reduced."	FLINK	Resolved	3	4	8742	pull-request-available
13393606	BatchingStateChangeUploaderTest.testDelay fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21518&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22202

{code}
Aug 04 16:31:14 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.616 s <<< FAILURE! - in org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest
Aug 04 16:31:14 [ERROR] testDelay(org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest)  Time elapsed: 0.198 s  <<< FAILURE!
Aug 04 16:31:14 java.lang.AssertionError: expected:<[logId=96102f85-c8a5-454a-9651-14bc1c6b0858, sequenceNumber=0, changes=[keyGroup=0, dataSize=4]]> but was:<[]>
Aug 04 16:31:14 	at org.junit.Assert.fail(Assert.java:89)
Aug 04 16:31:14 	at org.junit.Assert.failNotEquals(Assert.java:835)
Aug 04 16:31:14 	at org.junit.Assert.assertEquals(Assert.java:120)
Aug 04 16:31:14 	at org.junit.Assert.assertEquals(Assert.java:146)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.lambda$testDelay$4(BatchingStateChangeUploaderTest.java:105)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.withStore(BatchingStateChangeUploaderTest.java:210)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.testDelay(BatchingStateChangeUploaderTest.java:97)
Aug 04 16:31:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 04 16:31:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 04 16:31:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 04 16:31:14 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 04 16:31:14 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 04 16:31:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 04 16:31:14 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 04 16:31:14 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13437214	With legacy restore mode, incremental checkpoints would be deleted by mistake	"Before flink-1.15, the restored checkpoint would be regsiterd and not discard on subsume, which means the restored incremental checkpoint would have one more reference counting to avoid discard.

However, after state registry refactored, we could delete artificats in the restored incremental checkpoint with legacy restore mode.

The error could be reproduced via {{ResumeCheckpointManuallyITCase#testExternalizedIncrementalRocksDBCheckpointsStandalone}} in my [local branch|https://github.com/Myasuka/flink/tree/legacy-error].


Thanks for [~masteryhx] who found this problem in the manual test."	FLINK	Resolved	1	1	8742	pull-request-available
13353899	Add E2E/ITCase test for checkpoints after tasks finished	"We should write an E2e test for the feature. The corner cases should include:
* must check for committing side effects (notifyCheckpointComplete() was called)
* BroadcastState
* both non keyed followed by keyed exchanges? 
* aligned/unaligned checkpoints? (with unaligned checkpoint keyed operators/subtasks can be partially finished, which is very very rare in aligned checkpoints) - maybe it’s good enough to test only unaligned checkpoints."	FLINK	Closed	3	7	8742	pull-request-available
13396105	Remove limitation about JDBC exactly once sink about multiple connections per transaction 	"[https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/datastream/jdbc/#jdbcsinkexactlyoncesink]
 says:
{quote}Attention: In 1.13, Flink JDBC sink does not support exactly-once mode with MySQL or other databases that do not support multiple XA transaction per connection. We will improve the support in FLINK-22239.
{quote}
 

In FLINK-22239, connection pooling was added so this limitation doesn't apply anymore.
 So this should be removed or replaced with a list of databases.

Connection pooling should be enabled explicitly: should list DBs requiring this (mysql, postgres) and give an example code."	FLINK	Closed	1	4	8742	pull-request-available
13567207	Wrong JobID in CheckpointStatsTracker	"The job id is generated randomly:
```
    public CheckpointStatsTracker(int numRememberedCheckpoints, MetricGroup metricGroup) {
        this(numRememberedCheckpoints, metricGroup, new JobID(), Integer.MAX_VALUE);
    }
```
This affects how it is logged (or reported elsewhere)."	FLINK	Closed	4	1	8742	pull-request-available
13423852	Include changelog jars into distribution	"Add changelog jars to dist/opt folder:
- flink-dstl-dfs - so users can add it to plugins/ easily (plugin, cluster level)
- flink-statebackend-changelog - so that it can be added to lib/ if needed (not plugin, cluster or job-level)

Update docs if done after FLINK-25024.

cc: [~chesnay]"	FLINK	Resolved	3	7	8742	pull-request-available
13509213	TaskExecutorTest.testSharedResourcesLifecycle fails	"We observe a test failure in {{TaskExecutorTest.testSharedResourcesLifecycle}}:
{code}
Dec 02 03:35:32 [ERROR] Tests run: 37, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.604 s <<< FAILURE! - in org.apache.flink.runtime.taskexecutor.TaskExecutorTest
Dec 02 03:35:32 [ERROR] org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testSharedResourcesLifecycle  Time elapsed: 0.549 s  <<< FAILURE!
Dec 02 03:35:32 java.lang.AssertionError: expected:<0> but was:<1>
Dec 02 03:35:32 	at org.junit.Assert.fail(Assert.java:89)
Dec 02 03:35:32 	at org.junit.Assert.failNotEquals(Assert.java:835)
Dec 02 03:35:32 	at org.junit.Assert.assertEquals(Assert.java:647)
Dec 02 03:35:32 	at org.junit.Assert.assertEquals(Assert.java:633)
Dec 02 03:35:32 	at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testSharedResourcesLifecycle(TaskExecutorTest.java:3080)
[...]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43662&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7466"	FLINK	Reopened	2	1	8742	pull-request-available, stale-assigned, test-stability
13291142	Extract static classes from StreamTask	"StreamTask is currently 1400+ LOC.

We can cut it to 1100+ by simply extracting these static classes into separate files:
 * `CheckpointingOperation`
 * `AsyncCheckpointRunnable`"	FLINK	Closed	4	4	8742	pull-request-available
13340030	Race conditions in InputChannel.ChannelStatePersister	"In InputChannel.ChannelStatePersister, stopPersisting() and checkForBarrier() always update pendingCheckpointBarrierId, potentially overwriting newer id (or BARRIER_RECEIVED value) with an old one.


For stopPersisting(), consider a case:
 # Two consecutive UC barriers arrive at the same channel (1st being stale at some point)
 # In RemoteInputChannel.onBuffer, netty thread updates pendingCheckpointBarrierId to BARRIER_RECEIVED
 # Task thread processes the 1st barrier and triggers a checkpoint
Task thread processes the 2nd barrier and aborts 1st checkpoint, calling stopPersisting() from UC controller and setting pendingCheckpointBarrierId to CHECKPOINT_COMPLETED
 # Task thread starts 2nd checkpoint and calls startPersisting() setting pendingCheckpointBarrierId to 2
 # now new buffers have a chance to be included in the 2nd checkpoint (though they belong to the next one)

 

For pendingCheckpointBarrierId(), consider an input gate with two channels A and B and two barriers 1 and 2:
 # Channel A receives both barriers, channel B receives nothing yet
 # Task thread processes both barriers on A, eventually triggering 2nd checkpoint
 # Channel A state is now BARRIER_RECEIVED, channel B - pending (with id=2)
 # Channel B receives the 1st barrier and becomes BARRIER_RECEIVED
 # No buffers in B between barriers 1 and 2 will be included in the checkpoint 
 # Channel B receives the 2nd barrier which will eventually conclude the checkpoint

 

I see a solution in doing an action only if passed checkpointId >= pendingCheckpointId. For that, a separate field will be needed to hold the status (RECEIVED/COMPLETED/PENDING). The class isn't thread-safe so it shouldn't be a problem.
 "	FLINK	Resolved	2	1	8742	pull-request-available
13431603	[Changelog] Materialization interleaved with task cancellation can fail the job	"When the task is cancelled, AsyncCheckpointRunnables are cancelled as well.
Cancellation exception can reach PeriodicMaterializationManager.
When the failure limit is reached, it invokes asyncExceptionHandler.handleAsyncException, which fails the job.
"	FLINK	Resolved	3	1	8742	pull-request-available
13311386	Tests are crashing with exit code 239	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3467&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8]
Kafka011ProducerExactlyOnceITCase
 
{code:java}
2020-06-15T03:24:28.4677649Z [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
2020-06-15T03:24:28.4692049Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-connector-kafka-0.11_2.11: There are test failures.
2020-06-15T03:24:28.4692585Z [ERROR] 
2020-06-15T03:24:28.4693170Z [ERROR] Please refer to /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire-reports for the individual test results.
2020-06-15T03:24:28.4693928Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-06-15T03:24:28.4694423Z [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4696762Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4698486Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4699066Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4699458Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4699960Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4700849Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4703760Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4705501Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4706297Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4706592Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4706895Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4707386Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2020-06-15T03:24:28.4708053Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2020-06-15T03:24:28.4708908Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2020-06-15T03:24:28.4709720Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-06-15T03:24:28.4710497Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-06-15T03:24:28.4711448Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-06-15T03:24:28.4712395Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-06-15T03:24:28.4712997Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-06-15T03:24:28.4713524Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-06-15T03:24:28.4714079Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-06-15T03:24:28.4714560Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-06-15T03:24:28.4715096Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-06-15T03:24:28.4715672Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-06-15T03:24:28.4716445Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-06-15T03:24:28.4717024Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-06-15T03:24:28.4717478Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-06-15T03:24:28.4717939Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-06-15T03:24:28.4718378Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-06-15T03:24:28.4718852Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-06-15T03:24:28.4719230Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-06-15T03:24:28.4719676Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-15T03:24:28.4720309Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-15T03:24:28.4720882Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-15T03:24:28.4721339Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-15T03:24:28.4721888Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-06-15T03:24:28.4722658Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-06-15T03:24:28.4723430Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-06-15T03:24:28.4724062Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-06-15T03:24:28.4724657Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4726770Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4728582Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4729202Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4729612Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4730247Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4730781Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-06-15T03:24:28.4731292Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-06-15T03:24:28.4731829Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2020-06-15T03:24:28.4732353Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2020-06-15T03:24:28.4732792Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-15T03:24:28.4733235Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-15T03:24:28.4733718Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-15T03:24:28.4734170Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2020-06-15T03:24:28.4734682Z [ERROR] -> [Help 1]
2020-06-15T03:24:28.4734859Z [ERROR] 
2020-06-15T03:24:28.4735312Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-15T03:24:28.4735927Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-15T03:24:28.4736439Z [ERROR] 
2020-06-15T03:24:28.4736952Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-15T03:24:28.4737706Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2020-06-15T03:24:28.4738167Z [ERROR] 
2020-06-15T03:24:28.4738553Z [ERROR] After correcting the problems, you can resume the build with the command
2020-06-15T03:24:28.4739663Z [ERROR]   mvn <goals> -rf :flink-connector-kafka-0.11_2.11
2020-06-15T03:24:29.0980029Z MVN exited with EXIT CODE: 1.
{code}

This could be a CI environment issue...
When did it start?"	FLINK	Closed	1	1	8742	pull-request-available, test-stability
13420752	If enabled changelog, RocksDB incremental checkpoint would always be full	"Once changelog is enabled, RocksDB incremental checkpoint would only be executed during materialization. During this phase, it will leverage the {{materization id}} as the checkpoint id for RocksDB state backend's snapshot method.

However, current incremental checkpoint mechanism heavily depends on the checkpoint id. And {{SortedMap<Long, Set<StateHandleID>> uploadedStateIDs}} with checkpoint id as the key within {{RocksIncrementalSnapshotStrategy}} is the kernel for incremental checkpoint. Once we notify checkpoint complete of previous checkpoint, it will then remove the uploaded stateIds of that checkpoint, leading to we cannot get proper checkpoint information on the next RocksDBKeyedStateBackend#snapshot. That is to say, we will always upload all RocksDB artifacts."	FLINK	Resolved	3	1	8742	pull-request-available
13427313	[Changelog] PriorityQueue elements recovered out-of-order	"StateChangeFormat is the class responsible for writing out changelog data.
Each chunk of data is sorted by: logId -> sequenceNumber -> keyGroup.
Sorting by sequenceNumber preserves temporal order.
Sorting by keyGroup a) puts metadata (group -1) at the beginning and b) allows to write KG only once.

However, the assumption that the order of changes across groups currently doesn't hold: poll operation of InternalPriorityQueue may affect any group (the smaller item across groups so far will be polled).

This results in wrong processing time timers being removed on recovery in ProcessingTimeWindowCheckpointingITCase#testAggregatingSlidingProcessingTimeWindow

One way to solve this probelm is to simply disable KG-sorting and grouping (only output metadata at the beginning). 
The other one is to associate polled element with the correct key group while logging changes.

Both ways should work with re-scaling.

cc: [~masteryhx], [~ym], [~yunta]"	FLINK	Closed	3	1	8742	pull-request-available
13574402	"JobIDLoggingITCase fails because of ""checkpoint confirmation for unknown task"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58640&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8735]
{code:java}
Mar 30 03:46:07 03:46:07.807 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.147 s <<< FAILURE! -- in org.apache.flink.test.misc.JobIDLoggingITCase
Mar 30 03:46:07 03:46:07.807 [ERROR] org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(ClusterClient) -- Time elapsed: 2.301 s <<< FAILURE!
Mar 30 03:46:07 java.lang.AssertionError: 
Mar 30 03:46:07 [too many events without Job ID logged by org.apache.flink.runtime.taskexecutor.TaskExecutor] 
Mar 30 03:46:07 Expecting empty but was: [Logger=org.apache.flink.runtime.taskexecutor.TaskExecutor Level=DEBUG Message=TaskManager received a checkpoint confirmation for unknown task b45d406844d494592784a88e47d201e2_cbc357ccb763df2852fee8c4fc7d55f2_0_0.]
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:264)
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:149)
Mar 30 03:46:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 30 03:46:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
[https://github.com/apache/flink/actions/runs/8502821551/job/23287730632#step:10:8131]

[https://github.com/apache/flink/actions/runs/8507870399/job/23300810619#step:10:8086]

 

 "	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13381494	ChangelogStateBackend tests use nested backend on recovery	"For example, ChangelogDelegateMemoryStateBackendTest overrides createKeyedBackend() but does NOT getStateBackend(). The latter is being used on recovery.

cc: [~ym]]"	FLINK	Closed	3	7	8742	pull-request-available
13413442	Add ChangelogBackend documentation	"Currently, changelog backend is hidden from users documentation-wise.

Once the feature is ready, the following needs to be documented:
 * General description (page [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/] )
 * Configuration (page [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/] - StateChangelogOptions, FsStateChangelogOptions)
 * Uploader metrics (page [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/] , see FLINK-23486)"	FLINK	Resolved	3	7	8742	pull-request-available
13521245	RocksDB Memory Management end-to-end test failed due to unexpected exception	"We see a test instability with {{RocksDB Memory Management end-to-end test}}. The test failed because an exception was detected in the logs:
{code}
2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...
2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.
2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...
2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:
[...]
2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...
2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.
2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 
2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

The only exception being reported in the Flink logs is due to a warning:
{code}
2023-01-25 02:47:38,242 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5117"	FLINK	Closed	2	1	8742	pull-request-available, test-stability
13352479	Checkpoint cleanup can kill JobMaster	"A user reported that cancelling a job can lead to an uncaught exception which kills the {{JobMaster}}. The problem seems to be that the {{CheckpointsCleaner}} might trigger {{CheckpointCoordinator}} actions after the job has reached a terminal state and, thus, is shut down. Apparently, we do not properly manage the lifecycles of {{CheckpointCoordinator}} and checkpoint post clean up actions.

The uncaught exception is 

{code}
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@41554407 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5d0ec6f7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 25977] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063
 at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830
 at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326
 at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533
 at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622
 at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668
 at org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter.execute(ScheduledExecutorServiceAdapter.java:62
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.scheduleTriggerRequest(CheckpointCoordinator.java:1152
 at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.lambda$cleanCheckpoint$0(CheckpointsCleaner.java:58
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624
 at java.lang.Thread.run(Thread.java:748 undefined)
{code}

cc [~roman_khachatryan].

https://lists.apache.org/thread.html/r75901008d88163560aabb8ab6fc458cd9d4f19076e03ae85e00f9a3a%40%3Cuser.flink.apache.org%3E"	FLINK	Closed	2	1	8742	pull-request-available
13363076	Add extension points to heap backend to read/write incremental snapshots	"For each state, incremental backend can create either incremental or full snapshot.

States of type PQ or nested tables are always fully snapshotted.

The goal is allow future incremental backend to:
 # distinguish between full/inc snapshots on recovery
 # read incremental snapshot

This ticket is only about changing the existing classes (providing extension points).

 

Optional goal: compatibility between backends."	FLINK	Resolved	3	7	8742	pull-request-available
13526527	Make AdaptiveScheduler aware of the (local) state size	"FLINK-21450 makes the Adaptive Schulder aware of Local Recovery.

Each slot-group pair is assigned a score based on a keyGroupRange size.

That score isn't always optimlal - it could be improved by computing the score based on the actual state size on disk."	FLINK	Closed	3	4	8742	pull-request-available, stale-assigned
13305181	Can't subsume checkpoint with no channel state in UC mode	"When there are no channel state handles, the underlying FS stream is still created.

On discard it is not deleted because it's not referenced by any state handles."	FLINK	Closed	2	1	8742	pull-request-available
13282353	StreamSourceOperatorWatermarksTest.testNoMaxWatermarkOnAsyncCancel fails on Travis	"https://api.travis-ci.org/v3/job/643480766/log.txt

{code}
08:06:17.382 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.812 s <<< FAILURE! - in org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest
08:06:17.382 [ERROR] testNoMaxWatermarkOnAsyncCancel(org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest)  Time elapsed: 0.235 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.testNoMaxWatermarkOnAsyncCancel(StreamSourceOperatorWatermarksTest.java:127)
{code}"	FLINK	Closed	1	1	8742	pull-request-available, test-stability
13306013	Memory threshold is ignored for channel state	"Config parameter state.backend.fs.memory-threshold is ignored for channel state. Causing each subtask to have a file per checkpoint. Regardless of the size of channel state (of this subtask).

This also causes slow cleanup and delays the next checkpoint.

 

The problem is that {{ChannelStateCheckpointWriter.finishWriteAndResult}} calls flush(); which actually flushes the data on disk.

 

From FSDataOutputStream.flush Javadoc:

A completed flush does not mean that the data is necessarily persistent. Data persistence can is only assumed after calls to close() or sync().

 

Possible solutions:

1. not to flush in {{ChannelStateCheckpointWriter.finishWriteAndResult (which can lead to data loss in a wrapping stream).}}

{{2. change }}{{FsCheckpointStateOutputStream.flush behavior}}

{{3. wrap }}{{FsCheckpointStateOutputStream to prevent flush}}{{}}{{}}"	FLINK	Closed	2	1	8742	pull-request-available
13357979	Implement incremental checkpointing and recovery using state changelog	"* Including state handles serialization (MetadataV2V3SerializerBase)
* Not including  materialization"	FLINK	Closed	3	7	8742	auto-unassigned, pull-request-available
13433222	[Changelog] Deadlock in FsStateChangelogWriter	"The issue occurs when sizes of buffers are set to minimum (e.g. 1 byte).
Task thread tries to update state -> schedules to upload changes -> waits for capacity.
Upload threads do release capacity on upload completion; however, they are unable to send back the results because the Writer lock is taken; therefore, they're unable to proceed with the next uploads."	FLINK	Resolved	3	1	8742	pull-request-available
13419872	Changlog materialization with incremental checkpoint cannot work well in local tests	"Currently, changelog materialization would call RocksDB state backend's snapshot method to generate {{IncrementalRemoteKeyedStateHandle}} as ChangelogStateBackendHandleImpl's materialized artifacts. And before next materialization, it will always report the same {{IncrementalRemoteKeyedStateHandle}} as before.

For local tests, TM would report the {{IncrementalRemoteKeyedStateHandle}} to JM via local {{LocalRpcInvocation}}. However, as {{LocalRpcInvocation}} would not de/serialize message, which leads once we register the {{IncrementalRemoteKeyedStateHandle}} on JM side, it will also add a {{sharedStateRegistry}} to the one located on TM side. For the 2nd checkpoint, TM would reported same {{IncrementalRemoteKeyedStateHandle}} with  {{sharedStateRegistry}} to JM. And it will then throw exception as it already contains a {{sharedStateRegistry}}:

IncrementalRemoteKeyedStateHandle
{code:java}
public void registerSharedStates(SharedStateRegistry stateRegistry, long checkpointID) {
       Preconditions.checkState(
                sharedStateRegistry != stateRegistry,
                ""The state handle has already registered its shared states to the given registry."");

}
{code}

This bug would go in distribution environment as {{IncrementalRemoteKeyedStateHandle}} would be serialized and {{sharedStateRegistry}} is tagged as {{transient}}."	FLINK	Resolved	3	1	8742	pull-request-available
13420537	Pre-emptively uploaded changelog not discarded up if materialized before checkpoint	"This is related to Garbage Collection. 

The problem is more severe than FLINK-25512 because there is leftover after materialization truncation each time (if pre-emptive uploads are enabled).

However, the probability can still be not high because of grouping of backends into a single file (so only if ALL the backends on TM materialize semi-simultaneously then it happen)."	FLINK	Closed	3	7	8742	pull-request-available
13339836	Modified UnalignedCheckpointITCase...MassivelyParallel fails	"In FLINK-19681, UnalignedCheckpointITCase was updated to put more backpressure.

This revealed a bug introduced in FLINK-19907: resultPartitions can be requested before the operator chain is initialized.

A proper fix would be to initialize the chain after the outputs but before the inputs."	FLINK	Resolved	3	1	8742	pull-request-available
13395473	DeleteExecutor NPE	"Encountered a situation where I get an NPE from JDBCUpsertOutputFormat.

This occurs when jdbc disconnected and try to reconnect.

I need to write data to mysql in upsert way in sql, So it must group by unique key and the JdbcBatchingOutputFormat of Jdbc sink would use TableJdbcUpsertOutputFormat.

 

Jdbc would disconnected when The data interval exceeds the set connection time.I see that when jdbc reconnect , only JdbcBatchingOutputFormat#jdbcStatementExecutor(insert) would prepareStatements but TableJdbcUpsertOutputFormat#deleteExecutor would not prepareStatements so that come up NPE.

if in JdbcBatchingOutputFormat have a protected function to reset PrepareStatement and TableJdbcUpsertOutputFormat override this function to reset deleteExecutor, it would work well.

prepareStatements "	FLINK	Resolved	2	1	8742	pull-request-available
13436939	Fix race condition in CopyOnWriteStateMap	"Currently, handling chained entry on copy-on-write uses object
identity to find the wanted entry in the chain. However, if
the same method is running concurrently, the object in the chain
can be replaced by its copy; the condition will never be met and
the chain end will be reached, causing an NPE."	FLINK	In Progress	3	7	8742	pull-request-available, stale-assigned
13432380	RetryingExecutorTest. testDiscardOnTimeout failed on azure	"{code:java}
Mar 06 01:20:29 [ERROR] Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.941 s <<< FAILURE! - in org.apache.flink.changelog.fs.RetryingExecutorTest
Mar 06 01:20:29 [ERROR] testTimeout  Time elapsed: 1.934 s  <<< FAILURE!
Mar 06 01:20:29 java.lang.AssertionError: expected:<500.0> but was:<1922.869766>
Mar 06 01:20:29 	at org.junit.Assert.fail(Assert.java:89)
Mar 06 01:20:29 	at org.junit.Assert.failNotEquals(Assert.java:835)
Mar 06 01:20:29 	at org.junit.Assert.assertEquals(Assert.java:555)
Mar 06 01:20:29 	at org.junit.Assert.assertEquals(Assert.java:685)
Mar 06 01:20:29 	at org.apache.flink.changelog.fs.RetryingExecutorTest.testTimeout(RetryingExecutorTest.java:145)
Mar 06 01:20:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 06 01:20:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 06 01:20:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 06 01:20:29 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 06 01:20:29 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 06 01:20:29 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 06 01:20:29 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 06 01:20:29 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 06 01:20:29 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Mar 06 01:20:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Mar 06 01:20:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 06 01:20:29 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Mar 06 01:20:29 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Mar 06 01:20:29 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Mar 06 01:20:29 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Mar 06 01:20:29 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Mar 06 01:20:29 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Mar 06 01:20:29 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Mar 06 01:20:29 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Mar 06 01:20:29 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Mar 06 01:20:29 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32569&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=cc452273-9efa-565d-9db8-ef62a38a0c10&l=22554"	FLINK	Closed	1	1	8742	auto-deprioritized-major, pull-request-available, test-stability
13339178	Barrier announcement causes outdated RemoteInputChannel#numBuffersOvertaken	"{{numBuffersOvertaken}} is set when the announcement is enqueued, but it can take quite a while until the checkpoint is actually started with quite a non-priority buffers being drained.

We should move away from {{numBuffersOvertaken}} and use sequence numbers."	FLINK	Resolved	3	1	8742	pull-request-available
13320514	FlinkKinesisProducer.backpressureLatch should be volatile	"(confirm first)

 

cc: [~rmetzger]"	FLINK	Closed	4	1	8742	pull-request-available
13382347	JdbcExactlyOnceSinkE2eTest.testInsert hangs on azure	https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=16658	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13427583	[Changelog] Incorrect key group logged for PQ.poll and remove	"Key group is logged so that state changes can be re-distributed or shuffled.
It is currently obtained from keyContext during poll() and remove() operations.
However, keyContext is not updated when dequeing processing time timers.

The impact is relatively small for remove(): in the worst case, the operation will be ignored.
poll() should probably be replaced with remove() anyways - see FLINK-26062.

One way to solve this problem is to extract key group from the polled element - if it is a timer.

cc: [~masteryhx], [~ym], [~yunta]"	FLINK	Resolved	3	1	8742	pull-request-available
13308737	Fix the bug of recycling buffer twice once exception in ChannelStateWriteRequestDispatcher#dispatch	"When task finishes, the `CheckpointBarrierUnaligner` will decline the current checkpoint, which would write abort request into `ChannelStateWriter`.

The abort request will be executed before other write output request in the queue, and close the underlying `CheckpointStateOutputStream`. Then when the dispatcher executes the next write output request to access the stream, it will throw ClosedByInterruptException to make dispatcher thread exit.

In this process, the underlying buffers for current write output request will be recycled twice. 
 * ChannelStateCheckpointWriter#write will recycle all the buffers in finally part, which can cover both exception and normal cases.
 * ChannelStateWriteRequestDispatcherImpl#dispatch will call `request.cancel(e)`  to recycle the underlying buffers again in the case of exception.

The effect of this bug can cause further exception in the network shuffle process, which references the same buffer as above, then this exception will send to the downstream side to make it failure.

 

This bug can be reproduced easily via running UnalignedCheckpointITCase#shouldPerformUnalignedCheckpointOnParallelRemoteChannel."	FLINK	Closed	1	1	8742	pull-request-available
13436935	Notify CheckpointStrategy about checkpoint completion/abortion	"Notifications could be used by incremental snapshot strategy
to replace state handles with placeholders."	FLINK	In Progress	3	7	8742	pull-request-available, stale-assigned
13393309	Cleanup unnecessary dependencies in dstl pom.xml	"- Clean up `flink-statebackend-changelog` denpendencies (move flink-statebackend-changelog depency to flink-test-utils) 
 - check whether some dependencies (i.e. flink-streaming-java, shaded guava, flink-test-utils-junit) are indeed necessary; comment if they are transitive
 - fix the scope of flink-runtime and flink-core - compile (not provided) and test for test-jar
 - Document how changelog randomization flags work."	FLINK	Closed	3	7	8742	pull-request-available
13368267	UnalignedCheckpointITCase fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15601&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc&l=4142


{code:java}
[ERROR] execute[parallel pipeline with remote channels, p = 5](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 60.018 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1859)
	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:69)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1839)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1822)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:138)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointITCase.execute(UnalignedCheckpointITCase.java:184)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13325182	"""Elasticsearch (v6.3.1) sink end-to-end test"" failed with ""SubtaskCheckpointCoordinatorImpl was closed without closing asyncCheckpointRunnable 1"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5986&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6

{code}
2020-08-29T22:20:02.3500263Z 2020-08-29 22:20:00,851 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Source: Sequence Source -> Flat Map -> Sink: Unnamed (1/1) - asynchronous part of checkpoint 1 could not be completed.
2020-08-29T22:20:02.3501112Z java.lang.IllegalStateException: SubtaskCheckpointCoordinatorImpl was closed without closing asyncCheckpointRunnable 1
2020-08-29T22:20:02.3502049Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:217) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3503280Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.registerAsyncCheckpointRunnable(SubtaskCheckpointCoordinatorImpl.java:371) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3504647Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.lambda$registerConsumer$2(SubtaskCheckpointCoordinatorImpl.java:479) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3505882Z 	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:95) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-08-29T22:20:02.3506614Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_265]
2020-08-29T22:20:02.3507203Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_265]
2020-08-29T22:20:02.3507685Z 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_265]
2020-08-29T22:20:02.3509577Z 2020-08-29 22:20:00,927 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: ca890bc4df19c66146370647d07bf510, jobId: 3522a3e4940d4b3cefc6dc1f22123f4b).
2020-08-29T22:20:02.3511425Z 2020-08-29 22:20:00,939 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 3522a3e4940d4b3cefc6dc1f22123f4b from job leader monitoring.
2020-08-29T22:20:02.3512499Z 2020-08-29 22:20:00,939 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 3522a3e4940d4b3cefc6dc1f22123f4b.
2020-08-29T22:20:02.3513174Z Checking for non-empty .out files...
2020-08-29T22:20:02.3513706Z No non-empty .out files.
2020-08-29T22:20:02.3513878Z 
2020-08-29T22:20:02.3514679Z [FAIL] 'Elasticsearch (v6.3.1) sink end-to-end test' failed after 0 minutes and 37 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
2020-08-29T22:20:02.3515138Z 
{code}"	FLINK	Closed	2	1	8742	pull-request-available, test-stability
13389603	Provide backpressure (currently job fails if a limit is hit)	"With the current approach, job will fail if dstl.dfs.upload.max-in-flight (bytes) is reached.

 

Unsetting the limit roughly matches the current behaviour for other backends: async phase doesn't backpressure

(state.backend.rocksdb.checkpoint.transfer.thread.num only sets the upload thread pool size which uses an unbounded queue).

 

Note that blocking caller in DfsWriter.persistInternal() will also block regular stream processing (because of pre-emptive writes). This may or may not be desired behaviour.

 

Blocking sync phase of a snapshot can also have some issues (e.g. not being able to abort the checkpoint) which should be considered."	FLINK	Resolved	3	7	8742	pull-request-available
13306407	Channel state handles, when inlined, duplicate underlying data	"If Unaligned checkpoints are enabled, channel state is written as state handles. Each channel has a handle and each such handle references the same underlying {{streamStateHandle}} (this is done to have a single file per subtask).
But, if the state is less then {{state.backend.fs.memory-threshold}}, the data is sent directly to JM as a byteStreamHandle. This causes each channel state handle to hold the whole subtask state.

This PR solves this by extracting relevant potions of the underlying handles if they are {{byteStreamHandle}}s."	FLINK	Closed	2	1	8742	pull-request-available
13337646	Add EndOfChannelRecovery rescaling epoch	"This event would allow to tear down ""virtual channels""     This event would allow to tear down ""virtual channels""     used to read channel state on recovery with unaligned checkpoints and     rescaling."	FLINK	Resolved	3	2	8742	pull-request-available
13556591	UnsupportedOperationException thrown from RocksDBIncrementalRestoreOperation	"When using the new rescaling API, it's possible to get
{code:java}
2023-10-31 18:25:05,179 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.
java.lang.UnsupportedOperationException: null
	at java.util.Collections$1.remove(Collections.java:4714) ~[?:?]
	at java.util.AbstractCollection.remove(AbstractCollection.java:299) ~[?:?]
	at org.apache.flink.runtime.checkpoint.StateObjectCollection.remove(StateObjectCollection.java:105) ~[flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithRescaling(RocksDBIncrementalRestoreOperation.java:294) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:167) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:327) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:512) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:338) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:735) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:710) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:676) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-10-31 18:25:05,182 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for KeyedProcessOperator_353a6b34b8b7f1c1d0fb4616d911049c_(1/2) from alternative (1/2), will retry while more alternatives are available.
org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:407) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:512) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:338) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:735) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:710) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:676) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.UnsupportedOperationException
	at java.util.Collections$1.remove(Collections.java:4714) ~[?:?]
	at java.util.AbstractCollection.remove(AbstractCollection.java:299) ~[?:?]
	at org.apache.flink.runtime.checkpoint.StateObjectCollection.remove(StateObjectCollection.java:105) ~[flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithRescaling(RocksDBIncrementalRestoreOperation.java:294) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:167) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:327) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	... 18 more

{code}
presumably on upscaling.
The job continues to recover (using the remote state).

The issue occurs on 1.17 and should be fixed in 1.18 and master."	FLINK	Resolved	4	1	8742	pull-request-available
13385626	Add percentiles to checkpoint stats	"Currently, only min/avg/max are shown, which doesn't allow to easily assess checkpointing times. 

Ideally, with breakdown by operator/channel state write times and sync/async phases (exact requirements up to implementer)"	FLINK	Closed	4	7	8742	pull-request-available
13330010	Checkpoint statistics for unfinished task snapshots	"If a checkpoint times out, there are currently no stats on the not-yet-finished tasks in the Web UI, so you have to crawl into (debug?) logs.

It would be nice to have these incomplete stats in there instead so that you know quickly what was going on. I could think of these ways to accomplish this:
 * the checkpoint coordinator could ask the TMs for it after failing the checkpoint or
 * the TMs could send the stats when they notice that the checkpoint is aborted

Maybe there are more options, but I think, this improvement in general would benefit debugging checkpoints."	FLINK	Closed	3	4	8742	pull-request-available, usability
13430115	ChangelogCompatibilityITCase.testRestore failed on azure	"{code:java}
Feb 22 19:09:43 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.815 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogCompatibilityITCase
Feb 22 19:09:43 [ERROR] ChangelogCompatibilityITCase.testRestore  Time elapsed: 1.309 s  <<< ERROR!
Feb 22 19:09:43 java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit2665494390926857042/junit2441494118455041375/accbd512d1546402f50f07832672cf2a/chk-1/._metadata.inprogress.7ebadfd7-88be-41ef-9889-f9cb5fa59113
Feb 22 19:09:43 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
Feb 22 19:09:43 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
Feb 22 19:09:43 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:499)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:486)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Feb 22 19:09:43 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.findAny(ReferencePipeline.java:536)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.hasMetadata(TestUtils.java:122)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.isCompletedCheckpoint(TestUtils.java:112)
Feb 22 19:09:43 	at java.nio.file.Files.lambda$find$2(Files.java:3691)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
Feb 22 19:09:43 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Feb 22 19:09:43 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Feb 22 19:09:43 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:582)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.getMostRecentCompletedCheckpointMaybe(TestUtils.java:105)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.waitForCheckpoint(ChangelogCompatibilityITCase.java:264)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.tryCheckpointAndStop(ChangelogCompatibilityITCase.java:145)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.runAndStoreIfAllowed(ChangelogCompatibilityITCase.java:109)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.testRestore(ChangelogCompatibilityITCase.java:103)
Feb 22 19:09:43 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 19:09:43 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 19:09:43 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 19:09:43 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 19:09:43 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 22 19:09:43 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 22 19:09:43 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 22 19:09:43 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 22 19:09:43 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32057&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12444"	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13375535	DefaultCompletedCheckpointStore drops unrecoverable checkpoints silently	"The {{DefaultCompletedCheckpointStore.recover()}} tries to be resilient if it cannot recover a checkpoint (e.g. due to a transient storage outage or a checkpoint being corrupted). This behaviour was introduced with FLINK-7783.

The problem is that this behaviour might cause us to ignore the latest valid checkpoint if there is a transient problem when restoring it. This might be ok for at least once processing guarantees, but it clearly violates exactly once processing guarantees. On top of it, it is very hard to spot.

I propose to change this behaviour so that {{DefaultCompletedCheckpointStore.recover()}} fails if it cannot read the checkpoints it is supposed to read. If the {{recover}} method fails during a recovery, it will kill the process. This will usually restart the process which will retry the checkpoint recover operation. If the problem is of transient nature, then it should eventually succeed. In case that this problem occurs during an initial job submission, then the job will directly transition to a {{FAILED}} state.

The proposed behaviour entails that if there is a permanent problem with the checkpoint (e.g. corrupted checkpoint), then Flink won't be able to recover without the intervention of the user. I believe that this is the right decision because Flink can no longer give exactly once guarantees in this situation and a user needs to explicitly resolve this situation."	FLINK	Resolved	2	1	8742	pull-request-available
13483617	Remove Calcite classes which were fixed in 1.24	"{{SqlDotOperator}}, {{SqlItemOperator}}, {{AliasNamespace}} were introduced as copies from Calcite and with a fix inside at https://github.com/apache/flink/pull/12649
At the same side the fixed was applied in Calcite itself in 1.24. https://issues.apache.org/jira/browse/CALCITE-4085

Since now Flink depends on 1.26 the fix is included and there is no need to have these classes in Flink repo"	FLINK	Closed	3	1	9144	pull-request-available
13572681	Bump Checkstyle to 9+	"The issue with current checkstyle is that there is checkstyle IntellijIdea plugin

And recently it dropped checkstyle 8 support [1]

At the same time we can not move to Checkstyle 10 since 10.x requires java 11+

[1] https://github.com/jshiell/checkstyle-idea/blob/main/CHANGELOG.md"	FLINK	Closed	3	1	9144	pull-request-available
13529925	Reduce usage of CatalogTableImpl in planner	The task is similar to https://issues.apache.org/jira/browse/FLINK-30896 however about CatalogTableImpl which is deprecated	FLINK	Closed	3	7	9144	pull-request-available
13449377	Upgrade Janino version 	"Currently, the Janino version doesn't support JDK11 well. 

 
[https://lists.apache.org/thread/q052xdn1mnhjm9k4ojjjz22dk4r1xwfz]"	FLINK	Resolved	3	11500	9144	pull-request-available
13555007	Bump zookeeper  to address CVE-2023-44981	There is a [CVE-2023-44981|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-44981] which is fixed in 3.7.2, 3.8.3	FLINK	Closed	3	11500	9144	pull-request-available
13448298	Add ARRAY_APPEND and ARRAY_PREPEND supported in SQL & Table API	"{{ARRAY_APPEND}} - adds element to the end of the array and returns the resulting array
{{ARRAY_PREPEND}} - adds element to the beginning of the array and returns the resulting array

Syntax:
{code:sql}
ARRAY_APPEND( <array> , <new_element> );
ARRAY_PREPEND( <new_element> , <array> );
{code}
Arguments:

array: An ARRAY to to add a new element.

new_element: A new element.

Returns:

An array. If array is NULL, the result is NULL.

Examples:
{code:sql}
SELECT array_append(array[1, 2, 3], 4);
-- array[1, 2, 3, 4]
select array_append(cast(null as int array), 2);
-- null
SELECT array_prepend(4, array[1, 2, 3]);
-- array[4, 1, 2, 3]
SELECT array_prepend(null, array[1, 2, 3]);
-- array[null, 1, 2, 3]
{code}
See more:
{{ARRAY_APPEND}}
Snowflake [https://docs.snowflake.com/en/sql-reference/functions/array_append.html]
PostgreSQL [https://www.postgresql.org/docs/14/functions-array.html#ARRAY-FUNCTIONS-TABLE]
{{ARRAY_PREPEND}}
Snowflake [https://docs.snowflake.com/en/sql-reference/functions/array_prepend.html]
PostgreSQL [https://www.postgresql.org/docs/14/functions-array.html#ARRAY-FUNCTIONS-TABLE]"	FLINK	Closed	3	7	9144	pull-request-available, stale-assigned
13528615	"Backport ""FilterJoinRule misses opportunity to push filter to semijoin input"" to FlinkFilterJoinRule"	"In https://issues.apache.org/jira/browse/CALCITE-4499 there has been done an optimization.
Since Flink has it's own copy of slightly changed {{FilterJoiRule}} this optimization does not come with 1.28 update.

The idea is to apply this change to {{FlinkFilterJoinRule}}"	FLINK	Closed	3	4	9144	pull-request-available
13408414	Ceil, floor for some timeunit return wrong results or fail with CompileException	"There are issues
1. for {{TIMESTAMP WITHOUT TIMEZONE}} and {{DATE}} it returns wrong result for queries
{code:sql}
select ceil(timstamp'2020-26-10 12:12:12' to decade);
select ceil(timstamp'2020-26-10 12:12:12' to century);
select ceil(timstamp'2020-26-10 12:12:12' to millennium);
{code}

same for {{FLOOR}} and {{DATE}}
2. for {{TIMESTAMP WITH TIMEZONE}} it throws exception below.

Expected for the query
{code:sql}
select floor(date '2021-10-07' to decade) as floor_decade,      
         ceil(date '2021-10-07' to decade) as ceil_decade,
         floor(date '2021-10-07' to century) as floor_century,
         ceil(date '2021-10-07' to century) as ceil_century,
         floor(date '2021-10-07' to millennium) as floor_millennium,
         ceil(date '2021-10-07' to millennium) as ceil_millennium;

{code}
is
{noformat}
 floor_decade ceil_decade floor_century ceil_century floor_millennium ceil_millennium
   2020-01-01  2030-01-01    2001-01-01   2101-01-01       2001-01-01      3001-01-01
{noformat}
based on  PostgreSQL[1] and Vertica[2] defibitions 
And for both the definition is the same
{{DECADE}} - The year field divided by 10
{{CENTURY}} - The first century starts at 0001-01-01 00:00:00 AD, although they did not know it at the time. This definition applies to all Gregorian calendar countries. There is no century number 0, you go from -1 century to 1 century.
{{MILLENNIUM}} - The millennium number, where the first millennium is 1 and each millenium starts on 01-01-y001. For example, millennium 2 starts on 01-01-1001.
from
[1] https://www.postgresql.org/docs/14/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT
[2] https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Date-Time/DATE_PART.htm

{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 57, Column 0: No applicable constructor/method found for actual parameters ""org.apache.flink.table.data.TimestampData, org.apache.flink.table.data.TimestampData""; candidates are: ""public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, java.math.BigDecimal)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, int)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, java.math.BigDecimal)"", ""public static short org.apache.calcite.runtime.SqlFunctions.ceil(short, short)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal)"", ""public static double org.apache.calcite.runtime.SqlFunctions.ceil(double)"", ""public static float org.apache.calcite.runtime.SqlFunctions.ceil(float)"", ""public static byte org.apache.calcite.runtime.SqlFunctions.ceil(byte, byte)"", ""public static long org.apache.calcite.runtime.SqlFunctions.ceil(long, long)"", ""public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, int)""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9263)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:712)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:686)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:187)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:63)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:675)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:661)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:960)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:929)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:753)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:574)
	at java.base/java.lang.Thread.run(Thread.java:834)

{noformat}"	FLINK	Closed	3	1	9144	pull-request-available
13588651	SHOW CREATE VIEW returns invalid query	"especially for views with comments
for instance
1. create view
{code:sql}
CREATE VIEW v1 COMMENT 'test view' AS SELECT 1, 'a';
{code}
2. show create
{code:sql}
SHOW CREATE VIEW v1;
{code}
 it returns query
{code:sql}
CREATE VIEW `default_catalog`.`default_database`.`v1`(`EXPR$0`, `EXPR$1`) as
SELECT 1, 'a' COMMENT 'test view'
{code}

now if we try to execute it fails as
{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Incorrect syntax near the keyword 'COMMENT' at line 2, column 15.
Was expecting one of:
    <EOF>
    ""AS"" ...
    ""EXCEPT"" ...
{noformat}

the reason is that {{COMMENT}} should be before the query according to syntax mentioned at https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#create-view"	FLINK	Resolved	3	1	9144	pull-request-available
13560314	JoinRestoreTest is failing on AZP	"Since {{JoinRestoreTest}} was introduced in FLINK-33470 it seems to be a reason

{noformat}
Dec 02 04:42:26 04:42:26.408 [ERROR] Failures: 
Dec 02 04:42:26 04:42:26.408 [ERROR]   JoinRestoreTest>RestoreTestBase.testRestore:283 
Dec 02 04:42:26 Expecting actual:
Dec 02 04:42:26   [""+I[9, carol, apple, 9000]"",
Dec 02 04:42:26     ""+I[8, bill, banana, 8000]"",
Dec 02 04:42:26     ""+I[6, jerry, pen, 6000]""]
Dec 02 04:42:26 to contain exactly in any order:
Dec 02 04:42:26   [""+I[Adam, null]"",
Dec 02 04:42:26     ""+I[Baker, Research]"",
Dec 02 04:42:26     ""+I[Charlie, Human Resources]"",
Dec 02 04:42:26     ""+I[Charlie, HR]"",
Dec 02 04:42:26     ""+I[Don, Sales]"",
Dec 02 04:42:26     ""+I[Victor, null]"",
Dec 02 04:42:26     ""+I[Helena, Engineering]"",
Dec 02 04:42:26     ""+I[Juliet, Engineering]"",
Dec 02 04:42:26     ""+I[Ivana, Research]"",
Dec 02 04:42:26     ""+I[Charlie, People Operations]""]
{noformat}
examples of failures

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55120&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12099
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55129&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11786
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55136&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12099
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55137&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11779"	FLINK	Closed	2	1	9144	pull-request-available, test-stability
13300194	Support for casting collection types.	"Casts of collection types are not supported yet.
E.g. query: {{""SELECT cast (a as ARRAY<double>) FROM (VALUES (array[3, 2, 1])) AS T(a)""}}

fails with:
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Unsupported cast from 'ARRAY<INT NOT NULL> NOT NULL' to 'ARRAY<DOUBLE> NOT NULL'.

	at org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens$.generateCast(ScalarOperatorGens.scala:1284)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:691)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:486)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:52)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:288)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:132)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$5.apply(CalcCodeGenerator.scala:152)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$5.apply(CalcCodeGenerator.scala:152)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:152)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:179)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:49)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.translateToPlanInternal(BatchExecCalc.scala:62)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.translateToPlanInternal(BatchExecCalc.scala:38)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalcBase.translateToPlan(BatchExecCalcBase.scala:42)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToTransformation(BatchExecSink.scala:131)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.scala:97)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.scala:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlan(BatchExecSink.scala:49)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:72)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:71)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:71)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)
...
{code}

Similar behaviour can be observed for MULTISET, MAP, ROW"	FLINK	Closed	2	7	9144	pull-request-available
13407293	FlinkSQL multiline parser improvements	"Currently existing multiline parser has limitations e.g.
line could not end with semicolon e.g. as a part of field value, comment or column name.
Also if a query contains '--' e.g. as a part of varchar field value then it fails.

In case there is no objections I would put some efforts to improve this behavior;

here it is a list of sample problem queries
{code:sql}
select 123; -- comment

select 1 as `1--`;

select '--';

-- This query works if a user copy-pastes it to FlinkSQL, however it fails if a user types it in FlinkSQL
select '1;
';
{code}
"	FLINK	Closed	3	7	9144	pull-request-available, stale-assigned
13561197	Remove Hive connector from master branch	"The connector is going to be externalized at FLINK-30064
Once it is done it would make sense to remove it from master branch"	FLINK	Open	3	7	9144	pull-request-available
13449008	Use ResolvedSchema in flink-avro instead of TableSchema	"{{TableSchema}} is deprecated 
It is recommended to use {{ResolvedSchema}} and {{Schema}} in {{TableSchema}} javadoc"	FLINK	Resolved	4	11500	9144	pull-request-available
13566125	Bumping version of flatten plugin	"As it was found within FLINK-34148
 that newer version of shade plugin breaks previous behavior and non shaded artifacts are started being added to flink-shaded deps.

 

The tasks is to apply same check for flink-shaded with help of {{ShadeOptionalChecker}} which is already applied for Flink

 

UPD: as it was mentioned in comments update of flatten plugin should solve this issue FLINK-34148"	FLINK	Resolved	1	1	9144	pull-request-available
13168808	ATAN2 Sql Function support	"simple query fails {code}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());

DataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);
tableEnv.registerDataSet(""t1"", ds, ""x, y, z"");

String sqlQuery = ""SELECT atan2(1,2)"";
Table result = tableEnv.sqlQuery(sqlQuery);
{code}
while at the same time Calcite supports it and in Calcite's sqlline it works like {noformat}
0: jdbc:calcite:model=target/test-classes/mod> select atan2(1,2);
+-----------------+
|     EXPR$0      |
+-----------------+
| 0.4636476090008061 |
+-----------------+
1 row selected (0.173 seconds)
{noformat}"	FLINK	Resolved	4	2	9144	pull-request-available
13591063	Remove powermock usage	"Most of the tests are either moved to a different repo like connectors or rewritten in powermock free way.
Powermock itself became unmaintained (latest release was in 2020 https://github.com/powermock/powermock/releases/tag/powermock-2.0.9)
and latest commit 2 years ago https://github.com/powermock/powermock

also there is no support for junit5 (the request to support it and even PR from junit5 maintainers is ready for review since Feb 2023 https://github.com/powermock/powermock/pull/1146, however still no feedback from maintainers...)"	FLINK	Resolved	3	11500	9144	pull-request-available
13417000	Use ResolvedSchema in DataGen instead of TableSchema	"{{TableSchema}} is deprecated 
It is recommended to use {{ResolvedSchema}} and {{Schema}} in {{TableSchema}} javadoc"	FLINK	Closed	4	1	9144	pull-request-available
13494252	Upgrade Archunit to 1.0.0	Flink still uses Archunit version 0.22.0. Recently Archunit 1.0.0 has been introduced; we should upgrade to this major version and remove all the deprecated usages	FLINK	Closed	3	11500	9144	pull-request-available
13170099	PackagedProgram.extractContainedLibraries fails on Windows	"Submitting a jar that contains other jars on Windows fails with an exception:
{code}
org.apache.flink.client.program.ProgramInvocationException: Unknown I/O error while extracting contained jar files.
        at org.apache.flink.client.program.PackagedProgram.extractContainedLibraries(PackagedProgram.java:752)
        at org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:194)
        at org.apache.flink.client.cli.CliFrontend.buildProgram(CliFrontend.java:833)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:201)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1020)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$9(CliFrontend.java:1096)
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1096)
Caused by: org.apache.flink.client.program.ProgramInvocationException: An I/O error occurred while creating temporary file to extract nested library 'lib/antrunner.jar'.
        at org.apache.flink.client.program.PackagedProgram.extractContainedLibraries(PackagedProgram.java:708)
        ... 7 more
Caused by: java.io.IOException: Unable to create temporary file, C:\Users\XXX\AppData\Local\Temp\1751416743_6922010711856647205lib\antrunner.jar
        at java.io.File$TempDirectory.generateFile(Unknown Source)
        at java.io.File.createTempFile(Unknown Source)
        at java.io.File.createTempFile(Unknown Source)
        at org.apache.flink.client.program.PackagedProgram.extractContainedLibraries(PackagedProgram.java:702)
        ... 7 more
{code}

{{PackagedProgram.extractContainedLibraries}} tries to replace all path separators using the platform-dependent {{File.separateChar}}, however the path separator for jars (and zips for that matter) is always {{/}}.

{code}
final JarEntry entry = containedJarFileEntries.get(i);
String name = entry.getName();
name = name.replace(File.separatorChar, '_');
{code}"	FLINK	Closed	3	1	9144	pull-request-available
13474166	Upgrade Calcite version to 1.31	We should upgrade to Calcite 1.31 so we can benefit from https://issues.apache.org/jira/browse/CALCITE-4865	FLINK	Closed	3	11500	9144	pull-request-available
13568562	SqlValidatorException with LATERAL TABLE and JOIN	"found one regression issue. Query working Flink 1.17.2, but failing with Flink 1.18.+

 
{code:java}
-- Query working Flink 1.17.2, but failing with Flink 1.18.+

-- -- [ERROR] Could not execute SQL statement. Reason:

-- -- org.apache.calcite.sql.validate.SqlValidatorException: Table 's' not found

SELECT * FROM sample as s,
LATERAL TABLE(split(s.id,'[01]'))
CROSS JOIN (VALUES ('A'), ('B'));
{code}

The problem is not related to the the alias scope. Even if we replace split(s.id.. ) with split(id,...) the error

{code:java}
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'id' not found in any table
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
{code}

will be generated. This seems to be Calcite issue, since this test fails on Calcite v1.32 and does not fail on Calcite 1.29.0 and 1.30.0.
We tested it with Calcite versions 1.31.0, 1.32.0, 1.33.0, 1.34.0, 1.35.0, 1.36.0 and the main branch (c774c313a81d01c4e3e77cf296d04839c5ab04c0). The issue still remains"	FLINK	Resolved	2	1	9144	pull-request-available
13162788	Compilation fails after upgrade to Calcite 1.17	"
{noformat}
/apacheFlink/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/catalog/ExternalCatalogSchema.scala:40: error: class ExternalCatalogSchema needs to be abstract, since:
[ERROR] it has 2 unimplemented members.
[ERROR] /** As seen from class ExternalCatalogSchema, the missing signatures are as follows.
[ERROR]  *  For convenience, these are usable as stub implementations.
[ERROR]  */
[ERROR]   def getType(x$1: String): org.apache.calcite.rel.type.RelProtoDataType = ???
[ERROR]   def getTypeNames(): java.util.Set[String] = ???
[ERROR] 
[ERROR] class ExternalCatalogSchema(
[ERROR]       ^
[WARNING] two warnings found
[ERROR] one error found

{noformat}

while https://issues.apache.org/jira/browse/CALCITE-2045 into interface _org.apache.calcite.schema.Schema_ there were introduced two more methods: _org.apache.calcite.schema.Schema#getTypeNames_, _org.apache.calcite.schema.Schema#getType_"	FLINK	Closed	3	7	9144	pull-request-available
13559469	Upgrade Archunit to 1.1.0+	"ASM 9.5 (which has support for java 21[1]) came only with ArchUnit 1.1.0 [2]

With current ArchUnit(1.0.0) in case of jdk21 it fails

{noformat}
mvn clean install -DskipTests -Dfast -Pjava21-target
mvn verify -pl flink-architecture-tests/flink-architecture-tests-production/ -Darchunit.freeze.store.default.allowStoreUpdate=false
{noformat}
like
{noformat}
Nov 26 16:07:42 16:07:42.024 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.742 s <<< FAILURE! - in org.apache.flink.architecture.rules.ITCaseRules
Nov 26 16:07:42 16:07:42.025 [ERROR] ITCaseRules.ITCASE_USE_MINICLUSTER  Time elapsed: 0.005 s  <<< ERROR!
Nov 26 16:07:42 com.tngtech.archunit.library.freeze.StoreUpdateFailedException: Updating frozen violations is disabled (enable by configuration freeze.store.default.allowStoreUpdate=true)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.ViolationStoreFactory$TextFileBasedViolationStore.save(ViolationStoreFactory.java:125)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule$ViolationStoreLineBreakAdapter.save(FreezingArchRule.java:277)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.removeObsoleteViolationsFromStore(FreezingArchRule.java:154)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.removeObsoleteViolationsFromStoreAndReturnNewViolations(FreezingArchRule.java:146)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.evaluate(FreezingArchRule.java:127)
Nov 26 16:07:42 	at com.tngtech.archunit.lang.ArchRule$Assertions.check(ArchRule.java:84)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.check(FreezingArchRule.java:97)

{noformat}

[1] https://asm.ow2.io/versions.html#9.5
[2] https://github.com/TNG/ArchUnit/pull/1098"	FLINK	Closed	3	7	9144	pull-request-available
13480739	RexSimplify can not be removed after update to calcite 1.27	"It seems there is some work should be done to make it happen
Currently removal of RexSimplify from Flink repo leads to failure of several tests like
{{IntervalJoinTest#testFallbackToRegularJoin}}
{{CalcITCase#testOrWithIsNullInIf}}
{{CalcITCase#testOrWithIsNullPredicate}}
example of failure
{noformat}
Sep 07 11:25:08 java.lang.AssertionError: 
Sep 07 11:25:08 
Sep 07 11:25:08 Results do not match for query:
Sep 07 11:25:08   
Sep 07 11:25:08 SELECT * FROM NullTable3 AS T
Sep 07 11:25:08 WHERE T.a = 1 OR T.a = 3 OR T.a IS NULL
Sep 07 11:25:08 
Sep 07 11:25:08 
Sep 07 11:25:08 Results
Sep 07 11:25:08  == Correct Result - 4 ==   == Actual Result - 2 ==
Sep 07 11:25:08  +I[1, 1, Hi]               +I[1, 1, Hi]
Sep 07 11:25:08  +I[3, 2, Hello world]      +I[3, 2, Hello world]
Sep 07 11:25:08 !+I[null, 999, NullTuple]   
Sep 07 11:25:08 !+I[null, 999, NullTuple]   
Sep 07 11:25:08         
Sep 07 11:25:08 Plan:
Sep 07 11:25:08   == Abstract Syntax Tree ==
Sep 07 11:25:08 LogicalProject(inputs=[0..2])
Sep 07 11:25:08 +- LogicalFilter(condition=[OR(=($0, 1), =($0, 3), IS NULL($0))])
Sep 07 11:25:08    +- LogicalTableScan(table=[[default_catalog, default_database, NullTable3]])
Sep 07 11:25:08 
Sep 07 11:25:08 == Optimized Logical Plan ==
Sep 07 11:25:08 Calc(select=[a, b, c], where=[SEARCH(a, Sarg[1, 3; NULL AS TRUE])])
Sep 07 11:25:08 +- BoundedStreamScan(table=[[default_catalog, default_database, NullTable3]], fields=[a, b, c])
Sep 07 11:25:08 
Sep 07 11:25:08        
Sep 07 11:25:08 	at org.junit.Assert.fail(Assert.java:89)
Sep 07 11:25:08 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1(BatchTestBase.scala:154)
Sep 07 11:25:08 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1$adapted(BatchTestBase.scala:147)
Sep 07 11:25:08 	at scala.Option.foreach(Option.scala:257)
Sep 07 11:25:08 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:147)
Sep 07 11:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Sep 07 11:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Sep 07 11:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Sep 07 11:25:08 

{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13410580	Examples in documentation for value1 IS DISTINCT FROM value2 are wrong	"Currently it is stated in docs for {{value1 IS DISTINCT FROM value2}}
{quote}
E.g., 1 IS NOT DISTINCT FROM NULL returns TRUE; NULL IS NOT DISTINCT FROM NULL returns FALSE.
{quote}
In fact they return opposite values."	FLINK	Closed	4	1	9144	auto-deprioritized-major, pull-request-available
13540035	SQL array_union could return wrong result	"This is was mentioned at [https://github.com/apache/flink/pull/22717#issuecomment-1587333488]

 how to reproduce
{code:sql}
SELECT array_union(ARRAY[CAST(NULL AS INT)], ARRAY[1]); -- returns [NULL, 1], this is OK
SELECT array_union(ARRAY[1], ARRAY[CAST(NULL AS INT)]); -- returns [1, 0], this is NOT OK
{code}"	FLINK	Closed	3	1	9144	pull-request-available
13575613	SinkTestSuiteBase.testScaleDown is hanging for 1.20-SNAPSHOT	"Currently it is reproduced for elastic search connector
all the ci jobs (for all jdks) against 1.20-SNAPSHOT are hanging on 
{noformat}
2024-04-12T05:56:50.6179284Z ""main"" #1 prio=5 os_prio=0 cpu=18726.96ms elapsed=2522.03s tid=0x00007f670c025a50 nid=0x3c6d waiting on condition  [0x00007f6712513000]
2024-04-12T05:56:50.6180667Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2024-04-12T05:56:50.6181497Z 	at java.lang.Thread.sleep(java.base@17.0.10/Native Method)
2024-04-12T05:56:50.6182762Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:152)
2024-04-12T05:56:50.6184456Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2024-04-12T05:56:50.6186346Z 	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.checkResultWithSemantic(SinkTestSuiteBase.java:504)
2024-04-12T05:56:50.6188474Z 	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.restartFromSavepoint(SinkTestSuiteBase.java:327)
2024-04-12T05:56:50.6190145Z 	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.testScaleDown(SinkTestSuiteBase.java:224)
2024-04-12T05:56:50.6191247Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@17.0.10/Native Method)
2024-04-12T05:56:50.6192806Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@17.0.10/NativeMethodAccessorImpl.java:77)
2024-04-12T05:56:50.6193863Z 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@17.0.10/DelegatingMethodAccessorImpl.java:43)
2024-04-12T05:56:50.6194834Z 	at java.lang.reflect.Method.invoke(java.base@17.0.10/Method.java:568)
{noformat}

for 1.17, 1.18, 1.19 there is no such issue and everything is ok
https://github.com/apache/flink-connector-elasticsearch/actions/runs/8538572134"	FLINK	Resolved	1	1	9144	pull-request-available, test-stability
13548220	Review and update documentation	"There are a few pages in the documentation that need to be reviewed and updated for each release.
 * Ensure that there exists a release notes page for each non-bugfix release (e.g., 1.5.0) in {{{}./docs/release-notes/{}}}, that it is up-to-date, and linked from the start page of the documentation.
 * Upgrading Applications and Flink Versions: [https://ci.apache.org/projects/flink/flink-docs-master/ops/upgrading.html]
 * ...

 
----
h3. Expectations
 * Update upgrade compatibility table ([apache-flink:./docs/content/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content/docs/ops/upgrading.md#compatibility-table] and [apache-flink:./docs/content.zh/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content.zh/docs/ops/upgrading.md#compatibility-table]).
 * Update [Release Overview in Confluence|https://cwiki.apache.org/confluence/display/FLINK/Release+Management+and+Feature+Plan]
 * (minor only) The documentation for the new major release is visible under [https://nightlies.apache.org/flink/flink-docs-release-$SHORT_RELEASE_VERSION] (after at least one [doc build|https://github.com/apache/flink/actions/workflows/docs.yml] succeeded).
 * (minor only) The documentation for the new major release does not contain ""-SNAPSHOT"" in its version title, and all links refer to the corresponding version docs instead of {{{}master{}}}."	FLINK	Resolved	3	7	9144	pull-request-available
13557390	Make AWS connectors compilable with jdk17	Since 1.18 Flink with jdk 17 support is released it would make sense to add such support for connectors	FLINK	Resolved	3	4	9144	pull-request-available
13375100	Built-in functions for collections	"There is a number of built-in functions to work with collections are supported by other vendors. After looking at Postgresql, BigQuery, Spark there was selected a list of more or less generic functions for collections (for more details see [1]).
Feedback for the doc is  welcome

[1] [https://docs.google.com/document/d/1nS0Faur9CCop4sJoQ2kMQ2XU1hjg1FaiTSQp2RsZKEE/edit?usp=sharing]

MAP_KEYS
MAP_VALUES
MAP_FROM_ARRAYS"	FLINK	Reopened	3	7	9144	pull-request-available, stale-assigned
13475175	Update postgres driver because of CVE-2022-31197	More details about CVE at pgjdbc repo page https://github.com/pgjdbc/pgjdbc/security/advisories/GHSA-r38f-c4h4-hqq2	FLINK	Closed	3	11500	9144	pull-request-available
13474555	Update JUnit5 to v5.9.1	"Junit 5.9.0 is released

with release notes https://junit.org/junit5/docs/current/release-notes/#release-notes-5.9.0"	FLINK	Closed	4	11500	9144	pull-request-available, stale-assigned
13477889	Disable dependency-reduced-pom creation in quickstarts	"now maven-shade-plugin puts it in project folder and version control detects as a change.

The proposal is to put it to target folder as it's done for table-planner"	FLINK	Closed	4	4	9144	pull-request-available
13486274	Update jackson bom because of CVE-2022-42003	"There is a CVE-2022-42003 fixed in 2.13.4.1 and 2.14.0-rc1
https://nvd.nist.gov/vuln/detail/CVE-2022-42003


P.S. It seems there will not be 2.14.0 release until end of October according to https://github.com/FasterXML/jackson-databind/issues/3590#issuecomment-1270363915"	FLINK	Resolved	3	11500	9144	pull-request-available
13556711	Add java21-target profile	"Add a new profile analogous to the java11-target and java17-target profiles.
"	FLINK	Closed	3	7	9144	pull-request-available
13554831	VertexFlameGraphFactoryTest#verifyRecursively fails on Java 21	"With jdk 21 it fails as
{noformat}
java.lang.AssertionError: No lambdas encountered in the test, cleanup functionality was not tested

	at org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.testLambdaClassNamesCleanUp(VertexFlameGraphFactoryTest.java:54)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13415815	ISODOW for timestamps with timezones fails with Invalid start unit	"{{DECADE}} returns wrong result for timestamps with timezones and
{{ISODOW}}, {{ISOYEAR}} fail for timestamps with timezones like 
{noformat}
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: Invalid start unit.
	at org.apache.flink.table.utils.DateTimeUtils.getFactor(DateTimeUtils.java:1213)
	at org.apache.flink.table.utils.DateTimeUtils.convertExtract(DateTimeUtils.java:1164)
	at org.apache.flink.table.utils.DateTimeUtils.extractFromTimestamp(DateTimeUtils.java:993)
	at StreamExecCalc$13.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:330)

{noformat}"	FLINK	Closed	3	1	9144	pull-request-available
13559166	Wrong style in flink ui	"https://nightlies.apache.org/flink/flink-docs-master/

 !image-2023-11-23-16-06-44-000.png! "	FLINK	Open	3	1	9144	pull-request-available
13556961	Kubernetes operator supports compiling with Java 21	Since there is a new Java LTS version available (21) it would make sense to support it	FLINK	Closed	3	4	9144	pull-request-available
13161739	Support extract epoch, decade, millisecond, microsecond	"The task is to separate activity from depending on https://issues.apache.org/jira/browse/CALCITE-2303 from all others that could be done without upgrade avatica/calcite in  https://issues.apache.org/jira/browse/FLINK-8518

Now the implementations of next functions are blocked
{code:sql}
extract(decade from ...)
extract(epoch from ...)
extract(millisecond from ...)
extract(microsecond from ...)
extract(isodow from ...)
extract(isoyear from ...)
{code}
"	FLINK	Closed	4	2	9144	auto-deprioritized-major, auto-unassigned, pull-request-available
13470681	Update assertj to 3.23.1	"Among others there is a performance improvement for {{containsExactly}} at https://github.com/assertj/assertj-core/issues/2548

the full release notes are available at https://assertj.github.io/doc/#assertj-core-3-23-1-release-notes"	FLINK	Closed	4	11500	9144	pull-request-available
13124715	Key expressions on named row types do not work	"The following program fails with a {{ClassCastException}}. It seems that key expressions and rows are not tested well. We should add more tests for them.

{code}
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

TypeInformation[] types = new TypeInformation[] {Types.INT, Types.INT};

String[] fieldNames = new String[]{""id"", ""value""};
RowTypeInfo rowTypeInfo = new RowTypeInfo(types, fieldNames);

env.fromCollection(Collections.singleton(new Row(2)), rowTypeInfo)
.keyBy(""id"").sum(""value"").print();

env.execute(""Streaming WordCount"");
{code}"	FLINK	Closed	3	1	9144	pull-request-available
13562043	Support OpenSearch v2	"The main issue is that in OpenSearch v2 there were several breaking changes like 
[https://github.com/opensearch-project/OpenSearch/pull/9082]
[https://github.com/opensearch-project/OpenSearch/pull/5902]

which made current connector version failing while communicating with v2

 

Also it would make sense to add integration and e2e tests to test against v2"	FLINK	Resolved	3	4	9144	pull-request-available
13510079	[JUnit5 Migration] Migrate flink-statebackend-common	"There is a new module was added with junit4

it's better to use junit5 for new modules"	FLINK	Resolved	3	7	9144	pull-request-available
13170144	Typos found in docs after hunspell run	"After [hunspell|http://hunspell.github.io/] run the next typos found
||File|| Description||
|docs/dev/connectors/kinesis.md| {color:#d04437}speciyfing{color} vs {color:#14892c}specifying{color}|
|docs/dev/libs/ml/cross_validation.md| {color:#d04437}Validatation{color} vs {color:#14892c}Validation{color}|
|docs/dev/table/sql.md| {color:#d04437}CHARACTERISTICTS{color} vs {color:#14892c}CHARACTERISTICS {color}|
|docs/dev/table/sqlClient.md| {color:#d04437}developement{color} vs {color:#14892c}development{color}, {color:#d04437}possiblities{color} vs {color:#14892c}possibilities{color}|
|docs/dev/table/udfs.md| {color:#d04437}aggreagtions{color} vs {color:#14892c}aggregations{color}|
|docs/monitoring/metrics.md|{color:#d04437}instancesof{color} vs {color:#14892c}instances of{color}|
|docs/ops/deployment/mesos.md| {color:#d04437}Dipsatcher{color} vs {color:#14892c}Dispatcher{color}|
|docs/ops/security-kerberos.md| {color:#d04437}Kerbreros{color} vs {color:#14892c}Kerberos{color}|
|docs/quickstart/scala_api_quickstart.md| {color:#d04437}dependencoes{color} vs {color:#14892c}dependencies{color}|
|docs/start/dependencies.md| {color:#d04437}Dependenies{color} vs {color:#14892c}Dependencies{color}, {color:#d04437}bulding{color} vs {color:#14892c}building{color}, {color:#d04437}prorgams{color} vs {color:#14892c}programs{color}|
"	FLINK	Resolved	4	4	9144	pull-request-available
13423456	Use HashMap for MAP value constructors	Currently, the usage of maps is inconsistent. It is not ensured that duplicate keys get eliminated. For CAST and output conversion this is solved. However, we should have a similar implementation in MAP value constructor like in CAST.	FLINK	Closed	3	7	9144	pull-request-available, stale-assigned
13561064	ParquetTimestampITCase>FsStreamingSinkITCaseBase failing in CI	"From this CI run: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55334&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906]
{code:java}
Dec 07 19:57:30 19:57:30.026 [ERROR] Errors: 
Dec 07 19:57:30 19:57:30.026 [ERROR] ParquetTimestampITCase>FsStreamingSinkITCaseBase.testNonPart:84->FsStreamingSinkITCaseBase.testPartitionCustomFormatDate:151->FsStreamingSinkITCaseBase.test:186 » Validation 
Dec 07 19:57:30 19:57:30.026 [ERROR] ParquetTimestampITCase>FsStreamingSinkITCaseBase.testPart:89->FsStreamingSinkITCaseBase.testPartitionCustomFormatDate:151->FsStreamingSinkITCaseBase.test:186 » Validation 
Dec 07 19:57:30 19:57:30.026 [ERROR] ParquetTimestampITCase>FsStreamingSinkITCaseBase.testPartitionWithBasicDate:126->FsStreamingSinkITCaseBase.test:186 » Validation  {code}

The errors each appear somewhat similar:
{code:java}
Dec 07 19:54:43 19:54:43.934 [ERROR] org.apache.flink.formats.parquet.ParquetTimestampITCase.testPartitionWithBasicDate Time elapsed: 1.822 s <<< ERROR! 
Dec 07 19:54:43 org.apache.flink.table.api.ValidationException: Unable to find a field named 'f0' in the physical data type derived from the given type information for schema declaration. Make sure that the type information is not a generic raw type. Currently available fields are: [a, b, c, d, e] 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.patchDataTypeFromColumn(SchemaTranslator.java:350) 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.patchDataTypeFromDeclaredSchema(SchemaTranslator.java:337) 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.createConsumingResult(SchemaTranslator.java:235) 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.createConsumingResult(SchemaTranslator.java:180) 
Dec 07 19:54:43 at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.fromStreamInternal(AbstractStreamTableEnvironmentImpl.java:141) 
Dec 07 19:54:43 at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.createTemporaryView(StreamTableEnvironmentImpl.scala:121) 
Dec 07 19:54:43 at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:186) 
Dec 07 19:54:43 at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPartitionWithBasicDate(FsStreamingSinkITCaseBase.scala:126)  {code}"	FLINK	Closed	3	1	9144	pull-request-available, test-stability
13436903	Update multiple Jackson dependencies to v2.13.2 and v2.13.2.1	"There is a High [CVE-2020-36518|https://nvd.nist.gov/vuln/detail/CVE-2020-36518], https://github.com/advisories/GHSA-57j2-w4cx-62h2
which was fixed with 2.13.2.1"	FLINK	Closed	3	11500	9144	pull-request-available
13526876	TRY_CAST fails for constructed types	"In case of problems with cast it is expected to return {{null}}

however for arrays, maps it fails

example of failing queries
{code:sql}
select try_cast(array['a'] as array<int>);
select try_cast(map['a', '1'] as map<int, int>);
{code}

 {noformat}
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: 'a'. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at StreamExecCalc$15.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{noformat}"	FLINK	Resolved	3	1	9144	pull-request-available
13445345	[JUnit5 Migration] Migrate TypeInformationTestBase to Junit5	"The task is a follow up for the feedback comment
https://github.com/apache/flink/pull/19716#discussion_r873685730"	FLINK	Closed	3	7	9144	pull-request-available
13549257	NPE when using GREATEST() in Flink SQL	"Hi,

I see NPEs in flink 1.14 and flink 1.16 when running queries with GREATEST() and timestamps. Below is an example to help in reproducing the issue.
{code:java}
CREATE TEMPORARY VIEW Positions AS
SELECT
SecurityId,
ccy1,
CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp
FROM (VALUES
(1, 'USD', '2022-01-01'),
(2, 'GBP', '2022-02-02'),
(3, 'GBX', '2022-03-03'),
(4, 'GBX', '2022-04-4'))
AS ccy(SecurityId, ccy1, publishTimestamp);

CREATE TEMPORARY VIEW Benchmarks AS
SELECT
SecurityId,
ccy1,
CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp
FROM (VALUES
(3, 'USD', '2023-01-01'),
(4, 'GBP', '2023-02-02'),
(5, 'GBX', '2023-03-03'),
(6, 'GBX', '2023-04-4'))
AS ccy(SecurityId, ccy1, publishTimestamp);

SELECT *,
GREATEST(
IFNULL(Positions.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))),
IFNULL(Benchmarks.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3)))
)
FROM Positions
FULL JOIN Benchmarks ON Positions.SecurityId = Benchmarks.SecurityId {code}
 

Using ""IF"" is a workaround at the moment instead of using ""GREATEST""

  "	FLINK	Resolved	4	1	9144	pull-request-available
13554648	 java.security.manager is disallowed by default starting in java18	"The change was done within [https://openjdk.org/jeps/411]

{quote}
Starting in Java 18, the default value of java.security.manager will be disallow if not otherwise set via java -D.... As a result, applications and libraries that call System::setSecurityManager may fail due to an unexpected UnsupportedOperationException. In order for System::setSecurityManager to work as before, the end user will have to set java.security.manager to allow on the command line (java -Djava.security.manager=allow ...).
{quote}"	FLINK	Closed	3	7	9144	pull-request-available
13576182	Release flink-connector-opensearch v1.2.0 and v.2.0.0 for Flink 1.19	"[https://github.com/apache/flink-connector-opensearch]

 "	FLINK	In Progress	3	7	9144	pull-request-available
13561454	Allow passing parameters to database via jdbc url	"Currently it does not allow to pass extra properties e.g.
an attempt to connect to 
{{jdbc:postgresql://...?sslmode=require}}
fails with 
{noformat}
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:229)
	at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:83)
	... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation b70b5cf7-7068-4eb6-83a4-78aed36dbd35.
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)


{noformat}
because of of a logic at {{org.apache.flink.connector.jdbc.catalog.JdbcCatalogUtils#validateJdbcUrl}}"	FLINK	Open	3	4	9144	pull-request-available
13417217	Support nulls in DataGen	"Currently it is impossible to specify that some values should be null sometimes.
It would be nice to have some property something like {{null-rate}} telling how often there should be {{null}} value generated
something like that
{code:sql}
CREATE TABLE Orders (
    order_number STRING,
    price        DECIMAL(32,2),
    buyer        ROW<id INT, last_name STRING>,
    order_time   TIMESTAMP(3),
    my_map       MAP<INT,STRING>,
    my_arrray    ARRAY<STRING>
) WITH (
   'connector' = 'datagen',
   'fields.order_number.null-rate' = '0.7',
   'fields.price.null-rate' = '1.0',
   'fields.order_time.null-rate' = '0.5',
   'fields.buyer.id.null-rate' = '0.5',
   'fields.buyer.null-rate' = '0.5',
   'fields.my_map.key.null-rate' = '0.5',
   'fields.my_map.null-rate' = '0.5',
   'fields.my_array.element.null-rate' = '0.1',
   'fields.my_array.null-rate' = '0.5'
);
{code}"	FLINK	Resolved	4	1	9144	pull-request-available, stale-assigned
13556767	FlinkImageBuilder checks for Java 21	"Currently for java 21 it fails like
{noformat}
Nov 04 03:03:08 Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image ""flink-configured-jobmanager""
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)
Nov 04 03:03:08 	... 61 more
Nov 04 03:03:08 Caused by: java.lang.IllegalStateException: Unexpected Java version: 21
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.getJavaVersionSuffix(FlinkImageBuilder.java:284)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.lambda$buildBaseImage$3(FlinkImageBuilder.java:250)
Nov 04 03:03:08 	at org.testcontainers.images.builder.traits.DockerfileTrait.withDockerfileFromBuilder(DockerfileTrait.java:19)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:246)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)
Nov 04 03:03:08 	... 62 more
Nov 04 03:03:08 

{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13554658	FlinkSecurityManagerITCase fails on java 21	"There are 2 tests {{testForcedJVMExit}} and {{testIgnoredJVMExit}} failing like 
{noformat}

expected: 222
 but was: 1
	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at org.apache.flink.runtime.util.FlinkSecurityManagerITCase.testForcedJVMExit(FlinkSecurityManagerITCase.java:51)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)

{noformat}

and
{noformat}

expected: 0
 but was: 1
	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at org.apache.flink.runtime.util.FlinkSecurityManagerITCase.testIgnoredJVMExit(FlinkSecurityManagerITCase.java:65)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)

{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13555181	Bump Jackson to 2.15.3	"Among others there is a number of improvements regarding parsing of numbers (jackson-core)
https://github.com/FasterXML/jackson-core/blob/2.16/release-notes/VERSION-2.x
"	FLINK	Closed	3	11500	9144	pull-request-available
13533267	Update maven cyclonedx plugin to 2.7.7	"there are at least 2 related improvements

1. current version depends on jackson-databind 2.14.0 and has a memory issue described at [https://github.com/FasterXML/jackson-databind/issues/3665] which is fixed in later versions

2. current version leads to lots of traces in logs (e.g. {{mvn clean verify}} for {{flink-core}}) which is fixed in later versions
{noformat}
[ERROR] An error occurred attempting to read POM
org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) 
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)
    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)
    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)
    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)
    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)
    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)
    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)
    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:196)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)

{noformat}"	FLINK	Closed	3	11500	9144	pull-request-available
13476726	Header in janino generated java files can merge with line numbers	"Since Line numbers are generated only for debug output it should not be a big issue.
From the other side currently this behavior leads to not compiled code.
The suggestion is usage of one-line comments for header to prevent this"	FLINK	Closed	4	1	9144	pull-request-available
13527628	Harden modifiers for sql-gateway module	This is a follow up jira issue for https://github.com/apache/flink/pull/22127#discussion_r1129192778	FLINK	Resolved	4	7	9144	pull-request-available
13533005	Upgrade Calcite version to 1.34.0	Calcite 1.34.0 has been released https://calcite.apache.org/news/2023/03/14/release-1.34.0/	FLINK	Open	3	11500	9144	pull-request-available
13411910	SQL syntax highlighting in SQL Client	"What to hightlight: keywords, quoted strings, sql identifier quoted string, line comments, block comments, hints.
Property {{sql-client.color-schema}} to set current highlighting schema"	FLINK	Closed	3	7	9144	pull-request-available
13544064	build_wheels_on_macos fails on AZP	"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51394&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=102
fails as
{noformat}
2023-07-19T00:18:36.5467620Z       Failed to build fastavro
2023-07-19T00:18:36.5507410Z       ERROR: Could not build wheels for fastavro, which is required to install pyproject.toml-based projects
2023-07-19T00:18:36.5540080Z       [end of output]
2023-07-19T00:18:36.5568470Z   
2023-07-19T00:18:36.5603540Z   note: This error originates from a subprocess, and is likely not a problem with pip.
2023-07-19T00:18:36.5633470Z error: subprocess-exited-with-error
2023-07-19T00:18:36.5669130Z 
2023-07-19T00:18:36.5709780Z × pip subprocess to install build dependencies did not run successfully.
2023-07-19T00:18:36.5737700Z │ exit code: 1
2023-07-19T00:18:36.5764350Z ╰─> See above for output.
2023-07-19T00:18:36.5791010Z 
2023-07-19T00:18:36.5819050Z note: This error originates from a subprocess, and is likely not a problem with pip.
2023-07-19T00:18:36.5847430Z ##[endgroup]
2023-07-19T00:18:36.5884460Z                                                              [31m✕ [0m56.47s
2023-07-19T00:18:36.5921230Z [91mError[0m: Command ['python', '-m', 'pip', 'wheel', '/Users/runner/work/1/s/flink-python', '--wheel-dir=/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/cibw-run-pzvjusx9/cp37-macosx_x86_64/built_wheel', '--no-deps'] failed with code 1. None
{noformat}

probably this is the reason of failing 
{quote}
is required to install pyproject.toml-based projects
{quote}

however not clear why it is started to fail only recently
 "	FLINK	Closed	2	1	9144	pull-request-available
13169852	"Duplicate lines for ""Weekday name (Sunday .. Saturday)"""	"could be seen at https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql.html#date-format-specifier
+ attach"	FLINK	Closed	4	4	9144	pull-request-available
13561594	Update junit to 5.10.1	"There is logging improved for 
>Exceptions thrown for files that cannot be deleted when cleaning up a temporary directory created via {{@TempDir}} now include the root cause.

[https://junit.org/junit5/docs/current/release-notes/index.html#bug-fixes-2]

which could help with debugging of FLINK-33641"	FLINK	Resolved	3	11500	9144	pull-request-available
13542381	Update apache parquet to 1.13.1	"Now 1.13.1 is available
https://parquet.apache.org/blog/"	FLINK	Resolved	3	11500	9144	pull-request-available
13138423	Add tests and documentation for WINDOW clause	"We support queries with a {{WINDOW}} clause like:

{code}
SELECT a, SUM(c) OVER w, MIN(c) OVER w FROM MyTable WINDOW w AS (PARTITION BY a ORDER BY proctime ROWS BETWEEN 4 PRECEDING AND CURRENT ROW)
{code}

But this is neither documented nor tested."	FLINK	Closed	3	4	9144	pull-request-available
13411912	Enable line numbers in SQL Client	"Should be enabled/disabled via property {{sql-client.prompt.show-line-numbers}}
Also  add widget to make it possible to toggle with a key-stroke"	FLINK	Closed	3	7	9144	pull-request-available
13555023	A number of json plan tests fail with comparisonfailure	"for instance
{noformat}
[ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.SortJsonPlanTest.testSort  Time elapsed: 0.037 s  <<< FAILURE!
org.junit.ComparisonFailure: 
expected:<...alse"",
            ""[table-sink-class"" : ""DEFAULT"",
            ""connector"" : ""values]""
          }
      ...> but was:<...alse"",
            ""[connector"" : ""values"",
            ""table-sink-class"" : ""DEFAULT]""
          }
      ...>
	at org.junit.Assert.assertEquals(Assert.java:117)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyJsonPlan(TableTestBase.scala:846)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyJsonPlan(TableTestBase.scala:813)
	at org.apache.flink.table.planner.plan.nodes.exec.stream.SortJsonPlanTest.testSort(SortJsonPlanTest.java:64)

{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13557189	Optimize the EXISTS sub-query by Metadata RowCount	"If the sub-query is guaranteed to produce at least one row, just return TRUE. If the sub-query is guaranteed to produce no row, just return FALSE.

inspired by CALCITE-5117 however since there is {{FlinkSubQueryRemoveRule}} then it shold be adopted accordingly

examples
{code:sql}
SELECT * FROM T2 WHERE EXISTS (SELECT SUM(a1), COUNT(*) FROM T1 WHERE 1=2)
{code}

aggregation functions always return 1 row even if there is an empty table then we could just replace this query with 
{code:sql}
SELECT * FROM T2 
{code}

another example
{code:sql}
SELECT * FROM MyTable WHERE NOT EXISTS (SELECT a FROM MyTable LIMIT 0)
{code}

{{LIMIT 0}} means no rows so it cold be optimized to

{code:sql}
SELECT * FROM MyTable
{code}

"	FLINK	Closed	3	4	9144	pull-request-available
13559367	Cleanup the rest usage of deprecated TableEnvironment#registerFunction and in docs	"It seems there are still few places where it is not cleaned up , also docs
e.g. 
{noformat}
docs/content.zh/docs/dev/table/functions/udfs.md
docs/content.zh/docs/dev/table/tableApi.md
docs/content/docs/dev/table/tableApi.md
flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/AggregatedTable.java
flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/FlatAggregateTable.java
flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/GroupedTable.java
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/harness/TableAggregateHarnessTest.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/GroupWindowITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/MatchRecognizeITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/OverAggregateITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/utils/UserDefinedFunctionTestUtils.scala

{noformat}"	FLINK	Resolved	3	7	9144	pull-request-available
13573430	Cannot convert org.apache.flink.streaming.api.CheckpointingMode  to org.apache.flink.core.execution.CheckpointingMode	"After this change FLINK-34516 elasticsearch connector for 1.20-SNAPSHOT starts failing with
{noformat}
 Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-e2e-tests/flink-connector-elasticsearch-e2e-tests-common/src/main/java/org/apache/flink/streaming/tests/ElasticsearchSinkE2ECaseBase.java:[75,5] method does not override or implement a method from a supertype
Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-e2e-tests/flink-connector-elasticsearch-e2e-tests-common/src/main/java/org/apache/flink/streaming/tests/ElasticsearchSinkE2ECaseBase.java:[85,84] incompatible types: org.apache.flink.streaming.api.CheckpointingMode cannot be converted to org.apache.flink.core.execution.CheckpointingMode
{noformat}
https://github.com/apache/flink-connector-elasticsearch/actions/runs/8436631571/job/23104522666#step:15:12668

set blocker since now every build of elasticsearch connector against  1.20-SNAPSHOT  is failing
probably same issue is for opensearch connector"	FLINK	Closed	1	1	9144	pull-request-available
13548494	VertexFlameGraphFactoryTest.testLambdaClassNamesCleanUp failed on AZP	"This build fails [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52564&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=8747]
{noformat}
Aug 24 01:35:46 01:35:46.537 [ERROR] org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.testLambdaClassNamesCleanUp  Time elapsed: 0.024 s  <<< FAILURE!
Aug 24 01:35:46 java.lang.AssertionError: 
Aug 24 01:35:46 
Aug 24 01:35:46 Expecting actual:
Aug 24 01:35:46   ""org.junit.platform.engine.support.hierarchical.NodeTestTask$$Lambda$0/0x0""
Aug 24 01:35:46 to end with:
Aug 24 01:35:46   ""$Lambda$0/0""
Aug 24 01:35:46 
Aug 24 01:35:46 	at org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.verifyRecursively(VertexFlameGraphFactoryTest.java:70)
Aug 24 01:35:46 	at java.base/java.util.stream.ReferencePipeline$4$1.accept(ReferencePipeline.java:212)
Aug 24 01:35:46 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
Aug 24 01:35:46 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
Aug 24 01:35:46 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
Aug 24 01:35:46 	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
Aug 24 01:35:46 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 24 01:35:46 	at java.base/java.util.stream.IntPipeline.reduce(IntPipeline.java:491)
Aug 24 01:35:46 	at java.base/java.util.stream.IntPipeline.sum(IntPipeline.java:449)
Aug 24 01:35:46 	at org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.verifyRecursively(VertexFlameGraphFactoryTest.java:72)
Aug 24 01:35:46 	at java.base/java.util.stream.ReferencePipeline$4$1.accept(ReferencePipeline.java:212)

{noformat}"	FLINK	Resolved	1	1	9144	pull-request-available, test-stability
13480356	Use config based constructors for flink calcite Converter rules	"Since Calcite 1.25 [1] there was introduced {{Config}} based constructors for rules and all other constructors are marked as deprecated
The task is to use {{Config}} based constructors for {{ConverterRule}}.
The process how to do it is described at {{RelRule}}'s javadoc [2].
With non {{ConverterRule}} it will be trickier because in 1.28 there were introduced code generation with immutables and somehow it should be supported for scala rules as well.
[1] https://issues.apache.org/jira/browse/CALCITE-3923
[2] https://github.com/apache/calcite/blob/main/core/src/main/java/org/apache/calcite/plan/RelRule.java#L45-L114"	FLINK	Resolved	4	11500	9144	pull-request-available
13350780	Upgrade Calcite version to 1.27	"The following files should be removed from the Flink code base during an upgrade:
 - org.apache.calcite.rex.RexSimplify
 - org.apache.calcite.sql.SqlMatchRecognize
 - org.apache.calcite.sql.SqlTableRef
 - org.apache.calcite.sql2rel.RelDecorrelator
 - org.apache.flink.table.planner.functions.sql.SqlJsonObjectFunction (added in FLINK-16203)
 - Adopt calcite's behaviour and add SQL tests once [https://github.com/apache/calcite/pull/2555] is merged, (check FLINK-24576 )"	FLINK	Closed	3	4	9144	pull-request-available
13410010	Ceiling/flooring dates to day return wrong results	"Query to reproduce
{code:sql}
select ceil(date '2021-11-04' to day) as `ceil`,
       floor(date '2021-11-04' to day) as `floor`;
{code}
gives
{noformat}
ceil         floor
8525-03-02 1970-01-01
{noformat}

expected
{noformat}
ceil         floor
2021-11-05 2021-11-05
{noformat}"	FLINK	Closed	4	1	9144	auto-deprioritized-major, pull-request-available
13554559	Allow to customize jdk version for connectors	This will allow to enable jdk 17 per connector once it's ready to compile and run with jdk17 for instance	FLINK	Closed	3	11500	9144	pull-request-available
13544724	TableSourceJsonPlanTest.testReuseSourceWithoutProjectionPushDown is failing	"Blocker since it's failing on every build and reproduced locally
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51661&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11529"	FLINK	Closed	1	1	9144	pull-request-available, test-stability
13329976	Port NumberSequenceSource to FLIP-27 source interface	"Both {{DataStream}} and {{DataSet}} APIs have a source generating a sequence of numbers.
This is useful for debugging and testing.

We should port this source to the FLIP-27 interface, to support testing programs with the new source API."	FLINK	Closed	3	4	9657	pull-request-available
12719252	Wasteful Memory Management	"
---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/84
Created by: [dimalabs|https://github.com/dimalabs]
Labels: 
Created at: Fri Sep 06 16:16:17 CEST 2013
State: open
"	FLINK	Resolved	3	4	9657	github-import
13393531	Remove redundant test cases in EventTimeWindowCheckpointingITCase	"HashMap state store snapshots are always async right now, sync snapshots are no longer supported.

We should adjust the {{EventTimeWindowCheckpointingITCase}} to remove the now redundant cases {{MEM_ASYNC}} and {{FILE_ASYNC}} parameter runs.

The test is very time-intensive, so this is quite a time saver."	FLINK	Closed	4	1	9657	pull-request-available
13308969	CoordinatorEventsExactlyOnceITCase.checkListContainsSequence fails on CI	"CI: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=330&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374

{code}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.795 s <<< FAILURE! - in org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase
[ERROR] test(org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase)  Time elapsed: 4.647 s  <<< FAILURE!
java.lang.AssertionError: List did not contain expected sequence of 200 elements, but was: [152, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.failList(CoordinatorEventsExactlyOnceITCase.java:160)
	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.checkListContainsSequence(CoordinatorEventsExactlyOnceITCase.java:148)
	at org.apache.flink.runtime.operators.coordination.CoordinatorEventsExactlyOnceITCase.test(CoordinatorEventsExactlyOnceITCase.java:143)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)

{code}"	FLINK	Closed	1	1	9657	pull-request-available, test-stability
13287082	"Exclude ""SdkMBeanRegistrySupport"" from dynamically loaded AWS connectors"	"The AWS SDK always registers a {{MetricAdminMBean}} at the JMX MBean Server.
This allows users to turn on / off more detailed metrics via the JMX admin interface.

However, this registered bean keeps the user code classloader alive and thus causes a class leak.

Excluding the {{com.amazonaws.jmx.SdkMBeanRegistrySupport}} class from the connectors disables the default registration thus preventing the class leak. It should only disable that JMX metric admin interface and still allow users to manually configure/activate metrics for the AWS SDK."	FLINK	Closed	3	1	9657	pull-request-available
13330476	SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent gets stuck	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7158&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=b4cd3436-dbe8-556d-3bca-42f92c3cbf2f

{code}
020-10-01T21:55:34.9982203Z ""main"" #1 prio=5 os_prio=0 cpu=1048.80ms elapsed=921.99s tid=0x00007f8c00015800 nid=0xf6e in Object.wait()  [0x00007f8c06648000]
2020-10-01T21:55:34.9982807Z    java.lang.Thread.State: WAITING (on object monitor)
2020-10-01T21:55:34.9983177Z 	at java.lang.Object.wait(java.base@11.0.7/Native Method)
2020-10-01T21:55:34.9983871Z 	- waiting on <0x000000008e0be190> (a org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread)
2020-10-01T21:55:34.9984581Z 	at java.lang.Thread.join(java.base@11.0.7/Thread.java:1305)
2020-10-01T21:55:34.9985433Z 	- waiting to re-lock in wait() <0x000000008e0be190> (a org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread)
2020-10-01T21:55:34.9985998Z 	at org.apache.flink.core.testutils.CheckedThread.trySync(CheckedThread.java:112)
2020-10-01T21:55:34.9986511Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:100)
2020-10-01T21:55:34.9987004Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:89)
2020-10-01T21:55:34.9987707Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread.shutdown(SplitFetcherTest.java:301)
2020-10-01T21:55:34.9988427Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent(SplitFetcherTest.java:131)
2020-10-01T21:55:34.9989025Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@11.0.7/Native Method)
2020-10-01T21:55:34.9989531Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@11.0.7/NativeMethodAccessorImpl.java:62)
2020-10-01T21:55:34.9990117Z 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@11.0.7/DelegatingMethodAccessorImpl.java:43)
2020-10-01T21:55:34.9990626Z 	at java.lang.reflect.Method.invoke(java.base@11.0.7/Method.java:566)
2020-10-01T21:55:34.9991078Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-01T21:55:34.9991602Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-01T21:55:34.9992119Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-01T21:55:34.9992749Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-01T21:55:34.9993229Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-01T21:55:34.9993700Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-01T21:55:34.9994202Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-01T21:55:34.9994670Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-01T21:55:34.9995098Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-01T21:55:34.9995524Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-01T21:55:34.9995965Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-01T21:55:34.9996403Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-01T21:55:34.9996816Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-01T21:55:34.9997268Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-10-01T21:55:34.9997695Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-10-01T21:55:34.9998077Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-01T21:55:34.9998510Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-01T21:55:34.9998941Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-01T21:55:34.9999380Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-01T21:55:34.9999815Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-01T21:55:35.0000226Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-01T21:55:35.0000662Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-10-01T21:55:35.0001190Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-10-01T21:55:35.0001746Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
2020-10-01T21:55:35.0002432Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
2020-10-01T21:55:35.0002974Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-10-01T21:55:35.0003797Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-10-01T21:55:35.0004349Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-01T21:55:35.0004913Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-01T21:55:35.0005427Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-01T21:55:35.0005900Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-01T21:55:35.0006196Z 
2020-10-01T21:55:35.0006585Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 cpu=0.79ms elapsed=921.95s tid=0x00007f8c0027e800 nid=0xf7b waiting on condition  [0x00007f8bd0af1000]
2020-10-01T21:55:35.0007132Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0007513Z 	at java.lang.ref.Reference.waitForReferencePendingList(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0008018Z 	at java.lang.ref.Reference.processPendingReferences(java.base@11.0.7/Reference.java:241)
2020-10-01T21:55:35.0008484Z 	at java.lang.ref.Reference$ReferenceHandler.run(java.base@11.0.7/Reference.java:213)
2020-10-01T21:55:35.0008769Z 
2020-10-01T21:55:35.0009134Z ""Finalizer"" #3 daemon prio=8 os_prio=0 cpu=0.48ms elapsed=921.95s tid=0x00007f8c00283000 nid=0xf7d in Object.wait()  [0x00007f8bd09f0000]
2020-10-01T21:55:35.0009630Z    java.lang.Thread.State: WAITING (on object monitor)
2020-10-01T21:55:35.0009980Z 	at java.lang.Object.wait(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0010612Z 	- waiting on <0x000000008e704170> (a java.lang.ref.ReferenceQueue$Lock)
2020-10-01T21:55:35.0011047Z 	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.7/ReferenceQueue.java:155)
2020-10-01T21:55:35.0011672Z 	- waiting to re-lock in wait() <0x000000008e704170> (a java.lang.ref.ReferenceQueue$Lock)
2020-10-01T21:55:35.0012130Z 	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.7/ReferenceQueue.java:176)
2020-10-01T21:55:35.0012672Z 	at java.lang.ref.Finalizer$FinalizerThread.run(java.base@11.0.7/Finalizer.java:170)
2020-10-01T21:55:35.0012943Z 
2020-10-01T21:55:35.0013322Z ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 cpu=0.52ms elapsed=921.94s tid=0x00007f8c00297800 nid=0xf7f runnable  [0x0000000000000000]
2020-10-01T21:55:35.0013773Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0013975Z 
2020-10-01T21:55:35.0014366Z ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 cpu=641.40ms elapsed=921.94s tid=0x00007f8c00299800 nid=0xf80 waiting on condition  [0x0000000000000000]
2020-10-01T21:55:35.0014854Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0015120Z    No compile task
2020-10-01T21:55:35.0015267Z 
2020-10-01T21:55:35.0015674Z ""C1 CompilerThread0"" #15 daemon prio=9 os_prio=0 cpu=491.81ms elapsed=921.94s tid=0x00007f8c0029c000 nid=0xf81 waiting on condition  [0x0000000000000000]
2020-10-01T21:55:35.0016145Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0016409Z    No compile task
2020-10-01T21:55:35.0016558Z 
2020-10-01T21:55:35.0016934Z ""Sweeper thread"" #20 daemon prio=9 os_prio=0 cpu=53.17ms elapsed=921.94s tid=0x00007f8c0029e000 nid=0xf82 runnable  [0x0000000000000000]
2020-10-01T21:55:35.0017450Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0017688Z 
2020-10-01T21:55:35.0018064Z ""Service Thread"" #21 daemon prio=9 os_prio=0 cpu=0.20ms elapsed=921.90s tid=0x00007f8c00321800 nid=0xf8a runnable  [0x0000000000000000]
2020-10-01T21:55:35.0018510Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0018710Z 
2020-10-01T21:55:35.0019343Z ""Common-Cleaner"" #22 daemon prio=8 os_prio=0 cpu=2.51ms elapsed=921.89s tid=0x00007f8c0032d000 nid=0xf90 in Object.wait()  [0x00007f8ba3ffe000]
2020-10-01T21:55:35.0019861Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-10-01T21:55:35.0020238Z 	at java.lang.Object.wait(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0020701Z 	- waiting on <no object reference available>
2020-10-01T21:55:35.0021178Z 	at java.lang.ref.ReferenceQueue.remove(java.base@11.0.7/ReferenceQueue.java:155)
2020-10-01T21:55:35.0021880Z 	- waiting to re-lock in wait() <0x000000008e705d98> (a java.lang.ref.ReferenceQueue$Lock)
2020-10-01T21:55:35.0022450Z 	at jdk.internal.ref.CleanerImpl.run(java.base@11.0.7/CleanerImpl.java:148)
2020-10-01T21:55:35.0022860Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0023273Z 	at jdk.internal.misc.InnocuousThread.run(java.base@11.0.7/InnocuousThread.java:134)
2020-10-01T21:55:35.0023544Z 
2020-10-01T21:55:35.0024207Z ""surefire-forkedjvm-command-thread"" #23 daemon prio=5 os_prio=0 cpu=14.02ms elapsed=921.83s tid=0x00007f8c003de000 nid=0xf92 runnable  [0x00007f8ba3ad5000]
2020-10-01T21:55:35.0024689Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0025037Z 	at java.io.FileInputStream.readBytes(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0025453Z 	at java.io.FileInputStream.read(java.base@11.0.7/FileInputStream.java:279)
2020-10-01T21:55:35.0025895Z 	at java.io.BufferedInputStream.fill(java.base@11.0.7/BufferedInputStream.java:252)
2020-10-01T21:55:35.0026359Z 	at java.io.BufferedInputStream.read(java.base@11.0.7/BufferedInputStream.java:271)
2020-10-01T21:55:35.0026942Z 	- locked <0x000000008e707330> (a java.io.BufferedInputStream)
2020-10-01T21:55:35.0027426Z 	at java.io.DataInputStream.readInt(java.base@11.0.7/DataInputStream.java:392)
2020-10-01T21:55:35.0027979Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-10-01T21:55:35.0028623Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-10-01T21:55:35.0029053Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0029280Z 
2020-10-01T21:55:35.0029927Z ""surefire-forkedjvm-ping-30s"" #24 daemon prio=5 os_prio=0 cpu=847.16ms elapsed=921.75s tid=0x00007f8c00473800 nid=0xf93 waiting on condition  [0x00007f8ba35c8000]
2020-10-01T21:55:35.0030448Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-10-01T21:55:35.0030804Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0031430Z 	- parking to wait for  <0x000000008e702fe0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-10-01T21:55:35.0031930Z 	at java.util.concurrent.locks.LockSupport.parkNanos(java.base@11.0.7/LockSupport.java:234)
2020-10-01T21:55:35.0032543Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(java.base@11.0.7/AbstractQueuedSynchronizer.java:2123)
2020-10-01T21:55:35.0033139Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.7/ScheduledThreadPoolExecutor.java:1182)
2020-10-01T21:55:35.0033730Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(java.base@11.0.7/ScheduledThreadPoolExecutor.java:899)
2020-10-01T21:55:35.0034270Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@11.0.7/ThreadPoolExecutor.java:1054)
2020-10-01T21:55:35.0034751Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.7/ThreadPoolExecutor.java:1114)
2020-10-01T21:55:35.0035246Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.7/ThreadPoolExecutor.java:628)
2020-10-01T21:55:35.0035671Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0035882Z 
2020-10-01T21:55:35.0036266Z ""process reaper"" #25 daemon prio=10 os_prio=0 cpu=278.14ms elapsed=921.73s tid=0x00007f8b94016000 nid=0xf97 waiting on condition  [0x00007f8c064d5000]
2020-10-01T21:55:35.0036747Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-10-01T21:55:35.0037150Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0037822Z 	- parking to wait for  <0x000000008e704488> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-10-01T21:55:35.0038282Z 	at java.util.concurrent.locks.LockSupport.parkNanos(java.base@11.0.7/LockSupport.java:234)
2020-10-01T21:55:35.0038782Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(java.base@11.0.7/SynchronousQueue.java:462)
2020-10-01T21:55:35.0039467Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(java.base@11.0.7/SynchronousQueue.java:361)
2020-10-01T21:55:35.0039943Z 	at java.util.concurrent.SynchronousQueue.poll(java.base@11.0.7/SynchronousQueue.java:937)
2020-10-01T21:55:35.0040413Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(java.base@11.0.7/ThreadPoolExecutor.java:1053)
2020-10-01T21:55:35.0040907Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.7/ThreadPoolExecutor.java:1114)
2020-10-01T21:55:35.0041393Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.7/ThreadPoolExecutor.java:628)
2020-10-01T21:55:35.0041803Z 	at java.lang.Thread.run(java.base@11.0.7/Thread.java:834)
2020-10-01T21:55:35.0042026Z 
2020-10-01T21:55:35.0042516Z ""Queue Drainer"" #28 prio=10 os_prio=0 cpu=0.95ms elapsed=920.78s tid=0x00007f8c009d6800 nid=0xfb7 waiting on condition  [0x00007f8ba22bd000]
2020-10-01T21:55:35.0042993Z    java.lang.Thread.State: WAITING (parking)
2020-10-01T21:55:35.0043338Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.7/Native Method)
2020-10-01T21:55:35.0043949Z 	- parking to wait for  <0x000000008df00fa8> (a java.util.concurrent.CompletableFuture$Signaller)
2020-10-01T21:55:35.0044408Z 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.7/LockSupport.java:194)
2020-10-01T21:55:35.0044887Z 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.7/CompletableFuture.java:1796)
2020-10-01T21:55:35.0045357Z 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.7/ForkJoinPool.java:3128)
2020-10-01T21:55:35.0045830Z 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.7/CompletableFuture.java:1823)
2020-10-01T21:55:35.0046742Z 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.7/CompletableFuture.java:1998)
2020-10-01T21:55:35.0047375Z 	at org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.take(FutureCompletingBlockingQueue.java:235)
2020-10-01T21:55:35.0048103Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest$QueueDrainerThread.go(SplitFetcherTest.java:289)
2020-10-01T21:55:35.0048659Z 	at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:74)
2020-10-01T21:55:35.0048929Z 
2020-10-01T21:55:35.0049311Z ""Attach Listener"" #29 daemon prio=9 os_prio=0 cpu=0.39ms elapsed=0.10s tid=0x00007f8bb4001000 nid=0x1922 waiting on condition  [0x0000000000000000]
2020-10-01T21:55:35.0049756Z    java.lang.Thread.State: RUNNABLE
2020-10-01T21:55:35.0049952Z 
2020-10-01T21:55:35.0050238Z ""VM Thread"" os_prio=0 cpu=128.10ms elapsed=921.96s tid=0x00007f8c00276800 nid=0xf7a runnable  
2020-10-01T21:55:35.0050517Z 
2020-10-01T21:55:35.0050805Z ""GC Thread#0"" os_prio=0 cpu=31.75ms elapsed=921.98s tid=0x00007f8c00041000 nid=0xf72 runnable  
2020-10-01T21:55:35.0051086Z 
2020-10-01T21:55:35.0051375Z ""GC Thread#1"" os_prio=0 cpu=27.13ms elapsed=921.35s tid=0x00007f8bc8001000 nid=0xfaa runnable  
2020-10-01T21:55:35.0051656Z 
2020-10-01T21:55:35.0051947Z ""GC Thread#2"" os_prio=0 cpu=28.21ms elapsed=921.35s tid=0x00007f8bc8002800 nid=0xfab runnable  
2020-10-01T21:55:35.0052230Z 
2020-10-01T21:55:35.0052594Z ""GC Thread#3"" os_prio=0 cpu=29.39ms elapsed=921.35s tid=0x00007f8bc8004000 nid=0xfac runnable  
2020-10-01T21:55:35.0052871Z 
2020-10-01T21:55:35.0053157Z ""GC Thread#4"" os_prio=0 cpu=27.78ms elapsed=921.35s tid=0x00007f8bc8005800 nid=0xfad runnable  
2020-10-01T21:55:35.0053437Z 
2020-10-01T21:55:35.0053722Z ""GC Thread#5"" os_prio=0 cpu=28.21ms elapsed=921.35s tid=0x00007f8bc8007000 nid=0xfae runnable  
2020-10-01T21:55:35.0054000Z 
2020-10-01T21:55:35.0054290Z ""G1 Main Marker"" os_prio=0 cpu=0.88ms elapsed=921.98s tid=0x00007f8c00074800 nid=0xf73 runnable  
2020-10-01T21:55:35.0054572Z 
2020-10-01T21:55:35.0054851Z ""G1 Conc#0"" os_prio=0 cpu=0.10ms elapsed=921.98s tid=0x00007f8c00076800 nid=0xf74 runnable  
2020-10-01T21:55:35.0055127Z 
2020-10-01T21:55:35.0055518Z ""G1 Refine#0"" os_prio=0 cpu=0.83ms elapsed=921.98s tid=0x00007f8c001aa800 nid=0xf77 runnable  
2020-10-01T21:55:35.0055852Z 
2020-10-01T21:55:35.0056161Z ""G1 Young RemSet Sampling"" os_prio=0 cpu=257.38ms elapsed=921.98s tid=0x00007f8c001ac800 nid=0xf78 runnable  
2020-10-01T21:55:35.0056683Z ""VM Periodic Task Thread"" os_prio=0 cpu=657.91ms elapsed=921.90s tid=0x00007f8c00324000 nid=0xf8b waiting on condition  
2020-10-01T21:55:35.0056992Z 
2020-10-01T21:55:35.0057263Z JNI global refs: 29, weak refs: 0
{code}"	FLINK	Closed	3	1	9657	test-stability
12822647	NullPointerException at org.apache.flink.client.program.Client's constructor while using ExecutionEnvironment.createRemoteEnvironment	"Trace:

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.flink.client.program.Client.<init>(Client.java:104)
	at org.apache.flink.client.RemoteExecutor.executePlanWithJars(RemoteExecutor.java:86)
	at org.apache.flink.client.RemoteExecutor.executePlan(RemoteExecutor.java:82)
	at org.apache.flink.api.java.RemoteEnvironment.execute(RemoteEnvironment.java:70)
	at Wordcount.main(Wordcount.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
{code}

The constructor is trying to set configuration parameter {{jobmanager.rpc.address}} with {{jobManagerAddress.getAddress().getHostAddress()}}, but {{jobManagerAddress.holder.addr}} is {{null}}. {{jobManagerAddress.holder.hostname}} and {{port}} holds the valid information."	FLINK	Closed	3	1	9657	yarn, yarn-client
12726764	Provide Iterable instead of Iterator to grouped functions	"I would like the grouped functions to provide an Iterable instead of an Iterator to the user, e.g. for the {{reduce}} method of {{GroupReduceFunction}}.

We had a discussion about this previously (I couldn't find the respective issues/list threads right now) and the result was in favor of the change.

We never got around to really push for it, because of the API break. With the renaming, it should be less of an issue."	FLINK	Resolved	4	4	9657	breaking-api
13272732	Implement OperatorEvent passing RPC.	Add the RPC in TaskExecutorGateway and JobMasterGateway to introduce OperatorEvent passing, including the related classes such as OperatorCoordinator.	FLINK	Closed	3	7	9657	pull-request-available
13311506	"Replace ""slave"" file name with ""workers"""	See parent issue for a discussion of the rationale.	FLINK	Closed	3	7	9657	pull-request-available
13400485	Test Environment / Mini Cluster do not forward configuration.	"When using {{StreamExecutionEnvironment getExecutionEnvironment(Configuration)}}, the config should determine the characteristics of the execution.

The config is for example passed to the local environment in the local execution case, and used during the instantiation of the MiniCluster.

But when using the {{TestStreamEnvironment}} and the {{MiniClusterWithClientRule}}, the config is ignored.

The issue is that the {{StreamExecutionEnvironmentFactory}} in {{TestStreamEnvironment}} ignores the config that is passed to it."	FLINK	Closed	3	1	9657	pull-request-available
13341341	The CheckpointCoordinator should reset the OperatorCoordinators when fail before the first checkpoint.	"Right now, if a job failed before the first successful checkpoint, the CheckpointCoordinator will not reset the OperatorCoordinator state. This may leave the OperatorCoordinators in inconsistent state.

The CheckpointCoordinator should also reset the OperatorCoordinator state in this case, just like it does for the master hooks. It essentially means ""reset to no checkpoint"". There are two options for the fix:
 # Add a reset() method to the OperatorCoordinator.
 # Call resetToCheckpoint(null) on the OperatorCoordinator."	FLINK	Closed	1	1	9657	pull-request-available
12719962	Parametrize aggregators and convergence criteria	"Changes so that aggregators and convergence criteria are registered using an object, rather than a class. 
This allows to pass parameters to aggregators and convergence criteria (see ([#731|https://github.com/stratosphere/stratosphere/issues/731] | [FLINK-731|https://issues.apache.org/jira/browse/FLINK-731])).
Also, enables registering aggregators in Delta iterations.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/782
Created by: [vasia|https://github.com/vasia]
Labels: 
Created at: Fri May 09 20:06:32 CEST 2014
State: open
"	FLINK	Resolved	3	1	9657	github-import
12719158	[GitHub] Rework Configuration Objects	"Currently, the configurations are implemented hacky. Everything is represented as a serialized string and there is no clean interface, such that different flavors of configurations (global-, delegatin-, default) are inconsistent.

I propose to rework the configuration as a map of objects, which are serialized on demand with either a serialization library, or default serialization mechanisms. Factoring out the interface of a Configuration allows to keep all flavors consistent.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/12
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: enhancement, 
Created at: Mon Apr 29 23:43:11 CEST 2013
State: open
"	FLINK	Resolved	3	4	9657	github-import
13326343	Port File Sources to FLIP-27 API	"Porting the File sources to the FLIP-27 API means combining the
  - FileInputFormat from the DataSet Batch API
  - The Monitoring File Source from the DataStream API.

The two currently share the same reader code already and partial enumeration code.

*Structure*

The new File Source will have three components:

  - File enumerators that discover the files.
  - File split assigners that decide which reader gets what split
  - File Reader Formats, which deal with the decoding.


The main difference between the Bounded (Batch) version and the unbounded (Streaming) version is that the streaming version repeatedly invokes the file enumerator to search for new files.

*Checkpointing Enumerators*

The enumerators need to checkpoint the not-yet-assigned splits, plus, if they are in continuous discovery mode (streaming) the paths / timestamps already processed.

*Checkpointing Readers*

The new File Source needs to ensure that every reader can be checkpointed.
Some readers may be able to expose the position in the input file that corresponds to the latest emitted record, but many will not be able to do that due to
  - storing compresses record batches
  - using buffered decoders where exact position information is not accessible

We therefore suggest to expose a mechanism that combines seekable file offsets and records to read and skip after that offset. In the extreme cases, files can work only with seekable positions or only with records-to-skip. Some sources, like Avro, can have periodic seek points (sync markers) and count records-to-skip after these markers.

*Efficient and Convenient Readers*

To balance efficiency (batch vectorized reading of ORC / Parquet for vectorized query processing) and convenience (plug in 3-rd party CSV decoder over stream) we offer three abstraction for record readers

  - Bulk Formats that run over a file Path and return a iterable batch at a time _(most efficient)_

  - File Record formats which read files record-by-record. The source framework hands over a pre-defined-size batch from Split Reader to Record Emitter.

  - Stream Formats that decode an input stream and rely on the source framework to decide how to batch record handover _(most convenient)_"	FLINK	Closed	3	7	9657	pull-request-available
13309445	CoordinatedSourceITCase.testMultipleSources gets stuck	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2705&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6b04ca5f-0b52-511d-19c9-52bf0d9fbdfa

{code}
2020-06-04T11:19:39.6335702Z [INFO] 
2020-06-04T11:19:39.6337440Z [INFO] -------------------------------------------------------
2020-06-04T11:19:39.6338176Z [INFO]  T E S T S
2020-06-04T11:19:39.6339305Z [INFO] -------------------------------------------------------
2020-06-04T11:19:40.1906157Z [INFO] Running org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase
2020-06-04T11:34:51.0599860Z ==============================================================================
2020-06-04T11:34:51.0603015Z Maven produced no output for 900 seconds.
2020-06-04T11:34:51.0604174Z ==============================================================================
2020-06-04T11:34:51.0613908Z ==============================================================================
2020-06-04T11:34:51.0615097Z The following Java processes are running (JPS)
2020-06-04T11:34:51.0616043Z ==============================================================================
2020-06-04T11:34:51.0762007Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-06-04T11:34:51.2775341Z 29635 surefirebooter5307550588991461882.jar
2020-06-04T11:34:51.2931264Z 2100 Launcher
2020-06-04T11:34:51.3012583Z 32203 Jps
2020-06-04T11:34:51.3258038Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-06-04T11:34:51.5443730Z ==============================================================================
2020-06-04T11:34:51.5445134Z Printing stack trace of Java process 29635
2020-06-04T11:34:51.5445984Z ==============================================================================
2020-06-04T11:34:51.5528602Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-06-04T11:34:51.9617670Z 2020-06-04 11:34:51
2020-06-04T11:34:51.9619131Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-06-04T11:34:51.9619732Z 
2020-06-04T11:34:51.9620618Z ""Attach Listener"" #299 daemon prio=9 os_prio=0 tid=0x00007f4d60001000 nid=0x7e59 waiting on condition [0x0000000000000000]
2020-06-04T11:34:51.9621720Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:51.9622190Z 
2020-06-04T11:34:51.9623631Z ""flink-akka.actor.default-dispatcher-185"" #297 prio=5 os_prio=0 tid=0x00007f4ca0003000 nid=0x7db4 waiting on condition [0x00007f4d10136000]
2020-06-04T11:34:51.9624972Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9625716Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9627072Z 	- parking to wait for  <0x0000000080c557f0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-06-04T11:34:51.9628593Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-06-04T11:34:51.9629649Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-04T11:34:51.9630825Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-04T11:34:51.9631559Z 
2020-06-04T11:34:51.9633020Z ""flink-akka.actor.default-dispatcher-186"" #298 prio=5 os_prio=0 tid=0x00007f4d08006800 nid=0x7db3 waiting on condition [0x00007f4d12974000]
2020-06-04T11:34:51.9634074Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9634965Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9636384Z 	- parking to wait for  <0x0000000080c557f0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-06-04T11:34:51.9637683Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-06-04T11:34:51.9638795Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-04T11:34:51.9639845Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-04T11:34:51.9640475Z 
2020-06-04T11:34:51.9642008Z ""flink-akka.actor.default-dispatcher-182"" #293 prio=5 os_prio=0 tid=0x00007f4ce4007000 nid=0x7d7f waiting on condition [0x00007f4d1063b000]
2020-06-04T11:34:51.9643266Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:51.9644081Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9645784Z 	- parking to wait for  <0x0000000080c557f0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-06-04T11:34:51.9647361Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-06-04T11:34:51.9648516Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-06-04T11:34:51.9649692Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-06-04T11:34:51.9651016Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-06-04T11:34:51.9651739Z 
2020-06-04T11:34:51.9653311Z ""jobmanager-future-thread-16"" #236 daemon prio=5 os_prio=0 tid=0x00007f4c94002000 nid=0x7a99 waiting on condition [0x00007f4d12873000]
2020-06-04T11:34:51.9654578Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9655376Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9656968Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9658360Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9659770Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9661421Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9663047Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9664605Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9665917Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9667296Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9668392Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9668914Z 
2020-06-04T11:34:51.9670561Z ""Flink-DispatcherRestEndpoint-thread-4"" #119 daemon prio=5 os_prio=0 tid=0x00007f4cc0004800 nid=0x749b waiting on condition [0x00007f4d1328f000]
2020-06-04T11:34:51.9671805Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9672620Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9674216Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9675411Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9676984Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9678652Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9680193Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9681672Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9682904Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9684149Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9685116Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9685563Z 
2020-06-04T11:34:51.9687028Z ""Flink-DispatcherRestEndpoint-thread-3"" #118 daemon prio=5 os_prio=0 tid=0x00007f4cc0003800 nid=0x7490 waiting on condition [0x00007f4c825e4000]
2020-06-04T11:34:51.9688144Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9688792Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9690067Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9691152Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9692335Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9693745Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9695311Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9696509Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9697640Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9698724Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9699618Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9700045Z 
2020-06-04T11:34:51.9701396Z ""Flink-DispatcherRestEndpoint-thread-2"" #115 daemon prio=5 os_prio=0 tid=0x00007f4cc0002800 nid=0x7478 waiting on condition [0x00007f4d12f78000]
2020-06-04T11:34:51.9702417Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9703084Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9704565Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9705653Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9706804Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9708230Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9709574Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9710779Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9711890Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9713162Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9714226Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9714854Z 
2020-06-04T11:34:51.9715760Z ""SourceFetcher"" #112 prio=5 os_prio=0 tid=0x00007f4c10006800 nid=0x746b waiting on condition [0x00007f4c812d3000]
2020-06-04T11:34:51.9716852Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9717754Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9719392Z 	- parking to wait for  <0x000000008777dc80> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9726297Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9727728Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9729254Z 	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
2020-06-04T11:34:51.9730516Z 	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
2020-06-04T11:34:51.9731853Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:113)
2020-06-04T11:34:51.9733084Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:94)
2020-06-04T11:34:51.9734253Z 	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
2020-06-04T11:34:51.9735413Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-06-04T11:34:51.9736573Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-04T11:34:51.9737863Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-04T11:34:51.9739214Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9740349Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9740883Z 
2020-06-04T11:34:51.9743025Z ""SourceCoordinator-Source: TestingSource2"" #110 prio=5 os_prio=0 tid=0x00007f4cf406a000 nid=0x7469 waiting on condition [0x00007f4c814d5000]
2020-06-04T11:34:51.9744816Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9745675Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9747526Z 	- parking to wait for  <0x0000000087a7aff8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9749566Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9751072Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9752591Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:51.9753926Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9755414Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9756776Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9757999Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9758523Z 
2020-06-04T11:34:51.9759948Z ""SourceCoordinator-Source: TestingSource1"" #109 prio=5 os_prio=0 tid=0x00007f4cf4068800 nid=0x7468 waiting on condition [0x00007f4c815d6000]
2020-06-04T11:34:51.9760998Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9761673Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9762927Z 	- parking to wait for  <0x0000000087200178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9764012Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9765377Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9766639Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:51.9767882Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9769195Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9770567Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9771666Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9772235Z 
2020-06-04T11:34:51.9773191Z ""Sink: Unnamed (2/4)"" #105 prio=5 os_prio=0 tid=0x00007f4cf4060800 nid=0x7464 waiting on condition [0x00007f4c81bda000]
2020-06-04T11:34:51.9774776Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9775591Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9777459Z 	- parking to wait for  <0x0000000087a7b278> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9778794Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9780275Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9782114Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:146)
2020-06-04T11:34:51.9783693Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:279)
2020-06-04T11:34:51.9785319Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:190)
2020-06-04T11:34:51.9786679Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181)
2020-06-04T11:34:51.9788033Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:557)
2020-06-04T11:34:51.9789165Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:529)
2020-06-04T11:34:51.9790155Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-06-04T11:34:51.9791088Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-06-04T11:34:51.9791879Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9792357Z 
2020-06-04T11:34:51.9793428Z ""OutputFlusher for Source: TestingSource2"" #98 daemon prio=5 os_prio=0 tid=0x00007f4c10001800 nid=0x745d waiting on condition [0x00007f4c822e1000]
2020-06-04T11:34:51.9794635Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-06-04T11:34:51.9795319Z 	at java.lang.Thread.sleep(Native Method)
2020-06-04T11:34:51.9796321Z 	at org.apache.flink.runtime.io.network.api.writer.RecordWriter$OutputFlusher.run(RecordWriter.java:334)
2020-06-04T11:34:51.9797036Z 
2020-06-04T11:34:51.9797921Z ""Source: TestingSource2 (2/4)"" #94 prio=5 os_prio=0 tid=0x00007f4cf4055800 nid=0x7459 waiting on condition [0x00007f4c826e5000]
2020-06-04T11:34:51.9798908Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9799555Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9800954Z 	- parking to wait for  <0x0000000087500178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9802006Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9803187Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9804559Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:146)
2020-06-04T11:34:51.9805858Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:279)
2020-06-04T11:34:51.9807228Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:190)
2020-06-04T11:34:51.9808529Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181)
2020-06-04T11:34:51.9809766Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:557)
2020-06-04T11:34:51.9810856Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:529)
2020-06-04T11:34:51.9811862Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720)
2020-06-04T11:34:51.9812794Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545)
2020-06-04T11:34:51.9813580Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9814025Z 
2020-06-04T11:34:51.9814876Z ""CloseableReaperThread"" #89 daemon prio=5 os_prio=0 tid=0x00007f4c28004800 nid=0x7454 in Object.wait() [0x00007f4c82bea000]
2020-06-04T11:34:51.9815918Z    java.lang.Thread.State: WAITING (on object monitor)
2020-06-04T11:34:51.9816734Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:51.9817885Z 	- waiting on <0x00000000878034a0> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:51.9818759Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-06-04T11:34:51.9819928Z 	- locked <0x00000000878034a0> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:51.9820808Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-06-04T11:34:51.9821937Z 	at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:204)
2020-06-04T11:34:51.9822706Z 
2020-06-04T11:34:51.9823940Z ""Flink-MetricRegistry-thread-1"" #87 daemon prio=5 os_prio=0 tid=0x00007f4cf403a000 nid=0x7452 waiting on condition [0x00007f4c82dec000]
2020-06-04T11:34:51.9825063Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:51.9825768Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9827061Z 	- parking to wait for  <0x0000000080c46498> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9828195Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:51.9829405Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:51.9830790Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:51.9832105Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9833408Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9834537Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9835667Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9836568Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9837003Z 
2020-06-04T11:34:51.9838290Z ""jobmanager-future-thread-15"" #86 daemon prio=5 os_prio=0 tid=0x00007f4d000e4800 nid=0x7451 waiting on condition [0x00007f4c82eed000]
2020-06-04T11:34:51.9839293Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9839970Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9841223Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9842294Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9843451Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9844893Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:51.9846266Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9847511Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9848582Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9849641Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9850531Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9850957Z 
2020-06-04T11:34:51.9852196Z ""jobmanager-future-thread-14"" #85 daemon prio=5 os_prio=0 tid=0x00007f4d000e2800 nid=0x7450 waiting on condition [0x00007f4c82fee000]
2020-06-04T11:34:51.9853177Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9853845Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9855174Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9856287Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9857618Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9858956Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9860293Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9861465Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9862544Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9863626Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9864587Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9865040Z 
2020-06-04T11:34:51.9866307Z ""jobmanager-future-thread-13"" #84 daemon prio=5 os_prio=0 tid=0x00007f4d000e1000 nid=0x744f waiting on condition [0x00007f4c830ef000]
2020-06-04T11:34:51.9867374Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9868019Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9869286Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9870338Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9871506Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9872859Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:51.9874275Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9875582Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9876638Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9877780Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9878683Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9879109Z 
2020-06-04T11:34:51.9880327Z ""jobmanager-future-thread-12"" #83 daemon prio=5 os_prio=0 tid=0x00007f4d000df800 nid=0x744e waiting on condition [0x00007f4c831f0000]
2020-06-04T11:34:51.9881409Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9882126Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9883427Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9884608Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9885796Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9887213Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:51.9888559Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9889735Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9890801Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9891859Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9892754Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9893187Z 
2020-06-04T11:34:51.9894424Z ""jobmanager-future-thread-11"" #82 daemon prio=5 os_prio=0 tid=0x00007f4d000dd800 nid=0x744d waiting on condition [0x00007f4c832f1000]
2020-06-04T11:34:51.9895512Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9896193Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9897529Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9898711Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9899878Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9901271Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9902744Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9904043Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9905324Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9906518Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9907468Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9907922Z 
2020-06-04T11:34:51.9909177Z ""jobmanager-future-thread-10"" #81 daemon prio=5 os_prio=0 tid=0x00007f4d000db800 nid=0x744c waiting on condition [0x00007f4c833f2000]
2020-06-04T11:34:51.9910178Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9910825Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9912091Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9913164Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9914314Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9915996Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9917386Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9918590Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9919657Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9920718Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9921606Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9922033Z 
2020-06-04T11:34:51.9923282Z ""jobmanager-future-thread-9"" #80 daemon prio=5 os_prio=0 tid=0x00007f4d000da000 nid=0x744b waiting on condition [0x00007f4c834f3000]
2020-06-04T11:34:51.9924257Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9925107Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9926445Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9927613Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9928764Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9930131Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9931469Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9932647Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9933721Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9934914Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9935852Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9936281Z 
2020-06-04T11:34:51.9937591Z ""jobmanager-future-thread-8"" #79 daemon prio=5 os_prio=0 tid=0x00007f4d000d8000 nid=0x744a waiting on condition [0x00007f4c835f4000]
2020-06-04T11:34:51.9938593Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9939359Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9940645Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9941698Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9942880Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9944220Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9945780Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9946985Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9948097Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9949185Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9950060Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9950506Z 
2020-06-04T11:34:51.9951732Z ""jobmanager-future-thread-7"" #78 daemon prio=5 os_prio=0 tid=0x00007f4d000d6800 nid=0x7449 waiting on condition [0x00007f4c836f5000]
2020-06-04T11:34:51.9952738Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9953383Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9954784Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9956012Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9957224Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9958589Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9959909Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9961100Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9962169Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9963229Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9964122Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9964684Z 
2020-06-04T11:34:51.9965965Z ""jobmanager-future-thread-6"" #77 daemon prio=5 os_prio=0 tid=0x00007f4d000d5000 nid=0x7448 waiting on condition [0x00007f4c837f6000]
2020-06-04T11:34:51.9966947Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9967675Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9968927Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9970017Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9971191Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:51.9972532Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:51.9973873Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9975208Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9976321Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9977472Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9978347Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9978776Z 
2020-06-04T11:34:51.9980010Z ""jobmanager-future-thread-5"" #76 daemon prio=5 os_prio=0 tid=0x00007f4d000d3800 nid=0x7447 waiting on condition [0x00007f4c838f7000]
2020-06-04T11:34:51.9981144Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:51.9981812Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9983091Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9984161Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:51.9985490Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:51.9986902Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:51.9988288Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:51.9989486Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:51.9990539Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:51.9991623Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:51.9992519Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:51.9992945Z 
2020-06-04T11:34:51.9994158Z ""jobmanager-future-thread-4"" #75 daemon prio=5 os_prio=0 tid=0x00007f4d000d1000 nid=0x7446 waiting on condition [0x00007f4c839f8000]
2020-06-04T11:34:51.9995240Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:51.9996082Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:51.9997404Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:51.9998474Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:51.9999624Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0000989Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0002324Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0003501Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0004656Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0005750Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0006656Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0007083Z 
2020-06-04T11:34:52.0008361Z ""mini-cluster-io-thread-14"" #74 daemon prio=5 os_prio=0 tid=0x00007f4cf402b000 nid=0x7445 waiting on condition [0x00007f4c83af9000]
2020-06-04T11:34:52.0009334Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0010002Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0011256Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0012329Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0013501Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0014777Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0015971Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0017254Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0018345Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0019239Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0019668Z 
2020-06-04T11:34:52.0020806Z ""pool-2-thread-1"" #73 prio=5 os_prio=0 tid=0x00007f4cf4028800 nid=0x7444 waiting on condition [0x00007f4c83bfa000]
2020-06-04T11:34:52.0021869Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0022542Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0023806Z 	- parking to wait for  <0x000000008797ae48> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0024972Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0026160Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0027597Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:52.0028931Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0030107Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0031193Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0032255Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0033149Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0033574Z 
2020-06-04T11:34:52.0034882Z ""jobmanager-future-thread-3"" #72 daemon prio=5 os_prio=0 tid=0x00007f4cf4021000 nid=0x7441 waiting on condition [0x00007f4c83cfb000]
2020-06-04T11:34:52.0035891Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0036563Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0038036Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0039090Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0040263Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0041614Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0042956Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0044161Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0045427Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0046545Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0047480Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0047941Z 
2020-06-04T11:34:52.0049160Z ""mini-cluster-io-thread-13"" #71 daemon prio=5 os_prio=0 tid=0x00007f4d000c2000 nid=0x7440 waiting on condition [0x00007f4c83dfc000]
2020-06-04T11:34:52.0050151Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0050796Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0052074Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0053149Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0054301Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0055639Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0056678Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0057821Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0058882Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0059783Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0060209Z 
2020-06-04T11:34:52.0061450Z ""jobmanager-future-thread-2"" #70 daemon prio=5 os_prio=0 tid=0x00007f4d0802a800 nid=0x743f waiting on condition [0x00007f4c83efd000]
2020-06-04T11:34:52.0062567Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0063214Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0064577Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0065663Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0066842Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0068250Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0069584Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0070782Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0071836Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0072916Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0073789Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0074242Z 
2020-06-04T11:34:52.0075755Z ""FlinkCompletableFutureDelayScheduler-thread-1"" #69 daemon prio=5 os_prio=0 tid=0x00007f4d08021000 nid=0x743e waiting on condition [0x00007f4c83ffe000]
2020-06-04T11:34:52.0076828Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0077536Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0078926Z 	- parking to wait for  <0x000000008777e698> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0079998Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0081149Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0082514Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-06-04T11:34:52.0083830Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0085239Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0086384Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0087523Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0088428Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0088855Z 
2020-06-04T11:34:52.0090097Z ""mini-cluster-io-thread-12"" #68 daemon prio=5 os_prio=0 tid=0x00007f4d0800d000 nid=0x743d waiting on condition [0x00007f4d115da000]
2020-06-04T11:34:52.0091065Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0091736Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0092994Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0094069Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0095322Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0096546Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0097702Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0098758Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0099837Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0100733Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0101161Z 
2020-06-04T11:34:52.0102374Z ""mini-cluster-io-thread-11"" #67 daemon prio=5 os_prio=0 tid=0x00007f4d000b4800 nid=0x743c waiting on condition [0x00007f4d1196c000]
2020-06-04T11:34:52.0103496Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0104166Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0105557Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0106636Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0107863Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0109078Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0110111Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0111187Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0112263Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0113141Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0113590Z 
2020-06-04T11:34:52.0114888Z ""mini-cluster-io-thread-10"" #64 daemon prio=5 os_prio=0 tid=0x00007f4cb000b000 nid=0x7439 waiting on condition [0x00007f4d10338000]
2020-06-04T11:34:52.0115908Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0116557Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0117907Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0125989Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0127262Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0128486Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0129522Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0130608Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0131667Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0132564Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0132994Z 
2020-06-04T11:34:52.0134343Z ""mini-cluster-io-thread-9"" #63 daemon prio=5 os_prio=0 tid=0x00007f4cb0006800 nid=0x7438 waiting on condition [0x00007f4d10439000]
2020-06-04T11:34:52.0135607Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0136372Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0137749Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0138798Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0139975Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0141174Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0142229Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0143301Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0144361Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0145444Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0145909Z 
2020-06-04T11:34:52.0147243Z ""jobmanager-future-thread-1"" #62 daemon prio=5 os_prio=0 tid=0x00007f4cb001c000 nid=0x7437 waiting on condition [0x00007f4d1053a000]
2020-06-04T11:34:52.0148220Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0148887Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0150135Z 	- parking to wait for  <0x000000008797abe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0151379Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0152526Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0153885Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-06-04T11:34:52.0155318Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0156536Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0157700Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0158762Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0159663Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0160089Z 
2020-06-04T11:34:52.0161328Z ""mini-cluster-io-thread-8"" #60 daemon prio=5 os_prio=0 tid=0x00007f4cfc017800 nid=0x7435 waiting on condition [0x00007f4d1093c000]
2020-06-04T11:34:52.0162317Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0162964Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0164236Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0165367Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0166572Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0168520Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0169554Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0170631Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0171697Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0172596Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0173024Z 
2020-06-04T11:34:52.0174262Z ""mini-cluster-io-thread-7"" #59 daemon prio=5 os_prio=0 tid=0x00007f4cfc016000 nid=0x7434 waiting on condition [0x00007f4d10a3d000]
2020-06-04T11:34:52.0175318Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0176020Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0177340Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0178416Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0179587Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0180777Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0181838Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0182888Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0183966Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0185038Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0185530Z 
2020-06-04T11:34:52.0186770Z ""mini-cluster-io-thread-6"" #58 daemon prio=5 os_prio=0 tid=0x00007f4cfc014800 nid=0x7433 waiting on condition [0x00007f4d10b3e000]
2020-06-04T11:34:52.0187836Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0188506Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0189757Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0190825Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0191976Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0193303Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0194355Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0195678Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0197319Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0198196Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0198648Z 
2020-06-04T11:34:52.0199892Z ""mini-cluster-io-thread-5"" #57 daemon prio=5 os_prio=0 tid=0x00007f4cb4003800 nid=0x7432 waiting on condition [0x00007f4d10c3f000]
2020-06-04T11:34:52.0200877Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0201523Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0202784Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0203856Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0205091Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0206342Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0207441Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0208512Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0209704Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0210593Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0211018Z 
2020-06-04T11:34:52.0212247Z ""mini-cluster-io-thread-4"" #56 daemon prio=5 os_prio=0 tid=0x00007f4cc616e000 nid=0x7431 waiting on condition [0x00007f4d10d40000]
2020-06-04T11:34:52.0213243Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0213887Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0215292Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0216467Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0217738Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0218929Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0219990Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0221055Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0222114Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0223005Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0223439Z 
2020-06-04T11:34:52.0224768Z ""mini-cluster-io-thread-3"" #55 daemon prio=5 os_prio=0 tid=0x00007f4cf4014800 nid=0x7430 waiting on condition [0x00007f4d10e41000]
2020-06-04T11:34:52.0225773Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0226441Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0227766Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0228837Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0230003Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0231194Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0232248Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0233294Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0234581Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0235493Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0235938Z 
2020-06-04T11:34:52.0237207Z ""mini-cluster-io-thread-2"" #54 daemon prio=5 os_prio=0 tid=0x00007f4cc6141800 nid=0x742f waiting on condition [0x00007f4d10f42000]
2020-06-04T11:34:52.0238195Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0238860Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0240116Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0241187Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0242335Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0243547Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0244662Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0245767Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0246845Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0247775Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0248219Z 
2020-06-04T11:34:52.0249480Z ""Flink-DispatcherRestEndpoint-thread-1"" #53 daemon prio=5 os_prio=0 tid=0x00007f4cc60c4800 nid=0x742e waiting on condition [0x00007f4d11043000]
2020-06-04T11:34:52.0250644Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:52.0251313Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0252604Z 	- parking to wait for  <0x0000000087803238> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0253674Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:52.0254977Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:52.0256397Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:52.0257767Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0258964Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0260021Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0261102Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0261991Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0262415Z 
2020-06-04T11:34:52.0263611Z ""mini-cluster-io-thread-1"" #52 daemon prio=5 os_prio=0 tid=0x00007f4cc60c3000 nid=0x742d waiting on condition [0x00007f4d11144000]
2020-06-04T11:34:52.0264670Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0265335Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0266619Z 	- parking to wait for  <0x000000008797b270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0267750Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0268900Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0270115Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0271168Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0272219Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0273298Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0274314Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0274838Z 
2020-06-04T11:34:52.0276095Z ""flink-rest-server-netty-boss-thread-1"" #51 daemon prio=5 os_prio=0 tid=0x00007f4cc6062000 nid=0x742c runnable [0x00007f4d11245000]
2020-06-04T11:34:52.0277062Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0277813Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-06-04T11:34:52.0278653Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-06-04T11:34:52.0279581Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-06-04T11:34:52.0280504Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-06-04T11:34:52.0281888Z 	- locked <0x00000000879040b0> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2020-06-04T11:34:52.0283156Z 	- locked <0x00000000879040a0> (a java.util.Collections$UnmodifiableSet)
2020-06-04T11:34:52.0284265Z 	- locked <0x0000000087904058> (a sun.nio.ch.EPollSelectorImpl)
2020-06-04T11:34:52.0285156Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-06-04T11:34:52.0286383Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
2020-06-04T11:34:52.0287813Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)
2020-06-04T11:34:52.0288990Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)
2020-06-04T11:34:52.0290320Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-06-04T11:34:52.0291783Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-06-04T11:34:52.0292788Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0293213Z 
2020-06-04T11:34:52.0294026Z ""IOManager reader thread #1"" #46 daemon prio=5 os_prio=0 tid=0x00007f4e711b9000 nid=0x7429 waiting on condition [0x00007f4d1206d000]
2020-06-04T11:34:52.0295102Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0295781Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0297076Z 	- parking to wait for  <0x000000008797ba50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0298195Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0299365Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0300565Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0301714Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)
2020-06-04T11:34:52.0302413Z 
2020-06-04T11:34:52.0303226Z ""IOManager writer thread #1"" #45 daemon prio=5 os_prio=0 tid=0x00007f4e711b7000 nid=0x7428 waiting on condition [0x00007f4d1216e000]
2020-06-04T11:34:52.0304216Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0304942Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0306264Z 	- parking to wait for  <0x000000008777ee58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0307417Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0308735Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-06-04T11:34:52.0309996Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-06-04T11:34:52.0311166Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)
2020-06-04T11:34:52.0311865Z 
2020-06-04T11:34:52.0312999Z ""Timer-2"" #43 daemon prio=5 os_prio=0 tid=0x00007f4e7117f000 nid=0x7427 in Object.wait() [0x00007f4d1226f000]
2020-06-04T11:34:52.0313971Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-06-04T11:34:52.0314967Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0316159Z 	- waiting on <0x000000008797bc58> (a java.util.TaskQueue)
2020-06-04T11:34:52.0316929Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-06-04T11:34:52.0318069Z 	- locked <0x000000008797bc58> (a java.util.TaskQueue)
2020-06-04T11:34:52.0318790Z 	at java.util.TimerThread.run(Timer.java:505)
2020-06-04T11:34:52.0319253Z 
2020-06-04T11:34:52.0320336Z ""Timer-1"" #41 daemon prio=5 os_prio=0 tid=0x00007f4e7117c800 nid=0x7426 in Object.wait() [0x00007f4d12370000]
2020-06-04T11:34:52.0321313Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-06-04T11:34:52.0322018Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0322990Z 	- waiting on <0x0000000087803a80> (a java.util.TaskQueue)
2020-06-04T11:34:52.0323768Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-06-04T11:34:52.0324863Z 	- locked <0x0000000087803a80> (a java.util.TaskQueue)
2020-06-04T11:34:52.0325652Z 	at java.util.TimerThread.run(Timer.java:505)
2020-06-04T11:34:52.0326096Z 
2020-06-04T11:34:52.0326861Z ""BLOB Server listener at 41173"" #37 daemon prio=5 os_prio=0 tid=0x00007f4e7117a000 nid=0x7425 runnable [0x00007f4d12471000]
2020-06-04T11:34:52.0327864Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0328567Z 	at java.net.PlainSocketImpl.socketAccept(Native Method)
2020-06-04T11:34:52.0329437Z 	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
2020-06-04T11:34:52.0330392Z 	at java.net.ServerSocket.implAccept(ServerSocket.java:560)
2020-06-04T11:34:52.0331352Z 	at java.net.ServerSocket.accept(ServerSocket.java:528)
2020-06-04T11:34:52.0332267Z 	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)
2020-06-04T11:34:52.0332818Z 
2020-06-04T11:34:52.0333941Z ""Timer-0"" #38 daemon prio=5 os_prio=0 tid=0x00007f4e71160000 nid=0x7424 in Object.wait() [0x00007f4d12772000]
2020-06-04T11:34:52.0334980Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-06-04T11:34:52.0335747Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0336733Z 	- waiting on <0x0000000087803a68> (a java.util.TaskQueue)
2020-06-04T11:34:52.0337565Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-06-04T11:34:52.0338608Z 	- locked <0x0000000087803a68> (a java.util.TaskQueue)
2020-06-04T11:34:52.0339327Z 	at java.util.TimerThread.run(Timer.java:505)
2020-06-04T11:34:52.0339785Z 
2020-06-04T11:34:52.0341535Z ""flink-metrics-scheduler-1"" #33 prio=5 os_prio=0 tid=0x00007f4e71126000 nid=0x7420 waiting on condition [0x00007f4d12b76000]
2020-06-04T11:34:52.0342549Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-06-04T11:34:52.0343227Z 	at java.lang.Thread.sleep(Native Method)
2020-06-04T11:34:52.0344121Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-06-04T11:34:52.0345384Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-06-04T11:34:52.0346626Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-06-04T11:34:52.0347630Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0348057Z 
2020-06-04T11:34:52.0349204Z ""flink-scheduler-1"" #27 prio=5 os_prio=0 tid=0x00007f4e70d14800 nid=0x7418 waiting on condition [0x00007f4d13390000]
2020-06-04T11:34:52.0350172Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-06-04T11:34:52.0350872Z 	at java.lang.Thread.sleep(Native Method)
2020-06-04T11:34:52.0351746Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-06-04T11:34:52.0352929Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-06-04T11:34:52.0354102Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-06-04T11:34:52.0355209Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0355675Z 
2020-06-04T11:34:52.0356453Z ""process reaper"" #24 daemon prio=10 os_prio=0 tid=0x00007f4d18048800 nid=0x7415 waiting on condition [0x00007f4d13dce000]
2020-06-04T11:34:52.0357656Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:52.0358349Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0359574Z 	- parking to wait for  <0x0000000080c55a08> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-06-04T11:34:52.0360584Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:52.0361681Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-06-04T11:34:52.0362832Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-06-04T11:34:52.0363905Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-06-04T11:34:52.0365103Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-06-04T11:34:52.0366197Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0367350Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0368223Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0368669Z 
2020-06-04T11:34:52.0369901Z ""surefire-forkedjvm-ping-30s"" #23 daemon prio=5 os_prio=0 tid=0x00007f4e703f6000 nid=0x7412 waiting on condition [0x00007f4d343c2000]
2020-06-04T11:34:52.0370924Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-06-04T11:34:52.0371592Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0372853Z 	- parking to wait for  <0x0000000080c79c40> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-06-04T11:34:52.0374047Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-06-04T11:34:52.0375351Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-06-04T11:34:52.0376773Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-06-04T11:34:52.0378153Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-06-04T11:34:52.0379351Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-06-04T11:34:52.0380402Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-06-04T11:34:52.0381480Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-04T11:34:52.0382367Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0382797Z 
2020-06-04T11:34:52.0383991Z ""surefire-forkedjvm-command-thread"" #22 daemon prio=5 os_prio=0 tid=0x00007f4e703c4000 nid=0x7411 runnable [0x00007f4d346cd000]
2020-06-04T11:34:52.0385010Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0385725Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-06-04T11:34:52.0386500Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-06-04T11:34:52.0387475Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-06-04T11:34:52.0388394Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-06-04T11:34:52.0389548Z 	- locked <0x0000000080c9fff8> (a java.io.BufferedInputStream)
2020-06-04T11:34:52.0390386Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-06-04T11:34:52.0391401Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-06-04T11:34:52.0392588Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-06-04T11:34:52.0393518Z 	at java.lang.Thread.run(Thread.java:748)
2020-06-04T11:34:52.0393960Z 
2020-06-04T11:34:52.0394775Z ""Service Thread"" #21 daemon prio=9 os_prio=0 tid=0x00007f4e702d5000 nid=0x740f runnable [0x0000000000000000]
2020-06-04T11:34:52.0395757Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0396158Z 
2020-06-04T11:34:52.0396945Z ""C1 CompilerThread14"" #20 daemon prio=9 os_prio=0 tid=0x00007f4e702c8000 nid=0x740e waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0398088Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0398505Z 
2020-06-04T11:34:52.0399271Z ""C1 CompilerThread13"" #19 daemon prio=9 os_prio=0 tid=0x00007f4e702c5800 nid=0x740d waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0400201Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0400594Z 
2020-06-04T11:34:52.0401379Z ""C1 CompilerThread12"" #18 daemon prio=9 os_prio=0 tid=0x00007f4e702c3800 nid=0x740c waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0402290Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0402685Z 
2020-06-04T11:34:52.0403469Z ""C1 CompilerThread11"" #17 daemon prio=9 os_prio=0 tid=0x00007f4e702c1800 nid=0x740b waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0404393Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0404872Z 
2020-06-04T11:34:52.0405666Z ""C1 CompilerThread10"" #16 daemon prio=9 os_prio=0 tid=0x00007f4e702c0000 nid=0x740a waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0406601Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0406994Z 
2020-06-04T11:34:52.0407850Z ""C2 CompilerThread9"" #15 daemon prio=9 os_prio=0 tid=0x00007f4e702bd000 nid=0x7409 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0408779Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0409172Z 
2020-06-04T11:34:52.0409935Z ""C2 CompilerThread8"" #14 daemon prio=9 os_prio=0 tid=0x00007f4e702bb000 nid=0x7408 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0410862Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0411376Z 
2020-06-04T11:34:52.0412166Z ""C2 CompilerThread7"" #13 daemon prio=9 os_prio=0 tid=0x00007f4e702b9000 nid=0x7407 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0413076Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0413488Z 
2020-06-04T11:34:52.0414247Z ""C2 CompilerThread6"" #12 daemon prio=9 os_prio=0 tid=0x00007f4e702b7000 nid=0x7406 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0415279Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0415709Z 
2020-06-04T11:34:52.0416490Z ""C2 CompilerThread5"" #11 daemon prio=9 os_prio=0 tid=0x00007f4e702b5000 nid=0x7405 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0417456Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0417872Z 
2020-06-04T11:34:52.0418634Z ""C2 CompilerThread4"" #10 daemon prio=9 os_prio=0 tid=0x00007f4e702b3000 nid=0x7404 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0419554Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0419952Z 
2020-06-04T11:34:52.0420729Z ""C2 CompilerThread3"" #9 daemon prio=9 os_prio=0 tid=0x00007f4e702a9000 nid=0x7403 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0421635Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0422055Z 
2020-06-04T11:34:52.0422815Z ""C2 CompilerThread2"" #8 daemon prio=9 os_prio=0 tid=0x00007f4e702a6800 nid=0x7402 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0423747Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0424141Z 
2020-06-04T11:34:52.0424985Z ""C2 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f4e702a4800 nid=0x7401 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0425921Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0426339Z 
2020-06-04T11:34:52.0427094Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f4e702a2800 nid=0x7400 waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0428062Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0428457Z 
2020-06-04T11:34:52.0429191Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f4e702a0800 nid=0x73ff runnable [0x0000000000000000]
2020-06-04T11:34:52.0430050Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0430443Z 
2020-06-04T11:34:52.0431290Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f4e7029f000 nid=0x73fe waiting on condition [0x0000000000000000]
2020-06-04T11:34:52.0432508Z    java.lang.Thread.State: RUNNABLE
2020-06-04T11:34:52.0432969Z 
2020-06-04T11:34:52.0433783Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f4e7026e800 nid=0x73fd in Object.wait() [0x00007f4d7d0b2000]
2020-06-04T11:34:52.0434997Z    java.lang.Thread.State: WAITING (on object monitor)
2020-06-04T11:34:52.0435827Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0437241Z 	- waiting on <0x0000000080c56688> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:52.0438175Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-06-04T11:34:52.0439453Z 	- locked <0x0000000080c56688> (a java.lang.ref.ReferenceQueue$Lock)
2020-06-04T11:34:52.0440348Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-06-04T11:34:52.0441259Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-06-04T11:34:52.0441825Z 
2020-06-04T11:34:52.0442576Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f4e7026a000 nid=0x73fc in Object.wait() [0x00007f4d7d1b3000]
2020-06-04T11:34:52.0443567Z    java.lang.Thread.State: WAITING (on object monitor)
2020-06-04T11:34:52.0444269Z 	at java.lang.Object.wait(Native Method)
2020-06-04T11:34:52.0445010Z 	at java.lang.Object.wait(Object.java:502)
2020-06-04T11:34:52.0445862Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-06-04T11:34:52.0447007Z 	- locked <0x0000000080c56678> (a java.lang.ref.Reference$Lock)
2020-06-04T11:34:52.0447959Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-06-04T11:34:52.0448509Z 
2020-06-04T11:34:52.0449218Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f4e7000b800 nid=0x73c4 waiting on condition [0x00007f4e768fd000]
2020-06-04T11:34:52.0450228Z    java.lang.Thread.State: WAITING (parking)
2020-06-04T11:34:52.0450892Z 	at sun.misc.Unsafe.park(Native Method)
2020-06-04T11:34:52.0452052Z 	- parking to wait for  <0x000000008757aa28> (a java.util.concurrent.CompletableFuture$Signaller)
2020-06-04T11:34:52.0453050Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-06-04T11:34:52.0454097Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-06-04T11:34:52.0455234Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-06-04T11:34:52.0456299Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-06-04T11:34:52.0457383Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-06-04T11:34:52.0458470Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:671)
2020-06-04T11:34:52.0459639Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:81)
2020-06-04T11:34:52.0460933Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1685)
2020-06-04T11:34:52.0462322Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1667)
2020-06-04T11:34:52.0463724Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.executeAndVerify(CoordinatedSourceITCase.java:84)
2020-06-04T11:34:52.0465237Z 	at org.apache.flink.connector.base.source.reader.CoordinatedSourceITCase.testMultipleSources(CoordinatedSourceITCase.java:68)
2020-06-04T11:34:52.0466388Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-04T11:34:52.0467359Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-04T11:34:52.0468461Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-04T11:34:52.0469422Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-04T11:34:52.0470390Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-06-04T11:34:52.0471490Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-06-04T11:34:52.0472601Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-06-04T11:34:52.0473807Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-06-04T11:34:52.0474938Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-06-04T11:34:52.0475922Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-04T11:34:52.0476784Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-06-04T11:34:52.0477853Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-06-04T11:34:52.0478931Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-06-04T11:34:52.0479942Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-06-04T11:34:52.0480862Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-06-04T11:34:52.0481788Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-06-04T11:34:52.0482735Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-06-04T11:34:52.0483662Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-06-04T11:34:52.0484764Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T11:34:52.0485851Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-06-04T11:34:52.0486750Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-06-04T11:34:52.0487677Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-06-04T11:34:52.0488648Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-06-04T11:34:52.0489900Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-06-04T11:34:52.0491041Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-06-04T11:34:52.0492159Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-06-04T11:34:52.0493333Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-06-04T11:34:52.0494619Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-06-04T11:34:52.0495855Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-06-04T11:34:52.0496879Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-06-04T11:34:52.0497557Z 
2020-06-04T11:34:52.0498119Z ""VM Thread"" os_prio=0 tid=0x00007f4e70260800 nid=0x73fb runnable 
{code}"	FLINK	Closed	2	1	9657	pull-request-available, test-stability
13329568	SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent is instable	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6983&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab

{code}
2020-09-26T21:27:46.6223579Z [ERROR] testNotifiesWhenGoingIdleConcurrent(org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest)  Time elapsed: 0.602 s  <<< FAILURE!
2020-09-26T21:27:46.6224448Z java.lang.AssertionError
2020-09-26T21:27:46.6224804Z 	at org.junit.Assert.fail(Assert.java:86)
2020-09-26T21:27:46.6225136Z 	at org.junit.Assert.assertTrue(Assert.java:41)
2020-09-26T21:27:46.6225498Z 	at org.junit.Assert.assertTrue(Assert.java:52)
2020-09-26T21:27:46.6225984Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherTest.testNotifiesWhenGoingIdleConcurrent(SplitFetcherTest.java:129)
{code}"	FLINK	Closed	3	1	9657	pull-request-available, test-stability
13239381	Problems with ipv6 addresses	"I found some problems when using flink with ipv6. The easiest way to reproduce is add 
{noformat}
env.java.opts: -Djava.net.preferIPv6Addresses=true{noformat}
to the flink-conf.yaml and run start-cluster.sh script.

 Jobmanager log:
{noformat}
2019-06-13 19:16:01,088 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Trying to start actor system at localhost:6123

2019-06-13 19:16:01,147 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.Exception: Could not create actor system

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:267)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:153)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)

at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createRpcService(ClusterEntrypoint.java:296)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:264)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)

at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)

Caused by: com.typesafe.config.ConfigException$Parse: String: 56: Expecting close brace } or a comma, got ':' (if you intended ':' to be part of a key or string value, try enclosing the key or value in double quotes, or you may be able to rename the file .properties rather than .conf)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:475)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:595)

at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174)

at com.typesafe.config.impl.Parseable.parse(Parseable.java:299)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1046)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1056)

at org.apache.flink.runtime.akka.AkkaUtils$.getRemoteAkkaConfig(AkkaUtils.scala:601)

at org.apache.flink.runtime.akka.AkkaUtils$.getAkkaConfig(AkkaUtils.scala:218)

at org.apache.flink.runtime.akka.AkkaUtils.getAkkaConfig(AkkaUtils.scala)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:247)

... 12 more{noformat}
Taskmanager log:
{noformat}
2019-06-13 19:16:12,260 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Trying to start actor system at [2a02:6b8:c02:7e8:0:1459:44c2:764a]:0

2019-06-13 19:16:12,310 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner       - TaskManager initialization failed.

java.lang.Exception: Could not create actor system

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:267)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:153)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)

at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.createRpcService(TaskManagerRunner.java:412)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.<init>(TaskManagerRunner.java:134)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:332)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:302)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:299)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:299)

Caused by: com.typesafe.config.ConfigException$Parse: String: 55: List should have ended with ] or had a comma, instead had token: ':' (if you want ':' to be part of a string value, then double-quote it)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseArray(ConfigDocumentParser.java:533)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:249)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.consolidateValues(ConfigDocumentParser.java:152)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:420)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:595)

at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174)

at com.typesafe.config.impl.Parseable.parse(Parseable.java:299)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1046)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1056)

at org.apache.flink.runtime.akka.AkkaUtils$.getRemoteAkkaConfig(AkkaUtils.scala:601)

at org.apache.flink.runtime.akka.AkkaUtils$.getAkkaConfig(AkkaUtils.scala:218)

at org.apache.flink.runtime.akka.AkkaUtils.getAkkaConfig(AkkaUtils.scala)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:247)

... 11 more{noformat}
The problem is the following code in the AkkaUtils:

 
{code:java}
val hostnameConfigString =
  s""""""
     |akka {
     |  remote {
     |    netty {
     |      tcp {
     |        hostname = $effectiveHostname
     |        bind-hostname = $bindAddress
     |      }
     |    }
     |  }
     |}
   """""".stripMargin
{code}
I fixed it by adding double quotes. I ran flink with this fix and jobmanager worked fine but there was a new problem with taskmanager:
{noformat}
2019-06-13 21:12:37,451 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Actor system started at akka.tcp://flink@[2a02:6b8:c02:7e8:0:1459:44c2:764a]:14803

2019-06-13 21:12:37,470 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner       - TaskManager initialization failed.

org.apache.flink.configuration.IllegalConfigurationException: The configured hostname is not valid

at org.apache.flink.util.NetUtils.unresolvedHostToNormalizedString(NetUtils.java:150)

at org.apache.flink.util.NetUtils.unresolvedHostAndPortToNormalizedString(NetUtils.java:168)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:243)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:153)

at org.apache.flink.runtime.metrics.util.MetricUtils.startMetricsActorSystem(MetricUtils.java:132)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.<init>(TaskManagerRunner.java:135)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:332)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:302)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:299)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:299)

Caused by: java.lang.IllegalArgumentException

at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:123)

at org.apache.flink.util.NetUtils.unresolvedHostToNormalizedString(NetUtils.java:148)

... 10 more{noformat}
 

This problem caused by NetUtils.unresolvedHostToNormalizedString fails when gets an ipv6 address in brackets like [2a02:6b8:c02:7e8:0:1459:44c2:764a]. 

It works fine with fixed AkkaUtils and NetUtils."	FLINK	Closed	3	1	9657	pull-request-available
13283314	Fix Race Condition when releasing shared memory resource	"There is a race condition around releasing an {{OpaqueMemoryResource}} obtained via {{MemoryManager.getSharedMemoryResourceForManagedMemory}}.

{code}
final boolean allDisposed = sharedResources.release(type, leaseHolder);
if (allDisposed) {
	releaseMemory(type, MemoryType.OFF_HEAP, size);
}
{code}

If another allocation occurs while the releasing thread is between the ""sharedResources.release""and the ""if block"", then there is no resource to pick up any more and memory has not yet been returned to the memory bookkeeping."	FLINK	Closed	1	1	9657	pull-request-available
13367996	Transient RPC failure without TaskManager failure can lead to split assignment loss	"NOTE: This bug has not been actually observed. It is based on reviews of the current implementation.
I would expect it to be a pretty rare case, bu at scale, even the rare cases happen often enough.

h2. Problem

Intermediate RPC messages from JM to TM can get dropped, even when the TM is not marked as failed.
That can happen when the connection can be recovered before the heartbeat times out.

So RPCs generally retry, or handle failures: For example Deploy-Task-RPC retries, Trigger-Checkpoint RPC aborts the checkpoint on failure and triggers a new checkpoint.

The ""Send OperatorEvent"" RPC call (from Coordinator to Operator) gives you a Future with the acknowledgement. But if that one fails, we are in the situation where we do not know whether the event sending was successful or not (only the ack failed).

This is especially tricky for split assignments and checkpoints. Consider this sequence of actions:
  1. Coordinator assigns a split. Ack not yet received.
  2. Coordinator takes a checkpoint. Split was sent before the checkpoint, so is not included on the Coordinator.
  3. Split assignment RPC response is ""failed"".
  4. Checkpoint completes.

Now we don't know whether the split was in the checkpoint on the Operator (TaskManager) or not, and with that we don't know whether we should add it back to the coordinator. We need to do something to make sure the split is now either on the coordinator or on the Operator. Currently, the split is implicitly assumed to be on the Operator; if it isn't, then that split is lost.

Not, it is worth pointing out that this is a pretty rare situation, because it means that the RPC with the split assignment fails and the one for the checkpoint succeeds, even though they are in close proximity. The way the Akka-based RPC transport works (with retries, etc.), this can happen, but isn't very likely. That why we haven't so far seen this bug in practice or haven't gotten a report for it, yet.


h2. Proposed solution

The solution has two components:

  1. Fallback to consistent point: If the system doesn't know whether two parts are still consistent with each other (here coordinator and Operator), fall back to a consistent point. Here that is the case when the Ack-Future for the ""Send Operator Event"" RPC fails or times out. Then we call the scheduler to trigger a failover of the target operator to latest checkpoint and signaling the coordinator the same. That restores consistency. We can later optimize this (see below).

  2. We cannot trigger checkpoints while we are ""in limbo"" concerning our knowledge about splits. Concretely that means that the Coordinator can only acknowledge the checkpoint once the Acks for pending Operator Event RPCs (Assign-Splits) have arrived. The checkpoint future is conditional on all pending RPC futures. If the RPC futures fail (or time out) then the checkpoint cannot complete (and the target operator will anyways go through a failover). In the common case, RPC round trip time is milliseconds, which would be added to the checkpoint latency if the checkpoint happends to overlap with a split assignment (most won't).


h2. Possible Future Improvements

Step (1) above can be optimized by going with retries first and sequence numbers to deduplicate the calls. That can help reduce the number of cases were a failover is needed. However, the number of situations where the RPC would need a retry and has a chance of succeeding (the TM is not down) should be very few to begin with, so whether this optimization is worth it remains to be seen."	FLINK	Closed	1	1	9657	pull-request-available
13176549	Let Task release reference to Invokable on shutdown	"References to Task objects may under some conditions linger longer than for the lifetime of the task. For example, in case of local network channels, the receiving task may have a reference to the object of the task that produced the data.

To prevent against memory leaks, the Task needs to release all references to its AbstractInvokable when it shuts down or cancels.
"	FLINK	Closed	3	1	9657	pull-request-available
12719612	Renaming in pact-compiler	"I would like to do a cleanup and renaming in the pact-compiler. Most of the work is in line with the recent global renaming, but I also want to clear and organize the various representation structures for the optimized plan. 

I open this issue to keep track and discuss the suggested renaming.

We'll have to coordinate the merging of this issue because some renamings (e.g. PactCompiler -> Compiler) seem to affect a lot of other packages.

### Global Scope (Wide Dependencies)

The following names are part of the public API of stratosphere-compiler. Their renaming will probably affect a lot of other modules.

In ```eu.stratosphere.compiler```:
* ```PactCompiler``` ⇒ ```Compiler```

### Module Scope (Narrow Dependencies)

The following names are part of the internal API of stratosphere-compiler. Their renaming will probably affect only stratosphere-compiler and stratosphere-tests.

In ```eu.stratosphere.compiler```:

* ```DataStatistics``` ⇒ ```StatsStore``` This should be developed as an API for data stats over *expressions* instead of just over *data sources*.
* ```NonCachingDataStatistics``` ⇒ *delete*. This class does not seem to be used.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/441
Created by: [aalexandrov|https://github.com/aalexandrov]
Labels: 
Created at: Mon Jan 27 12:33:50 CET 2014
State: open
"	FLINK	Resolved	4	7	9657	github-import
13374684	CoordinatorEventsExactlyOnceITCase stalls on Adaptive Scheduler	"Logs of the test failure:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=11113

Steps to reproduce: Adjust the {{CoordinatorEventsExactlyOnceITCase}} and extend the MiniCluster configuration:
{code}
@BeforeClass
public static void startMiniCluster() throws Exception {
    final Configuration config = new Configuration();
    config.setString(RestOptions.BIND_PORT, ""0"");
    config.set(JobManagerOptions.SCHEDULER, JobManagerOptions.SchedulerType.Adaptive);
    config.set(ClusterOptions.ENABLE_DECLARATIVE_RESOURCE_MANAGEMENT, true);
{code}
"	FLINK	Closed	2	1	9657	pull-request-available, test-stability
13286264	Integrate Operator Coordinators with Checkpoints	"The operator coordinators are stateful and hence need to store state in checkpoints.

See FLIP-27 for details on operator coordinators: https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface

The initial implementation approach is to trigger coordinator checkpoints first, and when all coordinator checkpoints are done, then the source checkpoint barriers will be injected.

_Note:_ This functionality will eventually replace the checkpoint master hooks.

"	FLINK	Closed	3	7	9657	pull-request-available
12719335	Rework Nephele Task Hierarchy	"I'll keep it short and just quote @StephanEwen from ([#112|https://github.com/stratosphere/stratosphere/issues/112] | [FLINK-112|https://issues.apache.org/jira/browse/FLINK-112]):

> That Nephele Class hierarchy has has always bothered us with respect to duplicate logic in souce and sink tasks. The Regular Pact Task has many static methods to set up outputs for example, such that the logic can be called from an input task. [...]

> We should think about changing that class hierarchy, such that all tasks inherit from the same task class and implicitly become input and outputs. Potentially with a property to be set or a marker interface.

> That would also help us to get around fake tails in the iterations.

I think the timeline here should be post release, right?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/165
Created by: [uce|https://github.com/uce]
Labels: enhancement, runtime, 
Milestone: Release 0.6 (unplanned)
Created at: Wed Oct 16 12:20:22 CEST 2013
State: open
"	FLINK	Resolved	3	4	9657	github-import
13339408	SourceReaderTestBase.testAddSplitToExistingFetcher failed with NullPointerException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9322&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518

{code}
2020-11-08T21:49:29.6792941Z [ERROR] testAddSplitToExistingFetcher(org.apache.flink.connector.kafka.source.reader.KafkaSourceReaderTest)  Time elapsed: 0.632 s  <<< ERROR!
2020-11-08T21:49:29.6793408Z java.lang.NullPointerException
2020-11-08T21:49:29.6793998Z 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader$KafkaPartitionSplitRecords.nextSplit(KafkaPartitionSplitReader.java:363)
2020-11-08T21:49:29.6795970Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.moveToNextSplit(SourceReaderBase.java:187)
2020-11-08T21:49:29.6796596Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:159)
2020-11-08T21:49:29.6797317Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)
2020-11-08T21:49:29.6797942Z 	at org.apache.flink.connector.testutils.source.reader.SourceReaderTestBase.testAddSplitToExistingFetcher(SourceReaderTestBase.java:98)
{code}"	FLINK	Closed	2	1	9657	test-stability
12719911	Provide a way to parametrize aggregators and convergence criteria	"Currently, there is no way to pass parameters to an aggregator or a convergence criterion. @StephanEwen has suggested to add support for this by changing the code for aggregators and convergence criterion to take an object, rather than a class, i.e. instead of 
`iteration.registerAggregator(""my aggregator"", Customaggregator.class)`
do something like this:
`iteration.registerAggregator(""my aggregator"", new CustomAggregator(params));`

I can start working on this, but I have a question: What should the type of `params` be?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/731
Created by: [vasia|https://github.com/vasia]
Labels: 
Created at: Mon Apr 28 16:26:56 CEST 2014
State: open
"	FLINK	Resolved	3	1	9657	github-import
13161425	BlobClientSslTest does not work in all environments	"It seems that the {{BlobClientSslTest}} assumes SSL algorithms that are not present in every environment. Thus, they cause the Flink build to fail. It also affects {{NettyClientServerSslTest}}.

Environment:
{code}
Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-24T20:49:05+01:00)
Maven home: /usr/local/Cellar/maven/3.5.3/libexec
Java version: 1.8.0_102, vendor: Oracle Corporation
Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""mac os x"", version: ""10.13.3"", arch: ""x86_64"", family: ""mac""
{code}

Exception:
{code}
java.lang.IllegalArgumentException: Cannot support TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 with currently installed providers

	at sun.security.ssl.CipherSuiteList.<init>(CipherSuiteList.java:92)
	at sun.security.ssl.SSLServerSocketImpl.setEnabledCipherSuites(SSLServerSocketImpl.java:200)
	at org.apache.flink.runtime.net.SSLUtils.setSSLVerAndCipherSuites(SSLUtils.java:84)
	at org.apache.flink.runtime.blob.BlobServer.<init>(BlobServer.java:207)
	at org.apache.flink.runtime.blob.BlobClientSslTest.startSSLServer(BlobClientSslTest.java:65)
{code}"	FLINK	Closed	3	4	9657	pull-request-available
13342066	Duplicated output in FileSource continuous ITCase with TM failover	"If FileSourceTextLinesITCase::testContinuousTextFileSource includes TM restarts (after failing TM with TestingMiniCluster::terminateTaskExecutor, see testContinuousTextFileSourceWithTaskManagerFailover in [branch|https://github.com/azagrebin/flink/tree/FLINK-20118-it]) then sometimes I observe duplicated lines in the output after running the whole test suite FileSourceTextLinesITCase 5-10 times in IDE:
{code:java}
Test testContinuousTextFileSourceWithTaskManagerFailover(org.apache.flink.connector.file.src.FileSourceTextLinesITCase) failed with:
java.lang.AssertionError: 
Expected: [""And by opposing end them?--To die,--to sleep,--"", ""And enterprises of great pith and moment,"", ""And lose the name of action.--Soft you now!"", ""And makes us rather bear those ills we have"", ""And thus the native hue of resolution"", ""Be all my sins remember'd."", ""But that the dread of something after death,--"", ""Devoutly to be wish'd. To die,--to sleep;--"", ""For in that sleep of death what dreams may come,"", ""For who would bear the whips and scorns of time,"", ""Is sicklied o'er with the pale cast of thought;"", ""Must give us pause: there's the respect"", ""No more; and by a sleep to say we end"", ""No traveller returns,--puzzles the will,"", ""Or to take arms against a sea of troubles,"", ""Than fly to others that we know not of?"", ""That flesh is heir to,--'tis a consummation"", ""That makes calamity of so long life;"", ""That patient merit of the unworthy takes,"", ""The fair Ophelia!--Nymph, in thy orisons"", ""The heartache, and the thousand natural shocks"", ""The insolence of office, and the spurns"", ""The oppressor's wrong, the proud man's contumely,"", ""The pangs of despis'd love, the law's delay,"", ""The slings and arrows of outrageous fortune"", ""The undiscover'd country, from whose bourn"", ""Thus conscience does make cowards of us all;"", ""To be, or not to be,--that is the question:--"", ""To grunt and sweat under a weary life,"", ""To sleep! perchance to dream:--ay, there's the rub;"", ""When he himself might his quietus make"", ""When we have shuffled off this mortal coil,"", ""Whether 'tis nobler in the mind to suffer"", ""With a bare bodkin? who would these fardels bear,"", ""With this regard, their currents turn awry,""]
     but: was [""And by opposing end them?--To die,--to sleep,--"", ""And enterprises of great pith and moment,"", ""And lose the name of action.--Soft you now!"", ""And makes us rather bear those ills we have"", ""And thus the native hue of resolution"", ""Be all my sins remember'd."", ""But that the dread of something after death,--"", ""Devoutly to be wish'd. To die,--to sleep;--"", ""Devoutly to be wish'd. To die,--to sleep;--"", ""For in that sleep of death what dreams may come,"", ""For who would bear the whips and scorns of time,"", ""Is sicklied o'er with the pale cast of thought;"", ""Must give us pause: there's the respect"", ""No more; and by a sleep to say we end"", ""No more; and by a sleep to say we end"", ""No traveller returns,--puzzles the will,"", ""Or to take arms against a sea of troubles,"", ""Than fly to others that we know not of?"", ""That flesh is heir to,--'tis a consummation"", ""That flesh is heir to,--'tis a consummation"", ""That makes calamity of so long life;"", ""The fair Ophelia!--Nymph, in thy orisons"", ""The heartache, and the thousand natural shocks"", ""The heartache, and the thousand natural shocks"", ""The slings and arrows of outrageous fortune"", ""The undiscover'd country, from whose bourn"", ""Thus conscience does make cowards of us all;"", ""To be, or not to be,--that is the question:--"", ""To grunt and sweat under a weary life,"", ""To sleep! perchance to dream:--ay, there's the rub;"", ""To sleep! perchance to dream:--ay, there's the rub;"", ""When we have shuffled off this mortal coil,"", ""Whether 'tis nobler in the mind to suffer"", ""With a bare bodkin? who would these fardels bear,"", ""With this regard, their currents turn awry,""]
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.verifyResult(FileSourceTextLinesITCase.java:198)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSource(FileSourceTextLinesITCase.java:151)
	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSourceWithTaskManagerFailover(FileSourceTextLinesITCase.java:109)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}"	FLINK	Closed	1	1	9657	pull-request-available
13016765	Harden JobMaster logic	Some of Flip-6's {{JobMaster}} logic can be streamlined by using switch statements instead of if-else if cascades.	FLINK	Resolved	4	7	9657	flip-6
13339591	File Source requests an additional split on every restore.	"Currently, the {{FileSourceReader}} requests a new split when started. That includes cases when it was restored from a checkpoint.

So with every restore, the reader increases its split backlog size by one, causing problems for balanced split assignments."	FLINK	Closed	2	1	9657	pull-request-available
13247529	Rework CommonTestUtils.createClassNotInClassPath() to not use Unsafe.defineClass()	"The method {{Unsafe.defineClass()}} is removed in Java 11.
To support Java 11, we need to rework the method {{CommonTestUtils.createClassNotInClassPath()}} to use a different mechanism.

Java 11 introduces a new way to define a class from byte code via {{MethodHandles}}. However, because these do not exist in Java 8, we cannot use them if we want to keep supporting Java 8, which we most likely want to do for quite a while.

A method that works across both versions is to write the class byte code out to a temporary file and create a new URLClassLoader that loads the class from that file.

That solution is not a complete drop-in replacement, because it cannot add the class to an existing class loader, but can only create a new pair of (classloader & new-class-in-that-classloader). But it is seems straightforward to adjust the existing tests to work with that."	FLINK	Closed	2	7	9657	pull-request-available
13157690	Perform mutual authentication during SSL handshakes	"Currently, the Flink processes encrypted connections via SSL:

  - Data exchange TM - TM
  - RPC JM - TM
  - Blob Service JM - TM

However, the server side always accepts any client to build up the connection, meaning the connections are not strongly authenticated.

Activating SSL mutual authentication solves that - only processes that have the same certificate can connect."	FLINK	Resolved	3	2	9657	pull-request-available
13280517	Restructure Configuration Docs	"Currently, the configuration docs are very hard to parse. There is a minimal ""common"" section, and then all options are clustered by code component.

The common options misses quite a lot of common options.

The clustering by code component is not helpful to users, because it mixes options that are frequently necessary to adjust (for example task manager host) are mixed with options that should pretty much never be touched (other than during debugging), like taskmanager.registration.backoff.

The subtasks will outline suggestions for improvements. "	FLINK	Closed	1	4	9657	pull-request-available, usability
12904542	AggregatingAlignedProcessingTimeWindowOperatorTest failure	"{{testTumblingWindowDuplicateElements}} fails. 
Not much stack trace. Just an Assertion failure. Here's the entire build log though:
https://travis-ci.org/apache/flink/jobs/85108826
"	FLINK	Closed	3	1	9657	test-stability
13330733	Port LocatableInputSplitAssigner to new File Source API	"The new File Source API needs a locality aware input split assigner.

To preserve the experience, I suggest to port the existing {{LocatableInputSplitAssigner}} from the {{InputFormat}} API."	FLINK	Closed	3	4	9657	pull-request-available
12761820	Restructure directory layout	"When building Flink, the build results can currently be found under ""flink-root/flink-dist/target/flink-$FLINKVERSION-incubating-SNAPSHOT-bin/flink-$YARNVERSION-$FLINKVERSION-incubating-SNAPSHOT/"".

I think we could improve the directory layout with the following:

- provide the bin folder in the root by default
- let the start up and submissions scripts in bin assemble the class path
- in case the project hasn't been build yet, inform the user

The changes would make it easier to work with Flink from source."	FLINK	Resolved	4	4	9657	usability
13170468	Interrupt TaskThread only while in User/Operator code	"Upon cancellation, the task thread is periodically interrupted.
This helps to pull the thread out of blocking operations in the user code.

Once the thread leaves the user code, the repeated interrupts may interfere with the shutdown cleanup logic, causing confusing exceptions.

We should stop sending the periodic interrupts once the thread leaves the user code."	FLINK	Closed	3	4	9657	pull-request-available
13247879	Remove dependency on MapR artifact repository	"The MapR artifact repository causes some problems. It does not reliably offer a secure (https://) access.

We should change the MapR FS connector to work based on reflection and avoid a hard dependency on any of the MapR vendor-specific artifacts. That should allow us to get rid of the dependency without regressing on the support for the file system."	FLINK	Closed	1	1	9657	pull-request-available
13330545	Consolidate Source Events between Source API and Split Reader API	"The Source API (flink-core) and the SplitReader API (flink-connector-base) have currently separate copies of events to request splits and to signal that no splits are available.

We should consolidate those in flink-core."	FLINK	Closed	3	4	9657	pull-request-available
12738215	Inconsistent parameter naming in MapPartitionFunction	"The input parameter of the {{MapPartitionFunction}} is called {{records}} which is different from the naming in the previous functions ({{GroupReduceFunction}}, {{FlatCombineFunction}}) which name the Iterable {{values}}.

I propose to rename the parameter in {{MapPartitionFunction}} to {{values}} for consistency reasons."	FLINK	Resolved	4	4	9657	starter
13376321	OperatorCoordinatorHolderTest. verifyCheckpointEventOrderWhenCheckpointFutureCompletesLate fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17499&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=7502


{noformat}
2021-05-02T22:45:49.7343556Z May 02 22:45:49 java.lang.AssertionError
2021-05-02T22:45:49.7344688Z May 02 22:45:49 	at org.junit.Assert.fail(Assert.java:86)
2021-05-02T22:45:49.7345646Z May 02 22:45:49 	at org.junit.Assert.assertTrue(Assert.java:41)
2021-05-02T22:45:49.7346698Z May 02 22:45:49 	at org.junit.Assert.assertTrue(Assert.java:52)
2021-05-02T22:45:49.7353570Z May 02 22:45:49 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.checkpointEventValueAtomicity(OperatorCoordinatorHolderTest.java:363)
2021-05-02T22:45:49.7355384Z May 02 22:45:49 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolderTest.verifyCheckpointEventOrderWhenCheckpointFutureCompletesLate(OperatorCoordinatorHolderTest.java:331)
2021-05-02T22:45:49.7356826Z May 02 22:45:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-02T22:45:49.7904883Z May 02 22:45:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-02T22:45:49.7905443Z May 02 22:45:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-02T22:45:49.7905918Z May 02 22:45:49 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-02T22:45:49.7906402Z May 02 22:45:49 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-02T22:45:49.7907018Z May 02 22:45:49 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-02T22:45:49.7907555Z May 02 22:45:49 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-02T22:45:49.7909318Z May 02 22:45:49 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-02T22:45:49.7910078Z May 02 22:45:49 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-02T22:45:49.7910869Z May 02 22:45:49 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-02T22:45:49.7911597Z May 02 22:45:49 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-02T22:45:49.7912383Z May 02 22:45:49 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-02T22:45:49.7914058Z May 02 22:45:49 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-02T22:45:49.7915214Z May 02 22:45:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-02T22:45:49.7916058Z May 02 22:45:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-02T22:45:49.7916852Z May 02 22:45:49 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-02T22:45:49.7917550Z May 02 22:45:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-02T22:45:49.7919076Z May 02 22:45:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-02T22:45:49.7920292Z May 02 22:45:49 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-02T22:45:49.7921041Z May 02 22:45:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-02T22:45:49.7921788Z May 02 22:45:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-02T22:45:49.7922652Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-05-02T22:45:49.7923564Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-05-02T22:45:49.7924834Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-05-02T22:45:49.7925709Z May 02 22:45:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-05-02T22:45:49.7926617Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-02T22:45:49.7927661Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-02T22:45:49.7928497Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-02T22:45:49.7929783Z May 02 22:45:49 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}
"	FLINK	Closed	4	1	9657	pull-request-available, test-stability
12719801	Add a fail(Exception) method to the job graph, to report problems detected in RPC calls	"Inside RPC calls, throwing an exception kills only the RPC service. Instead, the job graph needs to be failed by the exception.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/625
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Mon Mar 24 18:58:23 CET 2014
State: open
"	FLINK	Resolved	3	1	9657	github-import
12988401	"Remove ""recoveryTimestamp"""	"I think we should remove the {{recoveryTimestamp}} that is attached on state restore calls.

Given that this is a wall clock timestamp from a master node, which may change when clocks are adjusted, and between different master nodes during leader change, this is an unsafe concept."	FLINK	Closed	3	1	9657	pull-request-available
13177061	Add docs for updates SSL model 	"Add docs about the ""internal"" versus ""external"" connectivity and new configuration options."	FLINK	Resolved	3	2	9657	pull-request-available
13401464	SourceCoordinator exception may fail Session Cluster	"The SourceCoordinator currently forwards all exceptions from `Source#createEnumerator` up the stack triggering a JobMaster failover. However, JobMaster failover only works if HA is enabled[1]. If HA is not enabled the fatal error handler will simply exit the JM process killing the entire cluster. This is problematic in the case of a session cluster where there may be multiple jobs running. It also does not play well with external tooling that does not expect job failure to cause a full cluster failure. 

 

It would be preferable if failure to create an enumerator did not take down the entire cluster, but instead failed that particular job. 

 

[1] [https://github.com/apache/flink/blob/7f69331294ab2ab73f77b40a4320cdda53246afe/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L898-L903]"	FLINK	Closed	1	1	9657	pull-request-available
13111440	HadoopS3FileSystemITCase failed on travis	"The {{HadoopS3FileSystemITCase}} is flaky on Travis because its access got denied by S3.

{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 3.354 sec <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase
testDirectoryListing(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase)  Time elapsed: 0.208 sec  <<< ERROR!
java.nio.file.AccessDeniedException: s3://[secure]/tests-9273972a-70c2-4f06-862e-d02936313fea/testdir: getFileStatus on s3://[secure]/tests-9273972a-70c2-4f06-862e-d02936313fea/testdir: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 9094999D7456C589), S3 Extended Request ID: fVIcROQh4E1/GjWYYV6dFp851rjiKtFgNSCO8KkoTmxWbuxz67aDGqRiA/a09q7KS6Mz1Tnyab4=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1579)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1249)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4194)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4141)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1256)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1232)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.getFileStatus(HadoopFileSystem.java:77)
	at org.apache.flink.core.fs.FileSystem.exists(FileSystem.java:509)
	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.testDirectoryListing(HadoopS3FileSystemITCase.java:163)

testSimpleFileWriteAndRead(org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase)  Time elapsed: 0.275 sec  <<< ERROR!
java.nio.file.AccessDeniedException: s3://[secure]/tests-9273972a-70c2-4f06-862e-d02936313fea/test.txt: getFileStatus on s3://[secure]/tests-9273972a-70c2-4f06-862e-d02936313fea/test.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: B3D8126BE6CF169F), S3 Extended Request ID: T34sn+a/CcCFv+kFR/UbfozAkXXtiLDu2N31Ok5EydgKeJF5I2qXRCC/MkxSi4ymiiVWeSyb8FY=
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1579)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1249)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4194)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4141)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1256)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1232)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.delete(S3AFileSystem.java:1234)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.delete(HadoopFileSystem.java:134)
	at org.apache.flink.fs.s3hadoop.HadoopS3FileSystemITCase.testSimpleFileWriteAndRead(HadoopS3FileSystemITCase.java:147)
{code}"	FLINK	Closed	3	1	9657	test-stability
12862800	CombineTaskTest.testCancelCombineTaskSorting	"I came across a build failure on this test with the exception:
{code}
testCancelCombineTaskSorting[0](org.apache.flink.runtime.operators.CombineTaskTest)  Time elapsed: 12.602 sec  <<< FAILURE!
java.lang.AssertionError: Task did not cancel properly within in 5 seconds.
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.apache.flink.runtime.operators.CombineTaskTest.testCancelCombineTaskSorting(CombineTaskTest.java:184)

testCancelCombineTaskSorting[0](org.apache.flink.runtime.operators.CombineTaskTest)  Time elapsed: 12.609 sec  <<< FAILURE!
java.lang.AssertionError: Memory Manager managed memory was not completely freed.
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.flink.runtime.operators.testutils.UnaryOperatorTestBase.shutdownAll(UnaryOperatorTestBase.java:377)
{code}

Here's the build log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/79484212/log.txt"	FLINK	Closed	3	1	9657	test-stability
13181675	Make nightly builds easier discoverable	"How to access the nightly builds is described in a bit of a hidden place, but is a question frequently popping up.

I suggest to add the nightly builds to the download page, with a prominent disclaimer that they are not endorsed releases."	FLINK	Closed	3	4	9657	pull-request-available
13376319	JVM crashes when running OperatorEventSendingCheckpointITCase.testOperatorEventAckLost	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17501&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4287
"	FLINK	Closed	2	1	9657	pull-request-available, test-stability
12958289	TimestampITCase testWatermarkPropagationNoFinalWatermarkOnStop failing intermittently	"Test failed randomly in Travis,
https://s3.amazonaws.com/archive.travis-ci.org/jobs/122624297/log.txt

{noformat}
java.lang.Exception: Stopping the job with ID ef892dfdf31b74a9a3da991d2240716e failed.
	at org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster.stopJob(LocalFlinkMiniCluster.scala:283)
	at org.apache.flink.streaming.timestamp.TimestampITCase$1.run(TimestampITCase.java:213)
Caused by: java.lang.IllegalStateException: Job with ID ef892dfdf31b74a9a3da991d2240716e is in state FAILING but stopping is only allowed in state RUNNING.
	at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:577)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.flink.runtime.testingUtils.TestingJobManagerLike$$anonfun$handleTestingMessage$1.applyOrElse(TestingJobManagerLike.scala:90)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33)
	at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:113)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)
	at akka.dispatch.Mailbox.run(Mailbox.scala:221)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:231)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Tests run: 12, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 15.087 sec <<< FAILURE! - in org.apache.flink.streaming.timestamp.TimestampITCase
testWatermarkPropagationNoFinalWatermarkOnStop(org.apache.flink.streaming.timestamp.TimestampITCase)  Time elapsed: 0.792 sec  <<< ERROR!
org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed.
	at org.apache.flink.client.program.Client.runBlocking(Client.java:381)
	at org.apache.flink.client.program.Client.runBlocking(Client.java:355)
	at org.apache.flink.client.program.Client.runBlocking(Client.java:348)
	at org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.executeRemotely(RemoteStreamEnvironment.java:206)
	at org.apache.flink.streaming.api.environment.RemoteStreamEnvironment.execute(RemoteStreamEnvironment.java:172)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1170)
	at org.apache.flink.streaming.timestamp.TimestampITCase.testWatermarkPropagationNoFinalWatermarkOnStop(TimestampITCase.java:223)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply$mcV$sp(JobManager.scala:805)
	at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:751)
	at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1$$anonfun$applyOrElse$7.apply(JobManager.scala:751)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:401)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NullPointerException: null
	at org.apache.flink.streaming.runtime.tasks.StoppableSourceStreamTask.stop(StoppableSourceStreamTask.java:36)
	at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:782)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}"	FLINK	Closed	4	1	9657	test-stability
13274839	Managed Memory Option for RocksDB not picked up from config	This is a missing lookup in the config in the State Backend's configure() method. Or, more precisely, in the State Backend's MemoryConfiguration's configure() method.	FLINK	Closed	1	1	9657	pull-request-available
12720001	Support for unconnected data flows	"Currently the compiler fails when given a plan with separate data flows.
```java
throw new CompilerException(""The given Pact program contains multiple disconnected data flows."");
```

However, I do not see a good conceptual reason to not support such data flows. Are there any technical issues?

To elaborate a bit on the issue: I have a higher level operator that may or may not produce separated data flows. In a current workaround I have to pipe the union of all data flows to a dummy sink to avoid such an exception.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/820
Created by: [AHeise|https://github.com/AHeise]
Labels: 
Created at: Thu May 15 16:51:46 CEST 2014
State: open
"	FLINK	Resolved	3	4	9657	github-import
12858980	Failing-Test: TaskAsyncCallTest	"{noformat}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec <<< FAILURE! - in org.apache.flink.runtime.taskmanager.TaskAsyncCallTest
testCheckpointCallsInOrder(org.apache.flink.runtime.taskmanager.TaskAsyncCallTest) Time elapsed: 0.009 sec <<< FAILURE!
java.lang.AssertionError: expected:<RUNNING> but was:<FINISHED>
at org.junit.Assert.fail(Assert.java:88)
at org.junit.Assert.failNotEquals(Assert.java:743)
at org.junit.Assert.assertEquals(Assert.java:118)
at org.junit.Assert.assertEquals(Assert.java:144)
at org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.testCheckpointCallsInOrder(TaskAsyncCallTest.java:92)
{noformat}

See here for Travis log: https://travis-ci.org/apache/flink/jobs/77313445"	FLINK	Closed	2	1	9657	test-stability
13272193	Typo in RocksDBStateBackend getNumberOfTransferingThread	"""getNumberOfTransferingThreads"" has a typo, it should be either
  - ""getNumberOfTransferringThreads"" (with two r)
  - getNumberOfTransferThread

I personally find the second option nicer."	FLINK	Closed	4	1	9657	pull-request-available
13312248	"HadoopRecoverableWriterOldHadoopWithNoTruncateSupportTest.createHDFS fails with ""Running in secure mode, but config doesn't have a keytab"" "	https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8184&view=logs&j=66592496-52df-56bb-d03e-37509e1d9d0f&t=ae0269db-6796-5583-2e5f-d84757d711aa	FLINK	Closed	3	1	9657	test-stability
13327894	Simplify handling of 'NoMoreSplitsEvent'	"Split assignment events are treated specially by the source API. Users do not create them directly but call methods on the contexts to assign splits.

The {{NoMoreSplitsEvent}} is in contrast to that a custom user event and needs to be handled like a custom event, when sent by enumerators and received by the readers.

That seems a bit confusing and inconsistent, given that NoMoreSplits is essential for all bounded stream use cases and is on the same level as the {{AddSplitEvent}}.

I suggest that we treat ""no more splits"" similarly, by having either a custom method or a custom ""SplitAssignment"" on the context and reader.

[~jqin] Curious what would be your take on this."	FLINK	Closed	3	4	9657	pull-request-available
12719247	Startup time of jobs	"Fix slow RPC service
Scalable algorithms in execution graph state machine

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/79
Created by: [dimalabs|https://github.com/dimalabs]
Labels: 
Milestone: Release 0.6 (unplanned)
Created at: Fri Sep 06 16:12:37 CEST 2013
State: open
"	FLINK	Resolved	4	4	9657	github-import
12719884	Check if TPC-H resources can be added	"Some of our tests and examples are derived from the TPC-H. 
Although we do not import source code or libraries, we should check if it is OK with the TPC-H / Apache license to add SQL DDL or data generated by the TPC-H generator to our distribution.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/705
Created by: [fhueske|https://github.com/fhueske]
Labels: 
Created at: Fri Apr 18 11:20:39 CEST 2014
State: open
"	FLINK	Closed	4	4	9657	github-import
12719836	Add Group Sorting to CoGroup	"Add group sorting to CoGroup to allow the user to work on sorted input groups in a CoGroup function.
This is analogous to group sorting for Reduce.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/658
Created by: [fhueske|https://github.com/fhueske]
Labels: enhancement, java api, user satisfaction, 
Milestone: Release 0.6 (unplanned)
Created at: Thu Apr 03 10:09:35 CEST 2014
State: open
"	FLINK	Resolved	3	4	9657	github-import
13194130	Let TypeSerializerSchemaCompatibility.resolveSchemaCompatibility() be the entry point for compatibility checks in framework code	"Currently, the state backend framework code still is exposed the now deprecated {{CompatibilityResult}} and relevant classes.

Instead, all compatibility checks should go through the new {{TypeSerializerSchemaCompatibility#resolveSchemaCompatibility}} method, and allow framework code to check against the more powerful {{TypeSerializerSchemaCompatibility}} for incompatibility / migration requirements."	FLINK	Closed	3	7	9657	pull-request-available
13296070	Add Architecture docs for Stateful Functions	We should add a section to the documentation describing the distributed setup and architecture of a Stateful Functions applications.	FLINK	Closed	3	4	9657	pull-request-available
13025129	Flip-6 mini cluster should start local TaskManagers if there is only one TaskManager	"MiniCluster execution becomes much faster for short jobs, if the TaskManagers do not start their Netty Network Environment unless needed.

If the MiniCluster has only one TaskManager, the NetworkEnvironment should not be started."	FLINK	Resolved	3	1	9657	flip-6
13329665	"FileSourceTextLinesITCase.testContinuousTextFileSource failed with ""SimpleStreamFormat is not splittable, but found split end (0) different from file length (198)"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7008&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code}
2020-09-27T21:58:38.9199090Z [ERROR] testContinuousTextFileSource(org.apache.flink.connector.file.src.FileSourceTextLinesITCase)  Time elapsed: 0.517 s  <<< ERROR!
2020-09-27T21:58:38.9199619Z java.lang.RuntimeException: Failed to fetch next result
2020-09-27T21:58:38.9200118Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2020-09-27T21:58:38.9200722Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:77)
2020-09-27T21:58:38.9201290Z 	at org.apache.flink.streaming.api.datastream.DataStreamUtils.collectRecordsFromUnboundedStream(DataStreamUtils.java:150)
2020-09-27T21:58:38.9201920Z 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testContinuousTextFileSource(FileSourceTextLinesITCase.java:136)
2020-09-27T21:58:38.9202570Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-27T21:58:38.9203054Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-27T21:58:38.9203539Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-27T21:58:38.9203968Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-27T21:58:38.9204369Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-27T21:58:38.9204844Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-27T21:58:38.9205359Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-27T21:58:38.9205814Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-27T21:58:38.9206240Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-27T21:58:38.9206611Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-27T21:58:38.9206971Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-27T21:58:38.9207404Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-27T21:58:38.9207971Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-27T21:58:38.9208404Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-27T21:58:38.9208877Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-27T21:58:38.9209279Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-27T21:58:38.9209680Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-27T21:58:38.9210064Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-27T21:58:38.9210476Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-27T21:58:38.9210881Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-27T21:58:38.9211272Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-27T21:58:38.9211638Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-27T21:58:38.9212305Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-27T21:58:38.9213157Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-27T21:58:38.9213663Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-27T21:58:38.9214123Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-27T21:58:38.9214620Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-27T21:58:38.9215148Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-27T21:58:38.9215650Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-27T21:58:38.9216095Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-27T21:58:38.9216516Z Caused by: java.io.IOException: Failed to fetch job execution result
2020-09-27T21:58:38.9217004Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
2020-09-27T21:58:38.9217595Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:126)
2020-09-27T21:58:38.9218182Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:103)
2020-09-27T21:58:38.9218585Z 	... 33 more
2020-09-27T21:58:38.9219037Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-09-27T21:58:38.9219563Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-09-27T21:58:38.9219987Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-09-27T21:58:38.9220511Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:172)
2020-09-27T21:58:38.9220915Z 	... 35 more
2020-09-27T21:58:38.9221225Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-09-27T21:58:38.9221680Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-09-27T21:58:38.9222277Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.lambda$getJobExecutionResult$2(PerJobMiniClusterFactory.java:196)
2020-09-27T21:58:38.9223032Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-09-27T21:58:38.9223479Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2020-09-27T21:58:38.9223938Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2020-09-27T21:58:38.9224503Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.getJobExecutionResult(PerJobMiniClusterFactory.java:194)
2020-09-27T21:58:38.9224926Z 	... 36 more
2020-09-27T21:58:38.9225315Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-09-27T21:58:38.9225889Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-09-27T21:58:38.9226529Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-09-27T21:58:38.9227128Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217)
2020-09-27T21:58:38.9227658Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210)
2020-09-27T21:58:38.9228200Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204)
2020-09-27T21:58:38.9228806Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:527)
2020-09-27T21:58:38.9229416Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:421)
2020-09-27T21:58:38.9229885Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-27T21:58:38.9230283Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-27T21:58:38.9230755Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-27T21:58:38.9231160Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-27T21:58:38.9231589Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
2020-09-27T21:58:38.9232086Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
2020-09-27T21:58:38.9232792Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-09-27T21:58:38.9233305Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-09-27T21:58:38.9233750Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-09-27T21:58:38.9234127Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-09-27T21:58:38.9234533Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-09-27T21:58:38.9234949Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-09-27T21:58:38.9235391Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-09-27T21:58:38.9235808Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-27T21:58:38.9236218Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-27T21:58:38.9236590Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-09-27T21:58:38.9236981Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-09-27T21:58:38.9237371Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-09-27T21:58:38.9237728Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-09-27T21:58:38.9238087Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-09-27T21:58:38.9238421Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-09-27T21:58:38.9238819Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-09-27T21:58:38.9239191Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-27T21:58:38.9239605Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-27T21:58:38.9240040Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-27T21:58:38.9240480Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-27T21:58:38.9240913Z Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2020-09-27T21:58:38.9241431Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:178)
2020-09-27T21:58:38.9242009Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:155)
2020-09-27T21:58:38.9242688Z 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)
2020-09-27T21:58:38.9243221Z 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:199)
2020-09-27T21:58:38.9243738Z 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:64)
2020-09-27T21:58:38.9244271Z 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
2020-09-27T21:58:38.9244798Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:368)
2020-09-27T21:58:38.9245369Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:185)
2020-09-27T21:58:38.9245853Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:594)
2020-09-27T21:58:38.9246417Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:558)
2020-09-27T21:58:38.9246841Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
2020-09-27T21:58:38.9247288Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-09-27T21:58:38.9247629Z 	at java.lang.Thread.run(Thread.java:748)
2020-09-27T21:58:38.9248047Z Caused by: java.lang.RuntimeException: SplitFetcher thread 1 received unexpected exception while polling the records
2020-09-27T21:58:38.9248558Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:123)
2020-09-27T21:58:38.9249151Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:91)
2020-09-27T21:58:38.9249647Z 	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
2020-09-27T21:58:38.9250090Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-09-27T21:58:38.9250506Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-09-27T21:58:38.9250920Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-27T21:58:38.9251369Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-27T21:58:38.9251694Z 	... 1 more
2020-09-27T21:58:38.9252087Z Caused by: java.lang.IllegalArgumentException: SimpleStreamFormat is not splittable, but found split end (0) different from file length (198)
2020-09-27T21:58:38.9252875Z 	at org.apache.flink.connector.file.src.reader.SimpleStreamFormat.checkNotSplit(SimpleStreamFormat.java:110)
2020-09-27T21:58:38.9253430Z 	at org.apache.flink.connector.file.src.reader.SimpleStreamFormat.createReader(SimpleStreamFormat.java:85)
2020-09-27T21:58:38.9253999Z 	at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.lambda$createReader$0(StreamFormatAdapter.java:68)
2020-09-27T21:58:38.9254516Z 	at org.apache.flink.connector.file.src.util.Utils.doWithCleanupOnException(Utils.java:45)
2020-09-27T21:58:38.9255034Z 	at org.apache.flink.connector.file.src.impl.StreamFormatAdapter.createReader(StreamFormatAdapter.java:67)
2020-09-27T21:58:38.9255655Z 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:103)
2020-09-27T21:58:38.9256225Z 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:68)
2020-09-27T21:58:38.9256723Z 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
2020-09-27T21:58:38.9257235Z 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:117)
2020-09-27T21:58:38.9257586Z 	... 7 more
{code}"	FLINK	Closed	3	1	9657	test-stability
12888751	SocketTextStreamFunctionTest failure	"testSocketSourceRetryTenTimes fails because of a deadlock.

Here's the build log: https://travis-ci.org/apache/flink/jobs/81141426"	FLINK	Closed	3	1	9657	test-stability
12719532	"Remote Jobmanager option for ""cancel"" and ""list"" option of CLI client"	"The ""run"" action allows to specify an external JobManager for job submission.
The ""cancel"" and ""list"" actions also need to access the JobManager and show also feature this option.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/361
Created by: [fhueske|https://github.com/fhueske]
Labels: enhancement, simple-issue, user satisfaction, 
Created at: Fri Dec 20 17:36:48 CET 2013
State: open
"	FLINK	Resolved	3	4	9657	github-import
12977350	Unstable test ConnectionUtilsTest	"The error is the following:

{code}
ConnectionUtilsTest.testReturnLocalHostAddressUsingHeuristics:59 expected:<testing-worker-linux-docker-e744e561-3361-linux-13/172.17.5.9> but was:</127.0.0.1>
{code}

The probable cause for the failure is that the test tries to select an unused closed port (from the ephemeral range), and then assumes that all connections to that port fail.

If a concurrent test actually uses that port, connections to the port will succeed."	FLINK	Closed	2	1	9657	pull-request-available, test-stability
13373659	Add missing stability annotation to Split Reader API classes	The Split Reader API currently has no stability annotations, it is unclear which classes are public API, which are internal, which are stable, and which are evolving.	FLINK	Closed	3	4	9657	pull-request-available
13199159	incremental Keyed state with RocksDB throws cannot create directory error in windows	"Facing error while enabling keyed state with RocksDBBackend with checkpointing to a local windows directory

 
{code:java}
Caused by: org.rocksdb.RocksDBException: Failed to create dir: /c:/tmp/data/job_dbe01128760d4d5cb90809cd94c2a936_op_StreamMap_b5c8d46f3e7b141acf271f12622e752b__3_8__uuid_45c1f62b-a198-44f5-add5-7683079b03f8/chk-1.tmp: Invalid argument

                at org.rocksdb.Checkpoint.createCheckpoint(Native Method)

                at org.rocksdb.Checkpoint.createCheckpoint(Checkpoint.java:51)

                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend$RocksDBIncrementalSnapshotOperation.takeSnapshot(RocksDBKeyedStateBackend.java:2549)

                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend$IncrementalSnapshotStrategy.performSnapshot(RocksDBKeyedStateBackend.java:2008)

                at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.snapshot(RocksDBKeyedStateBackend.java:498)

                at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:406)

                ... 13 more
{code}
 

 "	FLINK	Closed	3	1	9657	pull-request-available, usability
12973645	A metric with the name * was already registered	"The YARN tests detected the following failure while running WordCount.

{code}
2016-05-27 21:50:48,230 INFO  org.apache.flink.yarn.YarnTaskManager                         - Received task CHAIN DataSource (at main(WordCount.java:70) (org.apache.flink.api.java.io.TextInputFormat)) -> FlatMap (FlatMap at main(WordCount.java:80)) -> Combine(SUM(1), at main(WordCount.java:83) (1/2)
2016-05-27 21:50:48,231 ERROR org.apache.flink.metrics.reporter.JMXReporter                 - A metric with the name org.apache.flink.metrics:key0=testing-worker-linux-docker-6e03e1e8-3385-linux-1,key1=taskmanager,key2=ee7c10183f32c9a96f8e7cfd873863d1,key3=WordCount_Example,key4=CHAIN_DataSource_(at_main(WordCount.java-70)_(org.apache.flink.api.java.io.TextInputFormat))_->_FlatMap_(FlatMap_at_main(WordCount.java-80))_->_Combine(SUM(1)-_at_main(WordCount.java-83),name=numBytesIn was already registered.
javax.management.InstanceAlreadyExistsException: org.apache.flink.metrics:key0=testing-worker-linux-docker-6e03e1e8-3385-linux-1,key1=taskmanager,key2=ee7c10183f32c9a96f8e7cfd873863d1,key3=WordCount_Example,key4=CHAIN_DataSource_(at_main(WordCount.java-70)_(org.apache.flink.api.java.io.TextInputFormat))_->_FlatMap_(FlatMap_at_main(WordCount.java-80))_->_Combine(SUM(1)-_at_main(WordCount.java-83),name=numBytesIn
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.flink.metrics.reporter.JMXReporter.notifyOfAddedMetric(JMXReporter.java:76)
	at org.apache.flink.metrics.MetricRegistry.register(MetricRegistry.java:177)
	at org.apache.flink.metrics.groups.AbstractMetricGroup.addMetric(AbstractMetricGroup.java:191)
	at org.apache.flink.metrics.groups.AbstractMetricGroup.counter(AbstractMetricGroup.java:144)
	at org.apache.flink.metrics.groups.IOMetricGroup.<init>(IOMetricGroup.java:40)
	at org.apache.flink.metrics.groups.TaskMetricGroup.<init>(TaskMetricGroup.java:74)
	at org.apache.flink.metrics.groups.JobMetricGroup.addTask(JobMetricGroup.java:74)
	at org.apache.flink.metrics.groups.TaskManagerMetricGroup.addTaskForJob(TaskManagerMetricGroup.java:86)
	at org.apache.flink.runtime.taskmanager.TaskManager.submitTask(TaskManager.scala:1093)
	at org.apache.flink.runtime.taskmanager.TaskManager.org$apache$flink$runtime$taskmanager$TaskManager$$handleTaskMessage(TaskManager.scala:442)
	at org.apache.flink.runtime.taskmanager.TaskManager$$anonfun$handleMessage$1.applyOrElse(TaskManager.scala:284)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
	at org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
	at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33)
	at org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
	at org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.flink.runtime.taskmanager.TaskManager.aroundReceive(TaskManager.scala:124)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)
	at akka.dispatch.Mailbox.run(Mailbox.scala:221)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:231)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

Travis log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/133427703/log.txt
YARN logs: https://s3.amazonaws.com/flink-logs-us/travis-artifacts/apache/flink/9326/9326.2.tar.gz
File with exception: /yarn-tests/container_1464385813755_0002_01_000002/taskmanager.log"	FLINK	Resolved	3	1	9657	test-stability
13329025	Source API exception signatures are inconsistent	"The methods in {{org.apache.flink.api.connector.source.Source}} have inconsistent exception signatures:
  - the methods to create reader and enumerator do not throw a checked exception
  - the method to restore an enumerator throws an {{IOException}}.

Either all methods should allow throwing checked IOExceptions or all methods should not allo any checked exceptions."	FLINK	Closed	1	1	9657	pull-request-available
13285926	Unify AutoContextClassLoader and TemporaryClassLoaderContext	AutoContextClassLoader and TemporaryClassLoaderContext do the same thing. We should consolidate all uses to one of them.	FLINK	Closed	3	4	9657	pull-request-available
13286503	Improve error message on Windows when RocksDB Paths are too long	"Paths on Windows have a length limit by default (247 chars).
If RocksDB tries to open a longer path, it throws a misleading exception (directory not found).

We can check for this situation and improve the error message."	FLINK	Closed	3	4	9657	pull-request-available
12720071	"[Examples Website] Replace ""getInputData()"" methods"	"I think the website style of the examples needs a slight change.

Instead of having methods like `getTextDataSet(env)` would like to write `env.readTextFile(filePath);`.

That illustrates that it is really simple to load text files. The current way leaves you wondering whether there is some magic needed to get the input data sets.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/890
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: 
Created at: Fri May 30 21:01:50 CEST 2014
State: open
"	FLINK	Resolved	4	4	9657	github-import, starter
13287093	Rename 'checkpoints.savepoint.' package and classes to 'checkpoint.metadata.'	"These classes are responsible for the persistence of checkpoint metadata.

Originally, savepoints where the only snapshots with persistent metadata, while checkpoints had the metadata only in memory or in ZooKeeper (Java Serialized).

Nowadays, checkpoints and savepoints both persist metadata in the same way, and hence the package is not actually related to savepoints any more.

Because of that, I suggest to rename the classes from ""Savepoint*"" to ""Metadata*"" to better describe what they are really doing."	FLINK	Closed	3	4	9657	pull-request-available
13186415	Hadoop configurations on the classpath seep into the S3 file system configs	"The S3 connectors are based on a self-contained shaded Hadoop. By design, they should only use config value from the Flink configuration.

However, because Hadoop loads implicitly configs from the classpath, existing ""core-site.xml"" files can interfere with the configuration in ways intransparent for the user. We should ensure such configs are not loaded.


"	FLINK	Closed	3	1	9657	pull-request-available
13067430	RocksDBPerformanceTest.testRocksDbRangeGetPerformance fails on Travis	"The test case {{RocksDBPerformanceTest.testRocksDbRangeGetPerformance}} failed on Travis. Not sure whether the timeout are simply set too tight.

https://s3.amazonaws.com/archive.travis-ci.org/jobs/226347608/log.txt"	FLINK	Closed	2	1	9657	test-stability
13109452	Shade Akka's Netty Dependency	"In order to avoid clashes between different Netty versions we should shade Akka's Netty away.

These dependency version clashed manifest themselves in very subtle ways, like occasional deadlocks."	FLINK	Closed	3	4	9657	pull-request-available
12719162	[GitHub] Rework Nephele Execution Graph State Machine	"The Nephele ExecutionGraph state machine is currently not scalable and is susceptible to race conditions and illegal state transition.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/15
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: bug, runtime, 
Created at: Wed May 08 15:37:25 CEST 2013
State: open
"	FLINK	Resolved	3	1	9657	github-import
13367082	"Remove ""state.backend.async"" option."	"Checkpoints are always asynchronous, there is no case ever for a synchronous checkpoint.
The RocksDB state backend doesn't even support synchronous snapshots, and the HashMap Heap backend also has no good use case for synchronous snapshots (other than a very minor reduction in heap objects).

Most importantly, we should not expose this option in the constructors of the new state backend API classes, like {{HashMapStateBackend}}. 

I marked this a blocker because it is part of the new user-facing State Backend API and I would like to avoid that this option enters this API and causes confusion when we eventually remove it.

/cc [~sjwiesman] and [~liyu]"	FLINK	Closed	1	4	9657	pull-request-available
13220148	ClusterEntrypoint provides wrong executor to HaServices	"The {{ClusterEntrypoint}} provides the executor of the common {{RpcService}} to the {{HighAvailabilityServices}} which uses the executor to run io operations. In I/O heavy cases, this can block all {{RpcService}} threads and make the {{RpcEndpoints}} running in the respective {{RpcService}} unresponsive.

I suggest to introduce a dedicated I/O executor which is used for io heavy operations."	FLINK	Closed	2	1	10066	pull-request-available
12781662	Create machine learning library	Create the infrastructure for Flink's machine learning library. This includes the creation of the module structure and the implementation of basic types such as vectors and matrices.	FLINK	Closed	3	4	10066	ML
13379178	ZooKeeperHaServicesTest.testCleanupJobData failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18108&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=8172


{code:java}
May 19 01:30:02 Expected: a collection containing ""1a2850d5759a2e1f4fef9cc7e6abc675""
May 19 01:30:02      but: was ""resource_manager_lock""

{code}
"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13127942	Remove unneeded CommandLineOptions	With the refactorings of the {{CliFrontend}} we no longer have to keep the JobManager address and commandLine in the {{CommandLineOptions}}. Therefore, these fields should be removed.	FLINK	Closed	3	7	10066	flip-6
13103545	Port JobStoppingHandler to new REST endpoint	Port existing {{JobStoppingHandler}} to new REST endpoint	FLINK	Resolved	3	7	10066	flip-6
13131926	Add SerializableExecutionGraphStore to Dispatcher	The {{Dispatcher}} should have a {{SerializableExecutionGraphStore}} which it can use to store completed jobs. This store can then be used to serve historic job requests from the web UI, for example. The default implementation should persist the jobs to disk and evict the in memory instances once they grow to big in order to avoid memory leaks. Additionally, the store should expire elements from disk after a user defined time.	FLINK	Closed	3	4	10066	flip-6
13394535	"SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted failed with ""Stop with savepoint operation could not be completed"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21851&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4858

{code}
Aug 10 22:14:12 [ERROR] Tests run: 15, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 65.911 s <<< FAILURE! - in org.apache.flink.test.checkpointing.SavepointITCase
Aug 10 22:14:12 [ERROR] testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted  Time elapsed: 2.389 s  <<< FAILURE!
Aug 10 22:14:12 java.lang.AssertionError: 
Aug 10 22:14:12 
Aug 10 22:14:12 Expected: a string starting with ""org.apache.flink.util.FlinkException: Inconsistent execution state after stopping with savepoint. At least one execution is still in one of the following states: FAILED. A global fail-over is triggered to recover the job""
Aug 10 22:14:12      but: was ""org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.""
Aug 10 22:14:12 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Aug 10 22:14:12 	at org.junit.Assert.assertThat(Assert.java:964)
Aug 10 22:14:12 	at org.junit.Assert.assertThat(Assert.java:930)
Aug 10 22:14:12 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted(SavepointITCase.java:744)
Aug 10 22:14:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 10 22:14:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 10 22:14:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 10 22:14:12 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 10 22:14:12 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 10 22:14:12 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 10 22:14:12 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 10 22:14:12 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 10 22:14:12 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 10 22:14:12 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 10 22:14:12 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 10 22:14:12 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 10 22:14:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 10 22:14:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 10 22:14:12 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Aug 10 22:14:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Aug 10 22:14:12 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Aug 10 22:14:12 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 10 22:14:12 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Aug 10 22:14:12 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Aug 10 22:14:12 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Aug 10 22:14:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Aug 10 22:14:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13294511	Losing leadership does not clear rpc connection in JobManagerLeaderListener	"When losing the leadership the {{JobManagerLeaderListener}} closes the current {{rpcConnection}} but does not clear the field. This can lead to a failure of {{JobManagerLeaderListener#reconnect}} if this method is called after the {{JobMaster}} has lost its leadership.

I propose to clear the field so that {{RegisteredRpcConnection#tryReconnect}} won't be called on a closed rpc connection."	FLINK	Closed	3	1	10066	pull-request-available
13388528	HAQueryableStateRocksDBBackendITCase failed due to heap OOM	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=14431

{code}
Jul 08 21:43:22 [ERROR] Tests run: 12, Failures: 0, Errors: 9, Skipped: 1, Time elapsed: 246.345 s <<< FAILURE! - in org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase
Jul 08 21:43:22 [ERROR] testReducingState(org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase)  Time elapsed: 241.454 s  <<< ERROR!
Jul 08 21:43:22 java.lang.OutOfMemoryError: Java heap space
{code}"	FLINK	Closed	3	1	10066	auto-deprioritized-critical, test-stability
13210190	Dispatcher does not clean up blobs of failed submissions	The {{Dispatcher}} does not clean up blobs originating from a failed submissions. Compared to the legacy code, this is a regression and we should remove relevant blobs.	FLINK	Closed	3	1	10066	pull-request-available
13259537	Remove Akka specific parsing logic from LeaderConnectionInfo	The {{LeaderConnectionInfo}} assumes that every leader address is an Akka based address. This is not always true and unnecessarily restricts the leader contender to announce an Akka address. I propose to change this by doing the Akka address parsing outside of the {{LeaderConnectionInfo}}.	FLINK	Closed	4	4	10066	pull-request-available
13037025	YarnPreConfiguredMasterHaServicesTest fails sometimes	"This is the relevant part from the log:
{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.flink.yarn.highavailability.YarnPreConfiguredMasterHaServicesTest
Formatting using clusterid: testClusterID
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.407 sec - in org.apache.flink.yarn.highavailability.YarnPreConfiguredMasterHaServicesTest
Running org.apache.flink.yarn.highavailability.YarnIntraNonHaMasterServicesTest
Formatting using clusterid: testClusterID
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.479 sec <<< FAILURE! - in org.apache.flink.yarn.highavailability.YarnIntraNonHaMasterServicesTest
testClosingReportsToLeader(org.apache.flink.yarn.highavailability.YarnIntraNonHaMasterServicesTest)  Time elapsed: 0.836 sec  <<< FAILURE!
org.mockito.exceptions.verification.WantedButNotInvoked: 
Wanted but not invoked:
leaderContender.handleError(<any>);
-> at org.apache.flink.yarn.highavailability.YarnIntraNonHaMasterServicesTest.testClosingReportsToLeader(YarnIntraNonHaMasterServicesTest.java:120)
Actually, there were zero interactions with this mock.

	at org.apache.flink.yarn.highavailability.YarnIntraNonHaMasterServicesTest.testClosingReportsToLeader(YarnIntraNonHaMasterServicesTest.java:120)

Running org.apache.flink.yarn.YarnFlinkResourceManagerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.82 sec - in org.apache.flink.yarn.YarnFlinkResourceManagerTest
Running org.apache.flink.yarn.YarnClusterDescriptorTest
java.lang.RuntimeException: Couldn't deploy Yarn cluster
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploy(AbstractYarnClusterDescriptor.java:425)
	at org.apache.flink.yarn.YarnClusterDescriptorTest.testConfigOverwrite(YarnClusterDescriptorTest.java:90)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of virtual cores per node were configured with 2147483647 but Yarn only has 8 virtual cores available. Please note that the number of virtual cores is set to the number of task slots by default unless configured in the Flink config with 'yarn.containers.vcores.'
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.isReadyForDeployment(AbstractYarnClusterDescriptor.java:320)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:434)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploy(AbstractYarnClusterDescriptor.java:423)
	... 28 more
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.338 sec - in org.apache.flink.yarn.YarnClusterDescriptorTest

Results :

Failed tests: 
  YarnIntraNonHaMasterServicesTest.testClosingReportsToLeader:120 
Wanted but not invoked:
leaderContender.handleError(<any>);
-> at org.apache.flink.yarn.highavailability.YarnIntraNonHaMasterServicesTest.testClosingReportsToLeader(YarnIntraNonHaMasterServicesTest.java:120)
Actually, there were zero interactions with this mock.


Tests run: 10, Failures: 1, Errors: 0, Skipped: 0
{code}

https://s3.amazonaws.com/archive.travis-ci.org/jobs/194432647/log.txt"	FLINK	Resolved	3	1	10066	test-stability
13313194	Incompatible deprecated key type for registration cluster options 	"With FLINK-15827 we deprecated unused {{TaskManagerOptions}}. As part of this deprecation, we added them as deprecated keys for a couple of {{ClusterOptions}}. The problem is that the deprecated keys are of type {{Duration}} whereas the valid options are of type {{Long}}. Hence, the system will fail if a deprecated config option has been configured because it cannot be parsed as a long.

In order to solve the problem, I propose to remove the deprecated keys from the new {{ClusterOptions}}."	FLINK	Closed	2	1	10066	pull-request-available
13173910	OneInputStreamTaskTest.testWatermarkMetrics fails on Travis	"{{OneInputStreamTaskTest.testWatermarkMetrics}} fails on Travis with
{code}
java.lang.AssertionError: expected:<1> but was:<-9223372036854775808>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTest.testWatermarkMetrics(OneInputStreamTaskTest.java:731)
{code}

https://api.travis-ci.org/v3/job/407196285/log.txt"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
12782795	Add sparse vector and sparse matrix types to machine learning library	"Currently, the machine learning library only supports dense matrix and dense vectors. For future algorithms it would be beneficial to also support sparse vectors and matrices.

I'd propose to use the compressed sparse column (CSC) representation, because it allows rather efficient operations compared to a map backed sparse matrix/vector implementation. Furthermore, this is also the format the Breeze library expects for sparse matrices/vectors. Thus, it is easy to convert to a sparse breeze data structure which provides us with many linear algebra operations.

BIDMat [1] uses the same data representation.

Resources:
[1] [https://github.com/BIDData/BIDMat]"	FLINK	Closed	3	2	10066	ML
13139044	Don't let JobManagerRunner shut down itself	"Currently, the {{JobManagerRunner}} is allowed to shut down itself in case of a job completion. This, however, can cause problems when the {{Dispatcher}} receives a request for a {{JobMaster}}. If the {{Dispatcher}} is not told about the shut down of the {{JobMaster}} then it might still try to send requests to it. This will lead to time outs.

It would be better to simply let the {{JobManagerRunner}} not shut down itself and defer it to the owner (the {{Dispatcher}}). We can do this by listening on the {{JobManagerRunner#resultFuture}} which is completed by the {{JobManagerRunner}} in case of a successful job completion or a failure. That way we could also get rid of the {{OnCompletionActions}} and the {{FatalErrorHandler}}."	FLINK	Closed	3	4	10066	flip-6
13177923	Harden e2e Kafka shutdown	Due to KAFKA-4931, the shutdown of Kafka components can fail if the output of {{ps}} is limited to 4096 characters. Therefore, it can happen that e2e tests which start Kafka don't properly shut it down. I suggest to fix this problem by hardening our {{stop_kafka_cluster}} in {{kafka-common.sh}}.	FLINK	Closed	3	4	10066	pull-request-available
13173687	SlotPool#failAllocation is called outside of main thread	The {{JobMaster}} calls directly into the {{SlotPool#failAllocation}} in the method {{JobMaster#notifyAllocationFailure}}. This can the {{SlotPool}} to go into an inconsistent state.	FLINK	Closed	1	1	10066	pull-request-available
13118571	Bind logical slots to their request id instead of the slot allocation id	Since allocated slots can be reused to fulfil multiple slot requests, we should bind the resulting logical slots to their slot request id instead of the allocation id of the underlying allocated slot.	FLINK	Closed	3	4	10066	flip-6
13423956	YARNSessionFIFOSecuredITCase.testDetachedMode fails on AZP	"The test {{YARNSessionFIFOSecuredITCase.testDetachedMode}} fails on AZP:

{code}
2022-01-21T03:28:18.3712993Z Jan 21 03:28:18 java.lang.AssertionError: 
2022-01-21T03:28:18.3715115Z Jan 21 03:28:18 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-0_0/application_1642735639007_0002/container_1642735639007_0002_01_000001/jobmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2022-01-21T03:28:18.3716389Z Jan 21 03:28:18 [
2022-01-21T03:28:18.3717531Z Jan 21 03:28:18 2022-01-21 03:27:56,921 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2022-01-21T03:28:18.3720496Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@11c5f741db81:37697/user/rpc/dispatcher_0.
2022-01-21T03:28:18.3722401Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Interrupted while waiting for queue
2022-01-21T03:28:18.3723661Z Jan 21 03:28:18 java.lang.InterruptedException: null
2022-01-21T03:28:18.3724529Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014) ~[?:1.8.0_292]
2022-01-21T03:28:18.3725450Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048) ~[?:1.8.0_292]
2022-01-21T03:28:18.3726239Z Jan 21 03:28:18 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) ~[?:1.8.0_292]
2022-01-21T03:28:18.3727618Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:323) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3729147Z Jan 21 03:28:18 2022-01-21 03:27:56,927 WARN  org.apache.hadoop.ipc.Client                                 [] - Failed to connect to server: 11c5f741db81/172.25.0.2:39121: retries get failed due to exceeded maximum allowed retries number: 0
2022-01-21T03:28:18.3730293Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3730834Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3731499Z Jan 21 03:28:18 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[?:1.8.0_292]
2022-01-21T03:28:18.3732203Z Jan 21 03:28:18 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658) ~[?:1.8.0_292]
2022-01-21T03:28:18.3733478Z Jan 21 03:28:18 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3734470Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3735432Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3736414Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3737734Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3738853Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3739752Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1381) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3740638Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3741589Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3742621Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3743549Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3744684Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3745594Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3746221Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
2022-01-21T03:28:18.3746937Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
2022-01-21T03:28:18.3747615Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
2022-01-21T03:28:18.3748595Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3749706Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3750820Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3751915Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3753193Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3753988Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy52.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3754973Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainerInternal(NMClientImpl.java:316) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3756059Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainer(NMClientImpl.java:271) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3757457Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:541) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3758840Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:532) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3760149Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3761466Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3762578Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3763987Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3765348Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer.handle(NMClientAsyncImpl.java:617) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3766590Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor.run(NMClientAsyncImpl.java:676) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3767443Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
2022-01-21T03:28:18.3768153Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
2022-01-21T03:28:18.3768778Z Jan 21 03:28:18 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
2022-01-21T03:28:18.3769835Z Jan 21 03:28:18 2022-01-21 03:27:56,930 WARN  org.apache.flink.yarn.YarnResourceManagerDriver              [] - Error while calling YARN Node Manager to stop container container_1642735639007_0002_01_000002.
2022-01-21T03:28:18.3770910Z Jan 21 03:28:18 java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""11c5f741db81/172.25.0.2""; destination host is: ""11c5f741db81"":39121; 
2022-01-21T03:28:18.3772073Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3773188Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3774186Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3775072Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3776041Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3777094Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3777769Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) ~[?:?]
2022-01-21T03:28:18.3778869Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) ~[hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3779720Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3780182Z Jan 21 03:28:18 ]
2022-01-21T03:28:18.3780584Z Jan 21 03:28:18 	at org.junit.Assert.fail(Assert.java:89)
2022-01-21T03:28:18.3781169Z Jan 21 03:28:18 	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:591)
2022-01-21T03:28:18.3782025Z Jan 21 03:28:18 	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:82)
2022-01-21T03:28:18.3782677Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-21T03:28:18.3783557Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-21T03:28:18.3784250Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-21T03:28:18.3784878Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-21T03:28:18.3785487Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-21T03:28:18.3786168Z Jan 21 03:28:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-21T03:28:18.3786840Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-21T03:28:18.3787505Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-01-21T03:28:18.3788173Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-01-21T03:28:18.3788909Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3789557Z Jan 21 03:28:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-21T03:28:18.3790179Z Jan 21 03:28:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-21T03:28:18.3790755Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3791399Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-21T03:28:18.3792038Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-21T03:28:18.3792667Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-21T03:28:18.3793646Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-21T03:28:18.3794282Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T03:28:18.3794889Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T03:28:18.3795483Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T03:28:18.3796096Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T03:28:18.3796697Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T03:28:18.3797324Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-21T03:28:18.3797967Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-21T03:28:18.3798600Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3799223Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3799806Z Jan 21 03:28:18 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-21T03:28:18.3800367Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3800963Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T03:28:18.3801528Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-21T03:28:18.3802074Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-21T03:28:18.3802689Z Jan 21 03:28:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-21T03:28:18.3803675Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-21T03:28:18.3804487Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-21T03:28:18.3805205Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-21T03:28:18.3805990Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-21T03:28:18.3806794Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-21T03:28:18.3807616Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-21T03:28:18.3808420Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-21T03:28:18.3809106Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-21T03:28:18.3809759Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-21T03:28:18.3810463Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-21T03:28:18.3835763Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-21T03:28:18.3836503Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-21T03:28:18.3837248Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-21T03:28:18.3838212Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-01-21T03:28:18.3838953Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-21T03:28:18.3839649Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-21T03:28:18.3840279Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-21T03:28:18.3840921Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

The test {{YARNSessionFIFOSecuredITCase.testQueryCluster}} failed with:

{code}
2022-01-21T03:28:18.3842564Z Jan 21 03:28:18 java.lang.AssertionError: 
2022-01-21T03:28:18.3844597Z Jan 21 03:28:18 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-0_0/application_1642735639007_0002/container_1642735639007_0002_01_000001/jobmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2022-01-21T03:28:18.3845483Z Jan 21 03:28:18 [
2022-01-21T03:28:18.3846666Z Jan 21 03:28:18 2022-01-21 03:27:56,921 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2022-01-21T03:28:18.3848108Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@11c5f741db81:37697/user/rpc/dispatcher_0.
2022-01-21T03:28:18.3849225Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Interrupted while waiting for queue
2022-01-21T03:28:18.3849824Z Jan 21 03:28:18 java.lang.InterruptedException: null
2022-01-21T03:28:18.3850480Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014) ~[?:1.8.0_292]
2022-01-21T03:28:18.3851311Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048) ~[?:1.8.0_292]
2022-01-21T03:28:18.3852206Z Jan 21 03:28:18 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) ~[?:1.8.0_292]
2022-01-21T03:28:18.3853638Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:323) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3855047Z Jan 21 03:28:18 2022-01-21 03:27:56,927 WARN  org.apache.hadoop.ipc.Client                                 [] - Failed to connect to server: 11c5f741db81/172.25.0.2:39121: retries get failed due to exceeded maximum allowed retries number: 0
2022-01-21T03:28:18.3855925Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3856444Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3857075Z Jan 21 03:28:18 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[?:1.8.0_292]
2022-01-21T03:28:18.3857778Z Jan 21 03:28:18 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658) ~[?:1.8.0_292]
2022-01-21T03:28:18.3858858Z Jan 21 03:28:18 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3859778Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3860702Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3861692Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3862939Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3863944Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3864820Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1381) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3865660Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3866604Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3867586Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3868233Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3869278Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3870212Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3870850Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
2022-01-21T03:28:18.3871580Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
2022-01-21T03:28:18.3872230Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
2022-01-21T03:28:18.3873375Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3874477Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3875544Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3876609Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3877809Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3878470Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy52.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3879381Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainerInternal(NMClientImpl.java:316) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3880442Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainer(NMClientImpl.java:271) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3881623Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:541) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3883111Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:532) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3884553Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3885610Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3886619Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3887692Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3888827Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer.handle(NMClientAsyncImpl.java:617) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3890014Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor.run(NMClientAsyncImpl.java:676) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3890841Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
2022-01-21T03:28:18.3891528Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
2022-01-21T03:28:18.3892129Z Jan 21 03:28:18 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
2022-01-21T03:28:18.3893275Z Jan 21 03:28:18 2022-01-21 03:27:56,930 WARN  org.apache.flink.yarn.YarnResourceManagerDriver              [] - Error while calling YARN Node Manager to stop container container_1642735639007_0002_01_000002.
2022-01-21T03:28:18.3894308Z Jan 21 03:28:18 java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""11c5f741db81/172.25.0.2""; destination host is: ""11c5f741db81"":39121; 
2022-01-21T03:28:18.3895436Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3896356Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3897214Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3898060Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3898994Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3899987Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3900639Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) ~[?:?]
2022-01-21T03:28:18.3901798Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) ~[hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3902618Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3903137Z Jan 21 03:28:18 ]
2022-01-21T03:28:18.3903592Z Jan 21 03:28:18 	at org.junit.Assert.fail(Assert.java:89)
2022-01-21T03:28:18.3904183Z Jan 21 03:28:18 	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:591)
2022-01-21T03:28:18.3904901Z Jan 21 03:28:18 	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:82)
2022-01-21T03:28:18.3905526Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-21T03:28:18.3906105Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-21T03:28:18.3906769Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-21T03:28:18.3907469Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-21T03:28:18.3908059Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-21T03:28:18.3908700Z Jan 21 03:28:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-21T03:28:18.3909350Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-21T03:28:18.3909993Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-01-21T03:28:18.3910632Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-01-21T03:28:18.3911243Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3911869Z Jan 21 03:28:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-21T03:28:18.3912465Z Jan 21 03:28:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-21T03:28:18.3913092Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3913775Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-21T03:28:18.3914387Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-21T03:28:18.3914995Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-21T03:28:18.3915645Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-21T03:28:18.3916252Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T03:28:18.3916823Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T03:28:18.3917405Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T03:28:18.3917976Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T03:28:18.3918554Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T03:28:18.3919152Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-21T03:28:18.3919778Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-21T03:28:18.3920386Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3920980Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3921545Z Jan 21 03:28:18 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-21T03:28:18.3922166Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3922727Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T03:28:18.3923398Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-21T03:28:18.3923921Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-21T03:28:18.3924505Z Jan 21 03:28:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-21T03:28:18.3925163Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-21T03:28:18.3926090Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-21T03:28:18.3926927Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-21T03:28:18.3927700Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-21T03:28:18.3928487Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-21T03:28:18.3929388Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-21T03:28:18.3930162Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-21T03:28:18.3930858Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-21T03:28:18.3931520Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-21T03:28:18.3932245Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-21T03:28:18.3933105Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-21T03:28:18.3933889Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-21T03:28:18.3934645Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-21T03:28:18.3935380Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-01-21T03:28:18.3936073Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-21T03:28:18.3936725Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-21T03:28:18.3937348Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29841&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461"	FLINK	Closed	2	1	10066	test-stability
13240355	TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerProcessFailure failed on travis 	"Logs:  [https://api.travis-ci.org/v3/job/547509708/log.txt]

Build: [https://travis-ci.org/apache/flink/builds/547509701]"	FLINK	Resolved	2	1	10066	test-stability
13110026	Make MetricFetcher work with RestfulGateway	In order to make the {{MetricFetcher}} work together with the new architecture, we have to remove it's dependence on the {{JobManagerGateway}}.	FLINK	Closed	3	7	10066	flip-6
13428554	Update statefun-playground examples to use playground ingress/egress	With FLINK-26153, the statefun-playground has a new ingress/egress that allows to interact with via curl. I propose to update the existing examples to make use of it.	FLINK	Closed	3	4	10066	pull-request-available
13145042	Jobs will not recover if DFS is temporarily unavailable	"*Description*
Job graphs will be recovered only once from the DFS. If the DFS is unavailable at the recovery attempt, the jobs will simply be not running until the master is restarted again.

*Steps to reproduce*
# Submit job on Flink Cluster with HDFS as HA storage dir.
# Trigger job recovery by killing the master
# Stop HDFS NameNode
# Enable HDFS NameNode after job recovery is over
# Verify that job is not running.

*Expected behavior*
The new master should fail fast and exit. The new master should re-attempt the recovery.

*Stacktrace*
{noformat}
2018-03-14 14:01:37,704 ERROR org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Could not recover the job graph for a41d50b6f3ac16a730dd12792a847c97.
org.apache.flink.util.FlinkException: Could not retrieve submitted JobGraph from state handle under /a41d50b6f3ac16a730dd12792a847c97. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.
	at org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore.recoverJobGraph(ZooKeeperSubmittedJobGraphStore.java:192)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$recoverJobs$5(Dispatcher.java:557)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.net.ConnectException: Call From ip-172-31-43-54/172.31.43.54 to ip-172-31-32-118.eu-central-1.compute.internal:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:259)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:843)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:832)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:821)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:285)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1132)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:322)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.open(HadoopFileSystem.java:119)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.open(HadoopFileSystem.java:36)
	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68)
	at org.apache.flink.runtime.state.RetrievableStreamStateHandle.openInputStream(RetrievableStreamStateHandle.java:64)
	at org.apache.flink.runtime.state.RetrievableStreamStateHandle.retrieveState(RetrievableStreamStateHandle.java:57)
	at org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore.recoverJobGraph(ZooKeeperSubmittedJobGraphStore.java:186)
	... 7 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 40 more
{noformat}"	FLINK	Resolved	1	1	10066	flip6
13127991	Print help for DefaultCLI	Print help for the {{DefaultCLI}} when calling {{flink -help}}, for example.	FLINK	Closed	3	7	10066	flip-6
12781666	Add polynomial base feature mapper to ML library	Add feature mapper which maps a vector into the polynomial feature space. This can be used as a preprocessing step prior to applying a {{Learner}} of Flink's ML library.	FLINK	Closed	3	4	10066	ML
13424956	ExecutionVertex.getLatestPriorAllocation fails if there is an unsuccessful restart attempt	The {{ExecutionVertex.getLatestPriorAllocation}} does not return the latest prior allocation if there was an unsuccessful restart attempt in between. The problem is that we only look at the last {{Execution}}. Due to this, {{ExecutionVertex.getLatestPriorAllocation}} sometimes returns {{null}} even though there is a prior {{AllocationID}}.	FLINK	Closed	3	7	10066	pull-request-available
13425841	Add Java connected components example to flink-statefun-playground	In order to show what we can do with Stateful Functions I suggest to add an example for how to calculate the connected components from a stream of vertices.	FLINK	Closed	3	4	10066	pull-request-available
13386994	RpcService should fail result futures if messages could not be sent	The {{RpcService}} should fail result futures if messages could not be sent. This would speed up the failure detection mechanism because it would not rely on the timeout. One way to achieve this could be to listen to the dead letters and then sending a {{Failure}} message back to the sender.	FLINK	Closed	3	4	10066	pull-request-available
13375476	ClusterEntrypointTest.testCloseAsyncShouldBeExecutedInShutdownHook failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17313&view=logs&j=21408240-6569-5a01-c099-3adfe83ce651&t=b2761bb8-3852-5a0d-bc43-6a1d327b63cb&l=6207


{code:java}
52 [ERROR] testCloseAsyncShouldBeExecutedInShutdownHook(org.apache.flink.runtime.entrypoint.ClusterEntrypointTest)  Time elapsed: 9.83 s  <<< FAILURE!
Apr 27 21:20:52 java.lang.AssertionError: 
Apr 27 21:20:52 Process 843 does not exit within 3000 ms
Apr 27 21:20:52 Expected: is <true>
Apr 27 21:20:52      but: was <false>
Apr 27 21:20:52 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Apr 27 21:20:52 	at org.junit.Assert.assertThat(Assert.java:956)
Apr 27 21:20:52 	at org.apache.flink.runtime.entrypoint.ClusterEntrypointTest.testCloseAsyncShouldBeExecutedInShutdownHook(ClusterEntrypointTest.java:224)
Apr 27 21:20:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 27 21:20:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 27 21:20:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 27 21:20:52 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 27 21:20:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 27 21:20:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 27 21:20:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 27 21:20:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 27 21:20:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 27 21:20:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 27 21:20:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 27 21:20:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 27 21:20:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 27 21:20:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 27 21:20:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 21:20:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 21:20:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 21:20:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)

{code}
"	FLINK	Closed	4	1	10066	auto-deprioritized-major, pull-request-available, test-stability
12782794	Add support to read libSVM and SVMLight input files	"In order to train SVMs, the machine learning library should be able to read standard SVM input file formats. A widespread format is used by libSVM and SMVLight which has the following format:

<line> .=. <target> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>
<target> .=. +1 | -1 | 0 | <float> 
<feature> .=. <integer> | ""qid""
<value> .=. <float>
<info> .=. <string>

Details can be found [here|http://svmlight.joachims.org/] and [here|http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#/Q03:_Data_preparation]"	FLINK	Closed	3	2	10066	ML
13362328	TaskManager connected to invalid JobManager leading to TaskSubmissionException	"While testing reactive mode, I had to start my JobManager a few times to get the configuration right. While doing that, I had at least on TaskManager (TM6), which was first connected to the first JobManager (with a running job), and then to the second one.

On the second JobManager, I was able to execute my test job (on another TaskManager (TMx)), once TM6 reconnected, and reactive mode tried to utilize all available resources, I repeatedly ran into this issue:

{code}
2021-03-04 15:49:36,322 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (5/7) (ae8f39c8dd88148aff93c8f811fab22e) switched from DEPLOYING to FAILED on 192.168.2.173:64041-4f7521 @ macbook-pro-2.localdomain (dataPort=64044).
java.util.concurrent.CompletionException: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job bbe8634736b5b1d813dd322cfaaa08ea.
	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_252]
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1064) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:263) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:261) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:101) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:999) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:458) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job bbe8634736b5b1d813dd322cfaaa08ea.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$submitTask$3(TaskExecutor.java:523) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.Optional.orElseThrow(Optional.java:290) ~[?:1.8.0_252]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:514) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	... 9 more
{code}

I will upload all logs to this ticket and post my initial analysis.
"	FLINK	Closed	2	1	10066	pull-request-available
13397445	Move Powered By section into separate page	The PMC was just informed that it is not allowed to have a Powered By section on the main homepage. We need to move it into a dedicated page.	FLINK	Closed	1	11500	10066	pull-request-available
13284798	Improve error reporting when submitting batch job (instead of AskTimeoutException)	"While debugging the {{Shaded Hadoop S3A end-to-end test (minio)}} pre-commit test, I noticed that the JobSubmission is not producing very helpful error messages.

Environment:
- A simple batch wordcount job 
- a unavailable minio s3 filesystem service

What happens from a user's perspective:
- The job submission fails after 10 seconds with a AskTimeoutException:
{code}
2020-02-07T11:38:27.1189393Z akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-939201095]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-02-07T11:38:27.1189538Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-02-07T11:38:27.1189616Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-02-07T11:38:27.1189713Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-02-07T11:38:27.1189789Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-02-07T11:38:27.1189883Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2020-02-07T11:38:27.1189973Z 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2020-02-07T11:38:27.1190067Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2020-02-07T11:38:27.1190159Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2020-02-07T11:38:27.1190267Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2020-02-07T11:38:27.1190358Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2020-02-07T11:38:27.1190465Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-02-07T11:38:27.1190540Z 	at java.lang.Thread.run(Thread.java:748)
{code}

What a user would expect:
- An error message indicating why the job submission failed.
"	FLINK	Resolved	1	4	10066	pull-request-available
13260454	"Stuck in ""Job leader ... lost leadership"" error"	"This is the first exception caused restart loop. Later exceptions are the same. Job seems to stuck in this permanent failure state.

{code}
2019-10-03 21:42:46,159 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: clpevents -> device_filter -> processed_imps -> ios_processed_impression -> i
mps_ts_assigner (449/1360) (d237f5e99b6a4a580498821473763edb) switched from SCHEDULED to FAILED.
java.lang.Exception: Job leader for job id ecb9ad9be934edf7b1a4f7b9dd6df365 lost leadership.
        at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobLeaderListenerImpl.lambda$jobManagerLostLeadership$1(TaskExecutor.java:1526)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
        at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
        at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
        at akka.actor.ActorCell.invoke(ActorCell.scala:495)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
        at akka.dispatch.Mailbox.run(Mailbox.scala:224)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}"	FLINK	Resolved	2	1	10066	pull-request-available
13386143	JobMasterTest.testMultipleStartsWork unstable on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19578&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034&l=6143

{code}
[ERROR] Tests run: 28, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.067 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterTest
[ERROR] testMultipleStartsWork(org.apache.flink.runtime.jobmaster.JobMasterTest)  Time elapsed: 0.054 s  <<< ERROR!
java.util.concurrent.CompletionException: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka://flink/user/rpc/jobmanager_17 has not been started yet.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.jobmaster.JobMasterTest.testMultipleStartsWork(JobMasterTest.java:2314)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka://flink/user/rpc/jobmanager_17 has not been started yet.
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:170)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:517)
	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13183972	Blocking calls in Execution Graph creation bring down cluster	"When the execution graph creation has long calls (or blocking calls), it brings down the cluster. It seems to block the main actor thread of the dispatcher, making the entire cluster unresponsive (web UI, etc.)

h3. Reproducing

One way to reproduce this is to use a FileSystem that blocks on initialization or on opening a stream.

h3. Full Exception Log

{code}
at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1022)
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:985)
	at org.apache.flink.runtime.fs.hdfs.HadoopFsFactory.create(HadoopFsFactory.java:159)
	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:399)
	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:318)
	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298)
	at org.apache.flink.runtime.state.filesystem.FsCheckpointStorage.<init>(FsCheckpointStorage.java:61)
	at org.apache.flink.runtime.state.filesystem.FsStateBackend.createCheckpointStorage(FsStateBackend.java:443)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:248)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.enableCheckpointing(ExecutionGraph.java:495)
	at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:345)
	at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:100)
	at org.apache.flink.runtime.jobmaster.JobMaster.createExecutionGraph(JobMaster.java:1152)
	at org.apache.flink.runtime.jobmaster.JobMaster.createAndRestoreExecutionGraph(JobMaster.java:1132)
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:295)
	at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:157)
	at org.apache.flink.runtime.dispatcher.Dispatcher$DefaultJobManagerRunnerFactory.createJobManagerRunner(Dispatcher.java:936)
	at org.apache.flink.runtime.dispatcher.Dispatcher.createJobManagerRunner(Dispatcher.java:291)
	at org.apache.flink.runtime.dispatcher.Dispatcher.runJob(Dispatcher.java:281)
	at org.apache.flink.runtime.dispatcher.Dispatcher.persistAndRunJob(Dispatcher.java:266)
	at org.apache.flink.runtime.dispatcher.Dispatcher$$Lambda$108/1800332159.acceptWithException(Unknown Source)
	at org.apache.flink.util.function.ConsumerWithException.accept(ConsumerWithException.java:38)
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$waitForTerminatingJobManager$29(Dispatcher.java:820)
	at org.apache.flink.runtime.dispatcher.Dispatcher$$Lambda$110/1318857678.run(Unknown Source)
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705)
	at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:687)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)


As a result, everything becomes unresponsive and breaks, and log is full of these errors:

java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:06:58,679 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:08,629 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:08,663 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:08,679 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:08,700 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:18,712 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:18,729 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:28,739 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:28,749 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:38,709 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:38,729 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:38,769 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:38,778 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:48,788 ERROR org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
2018-08-24 17:07:48,799 ERROR org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler  - Could not retrieve the redirect address.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:770)
	at akka.dispatch.OnComplete.internal(Future.scala:258)
	at akka.dispatch.OnComplete.internal(Future.scala:256)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	at java.lang.Thread.run(Thread.java:745)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#146546706]] after [10000 ms]. Sender[null] sent message of type ""org.apache.flink.runtime.rpc.messages.LocalFencedMessage"".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	... 9 more
{code}"	FLINK	Resolved	1	1	10066	pull-request-available
12959129	LeaderChangeStateCleanupTest.testStateCleanupAfterListenerNotification fails on Travis	"The {{LeaderChangeStateCleanupTest.testStateCleanupAfterListenerNotification}} fails spuriously on Travis because of a {{NullPointerException}}. The reason is that it's not properly waited until the {{ResourceManager}} has been started. Due to this, it can happen that a leader notification message is tried to be sent to a {{LeaderRetrievalListener}} which has not been set by the {{ResourceManager}}.

[1] https://s3.amazonaws.com/archive.travis-ci.org/jobs/123271732/log.txt"	FLINK	Closed	2	1	10066	test-stability
13194381	Harden Confluent schema E2E test	The Confluent schema E2E test starts a Confluent schema registry to run against. If the schema registry cannot be started, the test proceeds and simply dead locks. In order to improve the situation I suggest to fail if we cannot start the Confluent schema registry.	FLINK	Closed	3	4	10066	pull-request-available
13190628	MemoryManagerConcurrentModReleaseTest.testConcurrentModificationWhileReleasing failed on travis	"{{MemoryManagerConcurrentModReleaseTest.testConcurrentModificationWhileReleasing}} failed on Travis.

[https://travis-ci.org/zentol/flink/jobs/439591249]

 
{code:java}
Running org.apache.flink.runtime.heartbeat.HeartbeatManagerTest
java.util.NoSuchElementException
	at java.util.ArrayList$Itr.next(ArrayList.java:860)
	at org.apache.flink.runtime.memory.MemoryManager.release(MemoryManager.java:415)
	at org.apache.flink.runtime.memory.MemoryManagerConcurrentModReleaseTest.testConcurrentModificationWhileReleasing(MemoryManagerConcurrentModReleaseTest.java:76)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.065 sec <<< FAILURE! - in org.apache.flink.runtime.memory.MemoryManagerConcurrentModReleaseTest
testConcurrentModificationWhileReleasing(org.apache.flink.runtime.memory.MemoryManagerConcurrentModReleaseTest)  Time elapsed: 0.977 sec  <<< FAILURE!
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.apache.flink.runtime.memory.MemoryManagerConcurrentModReleaseTest.testConcurrentModificationWhileReleasing(MemoryManagerConcurrentModReleaseTest.java:86){code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13259529	Simplify DispatcherResourceManagerComponent	With the completion of the FLINK-14281 it is now possible to encapsulate the shutdown logic of the {{MiniDispatcher}} within the {{DispatcherRunner}}. Consequently, it is no longer necessary to have separate {{DispatcherResourceManagerComponent}} implementations. I suggest to remove the special case implementations.	FLINK	Closed	4	4	10066	pull-request-available
13071525	ExecutionGraphMetricsTest fails on Windows CI	"The {{testExecutionGraphRestartTimeMetric}} fails every time i run it on AppVeyor. It also very rarely failed for me locally.

The test fails at Line 235 if the RUNNING timestamp is equal to the RESTARTING timestamp, which may happen when combining a fast test with a low resolution clock.

A simple fix would be to increase the timestamp between RUNNING and RESTARTING by adding a 50ms sleep timeout into the {{TestingRestartStrategy#canRestart()}} method, as this one is called before transitioning to the RESTARTING state."	FLINK	Closed	1	1	10066	test-stability
12996206	Refactor CLI to use RESTful client for cluster communication	"We have to refactor the Flink's CLI in order to be able to use the RESTful client to communicate with the new Flip-6 Flink cluster.

This could be done by implementing a new {{ClusterClient}} or by refactoring the {{ClusterClient}} to not use the static methods of the {{JobClient}} but instead an interface implementation."	FLINK	Resolved	3	4	10066	flip-6
13385188	'Queryable state (rocksdb) with TM restart end-to-end test' fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19310&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=12333

{code}
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Jun 22 14:04:55 MapState has 22 entries
Jun 22 14:04:56 TaskManager 422719 killed.
Jun 22 14:04:56 Number of running task managers 1 is not yet 0.
Jun 22 14:05:00 Number of running task managers 1 is not yet 0.
Jun 22 14:05:04 Number of running task managers 1 is not yet 0.
Jun 22 14:05:08 Number of running task managers has reached 0.
Jun 22 14:05:08 Latest snapshot count was 42
Jun 22 14:05:09 Starting taskexecutor daemon on host fv-az68-17.
Jun 22 14:05:09 Number of running task managers 0 is not yet 1.
Jun 22 14:05:13 Number of running task managers has reached 1.
Jun 22 14:05:15 Job (5b515e0f9168e338d1645bf2e9f92820) is running.
Jun 22 14:05:15 Starting to wait for completion of 18 checkpoints
Jun 22 14:05:15 13/18 completed checkpoints
Jun 22 14:05:17 13/18 completed checkpoints
Jun 22 14:05:19 17/18 completed checkpoints
Jun 22 14:05:21 17/18 completed checkpoints
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Jun 22 14:05:24 after: 40
Jun 22 14:05:24 An error occurred
Jun 22 14:05:24 [FAIL] Test script contains errors.
Jun 22 14:05:24 Checking of logs skipped.
Jun 22 14:05:24 
Jun 22 14:05:24 [FAIL] 'Queryable state (rocksdb) with TM restart end-to-end test' failed after 0 minutes and 50 seconds! Test exited with exit code 1
Jun 22 14:05:24 
14:05:24 ##[group]Environment Information

{code}"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13252967	Running HA (file, async) end-to-end test failed on Travis	"{{Running HA (file, async) end-to-end test}} failed on Travis:

https://api.travis-ci.org/v3/job/576002743/log.txt
https://api.travis-ci.org/v3/job/576002736/log.txt
https://api.travis-ci.org/v3/job/576002730/log.txt
https://api.travis-ci.org/v3/job/576002724/log.txt"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13137537	Allow JobMaster to rescale jobs	"The {{JobMaster}} should be able to rescale a job or a subset of its operators. In order to do that we have to expose RPC calls to trigger this action.

The rescaling works by first taking a savepoint, then suspending the old job, rescale it and then restart it from the taken savepoint."	FLINK	Closed	3	2	10066	flip-6
13085343	Add JobManagerLeaderRetrieval method with default JobManager address	The {{HighAvailabilityServices}} should have a method {{getJobManagerLeaderRetriever}} which can be called with a {{JobID}} and a default {{JobManager}} address. This is a requirement for Flip-6 where in the {{StandaloneHaServices}} case, we need to dynamically create a {{LeaderRetrievalService}} with the given {{JobManager}} address.	FLINK	Closed	3	7	10066	flip-6
13084361	Support fencing tokens to filter out outdated messages	"In order to guard against split brain situations, it is important that RPC calls are guarded with a fencing token. The sender attaches his fencing token to a RPC message which is then used on the receiver side to compare against the expected fencing token. An example is the leader session ID which we attach to all critical RPC messages.

So far, in the Flip-6 code base we send fencing tokens explicitly. This is not only cumbersome but also error-prone because you have to do it for all RPCs. Therefore, it would be better if we could automatically compare fencing tokens for a given RPC from a given source. This should ideally happen on the level of the RPC server."	FLINK	Closed	3	7	10066	flip-6
13084495	Start TaskExecutor via start-up script	Start the {{TaskExecutor}} via the start-up scripts.	FLINK	Closed	4	7	10066	flip-6
13216772	Flink fails to remove JobGraph from ZK even though it reports it did	"We recently have seen the following issue with Flink 1.5.5:

Given Flink Job ID 1d24cad26843dcebdfca236d5e3ad82a: 

1- A job is activated successfully and the job graph added to ZK:
{code:java}
Added SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null) to ZooKeeper.
{code}
2- Job is deactivated, Flink reports that the job graph has been successfully removed from ZK and the blob is deleted from the blob server (in this case S3):
{code:java}
Removed job graph 1d24cad26843dcebdfca236d5e3ad82a from ZooKeeper.
{code}
3- JM is later restarted, Flink for some reason attempts to recover the job that it reported earlier it has removed from ZK but since the blob has already been deleted the JM goes into a crash loop. The only way to recover it manually is to remove the job graph entry from ZK:
{code:java}
Recovered SubmittedJobGraph(1d24cad26843dcebdfca236d5e3ad82a, null).	
{code}
and
{code:java}
org.apache.flink.fs.s3presto.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 1BCDFD83FC4546A2), S3 Extended Request ID: OzZtMbihzCm1LKy99s2+rgUMxyll/xYmL6ouMvU2eo30wuDbUmj/DAWoTCs9pNNCLft0FWqbhTo= (Path: s3://blam-state-staging/flink/default/blob/job_1d24cad26843dcebdfca236d5e3ad82a/blob_p-c51b25cc0b20351f6e32a628bb6e674ee48a273e-ccfa96b0fd795502897c73714185dde3)
{code}

My question is under what circumstances would this happen? this seems to happen very infrequently but since the consequence is severe (JM crash loop) we'd like to understand how it would happen.

This  all seems a little similar to https://issues.apache.org/jira/browse/FLINK-9575 but that issue is reported fixed in Flink 1.5.2 and we are already on Flink 1.5.5"	FLINK	Closed	2	1	10066	pull-request-available
13425703	ClusterEntrypointTest.testWorkingDirectoryIsDeletedIfApplicationCompletes  failed on azure	"
{code:java}
2022-01-31T05:00:07.3113870Z Jan 31 05:00:07 java.util.concurrent.CompletionException: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.
2022-01-31T05:00:07.3115008Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-01-31T05:00:07.3115778Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-01-31T05:00:07.3116527Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-01-31T05:00:07.3117267Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-01-31T05:00:07.3118011Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-01-31T05:00:07.3118770Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-01-31T05:00:07.3119608Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:251)
2022-01-31T05:00:07.3120425Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-01-31T05:00:07.3121199Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-01-31T05:00:07.3121957Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-01-31T05:00:07.3122716Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-01-31T05:00:07.3123457Z Jan 31 05:00:07 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387)
2022-01-31T05:00:07.3124241Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-01-31T05:00:07.3125106Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-01-31T05:00:07.3126063Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-01-31T05:00:07.3127207Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-01-31T05:00:07.3127982Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-01-31T05:00:07.3128741Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-01-31T05:00:07.3129497Z Jan 31 05:00:07 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-01-31T05:00:07.3130385Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)
2022-01-31T05:00:07.3131092Z Jan 31 05:00:07 	at akka.dispatch.OnComplete.internal(Future.scala:299)
2022-01-31T05:00:07.3131695Z Jan 31 05:00:07 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-01-31T05:00:07.3132310Z Jan 31 05:00:07 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-01-31T05:00:07.3132943Z Jan 31 05:00:07 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-01-31T05:00:07.3133577Z Jan 31 05:00:07 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-01-31T05:00:07.3134340Z Jan 31 05:00:07 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-01-31T05:00:07.3135149Z Jan 31 05:00:07 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-01-31T05:00:07.3135898Z Jan 31 05:00:07 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-01-31T05:00:07.3136692Z Jan 31 05:00:07 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-01-31T05:00:07.3137454Z Jan 31 05:00:07 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-01-31T05:00:07.3138127Z Jan 31 05:00:07 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-01-31T05:00:07.3138726Z Jan 31 05:00:07 	at akka.actor.ActorRef.tell(ActorRef.scala:131)
2022-01-31T05:00:07.3139391Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.sendErrorIfSender(AkkaRpcActor.java:501)
2022-01-31T05:00:07.3140173Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:173)
2022-01-31T05:00:07.3140882Z Jan 31 05:00:07 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-01-31T05:00:07.3141535Z Jan 31 05:00:07 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-01-31T05:00:07.3142177Z Jan 31 05:00:07 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-01-31T05:00:07.3142822Z Jan 31 05:00:07 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-01-31T05:00:07.3143467Z Jan 31 05:00:07 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-01-31T05:00:07.3144145Z Jan 31 05:00:07 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-01-31T05:00:07.3145019Z Jan 31 05:00:07 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-01-31T05:00:07.3145744Z Jan 31 05:00:07 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-01-31T05:00:07.3146421Z Jan 31 05:00:07 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-01-31T05:00:07.3147053Z Jan 31 05:00:07 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-01-31T05:00:07.3147714Z Jan 31 05:00:07 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-01-31T05:00:07.3148417Z Jan 31 05:00:07 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-01-31T05:00:07.3149072Z Jan 31 05:00:07 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-01-31T05:00:07.3149725Z Jan 31 05:00:07 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-01-31T05:00:07.3231566Z Jan 31 05:00:07 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-01-31T05:00:07.3232417Z Jan 31 05:00:07 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-01-31T05:00:07.3233367Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-01-31T05:00:07.3234208Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-01-31T05:00:07.3234909Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-01-31T05:00:07.3235609Z Jan 31 05:00:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-01-31T05:00:07.3237023Z Jan 31 05:00:07 Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka.tcp://flink@127.0.0.1:6123/user/rpc/resourcemanager_2 has not been started yet.
2022-01-31T05:00:07.3238316Z Jan 31 05:00:07 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:175)
2022-01-31T05:00:07.3238886Z Jan 31 05:00:07 	... 20 more
2022-01-31T05:00:07.3239220Z Jan 31 05:00:07 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30491&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12987"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13384752	RunnablesTest.testExecutorService_uncaughtExceptionHandler fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19152&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034&l=6902
{code}
Jun 18 21:25:48 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.402 s <<< FAILURE! - in org.apache.flink.runtime.util.RunnablesTest
Jun 18 21:25:48 [ERROR] testExecutorService_uncaughtExceptionHandler(org.apache.flink.runtime.util.RunnablesTest)  Time elapsed: 0.121 s  <<< FAILURE!
Jun 18 21:25:48 java.lang.AssertionError: Expected handler to be called.
Jun 18 21:25:48 	at org.junit.Assert.fail(Assert.java:89)
Jun 18 21:25:48 	at org.junit.Assert.assertTrue(Assert.java:42)
Jun 18 21:25:48 	at org.apache.flink.runtime.util.RunnablesTest.testExecutorService_uncaughtExceptionHandler(RunnablesTest.java:56)
Jun 18 21:25:48 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 18 21:25:48 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 18 21:25:48 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 18 21:25:48 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 18 21:25:48 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jun 18 21:25:48 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 18 21:25:48 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jun 18 21:25:48 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 18 21:25:48 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jun 18 21:25:48 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jun 18 21:25:48 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13127930	Make CustomCommandLines non static in CliFrontend	For better testability and maintainability we should make the {{CustomCommandLine}} registration non-static in {{CliFrontend}}.	FLINK	Closed	3	7	10066	flip-6
13277212	Drop vendor specific repositories from pom.xml	"Since Flink no longer bundles Hadoop dependencies we also don't need the vendor specific Hadoop repositories in Flink's {{pom.xml}}. Consequently, I suggest to remove them. 

This idea has been discussed on Flink's dev ML: https://lists.apache.org/thread.html/be402a11bc986219eabd9dd8af507f36f49784d5400d0873e9ec0c2e%40%3Cdev.flink.apache.org%3E."	FLINK	Closed	4	4	10066	pull-request-available
13097784	Add StaticFileServerHandler to DispatcherRestEndpoint	The {{DispatcherRestEndpoint}} should be able to server static web content. In order to do that it should instantiate the {{StaticFileServerHandler}} if {{flink-runtime-web}} is in the classpath.	FLINK	Closed	4	7	10066	flip-6
13217237	Add onStart method to RpcEndpoint which is run in the actor's main thread	"I propose to introduce a {{RpcEndpoint#onStart}} method which is called when the {{RpcEndpoint}} is started via {{RpcEndpoint#start}}. At the moment, users will override {{#start}} where they need to remember to also call {{super.start()}} in order to actually start the {{RpcEndpoint}}. Moreover, the logic executed by {{start}} won't be run in the actor's main thread. This is problematic if the method triggers asynchronous behaviour which is executed in the actor's main thread. If that is the case, it can happen that the asynchronous operation is executed before the {{start}} method has been finished. 

Due to these problems, I suggest to introduce a {{RpcEndpoint#onStart}} method which can be overriden by sub classes similarly to the {{RpcEndpoint#onStop}} method. The {{onStart}} method can be used to setup the {{RpcEndpoint}} and is guaranteed to be executed before any other message is processed."	FLINK	Closed	3	4	10066	pull-request-available
13141606	Possible resource leak in Flip6	"In this build (https://travis-ci.org/zentol/flink/builds/347373839) I set the codebase to flip6 for half the profiles to find failing tests.

The ""libraries"" job (https://travis-ci.org/zentol/flink/jobs/347373851) failed with an OutOfMemoryError.

This could mean that there is a memory-leak somewhere."	FLINK	Resolved	1	1	10066	flip-6
13379751	MesosWorkerStore is started with an illegal namespace	The MesosWorkerStore is started with an illegal namespace because of FLINK-22636.	FLINK	Closed	2	1	10066	pull-request-available
13148212	Failing allocated slots not noticed	"When allocating slots for eager scheduling, it can happen that allocated slots get failed after they are assigned to the {{Execution}} (e.g. due to a {{TaskExecutor}} heartbeat timeout). If there are still some uncompleted slot futures, then this will not be noticed since the {{Execution}} is assigned to the {{LogicalSlot}} only after all slot futures are completed. Therefore, the allocated slot failure will go unnoticed until this happens.

In order to speed up failures, we should directly assign the {{Execution}} to the {{LogicalSlot}} once the slot is assigned to the {{Execution}}."	FLINK	Closed	2	1	10066	flip-6
13382782	RocksDBStateBackendWindowITCase fails with savepoint timeout	"Initially [reported|https://issues.apache.org/jira/browse/FLINK-22067?focusedCommentId=17358306&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17358306] in FLINK-22067

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18709&view=logs&j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&t=46a16c18-c679-5905-432b-9be5d8e27bc6&l=10183

Savepoint is triggered but is not completed in time.


{noformat}
2021-06-06T22:27:46.4845045Z Jun 06 22:27:46 java.lang.RuntimeException: Failed to take savepoint
2021-06-06T22:27:46.4846088Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:71)
2021-06-06T22:27:46.4847049Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:46)
2021-06-06T22:27:46.4848262Z Jun 06 22:27:46 	at org.apache.flink.state.api.SavepointWindowReaderITCase.testApplyEvictorWindowStateReader(SavepointWindowReaderITCase.java:350)
2021-06-06T22:27:46.4854133Z Jun 06 22:27:46 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-06-06T22:27:46.4855430Z Jun 06 22:27:46 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-06-06T22:27:46.4856528Z Jun 06 22:27:46 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-06-06T22:27:46.4857487Z Jun 06 22:27:46 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-06-06T22:27:46.4858685Z Jun 06 22:27:46 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-06-06T22:27:46.4859773Z Jun 06 22:27:46 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-06-06T22:27:46.4860964Z Jun 06 22:27:46 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-06-06T22:27:46.4862306Z Jun 06 22:27:46 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-06-06T22:27:46.4863756Z Jun 06 22:27:46 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-06-06T22:27:46.4864993Z Jun 06 22:27:46 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-06-06T22:27:46.4866179Z Jun 06 22:27:46 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-06-06T22:27:46.4867272Z Jun 06 22:27:46 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-06-06T22:27:46.4868255Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-06-06T22:27:46.4869045Z Jun 06 22:27:46 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-06-06T22:27:46.4869902Z Jun 06 22:27:46 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-06-06T22:27:46.4871038Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-06-06T22:27:46.4871756Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-06-06T22:27:46.4872502Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-06-06T22:27:46.4873389Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-06-06T22:27:46.4874150Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-06-06T22:27:46.4874914Z Jun 06 22:27:46 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-06-06T22:27:46.4875661Z Jun 06 22:27:46 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-06-06T22:27:46.4876382Z Jun 06 22:27:46 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-06-06T22:27:46.4877018Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-06-06T22:27:46.4877661Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-06-06T22:27:46.4878522Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-06-06T22:27:46.4879506Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-06-06T22:27:46.4880246Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-06-06T22:27:46.4881025Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-06-06T22:27:46.4881839Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-06-06T22:27:46.4882650Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-06-06T22:27:46.4883596Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-06-06T22:27:46.4884971Z Jun 06 22:27:46 Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2021-06-06T22:27:46.4886218Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-06-06T22:27:46.4887018Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-06-06T22:27:46.4887787Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:69)
2021-06-06T22:27:46.4888521Z Jun 06 22:27:46 	... 34 more
2021-06-06T22:27:46.4889560Z Jun 06 22:27:46 Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2021-06-06T22:27:46.4890708Z Jun 06 22:27:46 	at com.sun.proxy.$Proxy32.triggerSavepoint(Unknown Source)
2021-06-06T22:27:46.4891470Z Jun 06 22:27:46 	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:716)
2021-06-06T22:27:46.4892292Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-06-06T22:27:46.4893139Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-06-06T22:27:46.4894022Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-06-06T22:27:46.4894810Z Jun 06 22:27:46 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
2021-06-06T22:27:46.4895876Z Jun 06 22:27:46 	at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:714)
2021-06-06T22:27:46.4896736Z Jun 06 22:27:46 	at org.apache.flink.client.program.MiniClusterClient.triggerSavepoint(MiniClusterClient.java:101)
2021-06-06T22:27:46.4897610Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.triggerSavepoint(SavepointTestBase.java:93)
2021-06-06T22:27:46.4898651Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.lambda$takeSavepoint$0(SavepointTestBase.java:68)
2021-06-06T22:27:46.4899492Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
2021-06-06T22:27:46.4900311Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2021-06-06T22:27:46.4901105Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-06-06T22:27:46.4901882Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1646)
2021-06-06T22:27:46.4902703Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2021-06-06T22:27:46.4903544Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-06-06T22:27:46.4904457Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-06-06T22:27:46.4905221Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-06-06T22:27:46.4905948Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-06-06T22:27:46.4908488Z Jun 06 22:27:46 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#1085446192]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2021-06-06T22:27:46.4909806Z Jun 06 22:27:46 	at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:635)
2021-06-06T22:27:46.4910572Z Jun 06 22:27:46 	at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:650)
2021-06-06T22:27:46.4911233Z Jun 06 22:27:46 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2021-06-06T22:27:46.4911980Z Jun 06 22:27:46 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
2021-06-06T22:27:46.4912770Z Jun 06 22:27:46 	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109)
2021-06-06T22:27:46.4913636Z Jun 06 22:27:46 	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
2021-06-06T22:27:46.4914406Z Jun 06 22:27:46 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
2021-06-06T22:27:46.4915259Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2021-06-06T22:27:46.4916164Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2021-06-06T22:27:46.4917078Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:283)
2021-06-06T22:27:46.4917924Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:235)
2021-06-06T22:27:46.4918737Z Jun 06 22:27:46 	at java.lang.Thread.run(Thread.java:748)
{noformat}
"	FLINK	Closed	3	1	10066	auto-deprioritized-critical, pull-request-available, test-stability
13109952	Suppress ActorKilledExceptions	When stopping a {{RpcEndpoint}}, the {{AkkaRpcService}} sends a {{Kill}} message which causes an {{ActorKilledException}} to be thrown. This exception is logged by the {{StoppingSupervisorStrategy}}. This is not necessary because we voluntarily stopped the {{RpcEndpoint}}. In order to clean up logs, I think we should not log this kind of exception.	FLINK	Closed	4	4	10066	flip-6
13105782	test instability in ResourceManagerTest	"{code}
Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.033 sec <<< FAILURE! - in org.apache.flink.runtime.clusterframework.ResourceManagerTest
testHeartbeatTimeoutWithTaskExecutor(org.apache.flink.runtime.clusterframework.ResourceManagerTest)  Time elapsed: 0.716 sec  <<< ERROR!
java.util.concurrent.ExecutionException: org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token mismatch: Ignoring message LocalFencedMessage(null, LocalRpcInvocation(registerTaskExecutor(String, ResourceID, SlotReport, Time))) because the fencing token null did not match the expected fencing token 9320b08d25f4e942b332acbae9464e8b.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at org.apache.flink.runtime.clusterframework.ResourceManagerTest.testHeartbeatTimeoutWithTaskExecutor(ResourceManagerTest.java:541)
Caused by: org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token mismatch: Ignoring message LocalFencedMessage(null, LocalRpcInvocation(registerTaskExecutor(String, ResourceID, SlotReport, Time))) because the fencing token null did not match the expected fencing token 9320b08d25f4e942b332acbae9464e8b.
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleMessage(FencedAkkaRpcActor.java:73)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$onReceive$0(AkkaRpcActor.java:129)
	at akka.actor.ActorCell$$anonfun$become$1.applyOrElse(ActorCell.scala:534)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:97)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

https://travis-ci.org/NicoK/flink/jobs/280858456"	FLINK	Resolved	2	1	10066	test-stability
13359010	DispatcherTest.testInvalidCallDuringInitialization times out on azp	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13388&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0

{code}
2021-02-16T23:46:56.7270697Z [ERROR] testInvalidCallDuringInitialization(org.apache.flink.runtime.dispatcher.DispatcherTest)  Time elapsed: 6.691 s  <<< ERROR!
2021-02-16T23:46:56.7271801Z org.junit.runners.model.TestTimedOutException: test timed out after 5000 milliseconds
2021-02-16T23:46:56.7272538Z 	at sun.misc.Unsafe.park(Native Method)
2021-02-16T23:46:56.7273100Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2021-02-16T23:46:56.7273854Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2021-02-16T23:46:56.7274939Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2021-02-16T23:46:56.7275654Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2021-02-16T23:46:56.7287739Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-02-16T23:46:56.7289295Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest.lambda$testInvalidCallDuringInitialization$1(DispatcherTest.java:438)
2021-02-16T23:46:56.7290531Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest$$Lambda$156/1546789696.get(Unknown Source)
2021-02-16T23:46:56.7291284Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2021-02-16T23:46:56.7292120Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:129)
2021-02-16T23:46:56.7293172Z 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testInvalidCallDuringInitialization(DispatcherTest.java:436)
2021-02-16T23:46:56.7293966Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-16T23:46:56.7294581Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-16T23:46:56.7295338Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-16T23:46:56.7296014Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-16T23:46:56.7296653Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-02-16T23:46:56.7297426Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-02-16T23:46:56.7298200Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-02-16T23:46:56.7298961Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-02-16T23:46:56.7299906Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-02-16T23:46:56.7300910Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-02-16T23:46:56.7301585Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-02-16T23:46:56.7302118Z 	at java.lang.Thread.run(Thread.java:748)
{code}"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13225190	Not able to submit jobs on YARN when there's a firewall	"If there is a firewall around the YARN cluster and the machine, submitting flink job it is unpractical because new flink clusters start up with random ports for REST communication.

 

FLINK-5758 should've fixed this. But it seems FLINK-11081 either undid the changes or did not implement this. The relevant code is changed in FLINK-11081 ([https://github.com/apache/flink/commit/730eed71ef3f718d61f85d5e94b1060844ca56db#diff-487838863ab693af7008f04cb3359be3R102])

 

 "	FLINK	Resolved	1	1	10066	pull-request-available
13139005	Make MetricRegistryImpl#shutdown non blocking 	In order to better shut down multiple components concurrently, we should make all shutdown operation non-blocking if possible. This also includes the {{MetricRegistryImpl}}.	FLINK	Closed	3	4	10066	flip-6
13116982	Create common WebMonitorEndpoint	In order to reuse the existing the REST handlers, we should create a common {{WebMonitorEndpoint}} which is shared by the {{Dispatcher}} and the {{JobMaster}} component.	FLINK	Closed	3	7	10066	flip-6
13138475	Remove slot request timeout from SlotPool	After addressing FLINK-8643, we can further simplify the {{SlotPool}} by replacing the internal slot request timeout by the timeout given to {{SlotPool#allocateSlot}}. Since this request will timeout on the {{ProviderAndOwner}} side anyway, we should do the same on the {{SlotPool}} side.	FLINK	Closed	3	4	10066	flip-6
13255889	SavepointMigrationTestBase is super slow	"The subclasses of {{SavepointMigrationTestBase}} take super long to execute. On my local machine

* {{TypeSerializerSnapshotMigrationITCase}} takes 2min 30s
* {{StatefulJobWBroadcastStateMigrationITCase}} takes 1min 45s
* {{StatefulJobSavepointMigrationITCase}} takes 2min 5s

to execute. The reasons for the long runtimes seem to be that we are using the {{AccumulatorCountingSink}} which uses the accumulators to signal when a job is done. Since the accumulators are being sent with the TM heartbeats, the heartbeat interval how fast the client realizes that the job can be shut down. The default heartbeat interval is {{10 s}} and hence it takes always at least 10 seconds until the client stops the job.

I suggest to decrease the heartbeat interval in the {{SavepointMigrationTestBase}} to 300ms in order to speed up the tests. On my machine the test runtimes with this settings are:

* {{TypeSerializerSnapshotMigrationITCase}} takes 13s
* {{StatefulJobWBroadcastStateMigrationITCase}} takes 10s
* {{StatefulJobSavepointMigrationITCase}} takes 11s
"	FLINK	Closed	3	1	10066	pull-request-available
13196125	connection leak when partition discovery is disabled and open throws exception	"Here is the scenario to reproduce the issue
 * partition discovery is disabled
 * open method throws an exception (e.g. when broker SSL authorization denies request)

In this scenario, run method won't be executed. As a result, _partitionDiscoverer.close()_ won't be called. that caused the connection leak, because KafkaConsumer is initialized but not closed. That has caused outage that brought down our Kafka cluster, when a high-parallelism job got into a restart/failure loop."	FLINK	Resolved	3	1	10066	pull-request-available
13379458	Breaking HighAvailabilityServices interface by adding new method	As part of FLINK-20695 we introduced a new method to the {{HighAvailabilityServices.cleanupJobData}} interface. Since this method has not default implementation it is currently breaking change. Since Flink allows to implement custom Ha services using this interface, I suggest adding a default implementation for this method.	FLINK	Closed	2	1	10066	pull-request-available
13425117	KubernetesHighAvailabilityRecoverFromSavepointITCase. testRecoverFromSavepoint failed on azure	"
{code:java}
2022-01-27T06:08:57.7214748Z Jan 27 06:08:57 [INFO] Running org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase
2022-01-27T06:10:23.2568324Z Jan 27 06:10:23 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 85.553 s <<< FAILURE! - in org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase
2022-01-27T06:10:23.2572289Z Jan 27 06:10:23 [ERROR] org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint  Time elapsed: 84.078 s  <<< ERROR!
2022-01-27T06:10:23.2573945Z Jan 27 06:10:23 java.util.concurrent.TimeoutException
2022-01-27T06:10:23.2574625Z Jan 27 06:10:23 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
2022-01-27T06:10:23.2575381Z Jan 27 06:10:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-01-27T06:10:23.2576428Z Jan 27 06:10:23 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:104)
2022-01-27T06:10:23.2578437Z Jan 27 06:10:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-27T06:10:23.2579141Z Jan 27 06:10:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-27T06:10:23.2579893Z Jan 27 06:10:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-27T06:10:23.2594686Z Jan 27 06:10:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-27T06:10:23.2595622Z Jan 27 06:10:23 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-27T06:10:23.2596397Z Jan 27 06:10:23 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-27T06:10:23.2597158Z Jan 27 06:10:23 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-27T06:10:23.2597900Z Jan 27 06:10:23 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-27T06:10:23.2598630Z Jan 27 06:10:23 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-27T06:10:23.2599335Z Jan 27 06:10:23 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-27T06:10:23.2600044Z Jan 27 06:10:23 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-27T06:10:23.2600736Z Jan 27 06:10:23 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-27T06:10:23.2601408Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-27T06:10:23.2602124Z Jan 27 06:10:23 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-27T06:10:23.2602831Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-27T06:10:23.2603531Z Jan 27 06:10:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-27T06:10:23.2604270Z Jan 27 06:10:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-27T06:10:23.2604975Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-27T06:10:23.2605641Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-27T06:10:23.2606313Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-27T06:10:23.2607713Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-27T06:10:23.2608497Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-27T06:10:23.2609049Z Jan 27 06:10:23 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-27T06:10:23.2609623Z Jan 27 06:10:23 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-27T06:10:23.2610165Z Jan 27 06:10:23 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-27T06:10:23.2610700Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-27T06:10:23.2611621Z Jan 27 06:10:23 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-27T06:10:23.2612145Z Jan 27 06:10:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-27T06:10:23.2612644Z Jan 27 06:10:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-27T06:10:23.2613196Z Jan 27 06:10:23 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-27T06:10:23.2613967Z Jan 27 06:10:23 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-27T06:10:23.2614595Z Jan 27 06:10:23 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-27T06:10:23.2615259Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-27T06:10:23.2615978Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-27T06:10:23.2616717Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-27T06:10:23.2617485Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-27T06:10:23.2618235Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-27T06:10:23.2618907Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-27T06:10:23.2619515Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-27T06:10:23.2620205Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-27T06:10:23.2620927Z Jan 27 06:10:23 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-27T06:10:23.2621627Z Jan 27 06:10:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-27T06:10:23.2622339Z Jan 27 06:10:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-27T06:10:23.2623439Z Jan 27 06:10:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2022-01-27T06:10:23.2624147Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-27T06:10:23.2624773Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-27T06:10:23.2625353Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-27T06:10:23.2625943Z Jan 27 06:10:23 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-01-27T06:10:23.2626383Z Jan 27 06:10:23 
2022-01-27T06:10:23.6317559Z Jan 27 06:10:23 [INFO] 
2022-01-27T06:10:23.6318426Z Jan 27 06:10:23 [INFO] Results:
2022-01-27T06:10:23.6318913Z Jan 27 06:10:23 [INFO] 
2022-01-27T06:10:23.6320466Z Jan 27 06:10:23 [ERROR] Errors: 
2022-01-27T06:10:23.6321907Z Jan 27 06:10:23 [ERROR]   KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint:104 » Timeout
2022-01-27T06:10:23.6322588Z Jan 27 06:10:23 [INFO] 
2022-01-27T06:10:23.6323353Z Jan 27 06:10:23 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30266&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5230"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13259527	Introduce DispatcherRunner#getShutDownFuture	I suggest to extend the {{DispatcherRunner}} interface to return a shut down future. The idea of the shut down future is that it gets completed once the runner intends to be shut down by its owner.	FLINK	Closed	4	4	10066	pull-request-available
13254104	YarnPrioritySchedulingITCase fails on hadoop 2.4.1	"The {{YarnPrioritySchedulingITCase}} does an early exit in BeforeClass if run against a hadoop version lower than 2.8 . The AfterClass method in the YarnTestBase however cannot handle this case and fails with an NPE.
{code}
22:33:21.941 [ERROR] org.apache.flink.yarn.YarnPrioritySchedulingITCase.org.apache.flink.yarn.YarnPrioritySchedulingITCase
22:33:21.942 [ERROR]   Run 1: YarnPrioritySchedulingITCase.setup:40 Â» AssumptionViolated Priority scheduling...
22:33:21.943 [ERROR]   Run 2: YarnPrioritySchedulingITCase>YarnTestBase.teardown:956 Â» NullPointer
{code}

https://travis-ci.org/apache/flink/jobs/579264625
"	FLINK	Resolved	2	1	10066	test-stability
13396341	ZooKeeperLeaderRetrievalConnectionHandlingTest.testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22534&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=6647

{code}
Aug 20 07:54:25 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 14.652 s <<< FAILURE! - in org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest
Aug 20 07:54:25 [ERROR] testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection  Time elapsed: 2.244 s  <<< FAILURE!
Aug 20 07:54:25 java.lang.AssertionError: 
Aug 20 07:54:25 
Aug 20 07:54:25 Expected: is null
Aug 20 07:54:25      but: was <java.util.concurrent.CompletableFuture@2d6764b2[Completed normally]>
Aug 20 07:54:25 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Aug 20 07:54:25 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Aug 20 07:54:25 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest.testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:211)
Aug 20 07:54:25 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 20 07:54:25 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 20 07:54:25 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 20 07:54:25 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 20 07:54:25 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 20 07:54:25 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 20 07:54:25 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.evaluate(TestingFatalErrorHandlerResource.java:91)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.access$200(TestingFatalErrorHandlerResource.java:83)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:55)
Aug 20 07:54:25 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
{code}"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13195631	CoGroupConnectedComponentsITCase deadlocked on Travis	"The {{CoGroupConnectedComponentsITCase}} deadlocks on Travis: https://api.travis-ci.org/v3/job/449230962/log.txt.

It looks as if the iteration gets stuck and waits {{SuperstepKickoffLatch.awaitStartOfSuperstepOrTermination}} without ever getting a proper notification. This might indicate a serious bug in our iteration implementation. This could also be related to FLINK-10741."	FLINK	Closed	1	1	10066	pull-request-available, test-stability
13139093	Make ClusterEntrypoint shut down non-blocking	Make the {{ClusterEntrypoint}} shut down method non blocking. That way we don't have to use the common Fork-Join-Pool to shutDownAndTerminate the cluster entrypoint when the Dispatcher terminates.	FLINK	Closed	3	4	10066	flip-6
13137173	Add MiniDispatcher for job mode	In order to properly support the job mode, we need a {{MiniDispatcher}} which is started with a pre initialized {{JobGraph}} and launches a single {{JobManagerRunner}} with this job. Once the job is completed and if the {{MiniDispatcher}} is running in detached mode, the {{MiniDispatcher}} should terminate.	FLINK	Closed	3	2	10066	flip-6
13259516	Pass in ioExecutor into AbstractDispatcherResourceManagerComponentFactory	The {{ClusterEntrypoint}} already starts an {{ioExecutor}}. However, this executor is not being used by the {{DispatcherResourceManagerComponent}} because it is not given to the {{DispatcherResourceManagerComponentFactory}}. In order to prepare the component to use the {{ioExecutor}}, I suggest to pass the {{ioExecutor}} into the {{DispatcherResourceManagerComponentFactory}}.	FLINK	Closed	4	4	10066	pull-request-available
13178303	Use ExecutorThreadFactory instead of DefaultThreadFactory in RestServer/Client	Instead of using the {{DefaultThreadFactory}} in the {{RestServerEndpoint}}/{{RestClient}} we should use the {{ExecutorThreadFactory}} because it uses the {{FatalExitExceptionHandler}} per default as the uncaught exception handler. This should guard against uncaught exceptions by simply terminating the JVM.	FLINK	Closed	3	4	10066	pull-request-available
13259895	Extract JobGraphWriter from JobGraphStore	In order to follow the ISP, I suggest to extract a {{JobGraphWriter}} interface from the {{JobGraphStore}}. The background is that in the future, the {{Dispatcher}} does not need to recover jobs anymore and, hence, it should not know the recovery related methods of the {{JobGraphStore}}. Instead, the only methods it needs to know are {{putJobGraph}}, {{removeJobGraph}} and {{releaseJobGraph}}.	FLINK	Closed	4	4	10066	pull-request-available
13528327	Support copying a FileStoreTable with latest schema	To capture schema changes, CDC sinks of Flink Table Store should be able to use the latest schema at any time. This requires us to copy a \{{FileStoreTable}} with latest schema so that we can create \{{TableWrite}} with latest schema.	FLINK	Closed	3	7	10239	pull-request-available
13431423	Allow ManifestFile to write a list of ManifestFileMeta into multiple files if too many.	Currently {{ManifestFile#write}} will only produce a single file no matter how large the given {{ManifestFileMeta}} list is. However batch jobs are likely to produce a lot of changes hence the list might be very long. We need to allow {{ManifestFile#write}} to produce multiple files for effective merging and filtering.	FLINK	Closed	3	7	10239	pull-request-available
13437402	Use '<format>.<option>' instead of 'file.<format>.<option>' in table properties of managed table	Currently if we want to set compression method of file store we need to use 'file.orc.compress'. It would be better to use 'orc.compress' directly, just like what we do for filesystem connectors.	FLINK	Closed	3	7	10239	pull-request-available
13481412	Introduce a MergeFunction for full compaction	We need to introduce a special {{MergeFunction}} to produce changelogs.	FLINK	Closed	3	7	10239	pull-request-available
13438057	Reuse FileFormat instead of DecodingFormat/EncodingFormat to ensure thread safety.	"When testing table store I'm faced with the following warning messages:
{code}
org.apache.org.impl.MemoryManagerImpl [] - Owner thread expected Thread[Writer -> Local Committer (1/16)#0,5,Flink Task Threads], got Thread[pool-8-thread-1,5,Flink Task Threads]
{code}

This is because {{OrcBulkWriterFactory}} is not thread safe (it keeps {{WriterOptions}} as a class member and creates writers from the same {{WriterOptions}} object, which is not thread safe).

What we should do is to reuse {{FileFormat}} instead of {{DecodingFormat}}/{{EncodingFormat}} to ensure thread safety."	FLINK	Closed	3	7	10239	pull-request-available
13486027	Extract changelog files out of DataFileMeta#extraFiles	"Currently changelog files are stored as extra files in {{DataFileMeta}}. However for the full compaction changelog we're about to introduce, it cannot be added as extra files because their statistics might be different from the corresponding merge tree files.

We need to extract changelog files out of DataFileMeta#extraFiles."	FLINK	Closed	3	7	10239	pull-request-available
13424812	TableITCase.testCollectWithClose failed on azure	"
{code:java}
2022-01-25T08:35:25.3735884Z Jan 25 08:35:25 [ERROR] TableITCase.testCollectWithClose  Time elapsed: 0.377 s  <<< FAILURE!
2022-01-25T08:35:25.3737127Z Jan 25 08:35:25 java.lang.AssertionError: Values should be different. Actual: RUNNING
2022-01-25T08:35:25.3738167Z Jan 25 08:35:25 	at org.junit.Assert.fail(Assert.java:89)
2022-01-25T08:35:25.3739085Z Jan 25 08:35:25 	at org.junit.Assert.failEquals(Assert.java:187)
2022-01-25T08:35:25.3739922Z Jan 25 08:35:25 	at org.junit.Assert.assertNotEquals(Assert.java:163)
2022-01-25T08:35:25.3740846Z Jan 25 08:35:25 	at org.junit.Assert.assertNotEquals(Assert.java:177)
2022-01-25T08:35:25.3742302Z Jan 25 08:35:25 	at org.apache.flink.table.api.TableITCase.testCollectWithClose(TableITCase.scala:135)
2022-01-25T08:35:25.3743327Z Jan 25 08:35:25 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-25T08:35:25.3744343Z Jan 25 08:35:25 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-25T08:35:25.3745575Z Jan 25 08:35:25 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-25T08:35:25.3746840Z Jan 25 08:35:25 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-25T08:35:25.3747922Z Jan 25 08:35:25 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-25T08:35:25.3749151Z Jan 25 08:35:25 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-25T08:35:25.3750422Z Jan 25 08:35:25 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-25T08:35:25.3751820Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-25T08:35:25.3753196Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-25T08:35:25.3754253Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-25T08:35:25.3755441Z Jan 25 08:35:25 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
2022-01-25T08:35:25.3756656Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3757778Z Jan 25 08:35:25 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-25T08:35:25.3758821Z Jan 25 08:35:25 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-25T08:35:25.3759840Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-25T08:35:25.3760919Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-25T08:35:25.3762249Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-25T08:35:25.3763322Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-25T08:35:25.3764436Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-25T08:35:25.3765907Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-25T08:35:25.3766957Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-25T08:35:25.3768104Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-25T08:35:25.3769128Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-25T08:35:25.3770125Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-25T08:35:25.3771118Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-25T08:35:25.3772264Z Jan 25 08:35:25 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-01-25T08:35:25.3773118Z Jan 25 08:35:25 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-01-25T08:35:25.3774092Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-25T08:35:25.3775056Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-25T08:35:25.3776144Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-25T08:35:25.3777125Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-25T08:35:25.3778190Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-25T08:35:25.3779234Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3780354Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3781583Z Jan 25 08:35:25 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-25T08:35:25.3782721Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-25T08:35:25.3783724Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-25T08:35:25.3784663Z Jan 25 08:35:25 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-25T08:35:25.3785542Z Jan 25 08:35:25 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-25T08:35:25.3786641Z Jan 25 08:35:25 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-25T08:35:25.3787854Z Jan 25 08:35:25 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-25T08:35:25.3789028Z Jan 25 08:35:25 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-25T08:35:25.3790347Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-25T08:35:25.3791934Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-25T08:35:25.3793503Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-25T08:35:25.3794936Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-25T08:35:25.3796384Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-25T08:35:25.3797588Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-25T08:35:25.3798765Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-25T08:35:25.3799989Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-25T08:35:25.3801441Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-25T08:35:25.3802916Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-01-25T08:35:25.3804358Z Jan 25 08:35:25 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-01-25T08:35:25.3805461Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-01-25T08:35:25.3806909Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-25T08:35:25.3808260Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-01-25T08:35:25.3809487Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-25T08:35:25.3810614Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-25T08:35:25.3811838Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-25T08:35:25.3813068Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-01-25T08:35:25.3813890Z Jan 25 08:35:25 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30101&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9693"	FLINK	Closed	2	1	10239	pull-request-available, stale-assigned, test-stability
13449759	Refactor Flink connector for table store with FileStoreTable	We've prepared {{FileStoreTable}} for {{RowData}} reading and writing, so it's time to refactor Flink connector for table store with {{FileStoreTable}}.	FLINK	Closed	3	7	10239	pull-request-available
13473986	Add Tez execution engine test for Table Store Hive connector	Table Store Hive connector is supposed to support both MR and Tez engine. However current tests only runs on MR. We need to add Tez tests.	FLINK	Closed	3	4	10239	pull-request-available
13505880	Allow FileStoreScan to read incremental changes from OVERWRITE snapshot in Table Store	Currently {{AbstractFileStoreScan}} can only read incremental changes from APPEND snapshots. However in OVERWRITE snapshots, users will also append new records to table. These changes must be discovered by compact job source so that the overwritten partition can be compacted.	FLINK	Closed	3	7	10239	pull-request-available
13370364	PushFilterIntoTableSourceScanRule fails to deal with IN expressions	"Add the following test case to {{PushFilterIntoLegacyTableSourceScanRuleTest}} to reproduce this bug. {{PushFilterIntoTableSourceScanRuleTest}} extends this class and will also be tested.

{code:scala}
@Test
def myTest(): Unit = {
  util.verifyRelPlan(""SELECT * FROM MyTable WHERE name IN ('Alice', 'Bob', 'Dave')"")
}
{code}

The exception stack is
{code}
java.lang.AssertionError: OR(OR(=($0, _UTF-16LE'Alice':VARCHAR(5) CHARACTER SET ""UTF-16LE""), =($0, _UTF-16LE'Bob':VARCHAR(5) CHARACTER SET ""UTF-16LE"")), =($0, _UTF-16LE'Dave':VARCHAR(5) CHARACTER SET ""UTF-16LE""))

	at org.apache.calcite.rel.core.Filter.<init>(Filter.java:76)
	at org.apache.calcite.rel.logical.LogicalFilter.<init>(LogicalFilter.java:68)
	at org.apache.calcite.rel.logical.LogicalFilter.copy(LogicalFilter.java:126)
	at org.apache.calcite.rel.logical.LogicalFilter.copy(LogicalFilter.java:45)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.pushFilterIntoScan(PushFilterIntoLegacyTableSourceScanRule.scala:130)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.onMatch(PushFilterIntoLegacyTableSourceScanRule.scala:77)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:281)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlan(TableTestBase.scala:400)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.myTest(PushFilterIntoLegacyTableSourceScanRuleTest.scala:76)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}

This is because {{Filter}} in calcite requires its {{condition}} to be ""flat"", so we should first simplify the conditions before constructing a filter."	FLINK	Closed	3	1	10239	pull-request-available
13432794	Apply key & value filter in FileStoreScanImpl	Now that we've implemented sst file statistics in FLINK-26346, we can apply key & value filter in {{FileStoreScanImpl}}.	FLINK	Closed	3	7	10239	pull-request-available
13450194	Check Hive DDL against table store schema when creating table	As table store schema is supported, we should use this schema as the ground truth. Hive DDL should only be used for checking.	FLINK	Closed	3	7	10239	pull-request-available
13517439	Table Store Hive catalog supports specifying custom Hive metastore client	Currently Hive metastore client class is hard coded in {{HiveCatalog}}, however users may want to specify custom Hive metastore client to read and write Hive compliant storage.	FLINK	Closed	3	4	10239	pull-request-available
13450116	Optimize PredicateBuilder.in for lots of parameters	"The current treatment of in is to expand into multiple equals and join them using or.
But this does not perform well if there are particularly many parameters, we need to introduce In's Predicate to handle this case separately."	FLINK	Closed	3	4	10239	pull-request-available
13528326	Support migrating states between different instances of TableWriteImpl and AbstractFileStoreWrite	"Currently {{Table}} and {{TableWrite}} in Flink Table Store have a fixed schema. However to consume schema changes, Flink Table Store CDC sinks should have the ability to change its schema during a streaming job.

This require us to pause and store the states of a {{TableWrite}}, then create a {{TableWrite}} with newer schema and recover the states in the new {{TableWrite}}."	FLINK	Closed	3	7	10239	pull-request-available
13409604	Batch SQL file sink forgets to close the output stream	"I tried to write a large avro file into HDFS and discover that the displayed file size in HDFS is extremely small, but copying that file to local yields the correct size. If we create another Flink job and read that avro file from HDFS, the job will finish without outputting any record because the file size Flink gets from HDFS is the very small file size.

This is because the output format created in {{FileSystemTableSink#createBulkWriterOutputFormat}} only finishes the {{BulkWriter}}. According to the java doc of {{BulkWriter#finish}} bulk writers should not close the output stream and should leave them to the framework."	FLINK	Closed	3	1	10239	pull-request-available
13492154	PreAggregationITCase.LastValueAggregation and PreAggregationITCase.LastNonNullValueAggregation are unstable	{{PreAggregationITCase.LastValueAggregation}} and {{PreAggregationITCase.LastNonNullValueAggregation}} need to make sure that the order of input data is determined. However the default parallelism of {{FileStoreTableITCase}} is 2, so the order of input data might change across tests.	FLINK	Closed	3	1	10239	pull-request-available
13493693	"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow"	"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow.

This is due to a bug in {{FileStoreCommitImpl#filterCommitted}}. When this method finds an identifier, it removes the identifier from a map. However different snapshots may have the same identifier (for example an APPEND commit and the following COMPACT commit will have the same identifier), so we need to use another set to check for identifiers.

When snapshot committing is fast there is at most 1 identifier to check after the job restarts, so nothing happens. However when snapshot committing is slow, there will be multiple identifiers to check and some identifiers will be mistakenly kept."	FLINK	Closed	3	1	10239	pull-request-available
13438557	FileStoreCommitImpl#filterCommitted should exit when faced with expired files	Currently it is possible for {{FileStoreCommitImpl#filterCommitted}} to check an expired snapshot, which throws FileNotExists exception. When faced with an expired file {{FileStoreCommitImpl#filterCommitted}} should exit.	FLINK	Closed	3	7	10239	pull-request-available
13495169	Explicitly throw exception from Table Store sink when unaligned checkpoint is enabled	Currently table store sink does not support unaligned checkpoint but no exception is explicitly thrown. We should throw exception so that users can change their configurations.	FLINK	Closed	3	4	10239	pull-request-available
13482491	Avoid manifest corruption for incorrect checkpoint recovery	"When the job runs to checkpoint N, if the user recovers from an old checkpoint (such as checkpoint N-5), the sink of the current FTS will cause a manifest corruption because duplicate files may be committed.

We should avoid such corruption, and the storage should be robust enough."	FLINK	Closed	1	1	10239	pull-request-available
13385745	Clean up the old implementation of code splitting	In the third step we clean up the old implementation of code splitting.	FLINK	Closed	3	7	10239	pull-request-available
13389284	JavaCodeSplitter can not split the code from ProjectionCodeGenerator	"- JavaCodeSplitter can not split the method which has return value. We should add comments in JavaCodeSplitter.

- ProjectionCodeGenerator need has a method without return value."	FLINK	Closed	3	7	10239	pull-request-available
13429298	Introduce a better expire strategy	We can add the snapshot id in which the file/manifest was added to the table. With this snapshot id, we can have better expire strategy. Instead of scanning all files of the snapshot.	FLINK	Closed	3	7	10239	pull-request-available
13398489	"""Internal server error"" when quitting a SELECT query in the SqlClient"	"Release testing revealed an Internal server error being reported when quitting a SELECT statement in the SQL Client (see details on how to reproduce it in the [comment of FLINK-23850|https://issues.apache.org/jira/browse/FLINK-23850?focusedCommentId=17407235&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17407235]):
{code}
2021-08-31 12:34:37,091 WARN  org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (3df388d05273cc9782458228081ea5bf)
        at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:909)
        at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:923)
        at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:719)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

End of exception on server side>]
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_265]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:163) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:128) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370) [flink-table_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:74) [flink-sql-client_2.12-1.14.0.jar:1.14.0]
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (3df388d05273cc9782458228081ea5bf)
        at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:909)
        at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:923)
        at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:719)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

End of exception on server side>]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:532) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:512) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_265]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_265]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_265]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
{code}"	FLINK	Closed	3	1	10239	pull-request-available
13515681	Add documentation for writing Table Store with Spark3	Table Store 0.3 supports writing with Spark3. We need to add documentation.	FLINK	Closed	3	4	10239	pull-request-available
13528340	Remove schemaId from constructor of FileStoreCommitImpl and ManifestFile in Table Store	As schema may change during a CTAS streaming job, the schema ID of snapshots and manifest files may also change. We should remove \{{schemaId}} from their constructor and calculate the real \{{schemaId}} on the fly.	FLINK	Closed	3	7	10239	pull-request-available
13383835	NullPointerException when cast string literal to date or time	"sql:
{code:java}
CREATE TABLE source_table
(
    id               INT,
    score            INT,
    address          STRING,
    create_time      TIME,
    create_date      DATE,
    create_timestamp TIMESTAMP
) WITH (
      'connector' = 'datagen'
      );

CREATE TABLE console_table
(
    create_time      TIME,
    create_date      DATE,
    create_timestamp TIMESTAMP
) WITH (
      'connector' = 'print'
      );

INSERT INTO console_table
SELECT CASE
           WHEN A.create_time IS NULL
               OR A.create_time = '' THEN CURRENT_TIME
           ELSE A.create_time
           END
           AS create_time,
       CASE
           WHEN A.create_date IS NULL
               OR A.create_date = '' THEN CURRENT_DATE
           ELSE A.create_date
           END
           AS create_date,
       CASE
           WHEN A.create_timestamp IS NULL
               OR A.create_timestamp = '' THEN CURRENT_TIMESTAMP
           ELSE A.create_timestamp
           END
           AS create_timestamp
FROM source_table A;
{code}
exception:
{code:java}
java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23'java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:652) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:626) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:566) at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:181) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:548) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:537) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64) ... 12 moreCaused by: java.lang.NullPointerException at StreamExecCalc$23.<init>(Unknown Source) ... 16 more
{code}"	FLINK	Closed	3	1	10239	pull-request-available
13244889	Add tests for LongHashPartition, StringCallGen and EqualiserCodeGenerator	Add tests for LongHashPartition, StringCallGen and EqualiserCodeGenerator.	FLINK	Reopened	10200	11500	10239	pull-request-available, stale-assigned
13268491	Add TableResult#collect api for fetching data to client	"Currently, it is very unconvinient for user to fetch data of flink job unless specify sink expclitly and then fetch data from this sink via its api (e.g. write to hdfs sink, then read data from hdfs). However, most of time user just want to get the data and do whatever processing he want. So it is very necessary for flink to provide api Table#collect for this purpose. 

 

Other apis such as Table#head, Table#print is also helpful.  

 "	FLINK	Resolved	3	2	10239	pull-request-available, usability
13494210	Old record may overwrite new record in Table Store when snapshot committing is slow	"Consider the following scenario when snapshot committing is slow:
* A writer produces some records at checkpoint T.
* It produces no record at checkpoint T+1 and is closed.
* It produces some records at checkpoint T+2. It will be reopened and read the latest sequence number from disk. However snapshot at checkpoint T may not be committed so the sequence number it reads might be too small.

In this scenario, records from checkpoint T may overwrite records from checkpoint T+2 because they have larger sequence numbers."	FLINK	Closed	3	1	10239	pull-request-available
13338948	MultipleInputNodeCreationProcessor#isChainableSource should consider DataStreamScanProvider	{{MultipleInputNodeCreationProcessor#isChainableSource}} now only considers {{SourceProvider}}. However {{DataStreamScanProvider}} providing {{DataStream}} with {{SourceTransformation}} are also chainable sources, and we need to take them into consideration.	FLINK	Closed	3	1	10239	pull-request-available
13406713	Comparing timstamp_ltz with random string throws NullPointerException	"Add the following test case to {{org.apache.flink.table.planner.runtime.batch.sql.CalcITCase}} to reproduce this issue.
{code:scala}
@Test
def myTest(): Unit = {
  val data: Seq[Row] = Seq(row(
    LocalDateTime.of(2021, 10, 15, 0, 0, 0).toInstant(ZoneOffset.UTC)))
  val dataId = TestValuesTableFactory.registerData(data)
  val ddl =
    s""""""
       |CREATE TABLE MyTable (
       |  ltz TIMESTAMP_LTZ
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$dataId',
       |  'bounded' = 'true'
       |)
       |"""""".stripMargin
  tEnv.executeSql(ddl)

  checkResult(""SELECT ltz = uuid() FROM MyTable"", Seq(row(null)))
}
{code}

The exception stack is
{code}
java.lang.RuntimeException: Failed to fetch next result

	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:109)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest(CalcITCase.scala:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 41 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
	... 43 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:614)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1983)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
	... 43 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:678)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.lang.NullPointerException
	at BatchExecCalc$7.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:330)
{code}

This is because {{ScalarOperatorGens#generateCast}} will generate the following code when casting strings to timestamp_ltz.
{code:java}
result$6 = org.apache.flink.table.data.TimestampData.fromEpochMillis(org.apache.flink.table.runtime.functions.SqlDateTimeUtils.toTimestamp(result$3.toString(), timeZone));
{code}

{{org.apache.flink.table.runtime.functions.SqlDateTimeUtils.toTimestamp}} might returns {{null}} while {{org.apache.flink.table.data.TimestampData.fromEpochMillis}} only accepts primitive long values, thus causing this issue.

What we need to do is to check the result of {{toTimestamp}}."	FLINK	Closed	3	1	10239	pull-request-available
13368505	PushFilterIntoLegacyTableSourceScanRule fails to deal with INTERVAL types	"Add the following test case to {{PushFilterIntoLegacyTableSourceScanRuleTest}} to reproduce this bug:

{code:scala}
@Test
  def testWithInterval(): Unit = {
    val schema = TableSchema
      .builder()
      .field(""a"", DataTypes.STRING)
      .field(""b"", DataTypes.STRING)
      .build()

    val data = List(Row.of(""2021-03-30 10:00:00"", ""2021-03-30 11:00:00""))
    TestLegacyFilterableTableSource.createTemporaryTable(
      util.tableEnv,
      schema,
      ""MTable"",
      isBounded = true,
      data,
      List(""a"", ""b""))

    util.verifyRelPlan(
      """"""
        |SELECT * FROM MTable
        |WHERE
        |  TIMESTAMPADD(HOUR, 1, TO_TIMESTAMP(a)) >= TO_TIMESTAMP(b)
        |  OR
        |  TIMESTAMPADD(YEAR, 1, TO_TIMESTAMP(b)) >= TO_TIMESTAMP(a)
        |"""""".stripMargin)
  }
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: Data type 'INTERVAL SECOND(3) NOT NULL' with conversion class 'java.time.Duration' does not support a value literal of class 'java.math.BigDecimal'.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:286)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:79)
	at org.apache.flink.table.expressions.ApiExpressionUtils.valueLiteral(ApiExpressionUtils.java:251)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:451)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1173)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:459)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:458)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:359)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:136)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:135)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:135)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:101)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.pushFilterIntoScan(PushFilterIntoLegacyTableSourceScanRule.scala:90)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRule.onMatch(PushFilterIntoLegacyTableSourceScanRule.scala:77)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:81)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:304)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:927)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:797)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlan(TableTestBase.scala:402)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoLegacyTableSourceScanRuleTest.testWithInterval(PushFilterIntoLegacyTableSourceScanRuleTest.scala:188)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}

This is because {{RexNodeExtractor#visitLiteral}} does not deal with interval types."	FLINK	Closed	3	1	10239	pull-request-available
13471993	LSM tree structure may be incorrect when multiple jobs are committing into the same bucket	"Currently `FileStoreCommitImpl` only checks for conflicts by checking the files we're going to delete (due to compaction) are still there.

However, consider two jobs committing into the same LSM tree at the same time. For their first compaction no conflict is detected because they'll only delete their own level 0 files. But they will both produce level 1 files and the key ranges of these files may overlap. This is not correct for our LSM tree structure."	FLINK	Closed	1	1	10239	pull-request-available
13509819	Change table property key 'log.scan' to 'startup.mode' and add a default startup mode in Table Store	"We're introducing time-travel reading of Table Store for batch jobs. However this reading mode is quite similar to the ""from-timestamp"" startup mode for streaming jobs, just that ""from-timestamp"" streaming jobs only consume incremental data but not history data.

We can support startup mode for both batch and streaming jobs. For batch jobs, ""from-timestamp"" startup mode will produce all records from the last snapshot before the specified timestamp. For streaming jobs the behavior doesn't change.

Previously, in order to use ""from-timestamp"" startup mode, users will have to specify ""log.scan"" and also ""log.scan.timestamp-millis"", which is a little inconvenient. We can introduce a ""default"" startup mode and its behavior will base on the execution environment and other configurations. In this way, to use ""from-timestamp"" startup mode, it is enough for users to specify just ""startup.timestamp-millis""."	FLINK	Closed	3	4	10239	pull-request-available
13296755	Improve handling of unexpected input in config.sh#extractExecutionParams	"In FLINK-15727 {{BashJavaUtils}} now returns multiple lines of results to avoid using {{BashJavaUtils}} twice. But now the format checking in {{extractExecutionParams}} for the last line is incorrect. Instead of
{code:bash}
if ! [[ $execution_config =~ ^${EXECUTION_PREFIX}.* ]]; then
    echo ""[ERROR] Unexpected result: $execution_config"" 1>&2
    echo ""[ERROR] The last line of the BashJavaUtils outputs is expected to be the execution result, following the prefix '${EXECUTION_PREFIX}'"" 1>&2
    echo ""$output"" 1>&2
    exit 1
fi
{code}
It should be
{code:bash}
last_line=`echo ""$execution_config"" | tail -n 1`
if ! [[ ""$last_line"" =~ ^${EXECUTION_PREFIX}.* ]]; then
# ...
{code}"	FLINK	Closed	3	4	10239	pull-request-available
13522413	Table Store Hive Catalog throws java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList under certain environment	Table Store Hive Catalog throws {{java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList}} under certain environment. We need to package {{hive-storage-api}} dependency.	FLINK	Closed	3	1	10239	pull-request-available
13337822	Apply JoinDeriveNullFilterRule after join reorder	{{JoinDeriveNullFilterRule}} will filter out null join keys to prevent data skew for joins. Currently this rule is only applied before join reorder. After join reorder the join keys for some inputs might change and we need to apply this rule again.	FLINK	Closed	3	4	10239	pull-request-available
13318621	BytesHashMap#growAndRehash should release newly allocated segments before throwing the exception	"In {{BytesHashMap#growAndRehash}} we have the following code.

{code:java}
List<MemorySegment> newBucketSegments = new ArrayList<>(required);

try {
	int numAllocatedSegments = required - memoryPool.freePages();
	if (numAllocatedSegments > 0) {
		throw new MemoryAllocationException();
	}
	int needNumFromFreeSegments = required - newBucketSegments.size();
	for (int end = needNumFromFreeSegments; end > 0; end--) {
		newBucketSegments.add(memoryPool.nextSegment());
	}

	setBucketVariables(newBucketSegments);
} catch (MemoryAllocationException e) {
	LOG.warn(""BytesHashMap can't allocate {} pages, and now used {} pages"",
			required, reservedNumBuffers);
	throw new EOFException();
}
{code}

Newly allocated memory segments are temporarily stored in {{newBucketSegments}} before giving to the hash table. But if a {{MemoryAllocationException}} happens, these segments are not returned to the memory pool, causing the following exception stack trace.

{code}
java.lang.RuntimeException: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
        at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:84) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.growAndRehash(BytesHashMap.java:393) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.append(BytesHashMap.java:313) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at HashAggregateWithKeys$360.processElement(Unknown Source) ~[?:?]
        at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:161) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:178) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:153) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:345) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:191) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:560) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:834) [?:1.8.0_102]
        Suppressed: java.lang.RuntimeException: Should return all used memory before clean, page used: 2814
                at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.close(LazyMemorySegmentPool.java:99) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.free(BytesHashMap.java:486) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.table.runtime.operators.aggregate.BytesHashMap.free(BytesHashMap.java:475) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at HashAggregateWithKeys$360.close(Unknown Source) ~[?:?]
                at org.apache.flink.table.runtime.operators.TableStreamOperator.dispose(TableStreamOperator.java:44) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:707) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:687) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:626) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:542) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
                at java.lang.Thread.run(Thread.java:834) [?:1.8.0_102]
Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 512 pages
        at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:229) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        ... 15 more
Caused by: org.apache.flink.runtime.memory.MemoryReservationException: Could not allocate 16777216 bytes, only 0 bytes are remaining
        at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:159) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.memory.UnsafeMemoryBudget.reserveMemory(UnsafeMemoryBudget.java:85) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.runtime.memory.MemoryManager.allocatePages(MemoryManager.java:227) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.table.runtime.util.LazyMemorySegmentPool.nextSegment(LazyMemorySegmentPool.java:82) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        ... 15 more
{code}

We should first return these segments to the memory pool before throwing the exception."	FLINK	Closed	3	1	10239	pull-request-available
13505885	Introduce CompactFileStoreSource for compact jobs of Table Store	"In this ticket we create the {{CompactFileStoreSource}} Flink source for separated compact jobs.

The behavior of this source is sketched as follows:
* For batch compact jobs, this source produces records containing all partitions and buckets of the current table.
* For streaming compact jobs, this source produces records containing all modified partitions and buckets of a snapshot. This source will also monitor on newly created snapshots."	FLINK	Closed	3	7	10239	pull-request-available
13430382	Add statistics collecting to sst files	"Currently field statistics are not collected in sst files. With statistics we can do filter and other operations with better performance.

Some formats like orc already record statistics into file headers, so for these special formats we just need to read them directly from files. For others, however, we need to collect the statistics by hand."	FLINK	Closed	3	7	10239	pull-request-available
13427379	Support projection pushdown on keys and values in sst file readers	Projection pushdown is an optimization for sources. With this optimization, we can avoid reading useless columns and thus improve performance.	FLINK	Closed	3	7	10239	pull-request-available
13245744	Add tests for complex data formats	There are currently no tests guarding some complex data formats, for example nested row, generic array and generic map.	FLINK	Resolved	4	4	10239	pull-request-available
13472200	Throw exception intentionally when new snapshots are committed during restore	"Currently snapshots are committed in {{notifyCheckpointComplete}}. If the job fails between a successful checkpoint and the call of {{notifyCheckpointComplete}}, these snapshots will be committed after job restarts.

However when the writer starts they also need to read from the latest snapshot (to build the latest structure of LSM tree). These two steps may happen concurrently and what the writers see may not be the latest snapshot.

To fix this problem, we can throw exception intentionally after new snapshots are committed during restore. In this way the job will be forcefully restarted and it is very likely that the writers can see the latest snapshot."	FLINK	Closed	3	4	10239	pull-request-available
13336872	FlinkRelMdDistinctRowCount#getDistinctRowCount(Calc) will always return 0 when number of rows are large	"Due to CALCITE-4351 {{FlinkRelMdDistinctRowCount#getDistinctRowCount(Calc)}} will always return 0 when number of rows are large.

What I would suggest is to introduce our own {{FlinkRelMdUtil#numDistinctVals}} to treat small and large inputs in different ways. For small inputs we use the more precise {{RelMdUtil#numDistinctVals}} and for large inputs we copy the old, approximated implementation of {{RelMdUtil#numDistinctVals}}.

This is a temporary solution. When CALCITE-4351 is fixed we should revert this commit."	FLINK	Closed	3	1	10239	pull-request-available
13254315	Provide api to convert flink table to java List (blink planner)	"It would be nice to convert flink table to java List so that I can do other data manipulation in client side after execution flink job. For flink planner, I can convert flink table to DataSet and use DataSet#collect, but for blink planner, there's no such api.

EDIT from FLINK-14807:
Currently, it is very unconvinient for user to fetch data of flink job unless specify sink expclitly and then fetch data from this sink via its api (e.g. write to hdfs sink, then read data from hdfs). However, most of time user just want to get the data and do whatever processing he want. So it is very necessary for flink to provide api Table#collect for this purpose. 

Other apis such as Table#head, Table#print is also helpful.  "	FLINK	Closed	3	4	10239	pull-request-available
13444550	Implement filter pushdown for TableStoreHiveStorageHandler	Filter pushdown is a critical optimization for sources as it can decrease number of records to read. Hive provides a {{HiveStoragePredicateHandler}} interface for this purpose. We need to implement this interface in {{TableStoreHiveStorageHandler}}.	FLINK	Closed	3	7	10239	pull-request-available
13245743	Fix serializer snapshot recovery in BaseArray and BaseMap serializers.	In BaseArray and BaseMap serializers, their element (or key/value) serializers are not stored in the config snapshot. When restoring the BaseArray/BaseMap serializers from the snapshots, their element/key/value serializers might be incorrect. This situation will happen when user uses his custom kryo serializers as element/key/value serializers.	FLINK	Resolved	3	1	10239	pull-request-available
13447860	Refactor RecordReader and RecordIterator in table store into generic types	We'd like to introduce a `TableRead` class between file store and the users. This class should provide an iterator for `RowData` instead of `KeyValue`s. So we'll need an iterator not only for `KeyValue` but also for `RowData`.	FLINK	Closed	3	7	10239	pull-request-available
13505879	Modify compact interface for TableWrite and FileStoreWrite to support normal compaction in Table Store	"Currently the {{compact}} interface in {{TableWrite}} and {{FileStoreWrite}} can only trigger full compaction. However a separated compact job should not only perform full compaction, but also perform normal compaction once in a while, just like what the current Table Store sinks do.

We need to modify compact interface for TableWrite and FileStoreWrite to support normal compaction."	FLINK	Closed	3	7	10239	pull-request-available
13393002	SELECT SHA2(CAST(NULL AS VARBINARY), CAST(NULL AS INT)) AS ref0 can't compile	"Running the sql of SELECT SHA2(CAST(NULL AS VARBINARY), CAST(NULL AS INT)) AS ref0 will get the error below:

{code}
java.lang.RuntimeException: Could not instantiate generated class 'ExpressionReducer$3'

	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:108)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:759)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:699)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:306)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:282)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1702)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:833)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1301)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest(CalcITCase.scala:1614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)
	... 74 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	... 76 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	... 79 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 55, Column 85: No applicable constructor/method found for actual parameters ""byte[], java.security.MessageDigest""; candidates are: ""public static org.apache.flink.table.data.binary.BinaryStringData org.apache.flink.table.data.binary.BinaryStringDataUtil.hash(org.apache.flink.table.data.binary.BinaryStringData, java.lang.String) throws java.security.NoSuchAlgorithmException"", ""public static org.apache.flink.table.data.binary.BinaryStringData org.apache.flink.table.data.binary.BinaryStringDataUtil.hash(org.apache.flink.table.data.binary.BinaryStringData, java.security.MessageDigest)""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9263)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	... 85 more
{code}
"	FLINK	Closed	3	1	10239	pull-request-available
13369265	ResultType of GeneratedExpression in StringCallGen should be compatible with their definition in FlinkSqlOperatorTable	"Add the following test case to {{TableEnvironmentITCase}} to reproduce this bug:

{code:scala}
@Test
def myTest(): Unit = {
  tEnv.executeSql(
    """"""
      |CREATE TABLE my_source (
      |  a VARCHAR(10)
      |) WITH (
      |  'connector'='values',
      |  'bounded'='true'
      |)
      |"""""".stripMargin)
  tEnv.explainSql(""SELECT ifnull(substring(a, 2, 5), 'null') FROM my_source"")
}
{code}

The exception stack is
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Mismatch of function's argument data type 'VARCHAR(10)' and actual argument type 'STRING'.

	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:323)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:320)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.verifyArgumentTypes(BridgingFunctionGenUtil.scala:320)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCallWithDataType(BridgingFunctionGenUtil.scala:95)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCall(BridgingFunctionGenUtil.scala:65)
	at org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.generate(BridgingSqlFunctionCallGen.scala:73)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:832)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:529)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:56)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:155)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$2.apply(CalcCodeGenerator.scala:141)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$2.apply(CalcCodeGenerator.scala:141)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:141)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:167)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:50)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:127)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:71)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:70)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:70)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:109)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:47)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:691)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainSql(TableEnvironmentImpl.java:677)
{code}

This is because the return type inference of {{SUBSTR}} in {{FlinkSqlOperatorTable}} is {{ARG0_VARCHAR_FORCE_NULLABLE}}, but the result type of the GeneratedExpression is {{VARCHAR(INT_MAX)}}."	FLINK	Closed	3	1	10239	pull-request-available
13381823	TIMESTAMPADD + timestamp_ltz type throws CodeGenException	"Add the following test case to {{org.apache.flink.table.planner.runtime.batch.sql.CalcITCase}} to reproduce this issue.

{code:scala}
@Test
def myTest(): Unit = {
  checkResult(""SELECT TIMESTAMPADD(MINUTE, 10, CURRENT_TIMESTAMP)"", Seq())
}
{code}

The exception stack is
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(result$5,isNull$4,


isNull$4 = false || false;
result$5 = null;
if (!isNull$4) {
  
  result$5 = org.apache.flink.table.data.TimestampData.fromEpochMillis(((org.apache.flink.table.data.TimestampData) queryStartTimestamp).getMillisecond() + ((long) 600000L), ((org.apache.flink.table.data.TimestampData) queryStartTimestamp).getNanoOfMillisecond());
  
}
,TIMESTAMP_LTZ(3) NOT NULL,None)] type is [TIMESTAMP_LTZ(3) NOT NULL], result type is [TIMESTAMP(6) NOT NULL]

	at org.apache.flink.table.planner.codegen.ExprCodeGenerator$$anonfun$generateResultExpression$1.apply(ExprCodeGenerator.scala:311)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator$$anonfun$generateResultExpression$1.apply(ExprCodeGenerator.scala:299)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateResultExpression(ExprCodeGenerator.scala:299)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateResultExpression(ExprCodeGenerator.scala:255)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:142)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:167)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:50)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:247)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:80)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:79)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:79)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1657)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:797)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1258)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:577)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest2(CalcITCase.scala:81)
{code}

This is because {{org.apache.calcite.sql.fun.SqlTimestampAddFunction#deduceType}} always returns a timestamp type for some time unit. So it seems that we should write our own timestamp add function to get the correct return type."	FLINK	Closed	3	1	10239	auto-unassigned, pull-request-available
13505886	Refactor StoreCompactOperator to accept records containing partitions and buckets in Table Store	In this ticket we refactor StoreCompactOperator to accept records containing partitions and buckets in Table Store. The old {{ALTER TABLE COMPACT}} will also be disabled in this ticket due to various restrictions.	FLINK	Closed	3	7	10239	pull-request-available
13305988	CollectResultFetcher fails with EOFException in AggregateReduceGroupingITCase	"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1826&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129

{code}
2020-05-19T10:34:18.3224679Z [ERROR] testSingleAggOnTable_SortAgg(org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase)  Time elapsed: 7.537 s  <<< ERROR!
2020-05-19T10:34:18.3225273Z java.lang.RuntimeException: Failed to fetch next result
2020-05-19T10:34:18.3227634Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:92)
2020-05-19T10:34:18.3228518Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:63)
2020-05-19T10:34:18.3229170Z 	at org.apache.flink.shaded.guava18.com.google.common.collect.Iterators.addAll(Iterators.java:361)
2020-05-19T10:34:18.3229863Z 	at org.apache.flink.shaded.guava18.com.google.common.collect.Lists.newArrayList(Lists.java:160)
2020-05-19T10:34:18.3230586Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
2020-05-19T10:34:18.3231303Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:141)
2020-05-19T10:34:18.3231996Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:107)
2020-05-19T10:34:18.3232847Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable(AggregateReduceGroupingITCase.scala:176)
2020-05-19T10:34:18.3233694Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateReduceGroupingITCase.testSingleAggOnTable_SortAgg(AggregateReduceGroupingITCase.scala:122)
2020-05-19T10:34:18.3234461Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-19T10:34:18.3234983Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-19T10:34:18.3235632Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-19T10:34:18.3236615Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-19T10:34:18.3237256Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-19T10:34:18.3237965Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-19T10:34:18.3238750Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-19T10:34:18.3239314Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-19T10:34:18.3239838Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-19T10:34:18.3240362Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-19T10:34:18.3240803Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-19T10:34:18.3243624Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-19T10:34:18.3244531Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-19T10:34:18.3245325Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-19T10:34:18.3246086Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-19T10:34:18.3246765Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-19T10:34:18.3247390Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-19T10:34:18.3248012Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-19T10:34:18.3248779Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-19T10:34:18.3249417Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-19T10:34:18.3250357Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-19T10:34:18.3251021Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-05-19T10:34:18.3251597Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-19T10:34:18.3252141Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-19T10:34:18.3252798Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-19T10:34:18.3253527Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-19T10:34:18.3254458Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-19T10:34:18.3255420Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-19T10:34:18.3256207Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-19T10:34:18.3257025Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-19T10:34:18.3257719Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-19T10:34:18.3258447Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-19T10:34:18.3258948Z Caused by: java.io.EOFException
2020-05-19T10:34:18.3259379Z 	at java.io.DataInputStream.readUnsignedByte(DataInputStream.java:290)
2020-05-19T10:34:18.3259826Z 	at org.apache.flink.api.java.typeutils.runtime.MaskUtils.readIntoMask(MaskUtils.java:73)
2020-05-19T10:34:18.3260307Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.deserialize(RowSerializer.java:200)
2020-05-19T10:34:18.3261044Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.deserialize(RowSerializer.java:58)
2020-05-19T10:34:18.3261535Z 	at org.apache.flink.api.common.typeutils.base.ListSerializer.deserialize(ListSerializer.java:133)
2020-05-19T10:34:18.3262105Z 	at org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse.getResults(CollectCoordinationResponse.java:91)
2020-05-19T10:34:18.3262752Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher$ResultBuffer.dealWithResponse(CollectResultFetcher.java:291)
2020-05-19T10:34:18.3263385Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher$ResultBuffer.access$200(CollectResultFetcher.java:249)
2020-05-19T10:34:18.3264077Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:142)
2020-05-19T10:34:18.3264666Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:89)
2020-05-19T10:34:18.3265050Z 	... 40 more
{code}"	FLINK	Closed	1	1	10239	pull-request-available, test-stability
13515484	Fix UnsupportedFileSystemSchemeException when writing Table Store on OSS with other engines	"Currently when writing Table Store tables on OSS with other engines (for example Spark), the following exception will occur.

{code}
22/12/23 17:54:12 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (core-1-1.c-c9f1b761c8946269.cn-huhehaote.emr.aliyuncs.com executor 2): java.lang.RuntimeException: Failed to find latest snapshot id
  at org.apache.flink.table.store.file.utils.SnapshotManager.latestSnapshotId(SnapshotManager.java:81)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.scanExistingFileMetas(AbstractFileStoreWrite.java:87)
  at org.apache.flink.table.store.file.operation.KeyValueFileStoreWrite.createWriter(KeyValueFileStoreWrite.java:113)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.createWriter(AbstractFileStoreWrite.java:227)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.lambda$getWriter$1(AbstractFileStoreWrite.java:217)
  at java.util.HashMap.computeIfAbsent(HashMap.java:1128)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.getWriter(AbstractFileStoreWrite.java:217)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.write(AbstractFileStoreWrite.java:106)
  at org.apache.flink.table.store.table.sink.TableWriteImpl.write(TableWriteImpl.java:63)
  at org.apache.flink.table.store.spark.SparkWrite$WriteRecords.call(SparkWrite.java:124)
  at org.apache.flink.table.store.spark.SparkWrite$WriteRecords.call(SparkWrite.java:105)
  at org.apache.spark.api.java.JavaPairRDD$.$anonfun$toScalaFunction$1(JavaPairRDD.scala:1070)
  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)
  at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)
  at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)
  at org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1097)
  at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:136)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'oss'. The scheme is directly supported by Flink through the following plugin(s): flink-oss-fs-hadoop. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/.
  at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:515)
  at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
  at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274)
  at org.apache.flink.table.store.file.utils.SnapshotManager.findLatest(SnapshotManager.java:164)
  at org.apache.flink.table.store.file.utils.SnapshotManager.latestSnapshotId(SnapshotManager.java:79)
  ... 30 more
{code}"	FLINK	Closed	3	1	10239	pull-request-available
13361926	Flink SQL with CSV file input job hangs	"In extension to FLINK-21567, I actually also got the job to be stuck on cancellation by doing the following in the SQL client:

* configure SQL client defaults to run with parallelism 2
* execute the following statement

{code}
CREATE TABLE `airports` (
  `IATA_CODE` CHAR(3),
  `AIRPORT` STRING,
  `CITY` STRING,
  `STATE` CHAR(2),
  `COUNTRY` CHAR(3),
  `LATITUDE` DOUBLE NULL,
  `LONGITUDE` DOUBLE NULL,
  PRIMARY KEY (`IATA_CODE`) NOT ENFORCED
) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///tmp/kaggle-flight-delay/airports.csv',
  'format' = 'csv',
  'csv.allow-comments' = 'true',
  'csv.ignore-parse-errors' = 'true',
  'csv.null-literal' = ''
);

CREATE TABLE `flights` (
  `_YEAR` CHAR(4),
  `_MONTH` CHAR(2),
  `_DAY` CHAR(2),
  `_DAY_OF_WEEK` TINYINT,
  `AIRLINE` CHAR(2),
  `FLIGHT_NUMBER` SMALLINT,
  `TAIL_NUMBER` CHAR(6),
  `ORIGIN_AIRPORT` CHAR(3),
  `DESTINATION_AIRPORT` CHAR(3),
  `_SCHEDULED_DEPARTURE` CHAR(4),
  `SCHEDULED_DEPARTURE` AS TO_TIMESTAMP(`_YEAR` || '-' || `_MONTH` || '-' || `_DAY` || ' ' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 0 FOR 2) || ':' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 3) || ':00'),
  `_DEPARTURE_TIME` CHAR(4),
  `DEPARTURE_DELAY` SMALLINT,
  `DEPARTURE_TIME` AS TIMESTAMPADD(MINUTE, CAST(`DEPARTURE_DELAY` AS INT), TO_TIMESTAMP(`_YEAR` || '-' || `_MONTH` || '-' || `_DAY` || ' ' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 0 FOR 2) || ':' || SUBSTRING(`_SCHEDULED_DEPARTURE` FROM 3) || ':00')),
  `TAXI_OUT` SMALLINT,
  `WHEELS_OFF` CHAR(4),
  `SCHEDULED_TIME` SMALLINT,
  `ELAPSED_TIME` SMALLINT,
  `AIR_TIME` SMALLINT,
  `DISTANCE` SMALLINT,
  `WHEELS_ON` CHAR(4),
  `TAXI_IN` SMALLINT,
  `SCHEDULED_ARRIVAL` CHAR(4),
  `ARRIVAL_TIME` CHAR(4),
  `ARRIVAL_DELAY` SMALLINT,
  `DIVERTED` BOOLEAN,
  `CANCELLED` BOOLEAN,
  `CANCELLATION_REASON` CHAR(1),
  `AIR_SYSTEM_DELAY` SMALLINT,
  `SECURITY_DELAY` SMALLINT,
  `AIRLINE_DELAY` SMALLINT,
  `LATE_AIRCRAFT_DELAY` SMALLINT,
  `WEATHER_DELAY` SMALLINT
) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///tmp/kaggle-flight-delay/flights-small2.csv',
  'format' = 'csv',
  'csv.null-literal' = ''
);

SELECT `ORIGIN_AIRPORT`, `AIRPORT`, `STATE`, `NUM_DELAYS`
FROM (
  SELECT `ORIGIN_AIRPORT`, `AIRPORT`, `STATE`, COUNT(*) AS `NUM_DELAYS`,
    ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC) AS rownum
  FROM flights, airports
  WHERE `ORIGIN_AIRPORT` = `IATA_CODE` AND `DEPARTURE_DELAY` > 0
  GROUP BY `ORIGIN_AIRPORT`, `AIRPORT`, `STATE`)
WHERE rownum <= 10;
{code}

Results are shown in the CLI but after quitting the result view, the job seems stuck in CANCELLING until (at least) one of the TMs shuts itself down because a task wouldn't react to the cancelling signal. This appears in its TM logs:

{code}
2021-03-02 18:39:19,451 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Task 'Source: TableSourceScan(table=[[default_catalog, default_database, airports, project=[IATA_CODE, AIRPORT, STATE]]], fields=[IATA_CODE, AIRPORT, STATE]) (2/2)#0' did not react to cancelling signal for 30 seconds, but is stuck in method:
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:653)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:585)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:570)
java.lang.Thread.run(Thread.java:748)

...

2021-03-02 18:39:49,447 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Task did not exit gracefully within 180 + seconds.
org.apache.flink.util.FlinkRuntimeException: Task did not exit gracefully within 180 + seconds.
	at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1685) [flink-dist_2.12-1.12.1.jar:1.12.1]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
2021-03-02 18:39:49,448 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Fatal error occurred while executing the TaskManager. Shutting it down...
org.apache.flink.util.FlinkRuntimeException: Task did not exit gracefully within 180 + seconds.
	at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1685) [flink-dist_2.12-1.12.1.jar:1.12.1]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code}"	FLINK	Closed	4	1	10239	auto-deprioritized-major, pull-request-available
13392326	Add Flink 1.13 MigrationVersion	"Currently the largest MigrationVersion is 1.12. We need newer versions to add more serializer compatibility tests.

As stated in [https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release#CreatingaFlinkRelease-Checklisttoproceedtothenextstep.1] this should be the work of release manager."	FLINK	Closed	3	4	10239	pull-request-available
13335325	Update deadlock break-up algorithm to cover more cases	"Current deadlock breakup algorithm fails to cover the following case:

 !pasted image 0.png! 

We're going to introduce a new deadlock breakup algorithm to cover this."	FLINK	Closed	3	7	10239	pull-request-available
13440120	Add end to end tests for table store	End to end tests ensure that users can run table store as expected.	FLINK	Closed	3	7	10239	pull-request-available
13505882	Move split initialization and discovery logic fully into SnapshotEnumerator in Table Store	"It is possible that the separated compact job is started long after the write jobs. so compact job sources need a special split initialization logic: we will find the latest COMPACT snapshot, and start compacting right after this snapshot.

However, split initialization logic are currently coded into {{FileStoreSource}}. We should extract these logic into {{SnapshotEnumerator}} so that we can create our special {{SnapshotEnumerator}} for compact sources."	FLINK	Closed	3	7	10239	pull-request-available
13474439	Hive table store support can't be fully enabled by ADD JAR statement	"Current table store Hive document states that, to enable support in Hive, we should use ADD JAR statement to add hive connector jar.

However some types of queries, for example join statements in MR engine, may still throw ClassNotFound exception. The correct way to enable table store support is to create an auxlib directory under the root directory of Hive and copy jar into that directory."	FLINK	Closed	3	1	10239	pull-request-available
13247262	Add TPC-H queries as E2E tests	We should add the TPC-H queries as E2E tests in order to verify the blink planner.	FLINK	Resolved	1	4	10239	pull-request-available
13335328	Introduce multi-input operator construction algorithm	We should introduce an algorithm to organize exec nodes into multi-input exec nodes.	FLINK	Closed	3	7	10239	pull-request-available
13433824	Delegate table planner dependencies with separate modules	FLINK-25128 introduces the planner loader module to hide scala versions. All modules relying on flink-table-planner should change to flink-table-planner-loader instead.	FLINK	Closed	3	7	10239	pull-request-available
13447187	Add an abstraction layer for connectors to read and write row data instead of key-values	"Currently {{FileStore}} exposes an interface for reading and writing {{KeyValue}}. However connectors may have different ways to change a {{RowData}} to {{KeyValue}} under different {{WriteMode}}. This results in lots of {{if...else...}} branches and duplicated code.

We need to add an abstraction layer for connectors to read and write row data instead of key-values."	FLINK	Closed	3	4	10239	pull-request-available, stale-assigned
13448023	Introduce TableScan and TableRead as an abstraction layer above FileStore for reading RowData	In this step we introduce {{TableScan}} and {{TableRead}} They are an abstraction layer above {{FileStoreScan}} and {{FileStoreRead}} to provide {{RowData}} reading.	FLINK	Closed	3	7	10239	pull-request-available
13425302	Move states of AbstractAvroBulkFormat into its reader	FLINK-24565 ports avro format to {{BulkReaderFormatFactory}}. However the implementation leaves some states into the format factory itself. These states should be in the readers.	FLINK	Closed	3	1	10239	pull-request-available
13368368	PushFilterIntoTableSourceScanRule fails to deal with NULLs	"Add the following test case to {{PushFilterIntoTableSourceScanRuleTest}} to reproduce this bug:

{code:java}
@Test
public void myTest() {
    String ddl =
            ""CREATE TABLE MTable (""
                    + ""  a STRING,""
                    + ""  b STRING""
                    + "") WITH (""
                    + ""  'connector' = 'values',""
                    + ""  'bounded' = 'true'""
                    + "")"";
    util().tableEnv().executeSql(ddl);
    util().verifyRelPlan(""WITH MView AS (SELECT CASE\n""
            + ""  WHEN a = b THEN a\n""
            + ""  ELSE CAST(NULL AS STRING)\n""
            + ""  END AS a\n""
            + ""  FROM MTable)\n""
            + ""SELECT a FROM MView WHERE a IS NOT NULL"");
}
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: Data type 'STRING NOT NULL' does not support null values.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:272)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:79)
	at org.apache.flink.table.expressions.ApiExpressionUtils.valueLiteral(ApiExpressionUtils.java:251)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:463)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1173)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:470)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:470)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:138)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:137)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:137)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor.extractConjunctiveConditions(RexNodeExtractor.scala)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.pushFilterIntoScan(PushFilterIntoTableSourceScanRule.java:121)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.onMatch(PushFilterIntoTableSourceScanRule.java:100)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:276)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlan(TableTestBase.scala:400)
{code}

It seems that this bug is related to commit 957c49d56c80416ae712ae79cdd2784bb2387c80 by [~dwysakowicz]. This commit is a hotfix related to no issue. It adds a {{notNull}} to the return value of RexNodeExtractor#visitLiteral."	FLINK	Closed	3	1	10239	pull-request-available
13493443	FileStoreCommitTest is unstable and may stuck	{{FileStoreCommitTest}} may stuck because the {{FileStoreCommit}} in {{TestCommitThread}} does not commit APPEND snapshot when no new files are produced. In this case, if the following COMPACT snapshot conflicts with the current merge tree, the test will stuck.	FLINK	Closed	3	1	10239	pull-request-available
13424613	Implement partition and bucket filter of FileStoreScanImpl	"Implement partition and bucket filter in FileStoreScanImpl.

Also perform some small optimizations such as concurrent reads in FileStoreScanImpl."	FLINK	Closed	3	7	10239	pull-request-available
13450124	Introduce a tablestore HiveCatalog	"Now we have FileSystemCatalog, we can introduce a tablestore HiveCatalog, in this way.

There are some benefits:
 # Tables created in this catalog can be read directly by the Hive engine without having to create external tables.
 # Schema synchronization, users rely on hive to manage tables
 # lock to Object Store writing, support multiple concurrent writes"	FLINK	Closed	3	7	10239	pull-request-available
13385744	Introduce Java code splitter for code generation	In this first step we introduce a Java code splitter for the generated Java code. This splitter comes into effect after the original Java code is generated, hoping to solve the 64KB problem in one shot.	FLINK	Closed	3	7	10239	pull-request-available
13387834	Activate Java code splitter for all generated classes	In the second step we activate Java code splitter introduced in the first step.	FLINK	Closed	3	7	10239	pull-request-available
13441703	Package Flink runtime dependencies into associated table store modules instead of flink-table-store-dist	"Currently all Flink dependencies needed at runtime (for example, flink-sql-avro) by table store are packaged in flink-table-store-dist module. If we would like to enable other systems to read from table store in the future all these dependencies need to be copied to the distribution module of that system.

So instead we should move these dependencies to the associated table store module so that the number of dependencies in flink-table-store-dist can be decreased."	FLINK	Closed	3	4	10239	pull-request-available
13481410	Change DataFileWriter into a factory to create writers	"Currently {{DataFileWriter}} exposes {{write}} method for data files and extra files.

However, as the number of patterns to write files is increasing (for example, we'd like to write some records into a data file, then write some other records into an extra files when producing changelogs from full compaction) we'll have to keep adding methods to {{DataFileWriter}} if we keep the current implementation.

We'd like to refactor {{DataFileWriter}} into a factory to create writers, so that the users of writers can write however they like."	FLINK	Closed	3	7	10239	pull-request-available
13393366	SELECT ROUND(CAST(1.2345 AS FLOAT), 1) cannot compile	"Run this SQL {{SELECT ROUND(CAST(1.2345 AS FLOAT), 1)}} and the following exception will be thrown:

{code}
java.lang.RuntimeException: Could not instantiate generated class 'ExpressionReducer$2'

	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:108)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:759)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:699)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:306)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:282)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1702)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:833)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1301)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest2(CalcITCase.scala:1618)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)
	... 74 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	... 76 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	... 79 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 36, Column 23: Assignment conversion not possible from type ""double"" to type ""float""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	... 85 more
{code}"	FLINK	Closed	3	1	10239	pull-request-available
13494213	Change commitIdentifier in Table Store snapshot to long value	"Currently {{commitIdentifier}} in {{Snapshot}} is a {{String}} value. However there are many scenarios where we need to compare two identifiers to find out which one is newer. For example
* In FLINK-29840, we need to store the latest modified commit for each writer. Only when the latest snapshot is newer than this commit can we safely close the writer.
* In FLINK-29805, we can read the commit identifier of the latest snapshot. All identifiers older than that should be filtered out.
* In FLINK-29752, we need to trigger full compaction once in a few commits. We can read the latest commit identifier and compare it with the full compaction identifier to check if full compaction is successfully committed.
 "	FLINK	Closed	3	4	10239	pull-request-available
13396306	Upsert materializer is not inserted for all sink providers	The new {{SinkUpsertMaterializer}} is not inserted for {{TransformationSinkProvider}} or {{DataStreamSinkProvider}} which means that neither {{toChangelogStream}} nor the current {{KafkaDynamicSink}} work correctly.	FLINK	Closed	1	1	10269	pull-request-available
13481645	Relax allow-client-job-configurations for Table API and parameters	Currently, the {{execution.allow-client-job-configurations}} is a bit too strict. Due to the complexity of the configuration stack, it makes it impossible to use Table API and also prevents very common parameters like {{pipeline.global-job-parameters}}.	FLINK	Closed	3	4	10269	pull-request-available
13398959	RowDataSerializerTest fails on Azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23376&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30176

{code}
Sep 02 10:12:18 [ERROR] testSerializedCopyAsSequence  Time elapsed: 0.009 s  <<< FAILURE!
Sep 02 10:12:18 java.lang.AssertionError: Exception in test: Row arity of input element does not match serializers.
Sep 02 10:12:18 	at org.junit.Assert.fail(Assert.java:89)
Sep 02 10:12:18 	at org.apache.flink.api.common.typeutils.SerializerTestBase.testSerializedCopyAsSequence(SerializerTestBase.java:429)
Sep 02 10:12:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 02 10:12:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 02 10:12:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 02 10:12:18 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 02 10:12:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Sep 02 10:12:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Sep 02 10:12:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Sep 02 10:12:18 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Sep 02 10:12:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Sep 02 10:12:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Sep 02 10:12:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Sep 02 10:12:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Sep 02 10:12:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Sep 02 10:12:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Sep 02 10:12:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Sep 02 10:12:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Sep 02 10:12:18 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Sep 02 10:12:18 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Sep 02 10:12:18 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Sep 02 10:12:18 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Sep 02 10:12:18 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Sep 02 10:12:18 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Sep 02 10:12:18 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Sep 02 10:12:18 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Sep 02 10:12:18 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Sep 02 10:12:18 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Sep 02 10:12:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Sep 02 10:12:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Sep 02 10:12:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Sep 02 10:12:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:116)
Sep 02 10:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Sep 02 10:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Sep 02 10:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Sep 02 10:12:18 	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:199)
Sep 02 10:12:18 	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)
Sep 02 10:12:18 	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)
Sep 02 10:12:18 	at org.apache.flink.api.common.typeutils.SerializerTestBase$SerializerRunner.run(SerializerTestBase.java:580)

{code}"	FLINK	Closed	1	1	10269	pull-request-available, test-stability
13566848	Reduce instantiation of ScanRuntimeProvider in streaming mode	This is pure performance optimization by avoiding an additional call to \{{ScanTableSource#getScanRuntimeProvider}} in \{{org.apache.flink.table.planner.connectors.DynamicSourceUtils#validateScanSource}}.	FLINK	Closed	3	4	10269	pull-request-available
13377609	Drop usages of BatchTableEnvironment and old planner in Python	"This is a major cleanup of the Python module that drops support for BatchTableEnvironment and old planner.

Removes usages of:
 - DataSet
 - BatchTableEnvironment
 - Legacy planner
 - ExecutionEnvironment

 

Note: Batch processing is still possible via the unified \{{TableEnvironment}}."	FLINK	Closed	3	7	10269	pull-request-available
13231270	Add the user-facing classes of the new type system	FLINK-12253 introduces logical types that will be used mostly internally. Users will use the {{DataType}} stack described in FLIP-37. This issue describes the class hierarchy around this class.	FLINK	Closed	3	7	10269	pull-request-available
13377610	Drop BatchTableSource OrcTableSource and related classes	The BatchTableSource interface is not supported by the Blink planner. Therefore, we drop this class to unblock the removal of the legacy planner. An alternative ORC source and sink using the new interfaces is available.	FLINK	Closed	3	7	10269	pull-request-available
13318894	Support @DataTypeHint for TableFunction output types	"For ScalarFunctions, the return type of an eval method can be declared with a {{@DataTypeHint}}:


{code:java}
@DataTypeHint(""INT"")
public Integer eval(Integer value) {
  return value * 2;
}{code}

This does not work for TableFunctions because the {{@DataTypeHint}} annotation refers to the {{void}} return type. Hence, {{TableFunction}} {{eval()}} methods must always be annotated with the more complex {{@FunctionHint}} method.
However, I think that context, it is clear that the {{@DataTypeHint}} annotation refers to the actual return type of the table function (the type parameter of {{TableFunction<OUT>}}).



We could consider allowing {{@DataTypeHint}} annotations also on {{TableFunction}} classes (defining the output type of all eval methods) and {{eval()}} methods."	FLINK	Closed	4	7	10269	pull-request-available
13318705	Fix Scala code examples for UDF type inference annotations	"The Scala code examples for the [UDF type inference annotations|https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#type-inference] are not correct.

For example: the following {{FunctionHint}} annotation 

{code:scala}
@FunctionHint(
  input = Array(@DataTypeHint(""INT""), @DataTypeHint(""INT"")),
  output = @DataTypeHint(""INT"")
)
{code}


needs to be changed to

{code:scala}
@FunctionHint(
  input = Array(new DataTypeHint(""INT""), new DataTypeHint(""INT"")),
  output = new DataTypeHint(""INT"")
)
{code}"	FLINK	Closed	3	1	10269	pull-request-available
13237427	Fix ANY type serialization	"Every logical type needs to be string serializable. In old versions we used Java serialization logic for it. Since an any type has no type information anymore but just type serializer, we can use the snapshot to write out an any type into properties in a backward compatible way.

However, the current serialization logic is wrong."	FLINK	Closed	3	7	10269	pull-request-available
13328017	Support FLIP-107 interfaces in planner	Support `SupportsReadingMetadata` and `SupportsWritingMetadata` in the planner with testing connectors and formats.	FLINK	Closed	3	7	10269	pull-request-available
13322483	Update documentation about user-defined aggregate functions	The documentation needs an update because all functions support the new type inference now.	FLINK	Closed	3	7	10269	pull-request-available
13434389	Metadata keys should not conflict with physical columns	"If you have an field called timestamp and in addition want to read the timestamp from the metadata:

{code}
CREATE TABLE animal_sightings_with_metadata (
  `timestamp` TIMESTAMP(3),
  `name` STRING,
  `country` STRING,
  `number` INT,
  `append_time` TIMESTAMP(3) METADATA FROM 'timestamp',
  `partition` BIGINT METADATA VIRTUAL,
  `offset` BIGINT METADATA VIRTUAL,
  `headers` MAP<STRING, BYTES> METADATA,
  `timestamp-type` STRING METADATA,
  `leader-epoch` INT METADATA,
  `topic` STRING METADATA
)
{code}

This gives:
{code}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [timestamp]
{code}"	FLINK	Closed	3	7	10269	pull-request-available
13174049	Kafka table source factory produces invalid field mapping	The Kafka table source factory produces an invalid field mapping when referencing a rowtime attribute from an input field. The check in {{TableSourceUtil#validateTableSource}} therefore can fail.	FLINK	Resolved	3	1	10269	pull-request-available
13328719	Update API module for FLIP-107	This includes updating the `TableSchema`, `SqlToOperationConverter`, `MergeTableLikeUtil`. Everything until planning.	FLINK	Closed	3	7	10269	pull-request-available
13247585	ThreadLocalCache clashes for Blink planner	{{org.apache.flink.table.runtime.functions.ThreadLocalCache}} currently clashes.	FLINK	Closed	1	7	10269	pull-request-available
13420559	Add catalog object compile/restore options	A prerequisite for serialization/deserialization of various entities.	FLINK	Closed	3	7	10269	pull-request-available
13379066	Clean up examples to not use legacy planner anymore	Clean up the `flnk-examples-table` module to not reference the legacy planner anymore.	FLINK	Closed	3	7	10269	pull-request-available
13432422	Support the new type inference in Scala Table API table functions	"Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used ""inline"". We should support them as well."	FLINK	Closed	3	7	10269	pull-request-available
13339692	Add documentation for FLIP-107	"Add documentation for FLIP-107:

- Connector/format metadata in general
- Kafka key/value and metadata
- Debezium metadata"	FLINK	Closed	2	7	10269	pull-request-available
13381634	Drop remaining usages of legacy planner in E2E tests and Python	This removes the remaining usages of legacy planner outside of the {{flink-table}} module.	FLINK	Closed	3	7	10269	pull-request-available
13429756	Add convenient TableConfig.set	"Currently, the API for setting config options is cumbersome:

{code}
env.getConfig().getConfiguration().set(ConfigOption, )
{code}

could be improved to

{code}
env.getConfig().set()
{code}"	FLINK	Closed	3	7	10269	pull-request-available
13174320	Document unified table sources/sinks/formats	The recent unification of table sources/sinks/formats needs documentation. I propose a new page that explains the built-in sources, sinks, and formats as well as a page for customization of public interfaces.	FLINK	Resolved	3	4	10269	pull-request-available
13382142	"Remove ""blink"" suffix from table modules"	"In order to reduce confusion around ""What is Flink?/What is Blink?"" we should remove the term {{blink}} from the following modules:

{code}
flink-table-planner-blink
flink-table-runtime-blink
flink-table-uber-blink
{code}

It is up for discussion if we will:
- just perform the refactoring,
- keep the old modules for a while,
- or move the contents to new modules and link to those from the old modules

In any case we should make sure to keep the Git history."	FLINK	Closed	3	7	10269	pull-request-available
13306705	Use new type inference for SQL table and scalar functions	We urgently need to reduce the friction around different type systems and types of function. Therefore, we should update table/scalar functions in SQL to the new type inference.	FLINK	Closed	3	7	10269	pull-request-available
13244959	Allow switching planners in SQL Client	Once FLINK-13267 is resolved, we can also enable switching planners in the SQL Client via a execution property. Even though this is kind of a new feature, it had to be postponed after the feature-freeze as the relocation of FLINK-13267 would have created many merge conflicts otherwise.	FLINK	Closed	1	2	10269	pull-request-available
13315579	Scala varargs cause exception for new inference	Scala varargs are supported but cause an exception currently. Because there are two signatures (a valid and invalid one) in the class file.	FLINK	Closed	3	7	10269	pull-request-available
13381854	Remove the legacy planner code base	"This removes the legacy planner code base. In particular, it removes the content of {{flink-table-planner}}.

An equally named module will be reintroduced in a later subtask and will contain the content of {{flink-table-planner-blink}} which is the one and only planner from Flink 1.14 onwards."	FLINK	Closed	3	7	10269	pull-request-available
13389378	Disable AutoWatermarkInterval for bounded legacy sources	{{LegacySourceTransformationTranslator}} has currently no special path for bounded sources. However, the table planner might add bounded legacy sources while AutoWatermarkInterval is still enabled. We should have the same logic as in {{SourceTransformationTranslator}} for disabling intermediate watermarks.	FLINK	Closed	3	7	10269	pull-request-available
13195911	Interval join produces wrong result type in Scala API	"When stream is a Scala case class, the TypeInformation will fall back to GenericType in the process function which result in bad performance when union another DataStream.

In the union method of DataStream, the type is first checked for equality.

Here is an example:
{code:java}
object Test {

    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment

      val orderA: DataStream[Order] = env.fromCollection(Seq(
        Order(1L, ""beer"", 3),
         Order(1L, ""diaper"", 4),
         Order(3L, ""rubber"", 2)))

      val orderB: DataStream[Order] = env.fromCollection(Seq(
        new Order(2L, ""pen"", 3),
        new Order(2L, ""rubber"", 3),
        new Order(4L, ""beer"", 1)))

      val orderC: DataStream[Order] = orderA.keyBy(_.user)
        .intervalJoin(orderB.keyBy(_.user))
        .between(Time.seconds(0), Time.seconds(0))
        .process(new ProcessJoinFunction[Order, Order, Order] {
          override def processElement(left: Order, right: Order, ctx: ProcessJoinFunction[Order, Order, Order]#Context, out: Collector[Order]): Unit = {
            out.collect(left)
          }})

      println(""C: "" + orderC.dataType.toString)
      println(""B: "" + orderB.dataType.toString)

      orderC.union(orderB).print()

      env.execute()
    }

    case class Order(user: Long, product: String, amount: Int)
}{code}
Here is the Exception:
{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot union streams of different types: GenericType<com.manbuyun.awesome.flink.Test.Order> and com.manbuyun.awesome.flink.Test$Order(user: Long, product: String, amount: Integer)
 at org.apache.flink.streaming.api.datastream.DataStream.union(DataStream.java:219)
 at org.apache.flink.streaming.api.scala.DataStream.union(DataStream.scala:357)
 at com.manbuyun.awesome.flink.Test$.main(Test.scala:38)
 at com.manbuyun.awesome.flink.Test.main(Test.scala){code}
 "	FLINK	Closed	3	1	10269	pull-request-available
13381306	Drop usages of legacy planner in Hive module	Remove references to {{flink-table-planner}} in the Hive module.	FLINK	Closed	3	7	10269	pull-request-available
13381722	Add possibility to call built-in functions in SpecializedFunction	This is the last missing piece to avoid code generation when developing built-in functions. Core operations such as CAST, EQUALS, etc. will still use code generation but other built-in functions should be able to use these core operations without the need for generating code. It should be possible to call other built-in functions via a SpecializedFunction.	FLINK	Closed	3	7	10269	auto-unassigned, pull-request-available
13285755	testDynamicTableFunction fails	"https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/5186/logs/16

 
{code:java}
2020-02-14T14:46:56.8515984Z [ERROR] testDynamicTableFunction(org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase) Time elapsed: 3.452 s <<< FAILURE!
 2020-02-14T14:46:56.8517003Z java.lang.AssertionError:
 2020-02-14T14:46:56.8517232Z
 2020-02-14T14:46:56.8517485Z Expected: <[Test is a string, 42, null]>
 2020-02-14T14:46:56.8517739Z but: was <[42, Test is a string, null]>
 2020-02-14T14:46:56.8518067Z at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.testDynamicTableFunction(FunctionITCase.java:611){code}
 

 

The change was to enable chaining of the ContinuousFileReaderOperator (https://github.com/apache/flink/pull/11097)."	FLINK	Closed	2	1	10269	pull-request-available
13315437	New Table Function type inference fails	"For a simple UDTF like 
{code:java}
public class Split extends TableFunction<String> {
		public Split(){}
		public void eval(String str, String ch) {
			if (str == null || str.isEmpty()) {
				return;
			} else {
				String[] ss = str.split(ch);
				for (String s : ss) {
					collect(s);
				}
			}
		}
	}
{code}

register it using new function type inference {{tableEnv.createFunction(""my_split"", Split.class);}} and using it in a simple query will fail with following exception:

{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 93 to line 1, column 115: No match found for function signature my_split(<CHARACTER>, <CHARACTER>)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:716)
	at com.bytedance.demo.SqlTest.main(SqlTest.java:64)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 93 to line 1, column 115: No match found for function signature my_split(<CHARACTER>, <CHARACTER>)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)
	at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)
	... 7 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature my_split(<CHARACTER>, <CHARACTER>)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)
	... 35 more
{code}

it's reported from user-zh: http://apache-flink.147419.n8.nabble.com/Flink-1-11-SQL-UDTF-No-match-found-for-function-signature-td4553.html
CC [~twalthr]"	FLINK	Closed	3	1	10269	pull-request-available
13296425	Add core table source/sink interfaces	"This will add the most important interfaces for the new source/sink interfaces:
 * DynamicTableSource
 * ScanTableSource extends DynamicTableSource
 * LookupTableSource extends DynamicTableSource
 * DynamicTableSink

And some initial ability interfaces:
 * SupportsComputedColumnPushDown
 * SupportsFilterPushDown

All interfaces will have extended JavaDocs."	FLINK	Closed	3	7	10269	pull-request-available
13475673	Non-deterministic UID generation might cause issues during restore	"I want to use the savepoint mechanism to move existing jobs from one version of Flink to another, by:
 # Stopping a job with a savepoint
 # Creating a new job from the savepoint, on the new version.

In Flink 1.15.1, it fails, even when going from 1.15.1 to 1.15.1. I get this error, meaning that it could not map the state from the previous job to the new one because of one operator:
{quote}{{Failed to rollback to checkpoint/savepoint hdfs://hdfs-name:8020/flink-savepoints/savepoint-046708-238e921f5e78. Cannot map checkpoint/savepoint state for operator d14a399e92154660771a806b90515d4c to the new program, because the operator is not available in the new program.}}
{quote}
After investigation, the problematic operator corresponds to a {{ChangelogNormalize}} operator, that I do not explicitly create. It is generated because I use [{{tableEnv.fromChangelogStream(stream, schema, ChangelogMode.upsert())}}|https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.html#fromChangelogStream-org.apache.flink.streaming.api.datastream.DataStream-org.apache.flink.table.api.Schema-org.apache.flink.table.connector.ChangelogMode-] (the upsert mode is important, other modes do not fail). The table created is passed to an SQL query using the SQL API, which generates something like:
{quote}{{ChangelogNormalize[8] -> Calc[9] -> TableToDataSteam -> [my_sql_transformation] -> [my_sink]}}
{quote}
In previous versions of Flink it seems this operator was always given the same uid so the state could match when starting from the savepoint. In Flink 1.15.1, I see that a different uid is generated every time. I could not find a reliable way to set that uid manually. The only way I found was by going backwards from the transformation:
{quote}{{dataStream.getTransformation().getInputs().get(0).getInputs().get(0).getInputs().get(0).setUid(""the_user_defined_id"");}}
{quote}"	FLINK	Closed	1	1	10269	pull-request-available
13293753	Support new type inference for Table#flatMap	"Currently the Table#flatMap works only for {{TableFunctionDefinition}}. We should fix it to work also with functions that use the new type stack.

The problematic spot is in {{OperationTreeBuilder#flatMap}}"	FLINK	Closed	3	7	10269	pull-request-available
13310843	Update data type documentation for 1.11	Update the data type documentation for 1.11.	FLINK	Closed	2	7	10269	pull-request-available
13344610	Make implementing a built-in function straightforward	FLIP-65 has introduced new abstractions for functions. They make implementing user-defined functions straightforward. However, currently, these abstraction are not available for built-in functions. Adding a built-in function requires many changes at different locations as shown in FLINK-6810. This is error-prone, limits contributions, and is also not well supported by the current module system.	FLINK	Closed	3	7	10269	pull-request-available
13244916	Remove planner class clashes for planner type inference lookups	Currenlty, PlannerTypeInferenceUtilImpl exists in both planners. But we should avoid class clashes to make both planners available in a lib directory.	FLINK	Closed	1	7	10269	pull-request-available
13324445	Fix validation of table functions in projections	"While working on another change, I realized that the {{FunctionITCase.testInvalidUseOfTableFunction()}} tests throws a NullPointerException during execution.

This error is not visible, because TableEnvironmentImpl.executeInternal() does not wait for the final job status.
It submits the job using the job client ({{JobClient jobClient = execEnv.executeAsync(pipeline);}}), and it doesn't wait for the job to complete before returning a result. 

This is the null pointer that is hidden:
{code}

Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'insert-into_default_catalog.default_database.SinkTable'.
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1823)
	at org.apache.flink.table.planner.delegation.ExecutorBase.executeAsync(ExecutorBase.java:57)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:681)
	... 34 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:148)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$submitJob$2(PerJobMiniClusterFactory.java:92)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:195)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:188)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:182)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:523)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:422)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.runtime.collector.WrappingCollector.outputResult(WrappingCollector.java:43)
	at StreamExecCalc$245$TableFunctionResultConverterCollector$243.collect(Unknown Source)
	at org.apache.flink.table.functions.TableFunction.collect(TableFunction.java:201)
	at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase$RowTableFunction.eval(FunctionITCase.java:1024)
	at StreamExecCalc$245.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.pushToOperator(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:603)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:563)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:305)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:394)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
{code}"	FLINK	Closed	3	1	10269	pull-request-available
13377630	Drop BatchTableSource ParquetTableSource and related classes	The BatchTableSource interface is not supported by the Blink planner. Therefore, we drop this class to unblock the removal of the legacy planner. An alternative Parque source and sink using the new interfaces is available.	FLINK	Closed	3	7	10269	pull-request-available
13170260	Improve CLI responsiveness when cluster is not reachable	If the cluster was not started or is not reachable it takes a long time to cancel a result. This should not affect the main thread. The CLI should be responsive at all times.	FLINK	Closed	3	4	10269	pull-request-available
13306718	Don't allow self referencing structured type	"Currently, the logical constraint ""A type cannot be defined so that one of its attribute types (transitively) uses itself."" is not enforced during extraction and leads to a stack overflow. We should throw a helpful exception instead."	FLINK	Closed	3	7	10269	pull-request-available
13282140	Expose the new type inference for aggregate functions	This will allow to use aggregate functions with the new type inference. It requires different changes through the stack. This should be implemented after we have support for structured types. It also includes a long-term solution for the concept of DataViews with the new type system.	FLINK	Closed	3	7	10269	pull-request-available
13182166	User-defined function with LITERAL paramters yields CompileException	"When using a user-defined scalar function only with literal parameters, a {{CompileException}} is thrown. For example

{code}
SELECT myFunc(CAST(40.750444 AS FLOAT), CAST(-73.993475 AS FLOAT))

public class MyFunc extends ScalarFunction {

	public int eval(float lon, float lat) {
		// do something
	}
}
{code}

results in 

{code}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 5, Column 10: Cannot determine simple type name ""com""
{code}

The problem is probably caused by the expression reducer because it disappears if a regular attribute is added to a parameter expression."	FLINK	Closed	3	1	10269	pull-request-available
13303064	Support inline structured types	"Many locations in the code base already support structured types. The runtime treats them as row types. However, some final work is needed to support structured types though the stack. We start with inline structured types. Registered structured types in catalog are covered in a different issue.

Inline structured types are a prerequisite to enable aggregate functions in FLIP-65 again."	FLINK	Closed	3	7	10269	pull-request-available
13362189	Document Table/SQL API limitations regarding upgrades with savepoints	"We don't appear to mention anywhere that users cannot upgrade Table/SQL API applications with savepoints (or it is well hidden).

This should be mentioned in the API docs (presumably under streaming concepts) and the application upgrading documentation."	FLINK	Closed	2	4	10269	pull-request-available
13242958	Add a type parser utility	For both the SQL Client YAML files as well as future type annotations, we need a parser that can create logical types from strings. It is the reverse operation of {{org.apache.flink.table.types.logical.LogicalType#asSerializableString}}.	FLINK	Closed	3	7	10269	pull-request-available
12965022	Add a TableSink for Elasticsearch	Add a TableSink that writes data to Elasticsearch	FLINK	Resolved	3	2	10269	pull-request-available
13165840	Allow to define an auto watermark interval in SQL Client	Currently it is not possible to define an auto watermark interval in a non-programmatic way for the SQL Client.	FLINK	Resolved	3	4	10269	pull-request-available
13341001	FactoryUtil will give an incorrect error message when multiple factories fit the connector identifier	"I was playing with user-defined connectors when I found the following error message:

{code}
Caused by: org.apache.flink.table.api.ValidationException: Multiple factories for identifier 'odps' that implement 'org.apache.flink.table.factories.DynamicTableFactory' found in the classpath.

Ambiguous factory classes are:

java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
java.util.LinkedList
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:258)
	at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:370)
	... 71 more
{code}

This is caused by {{FactoryUtil.java}} line 265, where {{.map(f -> factories.getClass().getName())}} should be {{.map(f -> f.getClass().getName())}}"	FLINK	Closed	3	1	10269	pull-request-available
13366695	Update DynamicTableFactory.Context to use ResolvedCatalogTable	Update DynamicTableFactory.Context to use ResolvedCatalogTable.	FLINK	Closed	3	7	10269	pull-request-available
13164897	Parse 'integer' type as BigDecimal	"Currently,
{code:java}
FlinkTypeFactory.typeInfoToSqlTypeName(typeInfo: TypeInformation[_]) {code}
does not support _BigInteger_, and since this is default type returned by
{code:java}
JsonSchemaConverter.convert(String schema){code}
for all fields with type: _integer_ this can be problematic for anyone who wants to force values to be of type Integer or Long. 
In 1.5.0 it is possible to register stream with _BigInteger_ as a type, but using this field in __SELECT query will cause it to fail due to problem above.

 "	FLINK	Resolved	3	2	10269	pull-request-available
13300750	Ensure deterministic type inference extraction	The extracted type inference might differ depending on the used JVM. Since order of candidates matters when deriving an implicit type, we need to make the extraction deterministic.	FLINK	Closed	3	7	10269	pull-request-available
13381443	Drop usages of legacy planner in Scala shell	Remove references to {{flink-table-planner}} in the Scala shell.	FLINK	Closed	3	7	10269	pull-request-available
13351059	Support DataStream batch mode in StreamTableEnvironment	The batch mode of DataStream API should also trigger the batch mode in StreamTableEnvironment. So that both APIs have a consistent streaming/batch mode behavior.	FLINK	Closed	3	7	10269	pull-request-available
13410260	KafkaDynamicTableFactoryTest.testTableSourceWithKeyValueAndMetadata fails on Azure	"{code:java}
2021-11-05T11:17:50.6139382Z Nov 05 11:17:50 [INFO] Results:
2021-11-05T11:17:50.6139960Z Nov 05 11:17:50 [INFO] 
2021-11-05T11:17:50.6140979Z Nov 05 11:17:50 [ERROR] Failures: 
2021-11-05T11:17:50.6143015Z Nov 05 11:17:50 [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceWithKeyValueAndMetadata:355 expected:<org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource@655d9585> but was:<org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource@1d030551>
2021-11-05T11:17:50.6145725Z Nov 05 11:17:50 [INFO] 
2021-11-05T11:17:50.6146333Z Nov 05 11:17:50 [ERROR] Tests run: 329, Failures: 1, Errors: 0, Skipped: 42
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26018&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=6698"	FLINK	Closed	2	1	10269	test-stability
13384549	Consider ConfigOption fallback keys in FactoryUtil	We are currently not taking fallback keys into consideration when performing validation in FactoryUtil.	FLINK	Closed	3	7	10269	pull-request-available
13525719	CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy	"This issue is reported from the [user mail list|https://lists.apache.org/thread/y6fgzyx330omhkr40376knw8k4oczz3s].

The stacktrace is 
{code:java}
Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) {code}
The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is ""Missing""; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.
!screenshot-1.png|width=675,height=295!"	FLINK	Closed	3	1	10269	pull-request-available
13363463	Officially deprecate the legacy planner	"As discussed in https://lists.apache.org/thread.html/r0851e101e37fbab273775b6a252172c7a9f7c7927107c160de779831%40%3Cdev.flink.apache.org%3E we will perform the following steps in 1.13:

- Deprecate the `flink-table-planner` module
- Deprecate `BatchTableEnvironment` for both Java, Scala, and Python 
- Add a notice to release notes and documentation"	FLINK	Closed	3	7	10269	pull-request-available
13434519	Support the new type inference in Scala Table API aggregate functions	"Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used ""inline"". We should support them as well."	FLINK	Open	3	7	10269	pull-request-available, stale-assigned
13359278	Implement Schema, ResolvedSchema, SchemaResolver	Introduces the main classes and utilities around schema mentioned in FLIP-164.	FLINK	Closed	3	7	10269	pull-request-available
13370030	Allow casting between row and structured type	There are still some minor barriers that prevent using {{toDataStream}} to its full extent.	FLINK	Closed	3	7	10269	pull-request-available
13316171	Add Table.limit() which is the equivalent of SQL LIMIT	Currently, you can run a SQL query like {{select * FROM (VALUES 'Hello', 'CIAO', 'foo', 'bar') LIMIT 2;}} but you cannot run the equivalent in the Table API.	FLINK	Closed	3	1	10269	pull-request-available
13288321	Preserve nullability for nested types	"Currently, FlinkTypeFactory does not always preserve nullability attributes when handling nested types.

E.g. a table function that returns {{ROW<s STRING, sa ARRAY<STRING> NOT NULL>}} looses information to {{ROW<s STRING, sa ARRAY<STRING>>}}.

Same for built-in functions such as {{COLLECT}} which results in mismatches as mentioned in FLINK-14042."	FLINK	Closed	3	7	10269	pull-request-available
13389854	Expose a consistent GlobalDataExchangeMode	"The Table API makes the {{GlobalDataExchangeMode}} configurable via {{table.exec.shuffle-mode}}.

In Table API batch mode the StreamGraph is configured with {{ALL_EDGES_BLOCKING}} and in DataStream API batch mode {{FORWARD_EDGES_PIPELINED}}.

I would vote for unifying the exchange mode of both APIs so that complex SQL pipelines behave identical in {{StreamTableEnvironment}} and {{TableEnvironment}}. Also the feedback a got so far would make {{ALL_EDGES_BLOCKING}} a safer option to run pipelines successfully with limited resources.

[~lzljs3620320]
{quote}
The previous history was like this:
- The default value is pipeline, and we find that many times due to insufficient resources, the deployment will hang. And the typical use of batch jobs is small resources running large parallelisms, because in batch jobs, the granularity of failover is related to the amount of data processed by a single task. The smaller the amount of data, the faster the fault tolerance. So most of the scenarios are run with small resources and large parallelisms, little by little slowly running.

- Later, we switched the default value to blocking. We found that the better blocking shuffle implementation would not slow down the running speed much. We tested tpc-ds and it took almost the same time.
{quote}

[~dwysakowicz]
{quote}
I don't see a problem with changing the default value for DataStream batch mode if you think ALL_EDGES_BLOCKING is the better default option.
{quote}

In any case, we should make this configurable for DataStream API users and make the specific Table API option obsolete."	FLINK	Closed	3	7	10269	pull-request-available
13394413	Use consistent managed memory weights for StreamNode	"Managed memory that is declared on transformations via {{Transformation#declareManagedMemoryUseCaseAtOperatorScope(ManagedMemoryUseCase managedMemoryUseCase, int weight)}} should be declared using a weight.

Usually, a weight should be some kind of factor, however, in the table planner it is used a kibi byte value. This causes issues on the DataStream API side that sets it to {{1}} in {{org.apache.flink.streaming.runtime.translators.BatchExecutionUtils#applyBatchExecutionSettings}}."	FLINK	Closed	3	7	10269	pull-request-available
13318720	Calling ROW() in a UDF results in UnsupportedOperationException	"Given a UDF {{func}} that accepts a {{ROW(INT, STRING)}} as parameter, it cannot be called like this:
{code:java}
SELECT func(ROW(a, b)) FROM t{code}
while this works
{code:java}
SELECT func(r) FROM (SELECT ROW(a, b) FROM t){code}
 

The exception returned is:
{quote}org.apache.flink.table.api.ValidationException: SQL validation failed. null
{quote}
with an empty {{UnsupportedOperationException}} as cause."	FLINK	Closed	3	7	10269	pull-request-available
13274712	Map Flink's TypeInference to Calcite's interfaces	After a {{TypeInference}} is available (either through reflective extraction or manual definition), the information needs to be connected to Calcite's interfaces. In particular, {{SqlOperandTypeInference}}, {{SqlOperandTypeChecker}}, {{SqlReturnTypeInference}}.	FLINK	Closed	3	7	10269	pull-request-available
13180234	SQL Client table visualization mode does not update correctly	"The table visualization modes does not seem to update correctly.
When I run a query that groups and aggregates on a few (6) distinct keys, the client visualizes some keys multiple times. Also the aggregated values do not seem to be correct.
Due to the small number of keys, these get frequently updated."	FLINK	Resolved	3	1	10269	pull-request-available
13213922	SQL Client jar does not contain table-api-java	The SQL Client jar does not package {{flink-table-api-java}}.	FLINK	Closed	3	1	10269	pull-request-available
13443237	Flink table scala example does not including the scala-api jars	"Currently it seems the flink-scala-api, flink-table-api-scala-bridge is not including from the binary package[1]. However, currently the scala table examples seems not include the scala-api classes in the generated jar, If we start a standalone cluster from the binary distribution package and then submit a table / sql job in scala, it would fail due to not found the StreamTableEnvironment class.

 

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/dev/configuration/advanced/#anatomy-of-table-dependencies"	FLINK	Open	3	1	10269	pull-request-available, stale-assigned
13194067	Fix handling of retractions after clean up	"Our online Flink Job run about a week，job contain sql ：
{code:java}
select  `time`,  
        lower(trim(os_type)) as os_type, 
        count(distinct feed_id) as feed_total_view  
from  my_table 
group by `time`, lower(trim(os_type)){code}
 

  then occur NPE: 

 
{code:java}
java.lang.NullPointerException

at scala.Predef$.Long2long(Predef.scala:363)

at org.apache.flink.table.functions.aggfunctions.DistinctAccumulator.remove(DistinctAccumulator.scala:109)

at NonWindowedAggregationHelper$894.retract(Unknown Source)

at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.processElement(GroupAggProcessFunction.scala:124)

at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.processElement(GroupAggProcessFunction.scala:39)

at org.apache.flink.streaming.api.operators.LegacyKeyedProcessOperator.processElement(LegacyKeyedProcessOperator.java:88)

at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:202)

at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)

at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)

at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)

at java.lang.Thread.run(Thread.java:745)
{code}
 

 

View DistinctAccumulator.remove
 !image-2018-10-25-14-46-03-373.png!

 

this NPE should currentCnt = null lead to, so we simple handle like :
{code:java}
def remove(params: Row): Boolean = {
  if(!distinctValueMap.contains(params)){
    true
  }else{
    val currentCnt = distinctValueMap.get(params)
    // 
    if (currentCnt == null || currentCnt == 1) {
      distinctValueMap.remove(params)
      true
    } else {
      var value = currentCnt - 1L
      if(value < 0){
        value = 1
      }
      distinctValueMap.put(params, value)
      false
    }
  }
}{code}
 

Update:

Because state clean up happens in processing time, it might be
 the case that retractions are arriving after the state has
 been cleaned up. Before these changes, a new accumulator was
 created and invalid retraction messages were emitted. This
 change drops retraction messages for which no accumulator
 exists. "	FLINK	Closed	4	1	10269	pull-request-available
13328712	"AggregateITCase.testListAggWithDistinct failed with ""expected:<List(1,A, 2,B, 3,C#A, 4,EF)> but was:<List(1,A, 2,B, 3,C#A, 4,EF#EF)>"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6734&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29

{code}
2020-09-22T04:59:30.1229430Z [ERROR] testListAggWithDistinct[LocalGlobal=ON, MiniBatch=ON, StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase)  Time elapsed: 0.346 s  <<< FAILURE!
2020-09-22T04:59:30.1230120Z java.lang.AssertionError: expected:<List(1,A, 2,B, 3,C#A, 4,EF)> but was:<List(1,A, 2,B, 3,C#A, 4,EF#EF)>
2020-09-22T04:59:30.1232835Z 	at org.junit.Assert.fail(Assert.java:88)
2020-09-22T04:59:30.1233314Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-09-22T04:59:30.1233688Z 	at org.junit.Assert.assertEquals(Assert.java:118)
2020-09-22T04:59:30.1234034Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-09-22T04:59:30.1234528Z 	at org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testListAggWithDistinct(AggregateITCase.scala:667)
{code}"	FLINK	Closed	1	1	10269	pull-request-available, test-stability
13543815	Disable check for single rowtime attribute for sinks	"A common error that users face is the following:
{code}
The query contains more than one rowtime attribute column [%s] for writing into table '%s'.
Please select the column that should be used as the event-time timestamp for the 
table sink by casting all other columns to regular TIMESTAMP or TIMESTAMP_LTZ.
{code}

However, not every sink requires the rowtime set on the stream record. For example, the Kafka sink does not use it because it exposes metadata columns. The user can define which column is the rowtime column by projection.

Either we introduce a config option for disabling this check. Or we come up with an interface that a connector can implement. I would vote for the config option as an easy solution to get rid of this error message."	FLINK	Closed	3	4	10269	pull-request-available
13089438	Merge the flink-java8 project into flink-core	This issue removes the flink-java8 module and merges some tests into flink-core/flink-runtime. It ensures to have the possibility for passing explicit type information in DataStream API as a fallback. Since the tycho compiler approach was very hacky and seems not to work anymore, this commit also removes all references in the docs and quickstarts.	FLINK	Resolved	3	4	10269	pull-request-available
13288274	Support new type inference for temporal table functions	Temporal table functions have not been updated to the new type inference yet.	FLINK	Closed	3	7	10269	pull-request-available
13172033	Add a Kafka table sink factory	FLINK-8866 implements a unified way of creating sinks and using the format discovery for searching for formats (FLINK-8858). It is now possible to add a Kafka table sink factory for streaming environment that uses the new interfaces.	FLINK	Resolved	3	2	10269	pull-request-available
13296442	Add a changeflag to Row type	"In Blink planner, the change flag of records travelling through the pipeline are part of the record itself but not part of the logical schema. This simplifies the architecture and API in many cases.

Which is why we aim adopt the same mechanism for {{org.apache.flink.types.Row}}.

Take {{tableEnv.toRetractStream()}} as an example that returns either Scala or Java {{Tuple2<Boolean, Row>}}. For FLIP-95 we need to support more update kinds than just a binary boolean.

This means:
- Add a changeflag {{RowKind}} to to {{Row}}
- Update the {{Row.toString()}} method
- Update serializers in backwards compatible way"	FLINK	Closed	3	7	10269	pull-request-available
13339907	Remove RemoteFunctionStateMigrator code paths from StateFun 	"The {{RemoteFunctionStateMigrator}} was added to allow savepoints with versions <= 2.1.0 to have a migration path for upgrading to versions >= 2.2.0. The binary format of remote function state was changed due to demultiplexed remote state, introduced in 2.2.0.

With 2.2.0 already released with the new formats, it is now safe to fully remove this migration path.

For users, what this means that it would not be possible to directly upgrade from 2.0.x / 2.1.x to 2.3.x+. They'd have to perform incremental upgrades via 2.2.x, by restoring first with 2.2.x and then taking another savepoint, before upgrading to 2.3.x."	FLINK	Closed	1	4	10285	pull-request-available
13196182	Update TypeSerializerSnapshotMigrationTestBase and subclasses for 1.7	Update {{TypeSerializerSnapshotMigrationTestBase}} and subclasses to cover restoring from Flink 1.7.	FLINK	Closed	3	7	10285	pull-request-available
13325275	Simplify PersistedStateRegistry state registration methods	"Currently, to register for example a {{PersistedValue}} with a {{PersistedStateRegistry}}, you create it by doing {{registry.registerValue(name, type, expiration)}}.

This leads to duplicate signatures with the state class constructors, and therefore requires separate synchronization across the methods.

We should change the syntax to accept a {{PersistedValue}} object directly, to simplify things."	FLINK	Closed	3	4	10285	pull-request-available
13366099	Add a Java SDK showcase tutorial to StateFun playground	"This goal of the showcase project is intended for new StateFun users that would like to start implementing their StateFun application functions using Java (or any other JVM language).

The tutorial should be streamlined and split into a few parts which we recommend to go through a specific order.

Each part can demonstrate with some code snippets plus Javadocs and comments to guide new users through the SDK fundamentals."	FLINK	Closed	3	4	10285	pull-request-available
13291803	statefun-quickstart generates invalid pom.xml	"Generating a stateful functions via the mvn archetype statefun-quickstart

doesn't generate a valid pom.xml,

steps to reproduce:

1)
{code:java}
 mvn archetype:generate                    \
  -DarchetypeGroupId=org.apache.flink \
  -DarchetypeArtifactId=statefun-quickstart \
  -DarchetypeVersion=1.1-SNAPSHOT 

{code}
2) 
{code:java}
cd <generated project>
mvn clean install {code}"	FLINK	Closed	3	1	10285	pull-request-available
13527557	KafkaSink failed to commit transactions under EXACTLY_ONCE semantics	"When KafkaSink starts Exactly once and no data is written to the topic during a checkpoint, the transaction commit exception is triggered, with the following exception.

[Transiting to fatal error state due to org.apache.kafka.common.errors.InvalidTxnStateException: The producer attempted a transactional operation in an invalid state.]"	FLINK	Closed	1	1	10285	pull-request-available
13193141	End-to-end test: Take & resume from savepoint when using per-job mode	Cancel with savepoint a job running in per-job mode (using the {{StandaloneJobClusterEntrypoint}}) and resume from this savepoint. This effectively that FLINK-10309 has been properly fixed.	FLINK	Closed	3	7	10285	pull-request-available, stale-assigned
13327791	Validate Stateful Functions configuration only where necessary	"Currently, the Flink configuration is validated to contain necessary settings, such as parent-first classloading patterns whereever {{StatefulFunctionsConfig}} is instantiated.

This validation should not be part of creating a {{StatefulFunctionsConfig}}, and should be refactored out as a separate utility method that is called only on necessary execution paths."	FLINK	Closed	3	4	10285	pull-request-available
13291213	Reword Stateful Functions doc's tagline 	"The current tagline is ""A framework for stateful distributed applications by the original creators of Apache Flink®.""

The part about ""by the original creators of Apache Flink"" reads a bit out-of-place now, since the project is now maintained by the Apache Flink community."	FLINK	Closed	4	4	10285	pull-request-available
13295182	Disable the source-release-assembly execution goal when using the apache-release build profile	"The {{apache-release}} profile defined in the Apache Parent POM defines a {{source-release-assembly}} execution that packages a source release distribution to be published to Maven.

We should disable this, because we use our own tools to package source release distributions."	FLINK	Closed	3	4	10285	pull-request-available
13282891	JDK 8 and 11 CI build pipeline for Stateful Functions	"If we choose to use Travis, setting this up should be fairly straightforward.

On the other hand, if we choose to use Azure Pipelines to be consistent with the community's recent discussion to migrate from Travis to Azure Pipelines for {{apache/flink}}, things will be a bit more complicated.
As with {{apache/flink}}'s integration with Azure Pipelines, there is a problem with Azure Pipelines requiring write access for a proper integration, which ASF does not allow.
Therefore, for Stateful Functions we also need to follow the approach that all branches / PR are mirrored to a CI repository not managed by ASF, on which Azure Pipelines monitors.
The proposed concrete steps for setting this up for Stateful Functions with Azure Pipelines is as follows:

# Add a minimal {{azure-pipelines.yml}} to {{apache/flink-statefun}} that runs containerized builds with JDK 8 and JDK 11. Stateful Functions should not require a custom container for the CI build environment, for now. The official Maven images {{maven:3.x.x-jdk-8/11}} should be sufficient for our current needs.
# Request a new ""ci repository"" at {{flink-ci/flink-statefun}}. Setup Azure Pipelines for the ci repository.
# Setup a CIBot instance to observe on {{apache/flink-statefun}}, replicate to {{flink-ci/flink-statefun}}, and reports back build checks to {{apache/flink-statefun}}. We should be able to reuse {{flink-ci/ci-bot}}, probably with a change that allows it to run without a Travis auth token (because Stateful Functions would not have Travis integration from the beginning).
# Update project wiki with details on the setup.

We need to reach out to Ververica for steps 2. and 3, since the current CI repo {{flink-ci}} and the CIBot instances are managed by them."	FLINK	Closed	1	7	10285	pull-request-available
13374976	Make StateFun build with Java 11	StateFun is currently not building with Java 11 due to the removal of javax.annotations in JDK 11. This should be fixable by manually adding the dependency.	FLINK	Closed	3	4	10285	pull-request-available
13366631	Update StateFun version to 3.0-SNAPSHOT	"Our version is still on 2.3-SNAPSHOT in the main repository, since directly jumping to 3.0-SNAPSHOT was something that was decided during the development cycle.

To prepare for the upcoming release, we should update the main branch to 3.0-SNAPSHOT already."	FLINK	Closed	3	4	10285	pull-request-available
13159665	Remove writing serializers as part of the checkpoint meta information	"When writing meta information of a state in savepoints, we currently write both the state serializer as well as the state serializer's configuration snapshot.

Writing both is actually redundant, as most of the time they have identical information.
 Moreover, the fact that we use Java serialization to write the serializer and rely on it to be re-readable on the restore run, already poses problems for serializers such as the {{AvroSerializer}} (see discussion in FLINK-9202) to perform even a compatible upgrade.

The proposal here is to leave only the config snapshot as meta information, and use that as the single source of truth of information about the schema of serialized state.
 The config snapshot should be treated as a factory (or provided to a factory) to re-create serializers capable of reading old, serialized state."	FLINK	Resolved	2	7	10285	pull-request-available
13302819	Add more end-to-end tests for Stateful Functions	"Umbrella ticket to track adding a few more end-to-end tests for Stateful Functions.
We should add the end-to-end automated tests for verifications that we manually and repeatedly perform for releases (e.g. failure recovery, savepoint bootstrapping, etc.)"	FLINK	Closed	1	4	10285	pull-request-available
13307951	Revert manual merging of AWS KPL's THIRD_PARTY_NOTICE files content in Stateful Functions distribution artifact	"We manually merged the contents in FLINK-16901, because at the time the upstream Flink Kinesis connector wasn't yet properly handling the content.

Now that this is fixed upstream, we can revert the fix in StateFun."	FLINK	Closed	3	4	10285	pull-request-available
13307608	Do not multiplex remote function state into single PersistedTable	"We are currently multiplexing multiple remote function's user value states into a single {{PersistedTable}}, using the state name as the table key.

This is not nice since:
- It does not allow individual states to have different properties, such as TTL expiration.
- We are restricted to only value states for remote functions"	FLINK	Closed	3	4	10285	pull-request-available
13325839	Expose backpressure metrics / logs for function dispatcher operator	"As of now, there is no visibility on why or how backpressure is applied in Stateful Functions.
This JIRA attemps to add two metrics as an initial effort of providing more visibility:
- Total number of addresses that have asked to be blocked
- Total number of inflight pending async operations"	FLINK	Closed	3	4	10285	pull-request-available, stale-assigned
13342515	RequestReplyFunction should not silently ignore UNRECOGNIZED state value mutations types	"If a function's response has a {{PersistedValueMutation}} type that is {{UNRECOGNIZED}}, we currently just silently ignore that mutation:
https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-core/src/main/java/org/apache/flink/statefun/flink/core/reqreply/PersistedRemoteFunctionValues.java#L84

This is incorrect. The {{UNRECOGNIZED}} enum constant is a pre-defined constant used by the Protobuf Java SDK, to represent a constant that was unable to be deserialized (because the the serialized constant does not match any enums defined in the protobuf message).

Therefore, it should be handled by throwing an exception, preferably indicating that there is some sort of version mismatch between the function's Protobuf message definitions, and StateFun's Protobuf message definitions (i.e. most likely a mismatch in the invocation protocol versions)."	FLINK	Closed	4	1	10285	auto-unassigned, pull-request-available
13172875	End-to-end test: Elasticsearch 6.x connector	We have decided to try and merge the pending Elasticsearch 6.x PRs. This should also come with an end-to-end test that covers this.	FLINK	Resolved	1	7	10285	pull-request-available
13341818	Extend invocation protocol to allow functions to indicate incomplete invocation context	"Currently, users declare the states a function will access with a module YAML definition file. The modules are loaded once when starting a StateFun cluster, meaning that the state specifications remain static throughout the cluster's execution lifetime.

We propose that state specifications should be declared by the function themselves via the language SDKs, instead of being declared in the module YAMLs.

The state specifications, now living in the functions, can be made discoverable by the StateFun runtime through the invocation request-reply protocol.

Brief simplified sketch of the extended protocol:
- StateFun dispatches an invocation request, with states [A, B].
- Function receives request, but since it requires states [A, B, C, D], it responds with a {{IncompleteInvocationContext}} response that indicates state values for [C, D] is missing.
- StateFun receives this response, and registers new Flink state handles for [C, D].
- Finally, a new invocation request with the same input messages, but ""patched"" with new states to contain all values for [A, B, C, D] is resent to the function.

This JIRA only targets updating the Protobuf messages {{ToFunction}} and {{FromFunction}} to fulfill the extended protocol, and support handling {{IncompleteInvocationContext}} responses in the request dispatcher.

Updating SDKs should be separate subtask JIRAs."	FLINK	Closed	3	7	10285	pull-request-available
13531496	No suitable constructor found for DebeziumAvroSerializationSchema	"{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-kafka: Compilation failure
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java:[939,16] no suitable constructor found for DebeziumAvroSerializationSchema(org.apache.flink.table.types.logical.RowType,java.lang.String,java.lang.String,<nulltype>)
Error:      constructor org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroSerializationSchema.DebeziumAvroSerializationSchema(org.apache.flink.table.types.logical.RowType,java.lang.String,java.lang.String,java.lang.String,java.util.Map<java.lang.String,?>) is not applicable
Error:        (actual and formal argument lists differ in length)
Error:      constructor org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroSerializationSchema.DebeziumAvroSerializationSchema(org.apache.flink.formats.avro.AvroRowDataSerializationSchema) is not applicable
Error:        (actual and formal argument lists differ in length)
Error:  -> [Help 1]
Error:  
Error:  To see the full stack trace of the errors, re-run Maven with the -e switch.
Error:  Re-run Maven using the -X switch to enable full debug logging.
Error:  
Error:  For more information about the errors and possible solutions, please read the following articles:
Error:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
Error:  
Error:  After correcting the problems, you can resume the build with the command
Error:    mvn <args> -rf :flink-connector-kafka
Error: Process completed with exit code 1.
{code}

https://github.com/apache/flink-connector-kafka/actions/runs/4610715024/jobs/8149513647#step:13:153"	FLINK	Closed	2	7	10285	pull-request-available, test-stability
13388111	Support pluggable / extendable HTTP transport for StateFun remote invocations	"For some of our advanced users, it is required for them to use their own HTTP client implementations for remote function invocations.

Towards that end, we'd like to support a generic way to plugin custom implementations, with the HTTP client being one of the initially supported extensions.

This includes a few separate sub-tasks that will be added under this ticket."	FLINK	Closed	3	4	10285	pull-request-available, stale-assigned
13282826	Building Stateful Functions with JDK 11 fails due to outdated Spotbugs version	"When building with JDK 11, the build fails with this:
{code}
 [java] WARNING: An illegal reflective access operation has occurred
 [java] WARNING: Illegal reflective access by org.dom4j.io.SAXContentHandler (file:/home/shannon/ws/javaxmail-spotbugs/.m2/repository/dom4j/dom4j/1.6.1/dom4j-1.6.1.jar) to method com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser$LocatorProxy.getEncoding()
 [java] WARNING: Please consider reporting this to the maintainers of org.dom4j.io.SAXContentHandler
 [java] WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
 [java] WARNING: All illegal access operations will be denied in future versions
{code}

According to https://github.com/spotbugs/spotbugs/issues/499, this is because the current Spotbugs version we use (3.1.1) uses an outdated Dom4j version.

Upgrading to the latest Spotbugs stable version (3.1.12) fixes this issue."	FLINK	Closed	3	1	10285	pull-request-available
13202463	Remove redundant code path in CompatibilityUtil	"In {{CompatibilityUtil#resolveCompatibilityResult}}:
{code:java}
CompatibilityResult<T> initialResult = resolveCompatibilityResult(
     (TypeSerializerSnapshot<T>) precedingSerializerConfigSnapshot,
     newSerializer);

if (!initialResult.isRequiresMigration()) {
    return initialResult;
} else {
     if (precedingSerializer != null && !(precedingSerializer.getClass().equals(dummySerializerClassTag))) {         
         // if the preceding serializer exists and is not a dummy, use
         // that for converting instead of any provided convert deserializer
        return CompatibilityResult.requiresMigration((TypeSerializer<T>) precedingSerializer);
     } else {
        // requires migration (may or may not have a convert deserializer)
        return initialResult;
}

{code}
{{initialResult}} can actually be returned already, since the case where we try to provide a convert deserializer is no longer relevant. We do not need to check if a convert deserializer is provided because that will never be the case."	FLINK	Closed	3	4	10285	pull-request-available
13208327	RocksDBListState should use actual registered state serializer instead of serializer provided by descriptor	"Currently, when creating a {{RocksDBListState}}, the element serializer wrapped by the {{RocksDBListState}} is retrieved from the state descriptor:

{code:java}
static <E, K, N, SV, S extends State, IS extends S> IS create(
    StateDescriptor<S, SV> stateDesc,
    Tuple2<ColumnFamilyHandle, RegisteredKeyValueStateBackendMetaInfo<N, SV>> registerResult,
    RocksDBKeyedStateBackend<K> backend) {

    return (IS) new RocksDBListState<>(
        registerResult.f0,
        registerResult.f1.getNamespaceSerializer(),
        (TypeSerializer<List<E>>) registerResult.f1.getStateSerializer(),
        (List<E>) stateDesc.getDefaultValue(),
        ((ListStateDescriptor<E>) stateDesc).getElementSerializer(), // incorrect
        backend);
}
{code}

This is incorrect, since new serializers retrieved from state descriptors have not been checked for compatibility with restored state.

Instead, the element serializer should be retrieved from the register result, which contains the actual state serializer registered in the state backend for state access (and has already been checked for compatibility / reconfigured appropriately)."	FLINK	Closed	2	1	10285	pull-request-available
13283940	Document the release process for Stateful Functions in the community wiki	The release process for Stateful Functions should be documented under https://cwiki.apache.org/confluence/display/FLINK/Releasing.	FLINK	Closed	2	4	10285	stale-assigned
13287436	Add typed builder methods for setting dynamic configuration on StatefulFunctionsAppContainers	"Excerpt from: https://github.com/apache/flink-statefun/pull/32#discussion_r383644382

Currently, you'd need to provide a complete {{Configuration}} as dynamic properties when constructing a {{StatefulFunctionsAppContainers}}.

It'll be nicer if this is built like this:
{code}
public StatefulFunctionsAppContainers verificationApp =
    new StatefulFunctionsAppContainers(""sanity-verification"", 2)
        .withModuleGlobalConfiguration(""kafka-broker"", kafka.getBootstrapServers())
        .withConfiguration(ConfigOption option, configValue)
{code}

And by default the {{StatefulFunctionsAppContainers}} just only has the configs in the base template {{flink-conf.yaml}}.

This would require lazy construction of the containers on {{beforeTest}}."	FLINK	Closed	3	2	10285	pull-request-available
13249270	Update upgrade compatibility table (docs/ops/upgrading.md) for 1.9.0	Update upgrade compatibility table (docs/ops/upgrading.md) for 1.9.0	FLINK	Closed	1	4	10285	pull-request-available
13342173	Add a SmokeE2E test	"We need an E2E test that mimics random stateful function applications, that creates random failures.

This test should also verify that messages and state are consistent.

This test should be run:
 # in a dockerized environment (for example via test containers)
 # via the IDE (in a mini cluster) for debuggability.

 

 "	FLINK	Closed	3	4	10285	pull-request-available
13290728	Move Kafka client properties resolution in KafkaSinkProvider to KafkaEgressBuilder	"We've moved Kafka client properties resolution from the {{KafkaSourceProvider}} to be consolidated in the {{KafkaIngressBuilder}} in FLINK-15769.

The same should also be done for the producer side, i.e. for {{KafkaSinkProvider}}, by moving the properties resolution logic to {{KafkaEgressBuilder}}."	FLINK	Closed	4	4	10285	auto-unassigned
13290700	Stateful Function's KafkaSinkProvider should use `setProperty` instead of `put` for resolving client properties	"The {{put}} method is strongly discourage to be used on {{Properties}} as a bad practice, since it allows putting non-string values.

This has already caused a bug, where a long was put into the properties, while Kafka was expecting an integer:
{code}
org.apache.kafka.common.config.ConfigException: Invalid value 100000 for configuration transaction.timeout.ms: Expected value to be a 32-bit integer, but it was a java.lang.Long
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:669)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:471)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:464)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:62)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:75)
	at org.apache.kafka.clients.producer.ProducerConfig.<init>(ProducerConfig.java:396)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:326)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:298)
	at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.<init>(FlinkKafkaInternalProducer.java:76)
{code}"	FLINK	Closed	1	1	10285	pull-request-available
13285867	Add a AWS Kinesis Stateful Functions Ingress	"AWS Kinesis is also a popularly used sources for Apache Flink applications, given their capability to reset their consumer position to a specific offset that works well with Flink's fault-tolerance model.
This also applies to Stateful Functions, and having a shipped ingress for Kinesis supported will also ease the use of Stateful Functions for AWS users."	FLINK	Closed	3	2	10285	pull-request-available
13329339	Update flink-statefun-docker release scripts for cross release Java 8 and 11	"Currently, the {{add-version.sh}} script in the {{flink-statefun-docker}} repo does not generate Dockerfiles for different Java versions.
Since we have decided to cross-release images for Java 8 and 11, that script needs to be updated as well."	FLINK	Closed	1	2	10285	pull-request-available
13282659	Revisit Stateful Functions KafkaIngressBuilder properties resolution logic	"Currently, the properties resolution logic in {{KafkaIngressBuilder}} is a bit inconsistent for different configurations, and some actually incorrect.
The problem is around the fact that we allow users to directly pass in {{Properties}} to configure the Kafka client, but also support named methods to set some important configs like Kafka address / auto offset reset position.

For example, we always overwrite {{auto.offset.reset}} set in the properties with the {{autoOffsetResetPosition}} value in the builder. This is correct when the user had actually passed in a value via the named method {{withAutoOffsetResetPosition}}, but incorrect otherwise.
The same goes for the Kafka address configuration.

This should be revisited, so that we have a common strategy with dealing with named configurations v.s. properties, with an end goal that:

* Configs passed via named methods should always overwrite the value set via properties
* Any default values for named configuration methods should be defined in the builder
* If no config was passed via its named method, then we use the default value (if any) to overwrite the properties IFF the user also did not provide a value for it there."	FLINK	Closed	3	1	10285	pull-request-available
13323956	Add HDFS / S3 / GCS support to Flink-StateFun Docker image.	The 3rd party filesystem support is not enabled by default. To make checkpointing work in s3 / gcs etc' we need to add the relevant plugins.	FLINK	Closed	3	4	10285	pull-request-available
13202800	Broadcast state migration Incompatibility from 1.5.3 to 1.7.0	"When upgrading from Flink 1.5.3 to Flink 1.7.0, the migration of broadcast state throws the following error:
{noformat}
org.apache.flink.util.StateMigrationException: The new key serializer for broadcast state must not be incompatible.
at org.apache.flink.runtime.state.DefaultOperatorStateBackend.getBroadcastState(DefaultOperatorStateBackend.java:238)
at org.apache.flink.streaming.api.operators.co.CoBroadcastWithNonKeyedOperator.open(CoBroadcastWithNonKeyedOperator.java:87)
at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:424)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:290)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:704)
at java.lang.Thread.run(Thread.java:745){noformat}
The broadcast is using a MapState with StringSerializer as key serializer and a custom JsonSerializer as value serializer. 

There was no changes in the TypeSerializers used, only upgrade of version. 

 

With some debugging I see that at the moment of the validation of the compatibility of states in the DefaultOperatorStateBackend class, the ""*registeredBroadcastStates*"" containing the data about the 'old' state, contains wrong association of the key and value serializer. This is, JsonSerializer appears as key serializer and StringSerializer appears as value serializer. (when it should be the contrary)

 

After more digging, I see that the ""OperatorBackendStateMetaInfoReaderV2V3"" class is the responsible of this swap here:
https://github.com/apache/flink/blob/release-1.7/flink-runtime/src/main/java/org/apache/flink/runtime/state/metainfo/LegacyStateMetaInfoReaders.java#L165"	FLINK	Closed	1	1	10285	Migration, State, broadcast, pull-request-available
13325299	Make StateBinder a per-FunctionType entity	"Currently, a single {{StateBinder}} instance is used across multiple {{FunctionType}} s for binding state objects to Flink state.

While this is fine, it creates a leak of internal runtime information to the {{PersistedStateRegistry}} class. To fix this, we need to make {{FunctionType}} a property of each individual state binder, etc. each state binder binds state for a single {{FunctionType}}."	FLINK	Closed	3	4	10285	pull-request-available
13076558	Inconsistent state migration behaviour between different state backends	"The {{MemoryStateBackend}}, {{FsStateBackend}} and {{RocksDBStateBackend}} show a different behaviour when it comes to recovery from old state and state migration. For example, using the {{MemoryStateBackend}} it is possible to recover pojos which now have additional fields (at recovery time). The only caveat is that the recovered {{PojoSerializer}} will silently drop the added fields when writing the new Pojo. In contrast, the {{RocksDBStateBackend}} correctly recognizes that a state migration is necessary and thus fails with a {{StateMigrationException}}. The same applies to the case where Pojo field types change. The {{MemoryStateBackend}} and the {{FsStateBackend}} accept such a change as long as the fields still have the same length. The {{RocksDBStateBackend}} correctly fails with a {{StateMigrationException}}.

I think that all state backends should behave similarly and give the user the same recovery and state migration guarantees. Otherwise, it could happen that jobs run with one state backend but not with another (wrt semantic behaviour)."	FLINK	Closed	2	1	10285	flink-rel-1.3.1-blockers
13213003	Migrate PojoSerializer to use new serialization compatibility abstractions	This subtask covers migration of the {{PojoSerializer}}.	FLINK	Closed	3	7	10285	pull-request-available
13078850	Serializer for collection of Scala case classes are generated with different anonymous class names in 1.3	"In the Scala API, serializers are generated using Scala macros (via the {{org.apache.flink.streaming.api.scala.createTypeInformation(..)}} util).
The generated serializers are inner anonymous classes, therefore classnames will differ depending on when / order that the serializers are generated.

From 1.1 / 1.2 to Flink 1.3, the generated classnames for a serializer for a collections of case classes (e.g. {{List[SomeUserCaseClass]}}) will be different. In other words, the exact same user code written in the Scala API, compiling it with 1.1 / 1.2 and with 1.3 will result in different classnames.

This is problematic for restoring older savepoints that have Scala case class collections in their state, because the old serializer cannot be recovered (due to the generated classname change).

For now, I've managed to identify that the root cause for this is that in 1.3 the {{TypeSerializer}} base class additionally extends the {{TypeDeserializer}} interface. Removing this extending resolves the problem. The actual reason for why this affects the generated classname is still being investigated."	FLINK	Closed	1	1	10285	flink-rel-1.3.1-blockers
13340618	Consider switching from Travis CI to Github Actions for flink-statefun's CI workflows	"Travis-CI.com recently announced a new pricing model on Nov. 2, which affects public open source projects: https://blog.travis-ci.com/2020-11-02-travis-ci-new-billing

While its a bit unclear if Travis CI repos under {{travis-ci.com/github/apache}} is affected by this, this will definitely affect contributors who fork our repositories and enable Travis CI on their fork for development purposes.

Github Actions seems to be a popular alternative nowadays:
* No limited test time with its hosted builders, if repo is public
* Activation is automatic - one step / click less for contributors to get CI running for their forks

Given that the CI workflows in {{flink-statefun}} is very minimal right now, we propose to make the switch to Github Actions as the efforts to do that should be relatively trivial."	FLINK	Closed	3	4	10285	pull-request-available
13217770	Remove implementations of deprecated TypeSerializer#ensureCompatibility()	"All remaining implementations of the deprecated {{TypeSerializer#ensureCompatibility}} method should be removed.

Some previously may not have been removed yet because of various limitations causing the old snapshot class not being able to redirect schema compatibility checks to new snapshot class (e.g. the old snapshot class was shared across serializers).

These problems should be addressable now with the auxiliary {{SelfResolvingTypeSerializer}} / {{LegacySerializerSnapshotTransformer}} interfaces."	FLINK	Closed	3	4	10285	pull-request-available
13304013	Consolidate flink.version in Stateful Functions POM files	Right now, we have definitions of {{flink.version}} property scattered in multiple POM files in Stateful Functions. They should be consolidated in the root parent POM, as that should not likely change across modules.	FLINK	Closed	4	4	10285	pull-request-available
13342512	Remove support for eager state specifications in module YAML definitions	"With FLINK-20265, we now support declaring state in StateFun functions, and that can change dynamically without any system downtime.

It can be confusing for users if we continued to support the legacy way of statically declaring state specifications in the module YAML definitions.

Therefore, we propose to completely remove that by:
* No longer support module YAML format versions <= 2.0.
* Remove the {{PersistedRemoteFunctionValues}} constructor that accepts a list of eager state specifications

This would be a breaking change:
* Users upgrading to version 2.3.0 have to rewrite their module YAMLs to conform to format version 3.0
* They also have to correspondingly update their functions to use SDKs of version 2.3.0."	FLINK	Closed	3	7	10285	pull-request-available
13289103	Rename Protobuf Kafka Ingress / Routable Kafka Ingress type identifier strings	"Currently the strings for the ingress type identifier for the (Routable) Kafka ingresses are:
{{org.apache.flink.statefun.sdk.kafka/(routable-)protobuf-kafka-connector}}

The term {{connector}} is better renamed as ""ingress"", to be consistent with Stateful Functions terminology.
Also, we could maybe consider shortening the namespace string ({{org.apache.flink.statefun.sdk.kafka}}) to something more compact."	FLINK	Closed	1	4	10285	pull-request-available
13294595	Stateful Functions Quickstart archetype Dockerfile should reference a specific version tag	"Currently, the quickstart archetype provides a skeleton Dockerfile that always builds on top of the latest image:
{code}
FROM statefun
{code}

While it happens to work for the first ever release since the {{latest}} tag will (coincidentally) point to the correct version,
once we have multiple releases this will no longer be correct."	FLINK	Closed	2	1	10285	pull-request-available
13302827	Extend CompositeTypeSerializerSnapshot to allow composite serializers to signal migration based on outer configuration	"Compatibility of composite serializers is governed by the overall resolved compatibility of all its nested serializers, as well as any additional configuration (or what we call the ""outer configuration"" or ""outer snapshot"").

The compatibility resolution logic for these composite serializers is implemented in the {{CompositeTypeSerializerSnapshot}} abstract class.

One current limitation of this base class is that the implementation assumes that the outer configuration is always either compatible, or incompatible.

We should relax this to also allow signaling migration, purely based on the outer configuration. This is already a requirement by FLINK-16998."	FLINK	Closed	2	4	10285	pull-request-available
13376388	Rebase StateFun on Flink 1.13	Following the recent release of Flink 1.13, StateFun main branch needs to be rebased on that version.	FLINK	Resolved	3	4	10285	pull-request-available
13325450	Add more timeout options for remote function specs	"As of now, we only support setting the call timeout for remote functions, which spans a complete call including connection, writing request, server-side processing, and reading response times.

To allow more fine-grained control of this, we propose to introduce configuration keys for {{connectTimeout}} / {{readTimeout}} / {{writeTimeout}} to remote function specs.
By default, these values should be 10 to be coherent with the current behaviour."	FLINK	Closed	3	4	10285	pull-request-available
13145688	Add a test operator with keyed state that uses Kryo serializer (registered/unregistered/custom)	Add an operator with keyed state that uses Kryo serializer (registered/unregistered/custom).	FLINK	Resolved	3	7	10285	pull-request-available
13341899	Update Python SDK to implement new invocation protocol	"The Python SDK should be updated to implement the new protocol introduced by FLINK-20265.

Users should be able to declare states the Python function uses, and with information, the Python SDK {{RequestReplyHandler}} can then match on the provided states values in invocation requests against the declared states of a target function:

Proposed API:
{code}
@functions.bind(
    typename=""example/greeter"",
    states=[StateSpec(""seen_count"")])
def greet(context, greet_request: GreetRequest):
    state = context.state(""seen_count"").unpack(SeenCount)
    ...
{code}"	FLINK	Closed	3	7	10285	pull-request-available
13328203	Add documentation for Stateful Function's Flink DataStream SDK	The new Flink DataStream integration SDK for Stateful Functions is still lacking documentation.	FLINK	Closed	3	4	10285	pull-request-available
13172364	Update end-to-end test to use RocksDB backed timers	We should add or modify an end-to-end test to use RocksDB backed timers.	FLINK	Resolved	1	7	10285	pull-request-available
13295189	Adapt Dockerfiles from flink-statefun to be added to flink-statefun-docker	"The scripts under {{tools/docker/}} in the flink-statefun repo contains scripts that prepare the build context for building snapshot StateFun images that we use for development purposes.

Those can be adapted so that they may be used for submission to the Docker Hub official images repo:
- The build context should be readily in shape as is in the Dockerfile repo, instead of relying on a build script 
- {{statefun-flink-distribution}} and {{statefun-flink-core}} jars should be downloaded from Maven + signature verification needs to be performed
- We need some tooling in place that allows us to automate the release workflow of adding a new version for the images. We should be able to adapt those from the existing Flink Dockerfiles repo."	FLINK	Closed	3	4	10285	pull-request-available
13347508	Skip deployment of StateFun example artifacts	"Starting from the next Stateful Functions release, we'd like to stop publishing Maven artifacts for the examples.

We never expect users to be trying out examples through this artifacts, and therefore releasing them is not required."	FLINK	Closed	1	1	10285	pull-request-available
13325459	Add basic checkpoint and recovery config keys to template flink-conf.yaml	"How to enable checkpointing in Stateful Functions seems to be a recurring question.
Adding the relevant configuration keys to the template flink-conf.yaml in StateFun's Docker images could help with this. "	FLINK	Closed	3	4	10285	pull-request-available
13342509	Introduce function endpoint path templating in module YAML specifications	"In the current module specifications, function endpoints are defined like so:

{code}
functions:
    - function:
          meta:
              kind: http
              type: com.foo/world
          spec:
              endpoint: http://localhost:5959/statefun
{code}

A list of functions and their corresponding service endpoints are listed statically in the module specification file, which is loaded once on system startup. The system may only route messages to functions that have been defined. This prevents users from adding new functions to the application, without having to restart the system and reload new module specifications.

We propose that instead of specifying functions, users should specify a ""family"" of function endpoints, like so:

{code}
functionEndpoints:
    - functionEndpoint:
        meta:
            kind: http
        spec:
            target:
                typename:
                    namespace: com.foo.bar
                    function: *  # (can be wildcard * or a specific name)
                urlPathTemplate: ""https://bar.foo.com:8000/{typename.function}""
            connectTimeout: 1min
            # ... (other connection related configs that is shared for this endpoint family)
{code}

Note how users no longer define eager state per individual function. This is made possible by FLINK-20265, where state is now defined in the functions instead of in the module specifications.

Function endpoint templating should only be supported in a new module specification format version (next version being {{3.0}}), where the previous way of defining individual functions is no longer supported."	FLINK	Closed	3	7	10285	pull-request-available
13291278	Allow empty keys in Kafka egress messages for Statefun Python SDK	"The generic YAML-ized Kafka egress does allow messages to not have keys defined.
However, the convenience Kafka egress messages builder in the Python SDK requires keys to always be set."	FLINK	Closed	3	4	10285	pull-request-available
13125659	FlinkKafkaConsumerBase failing on Travis with no output in 10min	"Since a few days, Travis builds with the {{connectors}} profile keep failing more often with no new output being received within 10 minutes. It seems to start with the Travis build for https://github.com/apache/flink/commit/840cbfbf0845b60dbf02dd2f37f696f1db21b1e9 but may have been introduced earlier. The printed offsets look strange though.

{code}
16:33:12,508 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:12,520 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 2 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=851}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=716}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=461}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=206}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=971}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=836}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=581}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=326}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=71}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=956}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=701}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=446}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=191}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=56}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=821}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=566}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=311}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=881}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=626}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=371}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=236}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=746}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=491}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=356}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=101}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=866}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=611}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=476}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=221}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=986}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=731}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=596}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=341}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=86}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=656}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=401}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=146}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=911}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=776}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=521}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=266}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=11}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=896}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=641}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=386}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=131}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=761}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=506}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=251}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=116}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=176}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=941}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=686}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=431}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=296}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=41}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=806}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=551}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=416}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=161}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=926}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=671}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=536}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=281}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=26}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=791}=-915623761775}

16:33:12,580 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:12,592 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 3 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=972}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=717}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=462}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=207}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=72}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=837}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=582}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=327}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=192}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=957}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=702}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=447}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=312}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=57}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=822}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=567}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=882}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=627}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=492}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=237}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=747}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=612}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=357}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=102}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=867}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=732}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=477}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=222}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=987}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=852}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=597}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=342}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=87}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=912}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=657}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=402}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=147}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=12}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=777}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=522}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=267}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=132}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=897}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=642}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=387}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=252}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=762}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=507}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=372}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=117}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=432}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=177}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=942}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=687}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=552}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=297}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=42}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=807}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=672}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=417}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=162}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=927}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=792}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=537}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=282}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=27}=-915623761775}

16:33:12,653 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:12,676 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 4 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=208}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=973}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=718}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=463}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=328}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=73}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=838}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=583}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=448}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=193}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=958}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=703}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=568}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=313}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=58}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=823}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=883}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=748}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=493}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=238}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=868}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=613}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=358}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=103}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=988}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=733}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=478}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=223}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=88}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=853}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=598}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=343}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=913}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=658}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=403}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=268}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=13}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=778}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=523}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=388}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=133}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=898}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=643}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=508}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=253}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=763}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=628}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=373}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=118}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=688}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=433}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=178}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=943}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=808}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=553}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=298}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=43}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=928}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=673}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=418}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=163}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=28}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=793}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=538}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=283}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=148}=-915623761775}

16:33:12,769 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:12,775 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 5 will start reading 66 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=464}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=209}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=974}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=719}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=584}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=329}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=74}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=839}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=704}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=449}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=194}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=959}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=824}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=569}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=314}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=59}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=749}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=494}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=239}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=104}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=869}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=614}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=359}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=224}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=989}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=734}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=479}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=344}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=89}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=854}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=599}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=914}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=659}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=524}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=269}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=14}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=779}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=644}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=389}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=134}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=899}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=764}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=509}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=254}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=884}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=629}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=374}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=119}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=944}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=689}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=434}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=179}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=44}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=809}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=554}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=299}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=164}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=929}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=674}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=419}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=284}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=29}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=794}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=539}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=404}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=149}=-915623761775}

16:33:12,852 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:12,855 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 6 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=720}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=465}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=210}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=975}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=840}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=585}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=330}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=75}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=960}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=705}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=450}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=195}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=60}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=825}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=570}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=315}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=180}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=240}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=750}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=495}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=360}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=105}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=870}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=615}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=480}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=225}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=990}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=735}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=600}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=345}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=90}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=855}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=915}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=780}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=525}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=270}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=15}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=900}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=645}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=390}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=135}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=765}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=510}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=255}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=120}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=885}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=630}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=375}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=945}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=690}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=435}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=300}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=45}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=810}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=555}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=420}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=165}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=930}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=675}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=540}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=285}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=30}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=795}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=660}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=405}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=150}=-915623761775}

16:33:12,968 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:12,973 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 7 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=976}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=721}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=466}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=211}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=76}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=841}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=586}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=331}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=196}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=961}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=706}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=451}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=316}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=61}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=826}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=571}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=436}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=181}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=496}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=241}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=751}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=616}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=361}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=106}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=871}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=736}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=481}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=226}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=991}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=856}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=601}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=346}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=91}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=16}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=781}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=526}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=271}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=136}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=901}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=646}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=391}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=256}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=766}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=511}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=376}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=121}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=886}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=631}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=946}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=691}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=556}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=301}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=46}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=811}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=676}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=421}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=166}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=931}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=796}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=541}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=286}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=31}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=916}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=661}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=406}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=151}=-915623761775}

16:33:13,037 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:13,038 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 8 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=977}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=722}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=467}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=332}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=77}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=842}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=587}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=452}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=197}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=962}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=707}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=572}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=317}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=62}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=827}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=692}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=437}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=182}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=752}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=497}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=242}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=872}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=617}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=362}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=107}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=992}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=737}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=482}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=227}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=92}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=857}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=602}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=347}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=212}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=272}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=17}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=782}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=527}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=392}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=137}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=902}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=647}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=512}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=257}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=2}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=767}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=632}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=377}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=122}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=887}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=947}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=812}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=557}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=302}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=47}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=932}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=677}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=422}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=167}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=32}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=797}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=542}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=287}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=152}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=917}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=662}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=407}=-915623761775}

16:33:13,110 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:13,115 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 9 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=978}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=723}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=588}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=333}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=78}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=843}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=708}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=453}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=198}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=963}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=828}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=573}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=318}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=63}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=948}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=693}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=438}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=183}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=753}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=498}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=243}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=108}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=873}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=618}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=363}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=228}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=993}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=738}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=483}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=348}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=93}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=858}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=603}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=468}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=213}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=528}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=273}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=18}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=783}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=648}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=393}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=138}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=903}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=768}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=513}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=258}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=3}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=888}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=633}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=378}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=123}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=48}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=813}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=558}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=303}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=168}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=933}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=678}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=423}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=288}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=33}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=798}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=543}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=408}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=153}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=918}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=663}=-915623761775}

16:33:13,186 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:13,190 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 10 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=979}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=844}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=589}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=334}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=79}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=964}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=709}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=454}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=199}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=64}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=829}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=574}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=319}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=184}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=949}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=694}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=439}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=754}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=499}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=364}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=109}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=874}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=619}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=484}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=229}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=994}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=739}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=604}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=349}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=94}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=859}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=724}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=469}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=214}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=784}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=529}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=274}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=19}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=904}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=649}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=394}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=139}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=4}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=769}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=514}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=259}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=124}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=889}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=634}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=379}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=244}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=304}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=49}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=814}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=559}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=424}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=169}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=934}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=679}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=544}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=289}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=34}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=799}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=664}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=409}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=154}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=919}=-915623761775}

16:33:13,255 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:13,261 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 11 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=80}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=845}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=590}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=335}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=200}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=965}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=710}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=455}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=320}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=65}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=830}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=575}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=440}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=185}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=950}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=695}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=755}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=620}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=365}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=110}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=875}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=740}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=485}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=230}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=995}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=860}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=605}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=350}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=95}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=980}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=725}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=470}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=215}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=785}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=530}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=275}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=140}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=905}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=650}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=395}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=260}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=5}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=770}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=515}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=380}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=125}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=890}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=635}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=500}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=245}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=560}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=305}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=50}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=815}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=680}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=425}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=170}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=935}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=800}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=545}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=290}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=35}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=920}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=665}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=410}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=155}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=20}=-915623761775}

16:33:13,316 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:13,321 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 12 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=336}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=81}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=846}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=591}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=456}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=201}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=966}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=711}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=576}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=321}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=66}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=831}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=696}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=441}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=186}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=951}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=876}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=621}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=366}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=111}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=996}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=741}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=486}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=231}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=96}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=861}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=606}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=351}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=216}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=981}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=726}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=471}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=786}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=531}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=396}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=141}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=906}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=651}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=516}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=261}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=6}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=771}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=636}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=381}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=126}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=891}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=756}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=501}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=246}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=816}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=561}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=306}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=51}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=936}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=681}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=426}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=171}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=36}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=801}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=546}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=291}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=156}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=921}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=666}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=411}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=276}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=21}=-915623761775}

16:33:13,385 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:13,388 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 13 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=592}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=337}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=82}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=847}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=712}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=457}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=202}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=967}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=832}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=577}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=322}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=67}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=952}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=697}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=442}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=187}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=52}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=112}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=877}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=622}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=367}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=232}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=997}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=742}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=487}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=352}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=97}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=862}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=607}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=472}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=217}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=982}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=727}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=787}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=652}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=397}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=142}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=907}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=772}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=517}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=262}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=7}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=892}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=637}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=382}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=127}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=757}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=502}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=247}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=817}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=562}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=307}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=172}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=937}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=682}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=427}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=292}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=37}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=802}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=547}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=412}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=157}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=922}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=667}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=532}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=277}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=22}=-915623761775}

16:33:13,449 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Setting restore state in the FlinkKafkaConsumer: {KafkaTopicPartition{topic='test-topic', partition=0}=-915623761773, KafkaTopicPartition{topic='test-topic', partition=1}=-915623761773}

16:33:13,453 INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase  - Consumer subtask 14 will start reading 67 partitions with offsets in restored state: {KafkaTopicPartition{topic='test-topic', partition=848}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=593}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=338}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=83}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=968}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=713}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=458}=-915623761775, KafkaTopicPartition{topic='test-topic', partition=203}=-915623761775, KafkaTopicPartition{topic='tee: standard output: Resource temporarily unavailable

No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.

Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received

The build has been terminated
{code}
https://travis-ci.org/apache/flink/jobs/316399980

Some more instances:
- https://travis-ci.org/apache/flink/jobs/316414133
- https://travis-ci.org/apache/flink/jobs/316545759
- https://travis-ci.org/apache/flink/jobs/316804998
- "	FLINK	Resolved	2	1	10285	test-stability
13379814	Add robust default state configuration to StateFun	"We aim to reduce the state configuration complexity by applying a default configuration with robust settings, based on lessons learned in Flink.

*(1) Always use RocksDB.*

_That is already the case._

We keep this for now, as long as the only other alternative are backends with Objects on the heap, which are tricky in terms of predictable JVM performance. RocksDB has a significant performance cost, but more robust behavior.

*(2) Activate local recovery by default.*

That makes recovery cheao for soft tasks failures and gracefully cancelled tasks.
We need to set these options:
  - {{state.backend.local-recovery: true}}
  - {{taskmanager.state.local.root-dirs: <dir>}} - some local directory that will not possibly be wiped by the OS periodically, so typically some local directory that is not {{/tmp}}, for example {{/local/state/recovery}}.
  - {{state.backend.rocksdb.localdir: <dir>}} - a directory on the same FS / device as above, so that one can create hard links between them (required for RocksDB local checkpoints), for example {{/local/state/rocksdb}}.

Flink will most likely adopt this as a default setting as well in the future.
It still makes sense to pre-configer a different RocksDB working directory than {{/tmp}}.

*(3) Activate partitioned indexes by default.*

This may cost minimal performance in some cases, but can avoid massive performance regression in cases where the index blocks no longer fit into the memory cache (may happen more frequently when there are too many ColumnFamilies = states).

Set {{state.backend.rocksdb.memory.partitioned-index-filters: true}}.

See FLINK-20496 for details.

*(4) Increase number of transfer threads by default.*

This speeds up state recovery in many cases. The default value in Flink is a bit conservative, to avoid spamming DFS (like HDFS) by default. The more cloud-centric StateFun setups should be safe to use higher default value.

Set {{state.backend.rocksdb.checkpoint.transfer.thread.num: 8}}.

*(5) Increase RocksDB compaction threads by default.*

The number of RocksDB compaction threads is frequently a bottleneck.
Increasing it costs virtually nothing and mitigates that bottleneck in most cases.

{{state.backend.rocksdb.thread.num: 4}}

_(this value is chosen under the assumption that there is only one slot per TM in StateFun)._
"	FLINK	Closed	3	7	10285	pull-request-available
13288208	Eagerly validate strictly required Flink configurations for Stateful Functions	"Currently, when Stateful Functions users want to set their own Flink configurations, they are required to build on top of a base template {{flink-conf.yaml}} which has some strictly required configurations predefined, such as parent-first classloading and state backend settings.

These Flink settings should never (as of now) be changed by the user, but there is no validation of that in place. We should do that eagerly pre-submission of the translated job, probably in {{StatefulFunctionsConfig}}."	FLINK	Closed	2	4	10285	pull-request-available
13077084	TraversableSerializer should implement compatibility methods	The {{TraversableSerializer}} may be used as a serializer for managed state and takes part in checkpointing, therefore should implement the compatibility methods.	FLINK	Closed	1	1	10285	flink-rel-1.3.1-blockers
13076717	Stream join fails when checkpointing is enabled	"The combination of joining streams and checkpointing fails in 1.3.0. It used to work with the previous 1.2 version. Code example for failure:

{code:title=Example|borderStyle=solid}
    public static void main(String[] args) throws Exception {

        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // enable checkpoints
        env.enableCheckpointing(5000);

        // create two streams
        DataStreamSource<Long> one = env.generateSequence(0, 5000);
        DataStreamSource<Long> two = env.generateSequence(2000, 15000);

        // process both, provide a delay to make sure checkpoint will happen
        DataStream<String> oneProcessed = one.
                map(oneValue -> {
                    Thread.sleep(1000);
                    return ""val-"" + oneValue;
                });
        DataStream<String> twoProcessed = two.
                map(oneValue -> {
                    Thread.sleep(1000);
                    return ""val-"" + oneValue;
                });

        // join the two streams, join on string match
        DataStream<String> joinedStreams = oneProcessed.
                join(twoProcessed).
                where(String::toString).
                equalTo(String::toString).
                window(TumblingProcessingTimeWindows.of(Time.seconds(5))).
                apply(new JoinFunction<String, String, String>() {
                    @Override
                    public String join(String oneValue, String twoValue) {
                        // nothing really relevant, just concatenate string
                        return oneValue + ""+"" + twoValue;
                    }
                });

        // output results
        joinedStreams.print();

        env.execute(""Issue with stream join and checkpoints"");
    }
{code}

Stack trace:

{noformat}
java.lang.Exception: Could not perform checkpoint 1 for operator TriggerWindow(TumblingProcessingTimeWindows(5000), ListStateDescriptor{serializer=org.apache.flink.api.common.typeutils.base.ListSerializer@3769cce0}, ProcessingTimeTrigger(), WindowedStream.apply(CoGroupedStreams.java:300)) -> Sink: Unnamed (1/1).
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.io.BarrierBuffer.notifyCheckpoint(BarrierBuffer.java:378)
	at org.apache.flink.streaming.runtime.io.BarrierBuffer.processBarrier(BarrierBuffer.java:281)
	at org.apache.flink.streaming.runtime.io.BarrierBuffer.getNextNonBlocked(BarrierBuffer.java:183)
	at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:213)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:262)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:702)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Could not complete snapshot 1 for operator TriggerWindow(TumblingProcessingTimeWindows(5000), ListStateDescriptor{serializer=org.apache.flink.api.common.typeutils.base.ListSerializer@3769cce0}, ProcessingTimeTrigger(), WindowedStream.apply(CoGroupedStreams.java:300)) -> Sink: Unnamed (1/1).
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:406)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1157)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1089)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:653)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:589)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:542)
	... 8 more
Caused by: java.lang.UnsupportedOperationException: This serializer is not registered for managed state.
	at org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionSerializer.snapshotConfiguration(CoGroupedStreams.java:555)
	at org.apache.flink.api.common.typeutils.CompositeTypeSerializerConfigSnapshot.<init>(CompositeTypeSerializerConfigSnapshot.java:53)
	at org.apache.flink.api.common.typeutils.base.CollectionSerializerConfigSnapshot.<init>(CollectionSerializerConfigSnapshot.java:39)
	at org.apache.flink.runtime.state.ArrayListSerializer.snapshotConfiguration(ArrayListSerializer.java:149)
	at org.apache.flink.runtime.state.RegisteredKeyedBackendStateMetaInfo.snapshot(RegisteredKeyedBackendStateMetaInfo.java:71)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackend.snapshot(HeapKeyedStateBackend.java:267)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:396)
	... 13 more
{noformat}"	FLINK	Resolved	1	1	10285	flink-rel-1.3.1-blockers
13346492	Require unaligned checkpoints to be disabled in StateFun applications	"Due to how StateFun has feedback loops, with unaligned checkpoints a function dispatcher operator may receive a feedback checkpoint barrier (from other parallel subtasks that process the checkpoint barrier first) before it receives its own checkpoint barrier.

We need to further investigate how to properly support unaligned checkpointing in StateFun.
For the time being, we should strictly require aligned checkpointing for StateFun apps."	FLINK	Closed	1	4	10285	pull-request-available
13281522	Allow configuring offset startup positions for Stateful Functions Kafka Ingress	"It is quite typical that a user is capable of setting where to start consuming a Kafka topic.
Since the Stateful Functions Kafka ingress sits on top of Flink's Kafka consumer, there is already various options to start with:

* {{GROUP_OFFSETS}} (default): start with whatever offsets were committed to Kafka for given {{group.id}}
* {{LATEST}}: start from latest record in topic
* {{EARLIEST}}: start from earliest record in topic
* {{SPECIFIC_OFFSETS}}: provide a map of topic partition -> offset. This is particularly important for bootstrapped state scenarios, where the user would want to start from a specific position consistent with the state bootstrapped in their functions.
* {{TIMESTAMP}}: start from offsets written starting from the given timestamp.

The proposed API looks like so:

{code}
KafkaIngressBuilder<T> builder = KafkaIngressBuilder.forIdentifier(...)
    .withTopic(...)
    .withDeserializer(...)
    .withDefaultStartPosition(KafkaIngressStartPosition.fromEarliest()/fromLatest())
    .withSpecificStartOffsets(KafkaIngressStartOffsets.fromMap(Map)/fromTimestamp(Long))
{code}

The {{withDefaultStartPosition}} method is straightforward.
The reason to separate this from another {{withSpecificStartOffsets}} method is that there would be cases where some partition does not contain the offsets specified by {{withSpecificStartOffsets}}.
In this case, the ingress would need to fallback to some default configuration; this would be the {{withDefaultStartPosition}} configuration."	FLINK	Closed	3	2	10285	pull-request-available
13202442	Make serializers immutable / provide option TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer	"h2. Motivation

Right now, when a new serializer is provided to the old serializer (or, to be more specific, the old serializer's snapshot) for state schema compatibility checks, if the new serializer is reconfigurable so that it may be compatible, the only possible way to do this is reconfigure the new serializer in-place and return {{TypeSerializerSchemaCompatibility.compatibleAsIs()}} as the result of the compatibility check.

One solid example is the {{KryoSerializer}}. The {{KryoSerializer}} contains as configuration a map of serialized classes to their registered ids. This mapping may change on restore executions, and the new {{KryoSerializer}} must reconfigure this mapping to match with the previous execution before the new {{KryoSerializer}} can be used for state access.
Right now, this is performed by directly mutating the map in the new serializer instance.

This mutative behaviour is fragile, especially when taking into account scale down / up scenarios which could easily result in mismatching state serializer configurations across TMs.
h2. Proposed Approach
 # The {{TypeSerializerSchemaCompatibility}} result class should be extended to contain an option {{compatibleWithReconfiguredSerializer(TypeSerializer)}}, which would wrap a new instance of a reconfigured version of the new serializer.
 # Callers of the compatibility check needs to be aware of this case and respect it, using the provided reconfigured serializer instance when one is provided. In Flink, there are two places which performs compatibility checks on serializers: 1) composite serializers which contain nested serializers, and therefore needs to check compatibility of its nested serializers, and 2) in state backends, checking the compatibility of the new serializer with the old serializer.
 # Introduce {{CompositeTypeSerializerSnapshot}} to encapsulate logic of handling reconfiguration of nested serializers: if a composite serializer has a nested serializer that returns a new reconfigured instance of itself, than the result of the compatibility check on the composite serializer should also wrap a reconfigured version of the composite serializer that holds the reconfigured nested serializer. This logic should be captured in a base abstract class, say {{CompositeTypeSerializerSnapshot}} so that it can be commonly shared by many of Flink's composite serializers.
 # For composite serializers that is still using the legacy, less-powerful {{TypeSerializerConfigSnapshot}} and {{CompatibilityResult}} abstractions, while its nested serializer is signaling that it has reconfigured itself, this should be detected and an error is thrown complaining that the outer composite serializer needs to be upgraded to use the new serializer snapshot and compatibility abstractions. This approach follows the same way we handled bridging the new {{TypeSerializerSchemaCompatibility}} and old {{CompatibilityResult}} class in Flink 1.7."	FLINK	Closed	2	4	10285	pull-request-available
13336336	InternalTimeServiceManager fails to restore due to corrupt reads if there are other users of raw keyed state streams	"h2. *Diagnosis*

Currently, when restoring a {{InternalTimeServiceManager}}, we always attempt to read from the provided raw keyed state streams (using {{InternalTimerServiceSerializationProxy}}):
https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/InternalTimeServiceManagerImpl.java#L117

This is incorrect, since we don't write with the {{InternalTimerServiceSerializationProxy}} if the timers do not require legacy synchronous snapshots:
https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/InternalTimeServiceManagerImpl.java#L192
(we currently only require that when users use RocksDB backend + heap timers).

Therefore, the {{InternalTimeServiceManager}} can fail to be created on restore due to corrupt reads in the case where:
* a checkpoint was taken where {{useLegacySynchronousSnapshots}} is false (hence nothing was written, and the time service manager does not use the raw keyed stream)
* the raw keyed stream is used elsewhere (e.g. in the Flink application's user code)
* on restore from the checkpoint, {{InternalTimeServiceManagerImpl.create()}} attempts to read from the raw keyed stream with the {{InternalTimerServiceSerializationProxy}}.

Full error stack trace (with Flink 1.11.1):
{code}
2020-10-21 13:16:51
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:204)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:247)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:479)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:475)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:528)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:197)
	at java.io.DataInputStream.readUTF(DataInputStream.java:609)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.read(InternalTimerServiceSerializationProxy.java:110)
	at org.apache.flink.core.io.PostVersionedIOReadableWritable.read(PostVersionedIOReadableWritable.java:76)
	at org.apache.flink.streaming.api.operators.InternalTimeServiceManager.restoreStateForKeyGroup(InternalTimeServiceManager.java:217)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:234)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:167)
	... 9 more
{code}

h2. *Reproducing*

- Have an application with any operator that uses and writes to raw keyed state streams
- Use heap backend + any timer factory or RocksDB backend + RocksDB timers
- Take a savepoint or wait for a checkpoint, and trigger a restore

h2. *Proposed Fix*

The fix would be to also respect the {{useLegacySynchronousSnapshots}} flag in:
https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/InternalTimeServiceManagerImpl.java#L231"	FLINK	Closed	1	1	10285	pull-request-available
13341023	Restored feedback events may be silently dropped if per key-group header bytes were not fully read	"The attempt to read the per key-group header bytes here does not guarantee the header bytes are fully-read:
https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-core/src/main/java/org/apache/flink/statefun/flink/core/logger/UnboundedFeedbackLogger.java#L163

What could happen is the following:
* Say the input stream actually has the header bytes written in there
* Less then {{HEADER_BYTES.length}} number of bytes was read into the read buffer, in the above reference code line.
* The {{if (bytesRead > 0 && !Arrays.equals(header, HEADER_BYTES))}} check would be true, because the read byte array != the expected header bytes.
* We would mistakenly think that the header bytes are not in the input stream, and pushback. i.e. the header bytes were not being skipped, and the following reads would see the header bytes first.
* Most importantly, since the header bytes are not being skipped in this case, the {{STATEFUN_VERSION}} (which is {{0}}) is being incorrectly read by {{KeyGroupStream.readFrom(...)}} as the number of feedback elements to read: https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-core/src/main/java/org/apache/flink/statefun/flink/core/logger/KeyGroupStream.java#L57
* The end result of all of this is in this scenario: some checkpointed feedback events would be silently dropped.

Although it is hard to say how possible this would happen in reality, and would also depend on the actual implementation of the {{InputStream}}, from the general contracts of {{InputStream#read(byte[])}} this is definitely something that should be addressed.
"	FLINK	Closed	1	4	10285	pull-request-available
13336415	KeyGroupRangeOffsets#KeyGroupOffsetsIterator should skip key groups that don't have a defined offset	"Currently, on commit the {{UnboundedFeedbackLogger}} only calls {{startNewKeyGroup}} on the raw keyed stream for key groups that actually have logged messages:
https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-core/src/main/java/org/apache/flink/statefun/flink/core/logger/UnboundedFeedbackLogger.java#L102

This means that it might skip some key groups, if a key group doesn't have any logged messages.

This doesn't conform with the expected usage of Flink's {{KeyedStateCheckpointOutputStream}}, where it expects that for ALL key groups within the range, {{startNewKeyGroup}} needs to be invoked.
The reason for this is that underneath, calling {{startNewKeyGroup}} would also record the starting stream offset position for the key group.
However, when iterating through a raw keyed stream, the key group offsets iterator {{KeyGroupRangeOffsets#KeyGroupOffsetsIterator}} doesn't take into account that some key groups weren't written and therefore do not have offsets defined, and the streams will be seeked to incorrect positions.

Ultimately, if some key groups were skipped while writing to the raw keyed stream, the following error will be thrown on restore:
{code}
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:204)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:247)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:473)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: java.io.IOException: position out of bounds
	at org.apache.flink.runtime.state.StatePartitionStreamProvider.getStream(StatePartitionStreamProvider.java:58)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:235)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:167)
	... 9 more
Caused by: java.io.IOException: position out of bounds
	at org.apache.flink.runtime.state.memory.ByteStreamStateHandle$ByteStateHandleInputStream.seek(ByteStreamStateHandle.java:124)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl$KeyGroupStreamIterator.next(StreamTaskStateInitializerImpl.java:442)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl$KeyGroupStreamIterator.next(StreamTaskStateInitializerImpl.java:395)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.internalTimeServiceManager(StreamTaskStateInitializerImpl.java:228)
	... 10 more
{code}

h2. *Solution*

We change the {{KeyGroupRangeOffsets#KeyGroupOffsetsIterator}} in Flink to skip key groups that don't have a defined offset (i.e. {{startNewKeyGroup}} wasn't called for these key groups)."	FLINK	Closed	1	1	10285	pull-request-available
13287149	Support -p/--parallelism option for StatefulFunctionsClusterEntryPoint	"Currently the only way for users to specify parallelism > 1 for Stateful Functions applications is to provide a value for {{parallelism.default}} via {{flink-conf.yaml}}.

That is not so nice to use, as users would essentially need to rebuild the Stateful Functions application image just to change the parallelism."	FLINK	Closed	3	2	10285	pull-request-available
13394436	Remove SanityVerificationE2E and ExactlyOnceRemoteE2E	"Over time, the smoke E2E tests have proven to be extensive enough to subsume the {{SanityVerificationE2E}} and {{ExactlyOnceRemoteE2E}}, which are far less covering in terms of test scope. As a matter of fact, a large majority (if not all) if the more important bugs we have discovered over the last few releases were surfaced by the smoke E2Es.

As the build times are growing larger and larger in StateFun, we suggest to remove {{SanityVerificationE2E}} and {{ExactlyOnceRemoteE2E}} to be conservative on the build times."	FLINK	Closed	3	4	10285	pull-request-available
13347079	Upgrade StateFun to Flink 1.11.3	"Now that Flink 1.11.3 is out, we should upgrade StateFun to use 1.11.3.
This is important for FLINK-19741."	FLINK	Closed	1	4	10285	pull-request-available
13308946	Scaladocs not building since inner Java interfaces cannot be recognized	"Error:

{code}
/home/buildslave/slave/flink-docs-master/build/flink-scala/src/main/java/org/apache/flink/api/scala/typeutils/Tuple2CaseClassSerializerSnapshot.java:98: error: not found: type OuterSchemaCompatibility
	protected OuterSchemaCompatibility resolveOuterSchemaCompatibility(ScalaCaseClassSerializer<Tuple2<T1, T2>> newSerializer) {
                  ^
/home/buildslave/slave/flink-docs-master/build/flink-scala/src/main/java/org/apache/flink/api/scala/typeutils/TraversableSerializerSnapshot.java:101: error: not found: type OuterSchemaCompatibility
	protected OuterSchemaCompatibility resolveOuterSchemaCompatibility(TraversableSerializer<T, E> newSerializer) {
                  ^
/home/buildslave/slave/flink-docs-master/build/flink-scala/src/main/java/org/apache/flink/api/scala/typeutils/ScalaCaseClassSerializerSnapshot.java:106: error: not found: type OuterSchemaCompatibility
	protected OuterSchemaCompatibility resolveOuterSchemaCompatibility(ScalaCaseClassSerializer<T> newSerializer) {
                  ^
{code}

This is a similar issue as reported here: https://github.com/scala/bug/issues/10509.

This seems to be a problem with Scala 2.12.x. The only workaround is to
redundantly add the full-length qualifiers for such interfaces."	FLINK	Closed	2	1	10285	pull-request-available
13283186	Make Stateful Functions release-ready	"As a preparation for releasing the first release for Apache Flink Stateful Functions, there's a few things that needs to be done:

*Legal*
* Missing NOTICE files for canonical source distribution in project root
* Missing NOTICE / LICENSE files under {{META-INF}} directory of to-be-released Maven artifact jars.

*Tooling*
Need utility scripts to create release branches, create source distribution, building and deploying staging artifacts, etc. Most of this can probably be based on existing tooling in {{apache/flink}}.

Stateful Functions releases would also include a base Docker image for building applications, so we also need to look into how we want to do that. Most likely this will be blocked by the ongoing efforts in releasing an official Flink image endorsed by the Apache Flink PMC.

*Documentation*
All information about releasing a Stateful Functions release should also be documented in the Flink community project wiki.

This is an umbrella issue covering all necessary subtasks."	FLINK	Closed	1	4	10285	pull-request-available
13360880	Avoid copying when converting byte[] to ByteString in StateFun	There's a few places in StateFun where we can be more efficient with byte[] to Protobuf ByteString conversions, by just wrapping the byte[] instead of copying, since we know that the byte array can no longer be mutated.	FLINK	Closed	3	4	10285	pull-request-available
13250749	Add 1.9.0 release notes to documentation	"Similar to https://ci.apache.org/projects/flink/flink-docs-release-1.9/release-notes/flink-1.8.html, we need a release note page for 1.9.0.

This will be linked by the announcement blog post on the Apache Flink website."	FLINK	Closed	1	4	10285	pull-request-available
13295382	Duplicate LICENSE.txt pulled in should be excluded using ApacheLicenseResourceTransformer	The {{statefun-flink-distribution}} / {{statefun-ridesharing-example-simulatr}} has several dependencies that pull in a duplicate license under the file name {{LICENSE.txt}} / {{license.txt}}. Those can be excluded using the [ApacheLicenseResourceTransformer|http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ApacheLicenseResourceTransformer], since we already manually include their respective licenses under the {{META-INF/licenses}} folder.	FLINK	Closed	3	4	10285	stale-assigned
13553537	Memory leak in KafkaSourceReader if no data in consumed topic	"*Problem description*

Our Flink streaming job TaskManager heap gets full when the job has nothing to consume and process.

It's a simple streaming job with KafkaSource -> ProcessFunction -> KafkaSink. When there are no messages in the source topic the TaskManager heap usage starts increasing until the job exits after receiving a SIGTERM signal. We are running the job on AWS EMR with YARN.

The problems with the TaskManager heap usage do not occur when there is data to process. It's also worth noting that sending a single message to the source topic of a streaming job that has been sitting idle and suffers from the memory leak will cause the heap to be cleared. However it does not resolve the problem since the heap usage will start increasing immediately after processing the message.

!Screenshot 2023-10-10 at 12.49.37.png!

TaskManager heap used percentage is calculated by 

 
{code:java}
flink.taskmanager.Status.JVM.Memory.Heap.Used * 100 / flink.taskmanager.Status.JVM.Memory.Heap.Max{code}
 

 

 I was able to take heap dumps of the TaskManager processes during a high heap usage percentage. Heap dump analysis detected 912,355 instances of java.util.HashMap empty collections retaining >= 43,793,040 bytes.

!Screenshot 2023-10-09 at 14.13.43.png!

The retained heap seemed to be located at:

 
{code:java}
org.apache.flink.connector.kafka.source.reader.KafkaSourceReader#offsetsToCommit{code}
 

!Screenshot 2023-10-09 at 13.02.34.png!

 

*Possible hints:*

An empty HashMap is added during the snapshotState method to offsetsToCommit map if it does not already exist for the given checkpoint. [KafkaSourceReader line 107|https://github.com/apache/flink-connector-kafka/blob/b09928d5ef290f2a046dc1fe40b4c5cebe76f997/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java#L107]

 
{code:java}
Map<TopicPartition, OffsetAndMetadata> offsetsMap =
        offsetsToCommit.computeIfAbsent(checkpointId, id -> new HashMap<>()); {code}
 

If the startingOffset for the given split is >= 0 then a new entry would be added to the map from the previous step. [KafkaSourceReader line 113|https://github.com/apache/flink-connector-kafka/blob/b09928d5ef290f2a046dc1fe40b4c5cebe76f997/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java#L113]
{code:java}
if (split.getStartingOffset() >= 0) {
    offsetsMap.put(
        split.getTopicPartition(),
        new OffsetAndMetadata(split.getStartingOffset()));
}{code}
If the starting offset is smaller than 0 then this would leave the offsetMap created in step 1 empty. We can see from the logs that the startingOffset is -3 when the splits are added to the reader.

 
{code:java}
Adding split(s) to reader: [[Partition: source-events-20, StartingOffset: 1, StoppingOffset: -9223372036854775808], [Partition: source-events-44, StartingOffset: -3, StoppingOffset: -9223372036854775808], [Partition: source-events-12, StartingOffset: -3, StoppingOffset: -9223372036854775808], [Partition: source-events-36, StartingOffset: 1, StoppingOffset: -9223372036854775808], [Partition: source-events-4, StartingOffset: -3, StoppingOffset: -9223372036854775808], [Partition: source-events-28, StartingOffset: -3, StoppingOffset: -9223372036854775808]]{code}
 

 

The offsetsToCommit map is cleaned from entries once they have been committed to Kafka which happens during the callback function that is passed to the KafkaSourceFetcherManager.commitOffsets method in KafkaSourceReader.notifyCheckpointComplete method.

However if the committedPartitions is empty for the given checkpoint, then the KafkaSourceFetcherManager.commitOffsets method returns.  [KafkaSourceFetcherManager line 78|https://github.com/apache/flink-connector-kafka/blob/b09928d5ef290f2a046dc1fe40b4c5cebe76f997/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/fetcher/KafkaSourceFetcherManager.java#L78]
{code:java}
if (offsetsToCommit.isEmpty()) {
    return;
} {code}
We can observe from the logs that indeed an empty map is encountered at this step:
{code:java}
Committing offsets {}{code}
*Conclusion*

It seems that an empty map gets added per each checkpoint to offsetsToCommit map. Since the startingOffset in our case is -3 then the empty map never gets filled. During the offset commit phase the offsets for these checkpoints are ignored, since there is nothing to commit, however there isn't any cleanup either so the empty maps keep accumulating. 

 "	FLINK	Closed	1	1	10285	pull-request-available
13307683	Revert execution environment patching in StatefulFunctionsJob (FLINK-16926)	"In FLINK-16926, we explicitly ""patched"" the {{StreamExecutionEnvironment}} due to FLINK-16560.

Now that we have upgraded the Flink version in StateFun to 1.10.1 which includes a fix for FLINK-16560, we can now revert the patching of {{StreamExecutionEnvironment}} in the {{StatefulFunctionsJob}}."	FLINK	Closed	1	4	10285	pull-request-available
13184860	Add Cosh math function supported in Table API and SQL	"Implement udf of cosh, just like in oracle

[https://docs.oracle.com/cd/B28359_01/server.111/b28286/functions031.htm#SQLRF00623]

 "	FLINK	Closed	4	7	10999	pull-request-available
13194749	Fix sql client end to end test failure	"The log file contains the following sentence:
{code:java}
2018-10-29 03:27:39,209 WARN org.apache.flink.kafka010.shaded.org.apache.kafka.common.utils.AppInfoParser - Error while loading kafka-version.properties :null
{code}
The reason for this log is that we explicitly exclude the version description file of the kafka client when packaging the connector:
{code:java}
<filters>
   <filter>
      <artifact>*:*</artifact>
      <excludes>
         <exclude>kafka/kafka-version.properties</exclude>
      </excludes>
   </filter>
</filters>{code}
When the shell scan the ""error"" keyword with grep, it will hit, so the test will fail.
{code:java}
function check_logs_for_errors {
  error_count=$(grep -rv ""GroupCoordinatorNotAvailableException"" $FLINK_DIR/log \
      | grep -v ""RetriableCommitFailedException"" \
      | grep -v ""NoAvailableBrokersException"" \
      | grep -v ""Async Kafka commit failed"" \
      | grep -v ""DisconnectException"" \
      | grep -v ""AskTimeoutException"" \
      | grep -v ""WARN  akka.remote.transport.netty.NettyTransport"" \
      | grep -v  ""WARN  org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline"" \
      | grep -v ""jvm-exit-on-fatal-error"" \
      | grep -v '^INFO:.*AWSErrorCode=\[400 Bad Request\].*ServiceEndpoint=\[https://.*\.s3\.amazonaws\.com\].*RequestType=\[HeadBucketRequest\]' \
      | grep -v ""RejectedExecutionException"" \
      | grep -v ""An exception was thrown by an exception handler"" \
      | grep -v ""java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/exceptions/YarnException"" \
      | grep -v ""java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration"" \
      | grep -v ""org.apache.flink.fs.shaded.hadoop3.org.apache.commons.beanutils.FluentPropertyBeanIntrospector  - Error when creating PropertyDescriptor for public final void org.apache.flink.fs.shaded.hadoop3.org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property."" \
      | grep -ic ""error"")    //here
  if [[ ${error_count} -gt 0 ]]; then
    echo ""Found error in log files:""
    cat $FLINK_DIR/log/*
    EXIT_CODE=1
  fi
}
{code}"	FLINK	Closed	3	1	10999	pull-request-available
13268218	Mark TaskManagerOptions#EXIT_ON_FATAL_AKKA_ERROR with @Deprecated annotation	Since {{TaskManagerOptions#EXIT_ON_FATAL_AKKA_ERROR}} has no longer been  used. IMO, we can remove this config option.	FLINK	Closed	3	7	10999	pull-request-available
13242054	Elasticsearch 7.x support	"Elasticsearch 7.0.0 was released in April of 2019: [https://www.elastic.co/blog/elasticsearch-7-0-0-released]
The latest elasticsearch connector is [flink-connector-elasticsearch6|https://github.com/apache/flink/tree/master/flink-connectors/flink-connector-elasticsearch6]"	FLINK	Closed	3	2	10999	pull-request-available
13224313	Remove ASSIGNED_SLOT_UPDATER in Execution.tryAssignResource	After making access to ExecutionGraph single-threaded in FLINK-11417, we can simplify execution slot assignment in Execution.tryAssignResource and get rid of ASSIGNED_SLOT_UPDATER as it happens now only in one JM main thread. Though, it seems that we have to keep `assignedResource` as volatile at the moment which could be further investigated.	FLINK	Closed	4	7	10999	pull-request-available
13159022	In Standalone checkpoint recover mode many jobs with same checkpoint interval cause IO pressure	"currently, the periodic checkpoint coordinator startCheckpointScheduler uses *baseInterval* as the initialDelay parameter. the *baseInterval* is also the checkpoint interval. 

In standalone checkpoint mode, many jobs config the same checkpoint interval. When all jobs being recovered (the cluster restart or jobmanager leadership switched), all jobs' checkpoint period will tend to accordance. All jobs' CheckpointCoordinator would start and trigger in a approximate time point.

This caused the high IO cost in the same time period in our production scenario.

I suggest let the scheduleAtFixedRate's initial delay parameter as a API config which can let user scatter checkpoint in this scenario.

 

cc [~StephanEwen] [~Zentol]"	FLINK	Resolved	3	4	10999	pull-request-available
13233703	Remove GLOBAL_VERSION_UPDATER in ExecutionGraph	Since {{ExecutionGraph}} can only be accessed from a single thread. We can remove {{AtomicLongFieldUpdater<ExecutionGraph> GLOBAL_VERSION_UPDATER}} from {{ExecutionGraph}}.	FLINK	Closed	4	7	10999	pull-request-available
13266715	Change Type of Field currentExecutions from ConcurrentHashMap to HashMap	After FLINK-11417, we made ExecutionGraph be a single-thread mode. It will no longer be plagued by concurrency issues. So, we can degenerate the current ConcurrentHashMap type of currentExecutions to a normal HashMap type.	FLINK	Closed	3	7	10999	pull-request-available
13181841	Handle oversized response messages in AkkaRpcActor	The {{AkkaRpcActor}} should check whether an RPC response which is sent to a remote sender does not exceed the maximum framesize of the underlying {{ActorSystem}}. If this is the case we should fail fast instead. We can achieve this by serializing the response and sending the serialized byte array.	FLINK	Closed	3	4	10999	pull-request-available
13249296	Update FlinkKafkaProducerMigrationTest to restore from 1.9 savepoint	Update {{FlinkKafkaProducerMigrationTest}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13267648	Mark ExecutionVertex#deployToSlot with @VisibleForTesting annotation	From tracking the call chain of {{ExecutionVertex#deployToSlot}}, it seems this method is only been called in the test code. IMO, we'd better bring down {{ExecutionVertex#deployToSlot}} access modifier and mark it with @VisibleForTesting annotation to reduce the risk of incorrect calls	FLINK	Closed	4	4	10999	pull-request-available
13195344	Misleading clean_log_files() in common.sh	"In the `common.sh` base script of the end-to-end tests, there is a `clean_stdout_files` which cleans only the `*.out` files and a `clean_log_files` which cleans *both* `*.log` and `*.out` files.

Given the current behavior that at the end of a test, the logs are checked and if there are exceptions (even expected ones but not whitelisted), the tests fails, some tests chose to call the `clean_log_files` so that exceptions are ignored. In this case, also `*.out` files are cleaned so if a test was checking for errors in the `.out` files, then the test will falsely pass.

The solution is as simple as renaming the method to something more descriptive like `clean_logs_and_output_files`, but doing so, also includes checking if any existing tests were falsely passing."	FLINK	Closed	3	1	10999	pull-request-available
13177156	Allowable number of checkpoint failures 	"For intermittent checkpoint failures it is desirable to have a mechanism to avoid restarts. If, for example, a transient S3 error prevents checkpoint completion, the next checkpoint may very well succeed. The user may wish to not incur the expense of restart under such scenario and this could be expressed with a failure threshold (number of subsequent checkpoint failures), possibly combined with a list of exceptions to tolerate.

 "	FLINK	Closed	3	4	10999	pull-request-available
13167163	Extending 'KafkaJsonTableSource' according to comments will result in NPE	"According to the comments what is needed to extend the 'KafkaJsonTableSource' looks as follows:

 
{code:java}
A version-agnostic Kafka JSON {@link StreamTableSource}.
*
* <p>The version-specific Kafka consumers need to extend this class and
* override {@link #createKafkaConsumer(String, Properties, DeserializationSchema)}}.
*
* <p>The field names are used to parse the JSON file and so are the types.{code}
This will cause an NPE, since there is no default value for startupMode in the abstract class itself only in the builder of this class. 
For the 'getKafkaConsumer' method the switch statement will be executed on non-initialized 'startupMode' field:
{code:java}
switch (startupMode) {
case EARLIEST:
kafkaConsumer.setStartFromEarliest();
break;
case LATEST:
kafkaConsumer.setStartFromLatest();
break;
case GROUP_OFFSETS:
kafkaConsumer.setStartFromGroupOffsets();
break;
case SPECIFIC_OFFSETS:
kafkaConsumer.setStartFromSpecificOffsets(specificStartupOffsets);
break;
}{code}
 

 "	FLINK	Closed	3	1	10999	pull-request-available
13249290	Update CEPMigrationTest to restore from 1.9 savepoint	Update {{CEPMigrationTest}} to restore from 1.9 savepoint	FLINK	Resolved	1	7	10999	pull-request-available
13181529	Support listing of views	FLINK-10163 added initial support of views for the SQL Client. According to other database vendors, views are listed in the \{{SHOW TABLES}}. However, there should be a way of listing only the views. We can support the \{{SHOW VIEWS}} command.	FLINK	Closed	3	2	10999	pull-request-available
13249300	Update StatefulJobWBroadcastStateMigrationITCase.scala to restore from 1.9 savepoint	Update {{StatefulJobWBroadcastStateMigrationITCase.scala}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13178785	Add REPEAT supported in Table API and SQL	"Oracle : [https://docs.oracle.com/cd/E17952_01/mysql-5.1-en/string-functions.html#function_repeat]

MySql: https://dev.mysql.com/doc/refman/5.5/en/string-functions.html#function_repeat"	FLINK	Resolved	4	7	10999	pull-request-available
13025095	Add a Thread default uncaught exception handler on the JobManager	"When some JobManager threads die because of uncaught exceptions, we should bring down the JobManager. If a thread dies from an uncaught exception, there is a high chance that the JobManager becomes dysfunctional.

The only sfae thing is to rely on the JobManager being restarted by YARN / Mesos / Kubernetes / etc.

I suggest to add this code to the JobManager launch:

{code}
Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler() {

    @Override
    public void uncaughtException(Thread t, Throwable e) {
        try {
            LOG.error(""Thread {} died due to an uncaught exception. Killing process."", t.getName());
        } finally {
            Runtime.getRuntime().halt(-1);
        }
    }
});
{code}"	FLINK	Resolved	3	7	10999	pull-request-available
13196186	Update AbstractKeyedOperatorRestoreTestBase for 1.7	Update {{AbstractKeyedOperatorRestoreTestBase}} and subclasses so that it covers restoring from 1.7.	FLINK	Closed	3	7	10999	pull-request-available
13213882	Remove the deprecation of ExternalCatalogTable.builder()	We should remove the deprecation of ExternalCatalogTable.builder() or add it back once FLINK-11449 is resolved.	FLINK	Closed	3	4	10999	stale-assigned
13190808	Cleanup constant isNewMode in YarnTestBase	"This seems to be a residual problem with FLINK-10396. It is set to true in that PR. Currently it has three usage scenarios:

1. assert, caused an error
{code:java}
assumeTrue(""The new mode does not start TMs upfront."", !isNewMode);
{code}
2. if (!isNewMode) the logic in the block would not have invoked, the if block can be removed

3. if (isNewMode) always been invoked, the if statement can be removed."	FLINK	Resolved	3	7	10999	pull-request-available
13069687	Configure Memory Sizes with units	"Currently, memory sizes are configured by pure numbers, the interpretation is different from configuration parameter to parameter.

For example, heap sizes are configured in megabytes, network buffer memory is configured in bytes, alignment thresholds are configured in bytes.

I propose to configure all memory parameters the same way, with units similar to time. The JVM itself configured heap size similarly: {{Xmx5g}} or {{Xmx2000m}}.

{code}
10000  -> bytes
10 kb
64 mb
1 gb
...
{code}"	FLINK	Resolved	3	4	10999	pull-request-available
13153734	Add method SinkFunction[A]#contramap[B](f: B => A): SinkFunction[B]	"Just like it is very useful to use `DataStream[T]` as a sort of Functor or Monad with `map`/`flatMap`/`filter` methods, it would be extremely handy to have a `SinkFunction[A]#contramap[B](f: B => A): SinkFunction[B]` on `SinkFunctions`, so that you can reuse existing complex sink functions, but with a different input type. For example:
{code}
val bucketingStringSink: SinkFunction[String] = 
  new BucketingSink[String](""..."")
    .setBucketer(new DateTimeBucketer(""yyyy-MM-dd-HHmm"")

val bucketingIntListSink: SinkFunction[List[Int]] =
  bucketingStringSink.contramap[List[Int]](_.mkString("",""))
{code}

For some more formal motivation behind this, https://typelevel.org/cats/typeclasses/contravariant.html is definitely a great place to start!"	FLINK	Reopened	10200	4	10999	flink, stale-assigned
13199852	sql-client throws exception when paging through finished batch query 	"When paging through a batch query in state 'Finished' the sql client throws the following exception: 
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Could not find a result with result identifier '0c7dce30d287fdd13b934fbefe5a38d1'.{code}
 "	FLINK	Closed	3	1	10999	pull-request-available
13279832	Remove useless JobRetrievalException	Currently, the exception class {{JobRetrievalException}} has not been used anywhere in Flink codebase. IMO, we can remove it.	FLINK	Resolved	4	4	10999	pull-request-available
13155736	Add method Writer[A]#contramap[B](f: B => A): Writer[B]	"Similar to https://issues.apache.org/jira/browse/FLINK-9221, it would be very handy to have a `Add method Writer[A]#contramap[B](f: B => A): Writer[B]` method, which would allow reuse of existing writers, with a ""formatting"" function placed infront of it. For example:

val stringWriter = new StringWriter[String]()
val intWriter: Writer[Int] = stringWriter.contraMap[Int](_.toString) "	FLINK	Reopened	10200	4	10999	stale-assigned
13156154	Improve error message when TaskManager fails	"When a TaskManager fails, we frequently get a message

{code}
org.apache.flink.util.FlinkException: Releasing TaskManager container_1524853016208_0001_01_000102
{code}

This message is misleading in that it sounds like an intended operation, when it really is a failure of a container that the {{ResourceManager}} reports to the {{JobManager}}."	FLINK	Closed	2	4	10999	pull-request-available
13238751	Remove beta feature remark from the Universal Kafka connector	"I think we can remove this remark from the docs as in the last half year there were no issues reported that would say otherwise.

The remark about universal connector being a beta feature was introduced in: https://issues.apache.org/jira/browse/FLINK-10900"	FLINK	Closed	3	4	10999	pull-request-available
13186719	TaskManagerServicesConfiguration ctor contains self assignment	"TaskManagerServicesConfiguration has:
{code}
    this.systemResourceMetricsEnabled = systemResourceMetricsEnabled;
{code}
There is no systemResourceMetricsEnabled parameter to the ctor.

This was reported by findbugs."	FLINK	Closed	4	1	10999	pull-request-available
13249298	Update WindowOperatorMigrationTest to restore from 1.9 savepoint	Update {{WindowOperatorMigrationTest}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13100784	JDBCOutputFormat autoCommit	"Currently, if a connection is not created with autoCommit = true by default (e.g. Apache Phoenix), no data is written into the database.

So, in the JDBCOutputFormat.open() autoCommit should be forced on the created Connection, i.e.:

{code:java}
if (!conn.getAutoCommit()) {
  conn.setAutoCommit(true);
}
{code}

This should be well documented also.."	FLINK	Reopened	10200	4	10999	stale-assigned
13249295	Update FlinkKafkaProducerMigrationOperatorTest to restore from 1.9 savepoint	Update {{FlinkKafkaProducerMigrationOperatorTest}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13144112	CheckpointingStatisticsHandler fails to return PendingCheckpointStats 	"{noformat}
2018-03-10 21:47:52,487 ERROR org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler  - Implementation error: Unhandled exception.
java.lang.IllegalArgumentException: Given checkpoint stats object of type org.apache.flink.runtime.checkpoint.PendingCheckpointStats cannot be converted.
	at org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.generateCheckpointStatistics(CheckpointStatistics.java:276)
	at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:146)
	at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:54)
	at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:81)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}"	FLINK	Closed	1	1	10999	flip6
13204292	Allow configuration whether to fall back to savepoints for restore	"Ever since FLINK-3397, upon failure, Flink would restart from the latest checkpoint/savepoint which ever is more recent. With the introduction of local recovery and the knowledge that a RocksDB checkpoint restore would just copy the files, it may be time to re-consider / making this configurable:
In certain situations, it may be faster to restore from the latest checkpoint only (even if there is a more recent savepoint) and reprocess the data between. On the downside, though, that may not be correct because that might break side effects if the savepoint was the latest one, e.g. consider this chain: {{chk1 -> chk2 -> sp … restore chk2 -> …}}. Then all side effects between {{chk2 -> sp}} would be reproduced.

Making this configurable will allow the user to set whatever he needs / can to get the lowest recovery time in Flink."	FLINK	Closed	3	4	10999	pull-request-available
13196193	Update ContinuousFileProcessingMigrationTest for 1.7	Update {{ContinuousFileProcessingMigrationTest}} so that it covers restoring from 1.7.	FLINK	Closed	3	7	10999	pull-request-available
13196191	Update CEPMigrationTest for 1.7	Update {{CEPMigrationTest}} so that it covers restoring from 1.7.	FLINK	Closed	3	7	10999	pull-request-available
13202564	Support binding port range for REST server	Currently the {{RestServerEndpoint}} binds to the port specified by {{RestOptions#PORT}}. {{PORT}} is of type integer. Sometimes, it would be useful to being able to specify not only a single port but a port range to pick a port from. Therefore, I propose to add similar to {{RestOptions#BIND_ADDRESS}} another option {{RestOptions#BIND_PORT}} which allows to specify a port range for the {{RestServerEndpoint}} to pick a port from. {{RestOptions#PORT}} would then only be used by the client to connect to the started {{RestServerEndpoint}}.	FLINK	Closed	3	2	10999	pull-request-available
13176696	Migrate module flink-gelly-examples to flink-examples and rename it	I think we can put all the example modules into flink-examples module.	FLINK	Closed	4	4	10999	stale-minor
13249289	Add MigrationVersion.v1_9	Add {{MigrationVersion.v1_9}}	FLINK	Closed	1	7	10999	pull-request-available
13228022	Fix IllegalArgumentException thrown by FlinkKinesisConsumerMigrationTest#writeSnapshot	"Currently, {{FlinkKinesisConsumerMigrationTest#writeSnapshot}} throws an exception : 
{code:java}
java.lang.IllegalArgumentException: Cannot create enum from null value!

at com.amazonaws.regions.Regions.fromName(Regions.java:79)
at org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.createKinesisClient(AWSUtil.java:93)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.createKinesisClient(KinesisProxy.java:203)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.<init>(KinesisProxy.java:138)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.create(KinesisProxy.java:213)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.<init>(KinesisDataFetcher.java:275)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.<init>(KinesisDataFetcher.java:237)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest$TestFetcher.<init>(FlinkKinesisConsumerMigrationTest.java:422)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.writeSnapshot(FlinkKinesisConsumerMigrationTest.java:332)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.writeSnapshot(FlinkKinesisConsumerMigrationTest.java:113)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runners.Suite.runChild(Suite.java:128)
at org.junit.runners.Suite.runChild(Suite.java:27)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}
This exception may make the upgrader confused.

The exception is because the exists code did not initialize TestFetcher correctly. More details see the commits : https://issues.apache.org/jira/browse/FLINK-10785?focusedCommentId=16778899&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16778899 under FLINK-10785"	FLINK	Resolved	3	1	10999	pull-request-available
13224102	Bump universal Kafka connector to Kafka dependency to 2.2.0	Update the Kafka client dependency to version 2.2.0.	FLINK	Closed	3	4	10999	pull-request-available
13194561	Move modern kafka connector module into connector profile 	"The modern connector is run in the {{misc}} profile since it wasn't properly added to the {{connector profile in stage.sh click [here|https://github.com/apache/flink/pull/6890#issuecomment-431917344] to see more details.}}

*This issue is blocked by FLINK-10603.*"	FLINK	Closed	3	7	10999	pull-request-available
13165838	Divide-by-zero in PageRank	"{code}
// org.apache.flink.graph.library.linkanalysis.PageRank#AdjustScores#open

this.vertexCount = vertexCountIterator.hasNext() ? vertexCountIterator.next().getValue() : 0;

this.uniformlyDistributedScore = ((1 - dampingFactor) + dampingFactor * sumOfSinks) / this.vertexCount;
{code}"	FLINK	Resolved	3	1	10999	pull-request-available
13186853	Add Tanh math function supported in Table API and SQL	refer to : https://www.techonthenet.com/oracle/functions/tanh.php	FLINK	Closed	4	7	10999	pull-request-available
13243801	Remove WebMonitor interface from webmonitor package	Currently, the interface {{WebMonitor}} has already not been used anymore. IMO, we can remove it from the source code.	FLINK	Closed	3	7	10999	pull-request-available
13170005	Potential resource leak in RocksDBStateBackend#getDbOptions	"Here is related code:
{code}
    if (optionsFactory != null) {
      opt = optionsFactory.createDBOptions(opt);
    }
{code}
opt, an DBOptions instance, should be closed before being rewritten.

getColumnOptions has similar issue."	FLINK	Closed	4	1	10999	pull-request-available
13145741	YarnResourceManager spamming log file at INFO level	"For every requested resource, the {{YarnResourceManager}} spams the log with log-level INFO and the following messages:

{code}
2018-03-16 03:41:20,180 INFO  org.apache.flink.yarn.YarnResourceManager                     - Received new container: container_1521038088305_0257_01_000102 - Remaining pending container requests: 301
2018-03-16 03:41:20,180 INFO  org.apache.flink.yarn.YarnResourceManager                     - TaskExecutor container_1521038088305_0257_01_000102 will be started with container size 8192 MB, JVM heap size 5120 MB, JVM direct memory limit 3072 MB
2018-03-16 03:41:20,180 INFO  org.apache.flink.yarn.YarnResourceManager                     - TM:remote keytab path obtained null
2018-03-16 03:41:20,180 INFO  org.apache.flink.yarn.YarnResourceManager                     - TM:remote keytab principal obtained null
2018-03-16 03:41:20,180 INFO  org.apache.flink.yarn.YarnResourceManager                     - TM:remote yarn conf path obtained null
2018-03-16 03:41:20,180 INFO  org.apache.flink.yarn.YarnResourceManager                     - TM:remote krb5 path obtained null
2018-03-16 03:41:20,181 INFO  org.apache.flink.yarn.Utils                                   - Copying from file:/mnt/yarn/usercache/hadoop/appcache/application_1521038088305_0257/container_1521038088305_0257_01_000001/6766be70-82f7-4999-a371-11c27527fb6e-taskmanager-conf.yaml to hdfs://ip-172-31-1-91.eu-west-1.compute.internal:8020/user/hadoop/.flink/application_1521038088305_0257/6766be70-82f7-4999-a371-11c27527fb6e-taskmanager-conf.yaml
2018-03-16 03:41:20,190 INFO  org.apache.flink.yarn.YarnResourceManager                     - Prepared local resource for modified yaml: resource { scheme: ""hdfs"" host: ""ip-172-31-1-91.eu-west-1.compute.internal"" port: 8020 file: ""/user/hadoop/.flink/application_1521038088305_0257/6766be70-82f7-4999-a371-11c27527fb6e-taskmanager-conf.yaml"" } size: 595 timestamp: 1521171680190 type: FILE visibility: APPLICATION
2018-03-16 03:41:20,194 INFO  org.apache.flink.yarn.YarnResourceManager                     - Creating container launch context for TaskManagers
2018-03-16 03:41:20,194 INFO  org.apache.flink.yarn.YarnResourceManager                     - Starting TaskManagers with command: $JAVA_HOME/bin/java -Xms5120m -Xmx5120m -XX:MaxDirectMemorySize=3072m  -Dlog.file=<LOG_DIR>/taskmanager.log -Dlogback.configurationFile=file:./logback.xml -Dlog4j.configuration=file:./log4j.properties org.apache.flink.yarn.YarnTaskExecutorRunner --configDir . 1> <LOG_DIR>/taskmanager.out 2> <LOG_DIR>/taskmanager.err
{code}"	FLINK	Closed	1	1	10999	flip-6
13158139	Rename SubtasksAllAccumulatorsHandlers	The {{SubtasksAllAccumulatorsHandlers}} class should be called {{SubtasksAllAccumulatorsHeaders}}.	FLINK	Closed	4	1	10999	easy-fix
13183326	Generate JobGraph with fixed/configurable JobID in StandaloneJobClusterEntrypoint	The {{StandaloneJobClusterEntrypoint}} currently generates the {{JobGraph}} from the user code when being started. Due to the nature of how the {{JobGraph}} is generated, it will get a random {{JobID}} assigned. This is problematic in case of a failover because then, the {{JobMaster}} won't be able to detect the checkpoints. In order to solve this problem, we need to either fix the {{JobID}} assignment or make it configurable.	FLINK	Closed	2	4	10999	pull-request-available
13227122	Make the vcore that Application Master used configurable for Flink on YARN	"Now, for Flink on YARN deployment mode, each am's vcores is specified to 1 (hard code).

In some scene, we found many Akka timeout logs, the Flink web UI cannot be opened, but it is alive. I think there is no more threads resource to be used for am. So we suggest that make the vcores num of application master can be configurable."	FLINK	Resolved	3	4	10999	pull-request-available
13196105	Replace hard code of job graph file path with config option for FileJobGraphRetriever	"There is a config option to configure the value :

{code:java}
public static final ConfigOption<String> JOB_GRAPH_FILE_PATH = ConfigOptions
   .key(""internal.jobgraph-path"")
   .defaultValue(""job.graph"");
{code}

However, we used the default hard code in AbstractYarnClusterDescriptor. 

This is just a preliminary refactoring, and I finally recommend that we use Zookeeper-based storage. It has been implemented in FLINK-10292.
"	FLINK	Resolved	3	4	10999	pull-request-available
13163886	Add shade plugin executions to package table example jar	this is a preparatory work for issue FLINK-9519, so that we can get those examples' fat jar and then move them to example dir like batch and streaming. Because, there is no table examples in flink binary package.	FLINK	Closed	4	1	10999	pull-request-available
13178465	There should be a Scala DataSource	In Java, an ExecutionEnvironment's createInput method returns a DataSource, whereas the Scala version returns a DataSet. There is no Scala DataSource wrapper, and the Scala DataSet does not provide the Java DataSource methods, such as getSplitDataProperties.	FLINK	Closed	4	4	10999	datasource, pull-request-available, scala
13221284	Allow FileSystem Configs to be altered at Runtime	"This stems from a need to be able to pass in S3 auth keys at runtime in order to allow users to specify the keys they want to use. Based on the documentation it seems that currently S3 keys need to be part of the Flink cluster configuration, in a hadoop file (which the cluster needs to pointed to) or JVM args.


This only seems to apply to the streaming API. Also Feel free to correct the following if I am wrong, as there may be pieces I have no run across, or parts of the code I have misunderstood.


Currently it seems that FileSystems are inferred based on the extension type and a set of cached Filesystems that are generated in the background. These seem to use the config as defined at the time they are stood up. Unfortunately there is no way to tap into this control mechanism or override this behavior as many places in the code pulls from this cache. This is particularly painful in the sink instance as there are places where this is used that are not accessible outside the package it is implemented.

Through a pretty hacky mechanism I have proved out that this is a self imposed limitation, as I was able to change the code to pass in a Filesystem from the top level and have it read and write to S3 given keys I set at runtime.

The current methodology is convenient, however there should be finer grain controls for instances where the cluster is in a multitenant environment.

As a final note it seems like both the FileSystem and FileSystemFactory classes are not Serializable. I can see why this would be the case in former, but I am not clear as to why a factory class would not be Serializable (like in the case of BucketFactory). If this can be made serializable this should make this a much cleaner process."	FLINK	Reopened	10200	4	10999	stale-assigned
13232589	[State TTL] Consider setting a default background cleanup strategy in StateTtlConfig	"At the moment we have two efficient background cleanup strategies: incremental for heap and compaction filter for RocksDB. *StateTtlConfig* has 2 methods to activate them: *cleanupIncrementally* and *cleanupInRocksdbCompactFilter*. Each is activated only for certain backend type and inactive for other. They have different tuning parameters.

The idea is to add method *cleanupInBackground* which would activate default background cleanup. User does not need to think then about details or used backend if not needed. Depending on actually used backend, the corresponding cleanup will kick in. If original strategy is not set with *cleanupIncrementally* and *cleanupInRocksdbCompactFilter* then backends should check whether default background cleanup is activated and if so, use it with default parameters.

We can also deprecate the parameterless *cleanupInRocksdbCompactFilter()* in favour of this new method."	FLINK	Closed	3	4	10999	pull-request-available
13223770	Wrong check message about heartbeat interval for HeartbeatServices	"I am seeing:
{code:java}
The heartbeat timeout should be larger or equal than the heartbeat timeout{code}
due to bad configuration. I guess it should be instead:
{code:java}
The heartbeat interval should be larger or equal than the heartbeat timeout{code}
at:

https://github.com/apache/flink/blob/1f0e036bbf6a37bb83623fb62d4900d7c28a5e1d/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatServices.java#L43"	FLINK	Closed	4	1	10999	pull-request-available
13171716	Display the job description in the Flink web UI	refer to FLINK-9682 , split the original issue into two issues (backend and frontend), this issue is about frontend	FLINK	Reopened	10200	4	10999	stale-assigned
13223320	Remove warning about max fields in case class 	The [serialization documentation|https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/types_serialization.html#flinks-typeinformation-class] states that there is a limit of 22 fields in a case class. Since [Scala 2.11|https://github.com/scala/bug/issues/7296] this arity limit has been removed and therefore this limit should also be removed on this documentation page. 	FLINK	Closed	4	4	10999	pull-request-available
13174745	Add CHR function for table/sql API	"This function convert ASCII code to a character,

refer to : [https://doc.ispirer.com/sqlways/Output/SQLWays-1-071.html]

Considering ""CHAR"" always is a keyword in many database, so we use ""CHR"" keyword."	FLINK	Closed	4	2	10999	pull-request-available
13068777	Move history server configuration to a separate file	"I suggest to keep the {{flink-conf.yaml}} leaner by moving configuration of the History Server to a different file.

In general, I would propose to move configurations of separate, independent and optional components to individual config files."	FLINK	Closed	3	4	10999	pull-request-available
13249302	Update StatefulJobSavepointMigrationITCase to restore from 1.9 savepoint	Update {{StatefulJobSavepointMigrationITCase}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13198522	Upgrade Kafka client version to 2.0.1	Since the modern kafka connector only keeps track of the latest version of the kafka client. With the release of Kafka 2.0.1, we should upgrade the version of the kafka client maven dependency.	FLINK	Closed	3	7	10999	pull-request-available
13171583	Support IPv6 literal	"Currently we use colon as separator when parsing host and port.

We should support the usage of IPv6 literals in parsing."	FLINK	Reopened	10200	1	10999	stale-assigned
13175375	Add regexp_extract supported in TableAPI and SQL	"regex_extract is a very useful function, it returns a string based on a regex pattern and a index.

For example : 
{code:java}
regexp_extract('foothebar', 'foo(.*?)(bar)', 2) // returns 'bar.'
{code}

It is provided as a UDF in Hive, more details please see[1].

[1]: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"	FLINK	Resolved	4	7	10999	pull-request-available
13249292	Update BucketingSinkMigrationTest to restore from 1.9 savepoint	Update {{BucketingSinkMigrationTest}} to restore from 1.9 savepoint.	FLINK	Resolved	1	7	10999	pull-request-available
13249305	Update AbstractKeyedOperatorRestoreTestBase to restore from 1.9 savepoint	Update {{AbstractKeyedOperatorRestoreTestBase}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13282290	Add more description for FlinkKafkaProducerMigrationOperatorTest	"Currently, {{FlinkKafkaProducerMigrationOperatorTest}} is not friendly to the developers who help to do the migration test. We can move this comment to the doc of the {{FlinkKafkaProducerMigrationOperatorTest}} class.

Additionally, we can add more details about renaming the generated binary file to the format {{kafka-0.11-migration-kafka-producer-flink-"" + version + ""-snapshot}} that {{getOperatorSnapshotPath}} returned.

This will make it more friendly to developers.

More discussion please see [here|https://issues.apache.org/jira/browse/FLINK-13619?focusedCommentId=17025811&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17025811]."	FLINK	Closed	3	4	10999	pull-request-available
13218235	Update FlinkKafkaConsumerBaseMigrationTest for 1.8	Update {{FlinkKafkaConsumerBaseMigrationTest}} so that it covers restoring from 1.8.	FLINK	Resolved	1	7	10999	pull-request-available
13118190	User code ClassLoader not set before calling ProcessingTimeCallback	"The user code ClassLoader is not set as the context ClassLoader for the thread invoking {{ProcessingTimeCallback#onProcessingTime(long timestamp)}}:
https://github.com/apache/flink/blob/84a07a34ac22af14f2dd0319447ca5f45de6d0bb/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L222

This is problematic, for example, if user code dynamically loads classes in {{ProcessFunction#onTimer(long timestamp, OnTimerContext ctx, Collector<O> out)}} using the current thread's context ClassLoader (also see FLINK-8005)."	FLINK	Closed	4	1	10999	pull-request-available
13249299	Update StatefulJobSavepointMigrationITCase.scala to restore from 1.9 savepoint	Update {{StatefulJobSavepointMigrationITCase.scala}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13208275	Add keyed CoProcessFunction that allows accessing key	"Currently, we can access the key when using {{KeyedProcessFunction}} .

Simillar functionality would be very useful when processing connected keyed stream."	FLINK	Closed	3	2	10999	pull-request-available
13173769	Remove unit from fullRestarts metric docs	The [fullRestarts](https://ci.apache.org/projects/flink/flink-docs-master/monitoring/metrics.html#availability) metric documentation says that the unit for the metric is milliseconds, yet it is a simple count.	FLINK	Closed	3	4	10999	pull-request-available
13174041	Simplify taskmanager memory default values	"The default value for {{NETWORK_BUFFERS_MEMORY_MIN}} is currently defined is {{String.valueOf(64L << 20)}}, which in the documentation is represented as {{""67108864""}}.

Now that we have the {{MemorySize}} utility we can change the default to {{""64 mb""}}.

The same applies to {{NETWORK_BUFFERS_MEMORY_MAX}}."	FLINK	Closed	4	4	10999	pull-request-available
13268213	Remove STATE_UPDATER in Execution	After making access to ExecutionGraph single-threaded in FLINK-11417, we can simplify execution state update and get rid of STATE_UPDATER.	FLINK	Closed	3	7	10999	pull-request-available
13181842	Handle oversized metric messages	Since the {{MetricQueryService}} is implemented as an Akka actor, it can only send messages of a smaller size then the current {{akka.framesize}}. We should check similarly to FLINK-10251 whether the payload exceeds the maximum framesize and fail fast if it is true.	FLINK	Closed	2	7	10999	pull-request-available
13163763	Implement TTL config	"`TtlConfig` has to be in flink core module.

Another option is to consider adding TtlConfig builder."	FLINK	Closed	3	7	10999	pull-request-available
13249309	Update FlinkKinesisConsumerMigrationTest to restore from 1.9 savepoint	Update {{FlinkKinesisConsumerMigrationTest}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13169904	Make location for job graph files configurable	"During the job-submission by the {{RestClusterClient}} the {{JobGraph}} is serialized and written to a file.
Currently we just use {{Files.createTempFile}} for this purposes.
This location should be made configurable."	FLINK	Closed	3	4	10999	pull-request-available
13269008	Cleanup the description about container number config option in Scala and python shell doc	Currently, the config option {{-n}} for Flink on Yarn has not been supported since Flink 1.8+. FLINK-12362 did the cleanup job about this config option. However, the scala shell and python doc still contains some description about {{-n}} which may make users confused. This issue used to track the cleanup work.	FLINK	Closed	3	4	10999	pull-request-available
13409831	Remove CheckpointStatsTracker#getJobCheckpointingConfiguration	"The CheckpointStatsTracker is currently being used by the EG as some form of container for the checkpoint coordinator configuration.
We can just store this configuration in the EG, further decoupling the tracker from the EG."	FLINK	Closed	3	7	11245	pull-request-available
13384461	Move chill dependency to flink-scala	flink-runtime contains a few serializer classes that rely on scala. We could move these to flink-scala, which already depends on scala anyway, and is also bundled in flink-dist.	FLINK	Closed	3	7	11245	pull-request-available
13569337	Token delegation doesn't work with Presto S3 filesystem	"AFAICT it's not possible to use token delegation with the Presto filesystem.
The token delegation relies on the {{DynamicTemporaryAWSCredentialsProvider}}, but it doesn't have a constructor that presto required (ruling out presto.s3.credentials-provider), and other providers can't be used due to FLINK-13602."	FLINK	Closed	3	1	11245	pull-request-available
13181340	Remove usage of javax.xml.bind.DatatypeConverter	In java 9 {{javax.xml.bind.DatatypeConverter}} is no longer accessible by default. Since this calss is only used in 3 instances (and only the single method {{parseHexBinary}}) we should replace it with another implementation.	FLINK	Resolved	3	7	11245	pull-request-available
13487278	Remove japicmp dependency bumps	"Way back when we worked on Java 11 support we bumped several dependencies from japicmp.
These are no longer required for the latest version that we're using."	FLINK	Closed	3	11500	11245	pull-request-available
13278716	Bump jetty-util-ajax to 9.3.24	"{{flink-fs-hadoop-azure}} has transitive dependency on jetty-util-ajax:9.3.19, which has a security vulnerability: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-7657

This was fixed in {{9.3.24.v20180605}} ([source|https://bugs.eclipse.org/bugs/show_bug.cgi?id=535668]). Starting from version 3.2.1 {{hadoop-azure}} is using this version as well, but for a quick resolution I propose bumping this single dependency for the time being."	FLINK	Closed	3	4	11245	pull-request-available
13425932	NullArgumentException when accessing checkpoint stats on standby JobManager	"We have a job running on one node
after increasing number of nodes to e.g. 3 on a new nodes job starts failing with 
{noformat}ERROR Unhandled exception. (org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler:260)
 org.apache.commons.math3.exception.NullArgumentException: input array
         at org.apache.commons.math3.util.MathArrays.verifyValues(MathArrays.java:1650) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.commons.math3.stat.descriptive.AbstractUnivariateStatistic.test(AbstractUnivariateStatistic.java:158) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:272) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:241) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics$CommonMetricsSnapshot.getPercentile(DescriptiveStatisticsHistogramStatistics.java:158) >
         at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics.getQuantile(DescriptiveStatisticsHistogramStatistics.java:52) ~[flink-dist_2.12-1.14.3.>
         at org.apache.flink.runtime.checkpoint.StatsSummarySnapshot.getQuantile(StatsSummarySnapshot.java:108) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.flink.runtime.rest.messages.checkpoints.StatsSummaryDto.valueOf(StatsSummaryDto.java:81) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
         at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.createCheckpointingStatistics(CheckpointingStatisticsHandler.java:129) ~[fli>
         at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:84) ~[flink-dist_2.12-1.14>
         at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:58) ~[flink-dist_2.12-1.14>
         at org.apache.flink.runtime.rest.handler.job.AbstractAccessExecutionGraphHandler.handleRequest(AbstractAccessExecutionGraphHandler.java:68) ~[flink-dist_2.12-1.14.3>
         at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:87) ~[flink-dist_2.12-1.14.3.ja>
         at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) [?:?]
         at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) [?:?]
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
         at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
         at java.lang.Thread.run(Thread.java:829) [?:?]
{noformat}"	FLINK	Closed	3	1	11245	pull-request-available
13227116	Drop Elasticsearch 1 connector	"I'm proposing to remove the connector for elasticsearch 1.

The connector is used significantly less than more recent versions (2&5 
are downloaded 4-5x more), and hasn't seen any development for over a 
hear, yet still incurred maintenance overhead due to licensing and testing.

The issue was discussed on the ML; no concerns were raised.

ML discussion: https://lists.apache.org/thread.html/41bf3a6f7d3d21dc4311607cccb41f2f17daf251ba70c4fe267c094d@%3Cdev.flink.apache.org%3E"	FLINK	Closed	3	4	11245	pull-request-available
13318448	Bump netty to 4.1.49	"Bump netty to 4.1.49 for some security fixes.

This also entails bumping netty-tcnative to 2.30.0 ."	FLINK	Closed	3	7	11245	pull-request-available
13200263	flink-metrics-ganglia has LGPL dependency	"{{flink-metrics-ganglia}} depends on {{info.ganglia.gmetric4j:gmetric4j}} which depends on {{org.acplt:oncrpc}}.

{{org.acplt:oncrpc}} is licensed under the LGPL, which is a [category-x|https://www.apache.org/legal/resolved.html#what-can-we-not-include-in-an-asf-project-category-x] license.

For the time being we should drop this module from all future releases."	FLINK	Closed	3	1	11245	pull-request-available
13446832	table-planner should not use CodeSplitUtils#newName	"The {{table-planner}} has a direct dependency on the {{table-code-splitter}}, as several CastRules use {{CodeSplitUtil.newName}}.

This dependency is a bit hidden. In the IDE it is pulled in transitively via {{table-runtime}}, and in maven it uses the {{table-code-splitter}} dependency bundled by {{table-runtime}}.

It would be nice if we could add a {{provided}} dependency to the {{table-code-splitter}} to properly document this."	FLINK	Closed	3	7	11245	pull-request-available
13539448	Disable Hbase 2.x tests on Java 17	Lacking support on the HBase side. Version bumps may solve it, but that's out of scope of this issue since the connector is being externalized.	FLINK	Closed	3	7	11245	pull-request-available
13432827	Accepting slots without a matching requirement leads to unfulfillable requirements	"To allow recovered TMs to eagerly re-offer their slots we allowed the registration of slots without a matching requirement if the job is currently restarting.

All slots that the pool accepts are mapped to a certain requirement, in order to determine whether sufficient slots were received yet. If a slot is reserved for a requirement that does not coincide with the mapping the pool come up with, then the mapping and requirements are changed accordingly to ensure we still request sufficient slots.

This leads to issues with slots that were accepted without a matching requirement. Those were mapped to the actual resource profile of the slot (to fit into the book-keeping). With the above logic in place this could lead to a specific resource requirement being added, which the remaining JM components are not aware of (and thus will never get rid of)."	FLINK	Closed	1	1	11245	pull-request-available
12720033	DistributedCache doesn't preserve executable flag	"all files distributed with the DistributedCache are marked as not executable, which causes a problem for the python interface. (I currently try to package all scripts/dependencies into binaries and distribute those, since we cant distribute directories).

a fix for this would to to modify Filecache.CopyProcess.call() to include
```
new File(tmp.toString()).setExecutable(true);
```
but this is more of a shotgun approach, since now all files are executable. Is this an acceptable solution?

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/852
Created by: [zentol|https://github.com/zentol]
Labels: enhancement, user satisfaction, 
Milestone: Release 0.5.1
Created at: Sat May 24 13:44:04 CEST 2014
State: open
"	FLINK	Resolved	3	4	11245	github-import
13171462	Add ITCase for interactions of Jar handlers	"We have a number of jar handlers with varying responsibilities: accepting jar uploads, listing uploaded jars, running jars etc. .
These handlers may rely on the behavior of other handlers; for example they might expect a specific naming scheme.
We should add a test to ensure that a common life-cycle for a jar (upload -> list -> show plan -> run -> delete) doesn't cause problems."	FLINK	Closed	3	4	11245	pull-request-available
13513174	Rework sql_download_table shortcode	"The existing sql_[connector_]download_table shortcodes rely on data in the Flink source (data/sql_connectors.yml).

Instead we can move this data to the externalized connectors, and parameterize the data lookup with the connector name."	FLINK	Closed	3	11500	11245	pull-request-available
13433905	Migrate documentation build to Github Actions	"INFRA recently setup the required credentials to rsync content to nightlies.apache.org via github actions. This means we can migrate out buildbot setup to github actions instead.

This should make maintenance a lot easier, as we'd have more control over the environment. It'd also make it way easier to discover."	FLINK	Closed	3	11500	11245	pull-request-available
13410263	"""Post-job: Cache Maven local repo"" failed on Azure"	"{code:java}
2021-11-05T13:49:20.5298458Z Resolved to: maven|Linux|kL7EJ8TeMrJ0VZs51DUWRqheXcKK2cN2spGtx9IbVxQ=
2021-11-05T13:49:21.0445785Z ApplicationInsightsTelemetrySender will correlate events with X-TFS-Session b5cbe6a0-61e7-4345-81b6-efaed3924cbe
2021-11-05T13:49:21.0700758Z Getting a pipeline cache artifact with one of the following fingerprints:
2021-11-05T13:49:21.0702157Z Fingerprint: `maven|Linux|kL7EJ8TeMrJ0VZs51DUWRqheXcKK2cN2spGtx9IbVxQ=`
2021-11-05T13:49:21.3648278Z There is a cache miss.
2021-11-05T13:50:26.4782603Z tar: c9692460fbd54c808fca7be315d83578_archive.tar: Wrote only 2048 of 10240 bytes
2021-11-05T13:50:26.4784975Z tar: Error is not recoverable: exiting now
2021-11-05T13:50:27.0397318Z ApplicationInsightsTelemetrySender correlated 1 events with X-TFS-Session b5cbe6a0-61e7-4345-81b6-efaed3924cbe
2021-11-05T13:50:27.0531774Z ##[error]Process returned non-zero exit code: 2
2021-11-05T13:50:27.0666804Z ##[section]Finishing: Cache Maven local repo
{code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26018&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=96bd9872-da2e-43b4-b013-1295f1c23a41&l=220]"	FLINK	Reopened	2	1	11245	stale-assigned, test-stability
13216921	Enforce spaces around lambdas	Enforce spaces around lambdas so that {{()->x}} would be rejected. We commonly add spaces around lambdas, might as well enforce it properly.	FLINK	Closed	3	4	11245	pull-request-available
13240405	Remove legacy flink-python APIs	As discussed on the [mailing list|http://mail-archives.apache.org/mod_mbox/flink-user/201906.mbox/%3cCANC1h_uSoBi0nG1wL-4EATBSU_h2t46g=b9i8tEUSMxrMXRoBw@mail.gmail.com%3e], remove the older batch&streaming python API.	FLINK	Closed	3	4	11245	pull-request-available
13304580	Migrate e2e tests to flink-docker	"Some end-to-end tests make use of flink-container/docker.
We need to migrate them to flink-docker."	FLINK	Closed	3	7	11245	pull-request-available
13541770	Move common RPC utils to rpc-core	The ClassLoadingUtils and CleanupOnCloseRpcSystem classes are useful for any rpc system implementation and should thus be shared.	FLINK	Closed	3	7	11245	pull-request-available
13410615	AdaptiveScheduler#getJobStatus never returns CREATED	"None of the AdaptiveScheduler states return {{JobStatus#CREATED}} when {{requestJobStatus()}} is called. This violates the job state machine, and makes it a bit difficult to setup job state timestamps in the {{AdaptiveScheduler}} (FLINK-24775).

We are in the {{INITIALIZING}} state from {{Created}} -> {{WaitingForResources}} -> {{CreatingExecutionGraph}}, and then switch straight to RUNNING in {{Executing}}.

It is tricky to retain the same semantics for the {{Default}} and {{AdaptiveScheduler}}, but I think it would be fine to return {{CREATED}} once we reached {{WaitingForResources}} because from the users-perspective it behaves similarly."	FLINK	Closed	3	1	11245	pull-request-available
13323346	Sort global job parameters in WebUI job configuration view	"The configuration page for a job has various entries for set GlobalJobParameters.

It would be nice if these were sorted alphabetically."	FLINK	Closed	4	4	11245	pull-request-available
13398105	SavepointITCase.testStopWithSavepointWithDrainCallsFinishBeforeSnapshotState fails due to 'Service temporarily unavailable due to an ongoing leader election.'	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23043&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4857

{code}
Aug 29 22:12:23 [ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 50.922 s <<< FAILURE! - in org.apache.flink.test.checkpointing.SavepointITCase
Aug 29 22:12:23 [ERROR] testStopWithSavepointWithDrainCallsFinishBeforeSnapshotState  Time elapsed: 1.854 s  <<< ERROR!
Aug 29 22:12:23 java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Service temporarily unavailable due to an ongoing leader election. Please refresh.]
Aug 29 22:12:23 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Aug 29 22:12:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Aug 29 22:12:23 	at org.apache.flink.test.checkpointing.SavepointITCase.lambda$waitUntilAllTasksAreRunning$5(SavepointITCase.java:919)
Aug 29 22:12:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:152)
Aug 29 22:12:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:136)
Aug 29 22:12:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:128)
Aug 29 22:12:23 	at org.apache.flink.test.checkpointing.SavepointITCase.waitUntilAllTasksAreRunning(SavepointITCase.java:909)
Aug 29 22:12:23 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointWithDrainCallsFinishBeforeSnapshotState(SavepointITCase.java:246)
{code}"	FLINK	Closed	3	11500	11245	pull-request-available, test-stability
13364975	Replace StreamTaskTestHarness#TestTaskMetricGroup	Reduce reliance on MetricGroup constructors by creating the metric groups through the usual API.	FLINK	Closed	3	7	11245	auto-unassigned, pull-request-available
13447180	Remove netty dependency in flink-test-utils	For some reason we bundle a relocated version of netty in flink-test-utils. AFAICT this should be unnecessary because nothing makes use of the relocated version.	FLINK	Closed	3	7	11245	pull-request-available
13397723	Pin registration IDs of Java serializers	"We currently register a bunch of serializers with Kryo without specifying registration IDs. As a result we are locked in to a specific number of serializers.

In order to allow the KryoSerializer to be used without the Scala serializers being registered, we need to pin the registration IDs of our current java serializers. This also includes user-defined serializers in the execution config."	FLINK	Closed	3	7	11245	pull-request-available
13399987	Deadlock in QueryableState Client	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23750&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=15476

{code}
 Found one Java-level deadlock:
Sep 08 11:12:50 =============================
Sep 08 11:12:50 ""Flink Test Client Event Loop Thread 0"":
Sep 08 11:12:50   waiting to lock monitor 0x00007f4e380309c8 (object 0x0000000086b2cd50, a java.lang.Object),
Sep 08 11:12:50   which is held by ""main""
Sep 08 11:12:50 ""main"":
Sep 08 11:12:50   waiting to lock monitor 0x00007f4ea4004068 (object 0x0000000086b2cf50, a java.lang.Object),
Sep 08 11:12:50   which is held by ""Flink Test Client Event Loop Thread 0""

{code}"	FLINK	Closed	3	1	11245	pull-request-available, test-stability
13308875	MesosResourceManagerTest.testWorkerStarted:656 » NullPointer	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8113&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=8d823410-c7c7-5a4d-68bb-fa7b08da17b9

{code}

[ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.905 s <<< FAILURE! - in org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest
[ERROR] testWorkerStarted(org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest)  Time elapsed: 0.219 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.runtime.resourcemanager.ResourceManager.sendSlotReport(ResourceManager.java:406)
	at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest$7.<init>(MesosResourceManagerTest.java:680)
	at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerTest.testWorkerStarted(MesosResourceManagerTest.java:656)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)

{code}"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13263077	SavepointMigrationTestBase deadline should be setup in the test	"The {{SavepointMigrationTestBase}} contains a {{static final Deadline}} that is used in all tests. In practice this means that the deadline is quite unreliable, since it is setup when the class is instantiated, opposed to any tests being run.

If fork-reuse is enabled the tests consistently fail with a timeout for this reason."	FLINK	Closed	3	7	11245	pull-request-available
13143903	Web UI does not render error messages correctly in FLIP-6 mode	"*Description*

The Web UI renders error messages returned by the REST API incorrectly, e.g., on the job submission page. The JSON returned by the REST API is rendered as a whole. However, the UI should only render the contents of the {{errors}} field.

*Steps to reproduce*

Submit {{examples/streaming/SocketWindowWordCount.jar}} without specifying program arguments. Error message will be rendered as follows:
{noformat}
{""errors"":[""org.apache.flink.client.program.ProgramInvocationException: The program plan could not be fetched - the program aborted pre-maturely.\n\nSystem.err: (none)\n\nSystem.out: No port specified. Please run 'SocketWindowWordCount --hostname <hostname> --port <port>', where hostname (localhost by default) and port is the address of the text server\nTo start a simple text server, run 'netcat -l <port>' and type the input text into the command line\n""]}
{noformat}
Note that flip6 mode must be enabled.

"	FLINK	Closed	2	1	11245	flip6
13386930	JobMasterITCase fail on azure due to BindException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19753&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4251

{code}
Jul 01 00:00:27 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.272 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterITCase
Jul 01 00:00:27 [ERROR] testRejectionOfEmptyJobGraphs(org.apache.flink.runtime.jobmaster.JobMasterITCase)  Time elapsed: 3.009 s  <<< ERROR!
Jul 01 00:00:27 org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
Jul 01 00:00:27 	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:275)
Jul 01 00:00:27 	at org.apache.flink.runtime.minicluster.MiniCluster.createDispatcherResourceManagerComponents(MiniCluster.java:470)
Jul 01 00:00:27 	at org.apache.flink.runtime.minicluster.MiniCluster.setupDispatcherResourceManagerComponents(MiniCluster.java:429)
Jul 01 00:00:27 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:373)
Jul 01 00:00:27 	at org.apache.flink.runtime.jobmaster.JobMasterITCase.testRejectionOfEmptyJobGraphs(JobMasterITCase.java:56)
Jul 01 00:00:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 01 00:00:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 01 00:00:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 01 00:00:27 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 01 00:00:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 01 00:00:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 01 00:00:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 01 00:00:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 01 00:00:27 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 01 00:00:27 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 01 00:00:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 01 00:00:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 01 00:00:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 01 00:00:27 Caused by: java.net.BindException: Could not start rest endpoint on any port in port range 8081
Jul 01 00:00:27 	at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:234)
Jul 01 00:00:27 	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:172)
Jul 01 00:00:27 	... 34 more
{code}"	FLINK	Closed	3	1	11245	pull-request-available, test-stability
13195886	MetricGroup#getAllVariables can deadlock	"{{AbstractMetricGroup#getAllVariables}} acquires the locks of both the current and all parent groups when assembling the variables map. This can lead to a deadlock if metrics are registered concurrently on a child and parent if the child registration is applied first and the reporter uses said method (which many do).

Assume we have a MetricGroup Mc(hild) and Mp(arent).

2 separate threads Tc and Tp each register a metric on their respective group, acquiring the lock.
Let's assume that Tc has a slight headstart.
Tc will now call {{MetricRegistry#register}} first, acquiring the MR lock.
Tp will block on this lock.

Tc now iterates over all reporters calling {{MetricReporter#notifyOfAddedMetric}}. Assume that in this method {{MetricGroup#getAllVariables}} is called on Mc by Tc.
Tc still holds the lock to Mc, and attempts to acquire the lock to Mp.
The lock to Mp is still held by Tp however, which waits for the MR lock to be released by Tc.

Thus a deadlock is created. This may deadlock anything, be it minor threads, tasks, or entire components.

This has not surfaced so far since usually metrics are no longer added to a group once children have been created (since the component initialization at that point is complete)."	FLINK	Closed	2	1	11245	pull-request-available
13487100	MiniClusterExtension should respect DEFAULT_PARALLELISM if set	"MiniClusterExtension#registerEnv sets the default parallelism of the environment to the number of the slots the cluster has.

This effectively prevents multiple jobs from running on the same MiniCluster unless they specify a parallelism via the API.
This isn't ideal since it means you can't easily mix workloads during testing.

It would be better if the cluster would check the config for whether {{DEFAULT_PARALLELISM}} was set."	FLINK	Closed	3	4	11245	pull-request-available
13239501	Add travis profile for gelly/kafka	The misc/tests profiles frequently hit timeouts; we can resolve this by moving gelly and the universal kafka connector into a separate profile.	FLINK	Closed	3	4	11245	pull-request-available
13291983	Misleading SlotManagerImpl logging for slot reports of unknown task manager	"If the SlotManager receives a slot report from an unknown task manager it logs 2 messages:
{code}
public boolean reportSlotStatus(InstanceID instanceId, SlotReport slotReport) {
	[...]
	LOG.debug(""Received slot report from instance {}: {}."", instanceId, slotReport);

	TaskManagerRegistration taskManagerRegistration = taskManagerRegistrations.get(instanceId);

	if (null != taskManagerRegistration) {
		[...]
	} else {
		LOG.debug(""Received slot report for unknown task manager with instance id {}. Ignoring this report."", instanceId);
		[...]
	}
}
{code}

This leads to misleading output since it appears like the slot manager received 2 separate slot reports, with the first being for a known instance, the latter for an unknown one. This cost some time as I couldn't figure out why the ""latter"" report was suddenly being rejected.

I propose moving the first debug message into the non-null branch.

[~trohrmann] WDYT?"	FLINK	Closed	3	4	11245	pull-request-available
13215264	Prefix matching in ConfigDocsGenerator can result in wrong assignments	"There are some cases where the key-prefix matching does not work as intended:

* given the prefixes ""a.b"" and ""a.b.c.d"", then an option with a key ""a.b.c.X"" will be assigned to the default groups instead of ""a.b""
* given a prefix ""a.b"", an option ""a.c.b"" will be matched to that group instead of the default
"	FLINK	Closed	3	1	11245	pull-request-available
13424717	JsonTypeInfo property should be valid java identifier	"Some REST classes use the JsonTypeInfo with the property being named {{@class}}. This causes invalid java code to be generated by swagger.
Rename it to {{clazz}}."	FLINK	Closed	4	1	11245	pull-request-available
13450399	Replace IOUtils dependency on oss filesystem	"The oss fs has an undeclared dependency on commons-io for a single call to IOUtils.
We can make our lives a little bit easier by using the Flink IOUtils instead."	FLINK	Closed	3	7	11245	pull-request-available
13449200	Race condition between task/savepoint notification failure	"When a task throws an exception in notifyCheckpointComplete we send 2 messages to the JobManager:
1) we inform the CheckpointCoordinator about the failed savepoint
2) we inform the scheduler about the failed task.

Depending on how these arrive the adaptive scheduler exhibits different behaviors. If 1) arrives first it properly informs the user about the created savepoint which might contain uncommitted transactions; if 2) arrives first it just restarts the job.

I'm not sure how big of an issue the latter case is, but it does invalidate FLINK-26923.

In any case we might want to consider having the StopWithSavepoint state wait until the savepoint future has failed before doing anything else."	FLINK	Closed	3	1	11245	pull-request-available
13421976	Remove excessive surefire-plugin versions	Various modules overwrite the default surefire version. We should remove that unless there is a good reason to do so.	FLINK	Closed	3	11500	11245	pull-request-available
13338051	Unable to exclude metrics variables for the last metrics reporter.	"We discovered a bug that leads to the setting {{scope.variables.excludes}} being ignored for the very last metric reporter.

Because {{reporterIndex}} was incremented before the length check, the last metrics reporter setting is overflowed back to 0.

Interestingly, this bug does not trigger when there's only one metric reporter, because slot 0 is actually overwritten with that reporter's variables instead of being used to store all variables in that case.
{code:java}
public abstract class AbstractMetricGroup<A extends AbstractMetricGroup<?>> implements MetricGroup {
...
	public Map<String, String> getAllVariables(int reporterIndex, Set<String> excludedVariables) {
		// offset cache location to account for general cache at position 0
		reporterIndex += 1;
		if (reporterIndex < 0 || reporterIndex >= logicalScopeStrings.length) {
			reporterIndex = 0;
		}
		// if no variables are excluded (which is the default!) we re-use the general variables map to save space
		return internalGetAllVariables(excludedVariables.isEmpty() ? 0 : reporterIndex, excludedVariables);
	}

...
{code}
 [Github link to the above code|https://github.com/apache/flink/blob/3bf5786655c3bb914ce02ebb0e4a1863b205b829/flink-runtime/src/main/java/org/apache/flink/runtime/metrics/groups/AbstractMetricGroup.java#L122]"	FLINK	Closed	3	1	11245	pull-request-available
13441761	Drop scala 2.11 support from the flink-docker	Since now the scala 2.11 support has been dropped, we also dropped the support of scala 2.11 from release 1.15	FLINK	Closed	3	4	11245	pull-request-available
13165944	dist assemblies access jars outside of flink-dist	"The flink-dist assemblies access compiled jars outside of flink-dist, for example like this:
{code:java}
<source>../flink-libraries/flink-cep/target/flink-cep_${scala.binary.version}-${project.version}.jar</source>{code}
As usual, accessing files outside of the module that you're building is a terrible idea.

It's brittle as it relies on paths that aren't guaranteed to be stable, and requires these modules to be built beforehand. There's also an inherent potential for dependency conflicts when building flink-dist on it's own, as maven may download certain snapshot artifacts, but the assemblies ignore these and bundle jars present in Flink.

We can use the maven-dependency plugin to copy required dependencies into the {{target}} directory of flink-dist, and point the assemblies to these jars."	FLINK	Closed	3	1	11245	pull-request-available
13502880	Deduplicate plugin parser loops	The flink-ci-tools parse the output of various modules and each have their own looping logic to that end, that are pretty much identical though.	FLINK	Closed	3	11500	11245	pull-request-available
13361862	Skip requirement checks if slot report does not indicate any change	"While testing the reactive mode, I noticed that my job started normally, but after a few minutes, it started logging this:

{code}
2021-03-02 13:36:48,075 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Timestamps/Watermarks (2/3) (061b652dabc0ecfc83c942ee3e127ecd) switched from DEPLOYING to RUNNING.
2021-03-02 13:36:48,076 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (2/3) (6a715e3c70754aafa0b91332b69a736d) switched from DEPLOYING to RUNNING.
2021-03-02 13:36:48,077 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (1/3) (8655874da6905d13c01927a282ed2ce0) switched from DEPLOYING to RUNNING.
2021-03-02 13:36:48,080 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (3/3) (9514718713ffa453c43a7e7efde9920a) switched from DEPLOYING to RUNNING.
2021-03-02 13:40:28,893 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:29,474 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:29,475 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:29,475 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:39,495 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:39,496 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:39,497 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:49,518 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:49,518 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:49,519 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:59,536 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:59,536 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:40:59,537 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:41:09,556 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:41:09,557 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:41:09,557 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
2021-03-02 13:41:19,577 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 1283d12b281c35f33f3602611ef43b35.
{code}"	FLINK	Closed	2	7	11245	pull-request-available
13224606	Refactor MemoryLogger to accept termination future instead of ActorSystem	"The {{MemoryLogger}} currently accepts an {{ActorSystem}} and uses it to determine whether the logging thread should be shut down.

For this purpose a termination future would be sufficient."	FLINK	Closed	3	4	11245	pull-request-available
13282120	Refactor download page to make hadoop less prominant	"The downloads page is listing shaded-hadoop downloads for every Flink release.

This is ultimately redundant since we don't touch the packaging, and it reinforces that putting shaded-hadoop into /lib is the ""default"" way of adding hadoop, when users should first try exposing the hadoop classpath.

As such we should move shaded-hadoop into the additional components section."	FLINK	Closed	3	4	11245	pull-request-available
13202798	NoClassDefFoundError in presto-s3 filesystem	"A user has reporter an issue on the ML where using the presto-s3 filesystem fails with an exception due to a missing class. The missing class is indeed filtered out in the shade-plugin configuration.
{code:java}
java.lang.NoClassDefFoundError: org/apache/flink/fs/s3presto/shaded/com/facebook/presto/hadoop/HadoopFileStatus
	at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.directory(PrestoS3FileSystem.java:446)
	at org.apache.flink.fs.s3presto.shaded.com.facebook.presto.hive.PrestoS3FileSystem.delete(PrestoS3FileSystem.java:423)
	at org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.delete(HadoopFileSystem.java:147)
	at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:80)
	at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.doDiscard(CompletedCheckpoint.java:250)
	at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:219)
	at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:72)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:844)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:756)
	at org.apache.flink.runtime.jobmaster.JobMaster.lambda$acknowledgeCheckpoint$8(JobMaster.java:680)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107){code}"	FLINK	Resolved	1	1	11245	pull-request-available
13385112	Add flink-rpc-akka module	"Add a new module containing all Akka-reliant classes.

As a first step we will just move classes, dependencies and build stuff to this module.
Loading this module through a separate classloader will be handled in a follow-up."	FLINK	Closed	3	7	11245	pull-request-available
13280343	JarHandlerTest.testRunJar fails on travis	"The test JarHandlerTest.testRunJar fails on travis with error below:


{code:java}
09:48:58.828 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.061 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest
09:48:58.829 [ERROR] testRunJar(org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest)  Time elapsed: 0.2 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a string containing ""ProgramInvocationException""
     but: was ""[Service temporarily unavailable due to an ongoing leader election. Please refresh.]""
	at org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.runTest(JarHandlerTest.java:125)
	at org.apache.flink.runtime.webmonitor.handlers.JarHandlerTest.testRunJar(JarHandlerTest.java:74)
{code}

Seems the cause is the client sends request before the leader election is done.

https://api.travis-ci.com/v3/job/276751147/log.txt"	FLINK	Closed	2	1	11245	pull-request-available
13350884	SystemResourcesCounterTest  may caused endless loop when some error occured	When the function run() in  class SystemResourcesCounter  come across Throwable exception, the loop in the function run() of the SystemResourcesCounter class will break, the SystemResourcesCounter thread will finish and the loop in the class SystemResourcesCounterTest will continue because of the condition is still true, the test will in a endless loop.	FLINK	Closed	4	1	11245	auto-deprioritized-major, pull-request-available
13477547	Add utility to test POJO compliance without any Kryo usage	Add a variant of the test util added in FLINK-28636 that additionally asserts that Kryo is not used for any contained field.	FLINK	Closed	3	4	11245	pull-request-available
13493981	Clear static Jackson TypeFactory cache on CL release	"The Jackson TypeFactory contains a singleton instance that is at times used by Jackson, potentially containing user-classes for longer than necessary.

https://github.com/FasterXML/jackson-databind/issues/1363

We could clear this cache whenever a user code CL is being released similar to what was done in BEAM-6460."	FLINK	Closed	3	4	11245	pull-request-available
13366059	Outdated slot offer responses may release slot prematurely	"Here is the error stack:
{code:java}
2021-03-18 19:05:31org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy   
   at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:130 undefined)   
   at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:81 undefined)   
   at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221 undefined)   
   at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212 undefined)   
   at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203 undefined)   
   at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:701 undefined)   
   at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51 undefined)   
   at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1449 undefined)   
   at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1105 undefined)   
   at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1045 undefined)   
   at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:754 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:195 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:182 undefined)   
   at org.apache.flink.runtime.scheduler.SharedSlot.lambda$release$4(SharedSlot.java:271 undefined)   
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656 undefined)   
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:669 undefined)   
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:1997 undefined)   
   at org.apache.flink.runtime.scheduler.SharedSlot.release(SharedSlot.java:271 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:385 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.lambda$releaseSlot$1(DefaultDeclarativeSlotPool.java:376 undefined)   
   at java.util.Optional.ifPresent(Optional.java:159 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlot(DefaultDeclarativeSlotPool.java:374 undefined)   
   at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.failAllocation(DeclarativeSlotPoolService.java:198 undefined)   
   at org.apache.flink.runtime.jobmaster.JobMaster.internalFailAllocation(JobMaster.java:650 undefined)   
   at org.apache.flink.runtime.jobmaster.JobMaster.failSlot(JobMaster.java:636 undefined)   
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62 undefined)   
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43 undefined)   
   at java.lang.reflect.Method.invoke(Method.java:498 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:301 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212 undefined)   
   at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158 undefined)   
   at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26 undefined)   
   at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21 undefined)   
   at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123 undefined)   
   at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21 undefined)   
   at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170 undefined)   
   at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171 undefined)   
   at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171 undefined)   
   at akka.actor.Actor$class.aroundReceive(Actor.scala:517 undefined)   
   at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225 undefined)   
   at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592 undefined)   
   at akka.actor.ActorCell.invoke(ActorCell.scala:561 undefined)   
   at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258 undefined)   
   at akka.dispatch.Mailbox.run(Mailbox.scala:225 undefined)   
   at akka.dispatch.Mailbox.exec(Mailbox.scala:235 undefined)   
   at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260 undefined)   
   at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339 undefined)   
   at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979 undefined)   
   at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107 undefined)
Caused by: org.apache.flink.util.FlinkException: Could not mark slot 61a637e3977c58a0e6b73533c419297d active.   
   at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$handleAcceptedSlotOffers$18(TaskExecutor.java:1469 undefined)   
   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760 undefined)   
   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736 undefined)   
   at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440 undefined)   
   at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208 undefined)    ... 19 more
{code}"	FLINK	Closed	1	1	11245	pull-request-available
13211595	Upgrade Akka to 2.5	"{noformat}
2019-01-24 14:43:52,059 ERROR akka.remote.Remoting                                          - class [B cannot be cast to class [C ([B and [C are in module java.base of loader 'bootstrap')
java.lang.ClassCastException: class [B cannot be cast to class [C ([B and [C are in module java.base of loader 'bootstrap')
        at akka.remote.artery.FastHash$.ofString(LruBoundedCache.scala:18)
        at akka.remote.serialization.ActorRefResolveCache.hash(ActorRefResolveCache.scala:61)
        at akka.remote.serialization.ActorRefResolveCache.hash(ActorRefResolveCache.scala:55)
        at akka.remote.artery.LruBoundedCache.getOrCompute(LruBoundedCache.scala:110)
        at akka.remote.RemoteActorRefProvider.resolveActorRef(RemoteActorRefProvider.scala:403)
        at akka.actor.SerializedActorRef.readResolve(ActorRef.scala:433)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at java.base/java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1250)
        at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2096)
        at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1594)Running a jobmanager with java 11 fail with the following call stack:
{noformat}

Flink master is using akka 2.4.20.

After some investigation, the error in akka comes from the following line:
{code}
def ofString(s: String): Int = {
val chars = Unsafe.instance.getObject(s, EnvelopeBuffer.StringValueFieldOffset).asInstanceOf[Array[Char]]
{code}

from java 9 it is now an array of byte. The akka code in the newer version is:
{code}
    public static int fastHash(String str) {
          ...
        if (isJavaVersion9Plus) {
            final byte[] chars = (byte[]) instance.getObject(str, stringValueFieldOffset);
            ...
        } else {
            final char[] chars = (char[]) instance.getObject(str, stringValueFieldOffset);
 {code}"	FLINK	Closed	4	7	11245	pull-request-available
13315759	Race condition between task acknowledgement and first heartbeat	"FLINK-17075 introduced a reconciliation mechanism for detecting missing/unknown deployments.
For this purpose, we begin tracking deployments after the task submission was acknowledged by the TaskExecutor, and match these against the deployments on the TaskExecutor as reported by the heartbeats.

~~However, {{TaskManagerGateway#submitTask}} is not called in the main thread (so it doesn't spend time on serializating the TaskDeploymentDescriptor), and as such the receival of the acknowledgement is also not running in the main thread.~~

For some reason it appears that heartbeat requests sent after the acknowledge can end up being processed before the acknowledge.
This is pretty easy to reproduce by reverting 6e811e6f21e5baa6bfb1862a51815e1d151c7097 and introducing a sleep into the processing of the task submission ackowledgement within Execution#deploy().

As shown in the AccumulatorLiveITCase, this manifests as a task that was just deployed to being canceled.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56]

{code}
2020-07-08T21:46:15.4438026Z Printing stack trace of Java process 40159
2020-07-08T21:46:15.4442864Z ==============================================================================
2020-07-08T21:46:15.4475676Z Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2020-07-08T21:46:15.7672746Z 2020-07-08 21:46:15
2020-07-08T21:46:15.7673349Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-07-08T21:46:15.7673590Z 
2020-07-08T21:46:15.7673893Z ""Attach Listener"" #86 daemon prio=9 os_prio=0 tid=0x00007fef8c025800 nid=0x1b231 runnable [0x0000000000000000]
2020-07-08T21:46:15.7674242Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7674419Z 
2020-07-08T21:46:15.7675150Z ""flink-taskexecutor-io-thread-2"" #85 daemon prio=5 os_prio=0 tid=0x00007fef9c020000 nid=0xb03a waiting on condition [0x00007fefac1f3000]
2020-07-08T21:46:15.7675964Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7676249Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7680997Z 	- parking to wait for  <0x0000000087180a20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7681506Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7682009Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7682666Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7683100Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7683554Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7684013Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7684371Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7684559Z 
2020-07-08T21:46:15.7685213Z ""Flink-DispatcherRestEndpoint-thread-4"" #84 daemon prio=5 os_prio=0 tid=0x00007fef90431800 nid=0x9e49 waiting on condition [0x00007fef7df4a000]
2020-07-08T21:46:15.7685665Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7686052Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7686707Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7687184Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7687721Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7688342Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7688935Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7689579Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7690451Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7690928Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7691317Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7691502Z 
2020-07-08T21:46:15.7692183Z ""Flink-DispatcherRestEndpoint-thread-3"" #83 daemon prio=5 os_prio=0 tid=0x00007fefa01e2800 nid=0x9dc9 waiting on condition [0x00007fef7f1f4000]
2020-07-08T21:46:15.7692636Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7692920Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7693647Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7694105Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7694595Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7695178Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7695746Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7696445Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7696933Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7697394Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7697794Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7697986Z 
2020-07-08T21:46:15.7698687Z ""Flink-DispatcherRestEndpoint-thread-2"" #82 daemon prio=5 os_prio=0 tid=0x00007fefa01e1800 nid=0x9d97 waiting on condition [0x00007fef7e34c000]
2020-07-08T21:46:15.7699367Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7699671Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7700328Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7701040Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7701692Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7702310Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7702916Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7703457Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7704029Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7709397Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7709844Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7710036Z 
2020-07-08T21:46:15.7710763Z ""Thread-23"" #71 prio=5 os_prio=0 tid=0x00007fef81ddc800 nid=0x9d5e waiting on condition [0x00007fef7ebf0000]
2020-07-08T21:46:15.7711170Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7711465Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7712165Z 	- parking to wait for  <0x00000000871812a0> (a java.util.concurrent.CompletableFuture$Signaller)
2020-07-08T21:46:15.7712594Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7713049Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-07-08T21:46:15.7713499Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-07-08T21:46:15.7713957Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-07-08T21:46:15.7714396Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-07-08T21:46:15.7715121Z 	at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:107)
2020-07-08T21:46:15.7715618Z 	at org.apache.flink.test.accumulators.AccumulatorLiveITCase$1.go(AccumulatorLiveITCase.java:148)
2020-07-08T21:46:15.7716212Z 	at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:74)
2020-07-08T21:46:15.7716490Z 
2020-07-08T21:46:15.7717251Z ""AkkaRpcService-Supervisor-Termination-Future-Executor-thread-1"" #70 daemon prio=5 os_prio=0 tid=0x00007fefa401e000 nid=0x9d5c waiting on condition [0x00007fef7eff2000]
2020-07-08T21:46:15.7717761Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7718040Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7718688Z 	- parking to wait for  <0x00000000871814b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7719174Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7719714Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7720268Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7720731Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7721215Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7721689Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7722089Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7722279Z 
2020-07-08T21:46:15.7722934Z ""flink-taskexecutor-io-thread-1"" #69 daemon prio=5 os_prio=0 tid=0x00007fef9c18b000 nid=0x9d5b waiting on condition [0x00007fef7e74e000]
2020-07-08T21:46:15.7723481Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7723777Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7724461Z 	- parking to wait for  <0x0000000087180a20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7725025Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7725647Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7726288Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7726757Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7727243Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7727719Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7728118Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7728311Z 
2020-07-08T21:46:15.7728915Z ""pool-3-thread-1"" #68 prio=5 os_prio=0 tid=0x00007fefd8683800 nid=0x9d5a waiting on condition [0x00007fef7e44d000]
2020-07-08T21:46:15.7729425Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7729699Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7730550Z 	- parking to wait for  <0x0000000087181900> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7731026Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7731526Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7732122Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-07-08T21:46:15.7732706Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7733225Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7733691Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7734235Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7734622Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7734925Z 
2020-07-08T21:46:15.7735553Z ""Flink-MetricRegistry-thread-1"" #64 daemon prio=5 os_prio=0 tid=0x00007fef9c10a000 nid=0x9d52 waiting on condition [0x00007fef7ecf1000]
2020-07-08T21:46:15.7736123Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7736411Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7737066Z 	- parking to wait for  <0x0000000087181b80> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7737547Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7738100Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7738725Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7739426Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7739947Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7740517Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7740970Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7741454Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7741650Z 
2020-07-08T21:46:15.7742217Z ""pool-2-thread-1"" #61 prio=5 os_prio=0 tid=0x00007fefa0087000 nid=0x9d4f waiting on condition [0x00007fef7e84f000]
2020-07-08T21:46:15.7742628Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7742979Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7743629Z 	- parking to wait for  <0x0000000087181dc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7744110Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7744610Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7748719Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-07-08T21:46:15.7749446Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7749975Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7750562Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7751005Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7751365Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7751542Z 
2020-07-08T21:46:15.7752400Z ""jobmanager-future-thread-2"" #58 daemon prio=5 os_prio=0 tid=0x00007fef9c0ff800 nid=0x9d4c waiting on condition [0x00007fef7f2f5000]
2020-07-08T21:46:15.7752842Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7753133Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7753752Z 	- parking to wait for  <0x0000000087182040> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7754233Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7754770Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7755364Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7756121Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7756809Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7757294Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7757786Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7758170Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7758360Z 
2020-07-08T21:46:15.7759187Z ""FlinkCompletableFutureDelayScheduler-thread-1"" #57 daemon prio=5 os_prio=0 tid=0x00007fef9c0f8000 nid=0x9d4b waiting on condition [0x00007fef7f3f6000]
2020-07-08T21:46:15.7759642Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7759910Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7760656Z 	- parking to wait for  <0x0000000087182280> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7761106Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7761610Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7762188Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-07-08T21:46:15.7762857Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7763509Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7763977Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7764465Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7764863Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7765054Z 
2020-07-08T21:46:15.7765798Z ""jobmanager-future-thread-1"" #51 daemon prio=5 os_prio=0 tid=0x00007fefb4019800 nid=0x9d40 waiting on condition [0x00007fef7f0f3000]
2020-07-08T21:46:15.7766860Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7767199Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7767934Z 	- parking to wait for  <0x0000000087182040> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7768427Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7769046Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7769644Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7770228Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7770737Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7771326Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7771772Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7772144Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7772441Z 
2020-07-08T21:46:15.7773057Z ""mini-cluster-io-thread-8"" #46 daemon prio=5 os_prio=0 tid=0x00007fefa003e000 nid=0x9d3a waiting on condition [0x00007fef7f4f7000]
2020-07-08T21:46:15.7773594Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7773888Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7774524Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7775014Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7775545Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7776086Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7776692Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7777163Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7777654Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7778054Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7778243Z 
2020-07-08T21:46:15.7778993Z ""mini-cluster-io-thread-7"" #45 daemon prio=5 os_prio=0 tid=0x00007fefa003c800 nid=0x9d39 waiting on condition [0x00007fef7f5f8000]
2020-07-08T21:46:15.7779432Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7779716Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7780326Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7780804Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7781421Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7781934Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7782378Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7782815Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7783269Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7783628Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7784198Z 
2020-07-08T21:46:15.7785126Z ""mini-cluster-io-thread-6"" #44 daemon prio=5 os_prio=0 tid=0x00007fefa003b000 nid=0x9d38 waiting on condition [0x00007fef7f6f9000]
2020-07-08T21:46:15.7786719Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7787001Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7787771Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7788266Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7788785Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7789334Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7789797Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7790281Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7790770Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7791155Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7791350Z 
2020-07-08T21:46:15.7791985Z ""mini-cluster-io-thread-5"" #43 daemon prio=5 os_prio=0 tid=0x00007fefa4005000 nid=0x9d37 waiting on condition [0x00007fef7f7fa000]
2020-07-08T21:46:15.7792439Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7792718Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7793477Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7794120Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7794908Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7795453Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7795959Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7796446Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7796921Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7798102Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7798308Z 
2020-07-08T21:46:15.7799064Z ""mini-cluster-io-thread-4"" #42 daemon prio=5 os_prio=0 tid=0x00007fef81bcb000 nid=0x9d36 waiting on condition [0x00007fef7fafb000]
2020-07-08T21:46:15.7799503Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7799796Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7800432Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7800925Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7801557Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7802188Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7802641Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7803083Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7803537Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7803909Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7804085Z 
2020-07-08T21:46:15.7804669Z ""mini-cluster-io-thread-3"" #41 daemon prio=5 os_prio=0 tid=0x00007fefa0033800 nid=0x9d35 waiting on condition [0x00007fef7fbfc000]
2020-07-08T21:46:15.7805207Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7805488Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7806232Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7806723Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7807383Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7808642Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7809393Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7809867Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7810356Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7810742Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7810947Z 
2020-07-08T21:46:15.7811680Z ""mini-cluster-io-thread-2"" #40 daemon prio=5 os_prio=0 tid=0x00007fef81bc5800 nid=0x9d34 waiting on condition [0x00007fef7fcfd000]
2020-07-08T21:46:15.7812136Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7812413Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7813079Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7813561Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7814096Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7814648Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7815113Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7815598Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7816075Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7816474Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7816663Z 
2020-07-08T21:46:15.7817337Z ""Flink-DispatcherRestEndpoint-thread-1"" #38 daemon prio=5 os_prio=0 tid=0x00007fef81ba7800 nid=0x9d30 waiting on condition [0x00007fef7fdfe000]
2020-07-08T21:46:15.7817803Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7818256Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7818948Z 	- parking to wait for  <0x0000000087180cc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7819424Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7819954Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7820563Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-07-08T21:46:15.7821169Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7821709Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7822185Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7822675Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7823066Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7823268Z 
2020-07-08T21:46:15.7823894Z ""mini-cluster-io-thread-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fef81ba6800 nid=0x9d2f waiting on condition [0x00007fef881f5000]
2020-07-08T21:46:15.7824346Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7824625Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7825558Z 	- parking to wait for  <0x00000000871826a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7826069Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7826571Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7828100Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7830710Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7833058Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7833602Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7833991Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7834183Z 
2020-07-08T21:46:15.7834974Z ""flink-rest-server-netty-boss-thread-1"" #36 daemon prio=5 os_prio=0 tid=0x00007fef81ba2800 nid=0x9d2e runnable [0x00007fef886f6000]
2020-07-08T21:46:15.7837524Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7837854Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-07-08T21:46:15.7838216Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-07-08T21:46:15.7838640Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-07-08T21:46:15.7839059Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-07-08T21:46:15.7839863Z 	- locked <0x0000000087183808> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2020-07-08T21:46:15.7840536Z 	- locked <0x00000000871837f8> (a java.util.Collections$UnmodifiableSet)
2020-07-08T21:46:15.7841094Z 	- locked <0x00000000871837b0> (a sun.nio.ch.EPollSelectorImpl)
2020-07-08T21:46:15.7841467Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-07-08T21:46:15.7841995Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
2020-07-08T21:46:15.7842624Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)
2020-07-08T21:46:15.7843171Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)
2020-07-08T21:46:15.7843758Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-07-08T21:46:15.7844391Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-07-08T21:46:15.7844988Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7845195Z 
2020-07-08T21:46:15.7845547Z ""IOManager reader thread #1"" #30 daemon prio=5 os_prio=0 tid=0x00007fefd976f000 nid=0x9d09 waiting on condition [0x00007fef887f7000]
2020-07-08T21:46:15.7845994Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7846271Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7846965Z 	- parking to wait for  <0x0000000087183a70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7847459Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7847978Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7848536Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7849045Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)
2020-07-08T21:46:15.7849380Z 
2020-07-08T21:46:15.7849731Z ""IOManager writer thread #1"" #29 daemon prio=5 os_prio=0 tid=0x00007fefd976d000 nid=0x9d08 waiting on condition [0x00007fef888f8000]
2020-07-08T21:46:15.7853026Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7853377Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7854189Z 	- parking to wait for  <0x0000000087183c78> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7854691Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-07-08T21:46:15.7855215Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-07-08T21:46:15.7855767Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-07-08T21:46:15.7856438Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)
2020-07-08T21:46:15.7856783Z 
2020-07-08T21:46:15.7857397Z ""Timer-2"" #27 daemon prio=5 os_prio=0 tid=0x00007fefd9736800 nid=0x9d07 in Object.wait() [0x00007fef889f9000]
2020-07-08T21:46:15.7857846Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-07-08T21:46:15.7858155Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7858663Z 	- waiting on <0x0000000087183e80> (a java.util.TaskQueue)
2020-07-08T21:46:15.7859019Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-07-08T21:46:15.7859530Z 	- locked <0x0000000087183e80> (a java.util.TaskQueue)
2020-07-08T21:46:15.7859863Z 	at java.util.TimerThread.run(Timer.java:505)
2020-07-08T21:46:15.7860062Z 
2020-07-08T21:46:15.7860636Z ""Timer-1"" #25 daemon prio=5 os_prio=0 tid=0x00007fefd9734000 nid=0x9d06 in Object.wait() [0x00007fef88afa000]
2020-07-08T21:46:15.7861073Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-07-08T21:46:15.7861394Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7861888Z 	- waiting on <0x0000000087184060> (a java.util.TaskQueue)
2020-07-08T21:46:15.7880889Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-07-08T21:46:15.7884642Z 	- locked <0x0000000087184060> (a java.util.TaskQueue)
2020-07-08T21:46:15.7885023Z 	at java.util.TimerThread.run(Timer.java:505)
2020-07-08T21:46:15.7885225Z 
2020-07-08T21:46:15.7885568Z ""BLOB Server listener at 40413"" #21 daemon prio=5 os_prio=0 tid=0x00007fefd9731000 nid=0x9d05 runnable [0x00007fef88bfb000]
2020-07-08T21:46:15.7885962Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7886274Z 	at java.net.PlainSocketImpl.socketAccept(Native Method)
2020-07-08T21:46:15.7886677Z 	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
2020-07-08T21:46:15.7887099Z 	at java.net.ServerSocket.implAccept(ServerSocket.java:560)
2020-07-08T21:46:15.7887488Z 	at java.net.ServerSocket.accept(ServerSocket.java:528)
2020-07-08T21:46:15.7887891Z 	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:262)
2020-07-08T21:46:15.7888333Z 
2020-07-08T21:46:15.7888964Z ""Timer-0"" #22 daemon prio=5 os_prio=0 tid=0x00007fefd9719000 nid=0x9d04 in Object.wait() [0x00007fef88efc000]
2020-07-08T21:46:15.7889410Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-07-08T21:46:15.7889716Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7890222Z 	- waiting on <0x00000000871846a0> (a java.util.TaskQueue)
2020-07-08T21:46:15.7890561Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-07-08T21:46:15.7891090Z 	- locked <0x00000000871846a0> (a java.util.TaskQueue)
2020-07-08T21:46:15.7891423Z 	at java.util.TimerThread.run(Timer.java:505)
2020-07-08T21:46:15.7891621Z 
2020-07-08T21:46:15.7892216Z ""flink-metrics-scheduler-1"" #17 prio=5 os_prio=0 tid=0x00007fefd96de000 nid=0x9d00 waiting on condition [0x00007fefac2f4000]
2020-07-08T21:46:15.7892671Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-07-08T21:46:15.7892986Z 	at java.lang.Thread.sleep(Native Method)
2020-07-08T21:46:15.7893378Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-07-08T21:46:15.7896632Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-07-08T21:46:15.7901619Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-07-08T21:46:15.7902086Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7902278Z 
2020-07-08T21:46:15.7903094Z ""flink-akka.actor.default-dispatcher-3"" #15 prio=5 os_prio=0 tid=0x00007fefd914e800 nid=0x9cfe waiting on condition [0x00007fefac6f6000]
2020-07-08T21:46:15.7904034Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7904361Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7905300Z 	- parking to wait for  <0x0000000087184ce8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-07-08T21:46:15.7905963Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-07-08T21:46:15.7906421Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-07-08T21:46:15.7906846Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-07-08T21:46:15.7907313Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-07-08T21:46:15.7907592Z 
2020-07-08T21:46:15.7908832Z ""flink-akka.actor.default-dispatcher-2"" #14 prio=5 os_prio=0 tid=0x00007fefd9139800 nid=0x9cfb waiting on condition [0x00007fefac7f7000]
2020-07-08T21:46:15.7909581Z    java.lang.Thread.State: WAITING (parking)
2020-07-08T21:46:15.7909902Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7917391Z 	- parking to wait for  <0x0000000087184ce8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-07-08T21:46:15.7917891Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-07-08T21:46:15.7918347Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-07-08T21:46:15.7918803Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-07-08T21:46:15.7919099Z 
2020-07-08T21:46:15.7919706Z ""flink-scheduler-1"" #13 prio=5 os_prio=0 tid=0x00007fefd90a3800 nid=0x9cfa waiting on condition [0x00007fefac8f8000]
2020-07-08T21:46:15.7920155Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-07-08T21:46:15.7920447Z 	at java.lang.Thread.sleep(Native Method)
2020-07-08T21:46:15.7920850Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-07-08T21:46:15.7921378Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-07-08T21:46:15.7921924Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-07-08T21:46:15.7922347Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7922537Z 
2020-07-08T21:46:15.7922887Z ""process reaper"" #11 daemon prio=10 os_prio=0 tid=0x00007fef8c039800 nid=0x9cf5 waiting on condition [0x00007fefad336000]
2020-07-08T21:46:15.7923487Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7923790Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7924433Z 	- parking to wait for  <0x00000000871852d8> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-07-08T21:46:15.7924909Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7925392Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-07-08T21:46:15.7925923Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-07-08T21:46:15.7926411Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-07-08T21:46:15.7926861Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-07-08T21:46:15.7927346Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7927830Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7928232Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7928421Z 
2020-07-08T21:46:15.7929070Z ""surefire-forkedjvm-ping-30s"" #10 daemon prio=5 os_prio=0 tid=0x00007fefd8333800 nid=0x9cf2 waiting on condition [0x00007fefaddf4000]
2020-07-08T21:46:15.7929529Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-07-08T21:46:15.7929832Z 	at sun.misc.Unsafe.park(Native Method)
2020-07-08T21:46:15.7930483Z 	- parking to wait for  <0x000000008003f018> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-07-08T21:46:15.7930969Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-07-08T21:46:15.7931521Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-07-08T21:46:15.7932135Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-07-08T21:46:15.7932839Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-07-08T21:46:15.7933385Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-07-08T21:46:15.7933857Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-07-08T21:46:15.7934347Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-07-08T21:46:15.7934731Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7934935Z 
2020-07-08T21:46:15.7935575Z ""surefire-forkedjvm-command-thread"" #9 daemon prio=5 os_prio=0 tid=0x00007fefd831c000 nid=0x9cf1 runnable [0x00007fefc8110000]
2020-07-08T21:46:15.7936010Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7936301Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-07-08T21:46:15.7936656Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-07-08T21:46:15.7937053Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-07-08T21:46:15.7937482Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-07-08T21:46:15.7938078Z 	- locked <0x000000008003f298> (a java.io.BufferedInputStream)
2020-07-08T21:46:15.7938448Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-07-08T21:46:15.7938917Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-07-08T21:46:15.7939441Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-07-08T21:46:15.7939867Z 	at java.lang.Thread.run(Thread.java:748)
2020-07-08T21:46:15.7940058Z 
2020-07-08T21:46:15.7940383Z ""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007fefd820a800 nid=0x9cef runnable [0x0000000000000000]
2020-07-08T21:46:15.7940761Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7940950Z 
2020-07-08T21:46:15.7942892Z ""C1 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007fefd81ff800 nid=0x9cee waiting on condition [0x0000000000000000]
2020-07-08T21:46:15.7943483Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7943655Z 
2020-07-08T21:46:15.7944001Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007fefd81fd000 nid=0x9ced waiting on condition [0x0000000000000000]
2020-07-08T21:46:15.7945021Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7945236Z 
2020-07-08T21:46:15.7945548Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007fefd81fb000 nid=0x9cec runnable [0x0000000000000000]
2020-07-08T21:46:15.7945946Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7946121Z 
2020-07-08T21:46:15.7946503Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007fefd81f9800 nid=0x9ceb waiting on condition [0x0000000000000000]
2020-07-08T21:46:15.7946936Z    java.lang.Thread.State: RUNNABLE
2020-07-08T21:46:15.7947124Z 
2020-07-08T21:46:15.7947437Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fefd81c9800 nid=0x9cea in Object.wait() [0x00007fefc8dfc000]
2020-07-08T21:46:15.7947882Z    java.lang.Thread.State: WAITING (on object monitor)
2020-07-08T21:46:15.7948179Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7948525Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-07-08T21:46:15.7949696Z 	- locked <0x0000000080043288> (a java.lang.ref.ReferenceQueue$Lock)
2020-07-08T21:46:15.7950111Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-07-08T21:46:15.7950533Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-07-08T21:46:15.7950778Z 
2020-07-08T21:46:15.7951106Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fefd81c5000 nid=0x9ce9 in Object.wait() [0x00007fefc8efd000]
2020-07-08T21:46:15.7951551Z    java.lang.Thread.State: WAITING (on object monitor)
2020-07-08T21:46:15.7951862Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7952575Z 	at java.lang.Object.wait(Object.java:502)
2020-07-08T21:46:15.7953820Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-07-08T21:46:15.7954525Z 	- locked <0x0000000080043278> (a java.lang.ref.Reference$Lock)
2020-07-08T21:46:15.7954935Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-07-08T21:46:15.7955185Z 
2020-07-08T21:46:15.7955492Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fefd800b800 nid=0x9ce0 in Object.wait() [0x00007fefe2138000]
2020-07-08T21:46:15.7956693Z    java.lang.Thread.State: WAITING (on object monitor)
2020-07-08T21:46:15.7957487Z 	at java.lang.Object.wait(Native Method)
2020-07-08T21:46:15.7958906Z 	- waiting on <0x0000000087181118> (a org.apache.flink.test.accumulators.AccumulatorLiveITCase$1)
2020-07-08T21:46:15.7959330Z 	at java.lang.Thread.join(Thread.java:1252)
2020-07-08T21:46:15.7959948Z 	- locked <0x0000000087181118> (a org.apache.flink.test.accumulators.AccumulatorLiveITCase$1)
2020-07-08T21:46:15.7960408Z 	at org.apache.flink.core.testutils.CheckedThread.trySync(CheckedThread.java:112)
2020-07-08T21:46:15.7960896Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:100)
2020-07-08T21:46:15.7961352Z 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:89)
2020-07-08T21:46:15.7961901Z 	at org.apache.flink.test.accumulators.AccumulatorLiveITCase.submitJobAndVerifyResults(AccumulatorLiveITCase.java:178)
2020-07-08T21:46:15.7962593Z 	at org.apache.flink.test.accumulators.AccumulatorLiveITCase.testStreaming(AccumulatorLiveITCase.java:137)
2020-07-08T21:46:15.7963051Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-07-08T21:46:15.7963469Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-07-08T21:46:15.7963954Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-07-08T21:46:15.7964396Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-07-08T21:46:15.7964820Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-07-08T21:46:15.7965332Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-07-08T21:46:15.7965986Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-07-08T21:46:15.7966471Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-07-08T21:46:15.7966960Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-07-08T21:46:15.7967388Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-07-08T21:46:15.7976578Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-07-08T21:46:15.7977052Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-07-08T21:46:15.7996016Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-07-08T21:46:15.7996567Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-07-08T21:46:15.7998186Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-07-08T21:46:15.8000214Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-07-08T21:46:15.8001811Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-07-08T21:46:15.8002898Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-07-08T21:46:15.8003494Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-07-08T21:46:15.8004932Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-07-08T21:46:15.8005359Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-07-08T21:46:15.8006127Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-07-08T21:46:15.8006585Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-07-08T21:46:15.8007100Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-07-08T21:46:15.8007613Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-07-08T21:46:15.8008314Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-07-08T21:46:15.8008844Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-07-08T21:46:15.8009398Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-07-08T21:46:15.8009884Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-07-08T21:46:15.8010358Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-07-08T21:46:15.8010622Z 
2020-07-08T21:46:15.8010875Z ""VM Thread"" os_prio=0 tid=0x00007fefd81bb800 nid=0x9ce8 runnable 
2020-07-08T21:46:15.8011089Z 
2020-07-08T21:46:15.8011384Z ""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007fefd8020000 nid=0x9ce1 runnable 
2020-07-08T21:46:15.8011637Z 
2020-07-08T21:46:15.8011928Z ""Gang worker#1 (Parallel GC Threads)"" os_prio=0 tid=0x00007fefd8022000 nid=0x9ce2 runnable 
2020-07-08T21:46:15.8012181Z 
2020-07-08T21:46:15.8012467Z ""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007fefd8046000 nid=0x9ce6 runnable 
2020-07-08T21:46:15.8012717Z 
2020-07-08T21:46:15.8013007Z ""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007fefd8047800 nid=0x9ce7 runnable 
2020-07-08T21:46:15.8013287Z 
2020-07-08T21:46:15.8013565Z ""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007fefd8028000 nid=0x9ce5 runnable 
2020-07-08T21:46:15.8013829Z 
2020-07-08T21:46:15.8014102Z ""G1 Concurrent Refinement Thread#1"" os_prio=0 tid=0x00007fefd8026000 nid=0x9ce4 runnable 
2020-07-08T21:46:15.8034995Z 
2020-07-08T21:46:15.8035322Z ""G1 Concurrent Refinement Thread#2"" os_prio=0 tid=0x00007fefd8024800 nid=0x9ce3 runnable 
2020-07-08T21:46:15.8035581Z 
2020-07-08T21:46:15.8035879Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007fefd820d800 nid=0x9cf0 waiting on condition 
2020-07-08T21:46:15.8036137Z 
2020-07-08T21:46:15.8036337Z JNI global references: 1359
{code}"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13315282	The generated flink-docker file misses flink version in Tags	"Changes made in FLINK-17367 removed {{FLINK_VERSION}} from Dockerfile template, but the current [generate-stackbrew-library.sh|https://github.com/apache/flink-docker/blob/31794825ad02db8b0eb961372c74a309a4504bcd/generate-stackbrew-library.sh#L97] is still trying to parse `flink_version` from it, which will cause the generated `library/flink` file missing flink version in `Tags`.

While we could manually work-around the problem, a fix in script is needed."	FLINK	Closed	2	1	11245	pull-request-available
13499637	AkkaUtils#getAddress may cause memory leak	"We found a slow memory leak in JM. When MetricFetcherImpl tries to retrieve metrics, it always call MetricQueryServiceRetriever#retrieveService first. And the method will acquire the address of a task manager, which will use AkkaUtil#getAddress internally. While the getAddress method is implemented like this:

{code:java}
    public static Address getAddress(ActorSystem system) {
        return new RemoteAddressExtension().apply(system).getAddress();
    }
{code}

and the RemoteAddressExtension#apply is like this:

{code:scala}
  def apply(system: ActorSystem): T = {
    java.util.Objects.requireNonNull(system, ""system must not be null!"").registerExtension(this)
  }
{code}

This means every call of AkkaUtils#getAddress will register a new extension to the ActorSystem, and can never be released until the ActorSystem exits.

Most of the usage of the method are called only once while initializing, but as described above, MetricFetcherImpl will also use the method. It can happens periodically while users open the WebUI, or happens when the users call the RESTful API directly to get metrics. This means the memory may keep leaking. 

The leak may be introduced in FLINK-23662 when porting the scala version of AkkaUtils to the java one, while I'm not sure if the scala version has the same issue.

The leak seems very slow. We observed it on a job running for more than one month with only 1G memory for job manager. So I suppose it's not an emergency one but still needs to fix.
"	FLINK	Closed	3	1	11245	pull-request-available
13480875	Remove RpcService#getTerminationFuture	Unused method; unnecessary because stopService returns a termination future anyway.	FLINK	Closed	3	7	11245	pull-request-available
13413123	Fix JDK12+ profile activations	"All Java11 profiles should be active for Java 11 and all subsequent Java versions, but this currently only applies to the ""main"" profile in the root pom."	FLINK	Closed	3	7	11245	pull-request-available
13278191	TestUtils#copyDirectory should follow symbolic links	{{TestUtils#copyDirectory}} is used by the {{FlinkDistribution}} to create a copy of the distribution. Flink developers on unix are accustomed to pointing applications to the `build-target` symbolic, however this fails since FileVisitOption.FOLLOW_LINKS is not set.	FLINK	Closed	4	4	11245	pull-request-available
13162233	Compatibility table not up-to-date	The compatibility table https://ci.apache.org/projects/flink/flink-docs-master/ops/upgrading.html has not been updated since 1.3.x.	FLINK	Closed	3	4	11245	pull-request-available
13200231	Avro-confluent-registry does not bundle any dependency	"The {{flink-avro-confluent-registry}} is not bundling any dependencies, yet defines a relocation for the transitive jackson dependency pulled in by {{kafka-schema-registry-client}}.

It is like that the registry-client should be included in the jar."	FLINK	Closed	3	1	11245	pull-request-available
13541779	Replace Akka by Pekko	"Akka 2.6.x will not receive security fixes from September 2023 onwards (see https://discuss.lightbend.com/t/2-6-x-maintenance-proposal/9949). 

A mid-term plan to replace Akka is described in FLINK-29281. In the meantime, we suggest to replace Akka by Apache Pekko (incubating), which is a fork of Akka 2.6.x under the Apache 2.0 license. This way - if needed - we at least have the ability to release security fixes ourselves in collaboration with the Pekko community. "	FLINK	Closed	2	4	11245	pull-request-available
13480874	Remove RpcService#execute/scheduleRunnable	Subsumed be {{getScheduledExecutor()}} .	FLINK	Closed	3	7	11245	pull-request-available
13139899	Migrate JSONGenerator from Sling to Jackson	"The {{org.apache.flink.streaming.api.graph.JSONGenerator}} uses Slink for JSON encoding, adding an extra dependency. All other Flink parts use a specially shaded Jackson dependency.

Migrating the JSONGenerator would allow us to drop a dependency and make the code more homogeneous."	FLINK	Closed	4	1	11245	beginner, easy-fix, starter
13397314	Add additional availability timing metrics to Job lifecycle events	"Flink currently contains a number of availability lifecycle metrics[1] showing how long it takes to move through different job status'. We propose adding two additional metrics; startingTime, and cancellingTime (open to bikeshedding on the metric names). 

 
 * startingTime is the time it takes a job to get to running. 
 * cancellingTime is the time spent in status CANCELLING 

 

 [1]https://ci.apache.org/projects/flink/flink-docs-master/docs/ops/metrics/#availability"	FLINK	Closed	4	4	11245	pull-request-available
13071220	RocksDBKeyedStateBackend broken on Windows	"The {{RocksDBKeyedStateBackend}} cannot be used on Windows. We pass the result of {{org.apache.flink.core.fs.Path#getPath()}} to RocksDB which will fail when trying to create the checkpoint file as the path begins with a slash which isn't a valid Windows path:

{code}
/C:/Users/Zento/AppData/Local/Temp/junit572330160893758355/junit5754599533651878867/job-ecbdb9df76fd3a39108dac7c515e3214_op-Test_uuid-6a43f1f6-1f35-443e-945c-aab3643e62fc/chk-0.tmp
{code}
"	FLINK	Resolved	3	4	11245	pull-request-available
13374328	Unstable test ReactiveModeITCase.testScaleDownOnTaskManagerLoss()	"The test is stalling on Azure CI.

https://dev.azure.com/sewen0794/Flink/_build/results?buildId=292&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4865"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13171113	Watermark metrics for an operator&task shadow each other	"In FLINK-4812 we reworked the watermark metrics to be exposed for each operator.
In FLINK-9467 we made further modifications to also expose these metrics again for tasks.

In both JIRAs we register a single metric multiple times, for example the input watermark metric is registered for both the first operator in the task, and the task itself.

Unfortunately, the metric system assumes metric objects to be unique, as can be seen in virtually all reporter implementations as well as the MetricQueryService.

As a result the watermark metrics override each other in the reporter, causing only one to be reported, whichever was registered last.


"	FLINK	Closed	1	1	11245	pull-request-available
12719849	Python interface for new API (Map/Reduce)	"([#615|https://github.com/stratosphere/stratosphere/issues/615] | [FLINK-615|https://issues.apache.org/jira/browse/FLINK-615])

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/671
Created by: [zentol|https://github.com/zentol]
Labels: enhancement, java api, 
Milestone: Release 0.6 (unplanned)
Created at: Wed Apr 09 20:52:06 CEST 2014
State: open
"	FLINK	Resolved	3	2	11245	github-import
13246453	Scala compiler causes StackOverflowError	"Here is a instance:

[https://api.travis-ci.org/v3/job/562043336/log.txt]"	FLINK	Closed	1	1	11245	pull-request-available
13216936	Use configured RPC timeout in MiniCluster	The {{MiniCluster}} uses a fixed timeout of 10 seconds instead of the timeout defined in the given {{MiniClusterConfiguration}}.	FLINK	Closed	3	1	11245	pull-request-available
13533807	Skip ClosureCleaner if object can be serialized	"Given an object the ClosureCleaner currently recursively digs into every non-static/transient field of the given object. This causes a problem on Java 17 because these reflective accesses all need to be explicitly allowed beforehand.

Instead, we could limit the CC to objects that fail serialization, because if something can be serialized there isn't anything for the CC to do.
This should allow us to avoid a lot of unnecessary reflection accesses to immutable JDK classes, like Strings/BigDecimals etc etc."	FLINK	Closed	3	7	11245	pull-request-available
13267096	Add total number of partitions to ResultPartitionDeploymentDescriptor	"The ResourceManager requires some way to tell whether it has received all partitions for a given dataset; to ensure that a) no incomplete datasets are shows to the user [as completed] and b) to initiate a timeout-based cleanup of other partitions.

Conversely, since the RM only has access to what the TaskExecutor sends, this information must also be accessible to the TE.

A good candidate for placing this seems to be the {{ResultPartitionDeploymentDescriptor}} or {{PartitionDescriptor}}, as these already contain the DataSetID."	FLINK	Closed	3	7	11245	pull-request-available
12987502	Replace maven-assembly-plugin by maven-shade-plugin in flink-metrics	"The modules {{flink-metrics-dropwizard}}, {{flink-metrics-ganglia}} and {{flink-metrics-graphite}} use the {{maven-assembly-plugin}} to build a fat jar. The resulting fat jar has the suffix {{jar-with-dependencies}}. In order to make the naming consistent with the rest of the system we should create a fat-jar without this suffix.

Additionally we could replace the {{maven-assembly-plugin}} with the {{maven-shade-plugin}} to make it consistent with the rest of the system."	FLINK	Closed	4	4	11245	pull-request-available
13566681	Attach cluster config map labels at creation time	"We attach a set of labels to config maps that we create to ease the manual cleanup by users in case Flink fails unrecoverably.

For cluster config maps (that are used for leader election), these labels are not set at creation time, but when leadership is acquired, in contrast to job config maps.

This means there's a gap where we create a CM without any labels being attached, and should Flink fail before leadership can be acquired it will continue to lack labels indefinitely.

AFAICT it should be straight-forward, at least API-wise, to set these labels at creation time. "	FLINK	Closed	3	4	11245	pull-request-available
13457887	Remove workaround around avro sql jar	"Because of FLINK-17417 flink-python contains a workaround that manually assembles a sort-of avro sql jar.
Rely on the sql-avro jar instead and remove the workaround."	FLINK	Closed	3	11500	11245	pull-request-available
13261721	Annotate MiniCluster tests in flink-tests with AlsoRunWithSchedulerNG	"This task is to annotate all MiniCluster tests with AlsoRunWithSchedulerNG in flink-tests, so that we can know breaking changes in time when further improving the new generation scheduler.

We should also guarantee the annotated tests to pass, either by fixing failed tests, or not annotating a failed test and opening a ticket to track it.
The tickets for failed tests should be linked in this task."	FLINK	Resolved	3	7	11355	pull-request-available
13296746	Implements bulk allocation for physical slots	"SlotProvider should support bulk slot allocation so that we can check to see if the resource requirements of a slot request bulk can be fulfilled at the same time.

The SlotProvider interface should be extended with an bulk slot allocation method which accepts a bulk of slot requests as one of the parameters.

{code:java}
CompletableFuture<Collection<PhysicalSlotRequest.Result>> allocatePhysicalSlots(
  Collection<PhysicalSlotRequest> slotRequests,
  Time timeout);
 
class PhysicalSlotRequest {
  SlotRequestId slotRequestId;
  SlotProfile slotProfile;
  boolean slotWillBeOccupiedIndefinitely;

  class Result {
    SlotRequestId slotRequestId;
    PhysicalSlot physicalSlot;
  }
}
{code}

More details see [FLIP-119#Bulk Slot Allocation|https://cwiki.apache.org/confluence/display/FLINK/FLIP-119+Pipelined+Region+Scheduling#FLIP-119PipelinedRegionScheduling-BulkSlotAllocation]
"	FLINK	Closed	3	7	11355	pull-request-available
13323016	Unfulfillable slot requests of Blink planner batch jobs never timeout	"The unfulfillability check of batch slot requests are unconditionally disabled in {{SchedulerImpl#start() -> BulkSlotProviderImpl#start()}}.
This means slot allocation timeout will not be triggered if a Blink planner batch job cannot obtain any slot."	FLINK	Closed	1	1	11355	pull-request-available
13303105	Unify slot request timeout handling for streaming and batch tasks	"There are 2 different slot request timeout handling mechanism for batch and streaming tasks.
For streaming tasks, the slot request will fail if it is not fulfilled within slotRequestTimeout.
For batch tasks, the slot request will be checked periodically to see whether it is fulfillable, and only fails if it has been unfulfillable for a certain period(slotRequestTimeout).

With slot marked with whether they will be occupied indefinitely, we can unify these handling. See [FLIP-119|https://cwiki.apache.org/confluence/display/FLINK/FLIP-119+Pipelined+Region+Scheduling#FLIP-119PipelinedRegionScheduling-ExtendedSlotProviderInterface] for more details."	FLINK	Closed	3	7	11355	pull-request-available
13334948	Execution graph related tests are possibly broken due to registering duplicated ExecutionAttemptID	"Since FLINK-17295, many tests encounters unexpected global failure due to registering duplicated ExecutionAttemptID. Although these tests do not appear to be broken yet, they are potentially broken/unstable. And it further blocks to rework these tests to be based on the new scheduer (FLINK-17760). 

Below is a sample error which happens in ExecutionTest#testAllPreferredLocationCalculation():

{code:java}
2194 [main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job (unnamed job) (a22afb832b5f94b075d7ffb32fbc9023) switched from state CREATED to FAILING.
java.lang.Exception: Trying to register execution Attempt #0 (TestVertex (1/1)) @ (unassigned) - [CREATED] for already used ID a22afb832b5f94b075d7ffb32fbc9023_146968a4de2df0b2fef1e4b2e8297993_0_0
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.registerExecution(ExecutionGraph.java:1621) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.<init>(ExecutionVertex.java:181) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:211) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:139) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:448) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:419) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:411) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:452) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecution(ExecutionGraphTestUtils.java:477) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionTest.testAllPreferredLocationCalculation(ExecutionTest.java:298) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_261]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_261]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_261]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_261]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12]
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) [junit-4.12.jar:4.12]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) [junit-4.12.jar:4.12]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) [junit-4.12.jar:4.12]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) [junit-4.12.jar:4.12]
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) [junit-4.12.jar:4.12]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363) [junit-4.12.jar:4.12]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) [junit-4.12.jar:4.12]
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) [junit-rt.jar:?]
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) [junit-rt.jar:?]
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220) [junit-rt.jar:?]
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53) [junit-rt.jar:?]

{code}

This is because these tests improperly creates Execution/ExecutionVertex/ExecutionJobVertex from an existing ExecutionGraph which already contains the Execution/ExecutionVertex/ExecutionJobVertex. FLINK-17295 reveals this problem because it makes the attemptID no longer random. 

Below is an example of improper ExecutionJobVertex creation in ExecutionGraphTestUtils#getExecutionJobVertex():

{code:java}

		JobGraph jobGraph = new JobGraph(ajv);
		jobGraph.setScheduleMode(scheduleMode);

		ExecutionGraph graph = TestingExecutionGraphBuilder
			.newBuilder()
			.setJobGraph(jobGraph)
			.setIoExecutor(executor)
			.setFutureExecutor(executor)
			.build();

		graph.start(ComponentMainThreadExecutorServiceAdapter.forMainThread());

		return new ExecutionJobVertex(graph, ajv, 1, AkkaUtils.getDefaultTimeout());
{code}

We should get rid of such improper usages. 
Therefore, I would like to change these tests to get existing Execution/ExecutionVertex/ExecutionJobVertex from the generated ExecutionGraph, instead of invoking their constructors to create new ones."	FLINK	Closed	2	1	11355	pull-request-available
13263084	Keep only one execution topology in scheduler	"Currently there are 3 failover topology instances created, 2 permanently kept in JM. 2 scheduling topology instances created permanently kept in JM. It a waste of computation to build the topologies and memory to keep these topologies. Which may be a significant issue when the job scale is large.

With FLINK-14450 and FLINK-14451, the SchedulingTopology and FailoverTopology are able to share one default implementation. 
We can change the scheduler to create and keep only one such an execution topology instance to reduce the cost to build and host execution topologies.

More details see FLINK-14330 and the [design doc|https://docs.google.com/document/d/1f88luAOfUQ6Pm4JkxYexLXpfH-crcXJdbubi1pS2Y5A/edit#]."	FLINK	Resolved	3	7	11355	pull-request-available
13285942	Co-location constraints are not reset on task recovery in DefaultScheduler	"The colocation constraints are not reset on task recovery, which may lead to task recovery failures when allocating slots.
We should reset the colocation constraints before resetting vertices, just like what we do in the legacy scheduler."	FLINK	Resolved	2	1	11355	pull-request-available
13256246	Set slot sharing groups according to pipelined regions	"{{StreamingJobGraphGenerator}} set slot sharing group for operators at compiling time.
-Identify pipelined regions, with respect to {{allSourcesInSamePipelinedRegion}}-
-Set slot sharing groups according to pipelined regions -
-By default, each pipelined region should go into a separate slot sharing group-
-If the user sets operators in multiple pipelined regions into same slot sharing group, it should be respected-

The slot sharing group of a vertex is determined as:
 * if a vertex slot sharing group is specified to be {{null}}, the slot sharing group is null
 * if a vertex slot sharing group is specified to be a non-default value(the group name not ""default""), it will be respected
 * if a vertex slot sharing group is ""default"", it will be the slot sharing group of the vertex's logical pipelined region

The slot sharing group of a region is determined as:
 * if {{allVerticesInSameSlotSharingGroupByDefault}} is true, all regions are in the same ""default"" slot sharing group
 * if {{allVerticesInSameSlotSharingGroupByDefault}} is false, all regions will be in distinct slot sharing groups

This step should not introduce any behavior changes, given that later scheduled pipelined regions can reuse slots from previous scheduled pipelined regions. 

"	FLINK	Resolved	3	7	11355	pull-request-available
13204426	Log entire TaskManagerLocation instead of just the hostname	"Currently there is not a straight forward way to find in which TM a task locates in, especially when the task has failed.

We can find on which machine the task locates in by checking the JM log, sample as below:

{color:#707070}_""Deploying Flat Map (31/40) (attempt #0) to z05c19399""_{color}

But there can be multiple TMs on the machine and we need to check them one by one. So I'd suggest we *add the full task manager location representation in the deploying log* to support quick locating tasks. The task manager location contains resourceId. The resourceId is the containerId when job runs in yarn/mesos, or a unique AbstractID in standalone mode which can be easily identified at Flink web UI. 

 "	FLINK	Closed	4	4	11355	pull-request-available
13258474	Remove unnecessary scala Duration usages in flink-runtime	This ticket is to remove all usages of scala {{Duration/FiniteDuration}} in {{flink-runtime}}, except for those usages for {{Akka}} components (in AkkaUtils, AkkaRpcActor and ActorSystemScheduledExecutorAdapter).	FLINK	Resolved	3	7	11355	pull-request-available
13259228	LazyFromSourcesSchedulingStrategy should be able to restart terminated tasks	"{{LazyFromSourcesSchedulingStrategy}} now will only schedule tasks in CREATED state. 
However, the DefaultScheduler will reset failed tasks when actually restarting them.
This makes failed tasks not able to be restarted with {{LazyFromSourcesSchedulingStrategy}}.

We can keep {{LazyFromSourcesSchedulingStrategy}} to schedule tasks in CREATED state in normal scheduling. And make it schedule tasks in terminal state when trying to restart tasks.

This issue also fails some IT cases on DefaultScheduler, e.g. JobRecoveryITCase."	FLINK	Closed	3	7	11355	pull-request-available
13338376	Remove legacy FailoverStrategy	The legacy {{org.apache.flink.runtime.executiongraph.failover.FailoverStrategy}} is no longer in use since FLINK-19919 and thus can be removed.	FLINK	Closed	3	7	11355	pull-request-available
13242696	Optimize region failover performance on calculating vertices to restart	"Currently some region boundary structures are calculated each time of a region failover. This calculation can be heavy as its complexity goes up with execution edge count.

We tested it in a sample case with 8000 vertices and 16,000,000 edges. It takes ~2.0s to calculate vertices to restart.

(more details in [https://docs.google.com/document/d/197Ou-01h2obvxq8viKqg4FnOnsykOEKxk3r5WrVBPuA/edit?usp=sharing)]

That's why we'd propose to cache the region boundary structures to improve the region failover performance."	FLINK	Closed	3	4	11355	pull-request-available
13260067	Support building logical pipelined regions from JobGraph	"Logical pipelined region partitioning is needed by FLINK-14060 to determine JobVertex slot sharing group.
We can leverage PipelinedRegionComputeUtil#computePipelinedRegions to do this by adapting JobGraph to a base Topology.

With changes from FLINK-14453, we can build {{LogicalPipelinedRegions}} from {{JobGraph}} by:
1. Introduce LogicalTopology which extends Topology
2. Implement DefaultLogicalTopology as an adapter of JobGraph to LogicalTopology
3. Add DefaultLogicalTopology#getLogicalPipelinedRegions to return the logical pipelined regions of a JobGraph"	FLINK	Resolved	3	7	11355	pull-request-available
13260757	Introduce a unified topology interface	"When working on FLINK-14312 to partition {{JobGraph}} into logical pipelined regions, I found that we can hardly reuse the existing util {{PipelinedRegionComputeUtil#computePipelinedRegions(..)}}  to do it since it's based on the {{FailoverTopology}}.
To avoid code duplication, we need a unified topology base for {{FailoverTopology}} and {{JobGraph/LogicalTopology}}.

Besides that, the inconsistency of {{FailoverTopology}} and {{SchedulingTopology}} is also causing troubles for development and performance.

That's why I'd propose to unify the interfaces all these topologies. More details can be found in this [design doc|https://docs.google.com/document/d/1f88luAOfUQ6Pm4JkxYexLXpfH-crcXJdbubi1pS2Y5A/edit?usp=sharing]."	FLINK	Resolved	3	7	11355	pull-request-available
13451131	Rework DefaultScheduler to directly deploy executions	"Currently, the DefaultScheduler(base of AdaptiveBatchScheduler) can only perform ExecutionVertex level deployment. However, in this case, the scheduler is actually deploying the current execution attempt of the ExecutionVertex.

Therefore, we need to rework the DefaultScheduler to directly deploy executions."	FLINK	Closed	3	7	11355	pull-request-available
13363447	Job is possible to hang when restarting a FINISHED task with POINTWISE BLOCKING consumers	"Job is possible to hang when restarting a FINISHED task with POINTWISE BLOCKING consumers. This is because {{PipelinedRegionSchedulingStrategy#onExecutionStateChange()}} will try to schedule all the consumer tasks/regions of the finished *ExecutionJobVertex*, even though the regions are not the exact consumers of the finished *ExecutionVertex*. In this case, some of the regions can be in non-CREATED state because they are not connected to nor affected by the restarted tasks. However, {{PipelinedRegionSchedulingStrategy#maybeScheduleRegion()}} does not allow to schedule a non-CREATED region and will throw an Exception and breaks the scheduling of all the other regions. One example to show this problem case can be found at [PipelinedRegionSchedulingITCase#testRecoverFromPartitionException |https://github.com/zhuzhurk/flink/commit/1eb036b6566c5cb4958d9957ba84dc78ce62a08c].

To fix the problem, we can add a filter in {{PipelinedRegionSchedulingStrategy#onExecutionStateChange()}} to only trigger the scheduling of regions in CREATED state."	FLINK	Closed	1	1	11355	pull-request-available
13296750	Implement FIFO Physical Slot Assignment in SlotPoolImpl	"The SlotPool should try to fulfill the oldest pending slot request once it receives an available slot, no matter if the slot is returned by another terminated task or is just offered from a task manager. This naturally ensures that slot requests of an earlier scheduled region will be fulfilled earlier than requests of a later scheduled region.

We only need to change the slot assignment logic on slot offers. This is because the fields {{pendingRequests}} and {{waitingForResourceManager}} store the pending requests in LinkedHashMaps . Therefore, {{tryFulfillSlotRequestOrMakeAvailable(...)}} will naturally fulfill the pending requests in inserted order.

When a new slot is offered via {{SlotPoolImpl#offerSlot(...)}} , we should use it to fulfill the oldest fulfillable slot request directly by invoking {{tryFulfillSlotRequestOrMakeAvailable(...)}}. 

If a pending request (say R1) exists with the allocationId of the offered slot, and it is different from the request to fulfill (say R2), we should update the pendingRequest to replace AllocationID of R1 to be the AllocationID of R2. This ensures failAllocation(...) can fail slot allocation requests to trigger restarting tasks and re-allocating slots. "	FLINK	Closed	3	7	11355	pull-request-available
13256333	Use TimeUtils to parse duration configs	"FLINK-14069 makes TimeUtils able to parse all time unit labels supported by scala Duration.

We can now use TimeUtils to parse duration configs instead of using scala Duration.
Some config descriptors referring scala FiniteDuration should be updated as well.

This is one step for Flink core to get rid of scala dependencies."	FLINK	Resolved	3	7	11355	pull-request-available
13258751	Let fullRestart metric count fine grained restarts as well	"With fine grained recovery introduced in 1.9.0, the {{fullRestart}} metric only counts how many times the entire graph has been restarted, not including the number of fine grained failure restarts.

As many users leverage this metric for failure detecting monitoring and alerting, I'd propose to make it also count fine grained restarts.

The concrete proposal is:
- Add a counter {{numberOfRestartsCounter}} in {{ExecutionGraph}} to count all restarts. The counter is not to be registered to metric groups.
- Let fullRestart query the value of the counter, instead of {{ExecutionGraph#globalModVersion}}
- increment {{numberOfRestartsCounter}} in {{ExecutionGraph#incrementGlobalModVersion()}}
- increment {{numberOfRestartsCounter}} in {{AdaptedRestartPipelinedRegionStrategyNG#restartTasks(...)}}, to ensure that the fine grained recovery really happens
"	FLINK	Closed	3	4	11355	pull-request-available
13237217	Implement RestartBackoffTimeStrategyFactoryLoader	"We need to implement a RestartBackoffTimeStrategyFactoryLoader to instantiate RestartBackoffTimeStrategyFactory.

In order to be backwards compatible, the loader is responsible for converting *RestartStrategy* configurations([https://ci.apache.org/projects/flink/flink-docs-stable/dev/restart_strategies.html]）and *RestartStrategyConfiguration* to latest *RestartBackoffTimeStrategy* configurations.

The converted configurations will be used to create *RestartBackoffTimeStrategy.Factory* via *RestartBackoffTimeStrategy#createFactory(Configuration)*.

 "	FLINK	Closed	3	7	11355	pull-request-available
13256244	FLIP-53 Fine-grained Operator Resource Management	This is the umbrella issue of 'FLIP-53: Fine Grained Operator Resource Management'.	FLINK	Closed	3	2	11355	Umbrella
13262882	Enable BatchFineGrainedRecoveryITCase to pass with scheduler NG	"BatchFineGrainedRecoveryITCase currently fails with scheduler NG.
The failure cause is more failover counts than expected due to lacking of an optimization of region failover, see FLINK-14439.

We need to solve FLINK-14439 and annotate this test with AlsoRunWithSchedulerNG."	FLINK	Closed	3	7	11355	pull-request-available
13321665	"ExecutionGraphNotEnoughResourceTest.testRestartWithSlotSharingAndNotEnoughResources failed with ""Condition was not met in given timeout."""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=5300&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=66b5c59a-0094-561d-0e44-b149dfdd586d]

{code}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.673 s <<< FAILURE! - in org.apache.flink.runtime.executiongraph.ExecutionGraphNotEnoughResourceTest
[ERROR] testRestartWithSlotSharingAndNotEnoughResources(org.apache.flink.runtime.executiongraph.ExecutionGraphNotEnoughResourceTest)  Time elapsed: 3.158 s  <<< ERROR!
java.util.concurrent.TimeoutException: Condition was not met in given timeout.
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:129)
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:119)
	at org.apache.flink.runtime.executiongraph.ExecutionGraphNotEnoughResourceTest.testRestartWithSlotSharingAndNotEnoughResources(ExecutionGraphNotEnoughResourceTest.java:130)
{code}"	FLINK	Closed	2	1	11355	pull-request-available, test-stability
13326818	Enable pipelined scheduling by default	"This task is to enable pipelined region scheduling by default, via setting the default value of  config option ""jobmanager.scheduler.scheduling-strategy"" to ""region"".

Here are the required verifications before we can make this change:
1. CI, including all UT/IT cases and E2E tests
2. stability tests
3. TPC-DS benchmark in 1T/10T data scale"	FLINK	Closed	3	7	11355	pull-request-available
13261747	Enable ZooKeeperHighAvailabilityITCase to pass with scheduler NG	"ZooKeeperHighAvailabilityITCase currently fails with scheduler NG.
There are 3 reasons for the failure:
1. it will invoke ExecutionGraph#failGlobal for global recovery but it is not ready for use in scheduler NG
2. the test relies on NumberOfFullRestartsGauge for restart count, which is not correct with fine grained recovery. This issue can be fixed by changing the failover metric to numberOfRestarts introduced in FLINK-14164.

We need to support failGlobal in scheduler NG to make this case pass with scheduler NG. And then annotate it with AlsoRunWithSchedulerNG."	FLINK	Closed	3	7	11355	pull-request-available
13287480	Introduce factory methods to create DefaultScheduler for testing	"Currently tests create DefaultScheduler via its constructor. Having a builder and a set of factory methods can significantly reduce the complexity to instantiate DefaultScheduler.
It can be very helpful especially when we are to rework tests to base on the new scheduler."	FLINK	Closed	3	7	11355	pull-request-available
13260049	Get ExecutionVertexID from ExecutionVertex rather than creating new instances	"{{ExecutionVertexID}} is now added as a field to {{ExecutionVertex}}.

Many components, however, are still creating {{ExecutionVertexID}} from {{ExecutionVertex}} by themselves. This may lead to more memory consumption of JM. It also slows down the {{ExecutionVertexID}} equality check.

We should change them to use the field {{ExecutionVertex#executionVertexID}} directly."	FLINK	Closed	3	7	11355	pull-request-available
13263082	Refactor FailoverTopology to extend base topology	"This task is to change FailoverTopology to extend the base topology 
introduced in FLINK-14330. ExecutionGraphToSchedulingTopologyAdapter(default implementation of SchedulingTopology) should also implements and replace DefaultFailoverTopology.

More details see FLINK-14330 and the [design doc|https://docs.google.com/document/d/1f88luAOfUQ6Pm4JkxYexLXpfH-crcXJdbubi1pS2Y5A/edit#]."	FLINK	Resolved	3	7	11355	pull-request-available
13312357	NullPointerException can happen in SlotPoolImpl#maybeRemapOrphanedAllocation	"NullPointerException can happen in SlotPoolImpl#maybeRemapOrphanedAllocation, which indicates a bug.

https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/8189/logs/115

6:07:07,950 [flink-akka.actor.default-dispatcher-7] WARN  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Slot offering to JobManager failed. Freeing the slots and returning them to the ResourceManager.
java.lang.NullPointerException: null
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.maybeRemapOrphanedAllocation(SlotPoolImpl.java:599) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFulfillSlotRequestOrMakeAvailable(SlotPoolImpl.java:564) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.offerSlot(SlotPoolImpl.java:701) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.offerSlots(SlotPoolImpl.java:625) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.offerSlots(JobMaster.java:541) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-runtime_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
16:07:07,977 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=0.5000000000000000, taskHeapMemory=64.000mb (67108864 bytes), taskOffHeapMemory=0 bytes, managedMemory=2.000mb (2097152 bytes), networkMemory=1.563mb (1638400 bytes)}, allocationId: 4dcfe78bb09fcf1117bd0be11c039df9, jobId: 00a771c28c805577994e752b25bef01c)."	FLINK	Closed	2	1	11355	pull-request-available
13258470	Make TimeUtils able to parse duration string with plural form labels	"Scala Duration supports parsing plural form time unit label except for the shortest label of a TimeUnit. Namely:
{
""d day days"",
""h hour s"",
""min minute minutes"",
""s sec secs second seconds"",
""ms milli millis millisecond milliseconds"",
""µs micro micros microsecond microseconds"",
""ns nano nanos nanosecond nanoseconds""
}

TimeUtils should support them as well."	FLINK	Resolved	3	7	11355	pull-request-available
13472018	Speculative execution for new sources	This task enables new sources(FLIP-27) for speculative execution.	FLINK	Closed	3	7	11355	pull-request-available
13300424	Avoid scheduling deadlocks caused by cyclic input dependencies between regions	"Imagine a job like this:
A -- (pipelined FORWARD) --> B -- (blocking ALL-to-ALL) --> D
A -- (pipelined FORWARD) --> C -- (pipelined FORWARD) --> D
parallelism=2 for all vertices.

We will have 2 execution pipelined regions:
R1 = {A1, B1, C1, D1}
R2 = {A2, B2, C2, D2}

R1 has a cross-region input edge (B2->D1).
R2 has a cross-region input edge (B1->D2).

Scheduling deadlock will happen since we schedule a region only when all its inputs are consumable (i.e. blocking partitions to be finished). This is because R1 can be scheduled only if R2 finishes, while R2 can be scheduled only if R1 finishes.

To avoid this, one solution is to force a logical pipelined region with intra-region ALL-to-ALL blocking edges to form one only execution pipelined region, so that there would not be cyclic input dependency between regions.
Besides that, we should also pay attention to avoid cyclic cross-region POINTWISE blocking edges. "	FLINK	Closed	3	7	11355	pull-request-available
13259214	Annotate all MiniCluster tests in flink-runtime with AlsoRunWithSchedulerNG	"This task is to annotate all MiniCluster tests with AlsoRunWithSchedulerNG in flink-runtime, so that we can know breaking changes in time when further improving the new generation scheduler. 

We should also guarantee the annotated tests to pass, either by fixing failed tests, or not annotating a failed test and opening a ticket to track it.
"	FLINK	Closed	3	7	11355	pull-request-available
13259037	Support global failure handling for DefaultScheduler (SchedulerNG)	"Global failure handling(full restarts) is widely used in ExecutionGraph components and even other components to recover the job from an inconsistent state. 

We need to support it for DefaultScheduler to not break the safety net. More details see [here|https://github.com/apache/flink/pull/9663/files#r326892524].

There can be follow ups of this task to replace usages of full restarts with JVM termination, in cases that are considered as bugs/unexpected to happen.

Implementation plan:
1. Add {{getGlobalFailureHandlingResult(Throwable)}} in {{ExecutionFailureHandler}}
2. Add an interface {{handleGlobalFailure(Throwable)}} in {{SchedulerNG}} and implement it in {{DefaultScheduler}}
3. Add an interface {{notifyGlobalFailure(Throwable)}} in {{InternalTaskFailuresListener}} and rework the implementations to use {{SchedulerNG#handleGlobalFailure}}
4. Rework {{ExecutionGraph#failGlobal}} to invoke {{InternalTaskFailuresListener#notifyGlobalFailure}} for ng scheduler"	FLINK	Closed	3	7	11355	pull-request-available
13269902	Let tasks in a batch get scheduled in topological order and subtaskIndex ascending pattern	"I'd propose to let tasks in a batch get scheduled in topological order and subtaskIndex ascending pattern. 
There can be 2 benefits:
1. there would be less chance for a task to get launched before its upstream tasks, which reduces {{requestPartitionState}} RPCs to JobMaster.
2. logs could be more readable, e.g.
ordered:
Source: source (1/20) ... switched from CREATED to SCHEDULED.
Source: source (2/20) ... switched from CREATED to SCHEDULED.
...
Source: source (20/20) ... switched from CREATED to SCHEDULED.
Flat Map (1/20) ... switched from CREATED to SCHEDULED.
...
Flat Map (20/20) ... switched from CREATED to SCHEDULED.

disordered:
Source: source (1/20) ... switched from CREATED to SCHEDULED.
Flat Map (11/20) ... switched from CREATED to SCHEDULED.
Source: source (19/20) ... switched from CREATED to SCHEDULED.
Flat Map (2/20) ... switched from CREATED to SCHEDULED.
...

The detailed proposal is:
1. change scheduling related methods to take and return tasks as {{List}} instead of {{Collection}} in {{DefaultScheduler}} and related classes
2. sort the tasks received in {{DefaultScheduler#allocateSlotsAndDeploy}}  to be topological sorted (primary) and subtaskIndex ascending (secondary) order before scheduling them. The tasks scheduled by {{EagerSchedulingStrategy}} can be in order with this change.
3. Change {{LazyFromSourcesSchedulingStrategy}} to schedule tasks in the original order it receives the tasks, which is usually in the desired order. We do this because in FLINK-14162 we may invoke #allocateSlotsAndDeploy on each vertex individually in this scheduling strategy, so that the ordering in {{DefaultScheduler}} would not work.
Note that it's just best effort since we always receives a Set of tasks in #restartTasks. But it should be Ok since the disordering does not result in more {{requestPartitionState}} RPCs with this scheduling strategy, and batch jobs are usually in small regions so that the log disordering is not that obvious. "	FLINK	Closed	3	7	11355	pull-request-available
13256247	Introduce managed memory fractions to StreamConfig	"Introduce {{managedMemFractionOnHeap}} and {{managedMemFractionOffHeap}} in {{StreamConfig}}, so they can be set by {{StreamingJobGraphGenerator}} and used by operators in runtime. 

This step should not introduce any behavior changes."	FLINK	Resolved	3	7	11355	pull-request-available
13305383	Remove AdaptedRestartPipelinedRegionStrategyNG	"It was used by the legacy scheduler and is not used anymore.
Removing it can ease the work to further remove the legacy scheduling logics in ExecutionGraph."	FLINK	Closed	3	7	11355	pull-request-available
13273539	Errors happen in the scheduling of DefaultScheduler are not shown in WebUI	"WebUI relies on {{ExecutionGraph#failureInfo}} and {{Execution#failureCause}} to generate error info (via {{JobExceptionsHandler#createJobExceptionsInfo}}). 
Errors happen in the scheduling of DefaultScheduler are not recorded into those fields, thus cannot be shown to users in WebUI (nor via REST queries).

To solve it, 
1. global failures should be recorded into {{ExecutionGraph#failureInfo}}, via {{ExecutionGraph#initFailureCause}} which can be exposed as {{SchedulerBase#initFailureCause}}.
2. for task failures, one solution I can think of is to avoid invoking {{DefaultScheduler#handleTaskFailure}} directly on scheduler's internal failures. Instead, we can introduce {{ExecutionVertexOperations#fail(ExecutionVertex)}} to hand the error to {{ExecutionVertex}} as a common failure.

cc [~gjy]"	FLINK	Closed	1	1	11355	pull-request-available
13240095	Add an adapter of region failover NG for legacy scheduler	"We need an adapter to adapt flip1.RestartPipelinedRegionStrategy for legacy scheduler, so that the legacy scheduler can support fine grained recovery.

The failover recovery should respect scheduleMode(EAGER/LAZY_FROM_SOURCES) to avoid more tasks get re-scheduled than that before failover."	FLINK	Closed	3	7	11355	pull-request-available
13267414	Slot leaks if SharedSlotOversubscribedException happens	"If a {{SharedSlotOversubscribedException}} happens, the {{MultiTaskSlot}} will release some of its child {{SingleTaskSlot}}. The triggered releasing will trigger a re-allocation of the task slot right inside {{SingleTaskSlot#release(...)}}. So that a previous allocation in {{SloSharingManager#allTaskSlots}} will be replaced by the new allocation because they share the same {{slotRequestId}}.
However, the {{SingleTaskSlot#release(...)}} will then invoke {{MultiTaskSlot#releaseChild}} to release the previous allocation with the {{slotRequestId}}, which will unexpectedly remove the new allocation from the {{SloSharingManager}}.
In this way, slot leak happens because the pending slot request is not tracked by the {{SloSharingManager}} and cannot be released when its payload terminates.

A test case {{testNoSlotLeakOnSharedSlotOversubscribedException}} which exhibits this issue can be found in this [commit|https://github.com/zhuzhurk/flink/commit/9024e2e9eb4bd17f371896d6dbc745bc9e585e14].

The slot leak blocks the TPC-DS queries on flink 1.10, see FLINK-14674.

To solve it, I'd propose to strengthen the {{MultiTaskSlot#releaseChild}} to only remove its true child task slot from the {{SloSharingManager}}, i.e. add a check {{if (child == allTaskSlots.get(child.getSlotRequestId()))}} before invoking {{allTaskSlots.remove(child.getSlotRequestId())}}.
"	FLINK	Resolved	2	1	11355	pull-request-available
13261745	Enable ClassLoaderITCase to pass with scheduler NG	"ClassLoaderITCase, EventTimeWindowCheckpointingITCase and WindowCheckpointingITCase now fail with scheduler NG.
There are 3 reasons for the failure:
1. state restore is not supported in scheduler NG yet
2. the cause of the expected exception is a bit different
3. there are multiples tasks in multiple regions, which will result in more failovers than expected as scheduler NG is using region failover

We need to support the state restore in scheduler NG and the fix the case issues. And then annotate them with AlsoRunWithSchedulerNG."	FLINK	Closed	3	7	11355	pull-request-available
13260078	Allocate shared slot resources respecting the resources of all vertices in the group	"With FLINK-14058, it is assumed that a shared slot should be large enough to be used by one instance of each JobVertex in the group simultaneously.

To support it, a shared slot resources should be the sum of all JobVertex resources in the group.

Here's the concrete proposal:
1. Add a physicalSlotResourceProfile in SlotProfile. If the task is not in a shared slot, it is the task resource profile. Otherwise it is the slot sharing group resource profile. It should be used for physical slot allocation. Rename previous ResourceProfile to be taskResourceProfile for logical slot allocation.
2. SharedSlotOversubscribedException and its handling can be removed, including part of the children slots releasing and re-allocating. This is because partial fulfillment should not happen anymore with #1. A simple sanity check can be kept for oversubscribing.
"	FLINK	Closed	3	7	11355	pull-request-available
13337144	Job may try to leave SUSPENDED state in ExecutionGraph#failJob()	"{{SUSPENDED}} is a terminal state which a job is not supposed to leave this state once entering. However, {{ExecutionGraph#failJob()}} did not check it and may try to transition a job out from {{SUSPENDED}} state. This will cause unexpected errors and may lead to JM crash.
The problem can be visible if we rework {{ExecutionGraphSuspendTest}} to be based on {{DefaultScheduler}}.
We should harden the check in {{ExecutionGraph#failJob()}}."	FLINK	Closed	3	1	11355	pull-request-available
13338374	Remove legacy scheduling in ExecutionGraph components	"This is one step towards making ExecutionGraph a pure data structure.
Note that this task mainly targets to remove the legacy codes of scheduling and failover. Codes of Execution state transition and task deployment will be factored out in follow-up tasks."	FLINK	Closed	3	7	11355	pull-request-available
13260758	Reset vertices right after they transition to terminated states	"Currently in DefaultScheduler, tasks to restart will remain in terminated state until they are re-scheduled by the SchedulingStrategy.
This behavior may cause 2 problems:
1. Failed/Canceled tasks are possibly not be able to be restarted in lazy scheduling. e.g. The job A1--pipelined-->B1 fails. And only A1 will be re-scheduled on restartTasks() since the inputs of B1 are not ready. B1 should be scheduled later on the partition consumable event from restarted A1. But the terminal state of B1 will prevent B1 from being scheduled.
2. Keeping a task in FAILED/CANCELED state for a long time can happen if it takes a long time for its inputs to become ready again. This is also not friendly to users, which may cause confusions.

That's why I'd propose to reset vertices right after they transition to terminated states.
"	FLINK	Closed	3	7	11355	pull-request-available
13347342	Remove unknown input channels and process to update partitions	"With the latest pipelined region scheduling, Flink no longer launch a task before knowing the locations of all the partitions it consumes. `scheduleOrUpdateConsumers` is no longer needed and is removed in FLINK-20439.

Unknown input channels and the process to update it is also no longer needed. I'd propose to remove them and the benefits are:
1. simplifying the code of both scheduler and shuffle components
2. simplifying interfaces of ShuffleEnvironment and ShuffleDescriptor 
3. ensure the assumptions in InputGate#resumeConsumption() implementations
4. allow to remove ScheduleMode#allowLazyDeployment() and later completely remove ScheduleMode

"	FLINK	Closed	3	7	11355	pull-request-available
13457204	Enable to identify whether a job vertex contains source/sink operators	"Speculative execution does not support sources/sinks in the first version. Therefore, it will not create speculation instances for vertices which contains source/sink operators.

Note that a job vertex with no input/output does not mean it is a source/sink vertex. Multi-input sources can have input. And it's possible that the vertex with no output edge does not contain any sink operator. Besides that, a new sink with topology can spread the sink logic into multiple job vertices connected with job edges."	FLINK	Closed	3	7	11355	pull-request-available
13280382	Remove AlsoRunWithLegacyScheduler	"AlsoRunWithLegacyScheduler is used to enable IT cases for legacy scheduler.
This task is to remove it as well as the related test stages from travis."	FLINK	Resolved	3	7	11355	pull-request-available
13266305	Move allVerticesInSameSlotSharingGroupByDefault from ExecutionConfig to StreamGraph	"allVerticesInSameSlotSharingGroupByDefault is currently for internal use only.
It's better to not add it in ExecutionConfig which is for user configurations.
More details see discussion [this|https://issues.apache.org/jira/browse/FLINK-14059?focusedCommentId=16967392&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16967392] and [this|https://github.com/apache/flink/pull/10007#discussion_r342481078].

I'd propose to move it to StreamGraph."	FLINK	Closed	1	7	11355	pull-request-available
13269522	Remove nullable assumption of task slot sharing group	"In runtime, a null slot sharing group means the vertex should be run in an individual slot which is not shared with other vertices. 
However, at the moment, there is no way to set the slot sharing group of a vertex to null in production. This is because {{StreamGraphGenerator}} would always assign a non-null slot sharing group to an operator({{StreamNode}}), and that would be the group of the generated {{JobVertex}}. (In the case of {{DataSet}}, all vertices are in the ""default"" group.)

Moreover, currently users are already able to force an operator to run in an individual slot by specify a different slot sharing group key to that operator.

So looks that supporting null slot sharing groups in runtime does not make much sense. And it's making things more complex, with codes like {{@Nullable}} annotations for slot sharing group, task required resources calculations, and single task slot allocation. 

To make runtime code base and future development simpler, I'd propose to introduce the assumption that task slot sharing group should not be null. 
The detailed proposal is:
1. In {{StreamingJobGraphGenerator}}, we check to ensure the slot sharing group key of a {{StreamNode}} is non-null, so that we always set a non-null {{SlotSharingGroup}} to a JobVertex
2. Remove all the {{@Nullable}} annotations for {{SlotSharingGroup}} and {{SlotSharingGroupID}} in several classes
-3. Always use the slot sharing group resources as the physical slot resources for a task-
4. Remove {{SchedulerImp#allocateSingleSlot}} and the related code paths

cc [~chesnay] [~gjy] [~trohrmann]"	FLINK	Closed	3	4	11355	pull-request-available
13522075	SortAggITCase.testLeadLag failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45389&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12560

{code}
Jan 30 11:03:32 [ERROR] Tests run: 72, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.42 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase
Jan 30 11:03:32 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.testLeadLag  Time elapsed: 0.547 s  <<< FAILURE!
Jan 30 11:03:32 java.lang.AssertionError: 
Jan 30 11:03:32 
Jan 30 11:03:32 Results do not match for query:
Jan 30 11:03:32   
Jan 30 11:03:32 SELECT
Jan 30 11:03:32   a,
Jan 30 11:03:32   b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
Jan 30 11:03:32   c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
Jan 30 11:03:32   d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
Jan 30 11:03:32   e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
Jan 30 11:03:32   f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
Jan 30 11:03:32   g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
Jan 30 11:03:32   h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
Jan 30 11:03:32   i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
Jan 30 11:03:32   j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
Jan 30 11:03:32   k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
Jan 30 11:03:32   l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
Jan 30 11:03:32   m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
Jan 30 11:03:32   n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag
Jan 30 11:03:32 
Jan 30 11:03:32 FROM UnnamedTable$230
Jan 30 11:03:32 order by a
Jan 30 11:03:32 
Jan 30 11:03:32 
Jan 30 11:03:32 Results
Jan 30 11:03:32  == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
Jan 30 11:03:32  +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]
Jan 30 11:03:32  +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]
Jan 30 11:03:32 !+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null]
Jan 30 11:03:32         
Jan 30 11:03:32 Plan:
Jan 30 11:03:32   == Abstract Syntax Tree ==
Jan 30 11:03:32 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
Jan 30 11:03:32 +- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
Jan 30 11:03:32    +- LogicalUnion(all=[true])
[...]
{code}"	FLINK	Closed	2	1	11355	pull-request-available, test-stability
13296744	Integrate pipelined region scheduling	"This task is to integrate PipelinedRegionSchedulingStrategy with DefaultScheduler. A config option ""jobmanager.scheduler.scheduling-strategy"" should be introduced to control whether to use the new ""region"" scheduling or to use the ""legacy"" eager/lazy-from-sources scheduling."	FLINK	Closed	3	7	11355	pull-request-available
13305385	Rework tests to not rely on legacy scheduling logics in ExecutionGraph anymore	"The legacy scheduling logics in ExecutionGraph were used by the legacy scheduler. They are not in production use anymore.
In order to remove legacy scheduling logics, it is needed to rework the tests which relied on them."	FLINK	Closed	3	7	11355	pull-request-available
13265948	Fix matching logics of ResourceSpec/ResourceProfile/Resource considering double values	"There are resources of double type values, like cpuCores in ResourceSpec/ResourceProfiles or all extended resources. These values can be generated via a merge or subtract, so that there can be small deltas.

Currently, in resource matching, these resources are matched without considering the deltas, which may result in issues as below:
1. A shared slot cannot fulfill a slot request even if it should be able to (because it is possible that {{(d1 + d2) - d1 < d2}} for double values)
2. if a shared slot is used up, an unexpected error may occur when calculating its remaining resources in SlotSharingManager#listResolvedRootSlotInfo -> ResourceProfile#subtract
3. an unexpected error may happen when releasing a single task slot from a shared slot (in ResourceProfile#subtract)

To solve this issue, I'd propose to:
1. Change {{Resource}} to use {{BigDecimal}} to manage double values. This enabled the values able to be strictly compared, and able to be additively merged/subtracted with no precision loss. Extended resources can work correctly with double values with this change.
2. Introduce {{CPUResource}} to represent cpu cores. It is based on {{Resource}}
3. Change ResourceSpec/ResourceProfile to use CPUResource for cpu cores
"	FLINK	Closed	3	7	11355	pull-request-available
13261537	Prevent vertex from being affected by outdated deployment (SchedulerNG)	"DefaultScheduler currently will cancel latest execution of a vertex when the vertex version is outdated. This may lead to undesirable failover of a healthy vertex.

I'd propose to remove the vertex cancel process in {{DefaultScheduler#stopDeployment}} because the cancellation is not not needed here, since the version of a scheduled vertex is only outdated after the vertex is canceled by others."	FLINK	Closed	3	7	11355	pull-request-available
13314235	Add a code contribution section about how to look for what to contribute	This section is to give general advices about browsing open Jira issues and starter tasks.	FLINK	Closed	3	4	11355	pull-request-available
13470071	RemoveCachedShuffleDescriptorTest#testRemoveOffloadedCacheForPointwiseEdgeAfterFailover causes fatal error on CI	"{code:java}
Jul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in log
Jul 05 03:30:03 [ERROR] Process Exit Code: 239
Jul 05 03:30:03 [ERROR] Crashed tests:
Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest
Jul 05 03:30:03 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Jul 05 03:30:03 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter4932865857415988980.jar /__w/1/s/flink-runtime/target/surefire 2022-07-05T03-23-25_404-jvmRun1 surefire8916732512419442726tmp surefire_2130262314165063415tmp
Jul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in log
Jul 05 03:30:03 [ERROR] Process Exit Code: 239
Jul 05 03:30:03 [ERROR] Crashed tests:
Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8147"	FLINK	Closed	1	11500	11355	pull-request-available
13253607	Ensure defaultInputDependencyConstraint to be non-null when setting it in ExecutionConfig	"If a user invokes ExecutionConfig#setDefaultInputDependencyConstraint(null) to set the defaultInputDependencyConstraint to be null, the scheduling topology building will throw NPE in ExecutionGraph creating stage, causing a master node fatal error.

Thus we need to do checkNotNull on the ExecutionConfig#setDefaultInputDependencyConstraint param to remind users in an early stage.

 

Exception is as blow:

2019-08-28T15:19:21.287+0000 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Fatal error occurred in the cluster entrypoint.
 org.apache.flink.runtime.dispatcher.DispatcherException: Failed to take leadership with session id 2f8f7919-a81b-4529-ad57-9789dbf07707.
         at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$null$30(Dispatcher.java:915)
         at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
         at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
         at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
         at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
         at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:691)
         at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
         at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
         at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
         at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
         at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:739)
         at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
         at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
         at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
         at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
         at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
         at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
         at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
         at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
         at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
         at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
         at akka.actor.Actor.aroundReceive(Actor.scala:517)
         at akka.actor.Actor.aroundReceive$(Actor.scala:515)
         at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
         at akka.actor.ActorCell.invoke(ActorCell.scala:561)
         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
         at akka.dispatch.Mailbox.run(Mailbox.scala:225)
         at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
         at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
         at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
         at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
         at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
 Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
         at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
         at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
         at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
         at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
         ... 4 more
 Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
         at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:152)
         at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:83)
         at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:375)
         at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
         ... 7 more
 Caused by: java.lang.NullPointerException
         at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58)
         at org.apache.flink.runtime.scheduler.adapter.DefaultSchedulingExecutionVertex.<init>(DefaultSchedulingExecutionVertex.java:59)
         at org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapter.generateSchedulingExecutionVertex(ExecutionGraphToSchedulingTopologyAdapter.java:113)
         at org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapter.<init>(ExecutionGraphToSchedulingTopologyAdapter.java:65)
         at org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph(ExecutionGraph.java:939)
         at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:230)
         at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:106)
         at org.apache.flink.runtime.scheduler.LegacyScheduler.createExecutionGraph(LegacyScheduler.java:207)
         at org.apache.flink.runtime.scheduler.LegacyScheduler.createAndRestoreExecutionGraph(LegacyScheduler.java:184)
         at org.apache.flink.runtime.scheduler.LegacyScheduler.<init>(LegacyScheduler.java:176)
         at org.apache.flink.runtime.scheduler.LegacySchedulerFactory.createInstance(LegacySchedulerFactory.java:70)
         at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:275)
         at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:265)
         at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
         at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
         at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:146)
         ... 10 more"	FLINK	Closed	3	1	11355	pull-request-available
13244306	Add documentation for AdaptedRestartPipelinedRegionStrategyNG	"It should be documented that if {{jobmanager.execution.failover-strategy}} is set to _region_, the new pipelined region failover strategy ({{AdaptedRestartPipelinedRegionStrategyNG}}) will be used. 

*Acceptance Criteria*
* config values _region_ and _full_ are documented
* to be decided: config values _region-legacy_ and _individual_ remain undocumented"	FLINK	Resolved	1	7	11355	pull-request-available
13296748	Allocates slots in bulks on pipelined region scheduling	"The DefaultExecutionSlotAllocator should allocate slot in bulks, that means the bulks of slot requests will be sent together and fail if any of the request fails.

Note this is a first step to fully functional bulk slot allocation. Current limitations would be:
1. Slot sharing will be ignored
2. Co-location constraints are not allowed
3. intra-bulk input location preferences will be ignored"	FLINK	Closed	3	7	11355	pull-request-available
13305384	Remove RestartIndividualStrategy	"It was used by the legacy scheduler and is not used anymore.
Removing it can ease the work to further remove the legacy scheduling logics in ExecutionGraph."	FLINK	Closed	3	7	11355	pull-request-available
13230811	Implement a region failover strategy based on new FailoverStrategy interfaces	"It is a re-requisite of FLINK-12068. 

The new strategy interface design can be found at FLINK-10429."	FLINK	Closed	3	7	11355	pull-request-available
13256248	Set managed memory fractions according to slot sharing groups	"* For operators with specified {{ResourceSpecs}}, calculate fractions according to operators {{ResourceSpecs}}
 * For operators with unknown {{ResourceSpecs}}, calculate fractions according to number of operators -using managed memory-

This step should not introduce any behavior changes."	FLINK	Closed	3	7	11355	pull-request-available
13224859	Backtrack failover regions if intermediate results are unavailable	The batch failover strategy needs to be able to backtrack fail over regions if an intermediate result is unavailable. Either by explicitly checking whether the intermediate result partition is available or via a special exception indicating that a result partition is no longer available.	FLINK	Resolved	3	7	11355	pull-request-available
13472017	Speculative execution for InputFormat sources	This task enables InputFormat sources for speculative execution.	FLINK	Closed	3	7	11355	pull-request-available
13259491	Remove legacy RestartPipelinedRegionStrategy	"The legacy {{RestartPipelinedRegionStrategy}} has been superseded by {{AdaptedRestartPipelinedRegionStrategyNG}} in Flink 1.9.
It heavily depends on ExecutionGraph components and becomes a blocker for a clean scheduler re-architecture.

Since it is basically broken and is not a public interface(the failover config ""region"" now points to AdaptedRestartPipelinedRegionStrategyNG), it's safe to remove it.

I'd propose to remove the legacy {{RestartPipelinedRegionStrategy}} components to reduce efforts for further changes on flink runtime.

"	FLINK	Closed	4	7	11355	pull-request-available
13270976	Managed memory fractions should be rounded properly to not summed up to be more than 1.0	"Managed memory fractions should be rounded to floor at a certain precision when divided by the number of operators, otherwise the fractions can be summed up to be more than 1.0 due to the double precision issue, and the last operator may fail to allocate managed memory it is supposed to be able to acquire.

To achieve that, I think we should change {{StreamingJobGraphGenerator#setManagedMemoryFractionForOperator}} to use BigDecimal#divide(otherValue, scale, RoundMode.ROUND_DOWN) to calculate the fractions. In this way, the sum of the fractions will not exceed 1.0.
The scale can be a bit larger (maybe 16) so that we only lose little managed memory.

cc [~chesnay]"	FLINK	Closed	3	7	11355	pull-request-available
13264699	Make clear the way to aggregate specified cpuCores resources	"I'm raising this question because I find {{cpuCores}} in {{ResourceSpec#merge}} are aggregated with {{max()}}, while in {{ResourceProfile#merge}} it is {{sum()}}.

This means that when calculating resources of a vertex from within operators, the {{cpuCores}} is the max value. While it is a sum(or subtraction in {{ResourceProfile#subtract}}) when dealing with shared slot bookkeeping and related checks. 
This is confusing to me, especially when I'm considering how to generate a shared slot resource spec merged from all vertices in it(see FLINK-14314).

I'm not pretty sure if we already have a concise definition for it?
If there is, we need to respect it and change {{ResourceSpec}} or {{ResourceProfile}} accordingly.
If not, we need to decide it first before we can move on with fine grained resources.

Need to mention that if we take the {{max()}} way, we need to re-consider how we can conduct a correct {{ResourceProfile#subtract}} regarding {{cpuCores}}, since there is not a clear way to reverse a {{max()}} computation.

cc [~trohrmann] [~xintongsong] [~azagrebin]
"	FLINK	Resolved	3	7	11355	pull-request-available
13267935	Improve batch schedule check input consumable performance	"Now if we launch batch job with 1000+ parallelism:

Even if we set the akka timeout of 2 minutes, the heartbeat is likely to timeout.

 JobMaster is buzy:
{code:java}
java.lang.Thread.State: RUNNABLE
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958)
        at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
        at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.anyMatch(ReferencePipeline.java:449)
        at org.apache.flink.runtime.executiongraph.ExecutionVertex.isInputConsumable(ExecutionVertex.java:824)
        at org.apache.flink.runtime.executiongraph.ExecutionVertex$$Lambda$257/564237119.test(Unknown Source)
        at java.util.stream.MatchOps$2MatchSink.accept(MatchOps.java:119)
        at java.util.stream.Streams$RangeIntSpliterator.tryAdvance(Streams.java:89)
        at java.util.stream.IntPipeline.forEachWithCancel(IntPipeline.java:162)
        at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.IntPipeline.allMatch(IntPipeline.java:482)
        at org.apache.flink.runtime.executiongraph.ExecutionVertex.checkInputDependencyConstraints(ExecutionVertex.java:811)
        at org.apache.flink.runtime.executiongraph.Execution.scheduleOrUpdateConsumers(Execution.java:889)
        at org.apache.flink.runtime.executiongraph.Execution.markFinished(Execution.java:1074)
        at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateStateInternal(ExecutionGraph.java:1597)
        at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:1570)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:424)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
{code}"	FLINK	Closed	3	7	11355	pull-request-available
13474618	Assign speculative execution attempt with correct CREATED timestamp	Currently, newly created speculative execution attempt is assigned with a wrong CREATED timestamp in SpeculativeScheduler. We need to fix it.	FLINK	Closed	3	1	11355	pull-request-available
13275390	JobManager crashes in the standalone model when cancelling job which subtask' status is scheduled	"Use start-cluster.sh to start a standalone cluster, and then submit a job from the streaming's example which name is TopSpeedWindowing, parallelism is 20. Wait for one minute, cancel the job, jobmanager will crash. The exception stack is:

{noformat}
2019-12-19 10:12:11,060 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL: Thread 'flink-akka.actor.default-dispatcher-2' produced an uncaught exception. Stopping the process...2019-12-19 10:12:11,060 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL: Thread 'flink-akka.actor.default-dispatcher-2' produced an uncaught exception. Stopping the process...java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at org.apache.flink.runtime.scheduler.DefaultScheduler.propagateIfNonNull(DefaultScheduler.java:387) at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$deployAll$4(DefaultScheduler.java:372) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:705) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:170) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFulfillSlotRequestOrMakeAvailable(SlotPoolImpl.java:534) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseSingleSlot(SlotPoolImpl.java:479) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseSlot(SlotPoolImpl.java:390) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:557) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.releaseChild(SlotSharingManager.java:607) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.access$700(SlotSharingManager.java:352) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:716) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.releaseSharedSlot(SchedulerImpl.java:552) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.cancelSlotRequest(SchedulerImpl.java:184) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.returnLogicalSlot(SchedulerImpl.java:195) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:181) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:778) at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2140) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:178) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:125) at org.apache.flink.runtime.executiongraph.Execution.releaseAssignedResource(Execution.java:1451) at org.apache.flink.runtime.executiongraph.Execution.finishCancellation(Execution.java:1170) at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1150) at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1129) at org.apache.flink.runtime.executiongraph.Execution.cancelAtomically(Execution.java:1111) at org.apache.flink.runtime.executiongraph.Execution.cancel(Execution.java:804) at org.apache.flink.runtime.executiongraph.ExecutionVertex.cancel(ExecutionVertex.java:729) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.mapExecutionVertices(ExecutionJobVertex.java:505) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.cancelWithFuture(ExecutionJobVertex.java:494) at org.apache.flink.runtime.executiongraph.ExecutionGraph.cancelVerticesAsync(ExecutionGraph.java:952) at org.apache.flink.runtime.executiongraph.ExecutionGraph.cancel(ExecutionGraph.java:903) at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:432) at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:364) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:824) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ... 69 moreCaused by: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at org.apache.flink.runtime.executiongraph.ExecutionVertex.tryAssignResource(ExecutionVertex.java:701) at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$5(DefaultScheduler.java:409) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ... 70 more2019-12-19 10:12:11,066 INFO  org.apache.flink.runtime.blob.BlobServer                      - Stopped BLOB server at 0.0.0.0:54944
{noformat}"	FLINK	Resolved	1	1	11355	pull-request-available
13296751	Introduce GlobalDataExchangeMode for JobGraph Generation	"Introduce GlobalDataExchangeMode with 4 modes:
 * ALL_EDGES_BLOCKING
 * FORWARD_EDGES_PIPELINED
 * POINTWISE_EDGES_PIPELINED
 * ALL_EDGES_PIPELINED

StreamGraph will be extended with a new field to host the GlobalDataExchangeMode. In the JobGraph generation stage, this mode will be used to determine the data exchange type of each job edge.

More details see [FLIP-119#Global Data Exchange Mode|https://cwiki.apache.org/confluence/display/FLINK/FLIP-119+Pipelined+Region+Scheduling#FLIP-119PipelinedRegionScheduling-GlobalDataExchangeMode]"	FLINK	Closed	3	7	11355	pull-request-available
13268515	Replace Java Streams with for-loops in vertex input checking	"Vertex input checking is invoked in lazily triggered scheduling by a FINISHED vertex state update RPC or a {{scheduleOrUpdateConsumers}} RPC. 
Java Streams is used in it, but it should be avoided since it is performance critical code. See ref [1] and [2].
We should refactor these Java Streams to improve the performance, for both legacy scheduler and NG schedulers.

cc [~sewen] [~gjy]


[1] [flink code style guide|https://flink.apache.org/contributing/code-style-and-quality-java.html]
[2] [discussion & performance test|https://issues.apache.org/jira/browse/FLINK-14735?focusedCommentId=16974209&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16974209] which compares the performance."	FLINK	Closed	3	4	11355	pull-request-available
13286367	Replacing vertexExecution in ScheduledUnit with executionVertexID	"{{ScheduledUnit#vertexExecution}} is nullable but {{ProgrammedSlotProvider}} requires it to be non-null to work. This makes {{ProgrammedSlotProvider}} not able to be used by new scheduler tests since {{vertexExecution}} is never set in the new scheduler code path. It blocks us from reworking tests which are based legacy scheduling to base on the new scheduler.

Besides that, there are 2 other problems caused by the nullable vertexExecution:
1. The log printed in SchedulerImpl#allocateSlotInternal(...) may contain no useful info since the vertexExecution can be null.
2. NPE issue reported in FLINK-16145.

Thus I would propose to replace the nullable vertexExecution with a non-null executionVertexID.
"	FLINK	Resolved	3	7	11355	pull-request-available
13395438	StackOverflowException can happen if a large scale job failed to acquire enough slots in time	"When requested slots are not fulfilled in time, task failure will be triggered and all related tasks will be canceled and restarted. However, in this process, if a task is already assigned a slot, the slot will be returned to the slot pool and it will be immediately used to fulfill pending slot requests of the tasks which will soon be canceled. The execution version of those tasks are already bumped in {{DefaultScheduler#restartTasksWithDelay(...)}} so that the assignment will fail immediately and the slot will be returned to the slot pool and again used to fulfill pending slot requests. StackOverflow can happen in this way when there are many vertices, and fatal error can happen and lead to JM crash. A sample call stack is attached below.

To fix the problem, one way is to cancel the pending requests of all the tasks which will be canceled soon(i.e. tasks with version bumped) before canceling these tasks.

{panel}
...
        at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.cancelSlotRequest(PhysicalSlotProviderImpl.java:112) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.releaseSharedSlot(SlotSharingExecutionSlotAllocator.java:242) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.releaseExternally(SharedSlot.java:281) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.removeLogicalSlotRequest(SharedSlot.java:242) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.returnLogicalSlot(SharedSlot.java:234) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:203) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:200) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:130) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.releaseSlotIfPresent(DefaultScheduler.java:542) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$8(DefaultScheduler.java:505) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.fulfill(DeclarativeSlotPoolBridge.java:552) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequestSlotMatching.fulfillPendingRequest(DeclarativeSlotPoolBridge.java:587) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.newSlotsAreAvailable(DeclarativeSlotPoolBridge.java:171) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.lambda$freeReservedSlot$0(DefaultDeclarativeSlotPool.java:316) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.Optional.ifPresent(Optional.java:159) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeReservedSlot(DefaultDeclarativeSlotPool.java:313) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseSlot(DeclarativeSlotPoolBridge.java:335) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.cancelSlotRequest(PhysicalSlotProviderImpl.java:112) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
...

{panel}
"	FLINK	Closed	2	1	11355	pull-request-available
13442622	Remove unused notifyPartitionDataAvailable process	The `notifyPartitionDataAvailable` process was used to trigger downstream task scheduling once a pipelined partition has data produced at TM side. It is no longer used. Therefore I propose to remove it to cleanup the code base.	FLINK	Closed	3	11500	11355	pull-request-available
13263085	Support building pipelined regions from base topology	"Previously pipelined regions can only be built from FailoverTopology. With FLINK-14451 that extends FailoverTopology from base topology, PipelinedRegionComputeUtil can be modified a bit to able to build regions from the base topology.
This is a pre-requisite of FLINK-14312. And also enables SchedulingTopology to provide pipelined region info.

More details see FLINK-14330 and the [design doc|https://docs.google.com/document/d/1f88luAOfUQ6Pm4JkxYexLXpfH-crcXJdbubi1pS2Y5A/edit#]."	FLINK	Resolved	3	7	11355	pull-request-available
13254635	Facilitate enabling new Scheduler in MiniCluster Tests	"Currently, tests using the {{MiniCluster}} use the legacy scheduler by default. Once the new scheduler is implemented, we should run tests with the new scheduler enabled. However, it is not expected that all tests will pass immediately. Therefore, it should be possible to enable the new scheduler for a subset of tests. 

In the first step the tests should be able to run manually against new scheduler.

*Acceptance Criteria*
 * A junit test category {{AlsoRunWithSchedulerNG}} can be used to mark MiniCluster tests.
 * A new maven profile {{scheduler-ng}} will be enabled to support running {{AlsoRunWithSchedulerNG}} annotated tests with the new scheduler."	FLINK	Closed	3	7	11355	pull-request-available
13226848	Resetting ExecutionVertex in region failover may cause inconsistency of IntermediateResult status	"Two status may not be correct with region failover and current reset logic.
 # *numberOfRunningProducers* in *IntermediateResult*.
 # *hasDataProduced* in *IntermediateResultPartition*.

This is because currently only when the *ExecutionJobVertex* is reset will the related *IntermediateResult*(and the inner *IntermediateResultPartition*) get reset. But region failover only resets the affected *ExecutionVertex*(es),  rather than the entire *ExecutionJobVertex*, leaving the status listed above in an inconsistent state.

Problems below may occur as a result:
 # when a FINISHED vertex is restarted and finishes again, the *IntermediateResult.numberOfRunningProducers* may drop below 0 and throws exception to trigger global failover
 # the *IntermediateResult.numberOfRunningProducers* can be smaller than fact, letting the downstream vertices scheduled earlier than expected
 # the *IntermediateResultPartition* is reset and not started yet but the *hasDataProduced* remains true

That's why I'd propose we add IntermediateResult status adjust logic to *ExecutionVertex.**resetForNewExecution()***.

Detailed design: [https://docs.google.com/document/d/1YA3k8rwDEv1UdaV9NwoDmwc-XorG__JUXlpyJtDs4Ss/edit?usp=sharing] "	FLINK	Resolved	3	7	11355	pull-request-available
