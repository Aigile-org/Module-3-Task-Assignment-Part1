id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13055567	Concurrent refresh against the same table fails with ConcurrentModificationException	"Concurrent refresh statements against the same table fail with exception below


{code}
I0216 18:34:13.343709 158318 catalog-server.cc:316] Publishing update: CATALOG:2b3e096917a145d3:82177a82fe8dce0f@35450
I0216 18:34:20.475253 193290 jni-util.cc:169] java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
	at java.util.HashMap$EntryIterator.next(HashMap.java:962)
	at java.util.HashMap$EntryIterator.next(HashMap.java:960)
	at org.apache.impala.thrift.THdfsPartition$THdfsPartitionStandardScheme.write(THdfsPartition.java:1832)
	at org.apache.impala.thrift.THdfsPartition$THdfsPartitionStandardScheme.write(THdfsPartition.java:1546)
	at org.apache.impala.thrift.THdfsPartition.write(THdfsPartition.java:1389)
	at org.apache.impala.thrift.THdfsTable$THdfsTableStandardScheme.write(THdfsTable.java:1243)
	at org.apache.impala.thrift.THdfsTable$THdfsTableStandardScheme.write(THdfsTable.java:1071)
	at org.apache.impala.thrift.THdfsTable.write(THdfsTable.java:940)
	at org.apache.impala.thrift.TTable$TTableStandardScheme.write(TTable.java:1628)
	at org.apache.impala.thrift.TTable$TTableStandardScheme.write(TTable.java:1399)
	at org.apache.impala.thrift.TTable.write(TTable.java:1208)
	at org.apache.impala.thrift.TCatalogObject$TCatalogObjectStandardScheme.write(TCatalogObject.java:1241)
	at org.apache.impala.thrift.TCatalogObject$TCatalogObjectStandardScheme.write(TCatalogObject.java:1098)
	at org.apache.impala.thrift.TCatalogObject.write(TCatalogObject.java:938)
	at org.apache.impala.thrift.TCatalogUpdateResult$TCatalogUpdateResultStandardScheme.write(TCatalogUpdateResult.java:995)
	at org.apache.impala.thrift.TCatalogUpdateResult$TCatalogUpdateResultStandardScheme.write(TCatalogUpdateResult.java:867)
	at org.apache.impala.thrift.TCatalogUpdateResult.write(TCatalogUpdateResult.java:748)
	at org.apache.impala.thrift.TResetMetadataResponse$TResetMetadataResponseStandardScheme.write(TResetMetadataResponse.java:359)
	at org.apache.impala.thrift.TResetMetadataResponse$TResetMetadataResponseStandardScheme.write(TResetMetadataResponse.java:321)
	at org.apache.impala.thrift.TResetMetadataResponse.write(TResetMetadataResponse.java:269)
	at org.apache.thrift.TSerializer.serialize(TSerializer.java:79)
	at org.apache.impala.service.JniCatalog.resetMetadata(JniCatalog.java:156)
I0216 18:34:20.488674 193290 status.cc:114] ConcurrentModificationException: null
    @           0x837a89  impala::Status::Status()
    @           0xb80420  impala::JniUtil::GetJniExceptionMsg()
    @           0x82557b  impala::Catalog::ResetMetadata()
    @           0x818e31  CatalogServiceThriftIf::ResetMetadata()
    @           0x8ee85d  impala::CatalogServiceProcessor::process_ResetMetadata()
    @           0x8ebc09  impala::CatalogServiceProcessor::dispatchCall()
    @           0x803f2c  apache::thrift::TDispatchProcessor::process()
    @           0x9f858f  apache::thrift::server::TAcceptQueueServer::Task::run()
    @           0x9f2bb9  impala::ThriftThread::RunRunnable()
    @           0x9f3612  boost::detail::function::void_function_obj_invoker0<>::invoke()
    @           0xbd39c9  impala::Thread::SuperviseThread()
    @           0xbd43a4  boost::detail::thread_data<>::run()
    @           0xe19f1a  thread_proxy
    @       0x348e407aa1  (unknown)
    @       0x348e0e893d  (unknown)
E0216 18:34:20.488878 193290 catalog-server.cc:79] ConcurrentModificationException: null
{code}"	IMPALA	Resolved	3	1	1880	catalog-server, scalability
13053681	Incorrect placement of a !empty() predicate at an outer join.	"The following query incorrectly returns 0.
{code}
SELECT COUNT(*)
FROM table_3.field_87 a1
INNER JOIN table_1.field_18 a2 ON (a2.key) = (a1.value)
INNER JOIN a2.value a3
LEFT JOIN (
SELECT
(MAX(a7.pos)) + (711) AS int_col
FROM a2.value a4
INNER JOIN a4.item a5
INNER JOIN a4.item a6
INNER JOIN a2.value.item a7 ON ((a7.pos) = (a4.pos)) AND ((a7.pos) = (a4.pos))
WHERE
(822.8632536034) < (a6.pos)
) a8
RIGHT JOIN table_3.field_88.value a9 ON (a3.pos) >= (a3.pos)
WHERE
(a9.pos) = (a3.pos)
{code}

It's strange that when the Where clause (WHERE (a9.pos) = (a3.pos)) is removed, the following error is returned.
{code}
ERROR: NotImplementedException: Error generating a valid execution plan for this query. A RIGHT OUTER JOIN type with no equi-join predicates can only be executed with a single node plan.
{code}

The equivalent query in postgres returns 14299657:
{code}
SELECT COUNT(*)
FROM table_3_field_87 a1
INNER JOIN  table_1_field_18 a2 ON (a2.key) = (a1.value)
INNER JOIN LATERAL (
SELECT a3.*
FROM table_1_field_18__values a3
WHERE
(a2.id) = (a3.table_1_field_18_id)
) a3 ON True
LEFT JOIN LATERAL (
SELECT
(MAX(a7.idx)) + (711) AS int_col
FROM table_1_field_18__values a4
INNER JOIN LATERAL (
SELECT a5.*
FROM table_1_field_18__values__values a5
WHERE
(a4.id) = (a5.table_1_field_18__values_id)
) a5 ON True
INNER JOIN LATERAL (
SELECT a6.*
FROM table_1_field_18__values__values a6
WHERE
(a4.id) = (a6.table_1_field_18__values_id)
) a6 ON True
INNER JOIN  (
SELECT a7.*
FROM table_1_field_18__values tmp_alias_1
INNER JOIN  table_1_field_18__values__values a7 ON (tmp_alias_1.id) = (a7.table_1_field_18__values_id)
WHERE
(a2.id) = (tmp_alias_1.table_1_field_18_id)
) a7 ON ((a7.idx) = (a4.idx)) AND ((a7.idx) = (a4.idx))
WHERE
((a2.id) = (a4.table_1_field_18_id)) AND ((822.8632536034) < (a6.idx))
) a8 ON True
RIGHT JOIN  table_3_field_88__values a9 ON (a3.idx) >= (a3.idx)
WHERE
(a9.idx) = (a3.idx)
{code}

To reproduce:
{code}
ssh dev@vd0206.halxg.cloudera.com -p 22222 (pw: cloudera)
Impala db name: random_nested_db_0
Postgres db name: random_nested_db_flat_0

{code}"	IMPALA	Resolved	1	1	1880	correctness, nested_types, query_generator
13052708	Predicate from the ON clause of an inner join is dropped during planning	"Impala returns 546 rows and Postgres returns 0 rows for the following query.

Query:
{code}
SELECT
t2.timestamp_col,
t1.int_col_1
FROM (
SELECT
COALESCE(t1.smallint_col, t1.month, t1.month) AS int_col,
(COUNT(t1.int_col)) <= (COALESCE(t1.smallint_col, t1.month, t1.month)) AS boolean_col,
(t1.bigint_col) + (t1.smallint_col) AS int_col_1
FROM alltypes t1
GROUP BY
COALESCE(t1.smallint_col, t1.month, t1.month),
(t1.bigint_col) + (t1.smallint_col)
HAVING
((t1.bigint_col) + (t1.smallint_col)) != (COUNT((t1.bigint_col) + (t1.smallint_col)))
) t1
INNER JOIN alltypes t2 ON (((t2.month) = (t1.int_col)) AND ((t2.month) = (t1.int_col_1))) AND ((t2.tinyint_col) = (t1.int_col))
WHERE
(t2.int_col) IN (t1.int_col_1, t1.int_col)
{code}

As we can see from the plan below, the predicate t2.month = t1.int_col_1 is dropped from the ON clause of the inner join:
{noformat}
+---------------------------------------------------------------------------------------------------------------------------+
| Explain String                                                                                                            |
+---------------------------------------------------------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=170.00MB VCores=2                                                                 |
|                                                                                                                           |
| 07:EXCHANGE [UNPARTITIONED]                                                                                               |
| |                                                                                                                         |
| 03:HASH JOIN [INNER JOIN, BROADCAST]                                                                                      |
| |  hash predicates: (t2.month) = coalesce(t1.smallint_col, t1.month, t1.month)                                            |
| |  other predicates: (t2.int_col) IN ((t1.bigint_col) + (t1.smallint_col), coalesce(t1.smallint_col, t1.month, t1.month)) |
| |                                                                                                                         |
| |--06:EXCHANGE [BROADCAST]                                                                                                |
| |  |                                                                                                                      |
| |  05:AGGREGATE [FINALIZE]                                                                                                |
| |  |  output: (count:merge(t1.int_col)), (count:merge((t1.bigint_col) + (t1.smallint_col)))                               |
| |  |  group by: coalesce(t1.smallint_col, t1.month, t1.month), (t1.bigint_col) + (t1.smallint_col)                        |
| |  |  having: (t1.bigint_col) + (t1.smallint_col) != (count((t1.bigint_col) + (t1.smallint_col)))                         |
| |  |                                                                                                                      |
| |  04:EXCHANGE [HASH(coalesce(t1.smallint_col, t1.month, t1.month),(t1.bigint_col) + (t1.smallint_col))]                  |
| |  |                                                                                                                      |
| |  01:AGGREGATE                                                                                                           |
| |  |  output: (count(t1.int_col)), (count((t1.bigint_col) + (t1.smallint_col)))                                           |
| |  |  group by: coalesce(t1.smallint_col, t1.month, t1.month), (t1.bigint_col) + (t1.smallint_col)                        |
| |  |                                                                                                                      |
| |  00:SCAN HDFS [functional.alltypes t1]                                                                                  |
| |     partitions=24/24 files=24 size=478.45KB                                                                             |
| |                                                                                                                         |
| 02:SCAN HDFS [functional.alltypes t2]                                                                                     |
|    partitions=24/24 files=24 size=478.45KB                                                                                |
|    predicates: t2.month = t2.tinyint_col                                                                                  |
+---------------------------------------------------------------------------------------------------------------------------+
{noformat}"	IMPALA	Resolved	1	1	1880	correctness, planner, query_generator
13052791	Improve the calculation of the TupleDescriptor::avgSerializedSize_	"Whenever we serialize a row batch, even a row batch with 0 materialized slots, we always allocate an array of tuple_offsets per tuple. That means that there is a serialization overhead of 4B per tuple (per row).

Currently we do not consider this overhead when we calculate the TupleDescriptor::avgSerializedSize_ and consequently the avgRowSize_ which is used for example when we decide which input to broadcast/distribute.

We should take into account this overhead. Such a change may affect plans of queries with small avgRowSize_ or multiple tuples (joins)."	IMPALA	Resolved	3	4	1880	performance, planner
13063006	TestUdf test_native_functions failing	"TestUdf test_native_functions failed because it says the test table already exists:

{code}
13:05:45  TestUdfExecution.test_native_functions[exec_option: {'disable_codegen': True, 'exec_single_node_rows_threshold': 0, 'enable_expr_rewrites': True} | table_format: text/none] 
13:05:45 [gw0] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/bin/../infra/python/env/bin/python
13:05:45 query_test/test_udfs.py:283: in test_native_functions
13:05:45     use_db=unique_database)
13:05:45 common/impala_test_suite.py:324: in run_test_case
13:05:45     result = self.__execute_query(target_impalad_client, query, user=user)
13:05:45 common/impala_test_suite.py:532: in __execute_query
13:05:45     return impalad_client.execute(query, user=user)
13:05:45 common/impala_connection.py:160: in execute
13:05:45     return self.__beeswax_client.execute(sql_stmt, user=user)
13:05:45 beeswax/impala_beeswax.py:173: in execute
13:05:45     handle = self.__execute_query(query_string.strip(), user=user)
13:05:45 beeswax/impala_beeswax.py:337: in __execute_query
13:05:45     handle = self.execute_query_async(query_string, user=user)
13:05:45 beeswax/impala_beeswax.py:333: in execute_query_async
13:05:45     return self.__do_rpc(lambda: self.imp_service.query(query,))
13:05:45 beeswax/impala_beeswax.py:458: in __do_rpc
13:05:45     raise ImpalaBeeswaxException(self.__build_error_message(b), b)
13:05:45 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
13:05:45 E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
13:05:45 E    MESSAGE: AnalysisException: Table already exists: test_native_functions_dd844b0d.udfinserttest
{code}

Smells like a metadata issue, similar to the very classic issue IMPALA-3641. Needs investigation."	IMPALA	Resolved	1	1	1880	broken-build
13050819	Outer join on constant expressions returns incorrect results.	"First, the following query fails on a Preconditions check:

select * from (select 1 as a from alltypes limit 4) x left outer join (select 2 as a from alltypes limit 1) y on x.a = y.a;

The Preconditions check is easily fixed by swapping the order in which predicate-children tid-bindings are checked (Planner.java lines 902 following).

After the above fix the query runs but returns incorrect results, as follows.

+---+---+
| a | a |
+---+---+
| 1 | 2 |
| 1 | 2 |
| 1 | 2 |
| 1 | 2 |
+---+---+

An inner join correctly returns 0 results.

Note that this query also suffers from IMPALA-183."	IMPALA	Resolved	1	1	1880	correctness
13052707	Incorrect result (Analytic Functions)	"The query should clearly return 0 rows, however 2 rows are returned.

Query:
{code}
SELECT
LEAD(-496, 81) OVER (ORDER BY t1.double_col DESC, t1.id ASC) AS int_col
FROM alltypestiny t1
WHERE
5 = 6
UNION
SELECT
794.67 AS decimal_col
FROM alltypes t1
WHERE
5 = 6
{code}

Result:
{code}
+---------+
| int_col |
+---------+
| NULL    |
| 794.67  |
+---------+
{code}

Plan:
{code}
+--------------------------------------------------------------+
| Explain String                                               |
+--------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=170.00MB VCores=2    |
|                                                              |
| 10:EXCHANGE [UNPARTITIONED]                                  |
| |                                                            |
| 09:AGGREGATE [FINALIZE]                                      |
| |  group by: int_col                                         |
| |                                                            |
| 08:EXCHANGE [HASH(int_col)]                                  |
| |                                                            |
| 05:AGGREGATE                                                 |
| |  group by: int_col                                         |
| |                                                            |
| 00:UNION                                                     |
| |                                                            |
| |--04:SCAN HDFS [functional.alltypes t1]                     |
| |     partitions=24/24 files=24 size=478.45KB                |
| |                                                            |
| 07:EXCHANGE [RANDOM]                                         |
| |                                                            |
| 03:ANALYTIC                                                  |
| |  functions: lead(-496, 81, NULL)                           |
| |  order by: double_col DESC, id ASC                         |
| |  window: ROWS BETWEEN UNBOUNDED PRECEDING AND 81 FOLLOWING |
| |                                                            |
| 06:MERGING-EXCHANGE [UNPARTITIONED]                          |
| |  order by: double_col DESC, id ASC                         |
| |                                                            |
| 02:SORT                                                      |
| |  order by: double_col DESC, id ASC                         |
| |                                                            |
| 01:SCAN HDFS [functional.alltypestiny t1]                    |
|    partitions=4/4 files=4 size=460B                          |
+--------------------------------------------------------------+
{code}"	IMPALA	Resolved	1	1	1880	correctness, query_generator
13052834	"TestLastDdlTimeUpdate fails with ""Illegal reference to unmaterialized slot"""	"See http://sandbox.jenkins.cloudera.com/job/impala-master-code-coverage-cdh5/16/testReport/junit/metadata.test_last_ddl_time_update/TestLastDdlTimeUpdate/test_insert_exec_option____disable_codegen___True___abort_on_error___1___exec_single_node_rows_threshold___0___batch_size___0___num_nodes___0____table_format__text_none_/


{noformat}
metadata.test_last_ddl_time_update.TestLastDdlTimeUpdate.test_insert[exec_option: {'disable_codegen': True, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none] (from pytest)

Failing for the past 1 build (Since Unstable#16 )
Took 11 sec.
add description
Error Message

ImpalaBeeswaxException: ImpalaBeeswaxException:  INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>  MESSAGE: IllegalStateException: Illegal reference to non-materialized slot: tid=0 sid=0
Stacktrace

metadata/test_last_ddl_time_update.py:86: in test_insert
    ""where t.i < 10"" % FULL_NAME, False)
metadata/test_last_ddl_time_update.py:108: in run_test
    result = self.execute_query(query)
common/impala_test_suite.py:322: in wrapper
    return function(*args, **kwargs)
common/impala_test_suite.py:347: in execute_query
    return self.__execute_query(self.client, query, query_options)
common/impala_test_suite.py:414: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:158: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:161: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:325: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:321: in execute_query_async
    return self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:446: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: IllegalStateException: Illegal reference to non-materialized slot: tid=0 sid=0
Standard Error

-- executing against localhost:21000
drop table if exists metastore_update.ddltime_test;

-- executing against localhost:21000
drop database if exists metastore_update;

-- executing against localhost:21000
create database if not exists metastore_update LOCATION '/test-warehouse/metastore_update';

-- executing against localhost:21000
create external table if not exists metastore_update.ddltime_test (i int) partitioned by (j int, s string);

-- executing against localhost:21000
insert into metastore_update.ddltime_test partition(j=1, s='2012') select 10;

-- executing against localhost:21000
insert into metastore_update.ddltime_test partition(j, s) select 10, 2, '2013';

-- executing against localhost:21000
insert into metastore_update.ddltime_test partition(j, s) select * from (select 10 as i, 2 as j, '2013' as s) as t where t.i < 10;
{noformat}"	IMPALA	Resolved	1	1	1880	broken-build
13053587	Groovy script keeps hitting null pointer exception	"{code}
Groovy script failed:
java.lang.NullPointerException: Cannot invoke method child() on null object
	at org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:77)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:45)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:32)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at Script1.run(Script1.groovy:1)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:580)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:618)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:589)
	at org.jvnet.hudson.plugins.groovypostbuild.GroovyPostbuildRecorder.perform(GroovyPostbuildRecorder.java:312)
	at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20)
	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:785)
	at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:757)
	at hudson.model.Build$BuildExecution.post2(Build.java:183)
	at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:706)
	at hudson.model.Run.execute(Run.java:1690)
	at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)
	at hudson.model.ResourceController.execute(ResourceController.java:88)
	at hudson.model.Executor.run(Executor.java:246)
{code}

A bunch of GVMs are hitting this:
http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge/1791/
http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge/1800/
http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge/1801/
http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge/1802/
http://sandbox.jenkins.cloudera.com/job/impala-external-gerrit-verify-merge/1803/"	IMPALA	Resolved	1	1	1880	broken-build
13074590	Off-by-one error in testTableSample	"Number of partitions scanned is 12, but expected to be 13. This was in a build with legacy aggs and joins enabled, but it's not obvious if those are related.

{code}
FAILED:  org.apache.impala.planner.PlannerTest.testTableSample

Error Message:

Section PLAN of query:
select * from functional.alltypes tablesample system(50) repeatable(1234)

Actual does not match expected result:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=13/24 files=13 size=258.44KB
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   table stats: 7300 rows total
   column stats: all
   mem-estimate=96.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=3650

Expected:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=12/24 files=12 size=240.27KB
   table stats: 7300 rows total
   column stats: all
   mem-estimate=80.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=3650

Verbose plan:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
  PLAN-ROOT SINK
  |  mem-estimate=0B mem-reservation=0B
  |
  00:SCAN HDFS [functional.alltypes]
     partitions=13/24 files=13 size=258.44KB
     table stats: 7300 rows total
     column stats: all
     mem-estimate=96.00MB mem-reservation=0B
     tuple-ids=0 row-size=97B cardinality=3650

Section PLAN of query:
select * from functional.alltypes tablesample system(50) repeatable(1234)
where id < 10

Actual does not match expected result:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=13/24 files=13 size=258.44KB
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   predicates: id < 10
   table stats: 7300 rows total
   column stats: all
   parquet dictionary predicates: id < 10
   mem-estimate=96.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=365

Expected:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=12/24 files=12 size=239.26KB
   predicates: id < 10
   table stats: 7300 rows total
   column stats: all
   parquet dictionary predicates: id < 10
   mem-estimate=80.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=365

Verbose plan:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
  PLAN-ROOT SINK
  |  mem-estimate=0B mem-reservation=0B
  |
  00:SCAN HDFS [functional.alltypes]
     partitions=13/24 files=13 size=258.44KB
     predicates: id < 10
     table stats: 7300 rows total
     column stats: all
     parquet dictionary predicates: id < 10
     mem-estimate=96.00MB mem-reservation=0B
     tuple-ids=0 row-size=97B cardinality=365


Stack Trace:
java.lang.AssertionError:
Section PLAN of query:
select * from functional.alltypes tablesample system(50) repeatable(1234)

Actual does not match expected result:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=13/24 files=13 size=258.44KB
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   table stats: 7300 rows total
   column stats: all
   mem-estimate=96.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=3650

Expected:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=12/24 files=12 size=240.27KB
   table stats: 7300 rows total
   column stats: all
   mem-estimate=80.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=3650

Verbose plan:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
  PLAN-ROOT SINK
  |  mem-estimate=0B mem-reservation=0B
  |
  00:SCAN HDFS [functional.alltypes]
     partitions=13/24 files=13 size=258.44KB
     table stats: 7300 rows total
     column stats: all
     mem-estimate=96.00MB mem-reservation=0B
     tuple-ids=0 row-size=97B cardinality=3650

Section PLAN of query:
select * from functional.alltypes tablesample system(50) repeatable(1234)
where id < 10

Actual does not match expected result:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=13/24 files=13 size=258.44KB
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   predicates: id < 10
   table stats: 7300 rows total
   column stats: all
   parquet dictionary predicates: id < 10
   mem-estimate=96.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=365

Expected:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
PLAN-ROOT SINK
|  mem-estimate=0B mem-reservation=0B
|
00:SCAN HDFS [functional.alltypes]
   partitions=12/24 files=12 size=239.26KB
   predicates: id < 10
   table stats: 7300 rows total
   column stats: all
   parquet dictionary predicates: id < 10
   mem-estimate=80.00MB mem-reservation=0B
   tuple-ids=0 row-size=97B cardinality=365

Verbose plan:
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
  PLAN-ROOT SINK
  |  mem-estimate=0B mem-reservation=0B
  |
  00:SCAN HDFS [functional.alltypes]
     partitions=13/24 files=13 size=258.44KB
     predicates: id < 10
     table stats: 7300 rows total
     column stats: all
     parquet dictionary predicates: id < 10
     mem-estimate=96.00MB mem-reservation=0B
     tuple-ids=0 row-size=97B cardinality=365

        at org.junit.Assert.fail(Assert.java:88)
        at org.apache.impala.planner.PlannerTestBase.runPlannerTestFile(PlannerTestBase.java:759)
        at org.apache.impala.planner.PlannerTestBase.runPlannerTestFile(PlannerTestBase.java:721)
        at org.apache.impala.planner.PlannerTestBase.runPlannerTestFile(PlannerTestBase.java:716)
        at org.apache.impala.planner.PlannerTest.testTableSample(PlannerTest.java:414)
{code}"	IMPALA	Resolved	2	1	1880	broken-build
13125250	Wrong results for complex query with CTE, limit, group by and left join	"Impala may generate an incorrect plan for complex query.
(see NULL in id and a_id columns)
Can get correct result with commented lines (1, 2, 3, 4, 5)
Example query and incorrect plan:

{code}
with test as (
  select id, b as b from(
   select 1 as id , 10 as b union all
   select 2 as id , 20 as b union all
   select 3 as id , 30 as b union all
   select 4 as id , 40 as b union all
   select 5 as id , 50 as b
  ) t 
  group by id, b --1
  limit 3 --2
), 
test2 as (
  select 1 as id, 10 as a_id union all
  select 2, 10 union all
  select 3, 20 union all
  select 4, 20 union all
  select 5, 30 union all
  select 6, 40
)
select * from test 
left --3
join
(select id , a_id
   from (select id, a_id
           from test2
          where id in (select id from test) --4
          group by id, a_id) t
  group by id, a_id --5
) e on test.id = e.id;
{code}


Result:
{code}
+----+----+------+------+
| id | b  | id   | a_id |
+----+----+------+------+
| 2  | 20 | NULL | NULL |
| 3  | 30 | 3    | 20   |
| 5  | 50 | 5    | 30   |
+----+----+------+------+
{code}


Plan:
{code}
+--------------------------------------------------+
| Explain String                                   |
+--------------------------------------------------+
| Max Per-Host Resource Reservation: Memory=9.69MB |
| Per-Host Resource Estimates: Memory=41.94MB      |
| Codegen disabled by planner                      |
|                                                  |
| PLAN-ROOT SINK                                   |
| |                                                |
| 08:HASH JOIN [LEFT OUTER JOIN, BROADCAST]        |
| |  hash predicates: id = id                      |
| |                                                |
| |--10:EXCHANGE [UNPARTITIONED]                   |
| |  |                                             |
| |  07:AGGREGATE [FINALIZE]                       |
| |  |  group by: id, a_id                         |
| |  |                                             |
| |  06:AGGREGATE [FINALIZE]                       |
| |  |  group by: id, a_id                         |
| |  |                                             |
| |  05:HASH JOIN [LEFT SEMI JOIN, BROADCAST]      |
| |  |  hash predicates: id = id                   |
| |  |                                             |
| |  |--09:EXCHANGE [UNPARTITIONED]                |
| |  |  |                                          |
| |  |  04:AGGREGATE [FINALIZE]                    |
| |  |  |  group by: id, b                         |
| |  |  |  limit: 3                                |
| |  |  |                                          |
| |  |  03:UNION                                   |
| |  |     constant-operands=5                     |
| |  |                                             |
| |  02:UNION                                      |
| |     constant-operands=6                        |
| |                                                |
| 01:AGGREGATE [FINALIZE]                          |
| |  group by: id, b                               |
| |  limit: 3                                      |
| |                                                |
| 00:UNION                                         |
|    constant-operands=5                           |
+--------------------------------------------------+
{code}


Correct result:
{code}
with test as (
  select id, b as b from(
   select 1 as id , 10 as b union all
   select 2 as id , 20 as b union all
   select 3 as id , 30 as b union all
   select 4 as id , 40 as b union all
   select 5 as id , 50 as b
  ) t 
  --group by id, b --1
  limit 3 --2
), 
test2 as (
  select 1 as id, 10 as a_id union all
  select 2, 10 union all
  select 3, 20 union all
  select 4, 20 union all
  select 5, 30 union all
  select 6, 40
)
select * from test left join
(select id , a_id
   from (select id, a_id
           from test2
          where id in (select id from test) --3
          group by id, a_id) t
  group by id, a_id --4
) e on test.id = e.id;
{code}

Result:
{code:java}
+----+----+----+------+
| id | b  | id | a_id |
+----+----+----+------+
| 1  | 10 | 1  | 10   |
| 2  | 20 | 2  | 10   |
| 3  | 30 | 3  | 20   |
+----+----+----+------+
{code}

{code}
+--------------------------------------------------+
| Explain String                                   |
+--------------------------------------------------+
| Max Per-Host Resource Reservation: Memory=5.81MB |
| Per-Host Resource Estimates: Memory=21.94MB      |
| Codegen disabled by planner                      |
|                                                  |
| PLAN-ROOT SINK                                   |
| |                                                |
| 06:HASH JOIN [LEFT OUTER JOIN, BROADCAST]        |
| |  hash predicates: id = id                      |
| |                                                |
| |--08:EXCHANGE [UNPARTITIONED]                   |
| |  |                                             |
| |  05:AGGREGATE [FINALIZE]                       |
| |  |  group by: id, a_id                         |
| |  |                                             |
| |  04:AGGREGATE [FINALIZE]                       |
| |  |  group by: id, a_id                         |
| |  |                                             |
| |  03:HASH JOIN [LEFT SEMI JOIN, BROADCAST]      |
| |  |  hash predicates: id = id                   |
| |  |                                             |
| |  |--07:EXCHANGE [UNPARTITIONED]                |
| |  |  |                                          |
| |  |  02:UNION                                   |
| |  |     constant-operands=5                     |
| |  |     limit: 3                                |
| |  |                                             |
| |  01:UNION                                      |
| |     constant-operands=6                        |
| |                                                |
| 00:UNION                                         |
|    constant-operands=5                           |
|    limit: 3                                      |
+--------------------------------------------------+
{code}
"	IMPALA	Resolved	1	1	1880	correctness
13053891	No artifacts collected after a failed Jenkins job build	The following job failed (http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-core-non-partitioned-joins-and-aggs/278/) but there were no artifacts collected. In particular, it would be useful to have the data loading logs since this is where the build failed.	IMPALA	Resolved	4	1	1880	broken-build
13055089	Predicate hits NumberFormatException due to constant folding.	"Impala folds constants in a few specific places. For example, for predicates assigned to Kudu scan nodes, or for predicates used as partition filters. This constant folding may result in queries not being executable if the evaluation result is NaN or infinity (and possibly other special values).

The underlying problem is that our NumeritLiteral has no good way of representing NaN or infinity.

Repro:
{code}
explain select * from functional.alltypes
where year < (cast(1.1000000238418579 as double) / cast(0 as double));
ERROR: NumberFormatException: Infinite or NaN
{code}

Stack:
{code}
I1110 13:04:23.117324 16414 jni-util.cc:169] java.lang.NumberFormatException: Infinite or NaN
	at java.math.BigDecimal.<init>(BigDecimal.java:808)
	at org.apache.impala.analysis.LiteralExpr.create(LiteralExpr.java:202)
	at org.apache.impala.analysis.Expr.foldConstantChildren(Expr.java:1219)
	at org.apache.impala.planner.HdfsPartitionPruner.canEvalUsingPartitionMd(HdfsPartitionPruner.java:182)
	at org.apache.impala.planner.HdfsPartitionPruner.prunePartitions(HdfsPartitionPruner.java:123)
	at org.apache.impala.planner.SingleNodePlanner.createHdfsScanPlan(SingleNodePlanner.java:1199)
	at org.apache.impala.planner.SingleNodePlanner.createScanNode(SingleNodePlanner.java:1259)
	at org.apache.impala.planner.SingleNodePlanner.createTableRefNode(SingleNodePlanner.java:1482)
	at org.apache.impala.planner.SingleNodePlanner.createTableRefsPlan(SingleNodePlanner.java:757)
	at org.apache.impala.planner.SingleNodePlanner.createSelectPlan(SingleNodePlanner.java:597)
	at org.apache.impala.planner.SingleNodePlanner.createQueryPlan(SingleNodePlanner.java:248)
	at org.apache.impala.planner.SingleNodePlanner.createSingleNodePlan(SingleNodePlanner.java:145)
	at org.apache.impala.planner.Planner.createPlan(Planner.java:91)
	at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:987)
	at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:1067)
	at org.apache.impala.service.JniFrontend.createExecRequest(JniFrontend.java:155)
{code}

The easiest solution is to consider the constant folding a failure for special evaluation results like NaN/infinity and fall back to executing the original expression.
As a result, the index-based partition pruning may not kick in or predicates may not be pushed to Kudu. These cases seem exotic enough for this behavior to be acceptable in most cases.

Alternatively, we could work on proper representation of NaN and infinity in our NumericLiteral."	IMPALA	Resolved	3	1	1880	usability
13051928	IllegalStateException while planning scalar subquery with complex correlated predicate	"The planner throws an IllegalStateException for the following scalar subquery:

{code}

select 1 from alltypestiny t where (select sum(t1.id) from alltypesagg t1 inner join alltypes t2 on t1.id = t2.id where t1.id + t2.id = t.int_col) = t.int_col;


I0924 17:12:10.272449 23963 Planner.java:184] finalize plan fragments
I0924 17:12:10.272632 23963 jni-util.cc:177] java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
	at com.cloudera.impala.planner.PlanFragment.finalize(PlanFragment.java:157)
	at com.cloudera.impala.planner.Planner.createPlanFragments(Planner.java:186)
	at com.cloudera.impala.service.Frontend.createExecRequest(Frontend.java:835)
	at com.cloudera.impala.service.JniFrontend.createExecRequest(JniFrontend.java:137)

{code}"	IMPALA	Resolved	3	1	1880	impala
13054789	Incorrect double quoting of identifier in SQL generated by COMPUTE INCREMENTAL STATS.	"COMPUTE INCREMENTAL STATS fails to update the stats of new partitions if the name of a partition column is a reserved word. A syntax error is reported because a generated SQL statement incorrectly uses two sets of backticks for quoting the identifier.

Repro:
{code}
create table t (i int) partitioned by (`date` int);
insert into t partition(`date`) values (1, 1);
compute incremental stats t;
insert into t partition(`date`) values (2, 2);
[localhost:21000] > compute incremental stats t;
Query: compute incremental stats t
WARNINGS: AnalysisException: Syntax error in line 1:
...te` FROM default.t WHERE (``date``=2) GROUP BY `date`
                             ^
Encountered: EMPTY IDENTIFIER
Expected: CASE, CAST, EXISTS, FALSE, IF, INTERVAL, NOT, NULL, SELECT, TRUNCATE, TRUE, VALUES, WITH, IDENTIFIER

CAUSED BY: Exception: Syntax error
{code}

The problem is in HdfsPartition.getConjunctSql() where the double quoting is pretty obvious:
{code}
  public String getConjunctSql() {
    List<String> partitionCols = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
      partitionCols.add(ToSqlUtils.getIdentSql(getTable().getColumns().get(i).getName()));
    }

    List<String> conjuncts = Lists.newArrayList();
    for (int i = 0; i < partitionCols.size(); ++i) {
      LiteralExpr expr = getPartitionValues().get(i);
      String sql = expr.toSql();
      if (expr instanceof NullLiteral || sql.isEmpty()) {
        conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i))
            + "" IS NULL"");
      } else {
        conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i))
            + ""="" + sql);
      }
    }
    return ""("" + Joiner.on("" AND "" ).join(conjuncts) + "")"";
  }
{code}"	IMPALA	Resolved	1	1	1880	compute-stats, regression, usability
13077342	Union with constant exprs inside a subplan returns inconsistent results	"The follow queries returned wrong results 

{code}
select c_custkey, o_orderkey from tpch_nested_parquet.customer c,
  (select o_orderkey from c.c_orders
   union all
   values(100), (200), (300)) v
where c_custkey in (1, 2, 3)
Query submitted at: 2017-06-01 23:08:25 (Coordinator: http://anuj-OptiPlex-9020:25000)
Query progress can be monitored at: http://anuj-OptiPlex-9020:25000/query_plan?query_id=4d4822d9721bd930:bedfd0d800000000
+-----------+------------+
| c_custkey | o_orderkey |
+-----------+------------+
| 3         | 100        |
| 3         | 200        |
| 3         | 300        |
| 1         | 579908     |
| 1         | 4808192    |
| 1         | 3868359    |
| 1         | 4273923    |
| 1         | 454791     |
| 1         | 5133509    |
| 1         | 100        |
| 1         | 200        |
| 1         | 300        |
| 2         | 430243     |
| 2         | 2992930    |
| 2         | 1842406    |
| 2         | 1374019    |
| 2         | 1071617    |
| 2         | 1763205    |
| 2         | 3986496    |
+-----------+------------+
{code}
This should have also included the following rows in the output -
2, 100
2, 200
2, 300

This query works correctly sometimes and returns the 22 rows instead of 19 rows like above.
{code}
Similarly,
select c_custkey, o_orderkey from tpch_nested_parquet.customer c,
 (select o_orderkey from c.c_orders
  union all
  values(11), (22), (33),(44)) v
where c_custkey = 2
Query progress can be monitored at: http://anuj-OptiPlex-9020:25000/query_plan?query_id=f146eebc188c783d:518aa09100000000
+-----------+------------+
| c_custkey | o_orderkey |
+-----------+------------+
| 2         | 430243     |
| 2         | 2992930    |
| 2         | 1842406    |
| 2         | 1374019    |
| 2         | 1071617    |
| 2         | 1763205    |
| 2         | 3986496    |
+-----------+------------+
Fetched 7 row(s) in 0.23s
{code}

The results are inconsistent because it also returns correct values sometimes
{code}
Query: select c_custkey, o_orderkey from tpch_nested_parquet.customer c,
 (select o_orderkey from c.c_orders
  union all
  values(11), (22), (33),(44)) v
where c_custkey = 2
Query submitted at: 2017-06-05 13:33:21 (Coordinator: http://anuj-OptiPlex-9020:25000)
Query progress can be monitored at: http://anuj-OptiPlex-9020:25000/query_plan?query_id=5b4fa6fd6a294b6c:e0afbf9b00000000
+-----------+------------+
| c_custkey | o_orderkey |
+-----------+------------+
| 2         | 430243     |
| 2         | 2992930    |
| 2         | 1842406    |
| 2         | 1374019    |
| 2         | 1071617    |
| 2         | 1763205    |
| 2         | 3986496    |
| 2         | 11         |
| 2         | 22         |
| 2         | 33         |
| 2         | 44         |
+-----------+------------+
Fetched 11 row(s) in 0.23s
[localhost:21000] > 
{code}
"	IMPALA	Resolved	2	1	1880	correctness
13055191	COMPUTE STATS on Parquet tables uses MT_DOP=4 by default	"* COMPUTE STATS on Parquet tables should be run with MT_DOP=4 by default.
* COMPUTE STATS on non-Parquet tables will run without MT_DOP.
* Users can always override the behavior by setting MT_DOP manually. Setting MT_DOP to 0 means a statement will be run in the conventional execution mode (without intra-node paralellism based on multiple fragment instances)"	IMPALA	Resolved	2	7	1880	multithreading
13053031	Crash: impala::HdfsScanNode::Init	"Query:
{code}
SELECT
t1162.field_59.field_63 AS float_col
FROM table_3 t1162
LEFT JOIN (
WITH with_1 AS (SELECT
MIN(COALESCE(LEAST(392, -808), -814, 695)) OVER (ORDER BY t1166.pos DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 91 FOLLOWING) AS int_col,
FIRST_VALUE(-963) OVER (ORDER BY t1166.pos DESC) AS int_col_t1167,
t1166.pos AS int_col_t1168
FROM t1162.field_58 t1163
INNER JOIN t1163.value t1164
INNER JOIN t1163.value t1165 ON (((t1165.pos) = (t1164.pos)) AND ((t1165.pos) = (t1164.pos))) AND ((t1165.pos) = (t1164.pos))
INNER JOIN t1162.field_74 t1166 ON ((t1166.pos) = (t1165.pos)) AND ((t1166.pos) = (t1165.pos)))
SELECT
COALESCE(LAG(SUM(688), 80) OVER (PARTITION BY MIN(t1172.pos) ORDER BY MIN(t1172.pos) DESC, (t1170.pos) + (t1170.pos) DESC), -457, 49) AS int_col,
CAST(FIRST_VALUE(SUM(-265)) OVER (PARTITION BY MIN(t1172.pos) ORDER BY MIN(t1172.pos) ASC, COALESCE((t1170.pos) + (t1170.pos), MAX(COALESCE(t1170.pos, t1171.pos, t1171.pos)), (t1170.pos) + (t1170.pos)) ASC ROWS BETWEEN 64 FOLLOWING AND UNBOUNDED FOLLOWING) AS STRING) AS char_col,
MIN(t1172.pos) AS int_col_t1173,
(t1170.pos) + (t1170.pos) AS int_col_t1174,
COALESCE((t1170.pos) + (t1170.pos), MAX(COALESCE(t1170.pos, t1171.pos, t1171.pos)), (t1170.pos) + (t1170.pos)) AS int_col_t1175
FROM t1162.field_58 t1169
INNER JOIN t1169.value t1170
INNER JOIN t1162.field_74 t1171 ON ((t1171.pos) = (t1170.pos)) AND ((t1171.pos) = (t1170.pos))
LEFT JOIN t1169.value t1172 ON (t1172.pos) = (t1170.pos)
GROUP BY
(t1170.pos) + (t1170.pos)
HAVING
(CAST('2029-04-30 00:00:00' AS TIMESTAMP)) < (MAX(CAST('2004-12-18 00:00:00' AS TIMESTAMP)))
) t1176
INNER JOIN t1162.field_74 t1177 ON (t1177.pos) = (t1162.field_71)
INNER JOIN table_4 t1178 ON (((t1178.field_75) = (t1162.field_71)) AND ((t1178.field_75) = (t1162.field_73))) AND ((t1178.field_75) = (t1162.field_59.field_66))
{code}

Stack Trace:
{code}
#0  0x00007f51fa0c7cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f51fa0cb0d8 in __GI_abort () at abort.c:89
#2  0x0000000002118579 in google::DumpStackTraceAndExit () at src/utilities.cc:147
#3  0x000000000211162d in google::LogMessage::Fail () at src/logging.cc:1315
#4  0x00000000021134b5 in google::LogMessage::SendToLog (this=0x7f5146a90750) at src/logging.cc:1269
#5  0x0000000002111183 in google::LogMessage::Flush (this=this@entry=0x7f5146a90750) at src/logging.cc:1138
#6  0x0000000002113e0e in google::LogMessageFatal::~LogMessageFatal (this=0x7f5146a90750, __in_chrg=<optimized out>) at src/logging.cc:1836
#7  0x00000000015eed17 in impala::HdfsScanNode::Init (this=0x8351000, tnode=...) at /home/dev/Impala/be/src/exec/hdfs-scan-node.cc:132
#8  0x00000000015d2d2f in impala::ExecNode::CreateTreeHelper (pool=0x93a15e0, tnodes=..., descs=..., parent=0xc49bb80, node_idx=0x7f5146a9097c, root=0x0) at /home/dev/Impala/be/src/exec/exec-node.cc:258
#9  0x00000000015d2c3d in impala::ExecNode::CreateTreeHelper (pool=0x93a15e0, tnodes=..., descs=..., parent=0xcc0ac00, node_idx=0x7f5146a9097c, root=0x0) at /home/dev/Impala/be/src/exec/exec-node.cc:249
#10 0x00000000015d2c3d in impala::ExecNode::CreateTreeHelper (pool=0x93a15e0, tnodes=..., descs=..., parent=0x0, node_idx=0x7f5146a9097c, root=0xd218f30) at /home/dev/Impala/be/src/exec/exec-node.cc:249
#11 0x00000000015d28c2 in impala::ExecNode::CreateTree (pool=0x93a15e0, plan=..., descs=..., root=0xd218f30) at /home/dev/Impala/be/src/exec/exec-node.cc:214
#12 0x00000000015962d4 in impala::PlanFragmentExecutor::Prepare (this=0xd218f28, request=...) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:199
#13 0x000000000137d9e5 in impala::FragmentMgr::FragmentExecState::Prepare (this=0xd218d00, exec_params=...) at /home/dev/Impala/be/src/service/fragment-exec-state.cc:44
#14 0x0000000001376165 in impala::FragmentMgr::ExecPlanFragment (this=0xcb28de0, exec_params=...) at /home/dev/Impala/be/src/service/fragment-mgr.cc:51
#15 0x00000000012ef7a9 in impala::ImpalaInternalService::ExecPlanFragment (this=0xcb2b350, return_val=..., params=...) at /home/dev/Impala/be/src/service/impala-internal-service.h:37
#16 0x00000000014c6367 in impala::ImpalaInternalServiceProcessor::process_ExecPlanFragment (this=0xcb28d80, seqid=0, iprot=0xcc57c80, oprot=0xcc57c40, callContext=0xd2aa2c0) at /home/dev/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:949
#17 0x00000000014c6078 in impala::ImpalaInternalServiceProcessor::dispatchCall (this=0xcb28d80, iprot=0xcc57c80, oprot=0xcc57c40, fname=..., seqid=0, callContext=0xd2aa2c0) at /home/dev/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:922
#18 0x00000000012e631d in apache::thrift::TDispatchProcessor::process (this=0xcb28d80, in=..., out=..., connectionContext=0xd2aa2c0) at /home/dev/Impala/thirdparty/thrift-0.9.0/build/include/thrift/TDispatchProcessor.h:121
#19 0x00000000020ba553 in apache::thrift::server::TThreadedServer::Task::run (this=0xccf58c0) at src/thrift/server/TThreadedServer.cpp:70
#20 0x00000000012114c1 in impala::ThriftThread::RunRunnable (this=0xd2aa200, runnable=..., promise=0x7f519d33e790) at /home/dev/Impala/be/src/rpc/thrift-thread.cc:61
#21 0x0000000001212c89 in boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0xdcaa840, p=0xd2aa200, a1=..., a2=0x7f519d33e790) at /usr/include/boost/bind/mem_fn_template.hpp:280
#22 0x0000000001212ae4 in boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0xdcaa850, f=..., a=...) at /usr/include/boost/bind/bind.hpp:392
#23 0x0000000001212875 in boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0xdcaa840) at /usr/include/boost/bind/bind_template.hpp:20
#24 0x0000000001212796 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
#25 0x0000000001247b28 in boost::function0<void>::operator() (this=0x7f5146a91e00) at /usr/include/boost/function/function_template.hpp:767
#26 0x0000000001457d5b in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7f519d33e5d0) at /home/dev/Impala/be/src/util/thread.cc:314
#27 0x0000000001460f91 in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0xd2ac9c0, f=@0xd2ac9b8: 0x1457a52 <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>, a=...) at /usr/include/boost/bind/bind.hpp:457
#28 0x0000000001460edb in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0xd2ac9b8) at /usr/include/boost/bind/bind_template.hpp:20
#29 0x0000000001460ea0 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0xd2ac800) at /usr/include/boost/thread/detail/thread.hpp:117
#30 0x00007f51fd091a4a in ?? () from /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0
#31 0x00007f51fc53e182 in start_thread (arg=0x7f5146a92700) at pthread_create.c:312
#32 0x00007f51fa18b47d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{code}

impalad.FATAL:
{code}
F0924 13:16:48.641844 15662 hdfs-scan-node.cc:132] Check failed: conjuncts_map_[tuple_id_].empty()
{code}"	IMPALA	Resolved	1	1	1880	crash, nested_types, query_generator
13053783	Incorrect assignment of WHERE clause predicate through a grouping aggregation + outer join.	"Thanks to sobrien05 from the user list for reporting this!
Original post:
http://community.cloudera.com/t5/Interactive-Short-cycle-SQL/Predict-push-down-bug/m-p/38516

Repro tables and data:
{code}
create table orders(order_id int, address_id int);
insert into table orders values((1, 10), (2, 20), (3, 30));
create table addresses(address_id int, state string);
insert into table addresses values((10, ""CA""), (20, ""NY""), (30, ""CA""));
{code}

Repro query and bad plan:
{code}
Select *
FROM (
    Select
      shipping_addr.state,
      count(order_id) as num_orders
    FROM
    (
        Select
          order_id,
          address_id
        FROM orders
    ) order_data
    LEFT OUTER JOIN [shuffle]
    (
        Select address_id,
              state
        FROM addresses
    ) shipping_addr
    ON order_data.address_id = shipping_addr.address_id
    GROUP BY
      shipping_addr.state
) unf
WHERE
   lower(state) = lower('NY')

+------------------------------------------------------------------------------------+
| Explain String                                                                     |
+------------------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=2.16GB VCores=2                            |
| WARNING: The following tables are missing relevant table and/or column statistics. |
| debug.addresses, debug.orders                                                      |
|                                                                                    |
| 08:EXCHANGE [UNPARTITIONED]                                                        |
| |                                                                                  |
| 07:AGGREGATE [FINALIZE]                                                            |
| |  output: count:merge(order_id)                                                   |
| |  group by: shipping_addr.state                                                   |
| |                                                                                  |
| 06:EXCHANGE [HASH(shipping_addr.state)]                                            |
| |                                                                                  |
| 03:AGGREGATE                                                                       |
| |  output: count(order_id)                                                         |
| |  group by: state                                                                 |
| |                                                                                  |
| 02:HASH JOIN [LEFT OUTER JOIN, PARTITIONED]                                        |
| |  hash predicates: address_id = address_id                                        |
| |                                                                                  |
| |--05:EXCHANGE [HASH(address_id)]                                                  |
| |  |                                                                               |
| |  01:SCAN HDFS [debug.addresses]                                                  |
| |     partitions=1/1 files=1 size=18B                                              |
| |     predicates: lower(debug.addresses.state) = lower('NY')       <-- should also be assigned at node 02                |
| |                                                                                  |
| 04:EXCHANGE [HASH(address_id)]                                                     |
| |                                                                                  |
| 00:SCAN HDFS [debug.orders]                                                        |
|    partitions=1/1 files=1 size=15B                                                 |
+------------------------------------------------------------------------------------+
{code}

*Workaround*
Use an INNER JOIN instead of a LEFT OUTER JOIN. The query still has the same meaning."	IMPALA	Resolved	2	1	1880	correctness, downgraded
13053419	Flaky test: TestUnmatchedSchema.test_unmatched_schema	"The following test failed due to a missing column that had clearly been added a few tests sections before:

{code}
03:17:20.804 
03:17:20.829 =================================== FAILURES ===================================
03:17:20.830  TestUnmatchedSchema.test_unmatched_schema[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none] 
03:17:20.830 [gw4] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-master-cdh5-trunk/repos/Impala/bin/../infra/python/env/bin/python
03:17:20.831 query_test/test_scanners.py:117: in test_unmatched_schema
03:17:20.831     self.run_test_case('QueryTest/test-unmatched-schema', vector)
03:17:20.831 common/impala_test_suite.py:216: in run_test_case
03:17:20.831     result = self.__execute_query(target_impalad_client, query, user=user)
03:17:20.831 common/impala_test_suite.py:406: in __execute_query
03:17:20.831     return impalad_client.execute(query, user=user)
03:17:20.831 common/impala_connection.py:158: in execute
03:17:20.831     return self.__beeswax_client.execute(sql_stmt, user=user)
03:17:20.831 beeswax/impala_beeswax.py:162: in execute
03:17:20.831     handle = self.__execute_query(query_string.strip(), user=user)
03:17:20.831 beeswax/impala_beeswax.py:326: in __execute_query
03:17:20.831     handle = self.execute_query_async(query_string, user=user)
03:17:20.831 beeswax/impala_beeswax.py:322: in execute_query_async
03:17:20.831     return self.__do_rpc(lambda: self.imp_service.query(query,))
03:17:20.831 beeswax/impala_beeswax.py:447: in __do_rpc
03:17:20.831     raise ImpalaBeeswaxException(self.__build_error_message(b), b)
03:17:20.831 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
03:17:20.831 E    INNER EXCEPTION: <class 'impala._thrift_gen.beeswax.ttypes.BeeswaxException'>
03:17:20.831 E    MESSAGE: AnalysisException: Column 'new_int_col' does not exist in table: functional.jointbl_test
03:17:20.831 ---------------------------- Captured stderr setup -----------------------------
03:17:20.832 -- connecting to: localhost:21000
03:17:20.832 ----------------------------- Captured stderr call -----------------------------
03:17:20.833 -- executing against localhost:21000
03:17:20.833 use functional;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 drop table if exists jointbl_test;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 use functional;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 create external table jointbl_test like jointbl;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 use functional;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 describe formatted jointbl;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 use functional;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 alter table jointbl_test set location 'hdfs://localhost:20500/test-warehouse/jointbl';
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 use functional;
03:17:20.833 
03:17:20.833 SET disable_codegen=False;
03:17:20.833 SET abort_on_error=1;
03:17:20.833 SET exec_single_node_rows_threshold=0;
03:17:20.833 SET batch_size=0;
03:17:20.833 SET num_nodes=0;
03:17:20.833 -- executing against localhost:21000
03:17:20.833 select * from jointbl_test;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 alter table jointbl_test add columns(new_col string);
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 select * from jointbl_test;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 alter table jointbl_test add columns(new_int_col int);
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 select * from jointbl_test;
03:17:20.833 
03:17:20.833 -- executing against localhost:21000
03:17:20.833 alter table jointbl_test drop column new_int_col;
03:17:20.833 
03:17:20.833  generated xml file: /data/jenkins/workspace/impala-master-cdh5-trunk/repos/Impala/tests/results/TEST-impala-parallel.xml 
03:17:20.833 =========================== short test summary info ============================
{code}

Example run:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Nightly-Builds/job/impala-master-cdh5-trunk/1556/"	IMPALA	Resolved	2	3	1880	broken-build, test-infra
13054562	"Queries started failing with ""This file  has no row groups"" against small/invalid parquet files"	"This is blocking nightly performance runs. 

On August 2nd queries against tpch_nested_300_parquet started failing with 
{code}
Invalid file. This file: hdfs://vb0202.halxg.cloudera.com:8020/user/hive/warehouse/tpch_nested_300_parquet.db/customer_snappy/000371_0 has no row groups
Invalid file. This file: hdfs://vb0202.halxg.cloudera.com:8020/user/hive/warehouse/tpch_nested_300_parquet.db/customer_snappy/000466_0 has no row groups
{code}

These files appear to have invalid data given their size 
{code}
-rw-r--r--   3 mmokhtar hive        828 2016-02-23 12:48 /user/hive/warehouse/tpch_nested_300_parquet.db/customer_snappy/000371_0
-rw-r--r--   3 mmokhtar hive        828 2016-02-23 12:49 /user/hive/warehouse/tpch_nested_300_parquet.db/customer_snappy/000466_0
{code}

Queries against the same dataset use to succeed before.

This is very likely a behavioral change introduced by http://github.mtv.cloudera.com/CDH/Impala/commit/40c01a7f92d2248229e8e45291a1ef43b8c40f48"	IMPALA	Resolved	3	1	1880	regression
13053510	TestQueries.test_empty fails with legacy partitioned/joins aggs.	"See:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Nightly-Builds/job/impala-master-cdh5-trunk-non-partitioned-hash-and-aggs/209/

{code}
01:52:11 =================================== FAILURES ===================================
01:52:11  TestQueries.test_empty[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none] 
01:52:11 [gw5] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-master-cdh5-trunk-non-partitioned-hash-and-aggs/repos/Impala/bin/../infra/python/env/bin/python
01:52:11 query_test/test_queries.py:118: in test_empty
01:52:11     self.run_test_case('QueryTest/empty', vector)
01:52:11 common/impala_test_suite.py:255: in run_test_case
01:52:11     result = self.__execute_query(target_impalad_client, query, user=user)
01:52:11 common/impala_test_suite.py:448: in __execute_query
01:52:11     return impalad_client.execute(query, user=user)
01:52:11 common/impala_connection.py:161: in execute
01:52:11     return self.__beeswax_client.execute(sql_stmt, user=user)
01:52:11 beeswax/impala_beeswax.py:163: in execute
01:52:11     handle = self.__execute_query(query_string.strip(), user=user)
01:52:11 beeswax/impala_beeswax.py:327: in __execute_query
01:52:11     handle = self.execute_query_async(query_string, user=user)
01:52:11 beeswax/impala_beeswax.py:323: in execute_query_async
01:52:11     return self.__do_rpc(lambda: self.imp_service.query(query,))
01:52:11 beeswax/impala_beeswax.py:448: in __do_rpc
01:52:11     raise ImpalaBeeswaxException(self.__build_error_message(b), b)
01:52:11 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
01:52:11 E    INNER EXCEPTION: <class 'impala._thrift_gen.beeswax.ttypes.BeeswaxException'>
01:52:11 E    MESSAGE: 
01:52:11 E   Query referencing nested types is not supported because the --enable_partitioned_hash_join and/or --enable_partitioned_aggregation Impala Daemon start-up flags are set to false.
01:52:11 E   To enable nested types support please set those flags to true (they are enabled by default).
01:52:11 ----------------------------- Captured stderr call -----------------------------
{code}"	IMPALA	Resolved	1	1	1880	broken-build
13052915	Crash: impala::ExprContext::GetValue (Nested Types)	"DB: tpch_nested_parquet

This query causes a crash every time it is executed.

Query:
{code}
SELECT
LAG(800) OVER (ORDER BY t1.p_name, t2.c_nationkey DESC) AS int_col
FROM part t1
RIGHT JOIN customer t2 ON (((t2.c_custkey) = (t1.p_size)) AND ((t2.c_custkey) = (t1.p_partkey))) AND ((t2.c_phone) = (t1.p_container))
, t2.c_orders t3
{code}

Stack Trace:
{code}
#0  0x00007f787e190cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56	../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
#0  0x00007f787e190cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f787e1940d8 in __GI_abort () at abort.c:89
#2  0x0000000002102b79 in google::DumpStackTraceAndExit () at src/utilities.cc:147
#3  0x00000000020fbc2d in google::LogMessage::Fail () at src/logging.cc:1315
#4  0x00000000020fdab5 in google::LogMessage::SendToLog (this=0x7f77c932ba40) at src/logging.cc:1269
#5  0x00000000020fb783 in google::LogMessage::Flush (this=this@entry=0x7f77c932ba40) at src/logging.cc:1138
#6  0x00000000020fe40e in google::LogMessageFatal::~LogMessageFatal (this=0x7f77c932ba40, __in_chrg=<optimized out>) at src/logging.cc:1836
#7  0x0000000000fc5655 in impala::ExprContext::GetValue (this=0xdfb18c0, e=0xdfa8f00, row=0xe6b4000) at /home/dev/Impala/be/src/exprs/expr-context.cc:295
#8  0x0000000000fc50b5 in impala::ExprContext::GetValue (this=0xdfb18c0, row=0xe6b4000) at /home/dev/Impala/be/src/exprs/expr-context.cc:203
#9  0x000000000159b2ad in impala::Tuple::MaterializeExprs<true> (this=0x12be2000, row=0xe6b4000, desc=..., materialize_expr_ctxs=..., pool=0x0, non_null_var_len_values=0x7f77c932bc60, total_var_len=0x7f77c932bbd4) at /home/dev/Impala/be/src/runtime/tuple.cc:186
#10 0x000000000171b5b9 in impala::Sorter::Run::AddBatch<true> (this=0x8c68a80, batch=0x7f77c932be50, start_index=0, num_processed=0x7f77c932bda8) at /home/dev/Impala/be/src/runtime/sorter.cc:414
#11 0x0000000001716b3b in impala::Sorter::AddBatch (this=0xf6f8c80, batch=0x7f77c932be50) at /home/dev/Impala/be/src/runtime/sorter.cc:963
#12 0x00000000016bbb75 in impala::SortNode::SortInput (this=0xdfac800, state=0xd7c4d00) at /home/dev/Impala/be/src/exec/sort-node.cc:156
#13 0x00000000016bb021 in impala::SortNode::Open (this=0xdfac800, state=0xd7c4d00) at /home/dev/Impala/be/src/exec/sort-node.cc:76
#14 0x0000000001589193 in impala::PlanFragmentExecutor::OpenInternal (this=0xd5df828) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:334
#15 0x0000000001588f98 in impala::PlanFragmentExecutor::Open (this=0xd5df828) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:320
#16 0x000000000136fdc4 in impala::FragmentMgr::FragmentExecState::Exec (this=0xd5df600) at /home/dev/Impala/be/src/service/fragment-exec-state.cc:50
#17 0x0000000001368903 in impala::FragmentMgr::FragmentExecThread (this=0xdf46f00, exec_state=0xd5df600) at /home/dev/Impala/be/src/service/fragment-mgr.cc:70
#18 0x000000000136c10e in boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>::operator() (this=0x10532c20, p=0xdf46f00, a1=0xd5df600) at /usr/include/boost/bind/mem_fn_template.hpp:165
#19 0x000000000136bed1 in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list0> (this=0x10532c30, f=..., a=...) at /usr/include/boost/bind/bind.hpp:313
#20 0x000000000136b7eb in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >::operator() (this=0x10532c20) at /usr/include/boost/bind/bind_template.hpp:20
#21 0x000000000136b140 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
#22 0x000000000123b478 in boost::function0<void>::operator() (this=0x7f77c932ce00) at /usr/include/boost/function/function_template.hpp:767
#23 0x0000000001448a77 in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7f77c9bacea0) at /home/dev/Impala/be/src/util/thread.cc:314
#24 0x0000000001451cad in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0xdfac7c0, f=@0xdfac7b8: 0x144876e <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>, a=...) at /usr/include/boost/bind/bind.hpp:457
#25 0x0000000001451bf7 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0xdfac7b8) at /usr/include/boost/bind/bind_template.hpp:20
#26 0x0000000001451bbc in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0xdfac600) at /usr/include/boost/thread/detail/thread.hpp:117
#27 0x00007f7881159a4a in ?? () from /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0
#28 0x00007f7880606182 in start_thread (arg=0x7f77c932d700) at pthread_create.c:312
#29 0x00007f787e25447d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{code}

impalad.FATAL:
{code}
Check failed: false Type not implemented: ARRAY
{code}"	IMPALA	Resolved	1	1	1880	crash, nested_types, query_generator
13053302	Crash: impala::BufferedTupleStream::rows_returned	"Query:
{code}
use random_nested_db_0;
SELECT
COALESCE(LEAD(COALESCE(-948, -836), 43) OVER (ORDER BY COALESCE(t9.int_col_t4, t10.field_10.field_13, t11.field_16) DESC), LAG(202, 59) OVER (ORDER BY COALESCE(t9.int_col_t4, t10.field_10.field_13, t11.field_16)), COALESCE(479, NULL, 651)) AS int_col,
COALESCE(t9.int_col_t4, t10.field_10.field_13, t11.field_16) AS int_col_t12
FROM (
SELECT DISTINCT
COALESCE(t1.pos, t2.value, t1.pos) AS int_col,
COALESCE(t2.value, t1.pos, t1.pos) AS int_col_t4
FROM table_3.field_88.value t1
LEFT JOIN table_3.field_90.field_92.value t2 ON ((t2.value) = (t1.pos)) AND ((t2.value) = (t1.pos))
LEFT JOIN t1.item t3
UNION ALL
SELECT
(673.207169409) / (t5.pos) AS float_col,
ABS(836.211809943) AS float_col_t6
FROM table_1.field_19.value t5
UNION ALL
SELECT
LAG(((-371.351014454) / (761)) * (-604), 45) OVER (ORDER BY t8.pos DESC, t7.pos) AS float_col,
COALESCE(-940, -382, COALESCE(LEAST(COALESCE(LAG(821, 77) OVER (ORDER BY t8.item ASC, t8.pos DESC), 497), -903), 986)) AS int_col
FROM table_3.field_88.value t7
LEFT JOIN table_1.field_19.value.item.field_21.item t8 ON ((t8.pos) = (t7.pos)) AND ((t8.pos) = (t7.pos))
WHERE
(t8.pos) = (t7.pos)
) t9
LEFT JOIN table_0 t10 ON ((t10.field_10.field_13) = (t9.int_col_t4)) AND ((t10.field_5) = (t9.int_col))
INNER JOIN table_0 t11 ON ((t11.field_16) = (t9.int_col_t4)) AND ((t11.field_5) = (t9.int_col_t4))
{code}

Stack Trace:
{code}
#0  0x00007fc539d95cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007fc539d990d8 in __GI_abort () at abort.c:89
#2  0x00007fc53b9a89c5 in os::abort(bool) () from /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
#3  0x00007fc53bb29607 in VMError::report_and_die() () from /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
#4  0x00007fc53b9ad8af in JVM_handle_linux_signal () from /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  0x00000000016cf380 in impala::BufferedTupleStream::rows_returned (this=0x0) at /home/dev/Impala/be/src/runtime/buffered-tuple-stream.h:234
#7  0x00000000016fc1e2 in impala::AnalyticEvalNode::GetNext (this=0xe63a700, state=0xcb7cd00, row_batch=0x7fc480e9bd10, eos=0x7fc480e9bcff) at /home/dev/Impala/be/src/exec/analytic-eval-node.cc:752
#8  0x00000000016e0e73 in impala::SortNode::SortInput (this=0xf04bc00, state=0xcb7cd00) at /home/dev/Impala/be/src/exec/sort-node.cc:155
#9  0x00000000016e034c in impala::SortNode::Open (this=0xf04bc00, state=0xcb7cd00) at /home/dev/Impala/be/src/exec/sort-node.cc:76
#10 0x00000000016f8740 in impala::AnalyticEvalNode::Open (this=0xe63a380, state=0xcb7cd00) at /home/dev/Impala/be/src/exec/analytic-eval-node.cc:180
#11 0x000000000159d3a8 in impala::PlanFragmentExecutor::OpenInternal (this=0xcb7bd28) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:334
#12 0x000000000159d1a8 in impala::PlanFragmentExecutor::Open (this=0xcb7bd28) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:320
#13 0x00000000013814fc in impala::FragmentMgr::FragmentExecState::Exec (this=0xcb7bb00) at /home/dev/Impala/be/src/service/fragment-exec-state.cc:50
#14 0x000000000137a03b in impala::FragmentMgr::FragmentExecThread (this=0xda61020, exec_state=0xcb7bb00) at /home/dev/Impala/be/src/service/fragment-mgr.cc:83
#15 0x000000000137d846 in boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>::operator() (this=0xe6322c0, p=0xda61020, a1=0xcb7bb00) at /usr/include/boost/bind/mem_fn_template.hpp:165
#16 0x000000000137d609 in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list0> (this=0xe6322d0, f=..., a=...) at /usr/include/boost/bind/bind.hpp:313
#17 0x000000000137cf23 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >::operator() (this=0xe6322c0) at /usr/include/boost/bind/bind_template.hpp:20
#18 0x000000000137c878 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
#19 0x000000000124a744 in boost::function0<void>::operator() (this=0x7fc480e9ce00) at /usr/include/boost/function/function_template.hpp:767
#20 0x000000000145ba8d in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7fc485f25dd0) at /home/dev/Impala/be/src/util/thread.cc:314
#21 0x0000000001464cc3 in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0xf04b3c0, f=@0xf04b3b8: 0x145b784 <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>, a=...) at /usr/include/boost/bind/bind.hpp:457
#22 0x0000000001464c0d in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0xf04b3b8) at /usr/include/boost/bind/bind_template.hpp:20
#23 0x0000000001464bd2 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0xf04b200) at /usr/include/boost/thread/detail/thread.hpp:117
#24 0x00007fc53cd5fa4a in ?? () from /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0
#25 0x00007fc53c20c182 in start_thread (arg=0x7fc480e9d700) at pthread_create.c:312
#26 0x00007fc539e5947d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{code}

To access the machine with loaded data:
{code}
ssh vd0206.halxg.cloudera.com
ssh dev@192.168.123.64   # pw = cloudera
cd Impala  # proceed as usual
{code}"	IMPALA	Resolved	2	1	1880	crash, query_generator
13132086	Compute stats tablesample spends a lot of time in powf()	"[~mmokhtar] did perf profiling for COMPUTE STATS TABLESAMPLE and discovered that a lot of time is spent on finalizing HLL intermediates. Most time is spent in powf().

Relevant snippet from AggregateFunctions::HllFinalEstimate() in aggregate-functions-ir.cc:
{code}
  for (int i = 0; i < num_buckets; ++i) {
    harmonic_mean += powf(2.0f, -buckets[i]);
    if (buckets[i] == 0) ++num_zero_registers;
  }
{code}

Since we're doing a power of 2 using ldexp() should be much more efficient.

I did a microbenchmark and found that ldexp() is >10x faster than powf() for this scenario.
"	IMPALA	Resolved	3	4	1880	compute-stats, performance
13143105	Preconditions.checkState(val.getColValsSize() == 1); in EvalExprWithoutRow()	"Hit this when running an exhaustive build at this [commit|https://github.com/apache/impala/commit/5f2f445e7d29ed26f6818b5c41edda2fe7c49b59]. KRPC was disabled.

{noformat}
query_test/test_queries.py:111: in test_subquery
    self.run_test_case('QueryTest/subquery', vector)
common/impala_test_suite.py:397: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:612: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:160: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:173: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:339: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:335: in execute_query_async
    return self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:460: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: IllegalStateException: null
Standard Error
-- executing against localhost:21000
use functional_rc_def;

SET disable_codegen_rows_threshold=0;
SET disable_codegen=False;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=100;
SET batch_size=0;
SET num_nodes=0;
-- executing against localhost:21000
select a.id, a.int_col, a.string_col
from functional.alltypessmall a
where a.id in (select id from functional.alltypestiny where bool_col = false)
and a.id < 5;
{noformat}

{noformat}
I0306 10:59:28.357015 15877 Frontend.java:952] Analyzing query: select a.id, a.int_col, a.string_col
from functional.alltypessmall a
where a.id in (select id from functional.alltypestiny where bool_col = false)
and a.id < 5
I0306 10:59:28.358779 15877 Frontend.java:964] Analysis finished.
I0306 10:59:28.603314 27013 data-stream-mgr.cc:236] Reduced stream ID cache from 1891 items, to 1887, eviction took: 0
I0306 10:59:29.028396 15877 jni-util.cc:230] java.lang.IllegalStateException
        at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
        at org.apache.impala.service.FeSupport.EvalExprWithoutRow(FeSupport.java:169)
        at org.apache.impala.service.FeSupport.EvalPredicate(FeSupport.java:218)
        at org.apache.impala.analysis.Analyzer.isTrueWithNullSlots(Analyzer.java:1917)
        at org.apache.impala.planner.HdfsScanNode.addDictionaryFilter(HdfsScanNode.java:659)
        at org.apache.impala.planner.HdfsScanNode.computeDictionaryFilterConjuncts(HdfsScanNode.java:685)
        at org.apache.impala.planner.HdfsScanNode.init(HdfsScanNode.java:329)
        at org.apache.impala.planner.SingleNodePlanner.createHdfsScanPlan(SingleNodePlanner.java:1255)
        at org.apache.impala.planner.SingleNodePlanner.createScanNode(SingleNodePlanner.java:1298)
        at org.apache.impala.planner.SingleNodePlanner.createTableRefNode(SingleNodePlanner.java:1506)
        at org.apache.impala.planner.SingleNodePlanner.createTableRefsPlan(SingleNodePlanner.java:776)
        at org.apache.impala.planner.SingleNodePlanner.createSelectPlan(SingleNodePlanner.java:614)
        at org.apache.impala.planner.SingleNodePlanner.createQueryPlan(SingleNodePlanner.java:257)
        at org.apache.impala.planner.SingleNodePlanner.createSingleNodePlan(SingleNodePlanner.java:147)
        at org.apache.impala.planner.Planner.createPlan(Planner.java:101)
        at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:896)
        at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:1017)
        at org.apache.impala.service.JniFrontend.createExecRequest(JniFrontend.java:156)
I0306 10:59:30.051102 15877 status.cc:125] IllegalStateException: null
    @          0x1676fad  impala::Status::Status()
    @          0x1ad6032  impala::JniUtil::GetJniExceptionMsg()
    @          0x196ebe3  impala::JniUtil::CallJniMethod<>()
    @          0x196b451  impala::Frontend::GetExecRequest()
    @          0x198e5a6  impala::ImpalaServer::ExecuteInternal()
    @          0x198e0ee  impala::ImpalaServer::Execute()
    @          0x1a1833c  impala::ImpalaServer::query()
    @          0x2b2169e  beeswax::BeeswaxServiceProcessor::process_query()
    @          0x2b213ec  beeswax::BeeswaxServiceProcessor::dispatchCall()
    @          0x2b093db  impala::ImpalaServiceProcessor::dispatchCall()
    @          0x161bb70  apache::thrift::TDispatchProcessor::process()
    @          0x17f9347  apache::thrift::server::TAcceptQueueServer::Task::run()
    @          0x17f0fab  impala::ThriftThread::RunRunnable()
    @          0x17f26af  boost::_mfi::mf2<>::operator()()
    @          0x17f2545  boost::_bi::list3<>::operator()<>()
    @          0x17f2291  boost::_bi::bind_t<>::operator()()
    @          0x17f21a4  boost::detail::function::void_function_obj_invoker0<>::invoke()
    @          0x183639c  boost::function0<>::operator()()
    @          0x1b42041  impala::Thread::SuperviseThread()
    @          0x1b4a517  boost::_bi::list5<>::operator()<>()
    @          0x1b4a43b  boost::_bi::bind_t<>::operator()()
    @          0x1b4a3fe  boost::detail::thread_data<>::run()
    @          0x2dcfd5a  thread_proxy
    @       0x3ceb607851  (unknown)
    @       0x3ceb2e894d  (unknown)
I0306 10:59:30.523000 15877 impala-server.cc:1006] UnregisterQuery(): query_id=354679e4021b51f5:fe427b2100000000
I0306 10:59:30.523033 15877 impala-server.cc:1093] Cancel(): query_id=354679e4021b51f5:fe427b2100000000
{noformat}

The precondition check fired appears to be the following line:

{noformat}
  public static TColumnValue EvalExprWithoutRow(Expr expr, TQueryCtx queryCtx)
      throws InternalException {
  ......
      Preconditions.checkState(val.getColValsSize() == 1); <<<------
 .......
  }
{noformat}"	IMPALA	Resolved	1	1	1880	broken-build
13053029	Crash: impala::AnalyticEvalNode::Reset	"{code}
SELECT
(FIRST_VALUE(LEAST(977, 480)) OVER (PARTITION BY COALESCE(t28.pos, t28.pos, t28.pos) ORDER BY COALESCE(t28.pos, t28.pos, t28.pos) DESC, (MAX(t34.int_col)) + (MIN(t27.field_55)) DESC)) / (179.417146203) AS float_col,
(MAX(t34.int_col)) + (MIN(t27.field_55)) AS int_col,
COALESCE(t28.pos, t28.pos, t28.pos) AS int_col_t35,
(t28.pos) * (t28.pos) AS int_col_t36
FROM table_2 t27
INNER JOIN t27.field_46 t28 ON (((t28.pos) = (t27.field_55)) AND ((t28.pos) = (t27.field_55))) AND ((t28.pos) = (t27.field_55))
INNER JOIN (
SELECT
COALESCE(LAST_VALUE(608) OVER (ORDER BY GREATEST(COALESCE(t30.value.field_48, 520), COALESCE(t31.pos, 295)) DESC, CAST(t31.pos AS STRING)), LAG(2, 63) OVER (ORDER BY GREATEST(COALESCE(t30.value.field_48, 520), COALESCE(t31.pos, 295)) DESC, CAST(t31.pos AS STRING)), 933) AS int_col,
GREATEST(COALESCE(t30.value.field_48, 520), COALESCE(t31.pos, 295)) AS int_col_t32,
CAST(t31.pos AS STRING) AS char_col,
COALESCE(129, 30, LAG(464) OVER (ORDER BY GREATEST(COALESCE(t30.value.field_48, 520), COALESCE(t31.pos, 295)), CAST(t31.pos AS STRING))) AS int_col_t33
FROM t27.field_45 t29
INNER JOIN t28.item t30 ON (((t30.value.field_51) = (t29.pos)) AND ((t30.value.field_48) = (t29.pos))) AND ((t30.value.field_51) = (t29.pos))
LEFT JOIN t29.item t31
) t34 ON (t34.int_col_t33) = (t28.pos)
WHERE
(t34.int_col) NOT IN (t34.int_col_t32, t28.pos)
GROUP BY
COALESCE(t28.pos, t28.pos, t28.pos),
(t28.pos) * (t28.pos)
HAVING
((t28.pos) * (t28.pos)) NOT IN (MAX(t27.field_55), COALESCE(t28.pos, t28.pos, t28.pos))
{code}

Stack Trace:
{code}
#0  0x00007f6b46cd0cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f6b46cd4218 in __GI_abort () at abort.c:118
#2  0x00007f6b46cc9b86 in __assert_fail_base (fmt=0x7f6b46e1a830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x22b5bec ""px != 0"", file=file@entry=0x22b5bc0 ""/usr/include/boost/smart_ptr/scoped_ptr.hpp"", line=line@entry=99, function=function@entry=0x22b60a0 <boost::scoped_ptr<impala::BufferedTupleStream>::operator->() const::__PRETTY_FUNCTION__> ""T* boost::scoped_ptr<T>::operator->() const [with T = impala::BufferedTupleStream]"") at assert.c:92
#3  0x00007f6b46cc9c32 in __GI___assert_fail (assertion=0x22b5bec ""px != 0"", file=0x22b5bc0 ""/usr/include/boost/smart_ptr/scoped_ptr.hpp"", line=99, function=0x22b60a0 <boost::scoped_ptr<impala::BufferedTupleStream>::operator->() const::__PRETTY_FUNCTION__> ""T* boost::scoped_ptr<T>::operator->() const [with T = impala::BufferedTupleStream]"") at assert.c:101
#4  0x00000000016a68f3 in boost::scoped_ptr<impala::BufferedTupleStream>::operator-> (this=0x10af8a10) at /usr/include/boost/smart_ptr/scoped_ptr.hpp:99
#5  0x00000000016ea4e0 in impala::AnalyticEvalNode::Reset (this=0x10af8700, state=0x9592900) at /home/dev/Impala/be/src/exec/analytic-eval-node.cc:778
#6  0x00000000015d221a in impala::ExecNode::Reset (this=0xe4f3680, state=0x9592900) at /home/dev/Impala/be/src/exec/exec-node.cc:166
#7  0x0000000001690c28 in impala::NestedLoopJoinNode::Reset (this=0xe4f3680, state=0x9592900) at /home/dev/Impala/be/src/exec/nested-loop-join-node.cc:97
#8  0x00000000015d221a in impala::ExecNode::Reset (this=0x105ee420, state=0x9592900) at /home/dev/Impala/be/src/exec/exec-node.cc:166
#9  0x00000000016d067d in impala::SubplanNode::Reset (this=0x105ee420, state=0x9592900) at /home/dev/Impala/be/src/exec/subplan-node.cc:136
#10 0x00000000016cffc8 in impala::SubplanNode::GetNext (this=0x105ee6e0, state=0x9592900, row_batch=0x7f6a8dd8ef40, eos=0x7f6a8dd8ee8e) at /home/dev/Impala/be/src/exec/subplan-node.cc:81
#11 0x000000000169abf6 in impala::PartitionedAggregationNode::Open (this=0xd190600, state=0x9592900) at /home/dev/Impala/be/src/exec/partitioned-aggregation-node.cc:240
#12 0x0000000001598729 in impala::PlanFragmentExecutor::OpenInternal (this=0xca37828) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:334
#13 0x000000000159852e in impala::PlanFragmentExecutor::Open (this=0xca37828) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:320
#14 0x000000000137da8c in impala::FragmentMgr::FragmentExecState::Exec (this=0xca37600) at /home/dev/Impala/be/src/service/fragment-exec-state.cc:50
#15 0x00000000013765cb in impala::FragmentMgr::FragmentExecThread (this=0xcef6d20, exec_state=0xca37600) at /home/dev/Impala/be/src/service/fragment-mgr.cc:70
#16 0x0000000001379dd6 in boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>::operator() (this=0x113bec40, p=0xcef6d20, a1=0xca37600) at /usr/include/boost/bind/mem_fn_template.hpp:165
#17 0x0000000001379b99 in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list0> (this=0x113bec50, f=..., a=...) at /usr/include/boost/bind/bind.hpp:313
#18 0x00000000013794b3 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >::operator() (this=0x113bec40) at /usr/include/boost/bind/bind_template.hpp:20
#19 0x0000000001378e08 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
#20 0x0000000001247b28 in boost::function0<void>::operator() (this=0x7f6a8dd8fe00) at /usr/include/boost/function/function_template.hpp:767
#21 0x0000000001457d5b in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7f6a92e98ea0) at /home/dev/Impala/be/src/util/thread.cc:314
#22 0x0000000001460f91 in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0x1097cbc0, f=@0x1097cbb8: 0x1457a52 <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>, a=...) at /usr/include/boost/bind/bind.hpp:457
#23 0x0000000001460edb in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0x1097cbb8) at /usr/include/boost/bind/bind_template.hpp:20
#24 0x0000000001460ea0 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0x1097ca00) at /usr/include/boost/thread/detail/thread.hpp:117
#25 0x00007f6b49c9aa4a in ?? () from /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0
#26 0x00007f6b49147182 in start_thread (arg=0x7f6a8dd90700) at pthread_create.c:312
#27 0x00007f6b46d9447d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{code}"	IMPALA	Resolved	1	1	1880	crash, nested_types, query_generator
13081870	Query involving nested array and limit 0 hits IllegalStateException	"Queries involving nested collections (array and map) and limit 0 hit a Preconditions check.

{code}
explain select * from tpch_nested_parquet.customer c, c.c_orders limit 0;
ERROR: IllegalStateException: null
{code}

Stack:
{code}
I0622 11:26:44.929339 23672 jni-util.cc:176] java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
	at org.apache.impala.analysis.TupleDescriptor.recomputeMemLayout(TupleDescriptor.java:239)
	at org.apache.impala.planner.SingleNodePlanner.unmarkCollectionSlots(SingleNodePlanner.java:242)
	at org.apache.impala.planner.SingleNodePlanner.createEmptyNode(SingleNodePlanner.java:217)
	at org.apache.impala.planner.SingleNodePlanner.createQueryPlan(SingleNodePlanner.java:252)
	at org.apache.impala.planner.SingleNodePlanner.createSingleNodePlan(SingleNodePlanner.java:149)
	at org.apache.impala.planner.Planner.createPlan(Planner.java:90)
	at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:1008)
	at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:1104)
	at org.apache.impala.service.JniFrontend.createExecRequest(JniFrontend.java:156)
{code}

*Workaround*
Use LIMIT 1 instead of LIMIT 0."	IMPALA	Resolved	1	1	1880	regression
13051974	Reuse of a column in JOIN predicate may lead to incorrect results	"In the query below, the ""WHERE t1.int_col = tt1.year"" condition is dropped. This is very similar to other issues but Alex thinks fixing this case may be simpler.

{noformat}
Query: explain 
select straight_join 1
FROM alltypestiny t1
WHERE t1.int_col IN
  (SELECT tt1.year - tt1.year  AS int_col_1
   FROM alltypesagg tt1
   WHERE t1.int_col = tt1.year)
+----------------------------------------------------------+
| Explain String                                           |
+----------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=80.01MB VCores=2 |
|                                                          |
| 05:EXCHANGE [UNPARTITIONED]                              |
| |                                                        |
| 02:HASH JOIN [LEFT SEMI JOIN, PARTITIONED]               |
| |  hash predicates: t1.int_col = tt1.year - tt1.year     |
| |                                                        |
| |--04:EXCHANGE [HASH(tt1.year - tt1.year)]               |
| |  |                                                     |
| |  01:SCAN HDFS [functional.alltypesagg tt1]             |
| |     partitions=11/11 size=814.73KB                     |
| |                                                        |
| 03:EXCHANGE [HASH(t1.int_col)]                           |
| |                                                        |
| 00:SCAN HDFS [functional.alltypestiny t1]                |
|    partitions=4/4 size=460B                              |
+----------------------------------------------------------+
Fetched 16 row(s) in 0.02s
{noformat}"	IMPALA	Resolved	1	1	1880	correctness
13052716	Builds fail due to insufficient disk space	"This job http://sandbox.jenkins.cloudera.com/view/Impala/view/Nightly-Builds/job/impala-s3/142 failed trying to git clone because the disk is full. Ishaan says this is due to something going wrong when an EC2 slave is re-used.  [~ishaan], please add whatever info you know to this jira.

{code}
02:01:39 Cloning repository git://github.mtv.cloudera.com/CDH/Impala.git
02:01:39  > git init /data/jenkins/workspace/impala-s3/repos/Impala
02:01:39 ERROR: Error cloning remote repo 'origin' : Could not init /data/jenkins/workspace/impala-s3/repos/Impala
02:01:39 hudson.plugins.git.GitException: Could not init /data/jenkins/workspace/impala-s3/repos/Impala
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$4.execute(CliGitAPIImpl.java:476)
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$2.execute(CliGitAPIImpl.java:379)
02:01:39 	at org.jenkinsci.plugins.gitclient.AbstractGitAPIImpl.clone(AbstractGitAPIImpl.java:60)
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.clone(CliGitAPIImpl.java:87)
02:01:39 	at hudson.plugins.git.GitSCM$2.invoke(GitSCM.java:1006)
02:01:39 	at hudson.plugins.git.GitSCM$2.invoke(GitSCM.java:942)
02:01:39 	at hudson.FilePath$FileCallableWrapper.call(FilePath.java:2439)
02:01:39 	at hudson.remoting.UserRequest.perform(UserRequest.java:118)
02:01:39 	at hudson.remoting.UserRequest.perform(UserRequest.java:48)
02:01:39 	at hudson.remoting.Request$2.run(Request.java:328)
02:01:39 	at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:72)
02:01:39 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
02:01:39 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
02:01:39 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
02:01:39 	at java.lang.Thread.run(Thread.java:745)
02:01:39 Caused by: hudson.plugins.git.GitException: Command ""git init /data/jenkins/workspace/impala-s3/repos/Impala"" returned status code 128:
02:01:39 stdout: 
02:01:39 stderr: error: copy-fd: write returned No space left on device
02:01:39 fatal: cannot copy '/usr/share/git-core/templates/info/exclude' to '/data/jenkins/workspace/impala-s3/repos/Impala/.git/info/exclude': No space left on device
02:01:39 
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:1325)
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:1301)
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:1297)
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommand(CliGitAPIImpl.java:1084)
02:01:39 	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$4.execute(CliGitAPIImpl.java:474)
02:01:39 	... 14 more
02:01:39 Trying next repository
02:01:39 ERROR: Could not clone repository
{code}"	IMPALA	Resolved	2	1	1880	broken-build, test-infra
13106854	Remove use of Guava Hasher to avoid class conflicts with other components	"As we've seen in IMPALA-6009 several components that Impala depends on do not shade Guava and so we have several (sometime incompatible) versions of Guava in various jars in Impala's classpath. 

A particularly tricky one is the use of Hasher which has undergone API changes (see comments in IMPALA-6009). I think we should not use Hasher since we only use it to compute an MD5 to avoid the problems with conflicting Guava versions."	IMPALA	Resolved	1	4	1880	broken-build
13081242	Improve join cardinality estimation with a more robust FK/PK detection	"This JIRA is for tracking improvements to our join-cardinality estimation. In particular, we should improve the handling of many-to-many joins and multi-column joins. It is understood that some cases cannot be reliably detected with our limited metadata and statistics, but we should try our best given those limitations.

Further improving cardinality estimation with new metadata and statistics is tracked elsewhere, e.g.:
IMPALA-2416
IMPALA-3531
"	IMPALA	Resolved	2	4	1880	performance, planner, tpc-ds
13055473	COMPUTE INCREMENTAL STATS should ignore missing stats on complex columns	"After executing ""compute incremental stats"" on a table, by design, future calls to ""compute incremental stats"" only compute stats for partitions for which there are no statistics.  However, when statistics are found to be missing for a column, e.g. a column was added to the schema since the last computation, incremental stats will be recomputed for all partitions.  Impala doesn't currently compute statistics for complex columns, such as arrays and structs.  Because of this, stats for these types of columns are always found to be missing, which incorrectly causes stats to be re-computed for all partitions on every run.  Missing stats for complex columns on previously stat-computed partitions should be ignored when determining if re-computation is necessary, as re-computing stats will never remedy this situation."	IMPALA	Resolved	2	1	1880	compute-stats, ramp-up, testability
13053155	tuple.h:140] Check failed: offset != -1	"The query below fails a check

{noformat}
use random_nested_db_1;

SELECT
(266) / (AVG(-961)) AS float_col
FROM table_2.field_72.item t1
LEFT JOIN (
SELECT
IF(True, t8.char_col, t8.char_col) AS char_col,
t2.key AS char_col_t9,
CAST(-559 AS STRING) AS char_col_t10,
LEAD(CEIL((713.811952505) / (879)), 2) OVER (ORDER BY IF(True, t8.char_col, t8.char_col), t2.key DESC) AS float_col
FROM t1.value t2
INNER JOIN (
SELECT
CAST(863 AS STRING) AS char_col,
CAST(584 AS STRING) AS char_col_t4
FROM t1.value t3
UNION ALL
SELECT
t5.key AS char_col,
LEAD(COALESCE(CAST(985 AS STRING), '-616'), 65) OVER (ORDER BY t5.key) AS char_col_t7
FROM t1.value t5
LEFT JOIN t1.value t6 ON (((t6.key) = (t5.key)) AND ((t6.key) = (t5.key))) AND ((t6.key) = (t5.key))
WHERE
(719) IN (609, 170)
) t8 ON (((t8.char_col_t4) = (t2.key)) AND ((t8.char_col_t4) = (t2.key))) AND ((t8.char_col_t4) = (t2.key))
) t11
{noformat}

I didn't try to simplify the query.

stack trace

{noformat}
google::LogMessageFatal::~LogMessageFatal (this=0x7fed32ed0fc0, __in_chrg=) at src/logging.cc:1836
impala::Tuple::GetSlot (this=0x100d6060, offset=-1) at /home/dev/Impala/be/src/runtime/tuple.h:140
impala::HdfsParquetScanner::ScalarColumnReader::ReadValue (this=0xed76500, pool=0xc242b90, tuple=0x100d6060, conjuncts_passed=0x7fed32ed10ea) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:655
impala::HdfsParquetScanner::ScalarColumnReader::ReadValue (this=0xed76500, pool=0xc242b90, tuple=0x100d6060, conjuncts_passed=0x7fed32ed10ea) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:633
impala::HdfsParquetScanner::ReadRow (this=0xecb4b40, column_readers=..., tuple=0x100d6060, pool=0xc242b90, materialize_tuple=0x7fed32ed1240) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:1679
impala::HdfsParquetScanner::AssembleRows (this=0xecb4b40, tuple_desc=0xe12d8c0, column_readers=..., new_collection_rep_level=2, row_group_idx=-1, array_value_builder=0x7fed32ed1300) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:1618
impala::HdfsParquetScanner::CollectionColumnReader::ReadSlot (this=0xdd96ea0, slot=0xdf70028, pool=0xc242b90, conjuncts_passed=0x7fed32ed13fa) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:1388
impala::HdfsParquetScanner::CollectionColumnReader::ReadValue (this=0xdd96ea0, pool=0xc242b90, tuple=0xdf70000, conjuncts_passed=0x7fed32ed13fa) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:1360
impala::HdfsParquetScanner::ReadRow (this=0xecb4b40, column_readers=..., tuple=0xdf70000, pool=0xc242b90, materialize_tuple=0x7fed32ed14c0) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:1679
impala::HdfsParquetScanner::AssembleRows (this=0xecb4b40, tuple_desc=0xdf49680, column_readers=..., new_collection_rep_level=-1, row_group_idx=0, array_value_builder=0x0) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:1618
impala::HdfsParquetScanner::ProcessSplit (this=0xecb4b40) at /home/dev/Impala/be/src/exec/hdfs-parquet-scanner.cc:1535
impala::HdfsScanNode::ScannerThread (this=0xc588a00) at /home/dev/Impala/be/src/exec/hdfs-scan-node.cc:946
boost::_mfi::mf0::operator() (this=0x7fed32ed1e08, p=0xc588a00) at /usr/include/boost/bind/mem_fn_template.hpp:49
boost::_bi::list1 >::operator(), boost::_bi::list0> (this=0x7fed32ed1e18, f=..., a=...) at /usr/include/boost/bind/bind.hpp:253
boost::_bi::bind_t, boost::_bi::list1 > >::operator() (this=0x7fed32ed1e08) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::function::void_function_obj_invoker0, boost::_bi::list1 > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
{noformat}

This is from Taras's query gen setup running on docker.

To access:

{code}
$ ssh vd0206.halxg.cloudera.com
$ ssh dev@192.168.123.64   # pw = cloudera
$ cd Impala  # proceed as usual
{code}"	IMPALA	Resolved	2	1	1880	crash, nested_types, query_generator
13055098	Evaluation of a constant boolean expr results in a NULL instead of FALSE.	"When folding constant expressions Impala may incorrectly replace a constant boolean expression with NULL instead of FALSE. This may lead to incorrect results.

Repro:
{code}
explain select * from functional_kudu.alltypes
where bool_col = (true and false);

+------------------------------------------------------------------------------------+
| Explain String                                                                     |
+------------------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=0B VCores=1                                |
| WARNING: The following tables are missing relevant table and/or column statistics. |
| functional_kudu.alltypes                                                           |
|                                                                                    |
| PLAN-ROOT SINK                                                                     |
| |                                                                                  |
| 01:EXCHANGE [UNPARTITIONED]                                                        |
| |                                                                                  |
| 00:SCAN KUDU [functional_kudu.alltypes]                                            |
|    predicates: bool_col = NULL      <--- Should be FALSE                                               |
+------------------------------------------------------------------------------------+
{code}

The underlying issue is a simple one. We are not calling the correct isSet() function on a thrift struct. See LiteralExpr.create():
{code}
...
case BOOLEAN:
  if (val.isBool_val()) result = new BoolLiteral(val.bool_val);
  break;
...
{code}

The check should be: val.isSetBool_val()"	IMPALA	Resolved	1	1	1880	correctness
13052173	Crash: impala::UnionNode::EvalAndMaterializeExprs	"QUERY:
{code}
SELECT
COALESCE(t1.int_col, t1.smallint_col, t1.month) AS int_col,
COUNT(t1.smallint_col) AS int_col_2
FROM alltypestiny t1
WHERE
(t1.int_col) IN (t1.year, t1.id)
GROUP BY
COALESCE(t1.int_col, t1.smallint_col, t1.month)
UNION
SELECT
COALESCE(206, COALESCE(364, -20, COUNT(159.14)), -958) AS int_col,
CEIL(t1.float_col) AS float_col
FROM alltypestiny t1
WHERE
(t1.int_col) NOT IN (t1.bigint_col, t1.smallint_col)
GROUP BY
CEIL(t1.float_col)
UNION ALL
SELECT
COUNT(587.43) AS int_col,
(-224) + (SUM(305.95)) AS decimal_col
FROM alltypestiny
{code}

STACK:
{code}
raise () from /lib64/libc.so.6
raise () from /lib64/libc.so.6
abort () from /lib64/libc.so.6
google::DumpStackTraceAndExit () at src/utilities.cc:147
google::LogMessage::Fail () at src/logging.cc:1296
google::LogMessage::SendToLog (this=0x7f2c698f47d0) at src/logging.cc:1250
google::LogMessage::Flush (this=0x7f2c698f47d0) at src/logging.cc:1119
google::LogMessageFatal::~LogMessageFatal (this=0x7f2c698f47d0, __in_chrg=<value optimized out>) at src/logging.cc:1817
impala::UnionNode::EvalAndMaterializeExprs (this=0x5d09340, ctxs=std::vector of length -13797749814572, capacity -13797755578041 = {...}, const_exprs=false, tuple=0x7f2c698f4900, row_batch=0x82ce680) at /data/9/query-gen/Impala/be/src/exec/union-node.cc:221
impala::UnionNode::GetNext (this=0x5d09340, state=0x9cd8e00, row_batch=0x82ce680, eos=0x91b28a9) at /data/9/query-gen/Impala/be/src/exec/union-node.cc:139
impala::PlanFragmentExecutor::GetNextInternal (this=0x91b2780, batch=0xd40bbf0) at /data/9/query-gen/Impala/be/src/runtime/plan-fragment-executor.cc:478
impala::PlanFragmentExecutor::GetNext (this=0x91b2780, batch=0xd40bbf0) at /data/9/query-gen/Impala/be/src/runtime/plan-fragment-executor.cc:456
impala::Coordinator::GetNext (this=0x9d7a600, batch=0xd40bbf0, state=0x9cd8e00) at /data/9/query-gen/Impala/be/src/runtime/coordinator.cc:826
impala::ImpalaServer::QueryExecState::FetchNextBatch (this=0xd40a000) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:845
impala::ImpalaServer::QueryExecState::FetchRowsInternal (this=0xd40a000, max_rows=1024, fetched_rows=0x738b800) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:672
impala::ImpalaServer::QueryExecState::FetchRows (this=0xd40a000, max_rows=1024, fetched_rows=0x738b800) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:582
impala::ImpalaServer::FetchInternal (this=0x503a580, query_id=..., fetch_size=1024, fetch_first=false, fetch_results=0x7f2c698f52e8) at /data/9/query-gen/Impala/be/src/service/impala-hs2-server.cc:507
impala::ImpalaServer::FetchResults (this=0x503a580, return_val=..., request=...) at /data/9/query-gen/Impala/be/src/service/impala-hs2-server.cc:1017
apache::hive::service::cli::thrift::TCLIServiceProcessor::process_FetchResults (this=0x32070e0, seqid=0, iprot=0x4f7abc0, oprot=0x55d6300, callContext=0x6a32080) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:5343
apache::hive::service::cli::thrift::TCLIServiceProcessor::dispatchCall (this=0x32070e0, iprot=0x4f7abc0, oprot=0x55d6300, fname=""FetchResults"", seqid=0, callContext=0x6a32080) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4506
impala::ImpalaHiveServer2ServiceProcessor::dispatchCall (this=0x32070e0, iprot=0x4f7abc0, oprot=0x55d6300, fname=""FetchResults"", seqid=0, callContext=0x6a32080) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/ImpalaHiveServer2Service.cpp:463
apache::thrift::TDispatchProcessor::process (this=0x32070e0, in=..., out=..., connectionContext=0x6a32080) at /data/9/query-gen/Impala/thirdparty/thrift-0.9.0/build/include/thrift/TDispatchProcessor.h:121
apache::thrift::server::TThreadPoolServer::Task::run (this=0x5c247e0) at src/thrift/server/TThreadPoolServer.cpp:70
apache::thrift::concurrency::ThreadManager::Task::run (this=0x6a33540) at src/thrift/concurrency/ThreadManager.cpp:187
apache::thrift::concurrency::ThreadManager::Worker::run (this=0x5329290) at src/thrift/concurrency/ThreadManager.cpp:316
impala::ThriftThread::RunRunnable (this=0x4da7ac0, runnable=..., promise=0x7fff985e3670) at /data/9/query-gen/Impala/be/src/rpc/thrift-thread.cc:61
boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x5e77230, p=0x4da7ac0, a1=..., a2=0x7fff985e3670) at /usr/include/boost/bind/mem_fn_template.hpp:280
boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x5e77240, f=..., a=...) at /usr/include/boost/bind/bind.hpp:392
boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x5e77230) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
boost::function0<void>::operator() (this=0x7f2c698f5cd0) at /usr/include/boost/function/function_template.hpp:1013
impala::Thread::SuperviseThread (name=""hiveserver2-frontend-9"", category=""thrift-server"", functor=..., thread_started=0x7fff985e3470) at /data/9/query-gen/Impala/be/src/util/thread.cc:311
boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::string &, const std::string &, impala::Thread::ThreadFunctor, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x5d54910, f=@0x5d54908, a=...) at /usr/include/boost/bind/bind.hpp:457
boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x5d54908) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x5d54780) at /usr/include/boost/thread/detail/thread.hpp:61
thread_proxy ()
start_thread () from /lib64/libpthread.so.0
clone () from /lib64/libc.so.6
{code}

DB: Functional
File Format: Text/None
git Hash: c295948"	IMPALA	Resolved	1	1	1880	query_generator
13052139	Crash: impala::TupleIsNullPredicate::Prepare	"QUERY:
{code}
SELECT
t2.int_col,
COALESCE(t1.int_col, t1.day, t1.tinyint_col) AS int_col_2,
COALESCE(892, -329, COALESCE(LEAD(634, 24) OVER (PARTITION BY COALESCE(t1.tinyint_col, t1.bigint_col, t1.bigint_col) ORDER BY COALESCE(t1.tinyint_col, t1.bigint_col, t1.bigint_col) DESC, t2.int_col ASC), -524)) AS int_col_3,
COALESCE(t1.tinyint_col, t1.bigint_col, t1.bigint_col) AS int_col_4
FROM alltypesagg t1
LEFT JOIN (
SELECT
MIN(timestamp_col) + INTERVAL 85 MONTH AS timestamp_col,
COALESCE(MIN(year), 867, -463) AS int_col
FROM alltypestiny
) t2 ON (t2.int_col) = (t1.id)
{code}

STACK:
{code}
raise () from /lib64/libc.so.6
raise () from /lib64/libc.so.6
abort () from /lib64/libc.so.6
google::DumpStackTraceAndExit () at src/utilities.cc:147
google::LogMessage::Fail () at src/logging.cc:1296
google::LogMessage::SendToLog (this=0x7f8508fba8c0) at src/logging.cc:1250
google::LogMessage::Flush (this=0x7f8508fba8c0) at src/logging.cc:1119
google::LogMessageFatal::~LogMessageFatal (this=0x7f8508fba8c0, __in_chrg=<value optimized out>) at src/logging.cc:1817
impala::TupleIsNullPredicate::Prepare (this=0xc044700, state=0x977d500, row_desc=..., ctx=0x9f04f20) at /data/9/query-gen/Impala/be/src/exprs/tuple-is-null-predicate.cc:49
impala::Expr::Prepare (this=0xd019040, state=0x977d500, row_desc=..., context=0x9f04f20) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:353
impala::ExprContext::Prepare (this=0x9f04f20, state=0x977d500, row_desc=..., tracker=0x87f9ce0) at /data/9/query-gen/Impala/be/src/exprs/expr-context.cc:52
impala::Expr::Prepare (ctxs=std::vector of length 4, capacity 4 = {...}, state=0x977d500, row_desc=..., tracker=0x87f9ce0) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:344
impala::Coordinator::Exec (this=0x6bac000, schedule=..., output_expr_ctxs=0x941ab60) at /data/9/query-gen/Impala/be/src/runtime/coordinator.cc:352
impala::ImpalaServer::QueryExecState::ExecQueryOrDmlRequest (this=0x941a000, query_exec_request=...) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:401
impala::ImpalaServer::QueryExecState::Exec (this=0x941a000, exec_request=0x7f8508fbc950) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:138
impala::ImpalaServer::ExecuteInternal (this=0x53cab00, query_ctx=..., session_state=..., registered_exec_state=0x7f8508fbdb67, exec_state=0x7f8508fbddf0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:607
impala::ImpalaServer::Execute (this=0x53cab00, query_ctx=0x7f8508fbdc20, session_state=..., exec_state=0x7f8508fbddf0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:550
impala::ImpalaServer::ExecuteStatement (this=0x53cab00, return_val=..., request=...) at /data/9/query-gen/Impala/be/src/service/impala-hs2-server.cc:709
apache::hive::service::cli::thrift::TCLIServiceProcessor::process_ExecuteStatement (this=0x399b0e0, seqid=0, iprot=0x13d30700, oprot=0x13d31340, callContext=0x978ed00) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4695
apache::hive::service::cli::thrift::TCLIServiceProcessor::dispatchCall (this=0x399b0e0, iprot=0x13d30700, oprot=0x13d31340, fname=""ExecuteStatement"", seqid=0, callContext=0x978ed00) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4506
impala::ImpalaHiveServer2ServiceProcessor::dispatchCall (this=0x399b0e0, iprot=0x13d30700, oprot=0x13d31340, fname=""ExecuteStatement"", seqid=0, callContext=0x978ed00) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/ImpalaHiveServer2Service.cpp:463
apache::thrift::TDispatchProcessor::process (this=0x399b0e0, in=..., out=..., connectionContext=0x978ed00) at /data/9/query-gen/Impala/thirdparty/thrift-0.9.0/build/include/thrift/TDispatchProcessor.h:121
apache::thrift::server::TThreadPoolServer::Task::run (this=0x16bb67e0) at src/thrift/server/TThreadPoolServer.cpp:70
apache::thrift::concurrency::ThreadManager::Task::run (this=0x978fec0) at src/thrift/concurrency/ThreadManager.cpp:187
apache::thrift::concurrency::ThreadManager::Worker::run (this=0x5cd0de0) at src/thrift/concurrency/ThreadManager.cpp:316
impala::ThriftThread::RunRunnable (this=0x4009b00, runnable=..., promise=0x7fff8eea8e90) at /data/9/query-gen/Impala/be/src/rpc/thrift-thread.cc:61
boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x5f7d2c0, p=0x4009b00, a1=..., a2=0x7fff8eea8e90) at /usr/include/boost/bind/mem_fn_template.hpp:280
boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x5f7d2d0, f=..., a=...) at /usr/include/boost/bind/bind.hpp:392
boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x5f7d2c0) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
boost::function0<void>::operator() (this=0x7f8508fbecd0) at /usr/include/boost/function/function_template.hpp:1013
impala::Thread::SuperviseThread (name=""hiveserver2-frontend-13"", category=""thrift-server"", functor=..., thread_started=0x7fff8eea8c90) at /data/9/query-gen/Impala/be/src/util/thread.cc:311
boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::string &, const std::string &, impala::Thread::ThreadFunctor, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x5a7e550, f=@0x5a7e548, a=...) at /usr/include/boost/bind/bind.hpp:457
boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x5a7e548) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x5a7e3c0) at /usr/include/boost/thread/detail/thread.hpp:61
thread_proxy ()
start_thread () from /lib64/libpthread.so.0
clone () from /lib64/libc.so.6
{code}

DB: Functional
File Format: Text/None"	IMPALA	Resolved	2	1	1880	query_generator
13051723	Allow creating Avro tables without column definitions. Allow COMPUTE STATS to always work on Impala-created Avro tables.	"When trying to run {{COMPUTE STATS}} on a table I created without any column definition (all columns come from the Avro schema and the partition keys), it fails with the following error message:{code}
Query: compute stats mytable
ERROR: AnalysisException: Cannot COMPUTE STATS on Avro table 'mytable' because its column definitions do not match those in the Avro schema.
Missing column definition corresponding to Avro-schema column 'thefirstcolumn' of type 'STRING' at position '0'.
Please re-create the table with column definitions, e.g., using the result of 'SHOW CREATE TABLE'{code}

I feel this is somewhat related to [IMPALA-867|https://issues.cloudera.org/browse/IMPALA-867], and I also understand the workaround proposed in the error message (the same thing is proposed in the comments of [IMPALA-867|https://issues.cloudera.org/browse/IMPALA-867]).

The documentation states:{quote}Originally, Impala relied on users to run the Hive ANALYZE TABLE statement, but that method of gathering statistics proved unreliable and difficult to use. The Impala COMPUTE STATS statement is built from the ground up to improve the reliability and user-friendliness of this operation.{quote}

To me, having to re-create the table with column definitions in the Hive metastore is not so user-friendly. Since {{COMPUTE STATS}} was built from the ground up, can it not get the columns list from the schema and partitions, rather than use the Hive metastore for that?

Otherwise, I have to keep on re-creating the table... In case I use that workaround, how do I efficiently ""transfer"" all partitions to the new table?

-- 

*Update*
* As per Impala 1.4, CREATE TABLE will find the columns from the Avro schema
* What is still required is only the update of these columns as the schema evolves (at least when ALTER TABLE is used to change the schema URL, possibly also if the file on HDFS changes?)"	IMPALA	Resolved	3	4	1880	impala
13054301	Build fails due to compilation issue	"This problem happens in the release builds.
http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-2.6.0_5.8.x/job/impala-cdh5-2.6.0_5.8.0-core/147/console

{code}
-- Toolchain build.
-- Downloading and extracting dependencies.
-- Toolchain bootstrap complete.
-- Setup toolchain link flags -Wl,-rpath,/data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/lib64 -L/data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/lib64
-- Setup toolchain link flags -Wl,-rpath,/data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/lib64 -L/data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/lib64
-- The C compiler identification is GNU
-- The CXX compiler identification is GNU
-- Check for working C compiler: /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/bin/gcc
-- Check for working C compiler: /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/bin/gcc -- broken
CMake Error at /usr/share/cmake/Modules/CMakeTestCCompiler.cmake:32 (MESSAGE):
  The C compiler
  ""/data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/bin/gcc""
  is not able to compile a simple test program.

  It fails with the following output:

   Change Dir: /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/CMakeFiles/CMakeTmp

  

  Run Build Command:/usr/bin/gmake ""cmTryCompileExec/fast""

  /usr/bin/gmake -f CMakeFiles/cmTryCompileExec.dir/build.make
  CMakeFiles/cmTryCompileExec.dir/build

  gmake[1]: Entering directory
  `/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/CMakeFiles/CMakeTmp'


  /usr/bin/cmake -E cmake_progress_report
  /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/CMakeFiles/CMakeTmp/CMakeFiles
  1

  Building C object CMakeFiles/cmTryCompileExec.dir/testCCompiler.c.o

  
  /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/bin/gcc
  -o CMakeFiles/cmTryCompileExec.dir/testCCompiler.c.o -c
  /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/CMakeFiles/CMakeTmp/testCCompiler.c


  Linking C executable cmTryCompileExec

  /usr/bin/cmake -E cmake_link_script
  CMakeFiles/cmTryCompileExec.dir/link.txt --verbose=1

  
  /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/bin/gcc
  -fPIC CMakeFiles/cmTryCompileExec.dir/testCCompiler.c.o -o cmTryCompileExec
  -rdynamic

  /usr/bin/ld: cannot find -lgcc_s

  collect2: error: ld returned 1 exit status

  gmake[1]: Leaving directory
  `/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/CMakeFiles/CMakeTmp'


  gmake[1]: *** [cmTryCompileExec] Error 1

  gmake: *** [cmTryCompileExec/fast] Error 2

  

  

  CMake will not be able to correctly generate this project.
Call Stack (most recent call first):
  CMakeLists.txt:43 (project)
{code}"	IMPALA	Resolved	1	1	1880	broken-build
13053522	Crash: impala::RowDescriptor::GetTupleIdx	"Query:
{code}
USE functional;
SELECT
COALESCE(t1.smallint_col, t1.id, t1.day) AS int_col,
COALESCE(724, COALESCE(-420, 307, LEAD(996, 13) OVER (ORDER BY COALESCE(t1.smallint_col, t1.id, t1.day) DESC, COALESCE(t12.timestamp_col, t1.timestamp_col, t12.timestamp_col) ASC)), COALESCE(-674, -427)) AS int_col_t13,
COALESCE(COUNT(t1.day), COALESCE(t1.smallint_col, t1.id, t1.day), COALESCE(t1.smallint_col, t1.id, t1.day)) AS int_col_t14,
COALESCE(SUM(COALESCE(t1.smallint_col, t1.month, LEAST(COALESCE(t1.bigint_col, 382), COALESCE(t1.month, 485)))), COALESCE(t1.smallint_col, t1.id, t1.day), COALESCE(t1.smallint_col, t1.id, t1.day)) AS int_col_t15,
COALESCE(t12.timestamp_col, t1.timestamp_col, t12.timestamp_col) AS timestamp_col
FROM alltypesagg t1
LEFT JOIN (
SELECT
CAST(-849 AS STRING) AS char_col,
t9.boolean_col AS boolean_col,
(NULL) IN (945, NULL) AS boolean_col_t10,
(-560) IN (694, 44) AS boolean_col_t11,
COALESCE(t9.timestamp_col, t9.timestamp_col) AS timestamp_col
FROM (
WITH with_1 AS (SELECT
t3.id AS int_col,
LAG(696, 7) OVER (ORDER BY t3.id DESC, GREATEST(COALESCE(SUM(COALESCE(t3.day, t3.tinyint_col, t3.smallint_col)), 971), COALESCE(t3.id, 888))) AS int_col_t4,
GREATEST(COALESCE(SUM(COALESCE(t3.day, t3.tinyint_col, t3.smallint_col)), 971), COALESCE(t3.id, 888)) AS int_col_t5,
(COALESCE((482) * (31), -413)) * (FIRST_VALUE(-115) OVER (ORDER BY t3.id DESC, GREATEST(COALESCE(SUM(COALESCE(t3.day, t3.tinyint_col, t3.smallint_col)), 971), COALESCE(t3.id, 888)) DESC ROWS BETWEEN 81 PRECEDING AND 91 FOLLOWING)) AS int_col_t6
FROM alltypestiny t2
INNER JOIN alltypesagg t3 ON (((t3.month) = (t2.tinyint_col)) AND ((t3.day) = (t2.tinyint_col))) AND ((t3.string_col) = (t2.date_string_col))
GROUP BY
t3.id)
SELECT
(t7.int_col) IN (t8.day, t7.int_col) AS boolean_col,
LAG(CAST('1999-07-27 00:00:00' AS TIMESTAMP), 34) OVER (ORDER BY (t7.int_col) IN (t8.day, t7.int_col) DESC) AS timestamp_col
FROM alltypes t7
LEFT JOIN alltypesagg t8 ON (((((t8.day) = (t7.tinyint_col)) AND ((t8.id) = (t7.bigint_col))) AND ((t8.bigint_col) = (t7.year))) AND ((t8.date_string_col) = (t7.date_string_col))) AND ((t8.date_string_col) = (t7.date_string_col))
) t9
) t12 ON ((t12.timestamp_col) = (t1.timestamp_col)) AND ((t12.char_col) = (t1.date_string_col))
WHERE
(t1.smallint_col) NOT IN (t1.int_col, t1.id)
GROUP BY
COALESCE(t1.smallint_col, t1.id, t1.day),
COALESCE(t12.timestamp_col, t1.timestamp_col, t12.timestamp_col)
HAVING
(MIN(COALESCE(t1.smallint_col, t1.id, t1.day))) IN (SUM(-10.22), (COALESCE(t1.smallint_col, t1.id, t1.day)) + (COALESCE(t1.smallint_col, t1.id, t1.day)));
{code}

Stack Trace:
{code}
#0  0x00007f5d302b7cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f5d302bb0d8 in __GI_abort () at abort.c:89
#2  0x00000000021a0f59 in google::DumpStackTraceAndExit() ()
#3  0x000000000219a3cd in google::LogMessage::Fail() ()
#4  0x000000000219ccf6 in google::LogMessage::SendToLog() ()
#5  0x0000000002199eed in google::LogMessage::Flush() ()
#6  0x000000000219d79e in google::LogMessageFatal::~LogMessageFatal() ()
#7  0x00000000012b660c in impala::RowDescriptor::GetTupleIdx (this=0x11fb1248, id=8) at /home/dev/Impala/be/src/runtime/descriptors.cc:402
#8  0x00000000010fe896 in impala::TupleIsNullPredicate::Prepare (this=0xea6c800, state=0x1671d600, row_desc=..., ctx=0x10272c00) at /home/dev/Impala/be/src/exprs/tuple-is-null-predicate.cc:45
#9  0x000000000108c501 in impala::ExprContext::Prepare (this=0x10272c00, state=0x1671d600, row_desc=..., tracker=0x14eafa20) at /home/dev/Impala/be/src/exprs/expr-context.cc:53
#10 0x00000000010776f6 in impala::Expr::Prepare (ctxs=..., state=0x1671d600, row_desc=..., tracker=0x14eafa20) at /home/dev/Impala/be/src/exprs/expr.cc:357
#11 0x000000000176a33e in impala::SortExecExprs::Prepare (this=0x13a49d48, state=0x1671d600, child_row_desc=..., output_row_desc=..., expr_mem_tracker=0x14eafa20) at /home/dev/Impala/be/src/exec/sort-exec-exprs.cc:52
#12 0x000000000176ac8c in impala::SortNode::Prepare (this=0x13a49c00, state=0x1671d600) at /home/dev/Impala/be/src/exec/sort-node.cc:46
#13 0x0000000001644402 in impala::PlanFragmentExecutor::Prepare (this=0x9eb3428, request=...) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:236
#14 0x0000000001469b8f in impala::FragmentMgr::FragmentExecState::Prepare (this=0x9eb3200, exec_params=...) at /home/dev/Impala/be/src/service/fragment-exec-state.cc:44
#15 0x000000000146187b in impala::FragmentMgr::ExecPlanFragment (this=0xe1c7020, exec_params=...) at /home/dev/Impala/be/src/service/fragment-mgr.cc:64
#16 0x00000000013d354b in impala::ImpalaInternalService::ExecPlanFragment (this=0xea99890, return_val=..., params=...) at /home/dev/Impala/be/src/service/impala-internal-service.h:37
#17 0x000000000157a36c in impala::ImpalaInternalServiceProcessor::process_ExecPlanFragment (this=0xe1c6fc0, seqid=0, iprot=0x12b90e00, oprot=0x13a21a00, callContext=0x1259ff00) at /home/dev/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:949
#18 0x000000000157a0ba in impala::ImpalaInternalServiceProcessor::dispatchCall (this=0xe1c6fc0, iprot=0x12b90e00, oprot=0x13a21a00, fname=..., seqid=0, callContext=0x1259ff00) at /home/dev/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:922
#19 0x00000000013ca532 in apache::thrift::TDispatchProcessor::process (this=0xe1c6fc0, in=..., out=..., connectionContext=0x1259ff00) at /home/dev/Impala/toolchain/thrift-0.9.0-p2/include/thrift/TDispatchProcessor.h:121
#20 0x0000000002152bbf in apache::thrift::server::TThreadedServer::Task::run() ()
#21 0x00000000012b16ef in impala::ThriftThread::RunRunnable (this=0x13a20180, runnable=..., promise=0x7f5cd4d7f690) at /home/dev/Impala/be/src/rpc/thrift-thread.cc:61
#22 0x00000000012b2e3f in boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x13b5db00, p=0x13a20180, a1=..., a2=0x7f5cd4d7f690) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/bind/mem_fn_template.hpp:280
#23 0x00000000012b2cd5 in boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x13b5db10, f=..., a=...) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/bind/bind.hpp:392
#24 0x00000000012b2a21 in boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x13b5db00) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#25 0x00000000012b2934 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/function/function_template.hpp:153
#26 0x00000000012e79be in boost::function0<void>::operator() (this=0x7f5c78aafde0) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/function/function_template.hpp:767
#27 0x0000000001510879 in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7f5cd4d7f480) at /home/dev/Impala/be/src/util/thread.cc:316
#28 0x0000000001519046 in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0x135107c0, f=@0x135107b8: 0x15105b4 <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>, a=...) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/bind/bind.hpp:457
#29 0x0000000001518f89 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0x135107b8) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#30 0x0000000001518f4c in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0x13510600) at /home/dev/Impala/toolchain/boost-1.57.0/include/boost/thread/detail/thread.hpp:116
#31 0x000000000181305a in thread_proxy ()
#32 0x00007f5d3294f182 in start_thread (arg=0x7f5c78ab0700) at pthread_create.c:312
#33 0x00007f5d3037b47d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{code}"	IMPALA	Resolved	1	1	1880	crash, query_generator
13051742	Combining fragments with compatible data partitions can lead to incorrect results due to type incompatibilities (missing casts).	"The following query returns incorrect results but the the equivalent [broadcast] version returns correct results.
The reason is that we hoist up the compatible agg child fragment into the partitioned join fragment, but the hoisted-up agg fragment has an exchange that hashes on ""c"" as a int type. Even though the ""c"" columns are value equivalent their hashes are only equal if hashed as the same type.
One way to fix this could be to add a cast to the data exchange of the agg fragment.

{code}
create table foo (c int);
insert into foo values(1),(2),(3),(4),(5),(6);

create table bar (c bigint);
insert into bar values(1),(2),(3),(4),(5),(6);

select f.c, b.c from
  (select c from foo union select c from foo) f
inner join [shuffle]
  (select c from bar union select c from bar) b
on f.c = b.c
{code}"	IMPALA	Resolved	3	1	1880	correctness
13052719	test_partitioning.py failure: Estimated Per-Host Requirements changed	"impala-master-repeated-runs-cdh5 job (http://sandbox.jenkins.cloudera.com/job/impala-master-repeated-runs-cdh5/212/) failed with:

{code}
MainThread: Comparing QueryTestResults (expected vs actual):
'   partitions=4/11 files=4 size=8B' == '   partitions=4/11 files=4 size=8B'
'' == ''
'00:SCAN HDFS [hdfs_partitioning.all_insert_partition_col_types]' == '00:SCAN HDFS [hdfs_partitioning.all_insert_partition_col_types]'
'01:EXCHANGE [UNPARTITIONED]' == '01:EXCHANGE [UNPARTITIONED]'
'Estimated Per-Host Requirements: Memory=32.00MB VCores=1' != 'Estimated Per-Host Requirements: Memory=48.00MB VCores=1'
'WARNING: The following tables are missing relevant table and/or column statistics.' == 'WARNING: The following tables are missing relevant table and/or column statistics.'
'hdfs_partitioning.all_insert_partition_col_types' == 'hdfs_partitioning.all_insert_partition_col_types'
'|' == '|'
{code}

If you know offhand what commit would have changed the per-host mem estimate, let me know. Otherwise, I'll look at it."	IMPALA	Resolved	3	1	1880	test-infra
13052136	Crash: impala::TupleIsNullPredicate::Prepare	"{code}
WITH with_2 AS (WITH with_1 AS (SELECT
LEAST(COALESCE(t1.year, 304), COALESCE(t2.id, 154)) AS int_col
FROM alltypes t1
RIGHT JOIN alltypestiny t2 ON ((t2.timestamp_col) = (t1.timestamp_col)) AND ((t2.string_col) = (t1.string_col)))
SELECT
t1.smallint_col,
(t2.int_col) * (t1.month) AS int_col,
GREATEST(COALESCE(t2.int_col, 524), COALESCE(t1.bigint_col, -984)) AS int_col_2
FROM alltypestiny t1
LEFT JOIN alltypes t2 ON ((t2.timestamp_col) = (t1.timestamp_col)) AND ((t2.string_col) = (t1.string_col)))
SELECT
t1.float_col,
t1.bigint_col,
t2.smallint_col,
(t1.id) IN (t2.int_col_2, t2.smallint_col) AS boolean_col,
LEAD((-483) * (121), 3) OVER (ORDER BY t1.float_col DESC, t1.bigint_col ASC) AS int_col
FROM alltypestiny t1
LEFT JOIN with_2 t2 ON (t2.int_col_2) = (t1.id)
{code}

{code}
raise () from /lib64/libc.so.6
raise () from /lib64/libc.so.6
abort () from /lib64/libc.so.6
google::DumpStackTraceAndExit () at src/utilities.cc:147
google::LogMessage::Fail () at src/logging.cc:1296
google::LogMessage::SendToLog (this=0x7f4dee2304c0) at src/logging.cc:1250
google::LogMessage::Flush (this=0x7f4dee2304c0) at src/logging.cc:1119
google::LogMessageFatal::~LogMessageFatal (this=0x7f4dee2304c0, __in_chrg=<value optimized out>) at src/logging.cc:1817
impala::TupleIsNullPredicate::Prepare (this=0x8cf8e00, state=0x9d9ee00, row_desc=..., ctx=0x95c58c0) at /data/9/query-gen/Impala/be/src/exprs/tuple-is-null-predicate.cc:49
impala::Expr::Prepare (this=0x98936c0, state=0x9d9ee00, row_desc=..., context=0x95c58c0) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:353
impala::Expr::Prepare (this=0x938ddc0, state=0x9d9ee00, row_desc=..., context=0x95c58c0) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:353
impala::ScalarFnCall::Prepare (this=0x938ddc0, state=0x9d9ee00, desc=..., context=0x95c58c0) at /data/9/query-gen/Impala/be/src/exprs/scalar-fn-call.cc:50
impala::ExprContext::Prepare (this=0x95c58c0, state=0x9d9ee00, row_desc=..., tracker=0x74478c0) at /data/9/query-gen/Impala/be/src/exprs/expr-context.cc:52
impala::Expr::Prepare (ctxs=std::vector of length 5, capacity 8 = {...}, state=0x9d9ee00, row_desc=..., tracker=0x74478c0) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:344
impala::Coordinator::Exec (this=0x702ea00, schedule=..., output_expr_ctxs=0x7f0ab60) at /data/9/query-gen/Impala/be/src/runtime/coordinator.cc:352
impala::ImpalaServer::QueryExecState::ExecQueryOrDmlRequest (this=0x7f0a000, query_exec_request=...) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:401
impala::ImpalaServer::QueryExecState::Exec (this=0x7f0a000, exec_request=0x7f4dee232950) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:138
impala::ImpalaServer::ExecuteInternal (this=0x5f4c580, query_ctx=..., session_state=..., registered_exec_state=0x7f4dee233b67, exec_state=0x7f4dee233df0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:607
impala::ImpalaServer::Execute (this=0x5f4c580, query_ctx=0x7f4dee233c20, session_state=..., exec_state=0x7f4dee233df0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:550
impala::ImpalaServer::ExecuteStatement (this=0x5f4c580, return_val=..., request=...) at /data/9/query-gen/Impala/be/src/service/impala-hs2-server.cc:709
apache::hive::service::cli::thrift::TCLIServiceProcessor::process_ExecuteStatement (this=0x43f70e0, seqid=0, iprot=0x6556380, oprot=0x7336bc0, callContext=0x72e8900) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4695
apache::hive::service::cli::thrift::TCLIServiceProcessor::dispatchCall (this=0x43f70e0, iprot=0x6556380, oprot=0x7336bc0, fname=""ExecuteStatement"", seqid=0, callContext=0x72e8900) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4506
impala::ImpalaHiveServer2ServiceProcessor::dispatchCall (this=0x43f70e0, iprot=0x6556380, oprot=0x7336bc0, fname=""ExecuteStatement"", seqid=0, callContext=0x72e8900) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/ImpalaHiveServer2Service.cpp:463
apache::thrift::TDispatchProcessor::process (this=0x43f70e0, in=..., out=..., connectionContext=0x72e8900) at /data/9/query-gen/Impala/thirdparty/thrift-0.9.0/build/include/thrift/TDispatchProcessor.h:121
apache::thrift::server::TThreadPoolServer::Task::run (this=0x8723260) at src/thrift/server/TThreadPoolServer.cpp:70
apache::thrift::concurrency::ThreadManager::Task::run (this=0x72e8700) at src/thrift/concurrency/ThreadManager.cpp:187
apache::thrift::concurrency::ThreadManager::Worker::run (this=0x6801650) at src/thrift/concurrency/ThreadManager.cpp:316
impala::ThriftThread::RunRunnable (this=0x6b65c40, runnable=..., promise=0x7ffff59d19d0) at /data/9/query-gen/Impala/be/src/rpc/thrift-thread.cc:61
boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x64a9230, p=0x6b65c40, a1=..., a2=0x7ffff59d19d0) at /usr/include/boost/bind/mem_fn_template.hpp:280
boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x64a9240, f=..., a=...) at /usr/include/boost/bind/bind.hpp:392
boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x64a9230) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
boost::function0<void>::operator() (this=0x7f4dee234cd0) at /usr/include/boost/function/function_template.hpp:1013
impala::Thread::SuperviseThread (name=""hiveserver2-frontend-8"", category=""thrift-server"", functor=..., thread_started=0x7ffff59d17d0) at /data/9/query-gen/Impala/be/src/util/thread.cc:311
boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::string &, const std::string &, impala::Thread::ThreadFunctor, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x5c14af0, f=@0x5c14ae8, a=...) at /usr/include/boost/bind/bind.hpp:457
boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x5c14ae8) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x5c14960) at /usr/include/boost/thread/detail/thread.hpp:61
thread_proxy ()
start_thread () from /lib64/libpthread.so.0
clone () from /lib64/libc.so.6
{code}"	IMPALA	Resolved	3	1	1880	query_generator
13051907	Pull out common conjuncts from disjunctions	"I see this in tpch and tpcds where predicates look like this:

where
   ( col = 1 AND ... AND ...) or
   ( col = 1 AND ... AND ...) or
   ( col = 1 AND ... AND ...)

It's an OR of ANDs and there are predicates that are in each OR (col = 1). This could be pulled out."	IMPALA	Resolved	4	2	1880	documentation, performance, planner, tpc-ds
13052320	Using hints causes crash	"Tried to hint the query to force to use the SHUFFLE caused Impala daemon to crash:
Impala crashed (tcmalloc: large alloc 1073741824 bytes == 0x7fa457802000 @ 0x15d1e42 0xc006a8
terminate called after throwing an instance of 'std::bad_alloc'
what(): std::bad_alloc

The hinted and original query is attached."	IMPALA	Resolved	2	1	1880	performance, tpc-ds
13051952	Incorrect result after reorder to use RIGHT JOIN	"The first query below has incorrect results. The second query is the same except ""straight_join"" was added and the results are correct. It's interesting that the incorrect values returned by the top query are not from the column in the select list (the values should be dates).

{noformat}
[localhost.localdomain:21000] > 
select t2.date_string_col AS string_col_1 
FROM alltypestiny t1 
LEFT JOIN alltypestiny t2 ON t2.date_string_col = t1.string_col 
LEFT JOIN alltypesagg t3 ON t3.string_col = t1.date_string_col;
Query: select t2.date_string_col AS string_col_1 FROM alltypestiny t1 LEFT JOIN alltypestiny t2 ON t2.date_string_col = t1.string_col LEFT JOIN alltypesagg t3 ON t3.string_col = t1.date_string_col
+--------------+
| string_col_1 |
+--------------+
| 1            |
| NULL         |
| 1            |
| NULL         |
| 1            |
| NULL         |
| 1            |
| NULL         |
+--------------+
Fetched 8 row(s) in 0.45s


[localhost.localdomain:21000] > select straight_join t2.date_string_col AS string_col_1 FROM alltypestiny t1 LEFT JOIN alltypestiny t2 ON t2.date_string_col = t1.string_col LEFT JOIN alltypesagg t3 ON t3.string_col = t1.date_string_col;
Query: select straight_join t2.date_string_col AS string_col_1 FROM alltypestiny t1 LEFT JOIN alltypestiny t2 ON t2.date_string_col = t1.string_col LEFT JOIN alltypesagg t3 ON t3.string_col = t1.date_string_col
+--------------+
| string_col_1 |
+--------------+
| NULL         |
| NULL         |
| NULL         |
| NULL         |
| NULL         |
| NULL         |
| NULL         |
| NULL         |
+--------------+
Fetched 8 row(s) in 0.17s


[localhost.localdomain:21000] > explain select t2.date_string_col AS string_col_1 FROM alltypestiny t1 LEFT JOIN alltypestiny t2 ON t2.date_string_col = t1.string_col LEFT JOIN alltypesagg t3 ON t3.string_col = t1.date_string_col;
Query: explain select t2.date_string_col AS string_col_1 FROM alltypestiny t1 LEFT JOIN alltypestiny t2 ON t2.date_string_col = t1.string_col LEFT JOIN alltypesagg t3 ON t3.string_col = t1.date_string_col
+-----------------------------------------------------------+
| Explain String                                            |
+-----------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=80.00MB VCores=3  |
|                                                           |
| 09:EXCHANGE [UNPARTITIONED]                               |
| |  hosts=3 per-host-mem=unavailable                       |
| |  tuple-ids=2N,0,1N row-size=84B cardinality=40000       |
| |                                                         |
| 04:HASH JOIN [RIGHT OUTER JOIN, PARTITIONED]              |
| |  hash predicates: t3.string_col = t1.date_string_col    |
| |  hosts=3 per-host-mem=381B                              |
| |  tuple-ids=2N,0,1N row-size=84B cardinality=40000       |
| |                                                         |
| |--08:EXCHANGE [HASH(t1.date_string_col)]                 |
| |  |  hosts=3 per-host-mem=0B                             |
| |  |  tuple-ids=0,1N row-size=65B cardinality=16          |
| |  |                                                      |
| |  03:HASH JOIN [LEFT OUTER JOIN, PARTITIONED]            |
| |  |  hash predicates: t1.string_col = t2.date_string_col |
| |  |  hosts=3 per-host-mem=70B                            |
| |  |  tuple-ids=0,1N row-size=65B cardinality=16          |
| |  |                                                      |
| |  |--06:EXCHANGE [HASH(t2.date_string_col)]              |
| |  |  |  hosts=3 per-host-mem=0B                          |
| |  |  |  tuple-ids=1 row-size=24B cardinality=8           |
| |  |  |                                                   |
| |  |  01:SCAN HDFS [functional.alltypestiny t2, RANDOM]   |
| |  |     partitions=4/4 size=460B                         |
| |  |     table stats: 8 rows total                        |
| |  |     column stats: all                                |
| |  |     hosts=3 per-host-mem=32.00MB                     |
| |  |     tuple-ids=1 row-size=24B cardinality=8           |
| |  |                                                      |
| |  05:EXCHANGE [HASH(t1.string_col)]                      |
| |  |  hosts=3 per-host-mem=0B                             |
| |  |  tuple-ids=0 row-size=41B cardinality=8              |
| |  |                                                      |
| |  00:SCAN HDFS [functional.alltypestiny t1, RANDOM]      |
| |     partitions=4/4 size=460B                            |
| |     table stats: 8 rows total                           |
| |     column stats: all                                   |
| |     hosts=3 per-host-mem=32.00MB                        |
| |     tuple-ids=0 row-size=41B cardinality=8              |
| |                                                         |
| 07:EXCHANGE [HASH(t3.string_col)]                         |
| |  hosts=3 per-host-mem=0B                                |
| |  tuple-ids=2 row-size=19B cardinality=10000             |
| |                                                         |
| 02:SCAN HDFS [functional.alltypesagg t3, RANDOM]          |
|    partitions=11/11 size=814.73KB                         |
|    table stats: 11000 rows total                          |
|    column stats: all                                      |
|    hosts=3 per-host-mem=80.00MB                           |
|    tuple-ids=2 row-size=19B cardinality=10000             |
+-----------------------------------------------------------+
Fetched 52 row(s) in 0.04s
{noformat}"	IMPALA	Resolved	1	1	1880	correctness
13051721	Query with self joined table may produce incorrect results	"This one is tricky and I dont really understand what's causing this but the results are wrong and its reproducible.

To reproduce, create the two tables below:

{noformat}
[localhost:21000] > create table foo as select 1 int_col, true bool_col union select 1, false union select 1, null;
Query: create table foo as select 1 int_col, true bool_col union select 1, false union select 1, null
+-------------------+
| summary           |
+-------------------+
| Inserted 3 row(s) |
+-------------------+
Returned 1 row(s) in 0.87s
[localhost:21000] > select * from foo;
Query: select * from foo
+---------+----------+
| int_col | bool_col |
+---------+----------+
| 1       | false    |
| 1       | true     |
| 1       | NULL     |
+---------+----------+
Returned 3 row(s) in 0.64s


[localhost:21000] > create table bar as select * from alltypes union all select * from alltypes;
Query: create table bar as select * from alltypes union all select * from alltypes
+-----------------------+
| summary               |
+-----------------------+
| Inserted 14600 row(s) |
+-----------------------+
Returned 1 row(s) in 1.35s
{noformat}

Now you can see incorrect results:

{noformat}
[localhost:21000] > select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join bar t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10;
Query: select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join bar t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10
+----------+----------+----------+
| bool_col | bool_col | bool_col |
+----------+----------+----------+
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
| false    | true     | true     |
+----------+----------+----------+
Returned 10 row(s) in 1.09s


[localhost:21000] > explain select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join bar t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10;
Query: explain select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join bar t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10
+------------------------------------------------------------------------------------+
| Explain String                                                                     |
+------------------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=4.06GB VCores=3                            |
| WARNING: The following tables are missing relevant table and/or column statistics. |
| functional.bar, functional.foo                                                     |
|                                                                                    |
| 07:EXCHANGE [UNPARTITIONED]                                                        |
| |  limit: 10                                                                       |
| |                                                                                  |
| 04:HASH JOIN [INNER JOIN, BROADCAST]                                               |
| |  hash predicates: t2.bool_col = t3.bool_col                                      |
| |  limit: 10                                                                       |
| |                                                                                  |
| |--06:EXCHANGE [BROADCAST]                                                         |
| |  |                                                                               |
| |  02:SCAN HDFS [functional.bar t3]                                                |
| |     partitions=1/1 size=1.25MB                                                   |
| |                                                                                  |
| 03:HASH JOIN [INNER JOIN, BROADCAST]                                               |
| |  hash predicates: t1.int_col = t2.int_col                                        |
| |                                                                                  |
| |--05:EXCHANGE [BROADCAST]                                                         |
| |  |                                                                               |
| |  01:SCAN HDFS [functional.foo t2]                                                |
| |     partitions=1/1 size=20B                                                      |
| |                                                                                  |
| 00:SCAN HDFS [functional.foo t1]                                                   |
|    partitions=1/1 size=20B                                                         |
+------------------------------------------------------------------------------------+
Returned 26 row(s) in 0.01s
{noformat}

but if the query in run against the non-union version of ""bar"", ie ""alltypes"", then the results are correct.

{noformat}
[localhost:21000] > select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join alltypes t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10;
Query: select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join alltypes t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10
+----------+----------+----------+
| bool_col | bool_col | bool_col |
+----------+----------+----------+
| true     | true     | true     |
| false    | false    | false    |
| true     | true     | true     |
| false    | false    | false    |
| true     | true     | true     |
| false    | false    | false    |
| true     | true     | true     |
| false    | false    | false    |
| true     | true     | true     |
| false    | false    | false    |
+----------+----------+----------+
Returned 10 row(s) in 1.18s


[localhost:21000] > explain select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join alltypes t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10;
Query: explain select t1.bool_col, t2.bool_col, t3.bool_col from foo t1 join foo t2 on t2.int_col = t1.int_col join alltypes t3 on t3.bool_col = t2.bool_col and t3.bool_col = t1.bool_col limit 10
+------------------------------------------------------------------------------------+
| Explain String                                                                     |
+------------------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=4.16GB VCores=3                            |
| WARNING: The following tables are missing relevant table and/or column statistics. |
| functional.foo                                                                     |
|                                                                                    |
| 07:EXCHANGE [UNPARTITIONED]                                                        |
| |  limit: 10                                                                       |
| |                                                                                  |
| 04:HASH JOIN [INNER JOIN, BROADCAST]                                               |
| |  hash predicates: t1.int_col = t2.int_col, t3.bool_col = t2.bool_col             |
| |  limit: 10                                                                       |
| |                                                                                  |
| |--06:EXCHANGE [BROADCAST]                                                         |
| |  |                                                                               |
| |  01:SCAN HDFS [functional.foo t2]                                                |
| |     partitions=1/1 size=20B                                                      |
| |                                                                                  |
| 03:HASH JOIN [INNER JOIN, BROADCAST]                                               |
| |  hash predicates: t3.bool_col = t1.bool_col                                      |
| |                                                                                  |
| |--05:EXCHANGE [BROADCAST]                                                         |
| |  |                                                                               |
| |  00:SCAN HDFS [functional.foo t1]                                                |
| |     partitions=1/1 size=20B                                                      |
| |                                                                                  |
| 02:SCAN HDFS [functional.alltypes t3]                                              |
|    partitions=24/24 size=478.45KB                                                  |
+------------------------------------------------------------------------------------+
Returned 26 row(s) in 0.01s
{noformat}

So I guess the problem has something to do with the table sizes involved."	IMPALA	Resolved	1	1	1880	correctness
13052103	Query causes Impala crash	"QUERY:
{code}
USE functional;
SELECT
COALESCE(t1.smallint_col, t1.year, t1.tinyint_col) AS int_col,
t2.year,
GREATEST(COALESCE(t1.year, -329), COALESCE(t2.int_col, -960)) AS int_col_2
FROM alltypestiny t1
LEFT JOIN (
SELECT
t1.year,
COALESCE(-914, LAST_VALUE(-660) OVER (ORDER BY t1.year ASC, CAST(SUM(t1.year) AS STRING) DESC), -694) AS int_col,
CAST(SUM(t1.year) AS STRING) AS char_col
FROM alltypes t1
INNER JOIN alltypesagg t2 ON (t2.string_col) = (t1.date_string_col)
GROUP BY
t1.year
) t2 ON (((t2.int_col) = (t1.tinyint_col)) AND ((t2.char_col) = (t1.date_string_col))) AND ((t2.char_col) = (t1.date_string_col))
{code}

STACK:
{code}
#0  0x0000003a0ca32635 in raise () from /lib64/libc.so.6
#0  0x0000003a0ca32635 in raise () from /lib64/libc.so.6
#1  0x0000003a0ca33e15 in abort () from /lib64/libc.so.6
#2  0x0000000001e60cf9 in google::DumpStackTraceAndExit () at src/utilities.cc:147
#3  0x0000000001e583fd in google::LogMessage::Fail () at src/logging.cc:1296
#4  0x0000000001e5be87 in google::LogMessage::SendToLog (this=0x7f3a11920020) at src/logging.cc:1250
#5  0x0000000001e5b3e6 in google::LogMessage::Flush (this=0x7f3a11920020) at src/logging.cc:1119
#6  0x0000000001e5c31d in google::LogMessageFatal::~LogMessageFatal (this=0x7f3a11920020, __in_chrg=<value optimized out>) at src/logging.cc:1817
#7  0x0000000001394419 in impala::RowDescriptor::TupleIsNullable (this=0x5475948, tuple_idx=-1) at /data/9/query-gen/Impala/be/src/runtime/descriptors.cc:318
#8  0x00000000011bd0cb in impala::TupleIsNullPredicate::Prepare (this=0x548aa80, state=0xc576000, row_desc=..., ctx=0x9b0cb00) at /data/9/query-gen/Impala/be/src/exprs/tuple-is-null-predicate.cc:49
#9  0x0000000001175c4b in impala::Expr::Prepare (this=0x5cea820, state=0xc576000, row_desc=..., context=0x9b0cb00) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:353
#10 0x0000000001175c4b in impala::Expr::Prepare (this=0x99b6680, state=0xc576000, row_desc=..., context=0x9b0cb00) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:353
#11 0x0000000001175c4b in impala::Expr::Prepare (this=0x548b880, state=0xc576000, row_desc=..., context=0x9b0cb00) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:353
#12 0x00000000011bd88b in impala::ScalarFnCall::Prepare (this=0x548b880, state=0xc576000, desc=..., context=0x9b0cb00) at /data/9/query-gen/Impala/be/src/exprs/scalar-fn-call.cc:50
#13 0x0000000001175c4b in impala::Expr::Prepare (this=0x5248540, state=0xc576000, row_desc=..., context=0x9b0cb00) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:353
#14 0x00000000011bd88b in impala::ScalarFnCall::Prepare (this=0x5248540, state=0xc576000, desc=..., context=0x9b0cb00) at /data/9/query-gen/Impala/be/src/exprs/scalar-fn-call.cc:50
#15 0x0000000000e74f4b in impala::ExprContext::Prepare (this=0x9b0cb00, state=0xc576000, row_desc=..., tracker=0xb21a160) at /data/9/query-gen/Impala/be/src/exprs/expr-context.cc:52
#16 0x0000000001175a81 in impala::Expr::Prepare (ctxs=std::vector of length 3, capacity 4 = {...}, state=0xc576000, row_desc=..., tracker=0xb21a160) at /data/9/query-gen/Impala/be/src/exprs/expr.cc:344
#17 0x0000000001361af7 in impala::Coordinator::Exec (this=0x5359e00, schedule=..., output_expr_ctxs=0x1029ab50) at /data/9/query-gen/Impala/be/src/runtime/coordinator.cc:352
#18 0x000000000101c455 in impala::ImpalaServer::QueryExecState::ExecQueryOrDmlRequest (this=0x1029a000, query_exec_request=...) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:401
#19 0x00000000010195f8 in impala::ImpalaServer::QueryExecState::Exec (this=0x1029a000, exec_request=0x7f3a11922980) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:138
#20 0x0000000000f6eca2 in impala::ImpalaServer::ExecuteInternal (this=0x43a6b00, query_ctx=..., session_state=..., registered_exec_state=0x7f3a11923b77, exec_state=0x7f3a11923df0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:607
#21 0x0000000000f6e308 in impala::ImpalaServer::Execute (this=0x43a6b00, query_ctx=0x7f3a11923c30, session_state=..., exec_state=0x7f3a11923df0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:550
#22 0x0000000000fe7b3b in impala::ImpalaServer::ExecuteStatement (this=0x43a6b00, return_val=..., request=...) at /data/9/query-gen/Impala/be/src/service/impala-hs2-server.cc:709
#23 0x00000000012940b7 in apache::hive::service::cli::thrift::TCLIServiceProcessor::process_ExecuteStatement (this=0x2979360, seqid=0, iprot=0x599e080, oprot=0x599e040, callContext=0x56af3c0) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4695
#24 0x00000000012925d8 in apache::hive::service::cli::thrift::TCLIServiceProcessor::dispatchCall (this=0x2979360, iprot=0x599e080, oprot=0x599e040, fname=""ExecuteStatement"", seqid=0, callContext=0x56af3c0) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4506
#25 0x000000000125c37b in impala::ImpalaHiveServer2ServiceProcessor::dispatchCall (this=0x2979360, iprot=0x599e080, oprot=0x599e040, fname=""ExecuteStatement"", seqid=0, callContext=0x56af3c0) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/ImpalaHiveServer2Service.cpp:463
#26 0x0000000000f7ee4e in apache::thrift::TDispatchProcessor::process (this=0x2979360, in=..., out=..., connectionContext=0x56af3c0) at /data/9/query-gen/Impala/thirdparty/thrift-0.9.0/build/include/thrift/TDispatchProcessor.h:121
#27 0x0000000001e141f9 in apache::thrift::server::TThreadPoolServer::Task::run (this=0x60568a0) at src/thrift/server/TThreadPoolServer.cpp:70
#28 0x0000000001e0075f in apache::thrift::concurrency::ThreadManager::Task::run (this=0x56af440) at src/thrift/concurrency/ThreadManager.cpp:187
#29 0x0000000001e026a4 in apache::thrift::concurrency::ThreadManager::Worker::run (this=0x2fe6900) at src/thrift/concurrency/ThreadManager.cpp:316
#30 0x0000000000ec1ecb in impala::ThriftThread::RunRunnable (this=0x40b6840, runnable=..., promise=0x7fffd6783640) at /data/9/query-gen/Impala/be/src/rpc/thrift-thread.cc:61
#31 0x0000000000ec367b in boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x5d8cc00, p=0x40b6840, a1=..., a2=0x7fffd6783640) at /usr/include/boost/bind/mem_fn_template.hpp:280
#32 0x0000000000ec34d4 in boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x5d8cc10, f=..., a=...) at /usr/include/boost/bind/bind.hpp:392
#33 0x0000000000ec3251 in boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x5d8cc00) at /usr/include/boost/bind/bind_template.hpp:20
#34 0x0000000000ec315f in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
#35 0x0000000000ee75db in boost::function0<void>::operator() (this=0x7f3a11924cd0) at /usr/include/boost/function/function_template.hpp:1013
#36 0x0000000001123020 in impala::Thread::SuperviseThread (name=""hiveserver2-frontend-8"", category=""thrift-server"", functor=..., thread_started=0x7fffd6783440) at /data/9/query-gen/Impala/be/src/util/thread.cc:311
#37 0x000000000112b3b8 in boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::string &, const std::string &, impala::Thread::ThreadFunctor, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x4401090, f=@0x4401088, a=...) at /usr/include/boost/bind/bind.hpp:457
#38 0x000000000112b2ff in boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x4401088) at /usr/include/boost/bind/bind_template.hpp:20
#39 0x000000000112b28c in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x4400f00) at /usr/include/boost/thread/detail/thread.hpp:61
#40 0x0000000001545574 in thread_proxy ()
#41 0x0000003a0ce079d1 in start_thread () from /lib64/libpthread.so.0
#42 0x0000003a0cae886d in clone () from /lib64/libc.so.6
{code}"	IMPALA	Resolved	3	1	1880	query_generator
13050733	COUNT(DISTINCT col) returns wrong results -- does not ignore NULLs	"COUNT(DISTINCT col) should ignore NULL values, but it does not.
Citing http://www.postgresql.org/docs/9.2/static/sql-expressions.html#SYNTAX-AGGREGATES
For example, count(*) yields the total number of input rows; count(f1) yields the number of input rows in which f1 is non-null, since count ignores nulls; and count(distinct f1) yields the number of distinct non-null values of f1.

{code}
[impala:21000] > select version();
Query: select version()
Query finished, fetching results ...
impalad version 0.5 RELEASE (build 5a64c63eeb6475498788bd533c8b694e08276c93)
Built on Fri, 01 Feb 2013 11:43:54 PST
Returned 1 row(s) in 0.12s

[impala:21000] > select distinct i_class_id from item order by 1 limit 20;
Query: select distinct i_class_id from item order by 1 limit 20
Query finished, fetching results ...
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
NULL
Returned 17 row(s) in 1.01s

[impala:21000] > select count(distinct i_class_id) from item; 
Query: select count(distinct i_class_id) from item 
Query finished, fetching results ... 
17 
Returned 1 row(s) in 1.51s 
{code}"	IMPALA	Resolved	2	1	1880	wrongresults
13054248	Tooling for helping to clean up c++ code.	"It would be nice to have extra automation to help us sanitize our c++ code, for example, detecting superfluous includes or includes that can be avoided with forward declarations.

These free tools seem interesting:
https://github.com/myint/cppclean
https://github.com/include-what-you-use/include-what-you-use

The latter one is in an Alpha stage and has a pretty heavy-weight setup, but has more advanced capabilities (if they work). Maybe cppclean is good the simpler solution for now."	IMPALA	Resolved	3	2	1880	static-analysis
13052481	INSERT OVERWRITE does not apply constant predicates	"A constant predicate that should prevent an {{INSERT}} from writing any rows is dropped.

{code}
create table hp (col int) PARTITIONED BY (p int);

[localhost:21000] > INSERT OVERWRITE hp PARTITION(p=2) SELECT id from functional.alltypes WHERE 1 = 0;
Query: insert OVERWRITE hp PARTITION(p=2) SELECT id from functional.alltypes WHERE 1 = 0
Inserted 7300 row(s) in 0.57s

[localhost:21000] > EXPLAIN INSERT OVERWRITE hp PARTITION(p=2) SELECT id from functional.alltypes WHERE 1 = 0;
Query: explain INSERT OVERWRITE hp PARTITION(p=2) SELECT id from functional.alltypes WHERE 1 = 0
+------------------------------------------------------------------------------------+
| Explain String                                                                     |
+------------------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=160.01MB VCores=1                          |
| WARNING: The following tables are missing relevant table and/or column statistics. |
| functional.alltypes                                                                |
|                                                                                    |
| F00:PLAN FRAGMENT [RANDOM]                                                         |
|   WRITE TO HDFS [debug.hp, OVERWRITE=true, PARTITION-KEYS=(2)]                     |
|   |  partitions=1                                                                  |
|   |  hosts=3 per-host-mem=9.50KB                                                   |
|   |                                                                                |
|   00:SCAN HDFS [functional.alltypes, RANDOM]                                       |
|      partitions=24/24 files=24 size=478.45KB                                       |
|      table stats: 7300 rows total (24 partition(s) missing stats)                  |
|      column stats: all                                                             |
|      hosts=3 per-host-mem=160.00MB                                                 |
|      tuple-ids=0 row-size=4B cardinality=7300                                      |
+------------------------------------------------------------------------------------+
{code}"	IMPALA	Resolved	1	1	1880	correctness
13053420	Impala treats quotes in comments as literal quotes	"The following query will not work in Impala even though it should

{code}
-- It's working
select 1;
{code}

or 

{code}
/* It's working */ select 1;
{code}

The reason seems to be that the quote in the comment is treated as a literal quote and not ignored. This has two very very negative consequences



- The impala shell hangs when a comment with quotes is used and has to be killed.
- When the query is submitted via beeswax / hs2 the error message is missing quote, but the Impala debug webpage only shows ""Session closed"" with no further error message. Making it very hard to debug and since Impala shell hangs it's not even possible for the user to check what's happening locally."	IMPALA	Resolved	2	1	1880	sql-language
13054904	Parquet scanner with MT_DOP > 0 crashes when materializing no slots.	"Repro:
{code}
set mt_dop=1;
compute stats compute stats functional_parquet.alltypes
{code}

Stack:
{code}
#0  0x00007fd1ac08b425 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007fd1ac08eb8b in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00007fd1ae1daac5 in os::abort(bool) () from /home/abehm/jdk1.7.0_25/jre/lib/amd64/server/libjvm.so
#3  0x00007fd1ae33a137 in VMError::report_and_die() () from /home/abehm/jdk1.7.0_25/jre/lib/amd64/server/libjvm.so
#4  0x00007fd1ae1de5e0 in JVM_handle_linux_signal () from /home/abehm/jdk1.7.0_25/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  0x0000000001641264 in impala::RowBatch::row_byte_size (this=0x0) at /home/abehm/impala/be/src/runtime/row-batch.h:201
#7  0x000000000167a3e5 in impala::HdfsScanner::next_row (this=0xb16cd00, r=0xc240000)
    at /home/abehm/impala/be/src/exec/hdfs-scanner.h:467
#8  0x00000000017b21a4 in impala::HdfsScanner::WriteEmptyTuples (this=0xb16cd00, context=0xa038f80, row=0xc240000, num_tuples=300)
    at /home/abehm/impala/be/src/exec/hdfs-scanner.cc:269
#9  0x000000000169ecf5 in impala::HdfsParquetScanner::GetNextInternal (this=0xb16cd00, row_batch=0xb231320)
    at /home/abehm/impala/be/src/exec/hdfs-parquet-scanner.cc:342
#10 0x0000000001674c47 in impala::HdfsScanner::GetNext (this=0xb16cd00, row_batch=0xb231320)
    at /home/abehm/impala/be/src/exec/hdfs-scanner.h:132
#11 0x0000000001674605 in impala::HdfsScanNodeMt::GetNext (this=0xc52cf00, state=0xb0fe900, row_batch=0xb231320, eos=0x97ef949)
    at /home/abehm/impala/be/src/exec/hdfs-scan-node-mt.cc:93
#12 0x0000000001718657 in impala::PartitionedAggregationNode::GetRowsStreaming (this=0x97ef680, state=0xb0fe900, out_batch=0xb233ba0)
    at /home/abehm/impala/be/src/exec/partitioned-aggregation-node.cc:549
#13 0x00000000017166e1 in impala::PartitionedAggregationNode::GetNextInternal (this=0x97ef680, state=0xb0fe900, row_batch=0xb233ba0, 
    eos=0x7e27cb1) at /home/abehm/impala/be/src/exec/partitioned-aggregation-node.cc:451
#14 0x0000000001715adf in impala::PartitionedAggregationNode::GetNext (this=0x97ef680, state=0xb0fe900, row_batch=0xb233ba0, 
    eos=0x7e27cb1) at /home/abehm/impala/be/src/exec/partitioned-aggregation-node.cc:377
#15 0x000000000198a2ec in impala::PlanFragmentExecutor::OpenInternal (this=0x7e27b88)
    at /home/abehm/impala/be/src/runtime/plan-fragment-executor.cc:309
#16 0x0000000001989d9a in impala::PlanFragmentExecutor::Open (this=0x7e27b88)
    at /home/abehm/impala/be/src/runtime/plan-fragment-executor.cc:274
#17 0x0000000001504dec in impala::FragmentMgr::FragmentExecState::Exec (this=0x7e27800)
    at /home/abehm/impala/be/src/service/fragment-exec-state.cc:58
#18 0x00000000014fc57d in impala::FragmentMgr::FragmentThread (this=0xbf4adc0, fragment_instance_id=...)
    at /home/abehm/impala/be/src/service/fragment-mgr.cc:88
#19 0x0000000001500330 in boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>::operator() (this=0xac006f0, p=0xbf4adc0, 
    a1=...) at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/bind/mem_fn_template.hpp:165
#20 0x00000000015000ed in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list0> (this=0xac00700, f=..., a=...)
    at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/bind/bind.hpp:313
#21 0x00000000014ffa17 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId> > >::operator() (this=0xac006f0)
    at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
...
{code}"	IMPALA	Resolved	1	1	1880	multithreading
13052986	Crash: impala::Sorter::Reset	"Query:
{code}
SELECT
SUM(t2.pos) AS int_col,
MIN(NULL) OVER (ORDER BY SUM(t2.pos), COALESCE(MIN(t3.pos), NULL, -290) ASC) AS int_col_1,
COALESCE(MIN(t3.pos), NULL, -290) AS int_col_2
FROM table_0 t1
INNER JOIN t1.field_26.field_30 t2
INNER JOIN t2.item t3 ON (((t3.pos) = (t1.field_26.field_29)) AND ((t3.item.field_37) = (t2.pos))) AND ((t3.item.field_33) = (t2.pos))
WHERE
(t3.pos) IN (SELECT
LEAD(826, 85) OVER (ORDER BY t1.item.field_34 ASC, t1.item.field_36 DESC) AS int_col
FROM t2.item t1)
{code}

Stack Trace:
{code}
#0  0x00007fea32121cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007fea321250d8 in __GI_abort () at abort.c:89
#2  0x0000000002116799 in google::DumpStackTraceAndExit () at src/utilities.cc:147
#3  0x000000000210f84d in google::LogMessage::Fail () at src/logging.cc:1315
#4  0x00000000021116d5 in google::LogMessage::SendToLog (this=0x7fe97d1e9a70) at src/logging.cc:1269
#5  0x000000000210f3a3 in google::LogMessage::Flush (this=this@entry=0x7fe97d1e9a70) at src/logging.cc:1138
#6  0x000000000211202e in google::LogMessageFatal::~LogMessageFatal (this=0x7fe97d1e9a70, __in_chrg=<optimized out>) at src/logging.cc:1836
#7  0x000000000172a44a in impala::Sorter::Reset (this=0xc60d180) at /home/dev/Impala/be/src/runtime/sorter.cc:1070
#8  0x00000000016cd0eb in impala::SortNode::Reset (this=0xf1b3200, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/sort-node.cc:126
#9  0x00000000015d125a in impala::ExecNode::Reset (this=0xf1c9c00, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/exec-node.cc:166
#10 0x00000000016e8821 in impala::AnalyticEvalNode::Reset (this=0xf1c9c00, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/analytic-eval-node.cc:784
#11 0x00000000015d125a in impala::ExecNode::Reset (this=0xf214000, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/exec-node.cc:166
#12 0x00000000016ae665 in impala::PartitionedHashJoinNode::Reset (this=0xf214000, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/partitioned-hash-join-node.cc:210
#13 0x00000000015d125a in impala::ExecNode::Reset (this=0xf20a6e0, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/exec-node.cc:166
#14 0x00000000016ce85b in impala::SubplanNode::Reset (this=0xf20a6e0, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/subplan-node.cc:136
#15 0x00000000015d125a in impala::ExecNode::Reset (this=0xf217680, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/exec-node.cc:166
#16 0x000000000168fb70 in impala::NestedLoopJoinNode::Reset (this=0xf217680, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/nested-loop-join-node.cc:97
#17 0x00000000016ce1a6 in impala::SubplanNode::GetNext (this=0xf20a000, state=0xbd2fb00, row_batch=0x7fe97d1e9f40, eos=0x7fe97d1e9e8e) at /home/dev/Impala/be/src/exec/subplan-node.cc:81
#18 0x0000000001699a5c in impala::PartitionedAggregationNode::Open (this=0xf206c00, state=0xbd2fb00) at /home/dev/Impala/be/src/exec/partitioned-aggregation-node.cc:241
#19 0x0000000001597bf3 in impala::PlanFragmentExecutor::OpenInternal (this=0xbd30f28) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:334
#20 0x00000000015979f8 in impala::PlanFragmentExecutor::Open (this=0xbd30f28) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:320
#21 0x000000000137d290 in impala::FragmentMgr::FragmentExecState::Exec (this=0xbd30d00) at /home/dev/Impala/be/src/service/fragment-exec-state.cc:50
#22 0x0000000001375dcf in impala::FragmentMgr::FragmentExecThread (this=0xd5f6d80, exec_state=0xbd30d00) at /home/dev/Impala/be/src/service/fragment-mgr.cc:70
#23 0x00000000013795da in boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>::operator() (this=0x11727420, p=0xd5f6d80, a1=0xbd30d00) at /usr/include/boost/bind/mem_fn_template.hpp:165
#24 0x000000000137939d in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list0> (this=0x11727430, f=..., a=...) at /usr/include/boost/bind/bind.hpp:313
#25 0x0000000001378cb7 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >::operator() (this=0x11727420) at /usr/include/boost/bind/bind_template.hpp:20
#26 0x000000000137860c in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
#27 0x00000000012479d8 in boost::function0<void>::operator() (this=0x7fe97d1eae00) at /usr/include/boost/function/function_template.hpp:767
#28 0x0000000001457233 in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7fe97e26bea0) at /home/dev/Impala/be/src/util/thread.cc:314
#29 0x0000000001460469 in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0x1057c1c0, f=@0x1057c1b8: 0x1456f2a <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>, a=...) at /usr/include/boost/bind/bind.hpp:457
#30 0x00000000014603b3 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0x1057c1b8) at /usr/include/boost/bind/bind_template.hpp:20
#31 0x0000000001460378 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0x1057c000) at /usr/include/boost/thread/detail/thread.hpp:117
#32 0x00007fea350eba4a in ?? () from /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0
#33 0x00007fea34598182 in start_thread (arg=0x7fe97d1eb700) at pthread_create.c:312
#34 0x00007fea321e547d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{code}

impalad.FATAL:
{code}
F0918 12:55:08.053592 20580 sorter.cc:1070] Check failed: unsorted_run_ == __null
{code}

Steps to reproduce:
{code}
ssh dev@vd0206.halxg.cloudera.com -p 33333 (pw: cloudera)
run-all.sh && start-impala-cluster.py && impala-shell.sh;
use random_nested_db_0;
Core dumps are saved to /tmp/core_files on the machine
{code}"	IMPALA	Resolved	2	1	1880	crash, nested_types, query_generator
13052742	Precondition failure in EvalConstExpr evaluating integer limits in UNION	"A Precondition failed in EvalConstExpr while evaluating integer limits in a reasonable query with a UNION. This does not happen repeatedly, but this failed on a recent jenkins test run:
http://sandbox.jenkins.cloudera.com/job/impala-cdh5-2.2.0_5.4.x-repeated-runs/124/testReport/junit/failure/test_failpoints/TestFailpoints/

test_failpoints failed while attempting to EXPLAIN the following query:
{code}
select a.int_col, count(b.int_col) int_sum from functional_hbase.alltypesagg a
join
  (select * from alltypes
   where year=2009 and month=1 order by int_col limit 2500
   union all
   select * from alltypes
   where year=2009 and month=2 limit 3000) b
on (a.int_col = b.int_col)
group by a.int_col
order by int_sum
{code}


Analysis failed due to the Precondition failure:
{code}
I0704 15:38:52.268142 13995 jni-util.cc:177] com.cloudera.impala.common.AnalysisException
        at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:339)
        at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:294)
        at com.cloudera.impala.service.Frontend.analyzeStmt(Frontend.java:784)
        at com.cloudera.impala.service.Frontend.createExecRequest(Frontend.java:813)
        at com.cloudera.impala.service.JniFrontend.createExecRequest(JniFrontend.java:147)
Caused by: java.lang.IllegalStateException
        at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
        at com.cloudera.impala.service.FeSupport.EvalConstExpr(FeSupport.java:136)
        at com.cloudera.impala.analysis.LimitElement.evalIntegerExpr(LimitElement.java:140)
        at com.cloudera.impala.analysis.LimitElement.analyze(LimitElement.java:111)
        at com.cloudera.impala.analysis.QueryStmt.analyzeLimit(QueryStmt.java:106)
        at com.cloudera.impala.analysis.QueryStmt.analyze(QueryStmt.java:97)
        at com.cloudera.impala.analysis.SelectStmt.analyze(SelectStmt.java:137)
        at com.cloudera.impala.analysis.UnionStmt$UnionOperand.analyze(UnionStmt.java:73)
        at com.cloudera.impala.analysis.UnionStmt.analyze(UnionStmt.java:175)
        at com.cloudera.impala.analysis.InlineViewRef.analyze(InlineViewRef.java:129)
        at com.cloudera.impala.analysis.SelectStmt.analyze(SelectStmt.java:149)
        at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:319)
        ... 4 more
{code}


Seems odd that {{LimitElement.evalIntegerExpr()}} would fail since the limits are both integer literals.


Logs are available in the jenkins artifacts."	IMPALA	Resolved	3	1	1880	planner
13052839	Flaky test: test_shell_commandline.TestImpalaShell.test_queries_closed failures	"See http://sandbox.jenkins.cloudera.com/job/impala-s3/200/, also http://sandbox.jenkins.cloudera.com/job/impala-s3/198.

Not clear how this is S3 related, but it only shows up for S3 right now.

{code}
shell.test_shell_commandline.TestImpalaShell.test_queries_closed (from pytest)

Failing for the past 1 build (Since Failed#200 )
Took 5 sec.
add description
Error Message

assert 1 == 0  +  where 0 = <bound method ImpaladService.get_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x4082a50>>()  +    where <bound method ImpaladService.get_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x4082a50>> = <tests.common.impala_service.ImpaladService object at 0x4082a50>.get_num_in_flight_queries
Stacktrace

shell/test_shell_commandline.py:295: in test_queries_closed
    assert 1 == impalad.get_num_in_flight_queries()
E   assert 1 == 0
E    +  where 0 = <bound method ImpaladService.get_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x4082a50>>()
E    +    where <bound method ImpaladService.get_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x4082a50>> = <tests.common.impala_service.ImpaladService object at 0x4082a50>.get_num_in_flight_queries
Standard Error

MainThread: Found 3 impalad/1 statestored/1 catalogd process(es)
MainThread: Getting num_in_flight_queries from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
{code}"	IMPALA	Resolved	2	1	1880	broken-build, test-infra
13051594	Planner cardinality estimates from joins can be improved.	"I think we assume n:1 joins so the cardinality is not reduced after the join operator

Here's an example from tpcds q19 but it applies to almost all the tpcds queries.

{code}
Operator          Detail          #Hosts      Avg Time      Max Time         #Rows    Est. #Rows      Peak Mem   Est. Peak Mem
------------------------------------------------------------------------------------------------------------------------------
18:TOP-N                               1       3.708ms       3.708ms           100           100     117.44 KB         -1.00 B
17:EXCHANGE                            1     236.149us     236.149us          1000           100       9.77 KB         -1.00 B
10:TOP-N                              10     555.602us      591.50us          1000           100      36.00 KB         4.69 KB
16:AGGREGATE                          10       4.950ms        5.85ms          3227     264233526     184.84 KB         6.75 GB
15:EXCHANGE                           10      14s941ms      14s952ms         32270     264233526      30.09 KB               0
09:AGGREGATE                          10     201.615ms     280.335ms         32270     264233526      13.68 MB        12.99 GB
08:HASH JOIN      BROADCAST           10     112.415ms     168.229ms       4646655     264233526      12.32 MB        42.00 KB                       
|--14:EXCHANGE                        10     286.415ms     296.645ms          1350          1350      44.96 KB               0
|  04:SCAN HDFS   store                1      39.760ms      39.760ms          1350          1350     243.32 KB        32.00 MB
07:HASH JOIN      BROADCAST           10       3s519ms       3s900ms       4708900     264233526     993.89 MB       453.97 MB
|--13:EXCHANGE                        10       2s731ms       2s912ms      15000000      15000000      21.86 MB               0
|  03:SCAN HDFS   customer_address     8     328.503ms     862.653ms      15000000      15000000      26.30 MB        80.00 MB
06:HASH JOIN      BROADCAST           10      16s891ms      17s963ms       4708900     264233526       1.44 GB       377.66 MB
|--12:EXCHANGE                        10       3s785ms       4s422ms      30000000      30000000      17.84 MB               0
|  02:SCAN HDFS   customer             9     292.638ms       1s030ms      30000000      30000000      40.81 MB       176.00 MB
05:HASH JOIN      BROADCAST           10       2s903ms       3s397ms       4823041     264233526       1.10 MB       292.41 KB                       <-- We assumed all probe rows are returned and now the estimate is off by a factor of 54.
|--11:EXCHANGE                        10       1s629ms       1s642ms          6478          3429      99.51 KB               0      
|  01:SCAN HDFS   item                 1     284.503ms     284.503ms          6478          3429       7.76 MB       288.00 MB                       <-- The estimate of the first dim table is close
00:SCAN HDFS      store_sales         10     134.410ms      227.81ms     264233526     264233526     446.98 MB       352.00 MB                       <-- We have perfect stats on the left most table.
{code}"	IMPALA	Resolved	2	1	1880	performance
13055042	Wrong results with several conjunctive EXISTS subqueries that can be evaluated at query-compile time.	"Queries with several AND-ed EXISTS subqueries in the WHERE clause may produce incorrect results if some of the subqueries can be evaluated at query compile time.

Repro with wrong plan:
{code}
select 1
from functional.alltypestiny t1
where not exists
  (select id
   from functional.alltypes t2
   where t1.int_col = t2.int_col limit 0)
and not exists <-- this subquery should be folded to ""FALSE""
  (select min(int_col)
   from functional.alltypestiny t5
   where t1.id = t5.id and false)

+-----------------------------------------------------+
| Explain String                                      |
+-----------------------------------------------------+
| Estimated Per-Host Requirements: Memory=0B VCores=0 |
|                                                     |
| PLAN-ROOT SINK                                      |
| |                                                   |
| 00:SCAN HDFS [functional.alltypestiny t1]           |
|    partitions=4/4 files=4 size=460B                 |
+-----------------------------------------------------+
{code}

Same query as above but flipping the order of subqueries gives the correct plan:
{code}
select 1
from functional.alltypestiny t1
where not exists
  (select min(int_col)
   from functional.alltypestiny t5
   where t1.id = t5.id and false)
and not exists
  (select id
   from functional.alltypes t2
   where t1.int_col = t2.int_col limit 0)

+---------------------------------------------------------+
| Explain String                                          |
+---------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=1.00KB VCores=1 |
|                                                         |
| PLAN-ROOT SINK                                          |
| |                                                       |
| 00:EMPTYSET                                             |
+---------------------------------------------------------+
{code}

The underlying problem is that we substitute out the subqueries with constant literals using an ExprSubstitutionMap, but the Subquery.equals() function is not implemented properly, so the second subquery is replaced with whatever boolean literal corresponds to the first subquery."	IMPALA	Resolved	1	1	1880	correctness
13054571	Crash in HdfsParquetScanner::Close()	"http://sandbox.jenkins.cloudera.com/job/impala-cdh5-trunk-core-non-partitioned-joins-and-aggs/127/

{code}
lf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [shaded.parquet.org.slf4j.helpers.NOPLoggerFactory]
F0805 06:28:56.384275 13891 hdfs-parquet-scanner.cc:206] Check failed: scratch_batch_->mem_pool()->total_allocated_bytes() == 0 (8192 vs. 0) 
*** Check failure stack trace: ***
F0805 06:28:56.408095 13886 hdfs-parquet-scanner.cc:206] Check failed: scratch_batch_->mem_pool()->total_allocated_bytes() == 0 (8192 vs. 0) 
*** Check failure stack trace: ***
    @          0x281bffd  google::LogMessage::Fail()
    @          0x281bffd  google::LogMessage::Fail()
    @          0x281e926  google::LogMessage::SendToLog()
    @          0x281e926  google::LogMessage::SendToLog()
    @          0x281bb1d  google::LogMessage::Flush()
    @          0x281bb1d  google::LogMessage::Flush()
    @          0x281f3ce  google::LogMessageFatal::~LogMessageFatal()
    @          0x281f3ce  google::LogMessageFatal::~LogMessageFatal()
    @          0x16dbbd5  impala::HdfsParquetScanner::Close()
    @          0x16dbbd5  impala::HdfsParquetScanner::Close()
    @          0x169107d  impala::HdfsScanNode::CreateAndOpenScanner()
    @          0x169107d  impala::HdfsScanNode::CreateAndOpenScanner()
    @          0x16977da  impala::HdfsScanNode::ProcessSplit()
    @          0x16977da  impala::HdfsScanNode::ProcessSplit()
    @          0x1696c30  impala::HdfsScanNode::ScannerThread()
    @          0x1696c30  impala::HdfsScanNode::ScannerThread()
    @          0x16b2e5b  boost::_mfi::mf0<>::operator()()
    @          0x16b2e5b  boost::_mfi::mf0<>::operator()()
    @          0x16b1fde  boost::_bi::list1<>::operator()<>()
    @          0x16b1fde  boost::_bi::list1<>::operator()<>()
    @          0x16b0a8d  boost::_bi::bind_t<>::operator()()
    @          0x16b0a8d  boost::_bi::bind_t<>::operator()()
    @          0x16ae8d6  boost::detail::function::void_function_obj_invoker0<>::invoke()
    @          0x16ae8d6  boost::detail::function::void_function_obj_invoker0<>::invoke()
    @          0x135592a  boost::function0<>::operator()()
    @          0x135592a  boost::function0<>::operator()()
    @          0x16006db  impala::Thread::SuperviseThread()
    @          0x16070f4  boost::_bi::list4<>::operator()<>()
    @          0x16006db  impala::Thread::SuperviseThread()
    @          0x1607037  boost::_bi::bind_t<>::operator()()
    @          0x16070f4  boost::_bi::list4<>::operator()<>()
    @          0x1606f92  boost::detail::thread_data<>::run()
    @          0x1a4da3a  thread_proxy
    @       0x3f61407851  (unknown)
    @       0x3f610e894d  (unknown)
Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30000,server=y,suspend=n 
    @          0x1607037  boost::_bi::bind_t<>::operator()()
    @          0x1606f92  boost::detail::thread_data<>::run()
    @          0x1a4da3a  thread_proxy
    @       0x3f61407851  (unknown)
    @       0x3f610e894d  (unknown)
Wrote minidump to /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/logs/ee_tests/minidumps/impalad/7a9916a7-ef7c-f733-520afb81-66713e07.dmp
{code}"	IMPALA	Resolved	1	1	1880	crash
13052547	Shell hangs on certain queries against CSV table	"I have a CSV table with 14 columns, about 44 million rows. I found that with SELECT * and certain conditions in the WHERE clause, the shell would seem to output all the results (I would see the +------+ line at the bottom of the result table) but then would hang and never return to the impala-shell prompt.

If I SELECT COUNT(*) with the same WHERE clause, it works.
If I SELECT DISTINCT <one of the columns> with the same where clause, it works.
If I do a CTAS into a Parquet table with SELECT * from the original table (no WHERE clause), it works.

However, if I do the same query via 'impala-shell -q' or with delimited results via 'impala-shell -B -q', it hangs the same way.

When I hit Ctrl-C, the resulting message is:

^C Cancelling Query
Failed to reconnect and close: ERROR: Cancelled

 Cancelling Query

which is different than what I saw when cancelling other SELECT or INSERT statements that were still in progress. This seems like the query has finished but the shell doesn't close it properly. The equivalent COUNT(*) and DISTINCT queries finish in 1-2 seconds. I let the SELECT * run for 3 minutes or more and it stays stuck.

It's possible there is some anomaly somewhere in the CSV files, but like I say I can CTAS the whole contents of the table. It's only outputting in the shell that stalls.

Schema, stalled query, actual data, profiles, logs all available on the Cloudera network for diagnosis. (Ping me for the location.)"	IMPALA	Resolved	2	1	1880	shell
13054479	Cannot resolve correlated column reference inside a BETWEEN predicate.	"Impala fails to properly resolve correlated column references in a BETWEEN predicate.
This is a regression from Impala 2.2, introduced in 

Consider the following query with a correlated BETWEEN predicate in an IN subquery:
{code}
select 1 from functional.alltypes t1
where t1.id in
  (select test_id from functional.jointbl t2
   where t2.test_id between t1.tinyint_col and t1.int_col)
ERROR: AnalysisException: Could not resolve column/field reference: 't1.tinyint_col'

select 1 from functional.alltypes t1
where t1.id in
  (select test_id from functional.jointbl t2
   where t2.test_id between tinyint_col and int_col)
ERROR: AnalysisException: Could not resolve column/field reference: 'tinyint_col'
{code}

*Workaround*
Rewrite the BETWEEN predicate as an explicit range condition:
{code}
select 1 from functional.alltypes t1
where t1.id in
  (select test_id from functional.jointbl t2
   where t2.test_id >= t1.tinyint_col and t2.test_id <= t1.int_col)
{code}"	IMPALA	Resolved	1	1	1880	ramp-up
13140801	FrontendTest.TestGetTablesTypeTable failing on some builds	"{noformat}
org.apache.impala.service.FrontendTest.TestGetTablesTypeTable
Failing for the past 1 build (Since Failed#45 )
Took 9 ms.
add description
Error Message

expected:<1> but was:<2>

Stacktrace

java.lang.AssertionError: expected:<1> but was:<2>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.impala.service.FrontendTest.TestGetTablesTypeTable(FrontendTest.java:117)
{noformat}"	IMPALA	Resolved	1	1	1880	broken-build
13052939	Incorrect results with sending a collection-typed slot through a hash exchange.	"The following query returns incorrect results:
{code}
select straight_join t1.id, m.key
from complextypestbl t1 join [shuffle] complextypestbl t2, t2.int_map m
where t1.id = t2.id

Results:
+----+-----+
| id | key |
+----+-----+
| 8  | k1  |
| 1  |     |
| 1  | k2  |
| 2  |     |
| 2  | k2  |
| 7  |     |
| 7  | k3  |
+----+-----+
{code}

The same query run with a broadcast join returns correct results:
{code}
select straight_join t1.id, m.key
from complextypestbl t1 join [broadcast] complextypestbl t2, t2.int_map m
where t1.id = t2.id

Results:
+----+-----+
| id | key |
+----+-----+
| 8  | k1  |
| 1  | k1  |
| 1  | k2  |
| 2  | k1  |
| 2  | k2  |
| 7  | k1  |
| 7  | k3  |
+----+-----+
{code}"	IMPALA	Resolved	1	1	1880	correctness
13055656	Change default Parquet array resolution according to Parquet standard.	"With IMPALA-4725 we've introduced query options to control the field resolution behavior when scanning Parquet files with nested arrays. The current default strategy currently tries to auto-detect the array encoding within Parquet files, but this strategy can sometimes subtly go wrong and return incorrect results due to the inherent ambiguity of the 2/3-level encoding schemes in Parquet.

We should switch the default resolution strategy according to the Parquet standard 3-level encoding, instead of the current auto-detect."	IMPALA	Resolved	3	4	1880	include-in-v3, incompatibility
13054559	Different query plans produced when accessing base tables or identical views.	"There seems to be a bug related to join cardinality estimation when views are involved.

Different plans are produced when running the same query against either base tables or identical views. The view plan is worse and the join-cardinality estimates in the view plan are consistent with not being able to find the column stats from the view's base table columns.

Plan against base tables:
{code}
select count(a.int_col) from
functional.alltypessmall a
inner join functional.alltypes b on (a.id = b.id)
inner join functional.alltypestiny c on (b.id = c.id)
+-----------------------------------------------------------+
| Explain String                                            |
+-----------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=170.00MB VCores=4 |
|                                                           |
| 10:AGGREGATE [FINALIZE]                                   |
| |  output: count:merge(a.int_col)                         |
| |                                                         |
| 09:EXCHANGE [UNPARTITIONED]                               |
| |                                                         |
| 05:AGGREGATE                                              |
| |  output: count(a.int_col)                               |
| |                                                         |
| 04:HASH JOIN [INNER JOIN, PARTITIONED]                    |
| |  hash predicates: b.id = a.id                           |
| |  runtime filters: RF000 <- a.id                         |
| |                                                         |
| |--08:EXCHANGE [HASH(a.id)]                               |
| |  |                                                      |
| |  00:SCAN HDFS [functional.alltypessmall a]              |
| |     partitions=4/4 files=4 size=6.32KB                  |
| |                                                         |
| 07:EXCHANGE [HASH(b.id)]                                  |
| |                                                         |
| 03:HASH JOIN [INNER JOIN, BROADCAST]                      |
| |  hash predicates: b.id = c.id                           |
| |  runtime filters: RF001 <- c.id                         |
| |                                                         |
| |--06:EXCHANGE [BROADCAST]                                |
| |  |                                                      |
| |  02:SCAN HDFS [functional.alltypestiny c]               |
| |     partitions=4/4 files=4 size=460B                    |
| |     runtime filters: RF000 -> c.id                      |
| |                                                         |
| 01:SCAN HDFS [functional.alltypes b]                      |
|    partitions=24/24 files=24 size=478.45KB                |
|    runtime filters: RF000 -> b.id, RF001 -> b.id          |
+-----------------------------------------------------------+
{code}

Plan against views created with CREATE VIEW as SELECT * FROM basetable:
{code}
select count(a.int_col) from
alltypessmall_view a
inner join alltypes_view b on (a.id = b.id)
inner join alltypestiny_view c on (b.id = c.id);
+--------------------------------------------------------------------------------------+
| Explain String                                                                       |
+--------------------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=170.00MB VCores=3                            |
|                                                                                      |
| 09:AGGREGATE [FINALIZE]                                                              |
| |  output: count:merge(a.int_col)                                                    |
| |                                                                                    |
| 08:EXCHANGE [UNPARTITIONED]                                                          |
| |                                                                                    |
| 05:AGGREGATE                                                                         |
| |  output: count(functional.alltypessmall.int_col)                                   |
| |                                                                                    |
| 04:HASH JOIN [INNER JOIN, BROADCAST]                                                 |
| |  hash predicates: functional.alltypes.id = functional.alltypestiny.id              |
| |  runtime filters: RF000 <- functional.alltypestiny.id                              |
| |                                                                                    |
| |--07:EXCHANGE [BROADCAST]                                                           |
| |  |                                                                                 |
| |  02:SCAN HDFS [functional.alltypestiny]                                            |
| |     partitions=4/4 files=4 size=460B                                               |
| |                                                                                    |
| 03:HASH JOIN [INNER JOIN, BROADCAST]                                                 |
| |  hash predicates: functional.alltypes.id = functional.alltypessmall.id             |
| |  runtime filters: RF001 <- functional.alltypessmall.id                             |
| |                                                                                    |
| |--06:EXCHANGE [BROADCAST]                                                           |
| |  |                                                                                 |
| |  00:SCAN HDFS [functional.alltypessmall]                                           |
| |     partitions=4/4 files=4 size=6.32KB                                             |
| |     runtime filters: RF000 -> functional.alltypessmall.id                          |
| |                                                                                    |
| 01:SCAN HDFS [functional.alltypes]                                                   |
|    partitions=24/24 files=24 size=478.45KB                                           |
|    runtime filters: RF000 -> functional.alltypes.id, RF001 -> functional.alltypes.id |
+--------------------------------------------------------------------------------------+
{code}

This is most likely regression from Impala 2.5, possibly introduced by IMPALA-976 (not yet confirmed)."	IMPALA	Resolved	1	1	1880	performance, ramp-up, regression, usability
13051980	Incorrect plan after pushing predicate into inline view with FULL OUTER JOIN	"The plan below is incorrect. The predicate ""WHERE t3.boolean_col_1"" in the outer query is incorrectly pushed into the table scan with the FULL OUTER JOIN unchanged.

{noformat}
Query: 
explain 
select t3.boolean_col_1, COUNT(1)
FROM
  (SELECT t2.bool_col AS boolean_col_1
   FROM alltypes t1 
   FULL OUTER JOIN alltypestiny t2 ON t2.string_col = t1.string_col) t3
WHERE t3.boolean_col_1
GROUP BY t3.boolean_col_1
+-----------------------------------------------------------+
| Explain String                                            |
+-----------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=170.00MB VCores=2 |
|                                                           |
| 08:EXCHANGE [UNPARTITIONED]                               |
| |                                                         |
| 07:AGGREGATE [FINALIZE]                                   |
| |  output: count:merge(1)                                 |
| |  group by: t3.boolean_col_1                             |
| |                                                         |
| 06:EXCHANGE [HASH(t3.boolean_col_1)]                      |
| |                                                         |
| 03:AGGREGATE                                              |
| |  output: count(1)                                       |
| |  group by: t2.bool_col                                  |
| |                                                         |
| 02:HASH JOIN [FULL OUTER JOIN, PARTITIONED]               |
| |  hash predicates: t1.string_col = t2.string_col         |
| |                                                         |
| |--05:EXCHANGE [HASH(t2.string_col)]                      |
| |  |                                                      |
| |  01:SCAN HDFS [functional.alltypestiny t2]              |
| |     partitions=4/4 size=460B                            |
| |     predicates: t2.bool_col                             |
| |                                                         |
| 04:EXCHANGE [HASH(t1.string_col)]                         |
| |                                                         |
| 00:SCAN HDFS [functional.alltypes t1]                     |
|    partitions=24/24 size=478.45KB                         |
+-----------------------------------------------------------+
Fetched 27 row(s) in 0.01s
{noformat}

The first row below should not be returned.

{noformat}
Query: select t3.boolean_col_1, COUNT(1)
FROM
(SELECT t2.bool_col AS boolean_col_1
FROM alltypes t1 FULL
OUTER JOIN alltypestiny t2 ON t2.string_col = t1.string_col) t3
WHERE t3.boolean_col_1
GROUP BY t3.boolean_col_1
+---------------+----------+
| boolean_col_1 | count(1) |
+---------------+----------+
| NULL          | 6570     |
| true          | 2920     |
+---------------+----------+
Fetched 2 row(s) in 0.89s
{noformat}"	IMPALA	Resolved	3	1	1880	correctness
13055193	The uuid() function should not be treated as a constant expr.	"IMPALA-1788 introduced constant folding in the FE, but that change did not consider that the function UUID() is not a constant expression. The behavioral regression is that UUID() will only be evaluated once. Example:

{code}
select uuid() from functional.alltypestiny;
+--------------------------------------+
| uuid()                               |
+--------------------------------------+
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
| 9801bcbc-7b63-4703-bb48-221449d0d41e |
+--------------------------------------+
{code}

Similarly, there are places in the BE (scalar-fn-call.h/cc) where we should not treat UUID() as constant."	IMPALA	Resolved	1	1	1880	correctness, regression
13098202	Queries with full outer and left join miss result rows	"When combining a full outer join with a left join, some of the left join predicates seem to be treated as general WHERE-clauses, which leads to missing rows. Minimal working example:
{code:sql}
create table A (a int, av int);
create table B (a int, bv int);
create table C (a int, cv int);

insert into A values (1,1), (2,2), (3,3);
insert into B values (2,22),(4,44);
insert into C values (2,222);

-- all results are returned as expected
select * from A full outer join B on a.a=b.a left join C on (coalesce(a.a, b.a)=c.a);

-- only one row is returned, as if the last clause was a WHERE clause
select * from A full outer join B on a.a=b.a left join C on (coalesce(a.a, b.a)=c.a and coalesce(a.av,b.bv)=2);

-- no rows are returned at all, even though only the columns of C should be affected
select * from A full outer join B on a.a=b.a left join C on (coalesce(a.a, b.a)=c.a and coalesce(a.av,b.bv)=100);

-- removing the full outer join leads to the expected result
select * from A left join C on (coalesce(a.a)=c.a and coalesce(a.av)=100);
{code}

Running the exact same SQL in PostgreSQL, only the columns of C are ever affected by the left join ON condition, the number of rows never changes. As far as we understand, this should be the expected behaviour. "	IMPALA	Resolved	1	1	1880	correctness
13054988	DCHECK in Parquet scanner with MT_DOP > 1 when reading file with bad metadata.	"Repro:
{code}
1. Start impala cluster with MT_DOP > 0
2. ./run-tests.py query_test/test_scanners.py -k test_parquet --table_formats=parquet/none
{code}

DCHECK:
{code}
F1025 17:31:47.367564 29896 hdfs-scanner.h:155] Check failed: scan_node_->HasRowBatchQueue() 
{code}

Stack:
{code}
#7  0x000000000165d3ac in impala::HdfsScanner::batch (this=0xd1b5180) at /home/abehm/impala/be/src/exec/hdfs-scanner.h:155
#8  0x0000000001669a21 in impala::HdfsScanNodeBase::CreateAndOpenScanner (this=0xc0e9200, partition=0xf5f2600, context=0x8122870, scanner=0xc0e96c0)
    at /home/abehm/impala/be/src/exec/hdfs-scan-node-base.cc:638
#9  0x000000000167f495 in impala::HdfsScanNodeMt::GetNext (this=0xc0e9200, state=0xe546400, row_batch=0x10bcd960, eos=0x7f7df7726e7f)
    at /home/abehm/impala/be/src/exec/hdfs-scan-node-mt.cc:90
#10 0x00000000019b1780 in impala::PlanFragmentExecutor::ExecInternal (this=0xc9cca88) at /home/abehm/impala/be/src/runtime/plan-fragment-executor.cc:351
#11 0x00000000019b145f in impala::PlanFragmentExecutor::Exec (this=0xc9cca88) at /home/abehm/impala/be/src/runtime/plan-fragment-executor.cc:327
#12 0x000000000150f99a in impala::FragmentMgr::FragmentExecState::Exec (this=0xc9cc700) at /home/abehm/impala/be/src/service/fragment-exec-state.cc:59
#13 0x00000000015070e8 in impala::FragmentMgr::FragmentThread (this=0x83b2cc0, fragment_instance_id=...) at /home/abehm/impala/be/src/service/fragment-mgr.cc:86
#14 0x000000000150ae6a in boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>::operator() (this=0xe0b41e0, p=0x83b2cc0, a1=...)
    at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/bind/mem_fn_template.hpp:165
#15 0x000000000150ac27 in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list0> (this=0xe0b41f0, f=..., a=...)
    at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/bind/bind.hpp:313
#16 0x000000000150a551 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId> > >::operator() (this=0xe0b41e0)
    at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#17 0x0000000001509ee4 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId> > >, void>::invoke (function_obj_ptr=...)
    at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/function/function_template.hpp:153
#18 0x000000000130ca1a in boost::function0<void>::operator() (this=0x7f7df7727d30)
    at /home/abehm/impala/toolchain/boost-1.57.0/include/boost/function/function_template.hpp:767
{code}"	IMPALA	Resolved	3	1	1880	multithreading
13053454	Impala incorrectly reports an AuthorizationException when it is actually not ready yet to accept requests.	"When an impalad starts it must wait for an initial metadata update from the statestore before it can start accepting user requests.

When in this waiting state immediately after startup, Impala should report the following error in response to user requests:

""This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.""

See Analyzer.java:
{code}
  public ImpaladCatalog getCatalog() throws AnalysisException {
    if (!globalState_.catalog.isReady()) {
      throw new AnalysisException(""This Impala daemon is not ready to accept user "" +
          ""requests. Status: Waiting for catalog update from the StateStore."");
    }
    return globalState_.catalog;
  }
{code}

It looks like Impala has regressed in this respect, and in some cases incorrectly reports an AuthorizationException instead.

Reproduction on a local dev setup (replace paths/usernames appropriately):
1. Start Impala using the Sentry Service
{code}
./start-impala-cluster.py --impalad_args=""--sentry_config='/home/abehm/impala/fe/src/test/resources/sentry-site.xml' --server_name='server1'"" --catalogd_args=""--sentry_config='/home/abehm/impala/fe/src/test/resources/sentry-site.xml' --server_name='server1'""
{code}
2. Create a test role in the Impala Shell:
{code}
create role test_role;
grant all on table functional.alltypes to test_role;
grant role grant_revoke_test_ALL_SERVER to group abehm;
{code}
3. Try a query on functional.alltypes and verify that you can access it.
4. Now, add a sleep in catalogd to delay the initial metadata update. For example, put it in SentryProxy.java inside PolicyReader.run():
{code}
...
private class PolicyReader implements Runnable {
    public void run() {
      synchronized (SentryProxy.this) {
        try {
          for (int i = 0; i < 10; ++i) {
            System.out.println(""SLEEPING: "" + i);
            Thread.sleep(10000);
          }
        } catch (InterruptedException e1) {
          e1.printStackTrace();
        }
...
{code}
5. Re-start Impala with Sentry (re-run command from step 1)
6. Run a query against functional.alltypes, you'll get an AuthorizationException, even though the impalad has not received an initial metadata update yet"	IMPALA	Resolved	2	1	1880	ramp-up, usability
13054400	'describe formatted' output does not match Hive's output for Avro tables	"The comment in [DescribeResultFactory.java#L174|https://github.com/cloudera/Impala/blob/cdh5-trunk/fe/src/main/java/com/cloudera/impala/service/DescribeResultFactory.java#L174] sounds like we want to produce the same output for {{describe formatted}} as Hive. However I found the column comments to be different for {{functional_avro.alltypes}} (and probably other tables).

Hive:

{noformat}
$hive  -e ""describe formatted functional_avro.alltypes""
16/06/23 20:24:45 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/lv/i6/thirdparty/hbase-1.2.0-cdh5.8.0/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/lv/i6/thirdparty/hadoop-2.6.0-cdh5.8.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
16/06/23 20:24:46 WARN conf.HiveConf: HiveConf of name hive.access.conf.url does not exist

Logging initialized using configuration in file:/home/lv/i6/fe/src/test/resources/hive-log4j.properties
OK
# col_name              data_type               comment

id                      int
bool_col                boolean
tinyint_col             int
smallint_col            int
int_col                 int
bigint_col              bigint
float_col               float
double_col              double
date_string_col         string
string_col              string
timestamp_col           string

# Partition Information
# col_name              data_type               comment

year                    int
month                   int

# Detailed Table Information
Database:               functional_avro
Owner:                  lv
CreateTime:             Thu May 26 22:01:50 CEST 2016
LastAccessTime:         UNKNOWN
Protect Mode:           None
Retention:              0
Location:               hdfs://localhost:20500/test-warehouse/alltypes_avro
Table Type:             EXTERNAL_TABLE
Table Parameters:
        EXTERNAL                TRUE
        avro.schema.url         hdfs://localhost:20500//test-warehouse/avro_schemas/functional/alltypes.json
        transient_lastDdlTime   1464293060

# Storage Information
SerDe Library:          org.apache.hadoop.hive.serde2.avro.AvroSerDe
InputFormat:            org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat
OutputFormat:           org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat
Compressed:             No
Num Buckets:            0
Bucket Columns:         []
Sort Columns:           []
Storage Desc Params:
        escape.delim            \\
        field.delim             ,
        serialization.format    ,
Time taken: 1.897 seconds, Fetched: 46 row(s)
{noformat}

Impala:

{noformat}
$impala-shell.sh -q ""describe formatted functional_avro.alltypes""
Starting Impala Shell without Kerberos authentication
Connected to localhost:21000
Server version: impalad version 2.6.0-cdh5-INTERNAL DEBUG (build 338ba601c8b801bb83ecedd7306a5f7e2062797a)
Query: describe formatted functional_avro.alltypes
+------------------------------+-------------------------------------------------------------+------------------------------------------------------------------------------+
| name                         | type                                                        | comment                                                                      |
+------------------------------+-------------------------------------------------------------+------------------------------------------------------------------------------+
| # col_name                   | data_type                                                   | comment                                                                      |
|                              | NULL                                                        | NULL                                                                         |
| id                           | int                                                         | from deserializer                                                            |
| bool_col                     | boolean                                                     | from deserializer                                                            |
| tinyint_col                  | int                                                         | from deserializer                                                            |
| smallint_col                 | int                                                         | from deserializer                                                            |
| int_col                      | int                                                         | from deserializer                                                            |
| bigint_col                   | bigint                                                      | from deserializer                                                            |
| float_col                    | float                                                       | from deserializer                                                            |
| double_col                   | double                                                      | from deserializer                                                            |
| date_string_col              | string                                                      | from deserializer                                                            |
| string_col                   | string                                                      | from deserializer                                                            |
| timestamp_col                | string                                                      | from deserializer                                                            |
|                              | NULL                                                        | NULL                                                                         |
| # Partition Information      | NULL                                                        | NULL                                                                         |
| # col_name                   | data_type                                                   | comment                                                                      |
|                              | NULL                                                        | NULL                                                                         |
| year                         | int                                                         | NULL                                                                         |
| month                        | int                                                         | NULL                                                                         |
|                              | NULL                                                        | NULL                                                                         |
| # Detailed Table Information | NULL                                                        | NULL                                                                         |
| Database:                    | functional_avro                                             | NULL                                                                         |
| Owner:                       | lv                                                          | NULL                                                                         |
| CreateTime:                  | Thu May 26 22:01:50 CEST 2016                               | NULL                                                                         |
| LastAccessTime:              | UNKNOWN                                                     | NULL                                                                         |
| Protect Mode:                | None                                                        | NULL                                                                         |
| Retention:                   | 0                                                           | NULL                                                                         |
| Location:                    | hdfs://localhost:20500/test-warehouse/alltypes_avro         | NULL                                                                         |
| Table Type:                  | EXTERNAL_TABLE                                              | NULL                                                                         |
| Table Parameters:            | NULL                                                        | NULL                                                                         |
|                              | EXTERNAL                                                    | TRUE                                                                         |
|                              | avro.schema.url                                             | hdfs://localhost:20500//test-warehouse/avro_schemas/functional/alltypes.json |
|                              | transient_lastDdlTime                                       | 1464293060                                                                   |
|                              | NULL                                                        | NULL                                                                         |
| # Storage Information        | NULL                                                        | NULL                                                                         |
| SerDe Library:               | org.apache.hadoop.hive.serde2.avro.AvroSerDe                | NULL                                                                         |
| InputFormat:                 | org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat  | NULL                                                                         |
| OutputFormat:                | org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat | NULL                                                                         |
| Compressed:                  | No                                                          | NULL                                                                         |
| Num Buckets:                 | 0                                                           | NULL                                                                         |
| Bucket Columns:              | []                                                          | NULL                                                                         |
| Sort Columns:                | []                                                          | NULL                                                                         |
| Storage Desc Params:         | NULL                                                        | NULL                                                                         |
|                              | escape.delim                                                | \\                                                                           |
|                              | field.delim                                                 | ,                                                                            |
|                              | serialization.format                                        | ,                                                                            |
+------------------------------+-------------------------------------------------------------+------------------------------------------------------------------------------+
Fetched 46 row(s) in 0.01s
{noformat}

[~alex.behm], what is the expected behavior here? Should we omit outputting these comments?"	IMPALA	Open	4	1	1880	usability
13055109	Check failed: false Unexpected plan node with runtime filters	"The random query generator triggered the DCHECK in the summary with this stack:

{noformat}
#0  0x00007f41a827cc37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f41a8280028 in __GI_abort () at abort.c:89
#2  0x0000000002807db4 in google::DumpStackTraceAndExit() ()
#3  0x000000000280121d in google::LogMessage::Fail() ()
#4  0x0000000002803b46 in google::LogMessage::SendToLog() ()
#5  0x0000000002800d3d in google::LogMessage::Flush() ()
#6  0x00000000028045ee in google::LogMessageFatal::~LogMessageFatal() ()
#7  0x000000000198e70b in impala::Coordinator::UpdateFilterRoutingTable (this=0xa2f8000, fragment_params=...)
    at /home/dev/Impala/be/src/runtime/coordinator.cc:573
#8  0x000000000198ed03 in impala::Coordinator::StartFInstances (this=0xa2f8000) at /home/dev/Impala/be/src/runtime/coordinator.cc:607
#9  0x000000000198d580 in impala::Coordinator::Exec (this=0xa2f8000) at /home/dev/Impala/be/src/runtime/coordinator.cc:485
#10 0x00000000014e2d06 in impala::ImpalaServer::QueryExecState::ExecQueryOrDmlRequest (this=0xf222000, query_exec_request=...)
    at /home/dev/Impala/be/src/service/query-exec-state.cc:445
#11 0x00000000014dfd99 in impala::ImpalaServer::QueryExecState::Exec (this=0xf222000, exec_request=0x7f4140f06940)
    at /home/dev/Impala/be/src/service/query-exec-state.cc:154
#12 0x000000000146d8b4 in impala::ImpalaServer::ExecuteInternal (this=0xf0ce600, query_ctx=..., session_state=..., registered_exec_state=0x7f4140f07f8f,
    exec_state=0x7f4140f08330) at /home/dev/Impala/be/src/service/impala-server.cc:815
#13 0x000000000146d10c in impala::ImpalaServer::Execute (this=0xf0ce600, query_ctx=0x7f4140f08020, session_state=..., exec_state=0x7f4140f08330)
    at /home/dev/Impala/be/src/service/impala-server.cc:762
#14 0x00000000014d4d96 in impala::ImpalaServer::query (this=0xf0ce600, query_handle=..., query=...)
    at /home/dev/Impala/be/src/service/impala-beeswax-server.cc:66
#15 0x000000000192907c in beeswax::BeeswaxServiceProcessor::process_query (this=0xee474a0, seqid=0, iprot=0xee74660, oprot=0xefcd830,
    callContext=0x10144180) at /home/dev/Impala/be/generated-sources/gen-cpp/BeeswaxService.cpp:2979
#16 0x0000000001928dca in beeswax::BeeswaxServiceProcessor::dispatchCall (this=0xee474a0, iprot=0xee74660, oprot=0xefcd830, fname=..., seqid=0,
    callContext=0x10144180) at /home/dev/Impala/be/generated-sources/gen-cpp/BeeswaxService.cpp:2952
#17 0x0000000001912bc7 in impala::ImpalaServiceProcessor::dispatchCall (this=0xee474a0, iprot=0xee74660, oprot=0xefcd830, fname=..., seqid=0,
    callContext=0x10144180) at /home/dev/Impala/be/generated-sources/gen-cpp/ImpalaService.cpp:1673
#18 0x00000000011630fc in apache::thrift::TDispatchProcessor::process (this=0xee474a0, in=..., out=..., connectionContext=0x10144180)
    at /opt/Impala-Toolchain/thrift-0.9.0-p8/include/thrift/TDispatchProcessor.h:121
#19 0x00000000027b40eb in apache::thrift::server::TThreadPoolServer::Task::run() ()
#20 0x000000000279c959 in apache::thrift::concurrency::ThreadManager::Worker::run() ()
#21 0x000000000132719f in impala::ThriftThread::RunRunnable (this=0x1013c640, runnable=..., promise=0x7fff332e0650)
    at /home/dev/Impala/be/src/rpc/thrift-thread.cc:64
#22 0x00000000013288cb in boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x10150030, p=0x1013c640, a1=..., a2=0x7fff332e0650) at /opt/Impala-Toolchain/boost-1.57.0/include/boost/bind/mem_fn_template.hpp:280
#23 0x0000000001328761 in boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x10150040, f=..., a=...)
    at /opt/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:392
#24 0x00000000013284ad in boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x10150030)
    at /opt/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#25 0x00000000013283c0 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...)
    at /opt/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:153
#26 0x0000000001335d3e in boost::function0<void>::operator() (this=0x7f4140f08da0)
    at /opt/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:767
#27 0x00000000015e017d in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (
    name=..., category=..., functor=..., thread_started=0x7fff332e0440) at /home/dev/Impala/be/src/util/thread.cc:317
#28 0x00000000015e7156 in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0xf2855c0,
    f=@0xf2855b8: 0x15dfeb8 <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*)>,
    a=...) at /opt/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:457
#29 0x00000000015e7099 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0xf2855b8) at /opt/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#30 0x00000000015e6ff4 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0xf285400) at /opt/Impala-Toolchain/boost-1.57.0/include/boost/thread/detail/thread.hpp:116
#31 0x0000000001a3282a in thread_proxy ()
#32 0x00007f41a8613184 in start_thread (arg=0x7f4140f09700) at pthread_create.c:312
#33 0x00007f41a834037d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{noformat}

The query is:
{noformat}
USE randomness;

SELECT
MIN(a1.int_col_32) AS int_col,
MIN(a2.timestamp_col_17) + INTERVAL COALESCE(370, -275, 254) MONTH AS timestamp_col
FROM table_2 a1
LEFT JOIN table_10 a2 ON (a1.tinyint_col_11) IS DISTINCT FROM (a2.int_col_22)
WHERE
((a1.int_col_32) - (a1.bigint_col_6)) = (a2.int_col_22);
{noformat}

Plan:

{noformat}
+--------------------------------------------------------------------------+
| Explain String                                                           |
+--------------------------------------------------------------------------+
| Estimated Per-Host Requirements: Memory=430.00MB VCores=2                |
|                                                                          |
| PLAN-ROOT SINK                                                           |
| |                                                                        |
| 06:AGGREGATE [FINALIZE]                                                  |
| |  output: min:merge(a1.int_col_32), min:merge(a2.timestamp_col_17)      |
| |                                                                        |
| 05:EXCHANGE [UNPARTITIONED]                                              |
| |                                                                        |
| 03:AGGREGATE                                                             |
| |  output: min(a1.int_col_32), min(a2.timestamp_col_17)                  |
| |                                                                        |
| 02:NESTED LOOP JOIN [LEFT OUTER JOIN, BROADCAST]                         |
| |  join predicates: (a1.tinyint_col_11) IS DISTINCT FROM (a2.int_col_22) |
| |  predicates: ((a1.int_col_32) - (a1.bigint_col_6)) = (a2.int_col_22)   |
| |                                                                        |
| |--04:EXCHANGE [BROADCAST]                                               |
| |  |                                                                     |
| |  01:SCAN HDFS [randomness.table_10 a2]                                 |
| |     partitions=1/1 files=1 size=199.35MB                               |
| |                                                                        |
| 00:SCAN HDFS [randomness.table_2 a1]                                     |
|    partitions=1/1 files=1 size=68.25MB                                   |
|    runtime filters: RF000 -> ((a1.int_col_32) - (a1.bigint_col_6))       |
+--------------------------------------------------------------------------+
{noformat}

The logs include the multi-hundred-line text dump of the plan node, which is part of the DCHECK string. It seemed too long to paste or comb through here.

There's also a running Docker container that has the randomness database loaded. In that container, I was able to reproduce the DCHECK using the query above."	IMPALA	Resolved	1	1	1880	crash, query_generator
13053472	put: File /test-warehouse/widetable_500_cols/widetable_data.csv._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1)	"A build failed with HDFS replication errors:

http://sandbox.jenkins.cloudera.com/view/Impala/view/Nightly-Builds/job/impala-master-cdh5-trunk-non-partitioned-hash-and-aggs/196/console

{code}
00:23:06.225 Your existing hdfs warehouse directory  (/test-warehouse) will be removed.
00:23:07.294 Creating test-warehouse directory
00:23:09.824 Loading snapshot file: /data/jenkins/workspace/impala-master-cdh5-trunk-non-partitioned-hash-and-aggs/testdata/test-warehouse-SNAPSHOT/test-warehouse-cdh5-781-SNAPSHOT.tar.gz
00:23:09.832 Extracting tarball
00:26:17.648 Loading hive builtins
00:28:11.377 Copying data to hdfs
00:32:29.462 2016-01-14 23:42:17,961 INFO  [Thread-9831] hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1573)) - Exception in createBlockOutputStream
00:32:29.462 java.io.EOFException: Premature EOF: no length prefix available
00:32:29.462 	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
00:32:29.462 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1541)
00:32:29.462 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1463)
00:32:29.462 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:673)
00:32:29.462 2016-01-14 23:42:17,965 INFO  [Thread-9831] hdfs.DFSClient (DFSOutputStream.java:nextBlockOutputStream(1466)) - Abandoning BP-150084399-127.0.0.1-1452843084212:blk_1073747343_6519
00:32:29.469 2016-01-14 23:42:17,972 INFO  [Thread-9831] hdfs.DFSClient (DFSOutputStream.java:nextBlockOutputStream(1470)) - Excluding datanode DatanodeInfoWithStorage[127.0.0.1:31001,DS-b726e5fb-0f5c-47f9-8d02-9f116f740a03,DISK]
00:32:29.484 2016-01-14 23:42:17,987 WARN  [Thread-9831] hdfs.DFSClient (DFSOutputStream.java:run(782)) - DataStreamer Exception
00:32:29.484 org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /test-warehouse/tpch.lineitem_rc/000003_0._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.
00:32:29.484 	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1546)
00:32:29.484 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3289)
00:32:29.484 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:669)
00:32:29.484 	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
00:32:29.484 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
00:32:29.484 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
00:32:29.484 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
00:32:29.484 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
00:32:29.484 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2079)
00:32:29.484 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2075)
00:32:29.484 	at java.security.AccessController.doPrivileged(Native Method)
00:32:29.484 	at javax.security.auth.Subject.doAs(Subject.java:415)
00:32:29.484 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
00:32:29.484 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2073)
00:32:29.484 
00:32:29.484 	at org.apache.hadoop.ipc.Client.call(Client.java:1468)
00:32:29.484 	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
00:32:29.484 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
00:32:29.484 	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
00:32:29.484 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)
00:32:29.484 	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
00:32:29.484 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
00:32:29.484 	at java.lang.reflect.Method.invoke(Method.java:606)
00:32:29.484 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)
00:32:29.484 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
00:32:29.484 	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
00:32:29.484 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1634)
00:32:29.484 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1451)
00:32:29.484 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:673)
{code}

[~alex.behm], could you take a look (or pull someone in to help with infra)?"	IMPALA	Resolved	3	1	1880	broken-build
13052991	Extrapolate the number of rows in a scan based on the rows/byte ratio	"*This JIRA is intended to address the following problems*
* Some partitions may be missing the #rows stat
* Some partitions may have the #rows stat but it is stale because files were added/dropped since computing the #rows stat

*The main idea is to use available #rows stats to extrapolate the missing stats*
* Store an additional statistic rows/byte in the TBLPROPERTIES of the table (could also be rows/kbyte or whatever seems most suitable)
* That statistic is computed as part of COMPUTE [INCREMENTAL] STATS on the impalad side, and then shipped to the catalogd for it to be stored in the Metastore
* During query planning we use the rows/byte statistic to estimate the number of rows scanned for *all* partitions regardless of whether a partition has #rows or not. The rationale is that the #rows of a partition may be outdated and using the rows/byte ratio is more robust to data changes.
* We should augment SHOW TABLE STATS to display the stored #rows as well as the extrapolated #rows.
* We should have some way of reporting the stored rows/byte ratio for debugging purposes (maybe SHOW TABLE STATS or EXPLAIN?)

*Additional considerations*
* A table could have mixed formats
* Even if a table has the same format, files could be compressed differently
* It seems reasonable to ignore these issues in the first cut

*Non-Goals*
* Estimate statistics if there are no stats at all, e.g. purely based on file size without knowing any #rows
* Extrapolate column stats like NDV in a similar fashion. That is a much more invasive change with a smaller impact.
"	IMPALA	Resolved	2	2	1880	ramp-up
13054997	Failure in UdfExecutorTest.HiveStringsTest	"I suspect this is a flaky test problem, as I havent seen it before and it manifested after 13455b5a24a9d4d009d1dd0d72944c6cacd54829 but before or in c24e9da914e1d5e5dabd1bded5a78452bccff9b5

{noformat}
13:52:35.660 Running org.apache.impala.hive.executor.UdfExecutorTest
13:52:35.660 Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.086 sec <<< FAILURE! - in org.apache.impala.hive.executor.UdfExecutorTest
13:52:35.660 HiveStringsTest(org.apache.impala.hive.executor.UdfExecutorTest)  Time elapsed: 0.037 sec  <<< FAILURE!
13:52:35.660 	at org.apache.impala.hive.executor.UdfExecutorTest.TestUdfImpl(UdfExecutorTest.java:327)
13:52:35.660 	at org.apache.impala.hive.executor.UdfExecutorTest.TestHiveUdf(UdfExecutorTest.java:349)
13:52:35.660 	at org.apache.impala.hive.executor.UdfExecutorTest.HiveStringsTest(UdfExecutorTest.java:406)
13:52:35.660   UdfExecutorTest.HiveStringsTest:406->TestHiveUdf:349->TestUdfImpl:327 arrays first differed at element [0]; expected:<-96> but was:<2>
13:52:35.660 Tests run: 533, Failures: 1, Errors: 0, Skipped: 20
{noformat}"	IMPALA	Resolved	2	1	1880	broken-build, flaky
13054188	Generate code coverage report from FE unit tests.	"I've the following options for getting FE code coverage from test runs:
* Clover (commercial)
* Cobertura (free)
* Jacoco (free)

The tools are all somewhat similar and integrate well with Maven for getting unit and integration test coverage. However, Jacoco seems like a clear winner due to its runtime instrumentation based on a Java agent. This means that it will be easy to also get code coverage for our end-to-end pytests, stress tests and random query tests.

The first task is to use Jacoco for FE unit test coverage."	IMPALA	Resolved	3	2	1880	test-infra
13061262	Frontend requests metadata for one table at a time in the query 	"It appears that the Frontend serializes loading metadata for missing tables in a query, Catalog log shows that the queue size is alway 0. 


Query below references  9 tables and metadata is loaded for one table at a time. 

{code}
explain select i_item_id ,i_item_desc ,s_state ,count(ss_quantity) as store_sales_quantitycount ,avg(ss_quantity) as store_sales_quantityave ,stddev_samp(ss_quantity) as store_sales_quantitystdev ,stddev_samp(ss_quantity)/avg(ss_quantity) as store_sales_quantitycov ,count(sr_return_quantity) as store_returns_quantitycount ,avg(sr_return_quantity) as store_returns_quantityave ,stddev_samp(sr_return_quantity) as store_returns_quantitystdev ,stddev_samp(sr_return_quantity)/avg(sr_return_quantity) as store_returns_quantitycov ,count(cs_quantity) as catalog_sales_quantitycount ,avg(cs_quantity) as catalog_sales_quantityave ,stddev_samp(cs_quantity) as catalog_sales_quantitystdev ,stddev_samp(cs_quantity)/avg(cs_quantity) as catalog_sales_quantitycov from store_sales ,store_returns ,catalog_sales ,date_dim d1 ,date_dim d2 ,date_dim d3 ,store ,item where d1.d_quarter_name = '2000Q1' and d1.d_date_sk = ss_sold_date_sk and i_item_sk = ss_item_sk and s_store_sk = ss_store_sk and ss_customer_sk = sr_customer_sk and ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number and sr_returned_date_sk = d2.d_date_sk and d2.d_quarter_name in ('2000Q1','2000Q2','2000Q3') and sr_customer_sk = cs_bill_customer_sk and sr_item_sk = cs_item_sk and cs_sold_date_sk = d3.d_date_sk and d3.d_quarter_name in ('2000Q1','2000Q2','2000Q3') group by i_item_id ,i_item_desc ,s_state order by i_item_id ,i_item_desc ,s_state limit 100
{code}

Catalog log
{code}
I0403 14:17:32.471273 57286 TableLoadingMgr.java:285] Loading next table from queue: tpcds_1000_parquet.store_sales
I0403 14:17:32.471375 57286 TableLoadingMgr.java:287] Remaining items in queue: 0. Loads in progress: 0
I0403 14:17:32.471560 34156 TableLoader.java:58] Loading metadata for: tpcds_1000_parquet.store_sales
I0403 14:17:32.485390 34156 HdfsTable.java:1145] Fetching partition metadata from the Metastore: tpcds_1000_parquet.store_sales
I0403 14:17:32.760711 34156 HdfsTable.java:1149] Fetched partition metadata from the Metastore: tpcds_1000_parquet.store_sales
I0403 14:17:33.958519 34156 HdfsTable.java:844] Loading file and block metadata for 1824 partitions from 1 paths: tpcds_1000_parquet.store_sales
I0403 14:17:34.392324 34156 HdfsTable.java:848] Loaded file and block metadata for 1824 partitions from 1 paths: tpcds_1000_parquet.store_sales
I0403 14:17:34.392421 34156 TableLoader.java:97] Loaded metadata for: tpcds_1000_parquet.store_sales
I0403 14:17:36.058523 57304 catalog-server.cc:320] Publishing update: TABLE:tpcds_1000_parquet.store_sales@3840
I0403 14:17:36.065404 57304 catalog-server.cc:320] Publishing update: CATALOG:44dafc1672d34719:bf64b7285d2a5912@3840
I0403 14:17:38.279191 57271 TableLoadingMgr.java:285] Loading next table from queue: tpcds_1000_parquet.store_returns
I0403 14:17:38.279278 57271 TableLoadingMgr.java:287] Remaining items in queue: 0. Loads in progress: 0
I0403 14:17:38.279422 34244 TableLoader.java:58] Loading metadata for: tpcds_1000_parquet.store_returns
I0403 14:17:38.308568 34244 HdfsTable.java:1145] Fetching partition metadata from the Metastore: tpcds_1000_parquet.store_returns
I0403 14:17:38.579197 34244 HdfsTable.java:1149] Fetched partition metadata from the Metastore: tpcds_1000_parquet.store_returns
I0403 14:17:39.897581 34244 HdfsTable.java:844] Loading file and block metadata for 2004 partitions from 1 paths: tpcds_1000_parquet.store_returns
I0403 14:17:40.371350 34244 HdfsTable.java:848] Loaded file and block metadata for 2004 partitions from 1 paths: tpcds_1000_parquet.store_returns
I0403 14:17:40.371443 34244 TableLoader.java:97] Loaded metadata for: tpcds_1000_parquet.store_returns
I0403 14:17:42.088232 57304 catalog-server.cc:320] Publishing update: TABLE:tpcds_1000_parquet.store_returns@3841
I0403 14:17:42.092733 57304 catalog-server.cc:320] Publishing update: CATALOG:44dafc1672d34719:bf64b7285d2a5912@3841
I0403 14:17:44.361759 57273 TableLoadingMgr.java:285] Loading next table from queue: tpcds_1000_parquet.catalog_sales
I0403 14:17:44.361835 57273 TableLoadingMgr.java:287] Remaining items in queue: 0. Loads in progress: 0
I0403 14:17:44.362061 34289 TableLoader.java:58] Loading metadata for: tpcds_1000_parquet.catalog_sales
I0403 14:17:44.377027 34289 HdfsTable.java:1145] Fetching partition metadata from the Metastore: tpcds_1000_parquet.catalog_sales
I0403 14:17:44.650100 34289 HdfsTable.java:1149] Fetched partition metadata from the Metastore: tpcds_1000_parquet.catalog_sales
I0403 14:17:45.819257 34289 HdfsTable.java:844] Loading file and block metadata for 1837 partitions from 1 paths: tpcds_1000_parquet.catalog_sales
I0403 14:17:46.264878 34289 HdfsTable.java:848] Loaded file and block metadata for 1837 partitions from 1 paths: tpcds_1000_parquet.catalog_sales
I0403 14:17:46.264987 34289 TableLoader.java:97] Loaded metadata for: tpcds_1000_parquet.catalog_sales
I0403 14:17:48.093703 57304 catalog-server.cc:320] Publishing update: TABLE:tpcds_1000_parquet.catalog_sales@3842
I0403 14:17:48.098681 57304 catalog-server.cc:320] Publishing update: CATALOG:44dafc1672d34719:bf64b7285d2a5912@3842
I0403 14:17:50.438555 57272 TableLoadingMgr.java:285] Loading next table from queue: tpcds_1000_parquet.date_dim
I0403 14:17:50.438663 57272 TableLoadingMgr.java:287] Remaining items in queue: 0. Loads in progress: 0
I0403 14:17:50.438886 34319 TableLoader.java:58] Loading metadata for: tpcds_1000_parquet.date_dim
I0403 14:17:50.454288 34319 HdfsTable.java:1145] Fetching partition metadata from the Metastore: tpcds_1000_parquet.date_dim
I0403 14:17:50.455581 34319 HdfsTable.java:1149] Fetched partition metadata from the Metastore: tpcds_1000_parquet.date_dim
I0403 14:17:50.458034 34319 HdfsTable.java:844] Loading file and block metadata for 1 partitions from 1 paths: tpcds_1000_parquet.date_dim
I0403 14:17:50.458940 34319 HdfsTable.java:848] Loaded file and block metadata for 1 partitions from 1 paths: tpcds_1000_parquet.date_dim
I0403 14:17:50.459019 34319 TableLoader.java:97] Loaded metadata for: tpcds_1000_parquet.date_dim
I0403 14:17:52.067752 57304 catalog-server.cc:320] Publishing update: TABLE:tpcds_1000_parquet.date_dim@3843
I0403 14:17:52.068792 57304 catalog-server.cc:320] Publishing update: CATALOG:44dafc1672d34719:bf64b7285d2a5912@3843
I0403 14:17:54.451196 57276 TableLoadingMgr.java:285] Loading next table from queue: tpcds_1000_parquet.store
I0403 14:17:54.451275 57276 TableLoadingMgr.java:287] Remaining items in queue: 0. Loads in progress: 0
I0403 14:17:54.451402 34392 TableLoader.java:58] Loading metadata for: tpcds_1000_parquet.store
I0403 14:17:54.464722 34392 HdfsTable.java:1145] Fetching partition metadata from the Metastore: tpcds_1000_parquet.store
I0403 14:17:54.466107 34392 HdfsTable.java:1149] Fetched partition metadata from the Metastore: tpcds_1000_parquet.store
I0403 14:17:54.468161 34392 HdfsTable.java:844] Loading file and block metadata for 1 partitions from 1 paths: tpcds_1000_parquet.store
I0403 14:17:54.468992 34392 HdfsTable.java:848] Loaded file and block metadata for 1 partitions from 1 paths: tpcds_1000_parquet.store
I0403 14:17:54.469070 34392 TableLoader.java:97] Loaded metadata for: tpcds_1000_parquet.store
I0403 14:17:56.036121 57304 catalog-server.cc:320] Publishing update: TABLE:tpcds_1000_parquet.store@3844
I0403 14:17:56.037204 57304 catalog-server.cc:320] Publishing update: CATALOG:44dafc1672d34719:bf64b7285d2a5912@3844
I0403 14:17:58.457381 57274 TableLoadingMgr.java:285] Loading next table from queue: tpcds_1000_parquet.item
I0403 14:17:58.457473 57274 TableLoadingMgr.java:287] Remaining items in queue: 0. Loads in progress: 0
I0403 14:17:58.457653 34456 TableLoader.java:58] Loading metadata for: tpcds_1000_parquet.item
I0403 14:17:58.470528 34456 HdfsTable.java:1145] Fetching partition metadata from the Metastore: tpcds_1000_parquet.item
I0403 14:17:58.471864 34456 HdfsTable.java:1149] Fetched partition metadata from the Metastore: tpcds_1000_parquet.item
I0403 14:17:58.474072 34456 HdfsTable.java:844] Loading file and block metadata for 1 partitions from 1 paths: tpcds_1000_parquet.item
I0403 14:17:58.474925 34456 HdfsTable.java:848] Loaded file and block metadata for 1 partitions from 1 paths: tpcds_1000_parquet.item
I0403 14:17:58.475021 34456 TableLoader.java:97] Loaded metadata for: tpcds_1000_parquet.item
I0403 14:18:00.036249 57304 catalog-server.cc:320] Publishing update: TABLE:tpcds_1000_parquet.item@3845
I0403 14:18:00.037330 57304 catalog-server.cc:320] Publishing update: CATALOG:44dafc1672d34719:bf64b7285d2a5912@3845
{code}

Coordinator node log
{code}
I0403 14:17:32.471491 37742 Frontend.java:833] Requesting prioritized load of table(s): tpcds_1000_parquet.store_sales
I0403 14:17:38.279330 37742 Frontend.java:833] Requesting prioritized load of table(s): tpcds_1000_parquet.store_returns
I0403 14:17:44.361925 37742 Frontend.java:833] Requesting prioritized load of table(s): tpcds_1000_parquet.catalog_sales
I0403 14:17:50.438707 37742 Frontend.java:833] Requesting prioritized load of table(s): tpcds_1000_parquet.date_dim
I0403 14:17:54.451408 37742 Frontend.java:833] Requesting prioritized load of table(s): tpcds_1000_parquet.store
I0403 14:17:58.457484 37742 Frontend.java:833] Requesting prioritized load of table(s): tpcds_1000_parquet.item
I0403 14:18:02.465189 37742 Frontend.java:928] Compiled query.
I0403 14:18:02.593619 37742 impala-beeswax-server.cc:190] get_results_metadata(): query_id=664050071b49c3c8:10b184b900000000
I0403 14:18:02.618315 37742 impala-beeswax-server.cc:233] close(): query_id=664050071b49c3c8:10b184b900000000
I0403 14:18:02.618413 37742 impala-server.cc:921] UnregisterQuery(): query_id=664050071b49c3c8:10b184b900000000
I0403 14:18:02.618422 37742 impala-server.cc:1007] Cancel(): query_id=664050071b49c3c8:10b184b900000000
{code}"	IMPALA	Resolved	2	1	1880	Performance, frontend
13054955	Wrong results with nested union operands due to missing casts.	"Unions with nested unions can produce incorrect results due to missing casts.
This bug can happen for both UNION ALL and UNION DISTINCT.

Repro:
{code}
create table t (d double);
insert into t values(5.5);

select d from t union all values(cast(6.2 as float));
+------------------------+
| d                      |
+------------------------+
| 5.5                    |
| 5.369229582389968e-315 |
+------------------------+

select d from t union all (select 10 union all select 20);
+------------------------+
| d                      |
+------------------------+
| 5.5                    |
| 4.940656458412465e-323 |
| 9.881312916824931e-323 |
+------------------------+
{code}

The underlying problem is that after unnesting unions we need to re-apply casts to the exprs of the unnested operands which my not be the same as the exprs of the original operand."	IMPALA	Resolved	1	1	1880	correctness
13051870	Left anti join returns wrong results on empty table	"The left anti join returns wrong results when the right table/view is empty.
To reproduce:

{code}
create table t (a int, b int);
insert into t values (1, null);
insert into t values (1, 2);

select * from t left anti join (select a from t where a < 0) v on t.a = v.a;
Returns: 0 rows

It should return all the rows of table t
{code}"	IMPALA	Resolved	3	1	1880	correctness, impala
13055281	with Kudu tables, ClassCastException with NULL literal in IN list	"This seems to be a Kudu-specific bug, since the error is about initializing the Kudu scan node, and the equivalent query works fine in {{functional}}.

{noformat}
explain select id from functional_kudu.alltypestiny where id in (null);
{noformat}

This query is OK:

{noformat}
explain select id from functional_kudu.alltypestiny where id = null;
{noformat}

The backtrace is:

{noformat}
I1214 15:12:41.465106 25897 jni-util.cc:169] org.apache.impala.common.ImpalaRuntimeException: Unable to initialize the Kudu scan node
        at org.apache.impala.planner.KuduScanNode.init(KuduScanNode.java:135)
        at org.apache.impala.planner.SingleNodePlanner.createScanNode(SingleNodePlanner.java:1285)
        at org.apache.impala.planner.SingleNodePlanner.createTableRefNode(SingleNodePlanner.java:1498)
        at org.apache.impala.planner.SingleNodePlanner.createTableRefsPlan(SingleNodePlanner.java:773)
        at org.apache.impala.planner.SingleNodePlanner.createSelectPlan(SingleNodePlanner.java:613)
        at org.apache.impala.planner.SingleNodePlanner.createQueryPlan(SingleNodePlanner.java:254)
        at org.apache.impala.planner.SingleNodePlanner.createSingleNodePlan(SingleNodePlanner.java:147)
        at org.apache.impala.planner.Planner.createPlan(Planner.java:87)
        at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:998)
        at org.apache.impala.service.Frontend.createExecRequest(Frontend.java:1090)
        at org.apache.impala.service.JniFrontend.createExecRequest(JniFrontend.java:160)
Caused by: java.lang.ClassCastException: org.apache.impala.analysis.NullLiteral cannot be cast to org.apache.impala.analysis.NumericLiteral
        at org.apache.impala.planner.KuduScanNode.getKuduInListValue(KuduScanNode.java:404)
        at org.apache.impala.planner.KuduScanNode.tryConvertInListKuduPredicate(KuduScanNode.java:382)
        at org.apache.impala.planner.KuduScanNode.extractKuduConjuncts(KuduScanNode.java:287)
        at org.apache.impala.planner.KuduScanNode.init(KuduScanNode.java:123)
        ... 10 more
I1214 15:12:41.512305 25897 status.cc:114] ImpalaRuntimeException: Unable to initialize the Kudu scan node
CAUSED BY: ClassCastException: org.apache.impala.analysis.NullLiteral cannot be cast to org.apache.impala.analysis.NumericLiteral
    @          0x11c8de5  impala::Status::Status()
    @          0x15a0dfe  impala::JniUtil::GetJniExceptionMsg()
    @          0x1462fbe  impala::JniUtil::CallJniMethod<>()
    @          0x145f6a9  impala::Frontend::GetExecRequest()
    @          0x147ccc9  impala::ImpalaServer::ExecuteInternal()
    @          0x147c870  impala::ImpalaServer::Execute()
    @          0x14e547e  impala::ImpalaServer::query()
    @          0x1943d96  beeswax::BeeswaxServiceProcessor::process_query()
    @          0x1943ae4  beeswax::BeeswaxServiceProcessor::dispatchCall()
    @          0x192c4ef  impala::ImpalaServiceProcessor::dispatchCall()
    @          0x116fcac  apache::thrift::TDispatchProcessor::process()
    @          0x27d058b  apache::thrift::server::TThreadPoolServer::Task::run()
    @          0x27b8df9  apache::thrift::concurrency::ThreadManager::Worker::run()
    @          0x1336995  impala::ThriftThread::RunRunnable()
    @          0x13380c1  boost::_mfi::mf2<>::operator()()
    @          0x1337f57  boost::_bi::list3<>::operator()<>()
    @          0x1337ca3  boost::_bi::bind_t<>::operator()()
    @          0x1337bb6  boost::detail::function::void_function_obj_invoker0<>::invoke()
    @          0x1345534  boost::function0<>::operator()()
    @          0x15f21b5  impala::Thread::SuperviseThread()
    @          0x15f918e  boost::_bi::list4<>::operator()<>()
    @          0x15f90d1  boost::_bi::bind_t<>::operator()()
    @          0x15f902c  boost::detail::thread_data<>::run()
    @          0x1a4ecca  thread_proxy
    @     0x7f7619a75184  start_thread
    @     0x7f76197a237d  (unknown)
{noformat}"	IMPALA	Resolved	2	1	1880	correctness, query_generator
13316660	 Invalid offset index in Parquet file	"When reading parquet file in impala 3.4, encountered the following error:
{code:java}
I0714 16:11:48.307806 1075820 runtime-state.cc:207] 8c43203adb2d4fc8:0478df9b0000018b] Error from query 8c43203adb2d4fc8:0478df9b00000000: Invalid offset index in Parquet file hdfs://path/4844de7af4545a39-e8ebc7da0000005f_2015704758_data.0.parq.
I0714 16:11:48.834901 1075838 status.cc:126] 8c43203adb2d4fc8:0478df9b000002c0] Invalid offset index in Parquet file hdfs://path/4844de7af4545a39-e8ebc7da0000005f_2015704758_data.0.parq.
    @           0xbf4ef9
    @          0x1748c41
    @          0x174e170
    @          0x1750e58
    @          0x17519f0
    @          0x1748559
    @          0x1510b41
    @          0x1512c8f
    @          0x137488a
    @          0x1375759
    @          0x1b48a19
    @     0x7f34509f5e24
    @     0x7f344d5ed35c
I0714 16:11:48.835763 1075838 runtime-state.cc:207] 8c43203adb2d4fc8:0478df9b000002c0] Error from query 8c43203adb2d4fc8:0478df9b00000000: Invalid offset index in Parquet file hdfs://path/4844de7af4545a39-e8ebc7da0000005f_2015704758_data.0.parq.
I0714 16:11:48.893784 1075820 status.cc:126] 8c43203adb2d4fc8:0478df9b0000018b] Top level rows aren't in sync during page filtering in file hdfs://path/4844de7af4545a39-e8ebc7da0000005f_2015704758_data.0.parq.
    @           0xbf4ef9
    @          0x1749104
    @          0x17494cc
    @          0x1751aee
    @          0x1748559
    @          0x1510b41
    @          0x1512c8f
    @          0x137488a
    @          0x1375759
    @          0x1b48a19
    @     0x7f34509f5e24
    @     0x7f344d5ed35c
{code}
Corresponding source code:
{code:java}
Status HdfsParquetScanner::CheckPageFiltering() {
  if (candidate_ranges_.empty() || scalar_readers_.empty()) return Status::OK();  int64_t current_row = scalar_readers_[0]->LastProcessedRow();
  for (int i = 1; i < scalar_readers_.size(); ++i) {
    if (current_row != scalar_readers_[i]->LastProcessedRow()) {
      DCHECK(false);
      return Status(Substitute(
          ""Top level rows aren't in sync during page filtering in file $0."", filename()));
    }
  }
  return Status::OK();
}


{code}"	IMPALA	Resolved	3	1	2770	Parquet
13333749	Implement INSERT INTO for non-partitioned Iceberg tables (Parquet)	"Impala should be able to insert into non-partitioned Iceberg table when the underlying data file format is Parquet.

INSERT OVERWRITE and CTAS is out-of-scope for this sub-task."	IMPALA	Resolved	3	7	2770	impala-iceberg
13547371	Add better cardinality estimation for Iceberg V2 tables with deletes	"IMPALA-11797 is about the generic case, i.e. better cardinality for all ANTI JOIN operators.

For Iceberg V2 we can safely come up with a better cardinality estimation as we can assume that all rows at RHS have a match in LHS when there is no filtering. Though RHS might contain duplicate rows, see:
https://github.com/apache/iceberg/blob/462a203e67dd42d111a7fd2d3a0090b5aeb80833/api/src/main/java/org/apache/iceberg/RowDelta.java#L132-L133

So we can come up something like this:
Cardinality of DELETE operator = Cardinality(LHS) - (Cardinality(RHS) * selectivity of LHS)
With some safety checks if it becomes negative (due to duplicates in RHS)."	IMPALA	Resolved	3	1	2770	impala-iceberg
13393533	"Add support for ""FOR SYSTEM_TIME AS OF"" and ""FOR SYSTEM_VERSION AS OF"" for Iceberg tables"	"SQL11 introduced a couple of temporal features:
https://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf

One of them is FOR SYSTEM_TIME AS OF which can be part of the table reference:
https://jakewheat.github.io/sql-overview/sql-2011-foundation-grammar.html#query-system-time-period-specification

With this extension the users are able to query the table at a given time point.

HIVE-25344 also added support for this. It also adds another, non-standard clause: FOR SYSTEM_VERSION AS OF.

In Impala we should add both to support time travel for Iceberg tables."	IMPALA	Resolved	3	7	2770	impala-iceberg
13544773	Add support for UPDATE statements for Iceberg tables	"Add support for UPDATE statements for Iceberg tables.

Initial design doc of DELETEs and UPDATEs: [https://docs.google.com/document/d/1GuRiJ3jjqkwINsSCKYaWwcfXHzbMrsd3WEMDOB11Xqw/edit#heading=h.5bmfhbmb4qdk]

Limitations:
 * only write position delete files"	IMPALA	Resolved	3	2	2770	impala-iceberg
13085556	Parquet support for additional valid decimal representations	"This is an umbrella JIRA to implement valid representations of DECIMAL that Impala doesn't currently support.

See https://github.com/Parquet/parquet-format/blob/master/LogicalTypes.md"	IMPALA	Closed	3	4	2770	ramp-up
13308283	TestAcid.test_acid_negative failed in inserting data via Hive	"Saw the test fails as:
{code}
query_test/test_acid.py:65: in test_acid_negative
    self.run_test_case('QueryTest/acid-negative', vector, use_db=unique_database)
common/impala_test_suite.py:656: in run_test_case
    result = exec_fn(query, user=test_section.get('USER', '').strip() or None)
common/impala_test_suite.py:610: in __exec_in_hive
    result = h.execute(query, user=user)
common/impala_connection.py:334: in execute
    r = self.__fetch_results(handle, profile_format=profile_format)
common/impala_connection.py:441: in __fetch_results
    cursor._wait_to_finish()
/data/jenkins/workspace/impala-cdh-7.2.0.0-core-s3/repos/Impala/infra/python/env/lib/python2.7/site-packages/impala/hiveserver2.py:412: in _wait_to_finish
    raise OperationalError(resp.errorMessage)
E   OperationalError: Error while compiling statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort {code}

In HiveServer2's logs, looks like the query failed by bad configuration:
{code}
2020-05-19T11:58:21,805  INFO [HiveServer2-Background-Pool: Thread-66] common.LogUtils: Thread context registration is done.
2020-05-19T11:58:21,813  INFO [HiveServer2-Background-Pool: Thread-66] reexec.ReExecDriver: Execution #1 of query
2020-05-19T11:58:21,816  INFO [HiveServer2-Background-Pool: Thread-66] lockmgr.DbTxnManager: Setting lock request transaction to txnid:2619 for queryId=jenkins_20200519115820_bf6d1287-9e24-46c8-93d0-80c9c449fcd8
2020-05-19T11:58:21,822  INFO [HiveServer2-Background-Pool: Thread-66] lockmgr.DbLockManager: Requesting: queryId=jenkins_20200519115820_bf6d1287-9e24-46c8-93d0-80c9c449fcd8 LockRequest(component:[LockComponent(type:SHARED_READ, level:TABLE, dbname:test_acid_negative_44c33b5e, tablename:acid, operationType:INSERT, isTransactional:true, isDynamicPartitionWrite:false)], txnid:2619, user:jenkins, hostname:impala-ec2-centos74-m5-4xlarge-ondemand-11ab.vpc.cloudera.com, agentInfo:jenkins_20200519115820_bf6d1287-9e24-46c8-93d0-80c9c449fcd8)
2020-05-19T11:58:21,848  INFO [HiveServer2-Background-Pool: Thread-66] lockmgr.DbLockManager: Response to queryId=jenkins_20200519115820_bf6d1287-9e24-46c8-93d0-80c9c449fcd8 LockResponse(lockid:1330, state:ACQUIRED)
2020-05-19T11:58:21,858  INFO [HiveServer2-Background-Pool: Thread-66] ql.Driver: Executing command(queryId=jenkins_20200519115820_bf6d1287-9e24-46c8-93d0-80c9c449fcd8):
insert into acid values (1), (2), (3)
2020-05-19T11:58:21,858  INFO [HiveServer2-Background-Pool: Thread-66] ql.Driver: Query ID = jenkins_20200519115820_bf6d1287-9e24-46c8-93d0-80c9c449fcd8
2020-05-19T11:58:21,858  INFO [HiveServer2-Background-Pool: Thread-66] ql.Driver: Total jobs = 1
2020-05-19T11:58:21,879  INFO [HiveServer2-Background-Pool: Thread-66] ql.Driver: Launching Job 1 out of 1
2020-05-19T11:58:21,879  INFO [HiveServer2-Background-Pool: Thread-66] ql.Driver: Starting task [Stage-1:MAPRED] in serial mode

......

2020-05-19T12:18:29,556 ERROR [HiveServer2-Background-Pool: Thread-66] operation.Operation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:362) ~[hive-service-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:241) ~[hive-service-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:322) ~[hive-service-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_144]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_144]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:340) ~[hive-service-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_144]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_144]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_144]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_144]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
        at sun.reflect.GeneratedConstructorAccessor65.newInstance(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_144]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_144]
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1557) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client.call(Client.java:1499) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client.call(Client.java:1396) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source) ~[?:?]
        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:274) ~[hadoop-yarn-common-3.1.1.7.2.0.0-185.jar:?]
        at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source) ~[?:?]
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:270) ~[hadoop-yarn-client-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:278) ~[hadoop-yarn-client-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.tez.client.TezYarnClient.createApplication(TezYarnClient.java:71) ~[tez-api-0.9.1.7.2.0.0-185.jar:0.9.1.7.2.0.0-185]
        at org.apache.tez.client.TezClient.createApplication(TezClient.java:1153) ~[tez-api-0.9.1.7.2.0.0-185.jar:0.9.1.7.2.0.0-185]
        at org.apache.tez.client.TezClient.start(TezClient.java:401) ~[tez-api-0.9.1.7.2.0.0-185.jar:0.9.1.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.startSessionAndContainers(TezSessionState.java:535) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:373) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:298) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.open(TezSessionPoolSession.java:106) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.ensureSessionHasResources(TezTask.java:403) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:209) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:359) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:721) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:488) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:482) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:225) ~[hive-service-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        ... 13 more
Caused by: java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_144]
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_144]
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:699) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:803) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:413) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1627) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client.call(Client.java:1443) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.Client.call(Client.java:1396) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source) ~[?:?]
        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:274) ~[hadoop-yarn-common-3.1.1.7.2.0.0-185.jar:?]
        at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.1.1.7.2.0.0-185.jar:?]
        at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source) ~[?:?]
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:270) ~[hadoop-yarn-client-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:278) ~[hadoop-yarn-client-3.1.1.7.2.0.0-185.jar:?]
        at org.apache.tez.client.TezYarnClient.createApplication(TezYarnClient.java:71) ~[tez-api-0.9.1.7.2.0.0-185.jar:0.9.1.7.2.0.0-185]
        at org.apache.tez.client.TezClient.createApplication(TezClient.java:1153) ~[tez-api-0.9.1.7.2.0.0-185.jar:0.9.1.7.2.0.0-185]
        at org.apache.tez.client.TezClient.start(TezClient.java:401) ~[tez-api-0.9.1.7.2.0.0-185.jar:0.9.1.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.startSessionAndContainers(TezSessionState.java:535) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:373) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:298) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.open(TezSessionPoolSession.java:106) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.ensureSessionHasResources(TezTask.java:403) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:209) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:359) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:721) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:488) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:482) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166) ~[hive-exec-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:225) ~[hive-service-3.1.3000.7.2.0.0-185.jar:3.1.3000.7.2.0.0-185]
        ... 13 more
{code}

Uploaded HiveServer2's log file."	IMPALA	Resolved	2	1	2770	broken-build
13097425	Use page index in Parquet files to skip pages	Once IMPALA-5842 has been resolved, we should skip pages based on the page index in Parquet files.	IMPALA	Resolved	2	2	2770	parquet, performance
13238150	Implement INSERT for insert-only ACID tables	"Impala should support insertion for insert-only ACID tables.

For this we need to allocate a write ID for the target table, and write the data into the base/delta directories.

INSERT operation should create a new delta directory with the allocated write ID.

INSERT OVERWRITE should create a new base directory with the allocated write ID. This new base directory will only contain the data coming from this operation."	IMPALA	Resolved	2	2	2770	impala-acid
13579509	Speedup IcebergDeleteBuilder	"When there are lots of delete records IcebergDeleteBuilder can become a bottleneck. Since the left side of the JOIN is blocked on the build side any improvement we make here significantly improves Iceberg V2 table scanning.
* std vector growths need lots of re-allocations and memory copies
* inserting into the position vectors one-by-one is inefficient
* sorting the position vectors could be parallelized (especially in case of MT_DOP when there are MT_DOP threads blocked on the build)"	IMPALA	Open	3	4	2770	impala-iceberg
13591177	Schema resolution doesn't work for migrated partitioned Iceberg tables that have complex types	"Schema resolution doesn't work correctly for migrated partitioned Iceberg tables that have complex types.

When we face a Parquet/ORC file in an Iceberg table that doesn't have field IDs in the file metadata, we assume that it is an old data file before migration, and the schema is the very first one, hence we can mimic Iceberg's field ID generation to assign field IDs to the file schema elements.

This process didn't take the partition columns into account. This only matters when there are complex types in the table, as partition columns are always the last columns in legacy Hive tables, and field IDs are assigned via a ""BFS-like"" traversal. I.e. if there are only primitive types in the table we don't have any problems, but the children of complex types columns are assigned incorrectly."	IMPALA	Resolved	3	1	2770	impala-iceberg
13081184	Impala cannot scan Parquet decimal stored as int64_t/int32_t	This is supported according to the Parquet spec (https://github.com/Parquet/parquet-format/blob/master/LogicalTypes.md#decimal) but wasn't widely used. For some reason Spark decided to start writing this out as the default (see SPARK-20297) so we will likely start seeing this at some point.	IMPALA	Resolved	3	7	2770	parquet
13133469	Document IMPALA-5191: Incompatible change to alias and ordinal substitutions	We should carefully document the before and after behavior of IMPALA-5191 and be sure to call this out as an incompatible change.	IMPALA	Resolved	1	20	2770	incompatibility
13357751	Using JOIN ON syntax to join two full ACID collections produces wrong results	"The following query produces wrong results:
{noformat}
use functional_orc_def; // use full ACID tables
select a1.item, a2.item
from complextypestbl.int_array a1 join complextypestbl.int_array a2
on a1.item=a2.item
where a1.item<2;{noformat}
It creates a CROSS JOIN without the predicate ""a1.item = a2.item"", generating too many rows. The expected plan node would be an INNER JOIN on ""a1.item = a2.item"".

If we put the JOIN condition to the WHERE clause we get the correct plan:
{noformat}
select a1.item, a2.item
from complextypestbl.int_array a1 join complextypestbl.int_array a2
where a1.item=a2.item and a1.item<2{noformat}
We also get a correct plan if the right table is non-ACID:
{noformat}
select a1.item, a2.item
from complextypestbl.int_array a1 join functional_parquet.complextypestbl.int_array a2
on a1.item=a2.item
where a1.item<2;{noformat}
Or ACID table but the column is non-collection:
{noformat}
select c.id, a1.item
from complextypestbl.int_array a1 join complextypestbl c
on c.id=a1.item
where c.id<2;{noformat}"	IMPALA	Resolved	3	1	2770	CorrectnessBug, impala-acid
13317392	Scan orc failed when table contains timestamp column	"Recently, when I test impala query orc table, I found that scanning failed when table contains timestamp column, here is there exception: 

{code:java}
I0717 08:31:47.179124 78759 status.cc:129] 68436a6e0883be84:53877f7200000002] Encountered parse error in tail of ORC file hdfs://localhost:20500/test-warehouse/orc_scanner_test/00031-31-ac3cccf1-3ce7-40c6-933c-4fbd7bd57550-00000.orc: Unknown type kind
    @          0x1c9f753  impala::Status::Status()
    @          0x27aa049  impala::HdfsOrcScanner::ProcessFileTail()
    @          0x27a7fb3  impala::HdfsOrcScanner::Open()
    @          0x27365fe  impala::HdfsScanNodeBase::CreateAndOpenScannerHelper()
    @          0x28cb379  impala::HdfsScanNode::ProcessSplit()
    @          0x28caa7d  impala::HdfsScanNode::ScannerThread()
    @          0x28c9de5  _ZZN6impala12HdfsScanNode22ThreadTokenAvailableCbEPNS_18ThreadResourcePoolEENKUlvE_clEv
    @          0x28cc19e  _ZN5boost6detail8function26void_function_obj_invoker0IZN6impala12HdfsScanNode22ThreadTokenAvailableCbEPNS3_18ThreadResourcePoolEEUlvE_vE6invokeERNS1_15function_bufferE
    @          0x2053333  boost::function0<>::operator()()
    @          0x2675d93  impala::Thread::SuperviseThread()
    @          0x267dd30  boost::_bi::list5<>::operator()<>()
    @          0x267dc54  boost::_bi::bind_t<>::operator()()
    @          0x267dc15  boost::detail::thread_data<>::run()
    @          0x3e3c3c1  thread_proxy
    @     0x7f32360336b9  start_thread
    @     0x7f3232bfe41c  clone
I0717 08:31:47.325670 78759 hdfs-scan-node.cc:490] 68436a6e0883be84:53877f7200000002] Error preparing scanner for scan range hdfs://localhost:20500/test-warehouse/orc_scanner_test/00031-31-ac3cccf1-3ce7-40c6-933c-4fbd7bd57550-00000.orc(0:582). Encountered parse error in tail of ORC file hdfs://localhost:20500/test-warehouse/orc_scanner_test/00031-31-ac3cccf1-3ce7-40c6-933c-4fbd7bd57550-00000.orc: Unknown type kind
{code}

When I remove timestamp colum from table, and generate test data, query success. By the way, my test data is generated by spark."	IMPALA	Resolved	4	1	2770	impala-iceberg
13398331	Some parquet files are missing from Iceberg avro metadata	"I tried to load tpcds_3000_iceberg database based on existing parquet database. The source database is tpcds_3000_parquet (3TB scale) and the cluster has 10 nodes.

After loading table catalog_sales, I found that the row count oftpcds_3000_iceberg.catalog_sales is less than row count oftpcds_3000_parquet.catalog_sales. Further debugging reveals that the CTAS query actually finish writing parquet files, but only one parquet file per partition gets written into Iceberg avro metadata.

For example inspecting partiton 2451120, it says that there are 2 parquet files
{code:java}
 ~  sudo -u hdfs hdfs dfs -ls /warehouse/tablespace/external/hive/tpcds_3000_iceberg.db/catalog_sales/data/cs_sold_date_sk=2451120/
 Found 2 items
 rw-rw---+ 3 impala hive 264837186 2021-08-28 20:05 /warehouse/tablespace/external/hive/tpcds_3000_iceberg.db/catalog_sales/data/cs_sold_date_sk=2451120/ec48089182b28ba9-b2910c2d00000011_1004901849_data.0.parq
 rw-rw---+ 3 impala hive 80608775 2021-08-28 20:05 /warehouse/tablespace/external/hive/tpcds_3000_iceberg.db/catalog_sales/data/cs_sold_date_sk=2451120/ec48089182b28ba9-b2910c2d00000011_1004901849_data.1.parq{code}

 However, the avro files only haveec48089182b28ba9-b2910c2d00000011_1004901849_data.0.parq in it
{code:java}
 ~  avro-tools tojson /tmp/9e044ebf-f549-4c99-9952-3b8c0a70cafb-m0.avro| grep ""cs_sold_date_sk=2451120""
[main] WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{""status"":1,""snapshot_id"":{""long"":403055188554782479},""data_file"":{""file_path"":""hdfs://ve1315.halxg.cloudera.com:8020/warehouse/tablespace/external/hive/tpcds_3000_iceberg.db/catalog_sales/data/cs_sold_date_sk=2451120/ec48089182b28ba9-b2910c2d00000011_1004901849_data.0.parq"",""file_format"":""PARQUET"",""partition"":{""cs_sold_date_sk"":{""long"":2451120}},""record_count"":3616116,""file_size_in_bytes"":264837186,""block_size_in_bytes"":67108864,""column_sizes"":null,""value_counts"":null,""null_value_counts"":null,""nan_value_counts"":null,""lower_bounds"":null,""upper_bounds"":null,""key_metadata"":null,""split_offsets"":null}}
{code}
The CTAS query that I use on debugging is the following:
{code:java}
create table catalog_sales
partitioned by (cs_sold_date_sk)
stored as iceberg
as select
  cs_sold_time_sk,
  cs_ship_date_sk,
  cs_bill_customer_sk,
  cs_bill_cdemo_sk,
  cs_bill_hdemo_sk,
  cs_bill_addr_sk,
  cs_ship_customer_sk,
  cs_ship_cdemo_sk,
  cs_ship_hdemo_sk,
  cs_ship_addr_sk,
  cs_call_center_sk,
  cs_catalog_page_sk,
  cs_ship_mode_sk,
  cs_warehouse_sk,
  cs_item_sk,
  cs_promo_sk,
  cs_order_number,
  cs_quantity,
  cs_wholesale_cost,
  cs_list_price,
  cs_sales_price,
  cs_ext_discount_amt,
  cs_ext_sales_price,
  cs_ext_wholesale_cost,
  cs_ext_list_price,
  cs_ext_tax,
  cs_coupon_amt,
  cs_ext_ship_cost,
  cs_net_paid,
  cs_net_paid_inc_tax,
  cs_net_paid_inc_ship,
  cs_net_paid_inc_ship_tax,
  cs_net_profit,
  cs_sold_date_sk
from tpcds_3000_parquet.catalog_sales where cs_sold_date_sk > 2451100 and cs_sold_date_sk <= 2451200;
{code}"	IMPALA	Resolved	3	1	2770	impala-iceberg
13162631	Impala is unable to read Parquet decimal columns with lower precision/scale than table metadata	"This is similar to IMPALA-2515, except relates to a different precision/scale in the file metadata rather than just a mismatch in the bytes used to store the data. In a lot of cases we should be able to convert the decimal type on the fly to the higher-precision type.

{noformat}
ERROR: File '/hdfs/path/000000_0_x_2' column 'alterd_decimal' has an invalid type length. Expecting: 11 len in file: 8
{noformat}

It would be convenient to allow reading parquet files where the precision/scale in the file can be converted to the precision/scale in the table metadata without loss of precision."	IMPALA	Resolved	3	7	2770	decimal, parquet, ramp-up
13054422	Re-enable per-scan filtering for sequence-based scanners	See IMPALA-3798 - we should properly fix the issue, rather than disabling per-scanner filtering for Avro etc. files.	IMPALA	Closed	4	4	2770	runtime-filters
13594719	TestIcebergTable.test_migrated_table_field_id_resolution_complex fails with type difference	"TestIcebergTable.test_migrated_table_field_id_resolution_complex failed on a core job with:
{noformat}
query_test/test_iceberg.py:330: in test_migrated_table_field_id_resolution_complex
    vector, unique_database)
common/impala_test_suite.py:831: in run_test_case
    self.__verify_results_and_errors(vector, test_section, result, use_db)
common/impala_test_suite.py:638: in __verify_results_and_errors
    replace_filenames_with_placeholder)
common/test_result_verifier.py:460: in verify_raw_results
    verify_results(expected_types, actual_types, order_matters=True)
common/test_result_verifier.py:356: in verify_results
    assert expected_results == actual_results
E   assert ['STRING', 'I...', 'INT', ...] == ['STRING', 'IN...'BIGINT', ...]
E     At index 5 diff: 'INT' != 'BIGINT'
E     Right contains more items, first extra item: 'INT'
E     Use -v to get the full diff{noformat}
This test was introduced by https://issues.apache.org/jira/browse/IMPALA-13364 . This occurs in the same run as IMPALA-13420, so this might have a similar underlying cause."	IMPALA	Resolved	2	1	2770	broken-build
13243086	FileMetadataLoader skips empty directories	{{FileMetadataLoader}} has certain code paths like the one below which using {{listFiles}} API on the filesystem. This API ignores empty directories which is okay for non-transactional tables. However, in case of transactional table an empty base directory provides writeId information which is used to skip loading files which are not relevant for a given writeId. See {{AcidUtils#filterFilesForAcidState}} usage for details.	IMPALA	Resolved	3	1	2770	impala-acid
13436828	Impala reloads Iceberg tables per each data file	"Due to a bug in IMPALA-11053, Impala reloads the Iceberg table per each data file.

This causes a serious perf regression for table loads."	IMPALA	Resolved	3	1	2770	impala-iceberg
13416109	Impala should be able to read migrated partitioned Iceberg tables	"When Hive (and probably other engines as well) converts a legacy Hive table to Iceberg it doesn't rewrite the data files.

It means that the data files don't have write ids, moreover they don't have the partition columns neither.

Currently Impala expects tha partition columns to be present in the data files, so it won't be able to read converted partitioned tables.

So we need to inject partition values from the Iceberg metadata, plus resolve columns correctly (position-based resolution needs an offset)."	IMPALA	Resolved	3	1	2770	impala-iceberg
13523264	Test table iceberg_partitioned_orc has wrong metadata	"Iceberg table iceberg_partitioned_orc has wrong metadata.
The field 'file_size_in_bytes' is wrong for the data files.

This causes issues on object stores where we rely more on Iceberg metadata since IMPALA-11662."	IMPALA	Resolved	2	1	2770	impala-iceberg
13298286	Impala shouldn't create/remove staging directory during transactional INSERTs	Transactional INSERTs write their data into their final location, so there is no need to create and remove a staging directory.	IMPALA	Resolved	3	1	2770	impala-acid
13146565	Simplify tests that copy local files to tables	"tests/query_test/ contains several tests that use ""hdfs dfs -copyFromLocal"" to fill tables with files that cannot be created with Impala.  The current way of doing this looks like ""copy paste"" and adds unnecessary complexity to the test code. The common pattern of ""create a table, copy 1-2 files to it"" could be covered by a single function, or could be moved to the .test files, which support SHELL sections.

for an example of SHELL section, see
https://github.com/apache/impala/blob/8dde41e802e3566d07e2db7b2bf5cd76030ab3d3/testdata/workloads/functional-query/queries/QueryTest/parquet-resolution-by-name.test#L75 "	IMPALA	Resolved	4	4	2770	ramp-up
13326148	Support Apache Iceberg in Impala	"Apache Iceberg is an open table format for huge analytic datasets: [https://iceberg.apache.org/]

Impala should be able to handle this table format, i.e. support querying and writing such tables."	IMPALA	Open	3	15	2770	impala-iceberg
13557762	DELETE throws DateTimeParseException when deleting from DAY-partitioned Iceberg table	"DELETE throws DateTimeParseException when deleting from DAY-partitioned Iceberg table.

Stack trace:
{noformat}
 [1] java.time.format.DateTimeFormatter.parseResolved0 (DateTimeFormatter.java:1,949)
 [2] java.time.format.DateTimeFormatter.parse (DateTimeFormatter.java:1,851)
 [3] java.time.LocalDate.parse (LocalDate.java:400)
 [4] org.apache.iceberg.expressions.Literals$StringLiteral.to (Literals.java:495)
 [5] org.apache.iceberg.types.Conversions.fromPartitionString (Conversions.java:70)
 [6] org.apache.impala.util.IcebergUtil.getPartitionValue (IcebergUtil.java:748)
 [7] org.apache.impala.util.IcebergUtil.partitionDataFromDataFile (IcebergUtil.java:726)
 [8] org.apache.impala.service.IcebergCatalogOpExecutor.deleteRows (IcebergCatalogOpExecutor.java:364)
 [9] org.apache.impala.service.IcebergCatalogOpExecutor.execute (IcebergCatalogOpExecutor.java:337)
 [10] org.apache.impala.service.CatalogOpExecutor.updateCatalogImpl (CatalogOpExecutor.java:7,042)
 [11] org.apache.impala.service.CatalogOpExecutor.updateCatalog (CatalogOpExecutor.java:6,786)
 [12] org.apache.impala.service.JniCatalog.lambda$updateCatalog$15 (JniCatalog.java:487)
...{noformat}"	IMPALA	Resolved	3	1	2770	impala-iceberg
13416013	Add support for 'void' Iceberg partition transform	"Iceberg recently added a new partition transform called 'void':

[https://iceberg.apache.org/#spec/#partition-transforms]

We should add support for it in Impala."	IMPALA	Resolved	3	1	2770	impala-iceberg
13414816	Create documentation about Impala's Iceberg support	Impala's Iceberg support has greatly evolved in the past year. We should start documenting and publicizing it.	IMPALA	Resolved	3	20	2770	impala-iceberg
13326420	Add support for Iceberg HiveCatalog	"HadoopTables and HadoopCatalog only work on filesystemsthat support atomic rename. Therefore they are not safe on S3.

HiveCatalog uses the Hive Metastore tokeep track of Iceberg tables, so we should use this interface when we store our tables in object stores."	IMPALA	Resolved	3	7	2770	impala-iceberg
13410920	test_show_create_table flaky failure	"This test failed in one of the recent [GVO builds|https://jenkins.impala.io/job/ubuntu-16.04-dockerised-tests/4902/]:
Stacktrace:
{noformat}
metadata/test_show_create_table.py:64: in test_show_create_table
    unique_database)
metadata/test_show_create_table.py:127: in __run_show_create_table_test_case
    result = self.__exec(test_case.show_create_table_sql)
metadata/test_show_create_table.py:135: in __exec
    return self.execute_query_expect_success(self.client, sql_str)
common/impala_test_suite.py:831: in wrapper
    return function(*args, **kwargs)
common/impala_test_suite.py:839: in execute_query_expect_success
    result = cls.__execute_query(impalad_client, query, query_options, user)
common/impala_test_suite.py:956: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:212: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:189: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:365: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:359: in execute_query_async
    handle = self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:522: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: AnalysisException: org.apache.impala.catalog.TableLoadingException: Error opening Iceberg table 'test_show_create_table_8b557a01.iceberg_ctas'
E   CAUSED BY: TableLoadingException: Error opening Iceberg table 'test_show_create_table_8b557a01.iceberg_ctas'
E   CAUSED BY: InconsistentMetadataFetchException: Catalog object TCatalogObject(type:TABLE, catalog_version:6846, table:TTable(db_name:test_show_create_table_8b557a01, tbl_name:iceberg_ctas)) changed version between accesses.
{noformat}

Reached out to Zoltan about this and he mentioned that this is probably due to a failure to detect a self-event on an iceberg table which resulted in a change to the catalog version of the table during some operation."	IMPALA	Resolved	2	1	2770	broken-build, flaky-test
13054321	Store query context in thread-local variables and use in crash reports	"If Impala crashes or hits some kind of unexpected error, it's useful to know what query the thread that crashed belonged to and be able to easily find the state of the query.

We should store some state in thread-local variables and then make sure that it can be found in the event of a crash.

We should:
* Store a pointer to the RuntimeState object in a thread-local variable
* Report basic info in the breakpad crash report and logs if there is a crash
** Query ID
** Fragment instance ID
** Thread Name
** ???
* Document how to find the thread-local variables in coredumps"	IMPALA	Closed	4	1	2770	breakpad, observability, supportability
13559007	Data race due to StringValue::Smallify of template tuple	"Concurrently opened scanners that create template tuples can concurrently invoke Smallify() for the same string slots.

Uploaded thread sanitizer report: [^thread_sanitizer_small_string.txt]"	IMPALA	Resolved	3	1	2770	broken-build
13595811	Impala should ignore case of Iceberg schema elements	"Schema is case insensitive in Impala.

Via Spark it's possible to create schema elements with upper/lower case letters and store them in the metadata JSON files of Iceberg, e.g.:
{noformat}
   ""schemas"" : [ {
     ""type"" : ""struct"",
     ""schema-id"" : 0,
     ""fields"" : [ {
       ""id"" : 1,
       ""name"" : ""ID"",
       ""required"" : false,
       ""type"" : ""string""
     }, {
       ""id"" : 2,
       ""name"" : ""OWNERID"",
       ""required"" : false,
       ""type"" : ""string""
     } ]
   } ],
{noformat}

This can cause problems in Impala during predicate pushdown, as we can get a ValidationException from the Iceberg library (as Impala pushes down predicates with lower case column names, while Iceberg sees upper case names).

We should invoke Scan.caseSensitive(boolean caseSensitive) on the TableScan object to set case insensitivity."	IMPALA	Open	3	1	2770	impala-iceberg
13062820	Behavior of GROUP BY, HAVING, ORDER BY with column aliases should be more standard conforming	"Allow Impala to accept ""column alias"" and ""Column name"" indistinctly in the group by clause (also if Column is made by a function).
We are confident that this can contribute to the growing of this Engine. 

This is a sample of a statement that we would like to execute with success:
{noformat}
with w_test as
  (select '1' as one,
          2 as two,
          '3' as three)
select one as one,
       substring(cast(two as string), 1, 1) as two,
       three as three,
       count(1) as cnt
from w_test
group by one,
         substring(cast(two as string), 1, 1),
         three

ERROR: AnalysisException: select list expression not produced by aggregation output (missing from GROUP BY clause?): substring(CAST(two AS STRING), 1, 1)
{noformat}


It works writing the statements as:

*workaround.1:*
{noformat}
with w_test as
  (select '1' as one,
          2 as two,
          '3' as three)
select one,
       substring(cast(two as string), 1, 1),
       three,
       count(1)
from w_test
group by one,
         substring(cast(two as string), 1, 1),
         three
{noformat}

*workaround.2:*
{noformat}
with w_test as
  (select '1' as one,
          2 as two,
          '3' as three)
select one as one,
       substring(cast(two as string), 1, 1) as two,
       three as three,
       count(1) as cnt
from w_test
group by 1,
         2,
         3
{noformat}

*workaround 3:*
{noformat}
with w_test as
  (select '1' as one,
          2 as two,
          '3' as three)
select one as t_one,
       substring(cast(two as string), 1, 1) as t_two,
       three as t_three,
       count(1) as cnt
from w_test
group by t_one,
         t_two,
         t_three
{noformat}"	IMPALA	Closed	3	1	2770	incompatibility, ramp-up, sql-language
13390978	Hit DCHECK in DecimalUtil::DecodeFromFixedLenByteArray for core-s3 build	"Saw this build failure inasf-master-core-s3 build:

[https://master-03.jenkins.cloudera.com/job/impala-asf-master-core-s3/61/]



*Error Message*

DCHECK found in log file: /data/jenkins/workspace/impala-asf-master-core-s3/repos/Impala/logs/ee_tests/impalad.FATAL
h3. Standard Error

Log file created at: 2021/07/19 18:41:06 Running on machine:

[impala-ec2-centos74-m5-4xlarge-ondemand-072f.vpc.cloudera.com|http://impala-ec2-centos74-m5-4xlarge-ondemand-072f.vpc.cloudera.com/]

Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid [file:line|file:///line]] msg

F0719 18:41:06.730994 4601 decimal-util.h:129] fb4b98709a88f345:b51bf00b00000002] Check failed: fixed_len_size > 0 (-15 vs. 0)

F0719 18:41:08.161149 4711 decimal-util.h:129] e5432b6d3730539d:cf6c2d3100000002] Check failed: fixed_len_size > 0 (-15 vs. 0)



From timestamp, the issue seems happened in test: query_test/test_scanners_fuzz.py::TestScannersFuzzing::test_fuzz_uncompressed_parquet_orc"	IMPALA	Resolved	3	1	2770	broken-build
13570424	Invoke validateDataFilesExist for RowDelta operations	"We must invoke validateDataFilesExist for RowDelta operations (DELETE/UPDATE/MERGE).

Without this a concurrent RewriteFiles (compaction) and RowDelta can corrupt a table."	IMPALA	Resolved	3	1	2770	impala-iceberg
13584669	Failed test: test_migrated_table_field_id_resolution with missing metadata file	"Stack trace:
{code}
query_test/test_iceberg.py:270: in test_migrated_table_field_id_resolution
    vector, unique_database)
common/impala_test_suite.py:725: in run_test_case
    result = exec_fn(query, user=test_section.get('USER', '').strip() or None)
common/impala_test_suite.py:660: in __exec_in_impala
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:1013: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:216: in execute
    fetch_profile_after_close=fetch_profile_after_close)
beeswax/impala_beeswax.py:191: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:384: in __execute_query
    self.wait_for_finished(handle)
beeswax/impala_beeswax.py:405: in wait_for_finished
    raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:ImpalaRuntimeException: Failed to ALTER table 'iceberg_migrated_alter_test': Metadata file for version 3 is missing
{code}"	IMPALA	Resolved	1	1	2770	broken-build, impala-iceberg
13351493	Use Iceberg's fixed partition transforms	"Currently the Iceberg time and date partition transforms are wrong if the data is before the epoch.

There's already an Iceberg pull request about it: [https://github.com/apache/iceberg/pull/1981]

Becauce of this bug Impala doesn't prune Iceberg partitions if the predicate refers to timestamps before the epoch.

Once the above pull request is merged we need to update our Iceberg dependency."	IMPALA	Resolved	3	1	2770	impala-iceberg
13535820	Rewrite DELETE statements to TRUNCATE if possible	"If the user issues DELETE FROM t; to remove all rows from a table, we should rewrite it to TRUNCATE TABLE t; as it is much more efficient in some cases.

E.g., for Iceberg tables DELETE FROM t; would create delete files that contain all existing rows. Then subsequent readers would have to read all data files and delete files just to return an empty result set. Wherease TRUNCATE TABLE t; just creates a new empty table snapshot.

We'll need to investigate if it makes sense for Kudu tables as well."	IMPALA	Open	3	1	2770	impala-iceberg, performance
13358208	Hit DCHECK in parquet-column-readers.cc:  def_levels_.CacheRemaining() <= num_buffered_values_	"https://jenkins.impala.io/job/ubuntu-16.04-dockerised-tests/3814/

{noformat}
F0211 03:55:26.383247 14487 parquet-column-readers.cc:517] be46bb72819942fd:85934edd00000001] Check failed: def_levels_.CacheRemaining() <= num_buffered_values_ (921 vs. 916) 
*** Check failure stack trace: ***
    @          0x53646ec  google::LogMessage::Fail()
    @          0x5365fdc  google::LogMessage::SendToLog()
    @          0x536404a  google::LogMessage::Flush()
    @          0x5367c48  google::LogMessageFatal::~LogMessageFatal()
    @          0x2ff886f  impala::ScalarColumnReader<>::MaterializeValueBatch<>()
    @          0x2f8ae44  impala::ScalarColumnReader<>::MaterializeValueBatch<>()
    @          0x2f761bf  impala::ScalarColumnReader<>::ReadValueBatch<>()
    @          0x2f2889a  impala::ScalarColumnReader<>::ReadValueBatch()
    @          0x2ebd8c0  impala::HdfsParquetScanner::AssembleRows()
    @          0x2eb882e  impala::HdfsParquetScanner::GetNextInternal()
    @          0x2eb67bd  impala::HdfsParquetScanner::ProcessSplit()
    @          0x2aaf3f2  impala::HdfsScanNode::ProcessSplit()
    @          0x2aae773  impala::HdfsScanNode::ScannerThread()
    @          0x2aadadb  _ZZN6impala12HdfsScanNode22ThreadTokenAvailableCbEPNS_18ThreadResourcePoolEENKUlvE_clEv
    @          0x2aafe94  _ZN5boost6detail8function26void_function_obj_invoker0IZN6impala12HdfsScanNode22ThreadTokenAvailableCbEPNS3_18ThreadResourcePoolEEUlvE_vE6invokeERNS1_15function_bufferE
    @          0x220e331  boost::function0<>::operator()()
    @          0x2842e7f  impala::Thread::SuperviseThread()
    @          0x284ae1c  boost::_bi::list5<>::operator()<>()
    @          0x284ad40  boost::_bi::bind_t<>::operator()()
    @          0x284ad01  boost::detail::thread_data<>::run()
    @          0x406b291  thread_proxy
    @     0x7f2465cba6b9  start_thread
    @     0x7f24627e64dc  clone
rImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:431)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362)
	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:866)
{noformat}

It was likely a fuzz test:
{noformat}
19:55:23 query_test/test_mem_usage_scaling.py::TestTpchMemLimitError::test_low_mem_limit_q22[mem_limit: 50 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:23 [gw5] PASSED query_test/test_mem_usage_scaling.py::TestTpchMemLimitError::test_low_mem_limit_q22[mem_limit: 50 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:23 query_test/test_mem_usage_scaling.py::TestTpchMemLimitError::test_low_mem_limit_q22[mem_limit: 80 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:25 [gw2] PASSED query_test/test_queries.py::TestPartitionKeyScans::test_partition_key_scans[protocol: beeswax | exec_option: {'mt_dop': 0, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:25 query_test/test_queries.py::TestPartitionKeyScans::test_partition_key_scans[protocol: beeswax | exec_option: {'mt_dop': 1, 'exec_single_node_rows_threshold': 0} | table_format: avro/snap/block] 
19:55:26 [gw5] PASSED query_test/test_mem_usage_scaling.py::TestTpchMemLimitError::test_low_mem_limit_q22[mem_limit: 80 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:26 query_test/test_mem_usage_scaling.py::TestTpchMemLimitError::test_low_mem_limit_q22[mem_limit: 130 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:26 [gw6] PASSED query_test/test_scanners.py::TestIceberg::test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:26 query_test/test_scanners.py::TestIceberg::test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:27 [gw3] FAILED query_test/test_decimal_fuzz.py::TestDecimalFuzz::test_decimal_ops[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0}] 
19:55:28 query_test/test_decimal_fuzz.py::TestDecimalFuzz::test_width_bucket[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0}] 
19:55:28 [gw3] FAILED query_test/test_decimal_fuzz.py::TestDecimalFuzz::test_width_bucket[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0}] 
19:55:28 query_test/test_decimal_queries.py::TestDecimalQueries::test_queries[protocol: beeswax | exec_option: {'disable_codegen_rows_threshold': 0, 'disable_codegen': 'false', 'decimal_v2': 'false', 'batch_size': 0} | table_format: text/none] 
19:55:28 [gw6] ERROR query_test/test_scanners.py::TestIceberg::test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:28 [gw8] FAILED query_test/test_join_queries.py::TestJoinQueries::test_empty_build_joins[protocol: beeswax | table_format: parquet/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | enable_outer_join_to_inner_transformation: false | batch_size: 0 | mt_dop: 0] 
19:55:28 [gw13] FAILED query_test/test_parquet_stats.py::TestParquetStats::test_page_index[mt_dop: 0 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:28 [gw12] FAILED query_test/test_runtime_filters.py::TestRuntimeRowFilters::test_row_filters[mt_dop: 0 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:28 query_test/test_runtime_filters.py::TestRuntimeRowFilters::test_row_filters[mt_dop: 4 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
19:55:28 query_test/test_join_queries.py::TestJoinQueries::test_empty_build_joins[protocol: beeswax | table_format: parquet/none | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | enable_outer_join_to_inner_transformation: true | batch_size: 0 | mt_dop: 4] 
19:55:28 [gw1] FAILED query_test/test_scanners.py::TestScannersAllTableFormatsWithLimit::test_limit[mt_dop: 0 | protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] 
{noformat}"	IMPALA	Resolved	1	4	2770	broken-build, crash, flaky, parquet
13412219	Impala uses wrong file descriptors for Iceberg tables in local catalog mode	"When local catalog mode is used, Impala retrieves the Iceberg snapshot from CatalogD. The response contains a map of the file descriptors.
https://github.com/apache/impala/blob/b692a92fa2a2277a185fb5823592609b4603c0d8/fe/src/main/java/org/apache/impala/catalog/local/CatalogdMetaProvider.java#L1006

The file descriptors contain block location information, but the hosts are only referred by indexes.
https://github.com/apache/impala/blob/b692a92fa2a2277a185fb5823592609b4603c0d8/common/fbs/CatalogObjects.fbs#L50

In the Coordinator's local catalog the host indexes might refer to different hosts than in CatalogD. We should translate the host indexes to the coordinators host list. Similarly to the LocalFsTable:
https://github.com/apache/impala/blob/b692a92fa2a2277a185fb5823592609b4603c0d8/fe/src/main/java/org/apache/impala/catalog/local/CatalogdMetaProvider.java#L983
https://github.com/apache/impala/blob/b692a92fa2a2277a185fb5823592609b4603c0d8/fe/src/main/java/org/apache/impala/catalog/local/CatalogdMetaProvider.java#L1020-L1024"	IMPALA	Resolved	3	1	2770	impala-iceberg
13355148	Impala should write normalized paths in Iceberg manifests	"Currently Impala writes double slashes in the paths of datafiles for non-partitioned Iceberg tables, e.g.:
{noformat}
hdfs://localhost:20500/test-warehouse/ice_t/data//594828b035d480b7-9c9fd8eb00000000_173316607_data.0.parq{noformat}
Paths should be normalized so they won't cause any problems later."	IMPALA	Resolved	3	1	2770	impala-iceberg
13511419	Cardinality estimate for UNION in Iceberg position-delete plans can double the actual table cardinality	"The plan for Iceberg tables with position-delete files includes a UNION operator that takes the following inputs:
 * LHS: Scan of the data files that don't have corresponding delete files
 * RHS: ANTI JOIN that filters the data files that do have corresponding delete files based on the content of the delete files.

The planner's cardinality estimates for each of these two inputs to the UNION can be as large as the full row count of the table (assuming no other predicates in the scan) and the planner simply sums these in the UNION which can result in a cardinality estimate for the UNION that's twice the size of the table. For example here's an excerpt from the TPC-DS query 78 summary (the total row count for the full store_sales table is 8.64B and the planner estimate of the union cardinality is twice that):


{noformat}
Operator               #Hosts #Inst  Avg Time  Max Time  #Rows Est. #Rows  Peak Mem Est. Peak Mem Detail
...
| | 04:UNION              10  120  9.168ms  17.416ms  1.66B   17.28B      0       0
| | |--02:DELETE EVENTS HASH JOIN    10  120 138.821us 615.342us    0    8.64B  42.12 KB       0 LEFT ANTI JOIN, BROADCAST
| | | |--F22:JOIN BUILD        10   10  5s741ms  5s944ms             3.09 GB    3.07 GB
| | | | 35:EXCHANGE          10   10 212.271ms 264.374ms  12.84M   12.84M  14.98 MB    36.84 MB BROADCAST
| | | | F12:EXCHANGE SENDER      10  120  30.579ms  40.774ms             60.94 KB       0
| | | | 01:SCAN S3          10  120  75.266ms 417.286ms  12.84M   12.84M  468.96 KB    16.00 MB tpcds_3000_iceberg_parquet_v2_update_q1_98.store_sales-POSITION-DELETE-01 tpcds_3000_iceberg_parquet_v2_update_q1_98.store_sales-position-delete
| | | 00:SCAN S3            10  120  2s645ms  10s064ms    0    8.64B   4.01 MB    16.00 MB tpcds_3000_iceberg_parquet_v2_update_q1_98.store_sales
| | 03:SCAN S3             10  120  2s413ms  4s484ms  1.66B    8.64B  103.32 MB    88.00 MB tpcds_3000_iceberg_parquet_v2_update_q1_98.store_sales

{noformat}
The planner should account for the fact that each side of the UNION is only scanning a subset of the table (so the UNION cardinality can't be greater than the actual table size) and also if possible try to estimate the impact of the filtering done by the ANTI JOIN."	IMPALA	Resolved	3	1	2770	impala-iceberg
13341184	Iceberg tests failing with  Failed to load metadata for table: 'iceberg_partitioned'	" Failed to load metadata for table: 'iceberg_partitioned'

Affected tests
{noformat}
    query_test.test_iceberg.TestIcebergTable.test_iceberg_negative[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@1.0', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@1.0', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': 'HDFS_SCANNER_THREAD_CHECK_SOFT_MEM_LIMIT:FAIL@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': 'HDFS_SCANNER_THREAD_CHECK_SOFT_MEM_LIMIT:FAIL@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@1.0', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': '-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@1.0', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'debug_action': 'HDFS_SCANNER_THREAD_CHECK_SOFT_MEM_LIMIT:FAIL@0.5', 'exec_single_node_rows_threshold': 0} | table_format: parquet/none]
    query_test.test_scanners.TestIceberg.test_iceberg_profile[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': 'HDFS_SCANNER_THREAD_CHECK_SOFT_MEM_LIMIT:FAIL@0.5', 'exec_single_node_rows_threshold': 0} | 
{noformat}

Example error:
{noformat}
query_test.test_scanners.TestIceberg.test_iceberg_query[protocol: beeswax | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: parquet/none] (from pytest)
Failing for the past 1 build (Since Failed#672 )
Took 0.25 sec.
add description
Error Message

query_test/test_scanners.py:357: in test_iceberg_query     self.run_test_case('QueryTest/iceberg-query', vector) common/impala_test_suite.py:662: in run_test_case     result = exec_fn(query, user=test_section.get('USER', '').strip() or None) common/impala_test_suite.py:600: in __exec_in_impala     result = self.__execute_query(target_impalad_client, query, user=user) common/impala_test_suite.py:920: in __execute_query     return impalad_client.execute(query, user=user) common/impala_connection.py:205: in execute     return self.__beeswax_client.execute(sql_stmt, user=user) beeswax/impala_beeswax.py:187: in execute     handle = self.__execute_query(query_string.strip(), user=user) beeswax/impala_beeswax.py:363: in __execute_query     handle = self.execute_query_async(query_string, user=user) beeswax/impala_beeswax.py:357: in execute_query_async     handle = self.__do_rpc(lambda: self.imp_service.query(query,)) beeswax/impala_beeswax.py:520: in __do_rpc     raise ImpalaBeeswaxException(self.__build_error_message(b), b) E   ImpalaBeeswaxException: ImpalaBeeswaxException: E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'> E    MESSAGE: AnalysisException: Failed to load metadata for table: 'iceberg_partitioned' E   CAUSED BY: TableLoadingException: Unrecognized table type for table: functional_parquet.iceberg_partitioned

Stacktrace

query_test/test_scanners.py:357: in test_iceberg_query
    self.run_test_case('QueryTest/iceberg-query', vector)
common/impala_test_suite.py:662: in run_test_case
    result = exec_fn(query, user=test_section.get('USER', '').strip() or None)
common/impala_test_suite.py:600: in __exec_in_impala
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:920: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:205: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:187: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:363: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:357: in execute_query_async
    handle = self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:520: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: AnalysisException: Failed to load metadata for table: 'iceberg_partitioned'
E   CAUSED BY: TableLoadingException: Unrecognized table type for table: functional_parquet.iceberg_partitioned

Standard Error

SET client_identifier=query_test/test_scanners.py::TestIceberg::()::test_iceberg_query[protocol:beeswax|exec_option:{'batch_size':0;'num_nodes':0;'disable_codegen_rows_threshold':0;'disable_codegen':False;'abort_on_error':1;'debug_action':None;'exec_single_node_rows_threshold';
-- executing against localhost:21000

use functional_parquet;

-- 2020-11-13 12:52:58,534 INFO     MainThread: Started query 6c4f1f4b3e4405f9:22caf6de00000000
SET client_identifier=query_test/test_scanners.py::TestIceberg::()::test_iceberg_query[protocol:beeswax|exec_option:{'batch_size':0;'num_nodes':0;'disable_codegen_rows_threshold':0;'disable_codegen':False;'abort_on_error':1;'debug_action':None;'exec_single_node_rows_threshold';
SET batch_size=0;
SET num_nodes=0;
SET disable_codegen_rows_threshold=0;
SET disable_codegen=False;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
-- 2020-11-13 12:52:58,537 INFO     MainThread: Loading query test file: /data/jenkins/workspace/impala-asf-master-core-s3/repos/Impala/testdata/workloads/functional-query/queries/QueryTest/iceberg-query.test
-- 2020-11-13 12:52:58,540 INFO     MainThread: Starting new HTTP connection (1): localhost
-- executing against localhost:21000

SELECT count(*) from iceberg_partitioned;
{noformat}"	IMPALA	Resolved	3	1	2770	broken-build, flaky, iceberg
13250314	"test_acid_insert is failing with ""Processor has no capabilities"""	"{noformat}
query_test.test_insert.TestInsertQueries.test_acid_insert[compression_codec: none | protocol: beeswax | exec_option: {'sync_ddl': 0, 'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none] (from pytest)

Failing for the past 1 build (Since Failed#82 )
Took 40 ms.
add description
Error Message
MetaException: MetaException(_message='Processor has no capabilities, cannot create an ACID table.')
Stacktrace
query_test/test_insert.py:155: in test_acid_insert
    multiple_impalad=vector.get_value('exec_option')['sync_ddl'] == 1)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/tests/common/impala_test_suite.py:556: in run_test_case
    self.execute_test_case_setup(test_section['SETUP'], table_format_info)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/tests/common/impala_test_suite.py:656: in execute_test_case_setup
    self.__reset_table(db_name, table_name)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/tests/common/impala_test_suite.py:809: in __reset_table
    self.hive_client.create_table(table)
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/shell/gen-py/hive_metastore/ThriftHiveMetastore.py:2483: in create_table
    self.recv_create_table()
/data/jenkins/workspace/impala-cdpd-master-exhaustive-release/repos/Impala/shell/gen-py/hive_metastore/ThriftHiveMetastore.py:2509: in recv_create_table
    raise result.o3
E   MetaException: MetaException(_message='Processor has no capabilities, cannot create an ACID table.')
Standard Error
SET client_identifier=query_test/test_insert.py::TestInsertQueries::()::test_acid_insert[compression_codec:none|protocol:beeswax|exec_option:{'sync_ddl':0;'batch_size':0;'num_nodes':0;'disable_codegen_rows_threshold':0;'disable_codegen':True;'abort_on_error':1;'exec_single_nod;
-- executing against localhost:21000
use functional;

-- 2019-08-11 15:41:53,042 INFO     MainThread: Started query 904ec6d54245fbc8:98705d6400000000
SET client_identifier=query_test/test_insert.py::TestInsertQueries::()::test_acid_insert[compression_codec:none|protocol:beeswax|exec_option:{'sync_ddl':0;'batch_size':0;'num_nodes':0;'disable_codegen_rows_threshold':0;'disable_codegen':True;'abort_on_error':1;'exec_single_nod;
SET sync_ddl=0;
SET batch_size=0;
SET num_nodes=0;
SET disable_codegen_rows_threshold=0;
SET disable_codegen=True;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
{noformat}"	IMPALA	Resolved	1	1	2770	broken-build
13402295	TestIcebergTable.test_partitioned_insert fails with IOException	"The test query_test.test_iceberg.TestIcebergTable.test_partitioned_insert fails intermittently with a IOException and stack trace below.

{noformat}
uery_test/test_iceberg.py:80: in test_partitioned_insert
    use_db=unique_database)
common/impala_test_suite.py:682: in run_test_case
    result = exec_fn(query, user=test_section.get('USER', '').strip() or None)
common/impala_test_suite.py:620: in __exec_in_impala
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:940: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:212: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:189: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:367: in __execute_query
    self.wait_for_finished(handle)
beeswax/impala_beeswax.py:388: in wait_for_finished
    raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:RuntimeIOException: Failed to write json to file: hdfs://localhost:20500/test-warehouse/test_partitioned_insert_af8be2c3.db/ice_only_part/metadata/00002-b8d13a74-4839-4dd3-b74a-6df9436774a2.metadata.json
E   CAUSED BY: IOException: The stream is closed
{noformat}"	IMPALA	Open	2	1	2770	broken-build
13506649	CatalogD OOMkilled due to natively allocated memory	"We can hit this bug in several installations: https://bugs.openjdk.org/browse/JDK-8257032
(another nice description of the issue can be found here: https://medium.com/swlh/native-memory-the-silent-jvm-killer-595913cba8e7)

The problem is that we are creating our own Deflater object and pass it to the constructor of DeflaterOutputStream:
https://github.com/apache/impala/blob/84fa6d210d3966e5ece8b4ac84ff8bd8780dec4e/fe/src/main/java/org/apache/impala/util/CompressionUtil.java#L47

This means that Java's DeflaterOutputStream won't assume ownership on the Deflater, and won't invoke its end() method:
* https://github.com/openjdk/jdk/blob/a249a52501f3cd7d4fbe5293d14ac8d0d6ffcc69/src/java.base/share/classes/java/util/zip/DeflaterOutputStream.java#L144
* https://github.com/openjdk/jdk/blob/a249a52501f3cd7d4fbe5293d14ac8d0d6ffcc69/src/java.base/share/classes/java/util/zip/DeflaterOutputStream.java#L246-L247

The Deflater's methods are implemented in C and allocate native memory. This means that until the GC doesn't destroy the unreachable Deflater objects they can consume quite much native memory. In some scenarios it can even result in OOMKills by the kernel.

We should solve this issue by either
* override close() of DeflaterOutputStream (like mentioned in the above blog post)
* directly invoke end() on the Deflater object in a finally clause"	IMPALA	Resolved	2	1	2770	OOM
13433172	Iceberg table cannot be loaded when partition value is NULL	"We get NullPointerException when we try to load an Iceberg table with a NULL partition:

{noformat}
Thread-11[1] where
  [1] org.apache.impala.util.IcebergUtil.createPartitionTransformValue (IcebergUtil.java:891)
  [2] org.apache.impala.util.IcebergUtil.createPartitionKeys (IcebergUtil.java:879)
  [3] org.apache.impala.util.IcebergUtil.createIcebergMetadata (IcebergUtil.java:857)
  [4] org.apache.impala.util.IcebergUtil.createIcebergMetadata (IcebergUtil.java:844)
  [5] org.apache.impala.catalog.FeIcebergTable$Utils.loadAllPartition (FeIcebergTable.java:583)
  [6] org.apache.impala.catalog.IcebergTable.load (IcebergTable.java:348)
  [7] org.apache.impala.service.CatalogOpExecutor.loadTableMetadata (CatalogOpExecutor.java:1,430)
  [8] org.apache.impala.service.CatalogOpExecutor.updateCatalog (CatalogOpExecutor.java:6,386)
  [9] org.apache.impala.service.JniCatalog.updateCatalog (JniCatalog.java:431)
{noformat}

The issue is reproducible on current master by:

{noformat}
create table store_sales partitioned by spec (ss_sold_date_sk) stored as iceberg as select * from tpcds_parquet.store_sales;
{noformat}
"	IMPALA	Resolved	3	1	2770	impala-iceberg
13477408	Iceberg test test_expire_snapshots is flaky	"h2. Stacktrace

{noformat}
query_test/test_iceberg.py:104: in test_expire_snapshots
    impalad_client.execute(insert_q)
common/impala_connection.py:212: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:189: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:365: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:359: in execute_query_async
    handle = self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:522: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: AnalysisException: org.apache.impala.catalog.TableLoadingException: Error opening Iceberg table 'test_expire_snapshots_e488dbc3.expire_snapshots'
E   CAUSED BY: TableLoadingException: Error opening Iceberg table 'test_expire_snapshots_e488dbc3.expire_snapshots'
E   CAUSED BY: InconsistentMetadataFetchException: Catalog object TCatalogObject(type:TABLE, catalog_version:7309, table:TTable(db_name:test_expire_snapshots_e488dbc3, tbl_name:expire_snapshots)) changed version between accesses.
{noformat}

The error might be due to not detecting all self-events correctly, so the IcebergTable gets updated on the CatalogD side."	IMPALA	Resolved	3	1	2770	broken-build, impala-iceberg
13205802	Allow filtering on virtual column for file name	"An additional performance enhancement would be the capability to filter on file names using a virtual column. This would be somewhat like the current optimization of sorting data and skipping files based on parquet metadata, but instead you put something in the file name to indicate it's contents should be filtered.

For example say you were writing first names and then searching for them, during your writing phase you put the first letter of the first name into your file name, so if I'm storingAlice, Bob, Cathy, my file name is ""ABC"" then when doing a query you could filter based on where INPUT__FILE__NAME contains ""D"" when searching for David and skip reading the file.

Anotheruse would be if you had a daily partition, and you put the timestamp into the file name, then limit the search to only the last hour even though your partition is daily. This then gives you the ability to sort by another column making searches even faster on both.



This requiresIMPALA-801"	IMPALA	Resolved	3	4	2770	built-in-function
13333915	Implement CREATE TABLE AS SELECT for Iceberg tables	Add support for CTAS statements for Iceberg tables.	IMPALA	Resolved	3	7	2770	impala-iceberg
13342672	Correct Iceberg type mappings	"The Iceberg format spec defines what types to use for different file formats, e.g.:

[https://iceberg.apache.org/spec/#parquet]

Impala should follow the specification:
 * Strings in Iceberg tables should be annotated with UTF8 (when file format is Parquet)
 * Revise fixed(L) <-> CHAR(L) mapping
 ** CHAR(L) in Impala is annotated with UTF8
 ** fixed(L) is for fixed-length binary data
 ** We should probably remove this mapping

UPDATE: A [recent pull request|https://github.com/apache/iceberg/pull/1612] added a new Iceberg class, [HiveSchemaConverter|https://github.com/apache/iceberg/blob/master/mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaConverter.java]. We should use this class to convert between Iceberg and Hive schemas."	IMPALA	Resolved	3	1	2770	impala-iceberg
13546808	Add missing package-info file used by HiveVersionInfo	"We create a minimal-impala-hive-exec.jar based on Hive's hive-exec.jar:
https://github.com/apache/impala/blob/master/java/shaded-deps/hive-exec/pom.xml#L34

This excludes lots of class files, including org/apache/hive/common/package-info.class that is used by HiveVersionAnnotation and HiveVersionInfo classes.

Because of this HiveVersionInfo returns ""Unknown"" version resulting in failing Iceberg operations, e.g.:
https://github.com/apache/iceberg/blob/2535c3a8b18910a926df45d267271aecd83317b3/hive-metastore/src/main/java/org/apache/iceberg/hive/MetastoreLock.java#L364"	IMPALA	Resolved	3	1	2770	impala-iceberg
13477983	Create better cardinality estimates for Iceberg V2 tables with deletes	Create better cardinality estimates for Iceberg V2 tables with deletes.	IMPALA	Resolved	3	7	2770	impala-iceberg
13498595	Table properties are not updated in Iceberg metadata files	"This issue occurs in true external Hive Catalog tables.

Iceberg stores the default file format in a table property called 'write.format.default'. HMS also stores this value loaded from the Iceberg metadata json file.

However, when this table property is altered through Impala, it is only changed in HMS, but does not update the Iceberg snapshot. When the table data is reloaded from Iceberg metadata, the old value will appear in HMS and the change is lost.

This bug does not affect table properties that are not stored in Iceberg, because they will not be reloaded."	IMPALA	Resolved	2	1	2770	impala-iceberg
13155257	Wrong results with EXISTS subquery containing ORDER BY, LIMIT, and OFFSET	"Queries may return wrong results if an EXISTS subquery has an ORDER BY with a LIMIT and OFFSET clause. The EXISTS subquery may incorrectly evaluate to TRUE even though it s FALSE.

Reproduction:
{code}
select count(*) from functional.alltypestiny t where
exists (select id from functional.alltypestiny where id < 5 order by id limit 10 offset 6);
{code}
The query should return ""0"" but it incorrectly returns ""8"" because an incorrect plan without the offset is generated. See plan:
{code}
+-------------------------------------------------+
| Explain String                                  |
+-------------------------------------------------+
| Max Per-Host Resource Reservation: Memory=0B    |
| Per-Host Resource Estimates: Memory=84.00MB     |
| Codegen disabled by planner                     |
|                                                 |
| PLAN-ROOT SINK                                  |
| |                                               |
| 08:AGGREGATE [FINALIZE]                         |
| |  output: count:merge(*)                       |
| |                                               |
| 07:EXCHANGE [UNPARTITIONED]                     |
| |                                               |
| 04:AGGREGATE                                    |
| |  output: count(*)                             |
| |                                               |
| 03:NESTED LOOP JOIN [LEFT SEMI JOIN, BROADCAST] |
| |                                               |
| |--06:EXCHANGE [BROADCAST]                      |
| |  |                                            |
| |  05:MERGING-EXCHANGE [UNPARTITIONED]          |
| |  |  order by: id ASC                          |
| |  |  limit: 1                                  |
| |  |                                            |
| |  02:TOP-N [LIMIT=1]                           |
| |  |  order by: id ASC                          |
| |  |                                            |
| |  01:SCAN HDFS [functional.alltypestiny]       |
| |     partitions=4/4 files=4 size=460B          |
| |     predicates: id < 5                        |
| |                                               |
| 00:SCAN HDFS [functional.alltypestiny t]        |
|    partitions=4/4 files=4 size=460B             |
+-------------------------------------------------+
{code}

Evaluating the subquery by itself gives the expected results:
{code}
select id from functional.alltypestiny where id < 5 order by id limit 10 offset 6;
<empty result set>
{code}"	IMPALA	Resolved	1	1	2770	correctness, planner
13290894	Milestone 1: properly scan files that has full ACID schema	"

Full ACID row format looks like this:
{
 ""operation"": 0,
 ""originalTransaction"": 1,
 ""bucket"": 536870912,
 ""rowId"": 0,
 ""currentTransaction"": 1,
 ""row"": \{""i"": 1}
}

User columns are nested under ""row"". The frontend should create proper tuples and slot descriptors for the scan nodes to read the files correctly.

We should be able to query the ACID columns, at least for debugging/testing. Hive uses the special row__id identifier for that.

Impala should raise an error if there are delete deltas. Directory filtering should filter out minor compacted directories since the records from those need validation.

Non-goals in this sub-task:
 * row validation against validWriteIdList
 * reading ""original files"" (files in non-ACID format)
 * reading delete deltas"	IMPALA	Resolved	3	7	2770	impala-acid
13483523	Impala fails to read Iceberg snapshot tables created by Spark using call spark_catalog.system.snapshot() 	"I get an exception from catalog when Impala tries to read an Iceberg ""snapshot"" table created by a spark job using this API:[https://iceberg.apache.org/docs/latest/spark-procedures/#snapshot]


{noformat}
Query submitted at: 2022-09-27 09:43:15 (Coordinator: http://coordinator-0.coordinator-int.impala-1662757723-t7hk.svc.cluster.local:25000)
ERROR: AnalysisException: Failed to load metadata for table: 'customer_iceberg_snapshot'
CAUSED BY: TableLoadingException: Could not load table tpcds_3000_string_parquet_external.customer_iceberg_snapshot from catalog
CAUSED BY: TException: TGetPartialCatalogObjectResponse(status:TStatus(status_code:GENERAL, error_msgs:[IcebergTableLoadingException: Error loading metadata for Iceberg table s3a://drorke-dwxperf2/warehouse/tablespace/external/hive/tpcds_3000_string_parquet_external.db/customer_iceberg_snapshot
CAUSED BY: RuntimeException: FileSystem returned an unexpected path s3a://drorke-dwxperf2/warehouse/tablespace/external/hive/tpcds_3000_string_parquet_external.db/customer/0b443b4d91612be0-af2edb1b00000000_1194661597_data.0.parq for a file within s3a://drorke-dwxperf2/warehouse/tablespace/external/hive/tpcds_3000_string_parquet_external.db/customer_iceberg_snapshot]), lookup_status:OK)
Looks like Impala gets confused in FileSystemUtils.relativizePath() because the final directory component in the new snapshot table name (customer_iceberg_snapshot) is different from the corresponding path component in the file.
{noformat}

It looks like Impala gets confused in FileSystemUtils.relativizePath() because the final directory component in the new snapshot table name (customer_iceberg_snapshot) is different from the corresponding path component in the file.

"	IMPALA	Open	3	1	2770	impala-iceberg
13544121	Improve incremental load of Iceberg tables	"*The followings mostly affect HDFS/Ozone where we need to contact the NameNode to create file descriptors with block locations. On cloud object stores where there are no block locations, we only need the Iceberg metadata to create the file descriptors.*

Currently we always reload all the metadata belonging to an Iceberg table.
This means we recreate all the file descriptors even if only a few of them have changed.

We could check the amount of the newly added files, and if there's only a few of them then we should only load the file descriptors for those one by one.

We can fallback to a full reload if a significant amount of files have changed, i.e. when it is better to use a recursive file listing."	IMPALA	Resolved	3	1	2770	impala-iceberg, performance
13345092	Don't allow PARTITION BY SPEC for non-Iceberg tables	PARTITION BY SPEC is only valid for Iceberg tables so Impala should raise an error when it is used for non-Iceberg tables.	IMPALA	Resolved	3	1	2770	impala-iceberg
13533195	Include snapshot id of Iceberg tables in query plan / profile	"Including the snapshot id of Iceberg tables for the Iceberg SCAN operators can be useful to enable replayable queries.

Replayable queries are useful, so we can better investigate performance problems / bugs."	IMPALA	Resolved	3	1	2770	impala-iceberg, ramp-up
13571631	Optimized count(*) for Iceberg gives wrong results after a Spark rewrite_data_files	"Issue was introduced by https://issues.apache.org/jira/browse/IMPALA-11802 that implemented an optimized way to get results for count(*). However, if the table was compacted by Spark this optimization can give incorrect results.

The reason is that Spark can[ skip dropping delete files|https://iceberg.apache.org/docs/latest/spark-procedures/#rewrite_position_delete_files] that are pointing to compacted data files, as a result there might be delete files after compaction that are no longer applied to any data files.

Repro:

With Impala
{code:java}
create table default.iceberg_testing (id int, j bigint) STORED AS ICEBERG
TBLPROPERTIES('iceberg.catalog'='hadoop.catalog',
       'iceberg.catalog_location'='/tmp/spark_iceberg_catalog/',
       'iceberg.table_identifier'='iceberg_testing',
       'format-version'='2');
insert into iceberg_testing values
    (1, 1), (2, 4), (3, 9), (4, 16), (5, 25);
update iceberg_testing set j = -100 where id = 4;
delete from iceberg_testing where id = 4;{code}
Count * returns 4 at this point.

Run compaction in Spark:
{code:java}
spark.sql(s""CALL local.system.rewrite_data_files(table => 'default.iceberg_testing', options => map('min-input-files','2') )"").show() {code}
Now count * in Impala returns 8 (might require an IM if in HadoopCatalog). Hive returns correct results. Also a SELECT * returns correct results."	IMPALA	Resolved	2	1	2770	correctness, impala-iceberg
13481528	Use Iceberg APIs to update table properties for Iceberg tables	"COMPUTE STATS updates table-level stats via alter_table() HMS API. This replaces the whole HMS table, therefore if there are concurrent modifications by another engine, e.g. Hive, it's possible that these modifications are lost.

This is critical for Iceberg tables, as the 'metadata_location' table property must always point to the latest snapshot. Inadvertently rewriting it during COMPUTE STATS can result in a data loss.

Table-level stats like 'numRows' and 'totalSize' are already updated by Iceberg during table modifications, i.e. there is no need to update these values for COMPUTE STATS.

Column stats are not affected as they are updated via a different API call ([updateTableColumnStatistics|https://github.com/apache/impala/blob/4e813b7085c995a7244ef886b00c22e9d93cc80c/fe/src/main/java/org/apache/impala/service/CatalogOpExecutor.java#L1638()]), and it doesn't touch the table properties. But updating statistics also require us to update table property ""impala.lastComputeStatsTime"".  We should update it via Iceberg APIs when HiveCatalog is used:
https://github.com/apache/impala/blob/4e813b7085c995a7244ef886b00c22e9d93cc80c/fe/src/main/java/org/apache/impala/service/IcebergCatalogOpExecutor.java#L211

For other catalogs than HiveCatalog we still need to update the table property via HMS API. It should be safe as other catalogs don't depend on HMS table properties.

Reloading the HMS table before invoking 'alter_table()' can be considered in other cases (non-Iceberg tables as well), to decrease the possibility of losing concurrent table updates."	IMPALA	Resolved	3	1	2770	impala-iceberg
13439673	TestAcid.test_lock_timings is flaky	"query_test.test_acid.TestAcid.test_lock_timings[protocol: beeswax | exec_option: {'test_replan': 1, 'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none]

stacktrace:
query_test/test_acid.py:343: in test_lock_timings
    assert elapsed > 20 and elapsed < 25
E   assert (25.887187957763672 > 20 and 25.887187957763672 < 25)

"	IMPALA	Resolved	3	1	2770	flaky-test
13218455	Parquet writer sometimes hits DCHECK when handling empty string	"Encountered while doing a large insert into Parquet.

{code}
create table customer like tpcds_300_text.customer stored as parquetfile
insert overwrite table customer select * from tpcds_300_text.customer
{code}


{noformat}
F0227 01:34:53.052708 131295 parquet-column-stats.inline.h:213] 794c051ae3f3913c:71f00bc400000001] Check failed: static_cast<void*>(prev_page_min_value_.ptr) != static_cast<void*>(cs->min_value_.ptr) (0 vs. 0) 
*** Check failure stack trace: ***
    @          0x47ec7ec  google::LogMessage::Fail()
    @          0x47ee091  google::LogMessage::SendToLog()
    @          0x47ec1c6  google::LogMessage::Flush()
    @          0x47ef78d  google::LogMessageFatal::~LogMessageFatal()
    @          0x27e973c  impala::ColumnStats<>::Merge()
    @          0x27e3c74  impala::HdfsParquetTableWriter::BaseColumnWriter::FinalizeCurrentPage()
    @          0x27ee65f  impala::HdfsParquetTableWriter::BaseColumnWriter::AppendRow()
    @          0x27e653b  impala::HdfsParquetTableWriter::AppendRows()
    @          0x23177fc  impala::HdfsTableSink::WriteRowsToPartition()
    @          0x231aeeb  impala::HdfsTableSink::Send()
    @          0x1f53888  impala::FragmentInstanceState::ExecInternal()
    @          0x1f4fefa  impala::FragmentInstanceState::Exec()
    @          0x1f63333  impala::QueryState::ExecFInstance()
    @          0x1f61615  _ZZN6impala10QueryState15StartFInstancesEvENKUlvE_clEv
    @          0x1f64774  _ZN5boost6detail8function26void_function_obj_invoker0IZN6impala10QueryState15StartFInstancesEvEUlvE_vE6invokeERNS1_15function_bufferE
    @          0x1d76b9f  boost::function0<>::operator()()
    @          0x22245ee  impala::Thread::SuperviseThread()
    @          0x222c972  boost::_bi::list5<>::operator()<>()
    @          0x222c896  boost::_bi::bind_t<>::operator()()
    @          0x222c859  boost::detail::thread_data<>::run()
    @          0x3716329  thread_proxy
    @     0x7fba207e8dd4  start_thread
    @     0x7fba20511eac  __clone
{noformat}

This actually happened on multiple machines at almost exactly the same time:
{noformat}
Running on machine: vc1328.halxg.cloudera.com
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
F0227 01:34:53.025667 133025 parquet-column-stats.inline.h:213] 794c051ae3f3913c:71f00bc400000005] Check failed: static_cast<void*>(prev_page_min_value_.ptr) != static_cast<void*>(cs->min_value_.ptr) (0 vs. 0) 
...
F0227 01:34:53.025352 131082 parquet-column-stats.inline.h:213] 794c051ae3f3913c:71f00bc400000007] Check failed: static_cast<void*>(prev_page_min_value_.ptr) != static_cast<void*>(cs->min_value_.ptr) (0 vs. 0) 
{noformat}

Coordinator log indicates it failed very fast:
{noformat}
I0227 01:34:48.157472 147928 impala-server.cc:1063] 794c051ae3f3913c:71f00bc400000000] Registered query query_id=794c051ae3f3913c:71f00bc400000000 session_id=3345ef7013ba6bb2:55d8d105d3690a8e
I0227 01:34:48.157711 147928 Frontend.java:1251] 794c051ae3f3913c:71f00bc400000000] Analyzing query: insert overwrite table customer select * from tpcds_300_text.customer db: tpcds_300_decimal_parquet
I0227 01:34:48.158025 147928 FeSupport.java:285] 794c051ae3f3913c:71f00bc400000000] Requesting prioritized load of table(s): tpcds_300_decimal_parquet.customer
I0227 01:34:52.049566 147928 Frontend.java:1292] 794c051ae3f3913c:71f00bc400000000] Analysis finished.
I0227 01:34:52.067458 147991 admission-controller.cc:627] 794c051ae3f3913c:71f00bc400000000] Schedule for id=794c051ae3f3913c:71f00bc400000000 in pool_name=root.systest per_host_mem_estimate=1.62 GB PoolConfig: max_requests=-1 max_queued=200 max_mem=-1.00 B
I0227 01:34:52.067562 147991 admission-controller.cc:632] 794c051ae3f3913c:71f00bc400000000] Stats: agg_num_running=0, agg_num_queued=0, agg_mem_reserved=0,  local_host(local_mem_admitted=0, num_admitted_running=0, num_queued=0, backend_mem_reserved=0)
I0227 01:34:52.067620 147991 admission-controller.cc:664] 794c051ae3f3913c:71f00bc400000000] Admitted query id=794c051ae3f3913c:71f00bc400000000
I0227 01:34:52.067771 147991 coordinator.cc:93] 794c051ae3f3913c:71f00bc400000000] Exec() query_id=794c051ae3f3913c:71f00bc400000000 stmt=insert overwrite table customer select * from tpcds_300_text.customer
I0227 01:34:52.068926 147991 coordinator.cc:359] 794c051ae3f3913c:71f00bc400000000] starting execution on 9 backends for query_id=794c051ae3f3913c:71f00bc400000000
I0227 01:34:52.070919 47659 impala-internal-service.cc:50] 794c051ae3f3913c:71f00bc400000000] ExecQueryFInstances(): query_id=794c051ae3f3913c:71f00bc400000000 coord=vc1326.halxg.cloudera.com:22000 #instances=1
I0227 01:34:52.071800 147994 query-state.cc:624] 794c051ae3f3913c:71f00bc400000003] Executing instance. instance_id=794c051ae3f3913c:71f00bc400000003 fragment_idx=0 per_fragment_instance_idx=3 coord_state_idx=0 #in-flight=1
I0227 01:34:52.072952 147991 coordinator.cc:373] 794c051ae3f3913c:71f00bc400000000] started execution on 9 backends for query_id=794c051ae3f3913c:71f00bc400000000
I0227 01:34:52.074553 147992 coordinator.cc:611] Coordinator waiting for backends to finish, 9 remaining. query_id=794c051ae3f3913c:71f00bc400000000
F0227 01:34:52.949759 147994 parquet-column-stats.inline.h:213] 794c051ae3f3913c:71f00bc400000003] Check failed: static_cast<void*>(prev_page_min_value_.ptr) != static_cast<void*>(cs->min_value_.ptr) (0 vs. 0) 
{noformat}"	IMPALA	Resolved	1	1	2770	crash, parquet
13430668	Expose LOCK_RETRIES/LOCK_RETRY_WAIT_SECONDS settings for user	"Impala using private static final value for lock retry/wait.

https://github.com/apache/impala/blob/873fe2e5241c5714dfd94a186d524edc1cbad0ad/fe/src/compat-hive-3/java/org/apache/impala/compat/MetastoreShim.java#L142

{noformat}
// Number of retries to acquire an HMS ACID lock.
private static final int LOCK_RETRIES = 10;

// Time interval between retries of acquiring an HMS ACID lock
private static final int LOCK_RETRY_WAIT_SECONDS = 3;
{noformat}

In some cases it would make sense to make these values tunable.

We invoke acquireLock() in these places:
https://github.infra.cloudera.com/CDH/Impala/blob/3ceabc01822f53bb6f174dc29aac56ac3993d389/fe/src/main/java/org/apache/impala/service/Frontend.java#L2169

https://github.infra.cloudera.com/CDH/Impala/blob/7ae6fff71c860f7623fc0bfd3092073128c4959c/fe/src/main/java/org/apache/impala/catalog/Catalog.java#L745
invoked by
https://github.infra.cloudera.com/CDH/Impala/blob/ac62b98ab1d3475d8ece865c67fe90d463c340b5/fe/src/main/java/org/apache/impala/service/CatalogOpExecutor.java#L1936

In Frontend.java it's easy to access the query options. To have the tunables at the CatalogD -side we probably need to extend TDropTableOrViewParams  or TDdlExecRequest."	IMPALA	Resolved	3	1	2770	ramp-up
13308213	TestAcidRowValidation.test_row_validation fails in S3 test	"TestAcidRowValidation.test_row_validation is consistently failing in S3 tests:
{code}
query_test/test_acid_row_validation.py:67: in test_row_validation
    self._create_test_table(vector, unique_database, tbl_name)
query_test/test_acid_row_validation.py:52: in _create_test_table
    self.hdfs_client.make_dir(delta_dir)
E   AttributeError: 'TestAcidRowValidation' object has no attribute 'hdfs_client'
{code}"	IMPALA	Resolved	1	1	2770	broken-build
13350855	Impala crashes when it tries to write invalid timestamp value with INT64 Parquet timestamp type	"*Reproduction*
{noformat}
set parquet_timestamp_type=INT64_MICROS;
create table ts_test (ts timestamp) stored as parquet;
insert into ts_test values ('21-01-07 10:53:47.224885000');
{noformat}
*Stack trace*
{noformat}
#0 0x00007ff56e752438 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1 0x00007ff56e75403a in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2 0x00007ff571486789 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#3 0x00007ff57164b2c0 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#4 0x00007ff57149062f in JVM_handle_linux_signal () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#5 0x00007ff571483488 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#6 <signal handler called>
#7 0x0000000001daaf36 in boost::date_time::date<boost::gregorian::date, boost::gregorian::gregorian_calendar, boost::gregorian::date_duration>::is_not_a_date (this=0x8)
 at /opt/Impala-Toolchain/toolchain-packages-gcc7.5.0/boost-1.61.0-p2/include/boost/date_time/date.hpp:117
#8 0x0000000001daaec0 in boost::date_time::date<boost::gregorian::date, boost::gregorian::gregorian_calendar, boost::gregorian::date_duration>::is_special (this=0x8)
 at /opt/Impala-Toolchain/toolchain-packages-gcc7.5.0/boost-1.61.0-p2/include/boost/date_time/date.hpp:112
#9 0x0000000001daa36a in impala::TimestampValue::HasDate (this=0x0) at /home/boroknagyz/Impala/be/src/runtime/timestamp-value.h:182
#10 0x0000000001daa3a6 in impala::TimestampValue::HasDateAndTime (this=0x0) at /home/boroknagyz/Impala/be/src/runtime/timestamp-value.h:184
#11 0x0000000002bbf180 in impala::TimestampValue::FloorUtcToUnixTimeMicros (this=0x0, unix_time_micros=0x11b711c0) at /home/boroknagyz/Impala/be/src/runtime/timestamp-value.inline.h:137
#12 0x0000000002eae9d3 in impala::HdfsParquetTableWriter::Int64MicroTimestampColumnWriter::ConvertTimestamp (this=0x11b70f80, ts=..., result=0x11b711c0)
 at /home/boroknagyz/Impala/be/src/exec/parquet/hdfs-parquet-table-writer.cc:642
#13 0x0000000002eae870 in impala::HdfsParquetTableWriter::Int64TimestampColumnWriterBase::ConvertValue (this=0x11b70f80, value=0x0)
 at /home/boroknagyz/Impala/be/src/exec/parquet/hdfs-parquet-table-writer.cc:601
#14 0x0000000002eaeacd in impala::HdfsParquetTableWriter::BaseColumnWriter::AppendRow (this=0x11b70f80, row=0xcdc4000) at /home/boroknagyz/Impala/be/src/exec/parquet/hdfs-parquet-table-writer.cc:664
#15 0x0000000002ea6d00 in impala::HdfsParquetTableWriter::AppendRows (this=0xe1f2e00, batch=0x10518700, row_group_indices=..., new_file=0x7ff4bf5a301f)
...{noformat}
At frame #14 the timestamp value in the row is NULL, so ConvertValue() is invoked with 'value' being NULL which is not handled well.

"	IMPALA	Resolved	1	1	2770	crash, newbie, ramp-up
13052853	"Impala shell automatic reconnect does not appear to maintain ""use <db>"""	"After automatically reconnecting to the impalad, the shell doesn't reissue the ""use <db>"" statement like it does when you manually ""connect""."	IMPALA	Closed	4	1	2770	ramp-up
13580674	Use RoaringBitmap in IcebergDeleteNode	"IcebergDeleteNode currently uses an ordered int64_t array for each data file to hold the deleted positions. This can consume significant amount of memory when there are lots of deleted records.
E.g. 100 Million delete records consume 800 MiB memory.

RoaringBitmap is a highly compressed and highly efficient data structure to store bitmaps:
[https://arxiv.org/pdf/1603.06549]
[https://github.com/RoaringBitmap/CRoaring]

We could use it to store the deleted file positions instead of the sorted arrays, as
 * it consumes significantly less memory
 * makes the code simpler
 * *_might_* have perf benefits"	IMPALA	Resolved	3	4	2770	impala-iceberg
13258500	Grouping aggregator can cause segmentation fault when doing multiple aggregations.	"The following query deterministically crashes Impala:
{noformat}
select group_concat(distinct cast(l_orderkey as string))), group_concat(distinct(l_comment))) from tpch_parquet.lineitem group by l_comment{noformat}
The stack trace during the crash:
{noformat}
#0  0x00007fd3f605a428 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007fd3f605c02a in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00007fd3fa8ad149 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#3  0x00007fd3faa60d27 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#4  0x00007fd3fa8b6e4f in JVM_handle_linux_signal () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#5  0x00007fd3fa8a9e48 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#6  <signal handler called>
#7  0x00007fd400105bfa in impala::Tuple::IsNull (this=0x0, offset=...) at /home/boroknagyz/Impala/be/src/runtime/tuple.h:247
#8  0x00007fd3fd1c16d2 in impala::AggFnEvaluator::SerializeOrFinalize (this=0x75da4c0, src=0x0, dst_slot_desc=..., dst=0x0, fn=0x7fd3fd1cdc08 <impala::AggregateFunctions::StringValSerializeOrFinalize(impala_udf::FunctionContext*, impala_udf::StringVal const&)>)
    at /home/boroknagyz/Impala/be/src/exprs/agg-fn-evaluator.cc:398
#9  0x00007fd3fddbeb3a in impala::AggFnEvaluator::Serialize (this=0x75da4c0, tuple=0x0) at /home/boroknagyz/Impala/be/src/exprs/agg-fn-evaluator.h:267
#10 0x00007fd3fddbebe3 in impala::AggFnEvaluator::Serialize (evals=..., dst=0x0) at /home/boroknagyz/Impala/be/src/exprs/agg-fn-evaluator.h:296
#11 0x00007fd3fde4e847 in impala::GroupingAggregator::AddBatchStreamingImpl (this=0xd392300, agg_idx=1, needs_serialize=true, prefetch_mode=impala::TPrefetchMode::HT_BUCKET, in_batch=0xb7746c0, out_batch=0x758e040, ht_ctx=0xb336f80, remaining_capacity=0x7fd353bb6c40)
    at /home/boroknagyz/Impala/be/src/exec/grouping-aggregator-ir.cc:204
#12 0x00007fd3fde3e426 in impala::GroupingAggregator::AddBatchStreaming (this=0xd392300, state=0x7006e00, out_batch=0x758e040, child_batch=0xb7746c0, eos=0x7fd353bb6f0e) at /home/boroknagyz/Impala/be/src/exec/grouping-aggregator.cc:459
#13 0x00007fd3fdfa2773 in impala::StreamingAggregationNode::GetRowsStreaming (this=0x6205c00, state=0x7006e00, out_batch=0x758e040) at /home/boroknagyz/Impala/be/src/exec/streaming-aggregation-node.cc:160
#14 0x00007fd3fdfa1616 in impala::StreamingAggregationNode::GetNext (this=0x6205c00, state=0x7006e00, row_batch=0x758e040, eos=0x7fd353bb74c7) at /home/boroknagyz/Impala/be/src/exec/streaming-aggregation-node.cc:76
#15 0x00007fd3fe9ea566 in impala::FragmentInstanceState::ExecInternal (this=0x703b1e0) at /home/boroknagyz/Impala/be/src/runtime/fragment-instance-state.cc:368
#16 0x00007fd3fe9e6e39 in impala::FragmentInstanceState::Exec (this=0x703b1e0) at /home/boroknagyz/Impala/be/src/runtime/fragment-instance-state.cc:93
#17 0x00007fd3fea51c9d in impala::QueryState::ExecFInstance (this=0x6f9b200, fis=0x703b1e0) at /home/boroknagyz/Impala/be/src/runtime/query-state.cc:650
#18 0x00007fd3fea4ff6a in impala::QueryState::<lambda()>::operator()(void) const (__closure=0x7fd353bb7ca8) at /home/boroknagyz/Impala/be/src/runtime/query-state.cc:558
#19 0x00007fd3fea5397f in boost::detail::function::void_function_obj_invoker0<impala::QueryState::StartFInstances()::<lambda()>, void>::invoke(boost::detail::function::function_buffer &) (function_obj_ptr=...)
    at /opt/Impala-Toolchain/boost-1.57.0-p3/include/boost/function/function_template.hpp:153
#20 0x00007fd4005266f4 in boost::function0<void>::operator() (this=0x7fd353bb7ca0) at /opt/Impala-Toolchain/boost-1.57.0-p3/include/boost/function/function_template.hpp:767
#21 0x00007fd40020a44f in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::ThreadDebugInfo const*, impala::Promise<long, (impala::PromiseMode)0>*) (name=..., category=..., functor=..., 
    parent_thread_info=0x7fd354bbb950, thread_started=0x7fd354bba8f0) at /home/boroknagyz/Impala/be/src/util/thread.cc:360
#22 0x00007fd400213e13 in boost::_bi::list5<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::ThreadDebugInfo*>, boost::_bi::value<impala::Promise<long, (impala::PromiseMode)0>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::ThreadDebugInfo const*, impala::Promise<long, (impala::PromiseMode)0>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::ThreadDebugInfo const*, impala::Promise<long, (impala::PromiseMode)0>*), boost::_bi::list0&, int) (this=0x739f1c0, 
    f=@0x739f1b8: 0x7fd40020a0e8 <impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::ThreadDebugInfo const*, impala::Promise<long, (impala::PromiseMode)0>*)>, a=...)
    at /opt/Impala-Toolchain/boost-1.57.0-p3/include/boost/bind/bind.hpp:525
#23 0x00007fd400213d37 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::ThreadDebugInfo const*, impala::Promise<long, (impala::PromiseMode)0>*), boost::_bi::list5<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::ThreadDebugInfo*>, boost::_bi::value<impala::Promise<long, (impala::PromiseMode)0>*> > >::operator()() (this=0x739f1b8)
    at /opt/Impala-Toolchain/boost-1.57.0-p3/include/boost/bind/bind_template.hpp:20
#24 0x00007fd400213cfa in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::ThreadDebugInfo const*, impala::Promise<long, (impala::PromiseMode)0>*), boost::_bi::list5<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::ThreadDebugInfo*>, boost::_bi::value<impala::Promise<long, (impala::PromiseMode)0>*> > > >::run() (this=0x739f000)
    at /opt/Impala-Toolchain/boost-1.57.0-p3/include/boost/thread/detail/thread.hpp:116
#25 0x0000000000a3bcaa in thread_proxy ()
#26 0x00007fd3fb60a6ba in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0
#27 0x00007fd3f612c41d in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}
The problem is inbe/src/exec/grouping-aggregator-ir.cc:204:
{noformat}
AggFnEvaluator::Serialize(agg_fn_evals_, out_batch_iter.Get()->GetTuple(0));
{noformat}
GetTuple() should be called with 'agg_idx' instead of zero."	IMPALA	Resolved	3	1	2770	crash
13354459	CREATE Iceberg tables with old PARTITIONED BY syntax	"It's convenient for users to create Iceberg tables with the old syntax.

It's also easier to migrate existing workloads to Iceberg because the SQL scripts that create the table definitions don't need to change that much.

So users should be able to write the following:
{noformat}
CREATE TABLE ice_t (i int)
PARTITIONED BY (p int)
STORED AS ICEBERG;
{noformat}
Which should be equivalent to this:
{noformat}
CREATE TABLE ice_t (i int, p int)
PARTITION BY SPEC (p IDENTITY)
STORED AS ICEBERG;
{noformat}
Please note that the old-style CREATE TABLE creates IDENTITY-partitioned tables. For other partition transforms the users must use the new, more generic syntax.

Hive also supports the PARTITIONED BY syntax, see [https://github.com/apache/iceberg/pull/1612]"	IMPALA	Resolved	3	6	2770	impala-iceberg
13449885	test_full_acid_original_files() seems to be flaky	"We observed at https://jenkins.impala.io/job/ubuntu-16.04-from-scratch/16707/testReport/junit/query_test.test_acid/TestAcid/test_full_acid_original_files_protocol__beeswax___exec_option____test_replan___1___batch_size___0___num_nodes___0___disable_codegen_rows_threshold___5000___disable_codegen___False___abort_on_error___1___exec_single_node_rows_threshold___0____table_format__text_none_/ that the E2E test of [test_full_acid_original_files()|https://github.com/apache/impala/blob/master/tests/query_test/test_acid.py#L157] seems to be flaky.
{code}
Error Message
query_test/test_acid.py:171: in test_full_acid_original_files     self.run_test_case('QueryTest/full-acid-original-file', vector, unique_database) common/impala_test_suite.py:687: in run_test_case     result = exec_fn(query, user=test_section.get('USER', '').strip() or None) common/impala_test_suite.py:625: in __exec_in_impala     result = self.__execute_query(target_impalad_client, query, user=user) common/impala_test_suite.py:961: in __execute_query     return impalad_client.execute(query, user=user) common/impala_connection.py:212: in execute     return self.__beeswax_client.execute(sql_stmt, user=user) beeswax/impala_beeswax.py:189: in execute     handle = self.__execute_query(query_string.strip(), user=user) beeswax/impala_beeswax.py:367: in __execute_query     self.wait_for_finished(handle) beeswax/impala_beeswax.py:388: in wait_for_finished     raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None) E   ImpalaBeeswaxException: ImpalaBeeswaxException: E    Query aborted:Found original file with unexpected name: hdfs://localhost:20500/test-warehouse/alltypes_promoted_orc_def/year=2010/month=8/000000_1 Please run a major compaction on the partition/table to overcome this.
Stacktrace
query_test/test_acid.py:171: in test_full_acid_original_files
    self.run_test_case('QueryTest/full-acid-original-file', vector, unique_database)
common/impala_test_suite.py:687: in run_test_case
    result = exec_fn(query, user=test_section.get('USER', '').strip() or None)
common/impala_test_suite.py:625: in __exec_in_impala
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:961: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:212: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:189: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:367: in __execute_query
    self.wait_for_finished(handle)
beeswax/impala_beeswax.py:388: in wait_for_finished
    raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:Found original file with unexpected name: hdfs://localhost:20500/test-warehouse/alltypes_promoted_orc_def/year=2010/month=8/000000_1 Please run a major compaction on the partition/table to overcome this.
{code}"	IMPALA	Resolved	3	1	2770	broken-build
13547552	Implement Small String Optimization for StringValue	"Implement Small String Optimization for StringValue.

Current memory layout of StringValue is:
{noformat}
  char* ptr;  // 8 byte
  int len;    // 4 byte
{noformat}
For small strings with size up to 8 we could store the string contents in the bytes of the 'ptr'. Something like that:
{noformat}
  union {
    char* ptr;
    char small_buf[sizeof(ptr)];
  };
  int len;
{noformat}
Many C++ string implementations use the {{Small String Optimization}} to speed up work with small strings. For example:
{code:java}
Microsoft STL, libstdc++, libc++, Boost, Folly.{code}"	IMPALA	Resolved	3	4	2770	Performance
13379821	Use PARTITION-level locking for static partition INSERTs for ACID tables	"Currently Impala always create TABLE-level locks for INSERTs for ACID tables:

[https://github.com/apache/impala/blob/ced7b7d221cda30c65504e18082bb0af6c3cb595/fe/src/main/java/org/apache/impala/service/Frontend.java#L2153]

For static partition INSERTs we could create PARTITION-level locks instead."	IMPALA	Resolved	3	4	2770	newbe, ramp-up
13100238	Inconsistent specification of result set and result set metadata	"Some statements that return a result set declare it in Frontend.java, but other do not. Still the queries seem to work. We should take a closer look at how/where the result set metadata is specified and returned to the user, and ideally, make the paths today consistent (or provide a good explanation for the inconsistency).

For example, compute stats does return a result set, but we never declare it as returning a result set.
You can easily test the behavior of GetResultMetadata() by modifying one of our tests under tests/hs2/

Take a look at Frontend.java and search for COMPUTE_STATS. You will see that we incorrectly declare the result set to be empty.

From {{fe/src/main/java/org/apache/impala/service/Frontend.java}} at {{f8e7c31}}:
{noformat}
 403     } else if (analysis.isComputeStatsStmt()) {
 404       ddl.op_type = TCatalogOpType.DDL;
 405       TDdlExecRequest req = new TDdlExecRequest();
 406       req.setDdl_type(TDdlType.COMPUTE_STATS);
 407       req.setCompute_stats_params(analysis.getComputeStatsStmt().toThrift());
 408       ddl.setDdl_params(req);
 409       metadata.setColumns(Collections.<TColumn>emptyList());
 410     }
{noformat}

L409 shows an empty list for the result set metadata.

However in {{fe/src/main/java/org/apache/impala/service/CatalogOpExecutor.java}} we construct a result set:

{noformat}
 472         case UPDATE_STATS:
 473           Preconditions.checkState(params.isSetUpdate_stats_params());
 474           Reference<Long> numUpdatedColumns = new Reference<>(0L);
 475           alterTableUpdateStats(tbl, params.getUpdate_stats_params(), response,
 476               numUpdatedPartitions, numUpdatedColumns);
 477           reloadTableSchema = true;
 478           resultColVal.setString_val(""Updated "" + numUpdatedPartitions.getRef() +
 479               "" partition(s) and "" + numUpdatedColumns.getRef() + "" column(s)."");
 480           setResultSet = true;
 481           break;
{noformat}

This has the potential to break HS2 clients that expect a result set when computing stats. But in practice, compute stats works and returns a result set, so the current code/flow is confusing."	IMPALA	Resolved	2	1	2770	incompatibility
13481907	Avoid calling planFiles() on Iceberg tables when there are no predicates	"Currently we always invoke Iceberg's planFiles() API for creating Iceberg scans.

When there are no predicates (and no time travel) on the table we could avoid that because we already cache everything we need (schema, partition information, file descriptors).

We can also consider only pushing down predicates if at least one of the predicates refer to a partition column. Otherwise it's possible that the overhead of reading, decoding, evaluating all the manifest files is too large.

I think the change should be fairly simple, we just need to take care:
 * -store delete files separately, so we can still do the V2 scans from cache- (will be implemented by IMPALA-11826)
 * During time-travel we also cache old file descriptors, so we need to separate them from the actual snapshot's file descriptors."	IMPALA	Resolved	3	4	2770	impala-iceberg
13326414	Upgrade Iceberg to a version that is compatible with Hive3	To support Iceberg on S3, we'll need to use HiveCatalog. The current Iceberg jars only compatible with Hive2, but Impala uses Hive3. So we'll need an Iceberg version that is compatible with Hive3 as well.	IMPALA	Resolved	3	7	2770	impala-iceberg
13291992	Milestone 2: Validate each row against the valid write id list	"Minor compactions can compact several delta directories into a single delta directory. The current directory filtering algorithm needs to be modified to handle minor compacted directories and prefer those over plain delta directories.

On top of that, in minor compacted directories we need to filter out rows we cannot see. E.g. we can have the following delta directory:
{noformat}
full_acid/delta_0000001_0000010_0000/0000 # minWriteId: 1
# maxWriteId: 10
{noformat}
So this delta dir contains rows with write ids between 1 and 10. But maybe we are only allowed to see write ids less than 5. Therefore we need to check the ACID write id column (named originalTransaction) for each row to decide whether this row is valid or not.

There are several ways to optimize this. E.g. based on the min/max write ids of the delta directory, and the validWriteIdList, we can decide whether we need to validate the rows at all. Or, when we reach the high watermark (that tells us the max valid write id) we can stop the scanner since rows are ordered based on record ID."	IMPALA	Resolved	3	7	2770	impala-acid
13051136	add time-series for HDFS and HBase IO	"We'd like time series for the HDFS and HBase ""BytesRead"" IO counters."	IMPALA	Resolved	4	4	7280	Counters
13054489	Casting literals to TIMESTAMP throw when pushed to KuduScanNode	"{code}
select cast(timestamp_col as timestamp) from alltypesagg
where timestamp_col < cast('2010-01-01 00:05:20' as timestamp) and timestamp_col >= cast('2010-01-01 00:01:00' as timestamp)
{code}

Fails analysis with this error:
{code}
I0718 10:55:28.316362 10952 Frontend.java:875] analyze query select cast(timestamp_col as timestamp) from alltypesagg
where timestamp_col < cast('2010-01-01 00:05:20' as timestamp) and timestamp_col >= cast('2010-01-01 00:01:00' as timestamp)
I0718 10:55:28.316898 10952 Frontend.java:953] create plan
I0718 10:55:28.317456 10952 jni-util.cc:166] com.cloudera.impala.common.InternalException: Error while extracting Kudu conjuncts.
	at com.cloudera.impala.planner.KuduScanNode.init(KuduScanNode.java:98)
	at com.cloudera.impala.planner.SingleNodePlanner.createScanNode(SingleNodePlanner.java:1322)
	at com.cloudera.impala.planner.SingleNodePlanner.createTableRefNode(SingleNodePlanner.java:1549)
	at com.cloudera.impala.planner.SingleNodePlanner.createTableRefsPlan(SingleNodePlanner.java:807)
	at com.cloudera.impala.planner.SingleNodePlanner.createSelectPlan(SingleNodePlanner.java:646)
	at com.cloudera.impala.planner.SingleNodePlanner.createQueryPlan(SingleNodePlanner.java:235)
	at com.cloudera.impala.planner.SingleNodePlanner.createSingleNodePlan(SingleNodePlanner.java:141)
	at com.cloudera.impala.planner.Planner.createPlan(Planner.java:60)
	at com.cloudera.impala.service.Frontend.createExecRequest(Frontend.java:972)
	at com.cloudera.impala.service.JniFrontend.createExecRequest(JniFrontend.java:147)
Caused by: com.cloudera.impala.common.AnalysisException: DATE/DATETIME/TIMESTAMP literals not supported: CAST('2010-01-01 00:05:20' AS TIMESTAMP)
	at com.cloudera.impala.analysis.LiteralExpr.create(LiteralExpr.java:216)
	at com.cloudera.impala.analysis.Expr.foldConstantChildren(Expr.java:1207)
	at com.cloudera.impala.analysis.BinaryPredicate.normalizeSlotRefComparison(BinaryPredicate.java:130)
	at com.cloudera.impala.planner.KuduScanNode.extractKuduConjuncts(KuduScanNode.java:230)
	at com.cloudera.impala.planner.KuduScanNode.init(KuduScanNode.java:90)
	... 9 more
{code}

We should have filtered such predicates out of the set of predicates that we would try to push to Kudu."	IMPALA	Resolved	4	1	7280	kudu
13055096	Update Kudu toolchain build and bump client version	"Impala is currently building/testing against a relatively old version of Kudu: 1.0. There have been a number of important fixes since, and we need to find any new issues sooner rather than later.

I'll update the Kudu bits now to the latest Kudu master and then keep this JIRA open to track updating the Kudu bits closer to the release."	IMPALA	Resolved	2	3	7280	kudu, toolchain
13053963	Kudu scanner : Expensive per row per column IsNull check	"Kudu should annotate each column in the batch if it is nullable, as today per row per column from a kudu batch the scanner checks if the slot is null, it would be much more efficient to store a per column bit in the KuduScanBatch indicating nullability of a column. 

{code}
Status KuduScanner::KuduRowToImpalaTuple(const KuduScanBatch::RowPtr& row,
    RowBatch* row_batch, Tuple* tuple) {
  for (int i = 0; i < scan_node_->tuple_desc_->slots().size(); ++i) {
    const SlotDescriptor* info = scan_node_->tuple_desc_->slots()[i];
    void* slot = tuple->GetSlot(info->tuple_offset());

    if (row.IsNull(i)) {
      SetSlotToNull(tuple, *info);
      continue;
    }

    int max_len = -1;
    switch (info->type().type) {
      case TYPE_VARCHAR:
        max_len = info->type().len;
        DCHECK_GT(max_len, 0);
{code}

For a basic scan null check consumes 4% of the CPU cycles."	IMPALA	Resolved	3	1	7280	kudu, performance
13052411	Impala query hung after datanode transceivers hit limit	"When running multiple concurrent impala queries, if there are lots of remote read, Datanode might hit Transceivers limit, then impala queries could hung.

Here is the step to reproduce (on 10-node-cdh5 impala test cluster)
1. lower HDFS transceiver limit  to 48 (dfs.datanode.max.xcievers, dfs.datanode.max.transfer.threads)
2. disable short circuit read on both HDFS and Impala
3. add ""-abort_on_config_error=0"" to impalad safety valve
4. restart cluster
5. use several impala shell to run a large query repeatedly. I used a small app (from Skye) to do this.
repeat ./bin/impala-shell.sh -i e1118.halxg.cloudera.com -q ""use tpcds500gb; select count(ss_sold_date_sk) from store_sales;""
this ""select count"" query normally take about 80 seconds.

We see DN complains Xceiver exceeds limit
{code}
2014-10-21 20:45:06,844 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: e1220.halxg.cloudera.com:20002:DataXceiverServer: 
java.io.IOException: Xceiver count 49 exceeds the limit of concurrent xcievers: 48
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:137)
	at java.lang.Thread.run(Thread.java:744)
2014-10-21 20:45:06,912 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: e1220.halxg.cloudera.com:20002:DataXceiverServer: 
java.io.IOException: Xceiver count 49 exceeds the limit of concurrent xcievers: 48
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:137)
	at java.lang.Thread.run(Thread.java:744
{code}

Then Impalad throws connection issue
{code}
W1021 20:46:41.914271 14239 BlockReaderFactory.java:684] I/O error constructing remote block reader.
Java exception follows:
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:1986)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:409)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:843)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:144)
I1021 20:46:59.723688 14239 DFSInputStream.java:584] Successfully connected to /10.20.126.119:20002 for BP-1589291573-10.20.126.118-1400538633334:blk_1074148026_407202
W1021 20:46:59.739403 14239 BlockReaderFactory.java:684] I/O error constructing remote block reader.
Java exception follows:
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:1988)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:409)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786)
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665)
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:789)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:843)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:144)
{code}

The queries are hung. I'll attach back trace for Impalad.
After about 5 minutes, the coordinator seems lost track for those queries
{code}
I1021 20:47:02.828976 15496 impala-server.cc:1075] ReportExecStatus(): Received report for unknown query ID (probably closed or cancelled). (query_id: 924ffff021a39fd3:e5e653b8ac00b789, backend: 1, instance: 924ffff021a39fd3:e5e653b8ac00b78c done: false)
I1021 20:47:02.946990 55458 impala-server.cc:1075] ReportExecStatus(): Received report for unknown query ID (probably closed or cancelled). (query_id: 540a87b1cf86303:573508414076a2b1, backend: 0, instance: 540a87b1cf86303:573508414076a2b3 done: false)
I1021 20:47:02.948048 55459 impala-server.cc:1075] ReportExecStatus(): Received report for unknown query ID (probably closed or cancelled). (query_id: 684caeaae79f57fd:418b1bd87761a582, backend: 1, instance: 684caeaae79f57fd:418b1bd87761a585 done: false)
{code}

Btw, the # open files by impala doesn't increase after the query hung. the CPU, disk I/O back to almost idle.

gdb.e1118.2014.10.21T20.49.53_backtrace.txt is captured after queries are hung
gdb.e1118.2014.10.21T21.06.50_backtrace.txt is captured after ""unknown query ID""
gdb.e1118.2014.10.21T21.15.14_backtrace.txt is captured after cancelled all queries."	IMPALA	Resolved	3	1	7280	resource-management
13054906	EE tests fail to run when KUDU_IS_SUPPORTED=false	"On a system where Kudu is not supported and thus KUDU_IS_SUPPORTED=false, running tests fail to collect the test dimensions:

{code}
________________________________________ ERROR collecting query_test/test_scanners.py _________________________________________
query_test/test_scanners.py:494: in add_test_dimensions
   super(TestTextSplitDelimiters, cls).add_test_dimensions()
common/impala_test_suite.py:97: in add_test_dimensions
   cls.create_table_info_dimension(cls.exploration_strategy()))
common/impala_test_suite.py:567: in create_table_info_dimension
   tf_dimensions = load_table_info_dimension(cls.get_workload(), exploration_strategy)
common/test_dimensions.py:193: in load_table_info_dimension
   vector_values.append(TableFormatInfo(**vals))
common/test_dimensions.py:42: in __init__
   self.__validate()
common/test_dimensions.py:46: in __validate
   raise ValueError, 'Unknown file format: %s' % self.file_format
E   ValueError: Unknown file format: kudu
{code}"	IMPALA	Resolved	3	1	7280	kudu
13052993	Fix issues with the legacy join and agg nodes using --enable_partitioned_hash_join=false and --enable_partitioned_aggregation=false	"After properly configuring out Jenkins job to run with the legacy join and agg (IMPALA-2337), we see crashes/failures on that job.

Example run:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Builds%20-%202.3.0%20Release/job/impala-cdh5.5.x-non-partitioned-hash-and-aggs/13/

We must investigate and fix all issues because we decided to continue to support running Impala with the legacy join/agg.

I was able to repro a crash locally.
1. Start impala with:
{code}
./start-impala-cluster.py --impalad_args=""--enable_partitioned_aggregation=false --enable_partitioned_hash_join=false""
{code}
2. Run the end-to-end test suite:
{code}
./tests/run-tests.py
{code}

I got a core and dug in to see this query is crashing:
{code}
select a.int_col, count(b.int_col) int_sum from functional.alltypesagg a
join (select * from functional.alltypes
      where year=2009 and month=1 order by int_col limit 2500
      union all
      select * from functional.alltypes
      where year=2009 and month=2 limit 3000) b
on (a.int_col = b.int_col)
group by a.int_col
order by int_sum;
{code}

Here's the bt:
{code}
(gdb) bt
#0  0x00007f07a7156425 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f07a7159b8b in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00007f07a8c5fac5 in os::abort(bool) () from /home/abehm/jdk1.7.0_25/jre/lib/amd64/server/libjvm.so
#3  0x00007f07a8dbf137 in VMError::report_and_die() () from /home/abehm/jdk1.7.0_25/jre/lib/amd64/server/libjvm.so
#4  0x00007f07a8c635e0 in JVM_handle_linux_signal () from /home/abehm/jdk1.7.0_25/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  0x00000000012298bc in impala::TupleDescriptor::byte_size (this=0x0)
    at /home/abehm/impala/be/src/runtime/descriptors.h:337
#7  0x00000000016cea98 in impala::AggregationNode::Close (this=0x137138000, state=0x19e86d00)
    at /home/abehm/impala/be/src/exec/aggregation-node.cc:287
#8  0x00000000015cbe67 in impala::ExecNode::Close (this=0x19186c00, state=0x19e86d00)
    at /home/abehm/impala/be/src/exec/exec-node.cc:179
#9  0x00000000016bf9f6 in impala::SortNode::Close (this=0x19186c00, state=0x19e86d00)
    at /home/abehm/impala/be/src/exec/sort-node.cc:134
#10 0x0000000001595ce7 in impala::PlanFragmentExecutor::Close (this=0x19e86628)
    at /home/abehm/impala/be/src/runtime/plan-fragment-executor.cc:573
#11 0x000000000158fe87 in impala::PlanFragmentExecutor::~PlanFragmentExecutor (this=0x19e86628, 
    __in_chrg=<optimized out>) at /home/abehm/impala/be/src/runtime/plan-fragment-executor.cc:72
#12 0x000000000137e815 in impala::FragmentMgr::FragmentExecState::~FragmentExecState (this=0x19e86400, 
    __in_chrg=<optimized out>) at /home/abehm/impala/be/src/service/fragment-exec-state.h:42
#13 0x000000000137f82a in boost::checked_delete<impala::FragmentMgr::FragmentExecState> (x=0x19e86400)
    at /usr/include/boost/checked_delete.hpp:34
#14 0x0000000001381c2c in boost::detail::sp_counted_impl_p<impala::FragmentMgr::FragmentExecState>::dispose (
    this=0xb3414e0) at /usr/include/boost/smart_ptr/detail/sp_counted_impl.hpp:78
#15 0x0000000000f80904 in boost::detail::sp_counted_base::release (this=0xb3414e0)
    at /usr/include/boost/smart_ptr/detail/sp_counted_base_gcc_x86.hpp:145
#16 0x0000000000f8097d in boost::detail::shared_count::~shared_count (this=0x7f0727ac0fb8, __in_chrg=<optimized out>)
    at /usr/include/boost/smart_ptr/detail/shared_count.hpp:217
#17 0x00000000012fa062 in boost::shared_ptr<impala::FragmentMgr::FragmentExecState>::~shared_ptr (
    this=0x7f0727ac0fb0, __in_chrg=<optimized out>) at /usr/include/boost/smart_ptr/shared_ptr.hpp:168
#18 0x000000000137d9ee in impala::FragmentMgr::ExecPlanFragment (this=0xa327b60, exec_params=...)
    at /home/abehm/impala/be/src/service/fragment-mgr.cc:65
#19 0x00000000012f9f91 in impala::ImpalaInternalService::ExecPlanFragment (this=0x8f93b30, return_val=..., params=...)
    at /home/abehm/impala/be/src/service/impala-internal-service.h:37
#20 0x00000000014c4208 in impala::ImpalaInternalServiceProcessor::process_ExecPlanFragment (this=0xa327b00, seqid=0, 
    iprot=0x16f728c0, oprot=0x16f73580, callContext=0xffb5800)
    at /home/abehm/impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:949
#21 0x00000000014c3f3f in impala::ImpalaInternalServiceProcessor::dispatchCall (this=0xa327b00, iprot=0x16f728c0, 
    oprot=0x16f73580, fname=..., seqid=0, callContext=0xffb5800)
    at /home/abehm/impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:922
#22 0x00000000012f0a6c in apache::thrift::TDispatchProcessor::process (this=0xa327b00, in=..., out=..., 
    connectionContext=0xffb5800)
{code}

Note that running that above query by itself works fine."	IMPALA	Resolved	2	1	7280	correctness
13055532	Toolchain broken on centos6/ubuntu12 after Kudu added boost	"When Kudu added boost to their thirdparty directory, the toolchain builds on centos5/ubuntu12 started failing. I believe it to be a compiler configuration issue but need to investigate further.

{code}
17:35:56 g++: unrecognized option '-static-libstdc++'
17:35:56 cc1plus: error: unrecognized command line option ""-mno-avx2""
17:35:56 ...failed updating 1 target...
{code}"	IMPALA	Resolved	1	1	7280	kudu, toolchain
13054823	be test failure: KuduScanNodeTest: TestLimitsAreEnforced: row mismatch	"One failure so far.

{noformat}
    <testcase name=""TestLimitsAreEnforced"" status=""run"" time=""0.438"" classname=""KuduScanNodeTest"">
      <failure message=""Value of: row&#x0A;  Actual: &quot;[(0 0 hello_0)]&quot;&#x0A;Expected: rows[i]&#x0A;Which is: &quot;[(5 10 hello_5)]&quot;"" type=""""><![CDATA[/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/exec/kudu-scan-node-test.cc:152
Value of: row
  Actual: ""[(0 0 hello_0)]""
Expected: rows[i]
Which is: ""[(5 10 hello_5)]""]]></failure>
    </testcase>
{noformat}

http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-asf-master/job/impala-asf-master-exhaustive-release/"	IMPALA	Resolved	1	1	7280	broken-build, kudu
13066085	test_kudu_col_null_changed flaky	"Th following failed in what looks like a flaky test to me on the pre-commit run of an unrelated change:

http://jenkins.impala.io:8080/job/ubuntu-14.04-from-scratch/1188/consoleFull

{noformat}
04:26:12 FAIL query_test/test_kudu.py::TestKuduOperations::()::test_kudu_col_null_changed
04:26:12 =================================== FAILURES ===================================
04:26:12 ________________ TestKuduOperations.test_kudu_col_null_changed _________________
04:26:12 [gw7] linux2 -- Python 2.7.6 /home/ubuntu/Impala/bin/../infra/python/env/bin/python
04:26:12 query_test/test_kudu.py:209: in test_kudu_col_null_changed
04:26:12     session.flush()
04:26:12 kudu/client.pyx:1190: in kudu.client.Session.flush (kudu/client.cpp:14633)
04:26:12     ???
04:26:12 kudu/errors.pyx:62: in kudu.errors.check_status (kudu/errors.cpp:1073)
04:26:12     ???
04:26:12 E   KuduBadStatus: IO error: Some errors occurred
04:26:12 ---------------------------- Captured stderr setup -----------------------------
04:26:12 MainThread: Using database l28359 as default
04:26:12 SET sync_ddl=False;
04:26:12 -- executing against localhost:21000
04:26:12 DROP DATABASE IF EXISTS `test_kudu_col_null_changed_bc507455` CASCADE;
04:26:12 
04:26:12 SET sync_ddl=False;
04:26:12 -- executing against localhost:21000
04:26:12 CREATE DATABASE `test_kudu_col_null_changed_bc507455`;
04:26:12 
04:26:12 MainThread: Created database ""test_kudu_col_null_changed_bc507455"" for test ID ""query_test/test_kudu.py::TestKuduOperations::()::test_kudu_col_null_changed""
04:26:12 ----------------------------- Captured stderr call -----------------------------
04:26:12 W0422 11:08:21.155036 55261 batcher.cc:325] Timed out: Failed to write batch of 32 ops to tablet 2bb497696479460199dac69188fee4d3 after 1 attempt(s): Failed to write to server: f6a9a0b761cf46d0a40acd0a9208c257 (ip-172-31-7-130:31200): Write RPC to 127.0.0.1:31200 timed out after 5.000s (SENT)
04:26:12 W0422 11:08:21.705691 55261 outbound_call.cc:224] RPC callback for RPC call kudu.tserver.TabletServerService.Write -> {remote=127.0.0.1:31200, user_credentials={real_user=ubuntu}} blocked reactor thread for 550653us
{noformat}"	IMPALA	Resolved	3	1	7280	broken-build, flaky
13055058	Kudu DML should report the number of rows that could not be modified	"Kudu DML operations ignore schema-related conflicts/violations (IMPALA-3710) because Kudu does not support multi-row transactions.

While the operations will report the number of rows modified through beeswax and the profile (unfortunately HS2 cannot return this information yet, see IMPALA-3713), the operations should also report the total number of rows that could not be modified. There are several reasons why rows might not be successfully modified, and a more detailed mechanism for reporting these errors is desirable but out of scope for now (that work is tracked by IMPALA-4416)."	IMPALA	Resolved	2	4	7280	kudu, supportability
13055235	Specify more options when adding new kudu columns	"Currently, due to limitations in the Kudu API, Impala imposes a number of constraints on new columns:
1. nullable columns can't have default values (KUDU-1747)
2. no encoding/compression/block size can be specified (KUDU-1746)
Now that Kudu jiras have been resolved, we should remove these constraints on the Impala side."	IMPALA	Resolved	3	2	7280	kudu
13052861	SEGV in ScopedTimer during old agg node Open()	"In an environment with dynamic resource pools (llama) where Impala has both PHJ and PAGG disabled we see the following SEGV:

{code}
#0  0x00000032f7432925 in raise () from /lib64/libc.so.6
#1  0x00000032f7434105 in abort () from /lib64/libc.so.6
#2  0x00007f36126aca55 in os::abort(bool) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#3  0x00007f361282cf87 in VMError::report_and_die() () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#4  0x00007f361282d50e in crash_handler(int, siginfo*, void*) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#5  0x00007f36126abbf2 in os::Linux::chained_handler(int, siginfo*, void*) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#6  0x00007f36126b18d6 in JVM_handle_linux_signal () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#7  <signal handler called>
#8  0x00007f36125aa493 in Klass::oop_print_on(oopDesc*, outputStream*) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#9  0x00007f361213aa1b in arrayKlass::oop_print_on(oopDesc*, outputStream*) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#10 0x00007f36126a42ff in os::print_location(outputStream*, long, bool) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#11 0x00007f36126b1235 in os::print_register_info(outputStream*, void*) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#12 0x00007f361282b57c in VMError::report(outputStream*) () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#13 0x00007f361282cb8a in VMError::report_and_die() () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#14 0x00007f36126b196f in JVM_handle_linux_signal () from /usr/java/jdk1.7.0_67/jre/lib/amd64/server/libjvm.so
#15 <signal handler called>
#16 0x00007f354f8b6ad8 in ?? ()
#17 0x0000000000005575 in ?? ()
#18 0x00000000140bee70 in ?? ()
#19 0x00007f33208178a0 in ?? ()
#20 0x0000000000e398db in impala::ScopedTimer<impala::MonotonicStopWatch>::UpdateCounter() ()
#21 0x00000000014e1c13 in impala::AggregationNode::Open(impala::RuntimeState*) ()
#22 0x00000000013c5280 in impala::PlanFragmentExecutor::OpenInternal() ()
#23 0x00000000013c50fc in impala::PlanFragmentExecutor::Open() ()
#24 0x000000000104c568 in impala::FragmentMgr::FragmentExecState::Exec() ()
#25 0x00000000010439a6 in impala::FragmentMgr::FragmentExecThread(impala::FragmentMgr::FragmentExecState*) ()
#26 0x0000000001047f80 in boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>::operator()(impala::FragmentMgr*, impala::FragmentMgr::FragmentExecState*) const ()
#27 0x0000000001047d39 in void boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list0>(boost::_bi::type<void>, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>&, boost::_bi::list0&, int) ()
#28 0x0000000001047621 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >::operator()() ()
#29 0x0000000001046f6f in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >, void>::invoke(boost::detail::function::function_buffer&) ()
#30 0x0000000000ee0ea1 in boost::function0<void>::operator()() const ()
#31 0x000000000111ad76 in impala::Thread::SuperviseThread(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*) ()
#32 0x000000000112415a in void boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0&, int) ()
#33 0x00000000011240a1 in boost::_bi::bind_t<void, void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() ()
#34 0x0000000001124060 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() ()
#35 0x0000000001565833 in ?? ()
#36 0x00000032f78079d1 in start_thread () from /lib64/libpthread.so.0
#37 0x00000032f74e8b6d in clone () from /lib64/libc.so.6
{code}"	IMPALA	Resolved	2	1	7280	crash
13054492	Predicates are not always pushed to Kudu	"Predicates in some cases do not seem to be pushed to Kudu and are executed at the Impala scan node instead. Example from the TPC-H Q6:
{noformat}
# Q6 - Forecasting Revenue Change Query
select
 round(sum(l_extendedprice * l_discount), 2) as revenue
from
  tpch_kudu.lineitem
where
  l_shipdate >= '1994-01-01'
  and l_shipdate < '1995-01-01'
  and l_discount between 0.05 and 0.07
  and l_quantity < 24
---- PLAN
01:AGGREGATE [FINALIZE]
|  output: sum(l_extendedprice * l_discount)
|
00:SCAN KUDU [tpch_kudu.lineitem]
   *predicates: l_quantity < 24, l_shipdate < '1995-01-01'*
   kudu predicates: l_discount >= 0.05, l_discount <= 0.07, l_shipdate >= '1994-01-01'
{noformat}"	IMPALA	Resolved	1	1	7280	kudu, performance
13055097	Account for Kudu client memory in MemTracker	"The Kudu client may consume non-trivial amounts of memory and it is not accounted for in the query MemTracker, so it may be possible for the process to run out of memory.

In particular, we need to consider the following in the KuduTableSink:
# Buffer space for write ops to be sent, which is 100MB by default and is configurable via a flag.
# Per-row errors observed by the client (before they are fetched and deleted by Impala). Each error contains a string and a copy of the row. The client API indicates that the error handling could overflow, i.e. that it is bounded, but the implementation does not yet limit the errors so this could be unbounded.

We need to also understand whether there are any non-negligible memory allocations in the KuduScanNode."	IMPALA	Resolved	1	1	7280	kudu, memory, resource-management
13053807	toolchain cmake hangs on sles12	"Impala's toolchain (native-toolchain) doesn't build on sles12. It appears to be a cmake bug [1] though it hasn't been fixed yet. Until we resolve this we cannot update our toolchain bits.

The cmake bug description mentions a workaround by patching cmake to avoid calling POSIX {{select()}}, though it also warns this may have a performance impact.

We can try to produce a patched cmake build in the toolchain and use it for sles12 builds.

1: https://cmake.org/Bug/view.php?id=15873"	IMPALA	Resolved	1	1	7280	broken-build, toolchain
13055255	Add support for SLES12 for Impala/Kudu integration	"During remote cluster testing, we discovered that Impala integration is not supported on SLES12, even though SLES12 is on the list of Kudu's supported OS's.

{noformat}
(load-functional-query-exhaustive-impala-load-generated-kudu-none-none.sql):
INSERT into TABLE functional_kudu.alltypes
SELECT id, bool_col, tinyint_col, smallint_col, int_col, bigint_col, float_col, double_col, date_string_col, string_col,
       cast(timestamp_col as string), year, month
FROM functional.alltypes

Data Loading from Impala failed with error: ImpalaBeeswaxException:
 Query aborted:
Kudu is not supported on this operating system.
{noformat}

The problem is that we're not currently building a SLES12 client for Kudu. This is not a critical issue, but it should be added at some point."	IMPALA	Resolved	2	4	7280	kudu
13097382	Suggested MEM_LIMIT in rejected query error may be too low	"If you set a small memlimit, it may suggest that the memlimit should be at least 79.75 MB. Then, if you run it with 80MB, it'll suggest 92MB.

{code}
[philip-dev.gce.cloudera.com:21000] > set mem_limit=1024;
MEM_LIMIT set to 1024
[philip-dev.gce.cloudera.com:21000] > select count(*) from functional.alltypes;
Query: select count(*) from functional.alltypes
Query submitted at: 2017-08-24 08:36:37 (Coordinator: http://philip-dev.gce.cloudera.com:25000)
ERROR: Memory limit exceeded: Could not allocate aggregate expression intermediate value
Exprs could not allocate 16.00 B without exceeding limit.
Error occurred on backend philip-dev.gce.cloudera.com:22000 by fragment 704f7351ee03f7f1:5ed9277d00000003
Memory left in process limit: 31.34 GB
Memory left in query limit: 1.00 KB
Query(704f7351ee03f7f1:5ed9277d00000000): Limit=1.00 KB Reservation=0 ReservationLimit=0 OtherMemory=0 Total=0 Peak=0
  Fragment 704f7351ee03f7f1:5ed9277d00000000: Reservation=0 OtherMemory=0 Total=0 Peak=0
    AGGREGATION_NODE (id=3): Total=0 Peak=0
    EXCHANGE_NODE (id=2): Total=0 Peak=0
    DataStreamRecvr: Total=0 Peak=0
  Fragment 704f7351ee03f7f1:5ed9277d00000003: Reservation=0 OtherMemory=0 Total=0 Peak=0
    AGGREGATION_NODE (id=1): Total=0 Peak=0
    HDFS_SCAN_NODE (id=0): Total=0 Peak=0

[philip-dev.gce.cloudera.com:21000] >       select count(*)
      from tpch_parquet.lineitem join tpch_parquet.orders on l_orderkey = o_orderkey;
Query: select count(*)
      from tpch_parquet.lineitem join tpch_parquet.orders on l_orderkey = o_orderkey
Query submitted at: 2017-08-24 08:36:46 (Coordinator: http://philip-dev.gce.cloudera.com:25000)
ERROR: Rejected query from pool default-pool: minimum memory reservation is greater than memory available to the query for buffer reservations. Mem available for buffer reservations based on mem_limit: 1.00 KB, memory reservation needed: 4.75 MB. Set mem_limit to at least 79.75 MB. See the query profile for more information.

[philip-dev.gce.cloudera.com:21000] > set mem_limit=80mb;
MEM_LIMIT set to 80mb
[philip-dev.gce.cloudera.com:21000] >       select count(*)
      from tpch_parquet.lineitem join tpch_parquet.orders on l_orderkey = o_orderkey;
Query: select count(*)
      from tpch_parquet.lineitem join tpch_parquet.orders on l_orderkey = o_orderkey
Query submitted at: 2017-08-24 08:37:01 (Coordinator: http://philip-dev.gce.cloudera.com:25000)
ERROR: Rejected query from pool default-pool: minimum memory reservation is greater than memory available to the query for buffer reservations. Mem available for buffer reservations based on mem_limit: 80.00 MB, memory reservation needed: 17.00 MB. Set mem_limit to at least 92.00 MB. See the query profile for more information.
{code}

The join is switching from broadcast to partitioned based on the mem_limit, which then affects the per-node bytes estimate, which then affects the buffer size chosen by the planner, which then affects the reservation. This is the code
https://github.com/apache/incubator-impala/blob/c7db60aa46565c19634e8a791df3af8d116b9017/fe/src/main/java/org/apache/impala/planner/DistributedPlanner.java#L539

The good news is that it will converge after bumping the mem_limit to the recommended value the second time. This is still very wonky."	IMPALA	Resolved	3	1	7280	usability
13052805	Retry logic for Llama RPC may throw exception	"The code in resource-broker.cc that makes RPCs to Llama will attempt to retry the rpc some number of times (which is configurable) if the RPC returns a failure. If the RPC throws (which thrift may do), we try to reset the connection and then make the RPC again, but this time not guarded by a try/catch block. If this RPC throws, the process will crash.

We need to at least handle this thrift exception, and possibly simplify the logic as well."	IMPALA	Resolved	2	1	7280	llama, ramp-up, resource-management
13073198	Fix Kudu timestamp default values	"While support for TIMESTAMP columns in Kudu tables has been committed (IMPALA-5137), it does not support TIMESTAMP column default values. It turns out to be a bit tricky in the catalog.

In addition to lacking the ability to specify the default values in DDL (both CREATE and ALTER columns), this also means tables with timestamp default values created outside of Impala (e.g. via the Kudu python client) cannot be loaded by Impala:

{code}
import kudu
from pytz import utc
from datetime import datetime
from kudu.client import Partitioning

client = kudu.connect('localhost')
schema_builder = SchemaBuilder()

column_spec = schema_builder.add_column(""id"", INT64)
column_spec.nullable(False)

column_spec = schema_builder.add_column(""ts"", UNIXTIME_MICROS)
column_spec.default(datetime(1987, 5, 19, 0, 0, tzinfo=utc))

schema_builder.set_primary_keys([""id""])
schema = schema_builder.build()

client.create_table(""tsdefault"", schema,
    partitioning=Partitioning().set_range_partition_columns([""id""]))
{code}

then in Impala:
{code}
[localhost:21000] > create external table tsdefault stored as kudu TBLPROPERTIES (
  'kudu.table_name' = 'tsdefault' );
Query: create external table tsdefault stored as kudu TBLPROPERTIES (
  'kudu.table_name' = 'tsdefault' )

Fetched 0 row(s) in 0.22s
[localhost:21000] > show create table tsdefault;
Query: show create table tsdefault
ERROR: AnalysisException: Failed to load metadata for table: default.tsdefault. Running 'invalidate metadata default.tsdefault' may resolve this problem.
CAUSED BY: NullPointerException: null
CAUSED BY: TableLoadingException: Failed to load metadata for table: default.tsdefault. Running 'invalidate metadata default.tsdefault' may resolve this problem.
CAUSED BY: NullPointerException: null
{code}


This is tricky in the catalog because the {{KuduColumn}} class loads the column metadata from Kudu, and it contains the default value as a LiteralExpr, but Kudu represents the timestamp as a bigint unix time micros. Impala should convert that value to a TimestampValue, which isn't hard to do in the backend but isn't easy in the catalog. Unless the catalog were to call into BE code, the KuduColumn class would need to store the default value as a bigint and then all code that then uses the default value later would need to know that it isn't the same type as the column."	IMPALA	Resolved	2	1	7280	kudu
13052871	setting request_pool query option should not override query to pool rules	"Setting the query option request_pool should not override the query to queue rules specified in the fair-scheduler.xml config.

For example, if the fair-scheduler.xml  has only the rule ""user"" rule specified, the queue should always be ""root.<user>"", but if the ""request_pool"" query option is set, that will be used."	IMPALA	Resolved	3	1	7280	llama, ramp-up, resource-management
13052954	Query tests for non-partitioned aggs and joins build use partitioned aggs and joins	"I was digging into the cluster logs for IMPALA-2329. From impalad.INFO in http://sandbox.jenkins.cloudera.com/job/impala-master-cdh5-trunk-non-partitioned-hash-and-aggs/98/:

Log file created at: 2015/09/14 01:35:19
Running on machine: vd0212.halxg.cloudera.com
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0914 01:35:19.246165 27204 logging.cc:119] stdout will be logged to this file.
E0914 01:35:19.246368 27204 logging.cc:120] stderr will be logged to this file.
I0914 01:35:19.246759 27204 authentication.cc:996] Internal communication is not authenticated
I0914 01:35:19.246768 27204 authentication.cc:1017] External communication is not authenticated
I0914 01:35:19.246896 27204 init.cc:158] impalad version 2.3.0-cdh5-INTERNAL DEBUG (build ba75d79030100cd8a527939fecf4b82a5e6f94fb)
Built on Sun, 13 Sep 2015 23:15:34 PST
I0914 01:35:19.246901 27204 init.cc:159] Using hostname: vd0212.halxg.cloudera.com
I0914 01:35:19.247463 27204 logging.cc:155] Flags (see also /varz are on debug webserver):
<snip>
--enable_partitioned_aggregation=true
--enable_partitioned_hash_join=true

Not sure whether it is just the query tests and whether other builds are affected."	IMPALA	Resolved	1	1	7280	test-infra
13054442	TestHBaseQueries.test_hbase_inserts caused crash in impala::RuntimeProfile::total_time_counter	"Over the weekend, there was an Impala crash in old aggs/joins job. Subsequent jobs succeeded. Unfortunately, we can't get the core because the machine that had it has been deprovisioned. Because this happened on 2.6.0, we didn't have the infra in place to save the cores to S3 (that has since been fixed).

Failed run:

http://sandbox.jenkins.cloudera.com/job/impala-cdh5-2.6.0_5.8.0-core-non-partitioned-joins-and-aggs/47/

Here is the backtrace

{noformat}
#0  0x00000034476328e5 in raise () from /lib64/libc.so.6
#1  0x00000034476340c5 in abort () from /lib64/libc.so.6
#2  0x00007f062e805c55 in os::abort(bool) () from /opt/toolchain/sun-jdk-64bit-1.7.0.75/jre/lib/amd64/server/libjvm.so
#3  0x00007f062e987cd7 in VMError::report_and_die() () from /opt/toolchain/sun-jdk-64bit-1.7.0.75/jre/lib/amd64/server/libjvm.so
#4  0x00007f062e80ab6f in JVM_handle_linux_signal () from /opt/toolchain/sun-jdk-64bit-1.7.0.75/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  std::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const () at /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/gcc/build/x86_64-unknown-linux-gnu/libstdc++-v3/include/bits/basic_string.h:293
#7  0x0000000001085ef9 in std::operator< <char, std::char_traits<char>, std::allocator<char> > (__lhs=..., __rhs=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/basic_string.h:2590
#8  0x0000000001084723 in std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >::operator() (this=0x7e18408, __x=..., __y=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_function.h:371
#9  0x00000000012a516e in std::_Rb_tree<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*>, std::_Select1st<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> >, std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> > >::_M_lower_bound (this=0x7e18408, __x=0x3336323637, __y=0x7e18410, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_tree.h:1261
#10 0x00000000012a3cdf in std::_Rb_tree<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*>, std::_Select1st<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> >, std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> > >::lower_bound (this=0x7e18408, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_tree.h:927
#11 0x00000000012a2a75 in std::map<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, impala::RuntimeProfile::Counter*, std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> > >::lower_bound (this=0x7e18408, __x=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_map.h:902
#12 0x00000000012a1860 in std::map<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, impala::RuntimeProfile::Counter*, std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> > >::operator[] (this=0x7e18408, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_map.h:496
#13 0x000000000129fef8 in impala::RuntimeProfile::total_time_counter (this=0x7e183e0) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/runtime-profile.h:523
#14 0x0000000001872f23 in operator() (this=0x7f0561abbfd0, a=..., b=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/runtime/coordinator.cc:1669
#15 0x000000000188708c in __gnu_cxx::__ops::_Val_comp_iter<impala::InstanceComparator>::operator()<std::pair<impala::RuntimeProfile*, bool>, __gnu_cxx::__normal_iterator<std::pair<impala::RuntimeProfile*, bool>*, std::vector<std::pair<impala::RuntimeProfile*, bool> > > > (this=0x7f0561abbfd0, __val=..., __it=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/predefined_ops.h:166
#16 0x00000000018857bc in std::__unguarded_linear_insert<__gnu_cxx::__normal_iterator<std::pair<impala::RuntimeProfile*, bool>*, std::vector<std::pair<impala::RuntimeProfile*, bool> > >, __gnu_cxx::__ops::_Val_comp_iter<impala::InstanceComparator> > (__last=..., __comp=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_algo.h:1827
#17 0x0000000001882f3f in std::__insertion_sort<__gnu_cxx::__normal_iterator<std::pair<impala::RuntimeProfile*, bool>*, std::vector<std::pair<impala::RuntimeProfile*, bool> > >, __gnu_cxx::__ops::_Iter_comp_iter<impala::InstanceComparator> > (__first=..., __last=..., __comp=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_algo.h:1854
#18 0x000000000187ff9d in std::__final_insertion_sort<__gnu_cxx::__normal_iterator<std::pair<impala::RuntimeProfile*, bool>*, std::vector<std::pair<impala::RuntimeProfile*, bool> > >, __gnu_cxx::__ops::_Iter_comp_iter<impala::InstanceComparator> > (__first=..., __last=..., __comp=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_algo.h:1889
#19 0x000000000187c50f in std::__sort<__gnu_cxx::__normal_iterator<std::pair<impala::RuntimeProfile*, bool>*, std::vector<std::pair<impala::RuntimeProfile*, bool> > >, __gnu_cxx::__ops::_Iter_comp_iter<impala::InstanceComparator> > (__first=..., __last=..., __comp=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_algo.h:1970
#20 0x0000000001878f25 in std::sort<__gnu_cxx::__normal_iterator<std::pair<impala::RuntimeProfile*, bool>*, std::vector<std::pair<impala::RuntimeProfile*, bool> > >, impala::InstanceComparator> (__first=..., __last=..., __comp=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_algo.h:4716
#21 0x000000000187585b in impala::RuntimeProfile::SortChildren<impala::InstanceComparator>(const struct {...} &) (this=0xad63840, cmp=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/runtime-profile.h:445
#22 0x000000000186ecdb in impala::Coordinator::ReportQuerySummary (this=0x11060000) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/runtime/coordinator.cc:1768
#23 0x0000000001867ff7 in impala::Coordinator::Wait (this=0x11060000) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/runtime/coordinator.cc:1078
#24 0x0000000001416e31 in impala::ImpalaServer::QueryExecState::WaitInternal (this=0xfb1c000) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/service/query-exec-state.cc:618
#25 0x0000000001416b04 in impala::ImpalaServer::QueryExecState::Wait (this=0xfb1c000) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/service/query-exec-state.cc:594
#26 0x0000000001432be3 in boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>::operator() (this=0x7f0561abcca8, p=0xfb1c000) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/mem_fn_template.hpp:49
#27 0x00000000014326f2 in boost::_bi::list1<boost::_bi::value<impala::ImpalaServer::QueryExecState*> >::operator()<boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>, boost::_bi::list0> (this=0x7f0561abccb8, f=..., a=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:253
#28 0x00000000014323c3 in boost::_bi::bind_t<void, boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>, boost::_bi::list1<boost::_bi::value<impala::ImpalaServer::QueryExecState*> > >::operator() (this=0x7f0561abcca8) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#29 0x0000000001431b8a in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>, boost::_bi::list1<boost::_bi::value<impala::ImpalaServer::QueryExecState*> > >, void>::invoke (function_obj_ptr=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:153
#30 0x000000000126e60c in boost::function0<void>::operator() (this=0x7f0561abcca0) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:767
#31 0x00000000015085e5 in impala::Thread::SuperviseThread (name=..., category=..., functor=..., thread_started=0x7f059961ccf0) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/thread.cc:315
#32 0x000000000150f148 in boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::basic_string<char, std::char_traits<char>, std::allocator<char> > &, const std::basic_string<char, std::char_traits<char>, std::allocator<char> > &, boost::function<void()>, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x912c3c0, f=@0x912c3b8, a=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:457
#33 0x000000000150f08b in boost::_bi::bind_t<void, void (*)(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x912c3b8) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#34 0x000000000150f04e in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x912c200) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/thread/detail/thread.hpp:116
#35 0x000000000194a6ca in thread_proxy ()
#36 0x0000003447a07851 in start_thread () from /lib64/libpthread.so.0
#37 0x00000034476e894d in clone () from /lib64/libc.so.6
{noformat}

If I'm reading the console log, the failing test is 

{noformat}
query_test/test_hbase_queries.py::TestHBaseQueries::test_hbase_inserts[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: hbase/none]
{noformat}

Here is the full failure:

{noformat}
 TestHBaseQueries.test_hbase_inserts[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: hbase/none] 
query_test/test_hbase_queries.py:43: in test_hbase_inserts
    self.run_test_case('QueryTest/hbase-inserts', vector)
common/impala_test_suite.py:294: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:487: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:161: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:163: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:329: in __execute_query
    self.wait_for_completion(handle)
beeswax/impala_beeswax.py:341: in wait_for_completion
    query_state = self.get_state(query_handle)
beeswax/impala_beeswax.py:358: in get_state
    return self.__do_rpc(lambda: self.imp_service.get_state(query_handle))
beeswax/impala_beeswax.py:455: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(u), u)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'socket.error'>
E    MESSAGE: [Errno 104] Connection reset by peer
---------------------------- Captured stderr setup -----------------------------
-- connecting to: localhost:21000
----------------------------- Captured stderr call -----------------------------
-- executing against localhost:21000
use functional_hbase;

SET disable_codegen=False;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
SET batch_size=0;
SET num_nodes=0;
-- executing against localhost:21000
insert into table insertalltypesagg
select id, bigint_col, bool_col, date_string_col, day, double_col, float_col,
int_col, month, smallint_col, string_col, timestamp_col, tinyint_col, year from functional.alltypesagg;

-- executing against localhost:21000
select id, bool_col from insertalltypesagg
WHERE id > 300
ORDER BY id
LIMIT 2;

-- executing against localhost:21000
insert into table insertalltypesagg
select 9999999, bigint_col, false, date_string_col, day, double_col, float_col,
int_col, month, smallint_col, string_col, timestamp_col, tinyint_col, year from functional.alltypesagg;

-- executing against localhost:21000
select id, bool_col from insertalltypesagg
WHERE id = 9999999
ORDER BY id
LIMIT 2;

-- executing against localhost:21000
insert into table insertalltypesagg
select * from insertalltypesagg limit 1;

-- executing against localhost:21000
insert into table insertalltypesagg
select 9999999, bigint_col, false, ""\\N"", day, double_col, float_col,
int_col, month, smallint_col, ""\\N"", timestamp_col, tinyint_col, year from functional.alltypesagg limit 1;
{noformat}

A quick scan of the test code indicates ""executing against"" does happen before the actual query printed executes. Presumably then this last query listed above is the troublesome one."	IMPALA	Resolved	1	1	7280	broken-build, crash
13054389	Expose kudu client timeouts or set to more conservative values	"We observed some timeouts creating Kudu tables.

We should either set more conservative timeouts or expose a way to set the timeouts in both the FE and the BE. (In this case the issue occurred in the FE but we should evaluate our timeouts in both places.)"	IMPALA	Resolved	2	3	7280	kudu
13054408	Code coverage builds TestAdmissionControllerStress.test_admission_controller_with_flags and test_admission_controller_with_configs fail with timeouts	"See: http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-exhaustive-code-coverage/122/testReport/

Code coverage builds for custom_cluster.test_admission_controller.TestAdmissionControllerStress.test_admission_controller_with_flags and custom_cluster.test_admission_controller.TestAdmissionControllerStress.test_admission_controller_with_configs fail with timeouts

There are 10 timeout failures, 5 for custom_cluster.test_admission_controller.TestAdmissionControllerStress.test_admission_controller_with_flags and 5 for custom_cluster.test_admission_controller.TestAdmissionControllerStress.test_admission_controller_with_configs

I'm presuming they are all related so am grouping them here as on issue.

First error information:

*Impala.tests.custom_cluster.test_admission_controller.TestAdmissionControllerStress.test_admission_controller_with_configs[num_queries: 30 | submission_delay_ms: 50 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none | round_robin_submission: False] (from pytest)*

*Error Message*

{noformat}
AssertionError: Timed out waiting 30 seconds for metrics assert (1466777776.4375379 - 1466777745.8216169) < 30  +  where 1466777776.4375379 = time()
{noformat}

*Stacktrace*
{noformat}
self = <test_admission_controller.TestAdmissionControllerStress object at 0x29c3450>
vector = <tests.common.test_vector.TestVector object at 0x500f850>

    @pytest.mark.execute_serially
    @CustomClusterTestSuite.with_args(
        impalad_args=impalad_admission_ctrl_config_args(),
        statestored_args=_STATESTORED_ARGS)
    def test_admission_controller_with_configs(self, vector):
      self.pool_name = 'root.queueB'
>     self.run_admission_test(vector, {'request_pool': self.pool_name})

custom_cluster/test_admission_controller.py:681: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
custom_cluster/test_admission_controller.py:601: in run_admission_test
    ['admitted', 'queued', 'rejected'], initial_metrics, num_queries)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_admission_controller.TestAdmissionControllerStress object at 0x29c3450>
metric_names = ['admitted', 'queued', 'rejected']
initial = {'admitted': 0, 'dequeued': 0, 'queued': 0, 'rejected': 0, ...}
expected_delta = 30, timeout = 30

    def wait_for_metric_changes(self, metric_names, initial, expected_delta, timeout=30):
      """"""
        Waits for the sum of metrics in metric_names to change by at least expected_delta.
    
        This is similar to ImpalaService.wait_for_metric_value(), but it uses one or more
        metrics aggregated across all impalads, e.g. we want to wait for the total number of
        admitted, queued, and rejected metrics to change some amount in total, but we don't
        know exactly how the metrics will change individually.
        'metric_names' is a list of the keys returned by get_admission_metrics() which are
        expected to change.
        'initial' is the initial set of metrics returned by get_admission_metrics() to
        compare against.
        'expected_delta' is the total change expected across all impalads for the specified
        metrics.
        """"""
      log_metrics(""wait_for_metric_changes, initial="", initial)
      current = initial
      start_time = time()
      while True:
        current = self.get_admission_metrics()
        log_metrics(""wait_for_metric_changes, current="", current)
        deltas = compute_metric_deltas(current, initial, metric_names)
        delta_sum = sum([ deltas[x] for x in metric_names ])
        LOG.debug(""DeltaSum=%s Deltas=%s (Expected=%s for metrics=%s)"",\
            delta_sum, deltas, expected_delta, metric_names)
        if delta_sum >= expected_delta:
          LOG.debug(""Found all %s metrics after %s seconds"", delta_sum,
              round(time() - start_time, 1))
          return (deltas, current)
>       assert (time() - start_time < timeout),\
            ""Timed out waiting %s seconds for metrics"" % (timeout,)
E       AssertionError: Timed out waiting 30 seconds for metrics
E       assert (1466777776.4375379 - 1466777745.8216169) < 30
E        +  where 1466777776.4375379 = time()

custom_cluster/test_admission_controller.py:409: AssertionError
{noformat}

*Standard Output*
{noformat}
Starting State Store logging to /data/jenkins/workspace/impala-cdh5-trunk-exhaustive-code-coverage/repos/Impala/logs/custom_cluster_tests/statestored.INFO
Starting Catalog Service logging to /data/jenkins/workspace/impala-cdh5-trunk-exhaustive-code-coverage/repos/Impala/logs/custom_cluster_tests/catalogd.INFO
Starting Impala Daemon logging to /data/jenkins/workspace/impala-cdh5-trunk-exhaustive-code-coverage/repos/Impala/logs/custom_cluster_tests/impalad.INFO
Starting Impala Daemon logging to /data/jenkins/workspace/impala-cdh5-trunk-exhaustive-code-coverage/repos/Impala/logs/custom_cluster_tests/impalad_node1.INFO
Starting Impala Daemon logging to /data/jenkins/workspace/impala-cdh5-trunk-exhaustive-code-coverage/repos/Impala/logs/custom_cluster_tests/impalad_node2.INFO
Waiting for Catalog... Status: 52 DBs / 1061 tables (ready=True)
Waiting for Catalog... Status: 52 DBs / 1061 tables (ready=True)
Waiting for Catalog... Status: 52 DBs / 1061 tables (ready=True)
Impala Cluster Running with 3 nodes.
{noformat}
*Standard Error*
{noformat}
MainThread: Found 3 impalad/1 statestored/1 catalogd process(es)
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25000
MainThread: Waiting for num_known_live_backends=3. Current value: 0
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25000
MainThread: Waiting for num_known_live_backends=3. Current value: 0
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25000
MainThread: Waiting for num_known_live_backends=3. Current value: 0
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25000
MainThread: num_known_live_backends has reached value: 3
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25001
MainThread: num_known_live_backends has reached value: 3
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25002
MainThread: Waiting for num_known_live_backends=3. Current value: 2
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25002
MainThread: num_known_live_backends has reached value: 3
MainThread: Found 3 impalad/1 statestored/1 catalogd process(es)
MainThread: Getting metric: statestore.live-backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25010
MainThread: Metric 'statestore.live-backends' has reach desired value: 4
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25000
MainThread: num_known_live_backends has reached value: 3
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25001
MainThread: num_known_live_backends has reached value: 3
MainThread: Getting num_known_live_backends from impala-boost-static-burst-slave-1803.vpc.cloudera.com:25002
MainThread: num_known_live_backends has reached value: 3
-- connecting to: localhost:21000
-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

-- connecting to: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
-- executing against impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
use functional;

SET batch_size=0;
SET num_nodes=0;
SET disable_codegen=False;
SET abort_on_error=1;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 2;

SET batch_size=0;
SET batch_size=0;
SET num_nodes=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET batch_size=0;
SET num_nodes=0;
SET batch_size=0;
SET batch_size=0;
SET num_nodes=0;
SET disable_codegen=False;
SET batch_size=0;
SET batch_size=0;
SET num_nodes=0;
SET batch_size=0;
SET batch_size=0;
SET num_nodes=0;
SET batch_size=0;
SET disable_codegen=False;
SET abort_on_error=1;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET num_nodes=0;
SET disable_codegen=False;
SET abort_on_error=1;
SET num_nodes=0;
SET batch_size=0;
SET batch_size=0;
SET abort_on_error=1;
SET batch_size=0;
SET num_nodes=0;
SET num_nodes=0;
SET disable_codegen=False;
SET num_nodes=0;
SET disable_codegen=False;
SET abort_on_error=1;
SET num_nodes=0;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET disable_codegen=False;
SET batch_size=0;
SET request_pool=root.queueB;
SET disable_codegen=False;
SET num_nodes=0;
SET num_nodes=0;
SET request_pool=root.queueB;
SET num_nodes=0;
SET disable_codegen=False;
SET disable_codegen=False;
SET abort_on_error=1;
SET disable_codegen=False;
SET batch_size=0;
SET request_pool=root.queueB;
SET disable_codegen=False;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET abort_on_error=1;
SET num_nodes=0;
SET debug_action=0:GETNEXT:WAIT;
SET abort_on_error=1;
SET disable_codegen=False;
SET disable_codegen=False;
SET debug_action=0:GETNEXT:WAIT;
SET disable_codegen=False;
SET abort_on_error=1;
SET abort_on_error=1;
SET request_pool=root.queueB;
SET abort_on_error=1;
SET num_nodes=0;
SET debug_action=0:GETNEXT:WAIT;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 13;

SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET request_pool=root.queueB;
SET disable_codegen=False;
SET exec_single_node_rows_threshold=0;
SET request_pool=root.queueB;
SET abort_on_error=1;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 9;

SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
SET debug_action=0:GETNEXT:WAIT;
SET disable_codegen=False;
SET exec_single_node_rows_threshold=0;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 19;

SET batch_size=0;
SET num_nodes=0;
SET disable_codegen=False;
SET debug_action=0:GETNEXT:WAIT;
SET debug_action=0:GETNEXT:WAIT;
SET debug_action=0:GETNEXT:WAIT;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
SET debug_action=0:GETNEXT:WAIT;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
SET abort_on_error=1;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 17;

SET debug_action=0:GETNEXT:WAIT;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 12;

SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 5;

SET abort_on_error=1;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 20;

SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 23;

SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 25;

SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 1;

SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 18;

SET exec_single_node_rows_threshold=0;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 8;

SET abort_on_error=1;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 26;

SET abort_on_error=1;
SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 10;

SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 24;

-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 3;

SET exec_single_node_rows_threshold=0;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 7;

-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 21;

SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 27;

SET request_pool=root.queueB;
SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 29;

SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 11;

SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 30;

-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 16;

-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 4;

-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 6;

SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 15;

SET debug_action=0:GETNEXT:WAIT;
SET exec_single_node_rows_threshold=0;
-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 14;

-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 22;

-- executing async: impala-boost-static-burst-slave-1803.vpc.cloudera.com:21000
select * from alltypes where id != 28;
{noformat}"	IMPALA	Resolved	1	1	7280	broken-build, test_issue
13055553	Disable Kudu and Squeasel OpenSSL initialization	Disable Kudu and Sqeuasel OpenSSL initialization; they should defer to the thrift initialization.	IMPALA	Resolved	2	7	7280	kudu, security, ssl
13052355	Failed to expand memory when running llama on yarn because of bug in MemTracker::ExpandLimit	"The method of MemTracker::ExpandLimit is like this:

{code}
bool MemTracker::ExpandLimit(int64_t bytes) {
...
...
// Finally, check whether the allocation that we got took us over the limits for any of
  // our ancestors.
  int64_t bytes_allocated = resource.memory_mb * 1024 * 1024;
  BOOST_FOREACH(const MemTracker* tracker, all_trackers_) {
    if (tracker == this) continue;
    if (tracker->consumption_->current_value() + bytes_allocated > tracker->limit_) {
      // Don't adjust our limit; rely on query tear-down to release the resource.
      return false;
    }
  }
...
...
}
{code}

In the for loop, it does *not* check whether tracker->limit_<0; That leads to failure when expanding memory from llama;"	IMPALA	Resolved	3	1	7280	resource-management
13055186	TestKuduOperations.test_kudu_alter_table fails	"The exhaustive builds on RHEL7 hits the following error.

{noformat}

Stacktrace

query_test/test_kudu.py:62: in test_kudu_alter_table
    self.run_test_case('QueryTest/kudu_alter', vector, use_db=unique_database)
common/impala_test_suite.py:321: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:525: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:160: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:173: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:337: in __execute_query
    handle = self.execute_query_async(query_string, user=user)
beeswax/impala_beeswax.py:333: in execute_query_async
    return self.__do_rpc(lambda: self.imp_service.query(query,))
beeswax/impala_beeswax.py:458: in __do_rpc
    raise ImpalaBeeswaxException(self.__build_error_message(b), b)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
E    MESSAGE: AnalysisException: Target table 'test_kudu_alter_table_8df0adc5.tbl_to_alter' has fewer columns (3) than the SELECT / VALUES clause returns (5)
Standard Error

SET sync_ddl=False;
-- executing against localhost:21000
DROP DATABASE IF EXISTS `test_kudu_alter_table_8df0adc5` CASCADE;

SET sync_ddl=False;
-- executing against localhost:21000
CREATE DATABASE `test_kudu_alter_table_8df0adc5`;

MainThread: Created database ""test_kudu_alter_table_8df0adc5"" for test ID ""query_test/test_kudu.py::TestKuduOperations::()::test_kudu_alter_table[exec_option: {'disable_codegen': True, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none]""
-- executing against localhost:21000
use test_kudu_alter_table_8df0adc5;

SET disable_codegen=True;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
SET batch_size=0;
SET num_nodes=0;
-- executing against localhost:21000
create table simple (id int primary key, name string, valf float, vali bigint)
  distribute by hash (id) into 3 buckets stored as kudu;

-- executing against localhost:21000
alter table simple set tblproperties (
  'kudu.master_addresses' = 'localhost'
);

-- executing against localhost:21000
describe formatted simple;

-- executing against localhost:21000
alter table simple set tblproperties ('kudu.master_addresses' = '127.0.0.1');

-- executing against localhost:21000
alter table simple set tblproperties ('kudu.master_addresses' = 'invalid_host');

-- executing against localhost:21000
alter table simple rename to simple_new;

-- executing against localhost:21000
select count(*) from simple_new;

-- executing against localhost:21000
create table tbl_to_alter (id int primary key, name string null, vali bigint not null)
  distribute by range (id) (partition 1 < values <= 10) stored as kudu
  tblproperties('kudu.table_name'='tbl_to_alter');

-- executing against localhost:21000
alter table tbl_to_alter add range partition 10 < values <= 20;

-- executing against localhost:21000
insert into tbl_to_alter values (15, 'name', 100);

-- executing against localhost:21000
select * from tbl_to_alter limit 1000;

-- executing against localhost:21000
alter table tbl_to_alter add range partition value = 100;

-- executing against localhost:21000
insert into tbl_to_alter values (100, 'name1', 1000);

-- executing against localhost:21000
select * from tbl_to_alter limit 1000;

-- executing against localhost:21000
alter table tbl_to_alter add range partition 1000 < values;

-- executing against localhost:21000
alter table tbl_to_alter add range partition 10 < values <= 30;

-- executing against localhost:21000
alter table tbl_to_alter add if not exists range partition 10 < values <= 30;

-- executing against localhost:21000
alter table tbl_to_alter drop range partition value = 100;

-- executing against localhost:21000
select * from tbl_to_alter;

-- executing against localhost:21000
alter table tbl_to_alter drop range partition 10 < values <= 20;

-- executing against localhost:21000
alter table tbl_to_alter drop range partition 1 < values <= 10;

-- executing against localhost:21000

alter table tbl_to_alter drop range partition 1000 < values;

-- executing against localhost:21000
select count(*), count(id) from tbl_to_alter
  where id = 1 and cast(sin(id) as boolean) = true;

-- executing against localhost:21000
insert into tbl_to_alter values (1, 'name', 100);

-- executing against localhost:21000
alter table tbl_to_alter add range partition 1 < values <= 20;

-- executing against localhost:21000

alter table tbl_to_alter add columns (new_col1 int not null default 10,
  new_col2 bigint not null default 1000);

-- executing against localhost:21000
insert into tbl_to_alter values (2, 'test', 100, 1, 100);
{noformat}"	IMPALA	Resolved	1	1	7280	broken-build
13054265	kudu-tserver failing to start on build machines	"This has happened on cdh5-trunk and on cdh5-2.6.0_5.8.x, but not on cdh5-2.6.0_5.8.0 yet:

http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-2.6.0_5.8.x/job/impala-cdh5-2.6.0_5.8.x-core/93/
http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-core-data-load/942/
http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-core-s3/70/
http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-exhaustive-release/66/

{code:java}
03:44:18 Starting kudu (Web UI - http://localhost:8051)
03:44:23 Failed to start kudu-tserver. The end of the log (/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/testdata/cluster/cdh5/node-3/var/log/kudu-tserver.out) is:
03:44:23 /tmp/impala-deps/kudu-0.8.0-RC1/release/bin/kudu-tserver: error while loading shared libraries: libsasl2.so.3: cannot open shared object file: No such file or directory
03:44:23 Failed to start kudu-tserver. The end of the log (/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/testdata/cluster/cdh5/node-2/var/log/kudu-tserver.out) is:
03:44:23 /tmp/impala-deps/kudu-0.8.0-RC1/release/bin/kudu-tserver: error while loading shared libraries: libsasl2.so.3: cannot open shared object file: No such file or directory
03:44:23 Failed to start kudu-master. The end of the log (/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/testdata/cluster/cdh5/node-1/var/log/kudu-master.out) is:
03:44:23 /tmp/impala-deps/kudu-0.8.0-RC1/release/bin/kudu-master: error while loading shared libraries: libsasl2.so.3: cannot open shared object file: No such file or directory
03:44:23 Failed to start kudu-tserver. The end of the log (/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/testdata/cluster/cdh5/node-1/var/log/kudu-tserver.out) is:
03:44:23 /tmp/impala-deps/kudu-0.8.0-RC1/release/bin/kudu-tserver: error while loading shared libraries: libsasl2.so.3: cannot open shared object file: No such file or directory
03:44:23 Error in /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/testdata/bin/run-mini-dfs.sh at line 24: $IMPALA_HOME/testdata/cluster/admin start_cluster
03:44:23 Error in /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/testdata/bin/run-all.sh at line 38: tee ${IMPALA_CLUSTER_LOGS_DIR}/run-mini-dfs.log
03:44:23 Error in /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/buildall.sh at line 335: $IMPALA_HOME/testdata/bin/run-all.sh -format
03:44:23 + echo 'buildall.sh ' -release -format '-snapshot_file /data/jenkins/workspace/impala-umbrella-build-and-test/testdata/test-warehouse-SNAPSHOT/test-warehouse-cdh5-943-SNAPSHOT.tar.gz' '-metastore_snapshot_file /data/jenkins/workspace/impala-umbrella-build-and-test/testdata/hive_metastore_snapshot/hive_impala_dump_cdh5-943.txt failed.'
03:44:23 buildall.sh  -release -format -snapshot_file /data/jenkins/workspace/impala-umbrella-build-and-test/testdata/test-warehouse-SNAPSHOT/test-warehouse-cdh5-943-SNAPSHOT.tar.gz -metastore_snapshot_file /data/jenkins/workspace/impala-umbrella-build-and-test/testdata/hive_metastore_snapshot/hive_impala_dump_cdh5-943.txt failed.
{code}"	IMPALA	Resolved	1	1	7280	broken-build
13053605	Admission control crashed on unexpected topic deletion	"On bolt-80 we have seen a node crash when it receives a
    topic deletion for which there is no corresponding entry in
    the PoolStats::remote_stats_ map. The code could not handle
    this case. I don't know exactly how this scenario occurred,
    but we should avoid the crash for now and try to study this
    further in cluster testing. It seemed to happen on a
    particular node a few times in a row; the node would crash
    and upon restart, it occurred again a number of times but
    then stopped.

I have a fix and have tested it on bolt-80."	IMPALA	Resolved	2	1	7280	admission-control
13053815	Enable Admission Control by default (without limits)	Admission control was disabled by default in Impala 2.1. We should reenable it by default, but without limits such that the observable behavior should be the same.	IMPALA	Resolved	2	3	7280	admission-control
13055452	Use scheduling information to make per-node memory reservation tight	Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).	IMPALA	Resolved	3	4	7280	resource-management
13055487	"TestRequestPoolService.testUpdatingConfigs fails: ""checkModifiedConfigResults:245 expected:<root.queueC> but was:<null>"""	"TestRequestPoolService.testUpdatingConfigs fails with the error below.

[~mjacobs] - Im assigning this to you thinking you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped.

{noformat}
14:43:48 Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.313 sec <<< FAILURE! - in org.apache.impala.util.TestRequestPoolService
14:43:48 testUpdatingConfigs(org.apache.impala.util.TestRequestPoolService)  Time elapsed: 7.116 sec  <<< FAILURE!
14:43:48 	at org.apache.impala.util.TestRequestPoolService.checkModifiedConfigResults(TestRequestPoolService.java:245)
14:43:48 	at org.apache.impala.util.TestRequestPoolService.testUpdatingConfigs(TestRequestPoolService.java:206)

...

14:43:48   TestRequestPoolService.testUpdatingConfigs:206->checkModifiedConfigResults:245 expected:<root.queueC> but was:<null>
14:43:48 Tests run: 564, Failures: 1, Errors: 0, Skipped: 20
14:43:48 [INFO] BUILD FAILURE
14:43:48 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18:test (default-test) on project impala-frontend: There are test failures.
14:43:48 [ERROR] 
14:43:48 [ERROR] Please refer to /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/logs/fe_tests for the individual test results.
14:43:48 [ERROR] -> [Help 1]
14:43:48 [ERROR] 
14:43:48 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
14:43:48 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
14:43:48 [ERROR] 
14:43:48 [ERROR] For more information about the errors and possible solutions, please read the following articles:
14:43:48 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
14:43:48 mvn -fae test exited with code 0
{noformat}"	IMPALA	Resolved	1	1	7280	broken-build
13055061	FE ParserTests may report UnsatisfiedLinkError	"In some development environments, the ParserTests may always fail with an UnsatisfiedLinkError:

{code}
java.lang.UnsatisfiedLinkError: org.apache.impala.service.FeSupport.NativeGetStartupOptions()[B
	at org.apache.impala.service.FeSupport.NativeGetStartupOptions(Native Method)
	at org.apache.impala.service.FeSupport.GetStartupOptions(FeSupport.java:268)
	at org.apache.impala.common.RuntimeEnv.<init>(RuntimeEnv.java:47)
	at org.apache.impala.common.RuntimeEnv.<clinit>(RuntimeEnv.java:34)
	at org.apache.impala.testutil.TestUtils.assumeKuduIsSupported(TestUtils.java:288)
	at org.apache.impala.analysis.ParserTest.TestKuduUpdate(ParserTest.java:1697)
{code}

[~lv] reported seeing this last month, while no others saw this. As of this morning, I started to experience the same issue.

I believe the issue is related to some static loading of classes and/or libraries in Java because changing the ParserTest to initialize the Frontend makes the error go away. I haven't been able to pin-point the exact issue with loading, but it makes sense that the ParserTest should initialize the Frontend static state if it will be called by libfesupport later since it seems to be an issue affecting some environments and not others, i.e. subject to environmental factors."	IMPALA	Resolved	2	1	7280	test
13053323	Design for RM test infrastructure	For Impala 2.5	IMPALA	Resolved	2	7	7280	admission-control, resource-management, test-infra
13052750	Handle Llama expansions that are allocated after time out	"Impala may make many expansion requests to Llama, some of which may end up timing out in the ResourceBroker, but Llama doesn't know that about the timeout and may still fulfill the request later. If it is fulfilled, Impala is later notified and Impala just logs a message that a request that timed out was fulfilled.

The problem with this is that the resources are allocated to the reservation but they're never accounted for or used by Impala, so Llama thinks we have more resources than Impala does. In some cases, this seems to cause problems leading to the query failing, e.g. when the thread manager is oversubscribed and thus repeatedly sends expansion requests to get more vcores which continue to time out. Eventually, if some of the first expansion requests are fulfilled, the thread manager doesn't actually know this and will keep sending more expansion requests.

There are several things we might consider doing:
1) In Impala, attempt to account for the resources anyway, even though the request timed out. In some cases this would work fine, e.g. when the vcore expansion thread makes a request that times out. It will likely still want those vcores later and send another expansion request anyway (which is sometimes a problem as well, see IMPALA-1852). This might not work as well for memory because the mem tracker that needs the memory may no longer need it (e.g. if it spilled) or, even worse, it may have failed the query if the minimum buffers couldn't be acquired.
2) Add a mechanism to Llama to release the expansion resources (not yet possible, only possible to release the entire reservation).
3) Add a mechanism to Llama so that the timeouts occur in Llama first, then Llama is not charging the reservation for those resources."	IMPALA	Resolved	2	1	7280	llama, resource-management
13054322	Fix error message when violating key constraint in Kudu insert	"When inserting rows in Kudu using Impala, an unfriendly error message is reported from Impala when a unique key constraint is violated. We need to improve this. 

{code:java}
impala-shell> insert into t1 values (1,1);
WARNINGS: Error while flushing Kudu session:
Already present: entry already present in memrowset


Error while flushing Kudu session:
Already present: entry already present in memrowset
{code}

Another issue is during insert select statements when some inserted rows violate the unique key constraints:
{code}
impala-shell> create table t1 (a int, b int) ... <--- kudu table
impala-shell> insert into t1 values(1,1);
impala-shell> insert into t1 select cast(a + 100 as int), b from t1; <-- works well
impala-shell> insert into t1 select cast(a + 100 as int), b from t1;
{code}
The last statement reports an error, indicating that the insert failed. However, if we run a select on t1 we can see that some rows were successfully inserted. We should improve the error message and always report the number of inserted rows."	IMPALA	Resolved	2	4	7280	kudu, usability
13052151	Crash: impala::ExprContext::FreeLocalAllocations	"This crash is non-deterministic. Impala crashes after several consecutive executions of this query.

QUERY:
{code}
SELECT
CAST(t2.int_col AS STRING) AS char_col,
LAG(COALESCE(505, (334) + (NULL), -603), 20) OVER (ORDER BY CAST(t2.int_col AS STRING) DESC) AS int_col
FROM alltypes t1
INNER JOIN alltypestiny t2 ON (((t2.timestamp_col) = (t1.timestamp_col)) AND ((t2.timestamp_col) = (t1.timestamp_col))) AND ((t2.date_string_col) = (t1.date_string_col))
WHERE
(t2.int_col) IN (t2.smallint_col, t1.tinyint_col)
{code}

STACK:
{code}
raise () from /lib64/libc.so.6
raise () from /lib64/libc.so.6
abort () from /lib64/libc.so.6
os::abort(bool) () from /opt/toolchain/sun-jdk-64bit-1.6.0.31/jre/lib/amd64/server/libjvm.so
VMError::report_and_die() () from /opt/toolchain/sun-jdk-64bit-1.6.0.31/jre/lib/amd64/server/libjvm.so
JVM_handle_linux_signal () from /opt/toolchain/sun-jdk-64bit-1.6.0.31/jre/lib/amd64/server/libjvm.so
signalHandler(int, siginfo*, void*) () from /opt/toolchain/sun-jdk-64bit-1.6.0.31/jre/lib/amd64/server/libjvm.so
#6  <signal handler called>
impala_udf::FunctionContext::impl (this=0x0) at /data/9/query-gen/Impala/be/src/udf/udf.h:201
impala::ExprContext::FreeLocalAllocations (fn_ctxs=std::vector of length 1, capacity 1 = {...}) at /data/9/query-gen/Impala/be/src/exprs/expr-context.cc:120
impala::ExprContext::FreeLocalAllocations (this=0x98b3080) at /data/9/query-gen/Impala/be/src/exprs/expr-context.cc:109
impala::ExprContext::FreeLocalAllocations (ctxs=std::vector of length 3, capacity 4 = {...}) at /data/9/query-gen/Impala/be/src/exprs/expr-context.cc:114
impala::AnalyticEvalNode::QueryMaintenance (this=0x8912e00, state=0x7d2d100) at /data/9/query-gen/Impala/be/src/exec/analytic-eval-node.cc:763
impala::AnalyticEvalNode::ProcessChildBatches (this=0x8912e00, state=0x7d2d100) at /data/9/query-gen/Impala/be/src/exec/analytic-eval-node.cc:516
impala::AnalyticEvalNode::Open (this=0x8912e00, state=0x7d2d100) at /data/9/query-gen/Impala/be/src/exec/analytic-eval-node.cc:220
impala::PlanFragmentExecutor::OpenInternal (this=0x7194d20) at /data/9/query-gen/Impala/be/src/runtime/plan-fragment-executor.cc:332
impala::PlanFragmentExecutor::Open (this=0x7194d20) at /data/9/query-gen/Impala/be/src/runtime/plan-fragment-executor.cc:318
impala::Coordinator::Wait (this=0x6587800) at /data/9/query-gen/Impala/be/src/runtime/coordinator.cc:766
impala::ImpalaServer::QueryExecState::WaitInternal (this=0x6a72000) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:546
impala::ImpalaServer::QueryExecState::Wait (this=0x6a72000) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:522
boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>::operator() (this=0x7f1589392cd8, p=0x6a72000) at /usr/include/boost/bind/mem_fn_template.hpp:49
boost::_bi::list1<boost::_bi::value<impala::ImpalaServer::QueryExecState*> >::operator()<boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>, boost::_bi::list0> (this=0x7f1589392ce8, f=..., a=...) at /usr/include/boost/bind/bind.hpp:253
boost::_bi::bind_t<void, boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>, boost::_bi::list1<boost::_bi::value<impala::ImpalaServer::QueryExecState*> > >::operator() (this=0x7f1589392cd8) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf0<void, impala::ImpalaServer::QueryExecState>, boost::_bi::list1<boost::_bi::value<impala::ImpalaServer::QueryExecState*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
boost::function0<void>::operator() (this=0x7f1589392cd0) at /usr/include/boost/function/function_template.hpp:1013
impala::Thread::SuperviseThread (name=""wait-thread"", category=""query-exec-state"", functor=..., thread_started=0x7f15b4656920) at /data/9/query-gen/Impala/be/src/util/thread.cc:311
boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::string &, const std::string &, impala::Thread::ThreadFunctor, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x9960550, f=@0x9960548, a=...) at /usr/include/boost/bind/bind.hpp:457
boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x9960548) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x99603c0) at /usr/include/boost/thread/detail/thread.hpp:61
thread_proxy ()
start_thread () from /lib64/libpthread.so.0
clone () from /lib64/libc.so.6
{code}

DB: Functional
File Format: Text/None
git Hash: 09363ef"	IMPALA	Resolved	1	1	7280	crash, query_generator
13055208	Kudu now requires replication factor must be odd; tests broken	"In testing the latest kudu build in the Impala toolchain, I discovered that Kudu now requires the replication factor to be odd. We need to fix tests (some set it to 2) and check if any FE/Catalog handling may be necessary.

{code}
13:57:01 -- Test range partitioning with overlapping partitions
13:57:01 create table simple_range_with_overlapping (id int, name string, valf float, vali bigint,
13:57:01   primary key (id, name)) distribute by range (id)
13:57:01   (partition values <= 10, partition values < 20, partition value = 5) stored as kudu;
13:57:01 
13:57:01  TestKuduOperations.test_kudu_stats[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none] 
13:57:01 [gw0] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/bin/../infra/python/env/bin/python
13:57:01 query_test/test_kudu.py:66: in test_kudu_stats
13:57:01     self.run_test_case('QueryTest/kudu_stats', vector, use_db=unique_database)
13:57:01 common/impala_test_suite.py:324: in run_test_case
13:57:01     result = self.__execute_query(target_impalad_client, query, user=user)
13:57:01 common/impala_test_suite.py:532: in __execute_query
13:57:01     return impalad_client.execute(query, user=user)
13:57:01 common/impala_connection.py:160: in execute
13:57:01     return self.__beeswax_client.execute(sql_stmt, user=user)
13:57:01 beeswax/impala_beeswax.py:173: in execute
13:57:01     handle = self.__execute_query(query_string.strip(), user=user)
13:57:01 beeswax/impala_beeswax.py:337: in __execute_query
13:57:01     handle = self.execute_query_async(query_string, user=user)
13:57:01 beeswax/impala_beeswax.py:333: in execute_query_async
13:57:01     return self.__do_rpc(lambda: self.imp_service.query(query,))
13:57:01 beeswax/impala_beeswax.py:458: in __do_rpc
13:57:01     raise ImpalaBeeswaxException(self.__build_error_message(b), b)
13:57:01 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
13:57:01 E    INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'>
13:57:01 E    MESSAGE: 
13:57:01 E   ImpalaRuntimeException: Error creating Kudu table 'impala::test_kudu_stats_8950fab4.simple'
13:57:01 E   CAUSED BY: NonRecoverableException: illegal replication factor 2 (replication factor must be odd)
{code}

Testing w/ native-toolchain & build:
IMPALA_TOOLCHAIN_BUILD_ID=289-f12b0dd2e9
IMPALA_KUDU_VERSION=60aa54e"	IMPALA	Resolved	1	1	7280	kudu
13055273	Kudu scanner threads take a long time to close	"After a recent Kudu change (somewhere between a70c90 and e018a83), we started noticing that Kudu scanner threads were taking longer to Close(). This led to some test failures (see IMPALA-4642 and IMPALA-4645), which are going to be addressed by waiting longer for fragments to complete.

However, we need to understand why the scanner threads are taking so long."	IMPALA	Resolved	2	1	7280	kudu, memory, query-lifecycle, resource-management
13052509	FIRST_VALUE produces incorrect results with some strange windows	"The query generator found a bug with FIRST_VALUE when there is a window like ""ROWS X PRECEDING Y PRECEDING"", such that X < Y and (X > the size of a partition).

The query generator's reproduction is very complicated, but I have the following, simpler example:
{code}
select id, date_string_col,
first_value(id) over (partition by date_string_col order by id rows between 10 preceding and 3 preceding)
from alltypes where id < 15
{code}

This is somewhat of a corner case because it requires having a relatively strange window and data. On debug builds we get a DCHECK failure, but in release builds there may be incorrect results.

{code}
#0  0x00007f5d1abd30d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#0  0x00007f5d1abd30d5 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f5d1abd683b in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x0000000002091e49 in google::DumpStackTraceAndExit () at src/utilities.cc:147
#3  0x000000000208ad1d in google::LogMessage::Fail () at src/logging.cc:1315
#4  0x000000000208d4bf in google::LogMessage::SendToLog (this=0x7f5c6912e9b0) at src/logging.cc:1269
#5  0x000000000208a887 in google::LogMessage::Flush (this=0x7f5c6912e9b0) at src/logging.cc:1138
#6  0x000000000208dd4d in google::LogMessageFatal::~LogMessageFatal (this=0x7f5c6912e9b0, __in_chrg=<optimized out>) at src/logging.cc:1836
#7  0x00000000015a99d8 in impala::AnalyticEvalNode::InitNextPartition (this=0xcc77500, stream_idx=10) at /opt/Impala/be/src/exec/analytic-eval-node.cc:469
#8  0x00000000015a5763 in impala::AnalyticEvalNode::ProcessChildBatch (this=0xcc77500, state=0xba3a000) at /opt/Impala/be/src/exec/analytic-eval-node.cc:598
#9  0x00000000015a515e in impala::AnalyticEvalNode::ProcessChildBatches (this=0xcc77500, state=0xba3a000) at /opt/Impala/be/src/exec/analytic-eval-node.cc:531
#10 0x00000000015a6897 in impala::AnalyticEvalNode::GetNext (this=0xcc77500, state=0xba3a000, row_batch=0x7f5c6912ed20, eos=0x7f5c6912edff) at /opt/Impala/be/src/exec/analytic-eval-node.cc:707
#11 0x00000000015900f3 in impala::SortNode::SortInput (this=0x8615400, state=0xba3a000) at /opt/Impala/be/src/exec/sort-node.cc:143
#12 0x000000000158f668 in impala::SortNode::Open (this=0x8615400, state=0xba3a000) at /opt/Impala/be/src/exec/sort-node.cc:71
#13 0x0000000001481106 in impala::PlanFragmentExecutor::OpenInternal (this=0xb99d810) at /opt/Impala/be/src/runtime/plan-fragment-executor.cc:331
#14 0x0000000001480f10 in impala::PlanFragmentExecutor::Open (this=0xb99d810) at /opt/Impala/be/src/runtime/plan-fragment-executor.cc:317
#15 0x0000000001105a5c in impala::FragmentMgr::FragmentExecState::Exec (this=0xb99d600) at /opt/Impala/be/src/service/fragment-exec-state.cc:49
#16 0x00000000010fcc45 in impala::FragmentMgr::FragmentExecThread (this=0xee35d40, exec_state=0xb99d600) at /opt/Impala/be/src/service/fragment-mgr.cc:70
#17 0x0000000001101665 in boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>::operator() (this=0x7bf76a0, p=0xee35d40, a1=0xb99d600) at /usr/include/boost/bind/mem_fn_template.hpp:165
#18 0x0000000001101473 in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list0> (this=0x7bf76b0, f=..., a=...) at /usr/include/boost/bind/bind.hpp:313
#19 0x0000000001100cb9 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >::operator() (this=0x7bf76a0) at /usr/include/boost/bind/bind_template.hpp:20
#20 0x000000000110076a in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
#21 0x0000000000f9cbc0 in boost::function0<void>::operator() (this=0x7f5c6912fdb0) at /usr/include/boost/function/function_template.hpp:1013
#22 0x00000000011dcc6a in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) (name=..., category=..., functor=..., thread_started=0x7f5c6a130d70) at /opt/Impala/be/src/util/thread.cc:311
#23 0x00000000011e48f6 in boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) (this=0xd4eb810, f=@0xd4eb808: 0xceeb1b8, a=...) at /usr/include/boost/bind/bind.hpp:457
#24 0x00000000011e483f in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() (this=0xd4eb808) at /usr/include/boost/bind/bind_template.hpp:20
#25 0x00000000011e47d2 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() (this=0xd4eb680) at /usr/include/boost/thread/detail/thread.hpp:61
#26 0x00007f5d1d347ce9 in thread_proxy () from /usr/lib/libboost_thread.so.1.46.1
#27 0x00007f5d1d125e9a in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0
#28 0x00007f5d1ac908bd in clone () from /lib/x86_64-linux-gnu/libc.so.6
#29 0x0000000000000000 in ?? ()
{code}

From .FATAL file:
{code}
F0316 11:12:58.511064 10696 analytic-eval-node.cc:469] Check failed: last_result_idx_ == curr_partition_idx_ - 1 (6 vs. 9)
{code}

DB: Functional"	IMPALA	Resolved	1	1	7280	query_generator
13055217	Kudu table name missing from profile summary	"Names of Kudu backed tables is missing from the Detail column, this makes it hard to debug plan/performance issues

{code}
Operator              #Hosts   Avg Time   Max Time    #Rows  Est. #Rows  Peak Mem     Est. Peak Mem  Detail                         
------------------------------------------------------------------------------------------------------------------------------------
15:MERGING-EXCHANGE        1    0.000ns    0.000ns        0          20         0           -1.00 B  UNPARTITIONED                  
08:TOP-N                   8   35.113us   41.460us        0          20   4.00 KB           4.72 KB                                 
14:AGGREGATE               8    1.593ms    2.580ms        0          -1   2.36 MB  8589934592.00 GB  FINALIZE                       
13:EXCHANGE                8    0.000ns    0.000ns        0          -1         0                 0  HASH(c_custkey,c_name,c_acc... 
07:AGGREGATE               8  403.643us  997.300us        0          -1   1.36 MB  8589934592.00 GB  STREAMING                      
06:HASH JOIN               8   25s938ms   27s473ms        0          -1  58.25 KB          855.00 B  INNER JOIN, BROADCAST          
|--12:EXCHANGE             8    0.000ns    0.000ns        0          25         0                 0  BROADCAST                      
|  03:SCAN KUDU            1    3.629ms    3.629ms        0          25         0                 0                                 
05:HASH JOIN               8   39.547ms   43.720ms        0          -1   5.13 GB          12.59 GB  INNER JOIN, PARTITIONED        
|--11:EXCHANGE             8    4s483ms    4s838ms  187.22M     450.00M         0                 0  HASH(c_custkey)                
|  00:SCAN KUDU            9  245.498ms  302.740ms  184.12M     450.00M  51.83 MB                 0                                 
10:EXCHANGE                8    0.000ns    0.000ns        0          -1         0                 0  HASH(o_custkey)                
04:HASH JOIN               8    0.000ns    0.000ns        0          -1   4.38 GB           2.00 GB  INNER JOIN, BROADCAST          
|--09:EXCHANGE             8    9s320ms    9s670ms  172.06M          -1         0                 0  BROADCAST                      
|  01:SCAN KUDU            8  240.514ms  261.638ms  172.06M          -1   4.92 MB                 0                                 
02:SCAN KUDU               8    7.474ms    9.499ms        0       6.00B   6.62 MB                 0
{code}"	IMPALA	Resolved	3	1	7280	profile, supportability
13052770	Join build threads are not reported to the QueryResourceManager	"CPU intensive threads are supposed to be allocated by the ThreadResourceMgr to keep track of the total number of threads that may be allocated between queries on the host. However, when RM is enabled, threads need to be reported to the QueryResourceMgr so that additional VCPUs can be requested from Yarn.

The blocking join operator currently attempts to get a thread from the ThreadResourceMgr, but if it is allocated, it doesn't report it to the QueryResourceMgr. This results in Impala using more VCPUs than it has been allocated by Yarn. Unfortunately the fix is more involved than simply reporting the thread to the QRM because it may cause a child scan node to be starved of the thread that it is supposed to have, thus causing the query to hang when it can't start any scan threads. A quick fix may involve changing the way the scan node handles its first scanner thread, but we should consider rethinking the CPU thread management duality presented by the ThreadResourceMgr and the QueryResourceMgr."	IMPALA	Resolved	3	1	7280	query-lifecycle, resource-management
13053457	admission control always estimate memory by using all available backend even for single node plan.	"Admission control always estimates query total memory by using all available backends, even for single node plan.
{code}
void SimpleScheduler::UpdateMembership(
    const StatestoreSubscriber::TopicDeltaMap& incoming_topic_deltas,
    vector<TTopicDelta>* subscriber_topic_updates) {
...
if (metrics_ != NULL) num_backends_metric_->set_value(current_membership_.size());
...
}
{code}

schedule->set_num_hosts(max<int64_t>(num_backends_metric_->value(), 1));

{code}
int64_t QuerySchedule::GetClusterMemoryEstimate() const {
  DCHECK_GT(num_hosts_, 0);
  const int64_t total_cluster_mem = GetPerHostMemoryEstimate() * num_hosts_;
  DCHECK_GE(total_cluster_mem, 0); // Assume total cluster memory fits in an int64_t.
  return total_cluster_mem;
}
{code}

{code}
Status AdmissionController::CanAdmitRequest(const string& pool_name,
    const int64_t max_requests, const int64_t mem_limit, const QuerySchedule& schedule,
    bool admit_from_queue) {
const int64_t query_total_estimated_mem = schedule.GetClusterMemoryEstimate();
{code}

Especially on large cluster, this cause memory estimation far off for trivial queries (without stats)."	IMPALA	Resolved	3	1	7280	resource-management
13054832	Planner not pushing some predicates with constant exprs to Kudu	"The following query is unable to push to Kudu:

{code}
select count(*) from metrics where `timestamp` < 1475059765 + 10
{code}

Oddly, the following one works fine:

{code}
select count(*) from metrics where `timestamp` < 1000 + 10
{code}

I'm guessing that some kind of implicit CAST is getting inserted here which is blocking the pushdown. 'timestamp' is an 'int' column here."	IMPALA	Resolved	3	1	7280	kudu
13054375	Crash during impala::RuntimeProfile::ComputeTimeInProfile()	"The exhaustive release build failed because impalad crashed. The core dump says

{noformat}
#5  <signal handler called>
#6  _M_data (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/basic_string.h:293
#7  _M_rep (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/basic_string.h:301
#8  size (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/basic_string.h:725
#9  compare (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/basic_string.h:2246
#10 operator< <char, std::char_traits<char>, std::allocator<char> > (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/basic_string.h:2590
#11 operator() (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_function.h:371
#12 _M_lower_bound (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_tree.h:1261
#13 std::_Rb_tree<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*>, std::_Select1st<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> >, std::less<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::basic_string<char, std::char_traits<char>, std::allocator<char> > const, impala::RuntimeProfile::Counter*> > >::find (this=0xc9efcce8, __k=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_tree.h:1913
#14 0x0000000000bbe9f6 in find (this=0xc9efcce8, total=44822200) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/gcc-4.9.2/include/c++/4.9.2/bits/stl_map.h:860
#15 impala::RuntimeProfile::ComputeTimeInProfile (this=0xc9efcce8, total=44822200) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/runtime-profile.cc:341
#16 0x0000000000bbec41 in impala::RuntimeProfile::ComputeTimeInProfile (this=0xc9efcce8, total=44822200) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/runtime-profile.cc:367
#17 0x0000000000bbedd1 in impala::RuntimeProfile::ComputeTimeInProfile (this=0x1409afc0) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/runtime-profile.cc:330
#18 0x0000000000bc16d4 in impala::RuntimeProfile::UpdateAverage (this=0x1409afc0, other=0xf048d00) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/runtime-profile.cc:209
#19 0x0000000000d9444b in impala::Coordinator::UpdateAverageProfile (this=) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/runtime/coordinator.cc:1684
#20 0x0000000000da2601 in impala::Coordinator::UpdateFragmentExecStatus (this=0xadc4000, params=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/runtime/coordinator.cc:1549
#21 0x0000000000abe80f in impala::ImpalaServer::ReportExecStatus (this=0x7579100, return_val=..., params=...) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/service/impala-server.cc:1112
#22 0x0000000000cf28cc in impala::ImpalaInternalServiceProcessor::process_ReportExecStatus (this=0x82c6220, seqid=0, iprot=) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:1451
#23 0x0000000000cebd89 in impala::ImpalaInternalServiceProcessor::dispatchCall (this=0x82c6220, iprot=0x8b037a0, oprot=0x8b02f90, fname=..., seqid=0, callContext=0xb2672d40) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:1370
#24 0x000000000081686c in apache::thrift::TDispatchProcessor::process (this=0x82c6220, in=..., out=..., connectionContext=0xb2672d40) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/thrift-0.9.0-p8/include/thrift/TDispatchProcessor.h:121
#25 0x0000000001b3921f in apache::thrift::server::TThreadedServer::Task::run() ()
#26 0x0000000000a06359 in impala::ThriftThread::RunRunnable (this=) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/rpc/thrift-thread.cc:61
#27 0x0000000000a06db2 in operator() (function_obj_ptr=) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/mem_fn_template.hpp:280
#28 operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<long unsigned int>*>, boost::_bi::list0> (function_obj_ptr=) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:392
#29 operator() (function_obj_ptr=) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#30 boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:153
#31 0x0000000000be05e7 in operator() (name=) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:767
#32 impala::Thread::SuperviseThread (name=) at /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/be/src/util/thread.cc:315
#33 0x0000000000be0ef4 in operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> (this=0xb511000) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:457
#34 operator() (this=0xb511000) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#35 boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0xb511000) at /data/jenkins/workspace/impala-umbrella-build-and-test/Impala-Toolchain/boost-1.57.0/include/boost/thread/detail/thread.hpp:116
#36 0x0000000000e3fe6a in thread_proxy ()
#37 0x0000003d71807851 in start_thread () from /lib64/libpthread.so.0
#38 0x0000003d714e894d in clone () from /lib64/libc.so.6
{noformat}

There is more information in the build artifacts at http://sandbox.jenkins.cloudera.com/job/impala-umbrella-build-and-test/1793/ 

I don't see any recent commits related to this code."	IMPALA	Resolved	1	1	7280	broken-build, crash
13052687	TOpenSessionResp should contain coordinator host	It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.	IMPALA	Resolved	3	4	7280	supportability, usability
13101451	Remove admission control dependency on YARN resourcemanager	Impala's admission controller relies on the YARN fair-scheduler.xml for configuration. That configuration is loaded using YARN directly (ie. as a library by the frontend). In Hadoop 3, a number of changes were made to the YARN resourcemanager which break Impala. While we eventually want to rethink the admission control configuration (IMPALA-4159), in the meantime we at least should avoid using unsupported YARN APIs.	IMPALA	Closed	2	1	7280	admission-control, resource-management, rm
13073142	Inconsistent results when comparing string and timestamp fields	"{code}
create table t1 (c1 string, c2 string);
insert into t1 values ('a string', '2017-01-01');
create table t2 (c1 string) partitioned by (c2 string);
insert into t2 partition (c2) values ('a string', '2017-01-01');
{code}

This correctly returns 1 row
{code}
select * from t1 where c2 = cast('2017-01-01' as timestamp);
{code}

This incorrectly returns 0 rows
{code}
select * from t2 where c2 = cast('2017-01-01' as timestamp);
{code}

These 2 queries correctly return 1 row.
{code}
select * from t1 where c2 >= date_sub('2017-01-01', 1) and c2 <= '2017-01-02'; 
select * from t2 where c2 >= date_sub('2017-01-01', 1) and c2 <= '2017-01-02'; 
{code}

This queries behave differently. The 2nd query returns 0 rows:
{code}
select * from t1 where c2 between date_sub('2017-01-01', 1) and '2017-01-02'; 
select * from t2 where c2 between date_sub('2017-01-01', 1) and '2017-01-02'; 
{code}

This bug was not present in version:
Server version: impalad version 2.6.0-cdh5.8.2 RELEASE (build f25aa5b2bcdabf1eb4233747a7b04a067059ee3b)""

But is present in version:
Server version: impalad version 2.7.0-cdh5.10.1 RELEASE (build 876895d2a90346e69f2aea02d5528c2125ae7a32)"	IMPALA	Resolved	1	1	7280	correctness
13055447	Altering Kudu table schema outside of Impala may result in crash on read	"Creating a table in Impala, changing the column schema outside of Impala, and then reading again in Impala may result in a crash. Neither Impala nor the Kudu client validates the schema immediately before reading, so Impala may attempt to dereference pointers that aren't there. This happens if a string column is dropped and then a new, non string column is added with the old string column's name.

In Impala:
{code}
[localhost:21000] > create table t4 (i int primary key, s string) partition by hash(i) partitions 2 stored as kudu;
Query: create table t4 (i int primary key, s string) partition by hash(i) partitions 2 stored as kudu

Fetched 0 row(s) in 0.49s
[localhost:21000] > insert into t4 values (-1, ""foo"");
Query: insert into t4 values (-1, ""foo"")
Query submitted at: 2017-01-26 09:24:49 (Coordinator: http://mj-desktop.ca.cloudera.com:25000)
Query progress can be monitored at: http://mj-desktop.ca.cloudera.com:25000/query_plan?query_id=1e4e1c4693ecfaee:966d7d8d00000000
Modified 1 row(s), 0 row error(s) in 4.56s
[localhost:21000] > select * from t4;
Query: select * from t4
Query submitted at: 2017-01-26 09:24:57 (Coordinator: http://mj-desktop.ca.cloudera.com:25000)
Query progress can be monitored at: http://mj-desktop.ca.cloudera.com:25000/query_plan?query_id=ff4a805116b35e18:1911962100000000
+----+-----+
| i  | s   |
+----+-----+
| -1 | foo |
+----+-----+
Fetched 1 row(s) in 0.13s
{code}

In Python:
{code}
client = kudu.connect(""localhost"")
table = client.table(""impala::default.t4"")

# Drop 's' which was a string col
ta = client.new_table_alterer(table)
ta.drop_column(""s"")
table = ta.alter()

# Add 's' as an int
ta = client.new_table_alterer(table)
ta.add_column(""s"", ""int32"")
table = ta.alter()

# Add some rows
session = client.new_session()
for i in range(100):
  op = table.new_insert((i, i))
  session.apply(op)
session.flush()
{code}

And finally reading again in Impala:

{code}
Query: select * from t4
Query submitted at: 2017-01-26 09:27:49 (Coordinator: http://mj-desktop.ca.cloudera.com:25000)
Query progress can be monitored at: http://mj-desktop.ca.cloudera.com:25000/query_plan?query_id=874c96e67e4d5f19:f32fdd400000000
Socket error 104: Connection reset by peer
[Not connected] > Goodbye mj
{code}


Crash stack:
{code}
Stack: [0x00007f67acea7000,0x00007f67ad6a8000],  sp=0x00007f67ad6a61f8,  free space=8188k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libc.so.6+0x9807e]  envz_strip+0x7ae
C  [libRuntime.so+0x5904b9]  impala::Tuple::DeepCopy(impala::Tuple*, impala::TupleDescriptor const&, impala::MemPool*)+0x61
C  [libExec.so+0x574469]  impala::KuduScanner::DecodeRowsIntoRowBatch(impala::RowBatch*, impala::Tuple**)+0x1b7
C  [libExec.so+0x57306b]  impala::KuduScanner::GetNext(impala::RowBatch*, bool*)+0x1a7
C  [libExec.so+0x577a07]  impala::KuduScanNode::ProcessScanToken(impala::KuduScanner*, std::string const&)+0x141
C  [libExec.so+0x577d21]  impala::KuduScanNode::RunScannerThread(std::string const&, std::string const*)+0x193
C  [libExec.so+0x579cf6]  boost::_mfi::mf2<void, impala::KuduScanNode, std::string const&, std::string const*>::operator()(impala::KuduScanNode*, std::string const&, std::string const*) const+0x76
C  [libExec.so+0x579b9e]  void boost::_bi::list3<boost::_bi::value<impala::KuduScanNode*>, boost::_bi::value<std::string>, boost::_bi::value<std::string const*> >::operator()<boost::_mfi::mf2<void, impala::KuduScanNode, std::string const&, std::string const*>, boost::_bi::list0>(boost::_bi::type<void>, boost::_mfi::mf2<void, impala::KuduScanNode, std::string const&, std::string const*>&, boost::_bi::list0&, int)+0x88
C  [libExec.so+0x57971f]  boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::KuduScanNode, std::string const&, std::string const*>, boost::_bi::list3<boost::_bi::value<impala::KuduScanNode*>, boost::_bi::value<std::string>, boost::_bi::value<std::string const*> > >::operator()()+0x3b
C  [libExec.so+0x579555]  boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::KuduScanNode, std::string const&, std::string const*>, boost::_bi::list3<boost::_bi::value<impala::KuduScanNode*>, boost::_bi::value<std::string>, boost::_bi::value<std::string const*> > >, void>::invoke(boost::detail::function::function_buffer&)+0x23
C  [libUtil.so+0x2f3f20]  boost::function0<void>::operator()() const+0x52
C  [libUtil.so+0x2f34c7]  impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*)+0x2c5
C  [libUtil.so+0x2fadbe]  void boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0&, int)+0xb2
C  [libUtil.so+0x2fad01]  boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()()+0x3b
C  [libUtil.so+0x2fac5c]  boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > > >::run()+0x1e
{code}


We need to validate the schema when opening the kudu scan tokens (I believe it is guaranteed to work while we have a Kudu scan token open) and fail the query if the schema doesn't match anymore."	IMPALA	Resolved	1	1	7280	crash, kudu
13052661	Crash on impala::Sorter::Run::GetNext<true>	"Upon running a relatively large query I get unexpected exits on many of the impalad processes. Looking at the logs on the daemons, there are no errors reported as such, but under INFO it reports a JRE crash along the lines of:

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x0000000000bc6530, pid=20607, tid=140624489793280
#
# JRE version: Java(TM) SE Runtime Environment (7.0_67-b01) (build 1.7.0_67-b01)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.65-b04 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [impalad+0x7c6530]  impala::Status impala::Sorter::Run::GetNext<true>(impala::RowBatch*, bool*)+0x170
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /run/cloudera-scm-agent/process/3911-impala-IMPALAD/hs_err_pid20607.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.sun.com/bugreport/crash.jsp
#

I'm not sure what additional info may be helpful in diagnosing this, but if there's something else that I can supply then please let me know.

Thanks!"	IMPALA	Resolved	1	1	7280	impala
13092763	"test_kudu_insert failed with ""Remote error: Service unavailable: Soft memory limit exceeded"" "	"{noformat}
query_test/test_kudu.py:81: in test_kudu_insert
    self.run_test_case('QueryTest/kudu_insert', vector, use_db=unique_database)
common/impala_test_suite.py:390: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:598: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:160: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:173: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:339: in __execute_query
    self.wait_for_completion(handle)
beeswax/impala_beeswax.py:359: in wait_for_completion
    raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:Kudu error(s) reported, first error: Timed out: Failed to write batch of 33707 ops to tablet 3f5d8cdd0bf842359df0f3ed8467b3dc after 572 attempt(s): Failed to write to server: (no server available): Write(tablet: 3f5d8cdd0bf842359df0f3ed8467b3dc, num_ops: 33707, num_attempts: 572) passed its deadline: Remote error: Service unavailable: Soft memory limit exceeded (at 99.95% of capacity)
E   
E   Key already present in Kudu table 'impala::test_kudu_insert_886525c4.kudu_test'. (1 of 15 similar)
E   Error in Kudu table 'impala::test_kudu_insert_886525c4.kudu_test': Timed out: Failed to write batch of 33707 ops to tablet 3f5d8cdd0bf842359df0f3ed8467b3dc after 572 attempt(s): Failed to write to server: (no server available): Write(tablet: 3f5d8cdd0bf842359df0f3ed8467b3dc, num_ops: 33707, num_attempts: 572) passed its deadline: Remote error: Service unavailable: Soft memory limit exceeded (at 99.95% of capacity) (1 of 33707 similar)
{noformat}

[~mjacobs], can you please take a first look and re-assign to Kudu if necessary ?"	IMPALA	Resolved	1	1	7280	broken-build, kudu
13054558	Data loading may fail on tpch kudu	"Some gvms failed on data loading when tpch kudu tables already existed and load-data.py attempts to re-create the same tables. Ideally our data loading code would be smart enough to not re-create the same tables again, but it looks to me like we actually create tables for all other formats with ""IF NOT EXISTS"", so we should do the same for Kudu."	IMPALA	Resolved	2	1	7280	kudu, test-infra
13051769	Impala should limit the per-query core estimate to Yarn's per-node limit	"Yarn has {{yarn.nodemanager.resource.cpu-vcores}} which sets the maximum number of cores a single node can allocate. Impala uses Java's {{availableCores()}} method which looks at the whole machine. When RM is enabled, we should limit the estimated number of cores per-node to {{yarn.nodemanager.resource.cpu-vcores}}.

This is probably easiest to do in the backend in {{QuerySchedule::GetPerHostVCores()}}."	IMPALA	Resolved	4	1	7280	llama, resource-management
13055526	Unable to open scanner: Timed out errors when running COMPUTE STATS on Kudu-related tables	"I have noticed that when loading our test data onto a cdh5-trunk cluster, there are frequent errors when we run compute stats on Kudu-related tables, but these errors don't appear one earlier versions (e.g., with CDH 5.10). They also don't appear on mini-cluster tests.

{{compute-table-stats.log}} from cdh5-trunk test run:

{noformat}
Executing: compute stats functional_kudu.alltypestiny
  -> Error: ImpalaBeeswaxException:
 Query aborted:
Unable to open scanner: Timed out: unable to retry before timeout: Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 465216 to be safe (mode: NON-LEADER). Current safe time: L: 390220 Physical time difference: None (Logical clock): Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 431554 to be safe (mode: NON-LEADER). Current safe time: L: 367307 Physical time difference: None (Logical clock)

Executing: compute stats functional_kudu.jointbl
  -> Updated 1 partition(s) and 4 column(s).

Executing: compute stats functional_kudu.emptytable
  -> Error: ImpalaBeeswaxException:
 Query aborted:
Unable to open scanner: Timed out: Timed out waiting for ts: L: 501168 to be safe (mode: NON-LEADER). Current safe time: L: 449249 Physical time difference: None (Logical clock)

Executing: compute stats functional_kudu.nulltable
  -> Error: ImpalaBeeswaxException:
 Query aborted:
Unable to open scanner: Timed out: unable to retry before timeout: Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 535495 to be safe (mode: NON-LEADER). Current safe time: L: 438046 Physical time difference: None (Logical clock): Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 501998 to be safe (mode: NON-LEADER). Current safe time: L: 415262 Physical time difference: None (Logical clock)

Executing: compute stats functional_kudu.dimtbl
  -> Error: ImpalaBeeswaxException:
 Query aborted:
Unable to open scanner: Timed out: unable to retry before timeout: Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 570160 to be safe (mode: NON-LEADER). Current safe time: L: 461598 Physical time difference: None (Logical clock): Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 536646 to be safe (mode: NON-LEADER). Current safe time: L: 438814 Physical time difference: None (Logical clock)

Executing: compute stats functional_kudu.alltypessmall
  -> Error: ImpalaBeeswaxException:
 Query aborted:
Unable to open scanner: Timed out: unable to retry before timeout: Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 603727 to be safe (mode: NON-LEADER). Current safe time: L: 484424 Physical time difference: None (Logical clock): Remote error: Service unavailable: Timed out: could not wait for desired snapshot timestamp to be consistent: Timed out waiting for ts: L: 570916 to be safe (mode: NON-LEADER). Current safe time: L: 462086 Physical time difference: None (Logical clock)

Executing: compute stats functional_kudu.alltypesagg_idx
  -> Updated 1 partition(s) and 15 column(s).

Executing: compute stats functional_kudu.alltypesaggnonulls
  -> Updated 1 partition(s) and 14 column(s).

Executing: compute stats functional_kudu.tinytable
  -> Updated 1 partition(s) and 2 column(s).
{noformat}


{{compute-table-stats.log}} from CDH 5.10 test run:

{noformat}
Executing: compute stats functional_kudu.alltypestiny
  -> Updated 1 partition(s) and 13 column(s).

Executing: compute stats functional_kudu.jointbl
  -> Updated 1 partition(s) and 4 column(s).

Executing: compute stats functional_kudu.emptytable
  -> Updated 1 partition(s) and 2 column(s).

Executing: compute stats functional_kudu.nulltable
  -> Updated 1 partition(s) and 7 column(s).

Executing: compute stats functional_kudu.dimtbl
  -> Updated 1 partition(s) and 3 column(s).

Executing: compute stats functional_kudu.alltypessmall
  -> Updated 1 partition(s) and 13 column(s).

Executing: compute stats functional_kudu.alltypesagg_idx
  -> Updated 1 partition(s) and 15 column(s).

Executing: compute stats functional_kudu.alltypesaggnonulls
  -> Updated 1 partition(s) and 14 column(s).

Executing: compute stats functional_kudu.tinytable
  -> Updated 1 partition(s) and 2 column(s).
{noformat}"	IMPALA	Resolved	2	1	7280	remote_cluster_test
13052876	Crash: std::less<impala::TErrorCode::type>::operator()(impala::TErrorCode::type const&, impala::TErrorCode::type const&)	"The kerberized stress test crashed with the following (I haven't seen this before in a non-kerberized cluster)

{noformat}
[...skipped...]
#5  <signal handler called>
#6  0x00000000010a8036 in std::less<impala::TErrorCode::type>::operator()(impala::TErrorCode::type const&, impala::TErrorCode::type const&) const ()
#7  0x0000000001385154 in std::_Rb_tree<impala::TErrorCode::type, std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry>, std::_Select1st<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> >, std::less<impala::TErrorCode::type>, std::allocator<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> > >::_M_lower_bound(std::_Rb_tree_node<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> > const*, std::_Rb_tree_node<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> > const*, impala::TErrorCode::type const&) const ()
#8  0x0000000001384b2a in std::_Rb_tree<impala::TErrorCode::type, std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry>, std::_Select1st<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> >, std::less<impala::TErrorCode::type>, std::allocator<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> > >::find(impala::TErrorCode::type const&) const ()
#9  0x0000000001384983 in std::map<impala::TErrorCode::type, impala::TErrorLogEntry, std::less<impala::TErrorCode::type>, std::allocator<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> > >::find(impala::TErrorCode::type const&) const ()
#10 0x0000000001384583 in impala::ErrorCount(std::map<impala::TErrorCode::type, impala::TErrorLogEntry, std::less<impala::TErrorCode::type>, std::allocator<std::pair<impala::TErrorCode::type const, impala::TErrorLogEntry> > > const&) ()
#11 0x00000000011e4289 in impala::RuntimeState::LogError(impala::ErrorMsg const&) ()
#12 0x00000000012006f2 in impala::BufferedBlockMgr::WriteComplete(impala::BufferedBlockMgr::Block*, impala::Status const&) ()
#13 0x000000000120fc76 in boost::_mfi::mf2<void, impala::BufferedBlockMgr, impala::BufferedBlockMgr::Block*, impala::Status const&>::operator()(impala::BufferedBlockMgr*, impala::BufferedBlockMgr::Block*, impala::Status const&) const ()
#14 0x000000000120f9c9 in void boost::_bi::list3<boost::_bi::value<impala::BufferedBlockMgr*>, boost::_bi::value<impala::BufferedBlockMgr::Block*>, boost::arg<1> >::operator()<boost::_mfi::mf2<void, impala::BufferedBlockMgr, impala::BufferedBlockMgr::Block*, impala::Status const&>, boost::_bi::list1<impala::Status const&> >(boost::_bi::type<void>, boost::_mfi::mf2<void, impala::BufferedBlockMgr, impala::BufferedBlockMgr::Block*, impala::Status const&>&, boost::_bi::list1<impala::Status const&>&, int) ()
#15 0x000000000120f44a in void boost::_bi::bind_t<boost::_bi::unspecified, boost::_mfi::mf2<void, impala::BufferedBlockMgr, impala::BufferedBlockMgr::Block*, impala::Status const&>, boost::_bi::list3<boost::_bi::value<impala::BufferedBlockMgr*>, boost::_bi::value<impala::BufferedBlockMgr::Block*>, boost::arg<1> > >::operator()<impala::Status>(impala::Status const&) ()
#16 0x000000000120eaf0 in boost::detail::function::void_function_obj_invoker1<boost::_bi::bind_t<boost::_bi::unspecified, boost::_mfi::mf2<void, impala::BufferedBlockMgr, impala::BufferedBlockMgr::Block*, impala::Status const&>, boost::_bi::list3<boost::_bi::value<impala::BufferedBlockMgr*>, boost::_bi::value<impala::BufferedBlockMgr::Block*>, boost::arg<1> > >, void, impala::Status const&>::invoke(boost::detail::function::function_buffer&, impala::Status const&) ()
#17 0x0000000001223f9e in boost::function1<void, impala::Status const&>::operator()(impala::Status const&) const ()
#18 0x000000000122db28 in impala::DiskIoMgr::RequestContext::Cancel(impala::Status const&) ()
#19 0x000000000121ccc4 in impala::DiskIoMgr::GetNextRequestRange(impala::DiskIoMgr::DiskQueue*, impala::DiskIoMgr::RequestRange**, impala::DiskIoMgr::RequestContext**) ()
#20 0x000000000121de94 in impala::DiskIoMgr::WorkLoop(impala::DiskIoMgr::DiskQueue*) ()
#21 0x000000000122bc6a in boost::_mfi::mf1<void, impala::DiskIoMgr, impala::DiskIoMgr::DiskQueue*>::operator()(impala::DiskIoMgr*, impala::DiskIoMgr::DiskQueue*) const ()
#22 0x000000000122b847 in void boost::_bi::list2<boost::_bi::value<impala::DiskIoMgr*>, boost::_bi::value<impala::DiskIoMgr::DiskQueue*> >::operator()<boost::_mfi::mf1<void, impala::DiskIoMgr, impala::DiskIoMgr::DiskQueue*>, boost::_bi::list0>(boost::_bi::type<void>, boost::_mfi::mf1<void, impala::DiskIoMgr, impala::DiskIoMgr::DiskQueue*>&, boost::_bi::list0&, int) ()
#23 0x000000000122ae87 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::DiskIoMgr, impala::DiskIoMgr::DiskQueue*>, boost::_bi::list2<boost::_bi::value<impala::DiskIoMgr*>, boost::_bi::value<impala::DiskIoMgr::DiskQueue*> > >::operator()() ()
#24 0x000000000122a48a in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::DiskIoMgr, impala::DiskIoMgr::DiskQueue*>, boost::_bi::list2<boost::_bi::value<impala::DiskIoMgr*>, boost::_bi::value<impala::DiskIoMgr::DiskQueue*> > >, void>::invoke(boost::detail::function::function_buffer&) ()
#25 0x00000000011d0715 in boost::function0<void>::operator()() const ()
#26 0x00000000013e39e2 in impala::Thread::SuperviseThread(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*) ()
#27 0x00000000013ecb6a in void boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0&, int) ()
#28 0x00000000013ecab1 in boost::_bi::bind_t<void, void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() ()
#29 0x00000000013eca70 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() ()
#30 0x00000000016df8a3 in ?? ()
#31 0x0000003c83e079d1 in start_thread () from /lib64/libpthread.so.0
#32 0x0000003c83ae89dd in clone () from /lib64/libc.so.6
{noformat}

The server log doesn't say much

{noformat}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00000000010a8036, pid=31161, tid=139945377367808
#
# JRE version: Java(TM) SE Runtime Environment (7.0_67-b01) (build 1.7.0_67-b01)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.65-b04 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [impalad+0xca8036]  std::less<impala::TErrorCode::type>::operator()(impala::TErrorCode::type const&, impala::TErrorCode::type const&) const+0x14
#
# Core dump written. Default location: /var/log/impalad/core or core.31161
#
# An error report file with more information is saved as:
# /var/log/impalad/hs_err_pid31161.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.sun.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
{noformat}

There is a core at impala-stress-kerberized-4.vpc.cloudera.com:/var/log/impalad/core.31161"	IMPALA	Resolved	2	1	7280	resource-management, stress
13054530	custom_cluster/test_admission_controller.py	"{noformat}
self = <test_admission_controller.TestAdmissionController object at 0x4185450>

    def add_session(self):
      open_session_req = TCLIService.TOpenSessionReq()
      open_session_req.username = getuser()
      open_session_req.configuration = dict()
      if conf_overlay is not None:
        open_session_req.configuration = conf_overlay
      open_session_req.client_protocol = protocol_version
      resp = self.hs2_client.OpenSession(open_session_req)
      HS2TestSuite.check_response(resp)
      self.session_handle = resp.sessionHandle
      assert protocol_version <= resp.serverProtocolVersion
      try:
>       fn(self)

hs2/hs2_test_suite.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
custom_cluster/test_admission_controller.py:241: in test_set_request_pool
    [queueA_mem_limit, 'QUERY_TIMEOUT_S=5', 'REQUEST_POOL=root.queueA', batch_size])
custom_cluster/test_admission_controller.py:146: in __check_hs2_query_opts
    HS2TestSuite.check_response(execute_statement_resp)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

response = TExecuteStatementResp(status=TStatus(errorCode=None, errorMessage='Admission for query exceeded timeout 50ms. Queued r...mber of running queries 1 is over limit 1\n', sqlState='HY000', infoMessages=None, statusCode=3), operationHandle=None)
expected_status_code = 0, expected_error_prefix = None

    @staticmethod
    def check_response(response,
                       expected_status_code = TCLIService.TStatusCode.SUCCESS_STATUS,
                       expected_error_prefix = None):
>     assert response.status.statusCode == expected_status_code
E     assert 3 == 0
E      +  where 3 = 3
E      +    where 3 = TStatus(errorCode=None, errorMessage='Admission for query exceeded timeout 50ms. Queued reason: number of running queries 1 is over limit 1\n', sqlState='HY000', infoMessages=None, statusCode=3).statusCode
E      +      where TStatus(errorCode=None, errorMessage='Admission for query exceeded timeout 50ms. Queued reason: number of running queries 1 is over limit 1\n', sqlState='HY000', infoMessages=None, statusCode=3) = TStatus(errorCode=None, errorMessage='Admission for query exceeded timeout 50ms. Queued reason: number of running queries 1 is over limit 1\n', sqlState='HY000', infoMessages=None, statusCode=3)
E      +        where TStatus(errorCode=None, errorMessage='Admission for query exceeded timeout 50ms. Queued reason: number of running queries 1 is over limit 1\n', sqlState='HY000', infoMessages=None, statusCode=3) = TExecuteStatementResp(status=TStatus(errorCode=None, errorMessage='Admission for query exceeded timeout 50ms. Queued r...mber of running queries 1 is over limit 1\n', sqlState='HY000', infoMessages=None, statusCode=3), operationHandle=None).status

hs2/hs2_test_suite.py:84: AssertionError
{noformat}

http://sandbox.jenkins.cloudera.com/job/impala-cdh5-2.6.0_5.8.0-exhaustive/59/"	IMPALA	Resolved	4	1	7280	admission-control
13053100	Seemingly infinite loop in the sorter	"The following query results in an infinite loop in the sorter:

{code:java}
select
t1.field_102.field_104.field_107,
LAG(-496, 72) OVER (ORDER BY t1.field_102.field_104.field_107 ASC) AS int_col
FROM table_3 t1
INNER JOIN t1.field_86 t2
INNER JOIN t1.field_102.field_104.field_108.field_110 t3
INNER JOIN table_5 t4
WHERE
NOT EXISTS (SELECT
tt1.pos  AS int_col
FROM t1.field_102.field_104.field_108.field_110 tt1
CROSS JOIN t1.field_86 tt2
WHERE
((tt1.pos) IN (tt1.pos, -581.8)) AND (((t1.field_85) = (tt2.key)) AND ((t1.field_82) = (tt2.value.field_94))));
{code}

This dataset can be found in:
 ssh dev@vd0206.halxg.cloudera.com -p 33333

This seems to be happening in the sorter. Unfortunately I do not have more information about this now."	IMPALA	Resolved	2	1	7280	correctness, impala, nested_types
13054475	KuduScanNode doesn't scan all ranges under load	"When running queries under load, we observe the KuduScanNode can ""miss"" some scan ranges and thus produce incorrect results.

With tpch queries, running on the minicluster with 4 concurrent queries, q4 fails somewhat regularly with incorrect results. node with id=0 should scan a total of 9 ranges but only scanned 4. Profiles attached.

Correct execution:
{code}
      KUDU_SCAN_NODE (id=0):(Total: 140.212ms, non-child: 140.212ms, % non-child: 100.00%)
         - BytesRead: 0
         - NumScannerThreadsStarted: 5 (5)
         - PeakMemoryUsage: 6.68 MB (7004160)
         - RowsRead: 1.16M (1160553)
         - RowsReturned: 57.22K (57218)
         - RowsReturnedRate: 408.08 K/sec
         - ScanRangesComplete: 9 (9)
         - ScannerThreadsInvoluntaryContextSwitches: 992 (992)
         - ScannerThreadsTotalWallClockTime: 19s046ms
           - MaterializeTupleTime(*): 85.248ms
           - ScannerThreadsSysTime: 607.000us
           - ScannerThreadsUserTime: 1s570ms
           - TotalKuduReadTime: 847.033ms
         - ScannerThreadsVoluntaryContextSwitches: 109 (109)
         - TotalKuduScanRoundTrips: 72 (72)
         - TotalReadThroughput: 0.00 /sec
{code}

From another run while under load, this case is missing scan ranges:
{code}
      KUDU_SCAN_NODE (id=0):(Total: 85.715ms, non-child: 85.715ms, % non-child: 100.00%)
         - BytesRead: 0
         - NumScannerThreadsStarted: 1 (1)
         - PeakMemoryUsage: 4.16 MB (4358144)
         - RowsRead: 516.18K (516184)
         - RowsReturned: 25.63K (25630)
         - RowsReturnedRate: 299.01 K/sec
         - ScanRangesComplete: 4 (4)
         - ScannerThreadsInvoluntaryContextSwitches: 406 (406)
         - ScannerThreadsTotalWallClockTime: 1s039ms
           - MaterializeTupleTime(*): 248.612us
           - ScannerThreadsSysTime: 3.907ms
           - ScannerThreadsUserTime: 695.542ms
           - TotalKuduReadTime: 417.162ms
         - ScannerThreadsVoluntaryContextSwitches: 53 (53)
         - TotalKuduScanRoundTrips: 32 (32)
         - TotalReadThroughput: 0.00 /sec
{code}"	IMPALA	Resolved	1	1	7280	correctness, kudu
13051565	Impala caused datanode storm	"Hi, this morning we got ""datanode"" storm.
Suddenly impala started to read blocks from many datanodes. And we didn't have any running queries. When we restarted the whole impala service, storm stopped.
Please see networking when storm started (screenshot).
Affected datanodes had these log entries:

{code}
Previous Next
Host	Log Level	Time	Source	Message
prod-node0101.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: /10.66.49.226:50010, dest: /10.66.49.225:38969, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_722537315_1, offset: 0, srvID: DS-1895691478-10.66.49.226-50010-1392796222466, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_8166683644010943415_23956636, duration: 1006760
 View Log File
prod-node0101.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 132388256993022462, srvID: DS-1895691478-10.66.49.226-50010-1392796222466, success: true
 View Log File
prod-node0107.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: -6297200201966372620, srvID: DS-1792190177-10.66.49.232-50010-1392796222893, success: true
 View Log File
prod-node0118.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: /10.66.49.243:50010, dest: /10.66.49.232:54808, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_453101187_1, offset: 0, srvID: DS-1075945273-10.66.49.243-50010-1392796222416, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_-5476067019874567999_23956435, duration: 1184010
 View Log File
prod-node0118.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: /10.66.49.243:50010, dest: /10.66.49.244:36042, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1114160638_1, offset: 0, srvID: DS-1075945273-10.66.49.243-50010-1392796222416, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_1086074988829525783_23956481, duration: 1181419
 View Log File
prod-node0118.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: -7620517135406691819, srvID: DS-1075945273-10.66.49.243-50010-1392796222416, success: true
 View Log File
prod-node0120.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: /10.66.49.245:50010, dest: /10.66.49.224:43178, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1991936378_1, offset: 0, srvID: DS-1789713258-10.66.49.245-50010-1392796222375, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_1107624047476372504_23956836, duration: 1029509
 View Log File
prod-node0120.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: /10.66.49.245:50010, dest: /10.66.62.89:42025, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1092616849_1, offset: 0, srvID: DS-1789713258-10.66.49.245-50010-1392796222375, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_8764079704706642093_23956571, duration: 1583802
 View Log File
prod-node028.kyc.megafon.ru	INFO	15  2014 10:21	clienttrace	
src: /10.66.49.183:50010, dest: /10.66.62.59:43322, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-70190780_1, offset: 0, srvID: DS-1141421786-10.66.49.183-50010-1363767221252, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_6336672533820089443_23956530, duration: 1421004
 View Log File
{code}

View Log File:
{code}
10:21:26.601	INFO	org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace	
src: /10.66.49.226:50010, dest: /10.66.49.225:38969, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_722537315_1, offset: 0, srvID: DS-1895691478-10.66.49.226-50010-1392796222466, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_8166683644010943415_23956636, duration: 1006760
10:21:26.606	INFO	org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace	
src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 132388256993022462, srvID: DS-1895691478-10.66.49.226-50010-1392796222466, success: true
10:21:26.607	INFO	org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace	
src: /10.66.49.226:50010, dest: /10.66.49.241:33859, bytes: 132096, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_1845184176_1, offset: 0, srvID: DS-1895691478-10.66.49.226-50010-1392796222466, blockid: BP-2086241135-10.66.49.155-1363767213272:blk_2940094362365288428_23956714, duration: 1005634
{code}

""dest"" on other host is impala process (ps -ef, nestat -alnpt)"	IMPALA	Resolved	3	1	7280	impala
13055209	Kudu test failure; several error messages changed	"Several error messages changed which break Impala tests. 

For example:
{code}
13:57:01 -- Test range partitioning with overlapping partitions
13:57:01 create table simple_range_with_overlapping (id int, name string, valf float, vali bigint,
13:57:01   primary key (id, name)) distribute by range (id)
13:57:01   (partition values <= 10, partition values < 20, partition value = 5) stored as kudu;
{code}

{code}
13:57:01 [gw0] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/bin/../infra/python/env/bin/python
13:57:01 query_test/test_kudu.py:59: in test_kudu_partition_ddl
13:57:01     self.run_test_case('QueryTest/kudu_partition_ddl', vector, use_db=unique_database)
13:57:01 common/impala_test_suite.py:327: in run_test_case
13:57:01     self.__verify_exceptions(test_section['CATCH'], str(e), use_db)
13:57:01 common/impala_test_suite.py:218: in __verify_exceptions
13:57:01     (expected_str, actual_str)
13:57:01 E   AssertionError: Unexpected exception string. Expected: NonRecoverableException: overlapping range partitions: first range partition: [<start>, (int32 id=11)), second range partition: [<start>, (int32 id=20))
13:57:01 E   Not found in actual: ImpalaBeeswaxException: INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'> MESSAGE: ImpalaRuntimeException: Error creating Kudu table 'impala::test_kudu_partition_ddl_13aebfc6.simple_range_with_overlapping'CAUSED BY: NonRecoverableException: overlapping range partitions: first range partition: VALUES < 11, second range partition: VALUES < 20
{code}

There is also a change to :
{code}
13:57:01 E   AssertionError: Unexpected exception string. Expected: NonRecoverableException: Key column may not have type of BOOL, FLOAT, or DOUBLE
13:57:01 E   Not found in actual: ImpalaBeeswaxException: INNER EXCEPTION: <class 'beeswaxd.ttypes.BeeswaxException'> MESSAGE: ImpalaRuntimeException: Error creating Kudu table 'impala::test_create_kudu_50fff6f8.tab'CAUSED BY: NonRecoverableException: key column may not have type of BOOL, FLOAT, or DOUBLE
{code}

Testing w/ native-toolchain & build:
IMPALA_TOOLCHAIN_BUILD_ID=289-f12b0dd2e9
IMPALA_KUDU_VERSION=60aa54e

Updating the toolchain to the above version will be blocked on this."	IMPALA	Resolved	1	1	7280	kudu
13092253	Flaky test: query_test/test_udfs.py	"There were some random failures of the UDFs test in our builds.

One of the errors:
{code}
query_test/test_udfs.py:316: in test_java_udfs
    self.run_test_case('QueryTest/java-udf', vector, use_db=unique_database)
common/impala_test_suite.py:390: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:598: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:160: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:173: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:339: in __execute_query
    self.wait_for_completion(handle)
beeswax/impala_beeswax.py:359: in wait_for_completion
    raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:ImpalaRuntimeException: Unable to find class.
E   CAUSED BY: ClassNotFoundException: org.apache.impala.TestUdf
{code}"	IMPALA	Resolved	2	1	7280	flaky
13055181	impala crashes starting up on kerberized clusters where kudu is not supported	"commit de88f0c4af3a07ae6bd6b8c94edcb8748468f522 for ""IMPALA-4497: Fix Kudu client crash w/ SASL initialization"" causes a crash on secure clusters where kudu is not supported.

{code}
#0  0x00007f6e549c595e in ?? () from /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.1/lib/impala/lib/libkudu_client.so.0
#1  0x00007f6e549c650a in kudu::client::DisableSaslInitialization() () from /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.1/lib/impala/lib/libkudu_client.so.0
#2  0x00000000009d75a7 in impala::InitAuth(std::string const&) ()
#3  0x000000000082c54e in impala::InitCommonRuntime(int, char**, bool, impala::TestInfo::Mode) ()
#4  0x0000000000b23db0 in ImpaladMain(int, char**) ()
#5  0x00000000007d0fd3 in main ()
{code}

We need to ensure we do not call the Kudu client on systems where libkudu_client.so is a stub."	IMPALA	Resolved	1	1	7280	crash, kudu
13052679	Query resource reservation shouldn't be released until fragments complete	"As seen in the attached log snippet of impala and llama there are cases where the query is cancelled so impala releases resources yet later still calls into expand CPU resources for llama. Hence we end up returning UNKNOWN_RESERVATION_FOR_EXPANSION and an error is printed in the logs. We don't think this is causing any real problems because the query is already being torn down.

This only happens when a query is cancelled (internally or externally, e.g. if an error occurs) because the fragments are sent a cancellation request asynchronously and then immediately after the coordinator shuts down the query, including releasing the reservation. The coordinator should wait for fragments to finish closing before releasing the reservation."	IMPALA	Resolved	4	1	7280	llama, query-lifecycle, resource-management
13054130	Deadlock between ReportExecStatus/Coordinator	"Parent build: http://sandbox.jenkins.cloudera.com/job/impala-cdh5-trunk-exhaustive-release/39/?
Hung build: http://sandbox.jenkins.cloudera.com/job/impala-umbrella-build-and-test/596/? (still running as of this posting)

According to the debug webpage, it's hung on this query:
{noformat}
select cast(floor(id/3) as int), tinyint_col, first_value(tinyint_col)
over (partition by cast(floor(id/3) as int) order by tinyint_col nulls first),
first_value(cast(tinyint_col as string))
over (partition by cast(floor(id/3) as int) order by cast(tinyint_col as string) nulls first),
last_value(tinyint_col)
over (partition by cast(floor(id/3) as int) order by tinyint_col nulls first),
last_value(cast(tinyint_col as string))
over (partition by cast(floor(id/3) as int) order by cast(tinyint_col as string) nulls first)
from alltypesagg
where id < 50 and (tinyint_col is null or tinyint_col < 4)
{noformat}"	IMPALA	Resolved	2	1	7280	hang, query-lifecycle
13053612	Failure in test_analytic_fns because memory limit was not exceeded.	"MJ, assigning this to you to load balance fixing the broken builds.

See:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Nightly-Builds/job/impala-CI-rhel7/222/

{code}
02:11:07 =================================== FAILURES ===================================
02:11:07  TestQueries.test_analytic_fns[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 100, 'batch_size': 0, 'num_nodes': 0} | table_format: rc/def/block] 
02:11:07 [gw6] linux2 -- Python 2.7.5 /data/jenkins/workspace/impala-CI-rhel7/repos/Impala/bin/../infra/python/env/bin/python
02:11:07 query_test/test_queries.py:68: in test_analytic_fns
02:11:07     self.run_test_case('QueryTest/analytic-fns', vector)
02:11:07 common/impala_test_suite.py:273: in run_test_case
02:11:07     assert test_section['CATCH'].strip() == ''
02:11:07 E   assert 'Memory limit exceeded' == ''
02:11:07 E     - Memory limit exceeded
02:11:07 ----------------------------- Captured stderr call -----------------------------
{code}


This is the failing query
{code}
02:11:07 -- executing against localhost:21000
02:11:07 SET mem_limit=150m;
02:11:07 
02:11:07 -- executing against localhost:21000
02:11:07 
02:11:07 SELECT lag(-180, 13) over (ORDER BY t1.int_col ASC, t2.int_col ASC) AS int_col
02:11:07 FROM functional_parquet.alltypes t1 CROSS JOIN functional_parquet.alltypes t2 LIMIT 10;
{code}"	IMPALA	Resolved	1	1	7280	broken-build
13056401	Flaky tests: Kudu EE tests need longer HS2 connection timeouts	"The following test started failing randomly. 

{code}
MainThread: Created database ""test_kudu_col_null_changed_bc507455"" for test ID ""query_test/test_kudu.py::TestKuduOperations::()::test_kudu_col_null_changed""
11:38:31 ----------------------------- Captured stderr call -----------------------------
11:38:31 MainThread: Failed to open transport (tries_left=3)
11:38:31 Traceback (most recent call last):
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/impala/hiveserver2.py"", line 940, in _execute
11:38:31     return func(request)
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/impala/_thrift_gen/TCLIService/TCLIService.py"", line 265, in ExecuteStatement
11:38:31     return self.recv_ExecuteStatement()
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/impala/_thrift_gen/TCLIService/TCLIService.py"", line 276, in recv_ExecuteStatement
11:38:31     (fname, mtype, rseqid) = self._iprot.readMessageBegin()
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
11:38:31     sz = self.readI32()
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 206, in readI32
11:38:31     buff = self.trans.readAll(4)
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/thrift/transport/TTransport.py"", line 58, in readAll
11:38:31     chunk = self.read(sz - have)
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/thrift/transport/TTransport.py"", line 159, in read
11:38:31     self.__rbuf = StringIO(self.__trans.read(max(sz, self.__rbuf_size)))
11:38:31   File ""/data/jenkins/workspace/impala-umbrella-build-and-test/repos/Impala/infra/python/env/lib/python2.6/site-packages/thrift/transport/TSocket.py"", line 103, in read
11:38:31     buff = self.handle.recv(sz)
11:38:31 timeout: timed out
{code}

Not clear what is going on from the error message or the logs. MJ do you mind taking a quick look? Feel free to reassign as you see fit. "	IMPALA	Resolved	1	1	7280	broken-build, flaky
13052858	Llama expansion requests beyond NM node resources cause query to fail	When a query attempts to request resources from Llama that exceed the limits of the resources available to the NodeManager on that node, the query will fail, but for some expansion requests (e.g. CPU expansions and many memory expansions as well) the resources will  not be necessary for the completion of the query so the query should not fail.	IMPALA	Resolved	3	1	7280	llama, resource-management
13052798	MemTracker hierarchy may not correctly handle large RM limits	"The MemTracker RM limit might fail to be expanded when an expansion request comes back successfully and could potentially cause the query to fail.

Consider the following contrived example: The process mem limit is 10G and the child query tracker currently has an RM limit of 9.5G. If TryConsume is called, e.g. for a 2M buffer, we will send an expansion request for 2M but Llama will ""normalize"" the request and give us back the configured normalized size (rounding up) if it can, e.g. say 1G. Now we should be able to at least use 500M if we want within this query because it would still be less than the process 10G limit, but the code today will see that the current rm limit (9.5G) + the allocated size (1G) is above the 10G and then just say the consume failed. At this point the query may spill or may fail, but if it doesn't fail, it likely will ask for more memory again later (as it was already at the limit) and try to repeat the same process. Not only will we have repeatedly requested more memory from Llama than we can use or are using, but it's likely that eventually one of these calls to TryConsume() will result in the query failing. (It depends who called TryConsume and whether or not that memory was truly needed to continue executing.)"	IMPALA	Resolved	4	1	7280	llama, resource-management
13054474	KuduScanNode crash when pushing predicates including a cast	"After loading a few more functional tables for Kudu, a simple agg on alltypes crashed.

{code}
[localhost:21000] > use functional_kudu;
Query: use functional_kudu
[localhost:21000] > select count(*) from alltypes where year = 2009.0;
Error communicating with impalad: TSocket read 0 bytes
{code}

{code}
F0712 14:23:59.508661  5016 kudu-scan-node.cc:204] Check failed: predicate.nodes.size() == 3 (4 vs. 3) 
*** Check failure stack trace: ***
    @     0x7fb405dec19d  google::LogMessage::Fail()
    @     0x7fb405deeac6  google::LogMessage::SendToLog()
    @     0x7fb405debcbd  google::LogMessage::Flush()
    @     0x7fb405def56e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fb40a13b832  impala::KuduScanNode::TransformPushableConjunctsToRangePredicates()
    @     0x7fb40a13ad6f  impala::KuduScanNode::Open()
    @     0x7fb40a0eb7a7  impala::PartitionedAggregationNode::Open()
    @     0x7fb4081c3ec8  impala::PlanFragmentExecutor::OpenInternal()
    @     0x7fb4081c3bff  impala::PlanFragmentExecutor::Open()
    @     0x7fb407880f6a  impala::FragmentMgr::FragmentExecState::Exec()
    @     0x7fb40788e213  impala::FragmentMgr::FragmentThread()
    @     0x7fb4078a8d64  boost::_mfi::mf1<>::operator()()
    @     0x7fb4078a768d  boost::_bi::list2<>::operator()<>()
    @     0x7fb4078a6103  boost::_bi::bind_t<>::operator()()
    @     0x7fb4078a39a6  boost::detail::function::void_function_obj_invoker0<>::invoke()
    @           0x7d61be  boost::function0<>::operator()()
    @     0x7fb40675c9f9  impala::Thread::SuperviseThread()
    @     0x7fb406764480  boost::_bi::list4<>::operator()<>()
    @     0x7fb4067643c3  boost::_bi::bind_t<>::operator()()
    @     0x7fb40676431e  boost::detail::thread_data<>::run()
    @           0x8acd2a  thread_proxy
    @     0x7fb406270184  start_thread
    @     0x7fb402d7537d  (unknown)
Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30000,server=y,suspend=n 
Wrote minidump to /home/mj/dev/Impala/logs/cluster/minidumps/impalad/1511eaea-952d-efae-5ea04079-1217af5c.dmp
{code}

It looks like year gets cast to a double, but then we fail the DCHECK since the predicate isn't as simple as we expect.

For now we might need to avoid pushing such predicates down, but this kind of thing could lead to real perf issues."	IMPALA	Resolved	1	1	7280	kudu
13055190	InList predicates not being pushed to Kudu scans	Binary predicates are being properly pushed to Kudu scans, but InList predicates are not yet being added to the KuduScanTokenBuilder, and thus are not being pushed to the Kudu scan.	IMPALA	Resolved	3	1	7280	kudu
13053445	SEGV in AnalyticEvalNode touching NULL input_stream_	"A crash was reported in the following stack:

{code}
Stack: [0x00007fe1c7c8b000,0x00007fe1c848c000],  sp=0x00007fe1c8489bd0,  free space=8186k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [impalad+0x128fb26]  impala::BufferedTupleStream::rows_returned() const+0xc
C  [impalad+0x12bd3b3]  impala::AnalyticEvalNode::GetNext(impala::RuntimeState*, impala::RowBatch*, bool*)+0x821
C  [impalad+0x115f478]  impala::PlanFragmentExecutor::GetNextInternal(impala::RowBatch**)+0xec
C  [impalad+0x115dc92]  impala::PlanFragmentExecutor::OpenInternal()+0x272
C  [impalad+0x115d958]  impala::PlanFragmentExecutor::Open()+0x39e
C  [impalad+0xf30d88]  impala::FragmentMgr::FragmentExecState::Exec()+0x26
C  [impalad+0xf293a8]  impala::FragmentMgr::FragmentExecThread(impala::FragmentMgr::FragmentExecState*)+0x4c
{code}

The issue may have been introduced in a recent fix for:
IMPALA-2378: Part 2, IMPALA-2481: delete BufferedTupleStreams attached to batches
commit 916f3b29

I can reproduce this with the following query:
{code}
select max(t3.c1), max(t3.c2)
from (
  select
  avg( t1.timestamp_col )
    over (order by t1.id, t2.id rows between 5000 following and 50000 following) c1,
  avg( t2.timestamp_col )
    over (order by t1.id, t2.id rows between 5000 following and 50000 following) c2
  from alltypesagg t1 join alltypesagg t2 where t1.int_col = t2.int_col
) t3;
{code}

The issue has to do with allocated memory that gets passed to the output row batch. Normally memory gets allocated from a mempool and then transferred to the output row batch when it reaches 8mb. This may happen many times during the execution of the analytic node, and it works fine in the general case. However, when this transfer to the output row batch is supposed to happen at eos, we end up trying to do this transfer twice, which is where we end up touching a NULL pointer.


It shouldn't happen too frequently (none of our existing tests hit it), but it is unfortunately hard to predict when this will happen because it really depends on the query and the data.

There isn't an easy general workaround, but small changes that affect the cardinality of the data or the output tuple size of the analytic eval node may change when the data transfer happens and thus avoiding the crash."	IMPALA	Resolved	1	1	7280	crash
13055551	Support Kudu authentication	Parent task for Kudu authentication support	IMPALA	Resolved	1	2	7280	authentication, kudu
13055552	Force thrift to initialize SSL on process startup	Thrift currently initializes OpenSSL lazily, when the first connection is created. However, the OpenSSL initialization must happen before the Kudu library is used (because Kudu will not be doing the initialization itself, and it will check that the OpenSSL initialization already occurred), so thrift needs to do the initialization on startup.	IMPALA	Resolved	2	7	7280	kudu, ssl, thrift
13052718	BufferedBlockMgr DCHECK client->num_tmp_reserved_buffers_ == 0 failure w/ RM enabled	"On a 6 node cluster with RM (Yarn+Llama) enabled, running tpcds q46 results in a DCHECK failure. The query completes fine in the same cluster with RM disabled. (profile attached).

{code}
F0624 14:10:18.447474  2245 buffered-block-mgr.cc:259] Check failed: client->num_tmp_reserved_buffers_ == 0 (3 vs. 0) 
{code}

The core dump shows the following stack (missing symbols so not much else):
{code}

#6  0x0000000001fd3ecd in google::LogMessageFatal::~LogMessageFatal() ()
#7  0x00000000011d72da in impala::BufferedBlockMgr::TryAcquireTmpReservation(impala::BufferedBlockMgr::Client*, int) ()
#8  0x0000000001519597 in impala::BufferedTupleStream::PinStream(bool, bool*) ()
#9  0x000000000160f65b in impala::Status impala::PartitionedHashJoinNode::Partition::BuildHashTableInternal<false>(impala::RuntimeState*, bool*) ()
#10 0x0000000001601c92 in impala::PartitionedHashJoinNode::Partition::BuildHashTable(impala::RuntimeState*, bool*, bool) ()
#11 0x0000000001608a68 in impala::PartitionedHashJoinNode::BuildHashTables(impala::RuntimeState*) ()
#12 0x000000000160402f in impala::PartitionedHashJoinNode::ProcessBuildInput(impala::RuntimeState*, int) ()
#13 0x0000000001602cc6 in impala::PartitionedHashJoinNode::ConstructBuildSide(impala::RuntimeState*) ()
#14 0x00000000016412f5 in impala::BlockingJoinNode::BuildSideThread(impala::RuntimeState*, impala::Promise<impala::Status>*) ()
#15 0x0000000001642ff2 in boost::_mfi::mf2<void, impala::BlockingJoinNode, impala::RuntimeState*, impala::Promise<impala::Status>*>::operator()(impala::BlockingJoinNode*, impala::RuntimeState*, impala::Promise<impala::Status>*) const ()

{code}"	IMPALA	Resolved	3	1	7280	resource-management
13052838	Flaky Test: test_create_drop_data_src failed due to class-cache miss metric	"See http://sandbox.jenkins.cloudera.com/job/impala-s3/200/. 

{code}
metadata.test_ddl.TestDdlStatements.test_create_drop_data_src[exec_option: {'batch_size': 0, 'num_nodes': 0, 'sync_ddl': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none] (from pytest)

Failing for the past 1 build (Since Failed#200 )
Took 30 sec.
add description
Error Message

AssertionError: Metric value external-data-source.class-cache.misses did not reach value 4 in 10s
Stacktrace

metadata/test_ddl.py:256: in test_create_drop_data_src
    expected_cache_misses)
common/impala_service.py:82: in wait_for_metric_value
    (metric_name, expected_value, timeout)
E   AssertionError: Metric value external-data-source.class-cache.misses did not reach value 4 in 10s
Standard Error

-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

MainThread: Starting new HTTP connection (1): 0.0.0.0
MainThread: Starting new HTTP connection (1): 0.0.0.0
MainThread: Starting new HTTP connection (1): 0.0.0.0
MainThread: Found 3 impalad/1 statestored/1 catalogd process(es)
-- executing against localhost:21000
use default;

SET sync_ddl=1;
-- executing against localhost:21000
show databases;

-- connecting to: localhost:21000
SET sync_ddl=1;
-- executing against localhost:21000
create database data_src_test location 's3a://ishaan-impala/test-warehouse/data_src_test.db';

-- closing connection to: localhost:21000
SET batch_size=0;
SET num_nodes=1;
SET sync_ddl=0;
SET disable_codegen=False;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
-- executing against localhost:21000
use data_src_test;

-- executing against localhost:21000
drop table if exists data_src_tbl;

-- executing against localhost:21000
drop data source if exists test_data_src;

-- executing against localhost:21000
CREATE DATA SOURCE test_data_src LOCATION 's3a://ishaan-impala/test-warehouse/data-sources/test-data-source.jar' CLASS 'com.cloudera.impala.extdatasource.AllTypesDataSource' API_VERSION 'V1';

-- executing against localhost:21000
CREATE TABLE data_src_tbl (x int)
        PRODUCED BY DATA SOURCE test_data_src('dummy_init_string');

-- executing against localhost:21000
select * from data_src_tbl limit 1;

-- executing against localhost:21000
drop table  data_src_tbl;

-- executing against localhost:21000
drop data source  test_data_src;

-- executing against localhost:21000
CREATE DATA SOURCE test_data_src LOCATION 's3a://ishaan-impala/test-warehouse/data-sources/test-data-source.jar' CLASS 'com.cloudera.impala.extdatasource.AllTypesDataSource' API_VERSION 'V1';

-- executing against localhost:21000
CREATE TABLE data_src_tbl (x int)
        PRODUCED BY DATA SOURCE test_data_src('dummy_init_string');

-- executing against localhost:21000
select * from data_src_tbl limit 1;

-- executing against localhost:21000
drop table  data_src_tbl;

-- executing against localhost:21000
drop data source  test_data_src;

MainThread: Getting metric: external-data-source.class-cache.hits from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Metric 'external-data-source.class-cache.hits' has reach desired value: 0
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
MainThread: Getting metric: external-data-source.class-cache.misses from impala-boost-static-burst-slave-1a0d.vpc.cloudera.com:25001
MainThread: Waiting for metric value 'external-data-source.class-cache.misses'=4. Current value: 0
MainThread: Sleeping 1s before next retry.
{code}"	IMPALA	Resolved	1	1	7280	broken-build, test-infra
13055448	"Change default Kudu read behavior for ""RYW"""	"For 2.9 we want to change the default Kudu read behavior mode, which was previously exposed via an Impala gflag.

Currently the default read mode is set to ""READ_LATEST"", which essentially provides no guarantees on reading except that any read issued will read the latest value that the target replica happens to have. This is not necessarily a time _after_ a previous write operation in the same session. By changing the read mode to the misleadingly named ""READ_AT_SNAPSHOT"", we can ensure that Kudu reads will all be at times at least or greater than the latest ""observed"" time (which Impala already sets on the client). Note that this does not mean all reads are performed at the same timestamp (i.e. a snapshot read) because that requires setting a snapshot timestamp, but doing this will require more work in the future in both Impala (IMPALA-4685) and Kudu (which needs to make some client changes and also fix how they GC historical values).

This means that, after this change, values written within a session will always be visible to subsequent reads. Before this change, this was usually the case but not guaranteed. The Kudu team calls this ""Read Your Writes""."	IMPALA	Resolved	2	3	7280	kudu
13054195	Add option to strictly handle numeric overflow in text parsing	Add an option to more strictly handle integer and floating point overflow.	IMPALA	Resolved	3	4	7280	usability
13053189	Separate admission control config from YARN/Llama configs	"Today a single pair of fair-scheduler.xml and llama-site.xml files are used to configure both admission control and Impala on Llama/YARN, which assumes the user wants the same queue/pool configuration. We need to relax this requirement in order to express things like a very simple policy for Impala on YARN w/ fine grained pools in Impala admission control.

Additionally, we can get out of the business of trying to find ways to fit admission control settings into the fair-scheduler.xml and llama-site.xml configuration formats which don't necessarily align with the features we want (e.g. IMPALA-2538)."	IMPALA	Resolved	2	1	7280	admission-control, resource-management
13052179	DCHECK failure: FE produces tuple with decimal slot but agg fn returns double	"DCHECK failure in AggFnEvaluator. Intermediate tuple has a DECIMAL slot for the result of an agg fn which returns a double.

QUERY:
{code}
SELECT
FIRST_VALUE(-32.9) OVER (ORDER BY month ROWS BETWEEN 92 PRECEDING AND UNBOUNDED FOLLOWING)
FROM alltypestiny
{code}

STACK:
{code}
raise () from /lib64/libc.so.6
raise () from /lib64/libc.so.6
abort () from /lib64/libc.so.6
google::DumpStackTraceAndExit () at src/utilities.cc:147
google::LogMessage::Fail () at src/logging.cc:1296
google::LogMessage::SendToLog (this=0x7f6dd18ffde0) at src/logging.cc:1250
google::LogMessage::Flush (this=0x7f6dd18ffde0) at src/logging.cc:1119
google::LogMessageFatal::~LogMessageFatal (this=0x7f6dd18ffde0, __in_chrg=<value optimized out>) at src/logging.cc:1817
impala::AggFnEvaluator::Prepare (this=0x7b814a0, state=0x82e8000, desc=..., intermediate_slot_desc=0x4f7a190, output_slot_desc=0x4f7a190, agg_fn_pool=0x5dbd860, agg_fn_ctx=0x7f6dd1900078) at /data/9/query-gen/Impala/be/src/exprs/agg-fn-evaluator.cc:128
impala::AnalyticEvalNode::Prepare (this=0x6977180, state=0x82e8000) at /data/9/query-gen/Impala/be/src/exec/analytic-eval-node.cc:137
impala::PlanFragmentExecutor::Prepare (this=0x4e4cf00, request=...) at /data/9/query-gen/Impala/be/src/runtime/plan-fragment-executor.cc:234
impala::Coordinator::Exec (this=0x6495000, schedule=..., output_expr_ctxs=0xabbab60) at /data/9/query-gen/Impala/be/src/runtime/coordinator.cc:342
impala::ImpalaServer::QueryExecState::ExecQueryOrDmlRequest (this=0xabba000, query_exec_request=...) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:403
impala::ImpalaServer::QueryExecState::Exec (this=0xabba000, exec_request=0x7f6dd19028f0) at /data/9/query-gen/Impala/be/src/service/query-exec-state.cc:138
impala::ImpalaServer::ExecuteInternal (this=0x52d2580, query_ctx=..., session_state=..., registered_exec_state=0x7f6dd1903b67, exec_state=0x7f6dd1903df0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:607
impala::ImpalaServer::Execute (this=0x52d2580, query_ctx=0x7f6dd1903c20, session_state=..., exec_state=0x7f6dd1903df0) at /data/9/query-gen/Impala/be/src/service/impala-server.cc:550
impala::ImpalaServer::ExecuteStatement (this=0x52d2580, return_val=..., request=...) at /data/9/query-gen/Impala/be/src/service/impala-hs2-server.cc:709
apache::hive::service::cli::thrift::TCLIServiceProcessor::process_ExecuteStatement (this=0x37e50e0, seqid=0, iprot=0x598c800, oprot=0x5934300, callContext=0x598c900) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4695
apache::hive::service::cli::thrift::TCLIServiceProcessor::dispatchCall (this=0x37e50e0, iprot=0x598c800, oprot=0x5934300, fname=""ExecuteStatement"", seqid=0, callContext=0x598c900) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/TCLIService.cpp:4506
impala::ImpalaHiveServer2ServiceProcessor::dispatchCall (this=0x37e50e0, iprot=0x598c800, oprot=0x5934300, fname=""ExecuteStatement"", seqid=0, callContext=0x598c900) at /data/9/query-gen/Impala/be/generated-sources/gen-cpp/ImpalaHiveServer2Service.cpp:463
apache::thrift::TDispatchProcessor::process (this=0x37e50e0, in=..., out=..., connectionContext=0x598c900) at /data/9/query-gen/Impala/thirdparty/thrift-0.9.0/build/include/thrift/TDispatchProcessor.h:121
apache::thrift::server::TThreadPoolServer::Task::run (this=0x7e6a180) at src/thrift/server/TThreadPoolServer.cpp:70
apache::thrift::concurrency::ThreadManager::Task::run (this=0x598c880) at src/thrift/concurrency/ThreadManager.cpp:187
apache::thrift::concurrency::ThreadManager::Worker::run (this=0x5fedef0) at src/thrift/concurrency/ThreadManager.cpp:316
impala::ThriftThread::RunRunnable (this=0x5b0e7c0, runnable=..., promise=0x7fffdf865f10) at /data/9/query-gen/Impala/be/src/rpc/thrift-thread.cc:61
boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x65f3230, p=0x5b0e7c0, a1=..., a2=0x7fffdf865f10) at /usr/include/boost/bind/mem_fn_template.hpp:280
boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x65f3240, f=..., a=...) at /usr/include/boost/bind/bind.hpp:392
boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x65f3230) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
boost::function0<void>::operator() (this=0x7f6dd1904cd0) at /usr/include/boost/function/function_template.hpp:1013
impala::Thread::SuperviseThread (name=""hiveserver2-frontend-5"", category=""thrift-server"", functor=..., thread_started=0x7fffdf865d10) at /data/9/query-gen/Impala/be/src/util/thread.cc:311
boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::string &, const std::string &, impala::Thread::ThreadFunctor, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x6940910, f=@0x6940908, a=...) at /usr/include/boost/bind/bind.hpp:457
boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x6940908) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::string&, const std::string&, impala::Thread::ThreadFunctor, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x6940780) at /usr/include/boost/thread/detail/thread.hpp:61
thread_proxy ()
start_thread () from /lib64/libpthread.so.0
clone () from /lib64/libc.so.6
{code}

DB: Functional
File Format: Text/None
git Hash: a95e7ec"	IMPALA	Resolved	1	1	7280	correctness, query_generator
13054102	Add tests for S3<->HDFS multiple filesystem use cases	"As discussed, let's add coverage for use cases involving both HDFS and S3, and which runs only when S3 is enabled (i.e. S3 is the FS under test).

For example:

* INSERT to table with partitions on HDFS and S3
* LOAD DATA from S3 to HDFS and back again"	IMPALA	Resolved	3	4	8925	s3
13053865	Failed to mkdirs on core-local-filesystem build.	"Sailesh, I believe this is due to a recent change you made, hence assigning to you.

See:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-core-local-filesystem/130/

From the console:
{code}
20:40:01 /data/jenkins/workspace/impala-cdh5-trunk-core-local-filesystem/repos/Impala/testdata/target
20:40:03 SUCCESS, data generated into /data/jenkins/workspace/impala-cdh5-trunk-core-local-filesystem/repos/Impala/testdata/target
20:40:03 Loading HDFS data from snapshot: /data/jenkins/workspace/impala-cdh5-trunk-core-local-filesystem/testdata/test-warehouse-SNAPSHOT/test-warehouse-cdh5-869-SNAPSHOT.tar.gz (logging to load-test-warehouse-snapshot.log)... OK
20:47:10 Starting Impala cluster (logging to start-impala-cluster.log)... OK
20:47:20 Loading Hive UDFs (logging to build-and-copy-hive-udfs.log)... OK
20:47:56 Running custom post-load steps (logging to custom-post-load-steps.log)... FAILED
20:48:06 'custom-post-load-steps' failed. Tail of log:
20:48:06 Log for command 'custom-post-load-steps'
20:48:06 mkdir: `/test-warehouse/lineitem_sixblocks_parquet': Input/output error
20:48:06 mkdir: `/test-warehouse/lineitem_multiblock_one_row_group_parquet': Input/output error
20:48:06 Error in /data/jenkins/workspace/impala-cdh5-trunk-core-local-filesystem/repos/Impala/testdata/bin/create-load-data.sh at line 41: while [ -n ""$*"" ]
20:48:06 Error in /data/jenkins/workspace/impala-cdh5-trunk-core-local-filesystem/repos/Impala/buildall.sh at line 368: ${IMPALA_HOME}/testdata/bin/create-load-data.sh ${CREATE_LOAD_DATA_ARGS} <<< Y
{code}"	IMPALA	Resolved	1	1	8925	broken-build
13055223	Hung query for several hours during end-to-end testing on ubuntu1404 cluster	"From the impalad log, the last entry is:
{noformat}
09:51:25.582063 25017 coordinator.cc:1410] CancelFragmentInstances() query_id=db409a1bfe8222f4:6afdcdc400000000, tried to cancel 0 fragment instances
{noformat}

The time stamp corresponds to the last time stamp in the jenkins log where this test was being run:
{noformat}
09:51:25 [gw3] PASSED query_test/test_tpch_queries.py::TestTpchQuery::test_tpch[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none-TPC-H: Q18]
{noformat}

From the Query Details page on the CM host, the actual query is:
{noformat}
select id, cnt
from functional_parquet.complextypestbl t,
  (select count(item) cnt from t.int_array) v
order by id
limit 10
{noformat}

Attaching impalads log, stack traces, and minidump."	IMPALA	Resolved	1	1	8925	crash
13122650	KerberosOnAndOff/RpcMgrKerberizedTest.MultipleServices failing	We're seeing failures of {{KerberosOnAndOff/RpcMgrKerberizedTest.MultipleServices/1 (from KerberosOnAndOff_RpcMgrKerberizedTest)}} in a variety of test configurations. Commit {{IMPALA-5053: [SECURITY] Make KRPC work with Kerberos}}  is the last commit where these things happen.	IMPALA	Resolved	2	7	8925	broken-build
13112831	ASAN detects heap-use-after-free in thrift-server-test	"ASAN detected a heap-use-after-free in thrift-server-test in a private build.

[~sailesh] - You made changes to this test in this change: https://gerrit.cloudera.org/#/c/7938/

Can you please have a look?

Please reach out in person if you would like to access the artifacts of the private build.

{noformat}
21:53:06 =================================================================
21:53:06 ==28490==ERROR: AddressSanitizer: heap-use-after-free on address 0x60c000013318 at pc 0x00000129c04c bp 0x7fd43db71200 sp 0x7fd43db709b0
21:53:06 READ of size 103 at 0x60c000013318 thread T62
21:53:06     #0 0x129c04b in strlen /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/llvm/llvm-3.9.1.src/projects/compiler-rt/lib/asan/../sanitizer_common/sanitizer_common_interceptors.inc:227
21:53:06     #1 0x337cc0b0a0 in _sasl_strdup (/usr/lib64/libsasl2.so.2+0x337cc0b0a0)
21:53:06     #2 0x337cc1135c in sasl_server_new (/usr/lib64/libsasl2.so.2+0x337cc1135c)
21:53:06     #3 0x19b07b5 in sasl::TSaslServer::setupSaslContext() /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/transport/TSasl.cpp:214:16
21:53:06     #4 0x19b1fb7 in apache::thrift::transport::TSaslServerTransport::handleSaslStartMessage() /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/transport/TSaslServerTransport.cpp:124:10
21:53:06     #5 0x19b8299 in apache::thrift::transport::TSaslTransport::doSaslNegotiation() /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/transport/TSaslTransport.cpp:81:7
21:53:06     #6 0x19b273a in apache::thrift::transport::TSaslServerTransport::Factory::getTransport(boost::shared_ptr<apache::thrift::transport::TTransport>) /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/transport/TSaslServerTransport.cpp:174:24
21:53:06     #7 0x163ba86 in apache::thrift::server::TAcceptQueueServer::SetupConnection(boost::shared_ptr<apache::thrift::transport::TTransport>) /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/rpc/TAcceptQueueServer.cpp:146:46
21:53:06     #8 0x163dc32 in apache::thrift::server::TAcceptQueueServer::serve()::$_0::operator()(int, boost::shared_ptr<apache::thrift::transport::TTransport> const&) const /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/rpc/TAcceptQueueServer.cpp:220:15
21:53:06     #9 0x1645b43 in boost::function2<void, int, boost::shared_ptr<apache::thrift::transport::TTransport> const&>::operator()(int, boost::shared_ptr<apache::thrift::transport::TTransport> const&) const /data/jenkins/workspace/impala-asf-master-core-asan/Impala-Toolchain/boost-1.57.0-p3/include/boost/function/function_template.hpp:766:14
21:53:06     #10 0x1644ba5 in impala::ThreadPool<boost::shared_ptr<apache::thrift::transport::TTransport> >::WorkerThread(int) /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/util/thread-pool.h:152:9
21:53:06     #11 0x1645141 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::ThreadPool<boost::shared_ptr<apache::thrift::transport::TTransport> >, int>, boost::_bi::list2<boost::_bi::value<impala::ThreadPool<boost::shared_ptr<apache::thrift::transport::TTransport> >*>, boost::_bi::value<int> > >::operator()() /data/jenkins/workspace/impala-asf-master-core-asan/Impala-Toolchain/boost-1.57.0-p3/include/boost/bind/bind_template.hpp:20:16
21:53:06     #12 0x15e2622 in boost::function0<void>::operator()() const /data/jenkins/workspace/impala-asf-master-core-asan/Impala-Toolchain/boost-1.57.0-p3/include/boost/function/function_template.hpp:766:14
21:53:06     #13 0x1a89e47 in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/util/thread.cc:352:3
21:53:06     #14 0x1a94bb5 in void boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) /data/jenkins/workspace/impala-asf-master-core-asan/Impala-Toolchain/boost-1.57.0-p3/include/boost/bind/bind.hpp:457:9
21:53:06     #15 0x1a94a31 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() /data/jenkins/workspace/impala-asf-master-core-asan/Impala-Toolchain/boost-1.57.0-p3/include/boost/bind/bind_template.hpp:20:16
21:53:06     #16 0x2407ba9 in thread_proxy (/data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/build/debug/rpc/thrift-server-test+0x2407ba9)
21:53:06     #17 0x3379c07850 in start_thread (/lib64/libpthread.so.0+0x3379c07850)
21:53:06     #18 0x33798e894c in clone (/lib64/libc.so.6+0x33798e894c)
21:53:06 
21:53:06 0x60c000013318 is located 24 bytes inside of 127-byte region [0x60c000013300,0x60c00001337f)
21:53:06 freed by thread T0 here:
21:53:06     #0 0x13459a0 in operator delete(void*) /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/llvm/llvm-3.9.1.src/projects/compiler-rt/lib/asan/asan_new_delete.cc:110
21:53:06     #1 0x135f4d0 in ThriftParamsTest::SetUp() /data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/src/rpc/thrift-server-test.cc:141:3
21:53:06     #2 0x331fec2 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/build/debug/rpc/thrift-server-test+0x331fec2)
21:53:06 
21:53:06 previously allocated by thread T0 here:
21:53:06     #0 0x1345320 in operator new(unsigned long) /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/llvm/llvm-3.9.1.src/projects/compiler-rt/lib/asan/asan_new_delete.cc:78
21:53:06     #1 0x7fd4532e4c48 in __gnu_cxx::new_allocator<char>::allocate(unsigned long, void const*) /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/gcc/build-4.9.2/x86_64-unknown-linux-gnu/libstdc++-v3/include/ext/new_allocator.h:104
21:53:06     #2 0x7fd4532e4c48 in std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&) /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/gcc/build-4.9.2/x86_64-unknown-linux-gnu/libstdc++-v3/include/bits/basic_string.tcc:607
21:53:06 
21:53:06 Thread T62 created by T61 here:
21:53:06     #0 0x12679ed in __interceptor_pthread_create /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/llvm/llvm-3.9.1.src/projects/compiler-rt/lib/asan/asan_interceptors.cc:245
21:53:06     #1 0x2406f89 in boost::thread::start_thread_noexcept() (/data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/build/debug/rpc/thrift-server-test+0x2406f89)
21:53:06 
21:53:06 Thread T61 created by T0 here:
21:53:06     #0 0x12679ed in __interceptor_pthread_create /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/llvm/llvm-3.9.1.src/projects/compiler-rt/lib/asan/asan_interceptors.cc:245
21:53:06     #1 0x2406f89 in boost::thread::start_thread_noexcept() (/data/jenkins/workspace/impala-asf-master-core-asan/repos/Impala/be/build/debug/rpc/thrift-server-test+0x2406f89)
21:53:06 
21:53:06 SUMMARY: AddressSanitizer: heap-use-after-free /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-centos-6/toolchain/source/llvm/llvm-3.9.1.src/projects/compiler-rt/lib/asan/../sanitizer_common/sanitizer_common_interceptors.inc:227 in strlen
21:53:06 Shadow bytes around the buggy address:
21:53:06   0x0c187fffa610: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 00 00
21:53:06   0x0c187fffa620: 00 00 00 00 00 00 00 fa fa fa fa fa fa fa fa fa
21:53:06   0x0c187fffa630: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fa
21:53:06   0x0c187fffa640: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
21:53:06   0x0c187fffa650: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa
21:53:06 =>0x0c187fffa660: fd fd fd[fd]fd fd fd fd fd fd fd fd fd fd fd fd
21:53:06   0x0c187fffa670: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
21:53:06   0x0c187fffa680: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa
21:53:06   0x0c187fffa690: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
21:53:06   0x0c187fffa6a0: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 00 00
21:53:06   0x0c187fffa6b0: 00 00 00 00 00 00 00 fa fa fa fa fa fa fa fa fa
21:53:06 Shadow byte legend (one shadow byte represents 8 application bytes):
21:53:06   Addressable:           00
21:53:06   Partially addressable: 01 02 03 04 05 06 07 
21:53:06   Heap left redzone:       fa
21:53:06   Heap right redzone:      fb
21:53:06   Freed heap region:       fd
21:53:06   Stack left redzone:      f1
21:53:06   Stack mid redzone:       f2
21:53:06   Stack right redzone:     f3
21:53:06   Stack partial redzone:   f4
21:53:06   Stack after return:      f5
21:53:06   Stack use after scope:   f8
21:53:06   Global redzone:          f9
21:53:06   Global init order:       f6
21:53:06   Poisoned by user:        f7
21:53:06   Container overflow:      fc
21:53:06   Array cookie:            ac
21:53:06   Intra object redzone:    bb
21:53:06   ASan internal:           fe
21:53:06   Left alloca redzone:     ca
21:53:06   Right alloca redzone:    cb
21:53:06 ==28490==ABORTING
{noformat}"	IMPALA	Resolved	1	1	8925	broken-build
13053922	Add command-line flags to set S3 access configurations	"It's annoying that the only way to manage S3 credentials right now is to put them in plain-text in {{core-site.xml}}. Instead, we should add command line flags to set {{fs.s3a.access.key}} and {{fs.s3a.secret.key}} directly, using {{hdfsBuilderConfSetStr()}}.

The secret key at least should be expressed as a command to run which will yield the secret key (just like {{--pem_password_cmd}})."	IMPALA	Resolved	3	4	8925	s3
13054088	hdfs-util-test failing on local FS	"http://sandbox.jenkins.cloudera.com/job/impala-cdh5-trunk-core-local-filesystem/36/

I'm guessing this is related to the S3 change, so assigning for initial investigation.

{code}
I0503 08:23:43.183194 22248 simple-scheduler.cc:218] Simple-scheduler using 127.0.0.1 as IP address
I0503 08:23:43.218618 22248 statestore-subscriber.cc:179] Starting statestore subscriber
E0503 08:23:43.221992 22357 thrift-server.cc:176] ThriftServer 'StatestoreSubscriber' (on port: 23000) exited due to TException: Could not bind: Transport endpoint is not connected
E0503 08:23:43.222328 22248 thrift-server.cc:165] ThriftServer 'StatestoreSubscriber' (on port: 23000) did not start correctly 
{code}"	IMPALA	Resolved	1	1	8925	broken-build
13053709	ReopenClient() could NULL out 'client_key' causing a crash	"While running the stress tests with a custom patch for IMPALA-2592, I'm hitting a crash in DoRpc() with the following stack:

{code:java}
Stack: [0x00007fab64253000,0x00007fab64c54000],  sp=0x00007fab64c51f90,  free space=10235k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [impalad+0x10163c4]  impala::Status impala::ClientConnection<impala::ImpalaInternalServiceClient>::DoRpc<void (impala::ImpalaInternalServiceClient::*)(impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams, impala::TReportExecStatusResult>(void (impala::ImpalaInternalServiceClient::* const&)(impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams const&, impala::TReportExecStatusResult*)+0x108
C  [impalad+0x1015596]  impala::FragmentMgr::FragmentExecState::ReportStatusCb(impala::Status const&, impala::RuntimeProfile*, bool)+0x598
C  [impalad+0x100fe97]  boost::_mfi::mf3<void, impala::FragmentMgr::FragmentExecState, impala::Status const&, impala::RuntimeProfile*, bool>::operator()(impala::FragmentMgr::FragmentExecState*, impala::Status const&, impala::RuntimeProfile*, bool) const+0x7d
C  [impalad+0x100f65a]  void boost::_bi::list4<boost::_bi::value<impala::FragmentMgr::FragmentExecState*>, boost::arg<1>, boost::arg<2>, boost::arg<3> >::operator()<boost::_mfi::mf3<void, impala::FragmentMgr::FragmentExecState, impala::Status const&, impala::RuntimeProfile*, bool>, boost::_bi::list3<impala::Status const&, impala::RuntimeProfile*&, bool&> >(boost::_bi::type<void>, boost::_mfi::mf3<void, impala::FragmentMgr::FragmentExecState, impala::Status const&, impala::RuntimeProfile*, bool>&, boost::_bi::list3<impala::Status const&, impala::RuntimeProfile*&, bool&>&, int)+0xa8
C  [impalad+0x100efed]  void boost::_bi::bind_t<void, boost::_mfi::mf3<void, impala::FragmentMgr::FragmentExecState, impala::Status const&, impala::RuntimeProfile*, bool>, boost::_bi::list4<boost::_bi::value<impala::FragmentMgr::FragmentExecState*>, boost::arg<1>, boost::arg<2>, boost::arg<3> > >::operator()<impala::Status const, impala::RuntimeProfile*, bool>(impala::Status const&, impala::RuntimeProfile*&, bool&)+0x53
C  [impalad+0x100eacd]  boost::detail::function::void_function_obj_invoker3<boost::_bi::bind_t<void, boost::_mfi::mf3<void, impala::FragmentMgr::FragmentExecState, impala::Status const&, impala::RuntimeProfile*, bool>, boost::_bi::list4<boost::_bi::value<impala::FragmentMgr::FragmentExecState*>, boost::arg<1>, boost::arg<2>, boost::arg<3> > >, void, impala::Status const&, impala::RuntimeProfile*, bool>::invoke(boost::detail::function::function_buffer&, impala::Status const&, impala::RuntimeProfile*, bool)+0x39
C  [impalad+0x13fa176]  boost::function3<void, impala::Status const&, impala::RuntimeProfile*, bool>::operator()(impala::Status const&, impala::RuntimeProfile*, bool) const+0x68
C  [impalad+0x13f7e55]  impala::PlanFragmentExecutor::SendReport(bool)+0x10b
C  [impalad+0x13f7aef]  impala::PlanFragmentExecutor::ReportProfile()+0x6bf
C  [impalad+0x13fbc4b]  boost::_mfi::mf0<void, impala::PlanFragmentExecutor>::operator()(impala::PlanFragmentExecutor*) const+0x65
C  [impalad+0x13fb992]  void boost::_bi::list1<boost::_bi::value<impala::PlanFragmentExecutor*> >::operator()<boost::_mfi::mf0<void, impala::PlanFragmentExecutor>, boost::_bi::list0>(boost::_bi::type<void>, boost::_mfi::mf0<void, impala::PlanFragmentExecutor>&, boost::_bi::list0&, int)+0x4a
C  [impalad+0x13fb5f7]  boost::_bi::bind_t<void, boost::_mfi::mf0<void, impala::PlanFragmentExecutor>, boost::_bi::list1<boost::_bi::value<impala::PlanFragmentExecutor*> > >::operator()()+0x3b
C  [impalad+0x13fb3bc]  boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf0<void, impala::PlanFragmentExecutor>, boost::_bi::list1<boost::_bi::value<impala::PlanFragmentExecutor*> > >, void>::invoke(boost::detail::function::function_buffer&)+0x20
C  [impalad+0xe1fc76]  boost::function0<void>::operator()() const+0x52
C  [impalad+0x10cd6b9]  impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*)+0x2c5
C  [impalad+0x10d4d34]  void boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list0&, int)+0xb2
C  [impalad+0x10d4c77]  boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()()+0x3b
C  [impalad+0x10d4c3a]  boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*
> > > >::run()+0x1e
{code}

Looking at the disassembly, it fails here:

{code:java}
Dump of assembler code for function impala::ClientConnection<impala::ImpalaInternalServiceClient>::DoRpc<void (impala::ImpalaInternalServiceClient::*)(impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams, impala::TReportExecStatusResult>(void (impala::ImpalaInternalServiceClient::*&)(impala::ImpalaInternalServiceClient*, impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams const&, impala::TReportExecStatusResult*):
   0x00000000014162bc <+0>:	push   %rbp
   0x00000000014162bd <+1>:	mov    %rsp,%rbp
   0x00000000014162c0 <+4>:	push   %r13
   0x00000000014162c2 <+6>:	push   %r12
   0x00000000014162c4 <+8>:	push   %rbx
   0x00000000014162c5 <+9>:	sub    $0xf8,%rsp
   0x00000000014162cc <+16>:	mov    %rdi,-0xe8(%rbp)
   0x00000000014162d3 <+23>:	mov    %rsi,-0xf0(%rbp)
   0x00000000014162da <+30>:	mov    %rdx,-0xf8(%rbp)
   0x00000000014162e1 <+37>:	mov    %rcx,-0x100(%rbp)
   0x00000000014162e8 <+44>:	mov    %r8,-0x108(%rbp)
   0x00000000014162ef <+51>:	cmpq   $0x0,-0x108(%rbp)
   0x00000000014162f7 <+59>:	sete   %al
   0x00000000014162fa <+62>:	movzbl %al,%eax
   0x00000000014162fd <+65>:	mov    $0x0,%ebx
   0x0000000001416302 <+70>:	mov    $0x0,%r12d
   0x0000000001416308 <+76>:	test   %rax,%rax
   0x000000000141630b <+79>:	je     0x1416375 <impala::ClientConnection<impala::ImpalaInternalServiceClient>::DoRpc<void (impala::ImpalaInternalServiceClient::*)(impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams, impala::TReportExecStatusResult>(void (impala::ImpalaInternalServiceClient::*&)(impala::ImpalaInternalServiceClient*, impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams const&, impala::TReportExecStatusResult*)+185>
   0x000000000141630d <+81>:	lea    -0xe0(%rbp),%rax
   0x0000000001416314 <+88>:	mov    $0xe3,%edx
   0x0000000001416319 <+93>:	lea    0xf36628(%rip),%rsi        # 0x234c948
   0x0000000001416320 <+100>:	mov    %rax,%rdi
   0x0000000001416323 <+103>:	callq  0x223bce0 <_ZN6google15LogMessageFatalC2EPKci>
   0x0000000001416328 <+108>:	mov    $0x1,%ebx
   0x000000000141632d <+113>:	lea    -0xe0(%rbp),%rax
   0x0000000001416334 <+120>:	mov    %rax,%rdi
   0x0000000001416337 <+123>:	callq  0x106eab6 <google::LogMessage::stream()>
   0x000000000141633c <+128>:	lea    0xf36695(%rip),%rsi        # 0x234c9d8
   0x0000000001416343 <+135>:	mov    %rax,%rdi
   0x0000000001416346 <+138>:	callq  0x1018790 <_ZStlsISt11char_traitsIcEERSt13basic_ostreamIcT_ES5_PKc@plt>
   0x000000000141634b <+143>:	mov    %rax,%r13
   0x000000000141634e <+146>:	lea    -0xc1(%rbp),%rax
   0x0000000001416355 <+153>:	mov    %rax,%rdi
   0x0000000001416358 <+156>:	callq  0x106eacc <google::LogMessageVoidify::LogMessageVoidify()>
   0x000000000141635d <+161>:	mov    $0x1,%r12d
   0x0000000001416363 <+167>:	lea    -0xc1(%rbp),%rax
   0x000000000141636a <+174>:	mov    %r13,%rsi
   0x000000000141636d <+177>:	mov    %rax,%rdi
   0x0000000001416370 <+180>:	callq  0x106ead6 <google::LogMessageVoidify::operator&(std::ostream&)>
   0x0000000001416375 <+185>:	test   %r12b,%r12b
   0x0000000001416378 <+188>:	test   %bl,%bl
   0x000000000141637a <+190>:	je     0x141638c <impala::ClientConnection<impala::ImpalaInternalServiceClient>::DoRpc<void (impala::ImpalaInternalServiceClient::*)(impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams, impala::TReportExecStatusResult>(void (impala::ImpalaInternalServiceClient::*&)(impala::ImpalaInternalServiceClient*, impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams const&, impala::TReportExecStatusResult*)+208>
   0x000000000141637c <+192>:	nop
   0x000000000141637d <+193>:	lea    -0xe0(%rbp),%rax
---Type <return> to continue, or q <return> to quit---
   0x0000000001416384 <+200>:	mov    %rax,%rdi
   0x0000000001416387 <+203>:	callq  0x223bd00 <_ZN6google15LogMessageFatalD2Ev>
   0x000000000141638c <+208>:	nop
   0x000000000141638d <+209>:	mov    -0xf8(%rbp),%rax
   0x0000000001416394 <+216>:	mov    (%rax),%rax
   0x0000000001416397 <+219>:	and    $0x1,%eax
   0x000000000141639a <+222>:	test   %rax,%rax
   0x000000000141639d <+225>:	jne    0x14163ab <impala::ClientConnection<impala::ImpalaInternalServiceClient>::DoRpc<void (impala::ImpalaInternalServiceClient::*)(impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams, impala::TReportExecStatusResult>(void (impala::ImpalaInternalServiceClient::*&)(impala::ImpalaInternalServiceClient*, impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams const&, impala::TReportExecStatusResult*)+239>
   0x000000000141639f <+227>:	mov    -0xf8(%rbp),%rax
   0x00000000014163a6 <+234>:	mov    (%rax),%rax
   0x00000000014163a9 <+237>:	jmp    0x14163db <impala::ClientConnection<impala::ImpalaInternalServiceClient>::DoRpc<void (impala::ImpalaInternalServiceClient::*)(impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams, impala::TReportExecStatusResult>(void (impala::ImpalaInternalServiceClient::*&)(impala::ImpalaInternalServiceClient*, impala::TReportExecStatusResult&, impala::TReportExecStatusParams const&), impala::TReportExecStatusParams const&, impala::TReportExecStatusResult*)+287>
   0x00000000014163ab <+239>:	mov    -0xf0(%rbp),%rax
   0x00000000014163b2 <+246>:	mov    0x8(%rax),%rdx
   0x00000000014163b6 <+250>:	mov    -0xf8(%rbp),%rax
   0x00000000014163bd <+257>:	mov    0x8(%rax),%rax
   0x00000000014163c1 <+261>:	add    %rdx,%rax
=> 0x00000000014163c4 <+264>:	mov    (%rax),%rdx
   0x00000000014163c7 <+267>:	mov    -0xf8(%rbp),%rax
   0x00000000014163ce <+274>:	mov    (%rax),%rax
   0x00000000014163d1 <+277>:	sub    $0x1,%rax
   0x00000000014163d5 <+281>:	add    %rdx,%rax
   0x00000000014163d8 <+284>:	mov    (%rax),%rax
   0x00000000014163db <+287>:	mov    -0xf0(%rbp),%rdx
   0x00000000014163e2 <+294>:	mov    0x8(%rdx),%rcx
   0x00000000014163e6 <+298>:	mov    -0xf8(%rbp),%rdx
   0x00000000014163ed <+305>:	mov    0x8(%rdx),%rdx
   0x00000000014163f1 <+309>:	lea    (%rcx,%rdx,1),%rdi
   0x00000000014163f5 <+313>:	mov    -0x100(%rbp),%rdx
   0x00000000014163fc <+320>:	mov    -0x108(%rbp),%rcx
   0x0000000001416403 <+327>:	mov    %rcx,%rsi
   0x0000000001416406 <+330>:	callq  *%rax
   0x0000000001416408 <+332>:	mov    -0xe8(%rbp),%rax
   0x000000000141640f <+339>:	mov    %rax,%rdi
   0x0000000001416412 <+342>:	callq  0x108027f <impala::Status::OK()>
{code}

At that point, the 'client_' (from the class ClientConnection) should be in $rdx, but $rdx is NULL causing the crash. This is odd and there isn't a reasonable explanation as to why it happens as of yet.

The crash does not occur immediately, it happens only after remote nodes become unreachable (which under the conditions in the following run, happens after ~2 hours:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Stress/job/Impala-Stress-Test-EC2-CDH5-trunk/514/parameters/
)
Currently, it doesn't seem to be related to the patch for IMPALA-2592. It seems like the patch exposes an existing bug. I'm still digging into what causes the crash and don't know the reason yet. I will update once I have more information.

This crash does not show up without the patch. (The patch is here at: http://gerrit.cloudera.org:8080/#/c/2205/7)"	IMPALA	Resolved	1	1	8925	crash
13053286	"S3: Allow Frontend.loadTableData() to work with ""other"" filesystems"	"Frontend.loadTableData() expects that the underlying filesystem we use is HDFS. Change this to make it filesystem agnostic.

The bigger problem here is the staging of temporary files before renaming them to their final destination. Since S3 does not support renames, we need to choose one of the 3 options as mentioned in the parent JIRA or work out a better solution."	IMPALA	Resolved	3	7	8925	s3
13052483	Invalid bool value not reported as a scanner error	"In some cases, reading a bad boolean value does not report a scanner error though the value is NULL, as expected for an invalid value. This is only broken with codegen enabled.

Here is a query including the tinyint_col so that the row data is shown

{noformat}
[localhost:21000] > select bool_col, tinyint_col from functional.alltypeserror where id = 30;
Query: select bool_col, tinyint_col from functional.alltypeserror where id = 30
+----------+-------------+
| bool_col | tinyint_col |
+----------+-------------+
| NULL     | NULL        |
+----------+-------------+

ERRORS ENCOUNTERED DURING EXECUTION: 
Backend 1:Error converting column: 2 TO TINYINT (Data is: err30)
file: hdfs://localhost:20500/test-warehouse/alltypeserror/year=2009/month=3/0903.txt
record: 30,t\rue,err30,err30,err30,err300,err30..000000,err300.900000,01/01/10,10,0000-01-01 00:00:00

Returned 1 row(s) in 0.67s
{noformat}

now if only the bool_col is selected there is no error, but there is no valid boolean value in the row data.

{noformat}
[localhost:21000] > select bool_col from functional.alltypeserror where id = 30;
Query: select bool_col from functional.alltypeserror where id = 30
+----------+
| bool_col |
+----------+
| NULL     |
+----------+
Returned 1 row(s) in 0.63s
{noformat}

In both cases, there should be an error reported:

'Error converting column: 1 TO BOOL (Data is: t\rue)'"	IMPALA	Resolved	3	1	8925	codegen
13054881	LZO-scanner fails when reading large index files from S3	"The LZO-scanner always expects the index file to be a multiple of sizeof(uint64_t). In a loop, it reads the file 10MB at a time (which is a multiple of sizeof(uint64_t)) and processes this buffer uint64_t bytes at a time. This works fine for HDFS as HDFS returns the entire buffer as requested.

However, for S3, the internal read() relies on a stream, therefore it sends as many bytes as currently available. This means that the 'bytes read' may not be a multiple of sizeof(uint64_t). This currently causes the scanner to throw an error in this case.

We need to modify the LZO-scanner logic so that it carries over (buffer_size % sizeof(uint64_t)) bytes every time and process these remaining bytes as a part of the newly filled buffer."	IMPALA	Resolved	2	1	8925	lzo
13135998	rpc-mgr-kerberized-test fails on CentOS 6.4	"From the Jenkins logs:
{code}
15:25:38 /data/jenkins/workspace/impala-cdh5-trunk-core-data-load/repos/Impala/be/src/rpc/thrift-server-test.cc:176: Failure
15:25:38 Value of: ""No more data to read""
15:25:38   Actual: ""No more data to read""
15:25:38 Expected: a substring of non_ssl_client.Open().GetDetail()
15:25:38 Which is: ""Couldn't open transport for localhost:55428 (write() send(): Broken pipe)
15:25:38 ""
15:25:38 [  FAILED  ] KerberosOnAndOff/ThriftKerberizedParamsTest.SslConnectivity/2, where GetParam() = 2 (411 ms)
15:25:38 [----------] 3 tests from KerberosOnAndOff/ThriftKerberizedParamsTest (894 ms total)
15:25:38 
15:25:38 [----------] Global test environment tear-down
15:25:38 [==========] 17 tests from 6 test cases ran. (1716 ms total)
15:25:38 [  PASSED  ] 16 tests.
15:25:38 [  FAILED  ] 1 test, listed below:
15:25:38 [  FAILED  ] KerberosOnAndOff/ThriftKerberizedParamsTest.SslConnectivity/2, where GetParam() = 2
15:25:38 
15:25:38  1 FAILED TEST
15:25:38   YOU HAVE 1 DISABLED TEST
{code}

Any ideas, Sailesh?"	IMPALA	Resolved	1	7	8925	broken-build
13053940	Hive server does not start for S3 builds	"The hive server does not start for S3 builds because HDFS is marked as an unsupported service in testdata/cluster/admin; and so HDFS is not started at all, and so the Hive server is unable to start as well. Due to this, all our S3 builds fail.

Currently our S3 builds need HDFS to run correctly.

(This has to be reverted once IMPALA-1850 goes in, because then S3 can run as a default FS without HDFS)"	IMPALA	Resolved	1	1	8925	broken-build, test-infra
13052700	Add PERCENT_RANK, NTILE, CUME_DIST analytic window functions	We should add common percentile-based analytic functions (PERCENT_RANK, NTILE, CUME_DIST). They should be able to be implemented as rewrites in the planner.	IMPALA	Resolved	2	2	8925	planner, ramp-up
13055660	Allow AuthManager::Init() to be called more than once	"AuthManager::Init() is not idempotent. This generally isn't a problem since we call it only once per daemon, however, if we want to test with different security configurations as a part of the BE tests, this causes unnecessary failures.

This is because we use a global variable to track the completion of the kerberos environment setup which shouldn't be the case since we would want to re-setup the environment if we call AuthManager::Init() with a different configuration."	IMPALA	Resolved	3	1	8925	kerberos, security
13053720	Catch exception on failure to create thread	"Thread::StartThread() could fail to start a thread due to a lack of memory or because the daemon has reached its hard limit on the number of threads. It throws a *boost::thread_resource_error* when it cannot create a thread.

We saw this failure on one of the test clusters:

{code:java}
#0  0x0000003ab9e32625 in raise () from /lib64/libc.so.6
#1  0x0000003ab9e33d8d in abort () from /lib64/libc.so.6
#2  0x00007f2b7352f00d in __gnu_cxx::__verbose_terminate_handler() ()
   from /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.6/lib/impala/lib/libstdc++.so.6
#3  0x00007f2b7352d0e6 in ?? () from /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.6/lib/impala/lib/libstdc++.so.6
#4  0x00007f2b7352d131 in std::terminate() ()
   from /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.6/lib/impala/lib/libstdc++.so.6
#5  0x00007f2b7352d348 in __cxa_throw ()
   from /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.6/lib/impala/lib/libstdc++.so.6
#6  0x000000000077d2ed in void boost::throw_exception<boost::thread_resource_error>(boost::thread_resource_error const&) ()
#7  0x0000000000af9b40 in boost::thread::thread<void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::basic_string<char, std::char_traits<char>, std::allocator<char> >, boost::function<void ()()>, impala::Promise<long>*>(void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::basic_string<char, std::char_traits<char>, std::allocator<char> >, boost::function<void ()()>, impala::Promise<long>*) ()
#8  0x0000000000af68c5 in impala::Thread::StartThread(boost::function<void ()()> const&) ()
#9  0x0000000000cecc63 in impala::DataStreamSender::DataStreamSender(impala::ObjectPool*, int, impala::RowDescriptor const&, impala::TDataStreamSink const&, std::vector<impala::TPlanFragmentDestination, std::allocator<impala::TPlanFragmentDestination> > const&, int) ()
#10 0x0000000000cbdbbd in impala::DataSink::CreateDataSink(impala::ObjectPool*, impala::TDataSink const&, std::vector<impala::TExpr, std::allocator<impala::TExpr> > const&, impala::TPlanFragmentExecParams const&, impala::RowDescriptor const&, boost::scoped_ptr<impala::DataSink>*) ()
#11 0x0000000000cab5b1 in impala::PlanFragmentExecutor::Prepare(impala::TExecPlanFragmentParams const&) ()
#12 0x0000000000a62901 in impala::FragmentMgr::FragmentExecState::Prepare() ()
#13 0x0000000000a59023 in impala::FragmentMgr::FragmentThread(impala::TUniqueId) ()
#14 0x0000000000a5a46a in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId> > >, void>::invoke(boost::detail::function::function_buffer&) ()
#15 0x0000000000af8667 in impala::Thread::SuperviseThread(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*) ()
#16 0x0000000000af8f84 in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, boost::function<void ()()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void ()()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() ()
#17 0x0000000000d3240a in ?? ()
#18 0x0000003aba2079d1 in start_thread () from /lib64/libpthread.so.0
#19 0x0000003ab9ee88fd in clone () from /lib64/libc.so.6
{code}

From impalad.ERROR:
{code:java}
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::thread_resource_error> >'
{code}"	IMPALA	Resolved	2	1	8925	crash, memory
13072325	"Use new libHDFS API to address ""Unknown Error 255"""	"Now that HDFS-11529 is resolved, we can go ahead and use the new API to get more information if we encounter an ""Unknown Error 255"".

https://github.com/apache/hadoop/commit/fda86ef2a32026c02d9b5d4cca1ecb7b4decd872"	IMPALA	Resolved	2	1	8925	errorhandling
13055084	Don't hold process wide lock while serializing Runtime Profile in GetRuntimeProfileStr()	"In ImpalaServer::GetRuntimeProfileStr(), we hold the query_exec_state_map_lock_ while serializing the runtime profile.
https://github.com/apache/incubator-impala/blob/master/be/src/service/impala-server.cc#L568

The SerializeToArchiveString() is a slow operation and is called _at least_ once per query. We don't need to hold the map lock, we can instead take a shared_ptr reference and hold the individual exec_state lock while serializing the runtime profile.

If the query gets unregistered during the serialization operation, it will wait on the exec_state lock in QueryExecState::Done() until the serialization is complete.
https://github.com/apache/incubator-impala/blob/master/be/src/service/query-exec-state.cc#L526"	IMPALA	Resolved	3	4	8925	performance
13053520	Support LOAD DATA on S3	This is to track the development of LOAD DATA for Impala on S3	IMPALA	Resolved	3	7	8925	s3
13053507	Log error status for failures in backend tests	"When most of the backend tests expect a function that returns _Status_ to pass, they just do the following:

{code:java}
ASSERT_TRUE(Function().ok());
{code}

This is not very useful in case the assertion fails because it does not log the error associated with the failure.

Moreover, the Jenkins jobs on EC2 slaves record test failures but many times, the slave is shutdown before anyone can look at the logs. If we add logging during failures, we can get more information in the Jenkins jobs making a lot of build failures much easier to diagnose."	IMPALA	Resolved	3	4	8925	test-infra
13052493	test_result_verifier error msg normalization should not rely on hdfs qualified paths	"test_result_verifier.py attempts to normalize error messages so that they may be compared against expected results. This involves removing hdfs qualified paths which may vary, e.g. because they include a hostname.

This won't work for s3, we need to generalize this normalization mechanism so that it doesn't rely on hdfs paths and also so that it does not encode specific knowledge of error messages.

After doing so, remove tests marked with @skip_if_s3_qualified_path."	IMPALA	Open	4	1	8925	s3, test-infra
13054862	sorted streams[is_merging=true]  cause  DataStreamRecvr logical deadlock	"since   GetBatch()  Thread  async  Sequential consumption of all queues , but buffer  only have 10M 
sometimes  before  front queue RowBatch Coming   buffer(10M)  is full already

I think   the  kind of scene  should  limit every queue  size    and  every queue must promise at least one rowbatch could put into"	IMPALA	Resolved	2	1	8925	hang
13055279	KuduScanNode may not terminate scanners when parent nodes reach limit	"Related to IMPALA-4654 where the limit on a KuduScanNode was not respected and scanner threads would continue to completion.

There may be an additional bug if a limit is on another node. We do not currently have a repro for this but believe there could be an issue because the KuduScanner doesn't check the scan node's {{done_}} flag, or have some other way of being notified of cancellation except by checking the runtime state's status which may not be set in this case.

This is based on a discussion in this CR, see for context:
https://gerrit.cloudera.org/#/c/5493/"	IMPALA	Resolved	3	1	8925	kudu
13052491	Support users containing commas in --authorized_proxy_user_config	"{{--authorized_proxy_user_config}} expects a schema like this:

{{user1=proxy1,proxy2;user2=proxy}}

The parsing breaks if 'proxy1' contains a comma, as is likely if the delegated user is an LDAP compound name. We can't change the separator without breaking users, but maybe we can support an alternate form or a way to change the separator."	IMPALA	Resolved	3	4	8925	ramp-up
13054061	Build failed on S3: TestMaxNestingDepth.test_max_nesting_depth	"S3 build failed: http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-core-s3/32/

{code}

16:46:30 =================================== FAILURES ===================================
16:46:30  TestMaxNestingDepth.test_max_nesting_depth[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none] 
16:46:30 query_test/test_nested_types.py:491: in test_max_nesting_depth
16:46:30     self.run_test_case('QueryTest/max-nesting-depth', vector)
16:46:30 common/impala_test_suite.py:269: in run_test_case
16:46:30     result = self.__execute_query(target_impalad_client, query, user=user)
16:46:30 common/impala_test_suite.py:470: in __execute_query
16:46:30     return impalad_client.execute(query, user=user)
16:46:30 common/impala_connection.py:161: in execute
16:46:30     return self.__beeswax_client.execute(sql_stmt, user=user)
16:46:30 beeswax/impala_beeswax.py:163: in execute
16:46:30     handle = self.__execute_query(query_string.strip(), user=user)
16:46:30 beeswax/impala_beeswax.py:329: in __execute_query
16:46:30     self.wait_for_completion(handle)
16:46:30 beeswax/impala_beeswax.py:349: in wait_for_completion
16:46:30     raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
16:46:30 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
16:46:30 E    Query aborted:
16:46:30 E   Failed to open HDFS file s3a://impala-cdh5-trunk/test-warehouse/max_nesting_depth/struct/file.parq
16:46:30 E   Error(2): No such file or directory
16:46:30 E   
16:46:30 E   
16:46:30 E   
16:46:30 E   Failed to open HDFS file s3a://impala-cdh5-trunk/test-warehouse/max_nesting_depth/struct/file.parq
16:46:30 E   Error(2): No such file or directory
{code}"	IMPALA	Resolved	2	1	8925	broken-build, s3
13099091	Segmentation fault in ClientCacheHelper::ReleaseClient(void**) [breakpad?]	"Here is the stack trace of the crashing thread from a minidump. Please reach out if you want to access the full minidump. I may also be able to provide a core file.

{noformat}
Crash reason:  SIGSEGV
Crash address: 0x28

 0  impalad!impala::ClientCacheHelper::ReleaseClient(void**) [client-cache.cc : 153 + 0x0]
 1  impalad!impala::ClientConnection<impala::ImpalaBackendClient>::~ClientConnection() [client-cache.h : 412 + 0x5]
 2  impalad!impala::DataStreamSender::Channel::TransmitDataHelper(impala::TRowBatch const*) [data-stream-sender.cc : 208 + 0x9]
 3  impalad!impala::DataStreamSender::Channel::TransmitData(int, impala::TRowBatch const*) [data-stream-sender.cc : 185 + 0x5]
 4  impalad!impala::ThreadPool<impala::TRowBatch*>::WorkerThread(int) [function_template.hpp : 767 + 0x10]
 5  impalad!impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) [function_template.hpp : 767 + 0x7]
 6  impalad!boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > > >::run() [bind.hpp :      457 + 0x6]
 7  impalad!thread_proxy + 0xda
 8  libpthread-2.12.so + 0x7a51
 9  libc-2.12.so + 0xe893d
{noformat}"	IMPALA	Resolved	1	1	8925	crash
13054083	Enable writes to local filesystem	Now that we allow writes to span multiple filesystems, explore possibilities for enabling writes (INSERTs and LOAD DATA) to the local filesystem.	IMPALA	Resolved	4	2	8925	supportability
13054148	S3: test_truncate_cleans_hdfs_files fails because we skip INSERT staging	"On the introduction of IMPALA-3452, we defaulted to skipping the INSERT staging for S3.
test_truncate_cleans_hdfs_files assumes that it will always see the _\_impala_insert\_staging_ folder but we will not see that on S3."	IMPALA	Resolved	1	1	8925	broken-build, s3
13115714	Coordinator threads that publish RuntimeFilters continue to run after query failure/cancellation	"When running highly concurrent queries with lots of joins on a large cluster I noticed that there is lots of untracked memory on the coordinator node. 

Experiments showed that the majority of the untracked memory is coming from RunTimeFilters.

After canceling the concurrent queries the Queries tab showed 0 ""Running"" and 0 ""Waiting to be cancelled"" queries while the memz tab show that memory allocated for the cancelled/completed queries is still there. 

Same issue happens if remote fragments finish while the coordinator node is in the progress of sending the runtime filters to the remote nodes. 

{code}
CPU Time
1 of 1: 100.0% (20.348s of 20.348s)

impalad ! apache::thrift::transport::TSSLSocket::write - [unknown source file]
impalad ! apache::thrift::transport::TTransport::write + 0xc - TTransport.h:158
impalad ! apache::thrift::transport::TSaslTransport::write + 0x3b - TSaslTransport.cpp:233
impalad ! apache::thrift::transport::TBufferedTransport::writeSlow + 0x58 - [unknown source file]
impalad ! apache::thrift::transport::TTransport::write + 0xb - TTransport.h:158
impalad ! writeString<std::basic_string<char> > + 0x39 - TBinaryProtocol.tcc:186
impalad ! apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::writeBinary - TBinaryProtocol.tcc:193
impalad ! apache::thrift::protocol::TVirtualProtocol<apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>, apache::thrift::protocol::TProtocolDefaults>::writeBinary_virt + 0xb - TVirtualProtocol.h:421
impalad ! apache::thrift::protocol::TProtocol::writeBinary + 0x10 - TProtocol.h:468
impalad ! impala::TBloomFilter::write + 0x72 - ImpalaInternalService_types.cpp:4397
impalad ! impala::TPublishFilterParams::write + 0xad - ImpalaInternalService_types.cpp:4689
impalad ! impala::ImpalaInternalService_PublishFilter_pargs::write + 0x44 - ImpalaInternalService.cpp:903
impalad ! impala::ImpalaInternalServiceClient::send_PublishFilter + 0x78 - ImpalaInternalService.cpp:1310
impalad ! impala::ImpalaBackendClient::PublishFilter + 0x14 - backend-client.h:117
impalad ! impala::ClientConnection<impala::ImpalaBackendClient>::DoRpc<void (impala::TPublishFilterResult&, impala::TPublishFilterParams const&, bool*) impala::ImpalaBackendClient::*, impala::TPublishFilterParams, impala::TPublishFilterResult> + 0x57 - client-cache.h:240
impalad ! impala::Coordinator::BackendState::PublishFilter + 0x288 - coordinator-backend-state.cc:415
impalad ! impala::Coordinator::UpdateFilter + 0x5ff - coordinator.cc:1167
impalad ! impala::ImpalaServer::UpdateFilter + 0x32 - impala-server.cc:2089
impalad ! impala::ImpalaInternalServiceProcessor::process_UpdateFilter + 0x1e2 - ImpalaInternalService.cpp:1613
impalad ! impala::ImpalaInternalServiceProcessor::dispatchCall + 0xe8 - ImpalaInternalService.cpp:1370
impalad ! apache::thrift::TDispatchProcessor::process + 0xab - TDispatchProcessor.h:121
impalad ! apache::thrift::server::TAcceptQueueServer::Task::run + 0x15e - TAcceptQueueServer.cpp:77
impalad ! impala::ThriftThread::RunRunnable + 0x68 - thrift-thread.cc:74
impalad ! boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() + 0x3a - mem_fn_template.hpp:280
impalad ! operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<long unsigned int>*>, boost::_bi::list0> + 0x1e - bind.hpp:392
impalad ! boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable>>, boost::_bi::value<impala::Promise<unsigned long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable>>, boost::_bi::value<impala::Promise<unsigned long>*>>>, void>::invoke + 0x9 - function_template.hpp:153
impalad ! boost::function0<void>::operator() + 0x1a - function_template.hpp:767
impalad ! impala::Thread::SuperviseThread + 0x1a7 - thread.cc:352
impalad ! operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> + 0x5a - bind.hpp:457
impalad ! boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>>::run + 0x19 - thread.hpp:116
impalad ! thread_proxy + 0xd9 - [unknown source file]
libpthread-2.12.so ! start_thread + 0xd0 - [unknown source file]
libc-2.12.so ! clone + 0x6c - [unknown source file]
{code}"	IMPALA	Resolved	3	1	8925	memory, runtime-filters
13053504	StatestoreSslTest.SmokeTest failed	"Sailesh, can you take a first look since it seems possibly SSL related?

Failed build:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Build/job/impala-master-cdh5-trunk/1688/

Some information can be found in the Jenkins artifacts, and I've pasted the test INFO and ERROR log info for convenience here:

FROM /tmp/statestore-test.INFO
{code}
...
I0125 23:14:43.089670 24475 thrift-server.cc:431] ThriftServer 'StatestoreSubscriber' started on port: 60236
I0125 23:14:43.089689 24475 statestore-subscriber.cc:190] Registering with statestore
I0125 23:14:43.097177 24556 statestore.cc:373] Registering: smoke_sub1
I0125 23:14:43.097215 24556 statestore.cc:396] Subscriber 'smoke_sub1' registered (registration id: 514a972a5c3ca4b4:490164b0c15804ac)
I0125 23:14:43.097309 24475 statestore-subscriber.cc:194] statestore registration successful
I0125 23:14:43.097478 24475 statestore-subscriber.cc:179] Starting statestore subscriber
I0125 23:14:43.099148 24475 thrift-server.cc:431] ThriftServer 'StatestoreSubscriber' started on port: 60246
I0125 23:14:43.099155 24475 statestore-subscriber.cc:190] Registering with statestore
I0125 23:14:43.099510 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:14:43.099519 24475 thrift-client.cc:61] (Attempt 1 of 10)
I0125 23:14:46.099647 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:14:46.099664 24475 thrift-client.cc:61] (Attempt 2 of 10)
I0125 23:14:49.099793 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:14:49.099810 24475 thrift-client.cc:61] (Attempt 3 of 10)
I0125 23:14:52.099937 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:14:52.099954 24475 thrift-client.cc:61] (Attempt 4 of 10)
I0125 23:14:55.100081 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:14:55.100097 24475 thrift-client.cc:61] (Attempt 5 of 10)
I0125 23:14:58.100224 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:14:58.100241 24475 thrift-client.cc:61] (Attempt 6 of 10)
I0125 23:15:01.100369 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:15:01.100389 24475 thrift-client.cc:61] (Attempt 7 of 10)
I0125 23:15:04.100517 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:15:04.100538 24475 thrift-client.cc:61] (Attempt 8 of 10)
I0125 23:15:07.100664 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:15:07.100684 24475 thrift-client.cc:61] (Attempt 9 of 10)
I0125 23:15:10.100811 24475 thrift-client.cc:55] Unable to connect to localhost:60226
I0125 23:15:10.100828 24475 thrift-client.cc:61] (Attempt 10 of 10)
I0125 23:15:10.100867 24475 statestore-subscriber.cc:196] statestore registration unsuccessful: Failed to create socket: SSL_CTX_load_verify_locations: PEM lib
{code}

FROM /tmp/statestore-test.ERROR
{code}
Log file created at: 2016/01/25 23:14:43
Running on machine: impala-boost-static-burst-slave-12e9.vpc.cloudera.com
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
E0125 23:14:43.054327 24475 logging.cc:120] stderr will be logged to this file.
E0125 23:14:43.073817 24524 thrift-server.cc:176] ThriftServer 'StatestoreService' (on port: 52744) exited due to TException: Could not bind: Transport endpoint is not connected
E0125 23:14:43.073866 24475 thrift-server.cc:165] ThriftServer 'StatestoreService' (on port: 52744) did not start correctly 
E0125 23:14:43.078884 24530 thrift-server.cc:176] ThriftServer 'StatestoreSubscriber' (on port: 52764) exited due to TException: Could not bind: Transport endpoint is not connected
E0125 23:14:43.078935 24475 thrift-server.cc:165] ThriftServer 'StatestoreSubscriber' (on port: 52764) did not start correctly 
{code}

*Update: The logs above are not from the failed build. They are from another successful build and are expected logs.*"	IMPALA	Resolved	1	1	8925	broken-build
13052376	Constant expressions not checked for errors, no state cleanup on exception.	"Constant filter expressions are evaluated during query compilation, and not during execution. When evaluating those expressions during compilation, there is no check whether there is an error set or not.

Specifically, errors are not checked after GetConstVal() is called in ScalarFnCall::Open(). Also, in NativeEvalConstExprs in fe-support.cc, if an error takes place during evaluation in Open() or GetValue(), the state is not cleaned up (Close() is not called) before throwing an exception."	IMPALA	Resolved	3	1	8925	correctness
13054041	S3 : Parquet file size not honored when inserting in S3 target	"When inserting into a parquet file stored in S3 the PARQUET_FILE_SIZE is not honored, even 0m works fine. 

{code}
[impala-s3-r3-2.vpc.cloudera.com:21000] > set PARQUET_FILE_SIZE=0m;
PARQUET_FILE_SIZE set to 0m
[impala-s3-r3-2.vpc.cloudera.com:21000] > insert overwrite table lineitem_2m select * from lineitem_prefix limit 1000000;
Query: insert overwrite table lineitem_2m select * from lineitem_prefix limit 1000000
WARNINGS: Cancelled (1 of 41 similar)

Inserted 1000000 row(s) in 8.58s
[impala-s3-r3-2.vpc.cloudera.com:21000] > summary;
+--------------+--------+----------+----------+-------+------------+----------+---------------+-------------------------------------+
| Operator     | #Hosts | Avg Time | Max Time | #Rows | Est. #Rows | Peak Mem | Est. Peak Mem | Detail                              |
+--------------+--------+----------+----------+-------+------------+----------+---------------+-------------------------------------+
| 00:SCAN HDFS | 1      | 1.07s    | 1.07s    | 1.00M | 1.00M      | 1.45 GB  | -1 B          | tpch_300_parquet_s3.lineitem_prefix |
+--------------+--------+----------+----------+-------+------------+----------+---------------+-------------------------------------+
[impala-s3-r3-2.vpc.cloudera.com:21000] >



[impala-s3-r3-2.vpc.cloudera.com:21000] > describe formatted lineitem_2m;
Query: describe formatted lineitem_2m
+------------------------------+----------------------------------------------------------------+----------------------+
| name                         | type                                                           | comment              |
+------------------------------+----------------------------------------------------------------+----------------------+
| # col_name                   | data_type                                                      | comment              |
|                              | NULL                                                           | NULL                 |
| l_orderkey                   | bigint                                                         | NULL                 |
| l_partkey                    | bigint                                                         | NULL                 |
| l_suppkey                    | bigint                                                         | NULL                 |
| l_linenumber                 | bigint                                                         | NULL                 |
| l_quantity                   | decimal(12,2)                                                  | NULL                 |
| l_extendedprice              | decimal(12,2)                                                  | NULL                 |
| l_discount                   | decimal(12,2)                                                  | NULL                 |
| l_tax                        | decimal(12,2)                                                  | NULL                 |
| l_returnflag                 | string                                                         | NULL                 |
| l_linestatus                 | string                                                         | NULL                 |
| l_shipdate                   | string                                                         | NULL                 |
| l_commitdate                 | string                                                         | NULL                 |
| l_receiptdate                | string                                                         | NULL                 |
| l_shipinstruct               | string                                                         | NULL                 |
| l_shipmode                   | string                                                         | NULL                 |
| l_comment                    | string                                                         | NULL                 |
|                              | NULL                                                           | NULL                 |
| # Detailed Table Information | NULL                                                           | NULL                 |
| Database:                    | tpch_300_parquet_s3                                            | NULL                 |
| Owner:                       | root                                                           | NULL                 |
| CreateTime:                  | Mon Apr 25 21:10:42 PDT 2016                                   | NULL                 |
| LastAccessTime:              | UNKNOWN                                                        | NULL                 |
| Protect Mode:                | None                                                           | NULL                 |
| Retention:                   | 0                                                              | NULL                 |
| Location:                    | s3a://cloudera-impala-perf-w2/tpch_300_parquet.db/lineitem_2m  | NULL                 |
| Table Type:                  | MANAGED_TABLE                                                  | NULL                 |
| Table Parameters:            | NULL                                                           | NULL                 |
|                              | COLUMN_STATS_ACCURATE                                          | false                |
|                              | numFiles                                                       | 0                    |
|                              | numRows                                                        | -1                   |
|                              | rawDataSize                                                    | -1                   |
|                              | totalSize                                                      | 0                    |
|                              | transient_lastDdlTime                                          | 1461643856           |
|                              | NULL                                                           | NULL                 |
| # Storage Information        | NULL                                                           | NULL                 |
| SerDe Library:               | org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe    | NULL                 |
| InputFormat:                 | org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat  | NULL                 |
| OutputFormat:                | org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat | NULL                 |
| Compressed:                  | No                                                             | NULL                 |
| Num Buckets:                 | 0                                                              | NULL                 |
| Bucket Columns:              | []                                                             | NULL                 |
| Sort Columns:                | []                                                             | NULL                 |
+------------------------------+----------------------------------------------------------------+----------------------+
{code}


Files created 
{code}
[root@impala-s3-r3-2 ~]# hdfs dfs -ls -h s3a://test-bucket/tpch_300_parquet.db/lineitem_2m
Found 3 items
drwxrwxrwx   - root          0 2016-04-25 21:21 s3a://test-bucket/tpch_300_parquet.db/lineitem_2m/_impala_insert_staging
-rw-rw-rw-   1 root     29.1 M 2016-04-25 21:21 s3a://test-bucket/tpch_300_parquet.db/lineitem_2m/de4569fc1e5a03f9-4e483c8ba28068a2_2098843614_data.0.parq
-rw-rw-rw-   1 root      8.3 M 2016-04-25 21:21 s3a://test-bucket/tpch_300_parquet.db/lineitem_2m/de4569fc1e5a03f9-4e483c8ba28068a2_2098843614_data.1.parq
{code}"	IMPALA	Resolved	2	1	8925	s3
13053142	Get the right 'NumColumns' in the parquet scanner	The NumColumns counter that shows up in the runtime profile is only used as a part of the parquet scanner and is calculated wrong when the impalad is assigned more than one split.	IMPALA	Resolved	4	3	8925	impala
13053364	Thrift-client cleans openSSL state before using it in the case of the catalog	"The _ThriftClientImpl::CreateSocket()_ function creates a local _TSSLSocketFactory_ instance which gets cleaned up when the function exits in the catalogd.

{code:java}
Status ThriftClientImpl::CreateSocket() {
  if (!ssl_) {
    socket_.reset(new TSocket(address_.hostname, address_.port));
  } else {
    try {
      TSSLSocketFactory factory;
      // TODO: No need to do this every time we create a socket, the factory can be
      // shared. But since there may be many certificates, this needs some slightly more
      // complex infrastructure to do right.
      factory.loadTrustedCertificates(FLAGS_ssl_client_ca_certificate.c_str());
      socket_ = factory.createSocket(address_.hostname, address_.port);
    } catch (const TException& e) {
      return Status(Substitute(""Failed to create socket: $0"", e.what()));
    }
  }

  return Status::OK();
}
{code}

This code is shared by the statestored, impalad and the catalogd. The reason it happens only in the catalogd is because in the statestored and impalad, we create a _ThriftServer_ socket (see _ThriftServer::CreateSocket()_) before creating a _ThriftClientImpl_ socket. However, in the catalogd, we create the server after we create the client.

The _ThriftServer::CreateSocket()_ code creates a _TSSLSocketFactory_ shared pointer which is not destroyed when the function completes.

Why this makes a difference is because of how _TSSLSocketFactory_ is setup and destroyed:

{code:java}
// TSSLSocketFactory implementation
bool     TSSLSocketFactory::initialized = false;
uint64_t TSSLSocketFactory::count_ = 0;
Mutex    TSSLSocketFactory::mutex_;

TSSLSocketFactory::TSSLSocketFactory(): server_(false) {
  Guard guard(mutex_);
  if (count_ == 0) {
    initializeOpenSSL();
    randomize();
  }
  count_++;
  ctx_ = boost::shared_ptr<SSLContext>(new SSLContext);
}

TSSLSocketFactory::~TSSLSocketFactory() {
  Guard guard(mutex_);
  count_--;
  if (count_ == 0) {
    cleanupOpenSSL();
  }
}
{code}

The interesting variable here is _count\__. In case of the catalogd, since we run the _ThriftClientImpl::CreateSocket()_ code first, the _count\__ is decremented to 0 when the CreateSocket() function exits, causing it to call _cleanupOpenSSL()_, which loses all the state set up by the client."	IMPALA	Resolved	1	7	8925	catalog-server, impala, security
13053977	S3: Allow a secondary filesystem to be configured	"After IMPALA-1850, we can only run tests on one filesystem at a time. We should allow a secondary filesystem to be configured so that we can transfer between filesystems, have partitions on different filesystems, etc. through Impala.

This should mostly only involve infrastructure changes as Impala itself will fully support this after IMPALA-1878."	IMPALA	Resolved	3	3	8925	s3, test-infra, usability
13053210	Remove 'kinit -R' workaround	"- SaslAuthProvider::RunKinit() has a workloop which gets a new ticket every X seconds(depending on the config).
 - It also renews the ticket after 1500 ms after it gets the ticket just so that it can be put into the credential cache. (It has to wait 1500 ms because of a kerberos bug which only shows up in RHEL6 and Kerberos version 1.8.1 and up)
 - We believe that if any authentication happens in that window, we will get an error like so:

{code:java}
Couldn't open transport for vd0534.halxg.cloudera.com:22000 (SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure.  Minor code may provide more information (Credential cache is empty))
{code}

It is hard to reproduce and confirm, but I will update it here if I am able to.

We also believe that this bug causes an execution of a code path that results in hangs because of a channel being left open on another node indefinitely as documented in IMPALA-2592.

We need to investigate if there is a better way of doing this so as to avoid the window.

The links below explain why we do what we do in RunKinit():
http://www.cloudera.com/content/www/en-us/documentation/archive/cdh/3-x/3u6/CDH3-Security-Guide/cdh3sg_topic_14_2.html
https://jira.cloudera.com/browse/OPSAPS-11159"	IMPALA	Resolved	2	1	8925	impala, usability
13054527	gerrit.cloudera.org has multiple Impala ASF projects	"We have {{ImpalaASF}} and {{Impala-ASF}}. This will lead to confusion.

Is it possible to get whichever is wrong deleted from Gerrit, or if not deleted, otherwise hidden / disabled?

https://gerrit.cloudera.org/#/admin/projects/ImpalaASF
https://gerrit.cloudera.org/#/admin/projects/Impala-ASF"	IMPALA	Resolved	2	7	8925	asf, asf-milestone-1
13075013	Builds on CentOS 6.4 failing with broken python dependencies	"Builds on CentOS 6.4 fail due to dependencies not met for the new 'cryptography' python package.

This is from the following recent commit:
https://github.com/apache/incubator-impala/commit/50bd015f2d9cd66449bc8494bc35ca2aa741c156

The commit states that the new packages are only required for ADLS and that ADLS on a dev environment is only supported from CentOS 6.7.

The fix would be to attempt installing those packages only if the dev environment is configured to use ADLS."	IMPALA	Resolved	1	1	8925	broken-build
13063279	Error messages are sometimes dropped before reaching client	"On the nightly core s3 tests, the custom cluster test test_exchange_delays failed. It looks like the query failed for an unexplained reason. It says 'aborted' but we need to look at it more closely.

{code}
07:56:58 =================================== FAILURES ===================================
07:56:58  TestExchangeDelays.test_exchange_small_delay[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none] 
07:56:58 
07:56:58 self = <test_exchange_delays.TestExchangeDelays object at 0x3f054d0>
07:56:58 vector = <tests.common.test_vector.ImpalaTestVector object at 0x5a32750>
07:56:58 
07:56:58     @pytest.mark.execute_serially
07:56:58     @CustomClusterTestSuite.with_args(""--stress_datastream_recvr_delay_ms=10000""
07:56:58           "" --datastream_sender_timeout_ms=5000"")
07:56:58     def test_exchange_small_delay(self, vector):
07:56:58       """"""Test delays in registering data stream receivers where the first one or two
07:56:58         batches will time out before the receiver registers, but subsequent batches will
07:56:58         arrive after the receiver registers. Before IMPALA-2987, this scenario resulted in
07:56:58         incorrect results.
07:56:58         """"""
07:56:58 >     self.run_test_case('QueryTest/exchange-delays', vector)
07:56:58 
07:56:58 custom_cluster/test_exchange_delays.py:39: 
07:56:58 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
07:56:58 common/impala_test_suite.py:362: in run_test_case
07:56:58     self.__verify_exceptions(test_section['CATCH'], str(e), use_db)
07:56:58 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
07:56:58 
07:56:58 self = <test_exchange_delays.TestExchangeDelays object at 0x3f054d0>
07:56:58 expected_strs = ['Sender timed out waiting for receiver fragment instance\n']
07:56:58 actual_str = 'ImpalaBeeswaxException: Query aborted: (1 of 3 similar)'
07:56:58 use_db = None
07:56:58 
07:56:58     def __verify_exceptions(self, expected_strs, actual_str, use_db):
07:56:58       """"""
07:56:58         Verifies that at least one of the strings in 'expected_str' is a substring of the
07:56:58         actual exception string 'actual_str'.
07:56:58         """"""
07:56:58       actual_str = actual_str.replace('\n', '')
07:56:58       for expected_str in expected_strs:
07:56:58         # In error messages, some paths are always qualified and some are not.
07:56:58         # So, allow both $NAMENODE and $FILESYSTEM_PREFIX to be used in CATCH.
07:56:58         expected_str = expected_str.strip() \
07:56:58             .replace('$FILESYSTEM_PREFIX', FILESYSTEM_PREFIX) \
07:56:58             .replace('$NAMENODE', NAMENODE) \
07:56:58             .replace('$IMPALA_HOME', IMPALA_HOME)
07:56:58         if use_db: expected_str = expected_str.replace('$DATABASE', use_db)
07:56:58         # Strip newlines so we can split error message into multiple lines
07:56:58         expected_str = expected_str.replace('\n', '')
07:56:58         if expected_str in actual_str: return
07:56:58       assert False, 'Unexpected exception string. Expected: %s\nNot found in actual: %s' % \
07:56:58 >       (expected_str, actual_str)
07:56:58 E     AssertionError: Unexpected exception string. Expected: Sender timed out waiting for receiver fragment instance
07:56:58 E     Not found in actual: ImpalaBeeswaxException: Query aborted: (1 of 3 similar)
07:56:58 
07:56:58 common/impala_test_suite.py:253: AssertionError

07:56:58 ----------------------------- Captured stderr call -----------------------------
07:56:58 -- executing against localhost:21000
07:56:58 use functional;
07:56:58 
07:56:58 SET disable_codegen=False;
07:56:58 SET abort_on_error=1;
07:56:58 SET exec_single_node_rows_threshold=0;
07:56:58 SET batch_size=0;
07:56:58 SET num_nodes=0;
07:56:58 -- executing against localhost:21000
07:56:58 select count(*)
07:56:58 from tpch.lineitem
07:56:58   inner join tpch.orders on l_orderkey = o_orderkey;
07:56:58 
07:56:58 ======== 1 failed, 43 passed, 20 skipped, 8 xfailed in 3213.63 seconds =========
{code}"	IMPALA	Resolved	1	1	8925	broken-build
13053130	DCHECK on destroying an ExprContext	"Hit DCHECK
{code}
F1008 16:41:37.301750 24707 expr-context.cc:41] Check failed: !prepared_ || closed
{code}

When running the following query, 
{code}
select * from sp1 where code = from_unixtime(unix_timestamp(), 'yyyyMMMdd hh:mm:ss');
{code}

step to reproduce
{code}
CREATE TABLE default.sp1 (
  description STRING, 
  total_emp INT, 
  salary INT
)
PARTITIONED BY (
  code STRING
)
{code}
make sure insert more than one record.

This seems related to IMPALA-1756.

{code}
  for (int i = 0; i < expr_ctxs.size(); ++i) {
    Status open_status = expr_ctxs[i]->Open(&state);
    if (!open_status.ok()) {
      expr_ctxs[i]->Close(&state);
      (env)->ThrowNew(JniUtil::internal_exc_class(), open_status.GetDetail().c_str());
      return result_bytes;
    }
{code}
we return once Open() fails, but should all expr_ctxs be closed in this case?"	IMPALA	Resolved	2	1	8925	crash
13052870	Crash (likely race) tearing down BufferedBlockMgr on query failure	"When running a heavy workload on a 6-node cluster (happened to have RM enabled, but another cluster repro'd without RM but with Kerberos), a number of impalads crashed while tearing down the BufferedBlockMgr.

The impalad crashes with the following stack:
{code}
#9  impala::ErrorCount (errors=Cannot access memory at address 0xa00000008018
) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/util/error-util.cc:192
#10 0x0000000000976f92 in impala::RuntimeState::LogError (this=0x8bc3db00, message=...) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/runtime-state.cc:229
#11 0x00000000009886ae in impala::BufferedBlockMgr::WriteComplete (this=0x4931eb000, block=<value optimized out>, write_status=...) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/buffered-block-mgr.cc:778
#12 0x000000000099e930 in operator() (this=0x46b80f00, status=...) at /usr/include/boost/function/function_template.hpp:1013
#13 impala::DiskIoMgr::RequestContext::Cancel (this=0x46b80f00, status=...) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/disk-io-mgr-reader-context.cc:83
#14 0x000000000099553c in impala::DiskIoMgr::CancelContext (this=<value optimized out>, context=0x46b80f00, wait_for_disks_completion=true) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/disk-io-mgr.cc:420
#15 0x00000000009955ed in impala::DiskIoMgr::UnregisterContext (this=0x599f200, reader=0x46b80f00) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/disk-io-mgr.cc:388
#16 0x0000000000989821 in impala::BufferedBlockMgr::~BufferedBlockMgr (this=0x4931eb000, __in_chrg=<value optimized out>) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/buffered-block-mgr.cc:495
#17 0x000000000098f162 in checked_delete<impala::BufferedBlockMgr> (this=<value optimized out>) at /usr/include/boost/checked_delete.hpp:34
#18 boost::detail::sp_counted_impl_p<impala::BufferedBlockMgr>::dispose (this=<value optimized out>) at /usr/include/boost/smart_ptr/detail/sp_counted_impl.hpp:78
#19 0x0000000000979335 in release (this=0xdaf52900, __in_chrg=<value optimized out>) at /usr/include/boost/smart_ptr/detail/sp_counted_base_gcc_x86.hpp:145
#20 ~shared_count (this=0xdaf52900, __in_chrg=<value optimized out>) at /usr/include/boost/smart_ptr/detail/shared_count.hpp:217
#21 ~shared_ptr (this=0xdaf52900, __in_chrg=<value optimized out>) at /usr/include/boost/smart_ptr/shared_ptr.hpp:169
#22 reset (this=0xdaf52900, __in_chrg=<value optimized out>) at /usr/include/boost/smart_ptr/shared_ptr.hpp:386
#23 impala::RuntimeState::~RuntimeState (this=0xdaf52900, __in_chrg=<value optimized out>) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/runtime-state.cc:95
#24 0x0000000000be78b1 in checked_delete<impala::RuntimeState> (this=0xdaf55810, __in_chrg=<value optimized out>) at /usr/include/boost/checked_delete.hpp:34
#25 ~scoped_ptr (this=0xdaf55810, __in_chrg=<value optimized out>) at /usr/include/boost/smart_ptr/scoped_ptr.hpp:80
#26 impala::PlanFragmentExecutor::~PlanFragmentExecutor (this=0xdaf55810, __in_chrg=<value optimized out>) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/runtime/plan-fragment-executor.cc:78
#27 0x0000000000a2efee in ~FragmentExecState (x=0xdaf55600) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/service/fragment-exec-state.h:42
#28 boost::checked_delete<impala::FragmentMgr::FragmentExecState> (x=0xdaf55600) at /usr/include/boost/checked_delete.hpp:34
#29 0x00000000007766a9 in release (this=<value optimized out>, __in_chrg=<value optimized out>) at /usr/include/boost/smart_ptr/detail/sp_counted_base_gcc_x86.hpp:145
#30 boost::detail::shared_count::~shared_count (this=<value optimized out>, __in_chrg=<value optimized out>) at /usr/include/boost/smart_ptr/detail/shared_count.hpp:217
#31 0x0000000000a2d1d8 in ~shared_ptr (this=0x7273b00, exec_state=0xdaf55600) at /usr/include/boost/smart_ptr/shared_ptr.hpp:169
#32 impala::FragmentMgr::FragmentExecThread (this=0x7273b00, exec_state=0xdaf55600) at /data/3/jenkins/workspace/impala-master-64bit-PRIVATE-fast/repos/Impala/be/src/service/fragment-mgr.cc:100
{code}

In the case I observed, this was happening on the 6 node RM cluster when running 24 concurrent TPCDS streams across 2 YARN queues (12 streams in each queue) with preemption disabled. I don't have any reason to believe this is related to the llama-integration code, but rather I think that the heavy RM workload caused some calls to MemTracker::TryConsume by the BufferedBlockMgr to fail (waiting 5sec and then timing out due to lack of resources)  exposed some new possible race conditions. This is just my theory, it needs to be proven. This didn't happen when there was a single queue, and I suspect that may be due the further limitations on resources in one of the queues because preemption wasn't enabled. As I mentioned previously, I don't think this is actually related to the llama integration code but rather that this was exposed due to new interleavings between threads, where some calls to the MemTracker now take longer (5sec to timeout) and then fail to TryConsume.

Casey observed the same crash on a non-RM cluster, but had Kerberos enabled."	IMPALA	Resolved	1	1	8925	crash, llama, resource-management
13053209	DataStreamSender::Channel::CloseInternal() does not close the channel on an error.	"The DataStreamSender::Channel::CloseInternal() does not close the channel on an error. This will cause the node on the other side of the channel to wait indefinitely causing a hang.

We observed this occurring due to a kerberos issue related to the the credential cache being outdated, but in theory this can occur for any error that causes Channel::CloseInternal() to fail without closing properly.

The query ""hangs"" in that it remains registered forever and never makes progress, but the impalads remain responsive and continue to work as expected for other queries.

There is no workaround at this time."	IMPALA	Resolved	1	1	8925	hang, query-lifecycle, usability
13172300	Build fails on Centos6	"Due to recent change in IMPALA-7006, the build started failing on Centos6.

{noformat}
13:33:16 /data/jenkins/workspace/impala-asf-master-exhaustive-centos6/repos/Impala/be/src/kudu/util/net/socket.cc: In member function kudu::Status kudu::Socket::SetReusePort(bool):
13:33:16 /data/jenkins/workspace/impala-asf-master-exhaustive-centos6/repos/Impala/be/src/kudu/util/net/socket.cc:249:50: error: SO_REUSEPORT was not declared in this scope
13:33:16    RETURN_NOT_OK_PREPEND(SetSockOpt(SOL_SOCKET, SO_REUSEPORT, int_flag),
13:33:16                                                   ^
13:33:16 make[2]: *** [be/src/kudu/util/CMakeFiles/kudu_util.dir/net/socket.cc.o] Error 1
13:33:16 make[2]: *** Waiting for unfinished jobs....
{noformat}"	IMPALA	Resolved	1	1	8925	broken-build
13173345	Errors in HdfsScanner::Open() errors get swallowed up	"[https://jenkins.impala.io/job/parallel-all-tests/3826/|https://jenkins.impala.io/job/parallel-all-tests/3826/failed]failed with at test_udfs.py:
{noformat}
03:50:23 ] FAIL query_test/test_udfs.py::TestUdfExecution::()::test_udf_errors[exec_option: {'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'exec_single_node_rows_threshold': 100, 'enable_expr_rewrites': True} | table_format: text/none]
03:50:23 ] =================================== FAILURES ===================================
03:50:23 ]  TestUdfExecution.test_udf_errors[exec_option: {'disable_codegen_rows_threshold': 0, 'disable_codegen': True, 'exec_single_node_rows_threshold': 100, 'enable_expr_rewrites': True} | table_format: text/none] 
03:50:23 ] [gw13] linux2 -- Python 2.7.12 /home/ubuntu/Impala/bin/../infra/python/env/bin/python
03:50:23 ] query_test/test_udfs.py:415: in test_udf_errors
03:50:23 ]     self.run_test_case('QueryTest/udf-errors', vector, use_db=unique_database)
03:50:23 ] common/impala_test_suite.py:408: in run_test_case
03:50:23 ]     self.__verify_exceptions(test_section['CATCH'], str(e), use_db)
03:50:23 ] common/impala_test_suite.py:286: in __verify_exceptions
03:50:23 ]     (expected_str, actual_str)
03:50:23 ] E   AssertionError: Unexpected exception string. Expected: BadExpr2 prepare error
03:50:23 ] E   Not found in actual: ImpalaBeeswaxException: Query aborted:Cancelled
03:50:23 ] ---------------------------- Captured stderr setup -----------------------------
{noformat}
Digging through the log, the query which triggered the failure is {{774db10632a21589:5a62e77200000000}}

It appears that the error which this test intends to fault at isn't shown at the coordinator:
{noformat}
ExecState: query id=774db10632a21589:5a62e77200000000 finstance=774db10632a21589:5a62e77200000003 on host=ip-172-31-0-127:22001 (EXECUTING -> ERROR) status=Cancelled
{noformat}

In particular, the test aims to trigger a failure in {{HdfsScanner::Open()}} when scalar expr evaluator is cloned:
{noformat}
// This prepare function always fails for cloned evaluators to exercise IMPALA-6184.
// It does so by detecting whether the caller is a cloned evaluator and inserts an error
// in FunctionContext if that's the case.
void BadExpr2Prepare(FunctionContext* context,
    FunctionContext::FunctionStateScope scope) {
  if (scope == FunctionContext::FRAGMENT_LOCAL) {
    int32_t* state = reinterpret_cast<int32_t*>(context->Allocate(sizeof(int32_t)));
    *state = 0xf001cafe;
    context->SetFunctionState(scope, state);
    // Set the thread local state too to differentiate from cloned evaluators.
    context->SetFunctionState(FunctionContext::THREAD_LOCAL, state);
  } else {
    if (context->GetFunctionState(FunctionContext::THREAD_LOCAL) == nullptr) {
      context->SetError(""BadExpr2 prepare error"");
    }
  }
}
{noformat}
However, for some reasons, the actual failure to be propagated and instead the cancellation status was propagated instead. Staring at the code in {{HdfsScanNode}}, it's not immediately clear where the race is.

For the reference, the following is the expected error message:
{noformat}
ExecState: query id=64404101d8857592:173298a700000000 finstance=64404101d8857592:173298a700000002 on host=ip-172-31-0-127:22002 (EXECUTING -> ERROR) status=BadExpr2 prepare error
{noformat}"	IMPALA	Resolved	2	1	8925	broken-build
13052257	"""PartitionsCreated"" counter is not tracked correctly"	"""PartitionsCreated"" counter is incremented whenever new data file is written, this doesn't always mean a new partition is created. It's more like ""PartitionsUpdated"".
Also if two Impalad insert data into the same partition, the counter could be incremented twice."	IMPALA	Open	4	1	8925	ramp-up, supportability
13055031	Per operator timing in profile summary is incorrect when mt_dop > 0	"From the summary

|Operator||       #Hosts||   Avg Time||  Max Time||    #Rows  Est.|| #Rows||   Peak Mem||  Est. Peak Mem||  Detail|
|03:AGGREGATE        |1    |8.668ms   |8.668ms        |1           |1    |7.79 MB        |-1.00 B  |FINALIZE|                   
|02:EXCHANGE         |1    |4.914ms   |4.914ms      |480           |1    |      0        |-1.00 B  |UNPARTITIONED|              
|01:AGGREGATE      |480    |1s024ms  |21s191ms       |24           |1   |14.04 MB        |-1.00 B  |              |             
|00:SCAN HDFS      |480  |949.025ms  |20s143ms  |900.74M      |18.00B  |136.28 MB        |-1.00 B  |tpch_3000_parquet.lineitem|

The slowest fragment in the query
{code}
      Instance d1403bdbfb6c9d71:156 (host=d2403.halxg.cloudera.com:22000):(Total: 36s524ms, non-child: 15.297ms, % non-child: 0.04%)
        Hdfs split stats (<volume id>:<# splits>/<split lengths>): 17:1/128.00 MB 20:1/128.00 MB 21:1/128.00 MB 14:2/256.00 MB 10:1/128.00 MB 7:3/384.00 MB 22:1/128.00 MB 6:1/81.40 MB 23:1/128.00 MB 11:2/218.92 MB 8:2/256.00 MB 12:1/80.91 MB 19:1/128.00 MB 2:2/256.00 MB 18:2/256.00 MB 9:2/256.00 MB 
        MemoryUsage(1s000ms): 40.30 KB, 112.32 MB, 90.32 MB, 79.82 MB, 108.32 MB, 63.35 MB, 84.50 MB, 88.45 MB, 105.82 MB, 108.32 MB, 130.57 MB, 113.95 MB, 86.32 MB, 126.32 MB, 55.56 MB, 108.32 MB, 92.06 MB, 61.55 MB, 112.32 MB, 90.32 MB, 69.55 MB, 108.32 MB, 63.51 MB, 75.82 MB, 130.32 MB, 97.82 MB, 94.32 MB, 130.32 MB, 90.45 MB, 76.07 MB, 123.07 MB, 90.45 MB, 86.32 MB, 94.31 MB, 75.82 MB, 126.57 MB
        ThreadUsage(1s000ms): 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
         - AverageThreadTokens: 1.00 
         - BloomFilterBytes: 0
         - PeakMemoryUsage: 144.07 MB (151072208)
         - PerHostPeakMemUsage: 2.54 GB (2729446952)
         - RowsProduced: 1 (1)
         - TotalCpuTime: 32s934ms
         - TotalNetworkReceiveTime: 0.000ns
         - TotalNetworkSendTime: 250.604us
         - TotalStorageWaitTime: 3s600ms
        Fragment Instance Lifecycle Timings:
           - ExecTime: 563.108us
             - ExecTreeExecTime: 61.100us
           - OpenTime: 36s464ms
             - ExecTreeOpenTime: 35s741ms
           - PrepareTime: 59.505ms
             - ExecTreePrepareTime: 43.876ms
        CodeGen:(Total: 759.417ms, non-child: 759.417ms, % non-child: 100.00%)
           - CodegenTime: 13.868ms
           - CompileTime: 104.865ms
           - LoadTime: 0.000ns
           - ModuleBitcodeSize: 1.90 MB (1994596)
           - NumFunctions: 166 (166)
           - NumInstructions: 3.25K (3253)
           - OptimizationTime: 614.018ms
           - PrepareTime: 39.635ms
        DataStreamSender (dst_id=2):(Total: 135.680us, non-child: 135.680us, % non-child: 100.00%)
           - BytesSent: 6.21 KB (6360)
           - NetworkThroughput(*): 190.50 MB/sec
           - OverallThroughput: 44.70 MB/sec
           - RowsReturned: 1 (1)
           - SerializeBatchTime: 85.840us
           - TransmitDataRPCTime: 31.840us
           - UncompressedRowBatchSize: 16.54 KB (16938)
        AGGREGATION_NODE (id=1):(Total: 35s749ms, non-child: 19s337ms, % non-child: 54.09%)
          ExecOption: Codegen Enabled
           - BuildTime: 19s016ms
           - GetResultsTime: 0.000ns
           - HTResizeTime: 0.000ns
           - HashBuckets: 0 (0)
           - LargestPartitionPercent: 0 (0)
           - MaxPartitionLevel: 0 (0)
           - NumRepartitions: 0 (0)
           - PartitionsCreated: 0 (0)
           - PeakMemoryUsage: 13.79 MB (14464088)
           - RowsRepartitioned: 0 (0)
           - RowsReturned: 1 (1)
           - RowsReturnedRate: 0
           - SpilledPartitions: 0 (0)
        HDFS_SCAN_NODE (id=0):(Total: 16s411ms, non-child: 16s411ms, % non-child: 100.00%)
          Hdfs split stats (<volume id>:<# splits>/<split lengths>): 17:1/128.00 MB 20:1/128.00 MB 21:1/128.00 MB 14:2/256.00 MB 10:1/128.00 MB 7:3/384.00 MB 22:1/128.00 MB 6:1/81.40 MB 23:1/128.00 MB 11:2/218.92 MB 8:2/256.00 MB 12:1/80.91 MB 19:1/128.00 MB 2:2/256.00 MB 18:2/256.00 MB 9:2/256.00 MB 
          ExecOption: PARQUET Codegen Enabled, Codegen enabled: 24 out of 24
          Hdfs Read Thread Concurrency Bucket: 0:94.37% 1:5.634% 2:0% 3:0% 4:0% 5:0% 6:0% 7:0% 8:0% 9:0% 10:0% 11:0% 12:0% 13:0% 14:0% 15:0% 16:0% 17:0% 18:0% 19:0% 20:0% 21:0% 22:0% 23:0% 24:0% 25:0% 26:0% 27:0% 
          File Formats: PARQUET/NONE:360 
          BytesRead(1s000ms): 0, 121.18 MB, 183.47 MB, 283.25 MB, 373.75 MB, 429.74 MB, 500.75 MB, 586.76 MB, 686.16 MB, 751.84 MB, 873.22 MB, 935.50 MB, 1001.19 MB, 1.10 GB, 1.13 GB, 1.22 GB, 1.31 GB, 1.35 GB, 1.46 GB, 1.52 GB, 1.61 GB, 1.71 GB, 1.77 GB, 1.83 GB, 1.95 GB, 2.01 GB, 2.07 GB, 2.19 GB, 2.25 GB, 2.32 GB, 2.43 GB, 2.50 GB, 2.56 GB, 2.65 GB, 2.68 GB, 2.80 GB
           - FooterProcessingTime: (Avg: 16.417ms ; Min: 1.397ms ; Max: 107.045ms ; Number of samples: 24)
           - AverageHdfsReadThreadConcurrency: 0.06 
           - AverageScannerThreadConcurrency: 0.00 
           - BytesRead: 2.80 GB (3009913373)
           - BytesReadDataNodeCache: 0
           - BytesReadLocal: 2.80 GB (3008684573)
           - BytesReadRemoteUnexpected: 0
           - BytesReadShortCircuit: 2.80 GB (3008684573)
           - DecompressionTime: 0.000ns
           - MaxCompressedTextFileLength: 0
           - NumColumns: 15 (15)
           - NumDisksAccessed: 17 (17)
           - NumRowGroups: 24 (24)
           - NumScannerThreadsStarted: 0 (0)
           - PeakMemoryUsage: 136.03 MB (142634136)
           - PerReadThreadRawHdfsThroughput: 1.38 GB/sec
           - RemoteScanRanges: 12 (12)
           - RowsRead: 37.05M (37052184)
           - RowsReturned: 37.05M (37052184)
           - RowsReturnedRate: 2.26 M/sec
           - ScanRangesComplete: 24 (24)
           - ScannerThreadsInvoluntaryContextSwitches: 0 (0)
           - ScannerThreadsTotalWallClockTime: 0.000ns
             - MaterializeTupleTime(*): 12s392ms
             - ScannerThreadsSysTime: 0.000ns
             - ScannerThreadsUserTime: 0.000ns
           - ScannerThreadsVoluntaryContextSwitches: 0 (0)
           - TotalRawHdfsReadTime(*): 2s026ms
           - TotalReadThroughput: 78.64 MB/sec
{code}"	IMPALA	Resolved	3	1	8925	profile, supportability
13055213	WriteSlot and CodegenWriteSlot handle escaped NULL slots differently	"We set the length of a slot as '-len' (i.e. the negative of the length) if the slot contains an escaped character.

If the caller of WriteSlot() is about to pass a slot with an escaped character to WriteSlot(), it makes sure it sets the 'len' argument to positive and sets the 'need_escape' argument to true.

A slot with '\N' (a default negative slot) will always have a negative length of -2, since it contains an escape character. WriteSlot() will receive the length for this slot as '2' (positive) and do a NULL check successfully.

CodegenWriteSlot() uses the IRBuilder and the argument it receives for 'len' will be negative if the passed 'data' has an escape character. It then passes this negative length directly to the IrIsNullString() or IrGenericIsNullString() functions which do not expect a negative length.

Therefore no slots will be marked as NULL slots by this check when codegen is enabled.

We've been getting by so far because there was some buggy code after the NULL check which marked invalid blocks and NULL blocks as NULL instead of giving an error back to the user for the invalid blocks case, leading to IMPALA-1862."	IMPALA	Resolved	3	1	8925	correctness
13054194	S3: Consider allowing table-sink to stage in HDFS when writing to S3	"If users do not want to skip the staging step on INSERTs to S3, we could allow the table sink to stage the temporary files in HDFS (if available) and make the coordinator move the files to S3 on FinalizeSuccessfulInsert().

This could improve performance in INSERTs to S3 as writes to HDFS are faster than to S3 currently. Currently, when we do not skip the staging step, the sinks write to a temporary loaction in S3 and the coordinator copies over these files to the final location in S3 (as S3 doesn't support the rename() operation). So this would bring down the number of writes to S3 from 2 to 1 per file."	IMPALA	Resolved	4	4	8925	performance, s3
13106996	Remove InProcessImpalaServer	Now that we've refactored CreateImpalaServer() such that it can be used in tests (IMPALA-4786), we should get rid of InProcessImpalaServer, which was only historically there because the ImpalaServer couldn't be used in tests.	IMPALA	Open	3	3	8925	refactor
13052291	Print time and link to coordinator web UI once query is submitted in shell	To help supportability and debugging, it would be helpful to have impala shell print out time and link to co-ordinator web UI once query is submitted	IMPALA	Resolved	3	4	8925	debugging, ramp-up, shell, supportability, usability
13054672	Address follow up comments for IMPALA-3610	"Specifically:

 - Broadcast filter aggregation OOM in UpdateFilter shouldn't stop future updates from retrying.
 - Consider 'disable' filter only for error cases for debugging purposes.
 - Style changes."	IMPALA	Open	3	1	8925	impala
13054596	TransmitData() should not block.	"{{TransmitData()}} calls effectively block until an error occurs (e.g. {{datastream_sender_timeout_ms}} expires) or the RPC is serviced by the receiver.

This is bad for a variety of reasons, not least that it makes it impossible to cancel the sender without also cancelling the recipient (who might not even be running!). 

Instead, we should have {{TransmitData()}} return control to the caller with an {{EAGAIN}}-like error so that the row batch can be resent in the future if the caller decides to do so."	IMPALA	Resolved	3	4	8925	rpc
13054044	query_test/test_insert.py::TestInsertWideTable::test_insert_wide_table fails on S3	"{code}
[gw3] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-sailesh-s3/repos/Impala/bin/../infra/python/env/bin/python
query_test/test_insert.py:117: in test_insert_wide_table
    self.client.execute(insert_stmt)
common/impala_connection.py:161: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:163: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:329: in __execute_query
    self.wait_for_completion(handle)
beeswax/impala_beeswax.py:349: in wait_for_completion
    raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:Parquet file size 33554432 bytes is too small for a table with 2000 columns. Set query option PARQUET_FILE_SIZE to at least 393216000.
E   
E   
E   
E   Parquet file size 33554432 bytes is too small for a table with 2000 columns. Set query option PARQUET_FILE_SIZE to at least 393216000.
{code}"	IMPALA	Resolved	2	1	8925	s3
13072968	Review consistency of ADLS python client used for Impala testing	"The ADLS Python client seems to have consistency issues even though ADLS claims to be strongly consistent.

Some of our tests are skipped because of this issue, with the tag SkipIfADLS.slow_client.

The documentation for the Python client doesn't seem to state or address this as a known issue. It is however, a pre-release client.

This JIRA is meant to track this issue on the Impala side, and close it once it's addressed by ADLS."	IMPALA	Resolved	3	3	8925	infrastructure
13053841	Make a gerrit instance that can push to git.apache.org	"Once git.apache.org is populated with the initial code drop, and builds, we should switch gerrit.cloudera.org to push commits directly to the ASF repo (and implement any downstream syncing we want to do).

Check how the Kudu project set this up to see if there are best practices regarding how Gerrit authenticates with git.apache.org etc."	IMPALA	Resolved	2	7	8925	asf, asf-milestone-1
13117102	KRPC w/ TLS doesn't work on remote clusters after rebase	"It looks like depending on who initializes OpenSSL (KRPC or us), the behavior changes. After some cherry-picks, we're unable to run Impala on remote clusters with TLS with certain certificate types.

We get the following when we use intermediate CAs:
{code:java}
""F1108 10:47:36.532202 93303 impalad-main.cc:79] Could not build messenger: Runtime error: certificate does not match private key: error:0B080074:x509 certificate routines:X509_check_private_key:key values mismatch:x509_cmp.c:331""
{code}

And we get the following when we use self-signed certificates:
""self signed certificate in certificate chain"""	IMPALA	Resolved	1	7	8925	broken-build, security
13165051	Avoid unnecessarily pretty printing profiles per fragment instance	"In SendReport(), if VLOG_FILE_IS_ON is 'true' (which is not the most verbose logging level, but is higher than default), we pretty print the profile for every fragment instance, which is a very expensive operation, as serializing the profile is non-trivial (look at RuntimeProfile::PrettyPrint()), and printing large amounts of information to the logs isn't cheap as well. Lastly, it is very noisy.

This seems unnecessary since this will not benefit us, as all the profiles are merged at the coordinator side. We could argue that this might be necessary when an executor fails to send the profile to the coordinator, but that signifies a network issue which will not be reflected in the profile of any fragment instance.

This will help reduce noise in the logs when the log level is bumped up to find other real issues that VLOG_FILE can help with."	IMPALA	Resolved	4	4	8925	logs
13059815	Use Kudu's Kinit code to avoid expensive fork	"Impala does a kinit by doing a RunShell() command which basically forks the entire process (potentially expensive) and execs the 'kinit' command.

KuduRPC avoids the fork by calling into libkrb programatically. Since we eventually will be pulling in KuduRPC to Impala, we can get rid of the fork and call into the appropriate KuduRPC code."	IMPALA	Resolved	3	4	8925	security
13055121	test_partition_ddl_predicates breaks on non-HDFS filesystems	"This is possibly because non-HDFS filesystems do not support 'set cached', etc. from this test:
https://github.com/apache/incubator-impala/blob/b3cbc960a7e4beebf6cb9076a02b3513bb0b2954/testdata/workloads/functional-query/queries/QueryTest/partition-ddl-predicates.test

http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-asf-master/job/impala-asf-master-core-isilon/127/

http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-asf-master/job/impala-asf-master-core-local-filesystem/139/"	IMPALA	Resolved	1	1	8925	broken-build
13053980	query_test.test_scanners.TestScanRangeLengths.test_scan_ranges fails to open file	http://sandbox.jenkins.cloudera.com/view/Impala/view/Evergreen-cdh5-trunk/job/impala-cdh5-trunk-exhaustive-rhel7/288/	IMPALA	Resolved	1	1	8925	broken-build
13054138	Failed to delete staging directories in S3 build.	"Sailesh, have you seen this one before?

{code}
17:47:14 =================================== FAILURES ===================================
17:47:14  TestTimestampErrors.test_timestamp_scan_agg_errors[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none] 
17:47:14 data_errors/test_data_errors.py:142: in test_timestamp_scan_agg_errors
17:47:14     self._setup_test_table(table_name)
17:47:14 data_errors/test_data_errors.py:135: in _setup_test_table
17:47:14     self.client.execute(insert_stmt)
17:47:14 common/impala_connection.py:161: in execute
17:47:14     return self.__beeswax_client.execute(sql_stmt, user=user)
17:47:14 beeswax/impala_beeswax.py:163: in execute
17:47:14     handle = self.__execute_query(query_string.strip(), user=user)
17:47:14 beeswax/impala_beeswax.py:329: in __execute_query
17:47:14     self.wait_for_completion(handle)
17:47:14 beeswax/impala_beeswax.py:349: in wait_for_completion
17:47:14     raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
17:47:14 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
17:47:14 E    Query aborted:Error(s) deleting staging directories. First error (of 1) was: Hdfs op (DELETE s3a://impala-cdh5-trunk/test-warehouse/test_timestamp34761.db/scan_agg_timestamp_69278/_impala_insert_staging/c2432d3332e02ad0_b12c7ffee4d5a985/.c2432d3332e02ad0-b12c7ffee4d5a986_2036212051_dir/) failed, error was: Error(5): Input/output error
17:47:14 ---------------------------- Captured stderr setup -----------------------------
{code}

See this build:
http://sandbox.jenkins.cloudera.com/job/impala-umbrella-build-and-test-s3/1/

*To do when fixed*
I marked the build as keep forever, so please undo that when this is fixed."	IMPALA	Resolved	1	1	8925	broken-build
13131955	Find a reliable way to detect supported TLS versions	"The problem in brief is that when we build against an older version of OpenSSL and run against a higher version of OpenSSL, the SSLeay() function (which is supposed to return the runtime version of OpenSSL), returns the compile time version of OpenSSL instead of the version that it's actually running against.

Due to this, our version compatibility checking code doesn't allow us to use TLSv1.2 on certain platforms (specifically RHEL when it's built against OpenSSL 1.0.0 and run on a CentOS system with OpenSSL 1.0.1 or above).



This was filed as a bug against RHEL:
https://bugzilla.redhat.com/show_bug.cgi?id=1497859"	IMPALA	Resolved	1	1	8925	security
13157270	TestRuntimeRowFilters.test_row_filters failing with Memory limit exceeded	"
{code:java}
Error Message
query_test/test_runtime_filters.py:171: in test_row_filters     test_file_vars={'$RUNTIME_FILTER_WAIT_TIME_MS' : str(WAIT_TIME_MS)}) common/impala_test_suite.py:405: in run_test_case     result = self.__execute_query(target_impalad_client, query, user=user) common/impala_test_suite.py:620: in __execute_query     return impalad_client.execute(query, user=user) common/impala_connection.py:160: in execute     return self.__beeswax_client.execute(sql_stmt, user=user) beeswax/impala_beeswax.py:173: in execute     handle = self.__execute_query(query_string.strip(), user=user) beeswax/impala_beeswax.py:341: in __execute_query     self.wait_for_completion(handle) beeswax/impala_beeswax.py:361: in wait_for_completion     raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None) E   ImpalaBeeswaxException: ImpalaBeeswaxException: E    Query aborted:Memory limit exceeded: ParquetColumnReader::ReadDataPage() failed to allocate 65533 bytes for decompressed data. E   HDFS_SCAN_NODE (id=0) could not allocate 64.00 KB without exceeding limit. E   Error occurred on backend impala-boost-static-burst-slave-el7-1aa3.vpc.cloudera.com:22001 by fragment f9423526590ed30b:732f1db100000001 E   Memory left in process limit: 11.14 GB E   Memory left in query limit: 22.16 KB E   Query(f9423526590ed30b:732f1db100000000): Limit=200.00 MB Reservation=160.00 MB ReservationLimit=160.00 MB OtherMemory=39.98 MB Total=199.98 MB Peak=199.98 MB E     Fragment f9423526590ed30b:732f1db100000007: Reservation=134.00 MB OtherMemory=30.22 MB Total=164.22 MB Peak=164.22 MB E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB E       AGGREGATION_NODE (id=3): Total=4.00 KB Peak=4.00 KB E         Exprs: Total=4.00 KB Peak=4.00 KB E       HASH_JOIN_NODE (id=2): Reservation=132.00 MB OtherMemory=198.25 KB Total=132.19 MB Peak=132.19 MB E         Exprs: Total=25.12 KB Peak=25.12 KB E         Hash Join Builder (join_node_id=2): Total=157.12 KB Peak=157.12 KB E           Hash Join Builder (join_node_id=2) Exprs: Total=149.12 KB Peak=149.12 KB E       EXCHANGE_NODE (id=4): Reservation=14.91 MB OtherMemory=67.76 KB Total=14.97 MB Peak=14.97 MB E         KrpcDeferredRpcs: Total=67.76 KB Peak=67.76 KB E       EXCHANGE_NODE (id=5): Reservation=15.03 MB OtherMemory=0 Total=15.03 MB Peak=15.03 MB E         KrpcDeferredRpcs: Total=0 Peak=45.12 KB E       KrpcDataStreamSender (dst_id=6): Total=16.00 KB Peak=16.00 KB E     Fragment f9423526590ed30b:732f1db100000001: Reservation=26.00 MB OtherMemory=9.75 MB Total=35.75 MB Peak=35.75 MB E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB E       HDFS_SCAN_NODE (id=0): Reservation=24.00 MB OtherMemory=9.30 MB Total=33.30 MB Peak=33.30 MB E         Exprs: Total=260.00 KB Peak=260.00 KB E       KrpcDataStreamSender (dst_id=4): Total=426.57 KB Peak=458.57 KB E         KrpcDataStreamSender (dst_id=4) Exprs: Total=256.00 KB Peak=256.00 KB E     Fragment f9423526590ed30b:732f1db100000004: Reservation=0 OtherMemory=0 Total=0 Peak=29.59 MB E       HDFS_SCAN_NODE (id=1): Reservation=0 OtherMemory=0 Total=0 Peak=29.32 MB E       KrpcDataStreamSender (dst_id=5): Total=0 Peak=266.57 KB E    E   Memory limit exceeded: ParquetColumnReader::ReadDataPage() failed to allocate 65533 bytes for decompressed data. E   HDFS_SCAN_NODE (id=0) could not allocate 64.00 KB without exceeding limit. E   Error occurred on backend impala-boost-static-burst-slave-el7-1aa3.vpc.cloudera.com:22001 by fragment f9423526590ed30b:732f1db100000001 E   Memory left in process limit: 11.14 GB E   Memory left in query limit: 22.16 KB E   Query(f9423526590ed30b:732f1db100000000): Limit=200.00 MB Reservation=160.00 MB ReservationLimit=160.00 MB OtherMemory=39.98 MB Total=199.98 MB Peak=199.98 MB E     Fragment f9423526590ed30b:732f1db100000007: Reservation=134.00 MB OtherMemory=30.22 MB Total=164.22 MB Peak=164.22 MB E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB E       AGGREGATION_NODE (id=3): Total=4.00 KB Peak=4.00 KB E         Exprs: Total=4.00 KB Peak=4.00 KB E       HASH_JOIN_NODE (id=2): Reservation=132.00 MB OtherMemory=198.25 KB Total=132.19 MB Peak=132.19 MB E         Exprs: Total=25.12 KB Peak=25.12 KB E         Hash Join Builder (join_node_id=2): Total=157.12 KB Peak=157.12 KB E           Hash Join Builder (join_node_id=2) Exprs: Total=149.12 KB Peak=149.12 KB E       EXCHANGE_NODE (id=4): Reservation=14.91 MB OtherMemory=67.76 KB Total=14.97 MB Peak=14.97 MB E         KrpcDeferredRpcs: Total=67.76 KB Peak=67.76 KB E       EXCHANGE_NODE (id=5): Reservation=15.03 MB OtherMemory=0 Total=15.03 MB Peak=15.03 MB E         KrpcDeferredRpcs: Total=0 Peak=45.12 KB E       KrpcDataStreamSender (dst_id=6): Total=16.00 KB Peak=16.00 KB E     Fragment f9423526590ed30b:732f1db100000001: Reservation=26.00 MB OtherMemory=9.75 MB Total=35.75 MB Peak=35.75 MB E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB E       HDFS_SCAN_NODE (id=0): Reservation=24.00 MB OtherMemory=9.30 MB Total=33.30 MB Peak=33.30 MB E         Exprs: Total=260.00 KB Peak=260.00 KB E       KrpcDataStreamSender (dst_id=4): Total=426.57 KB Peak=458.57 KB E         KrpcDataStreamSender (dst_id=4) Exprs: Total=256.00 KB Peak=256.00 KB E     Fragment f9423526590ed30b:732f1db100000004: Reservation=0 OtherMemory=0 Total=0 Peak=29.59 MB E       HDFS_SCAN_NODE (id=1): Reservation=0 OtherMemory=0 Total=0 Peak=29.32 MB E       KrpcDataStreamSender (dst_id=5): Total=0 Peak=266.57 KB
Stacktrace
query_test/test_runtime_filters.py:171: in test_row_filters
    test_file_vars={'$RUNTIME_FILTER_WAIT_TIME_MS' : str(WAIT_TIME_MS)})
common/impala_test_suite.py:405: in run_test_case
    result = self.__execute_query(target_impalad_client, query, user=user)
common/impala_test_suite.py:620: in __execute_query
    return impalad_client.execute(query, user=user)
common/impala_connection.py:160: in execute
    return self.__beeswax_client.execute(sql_stmt, user=user)
beeswax/impala_beeswax.py:173: in execute
    handle = self.__execute_query(query_string.strip(), user=user)
beeswax/impala_beeswax.py:341: in __execute_query
    self.wait_for_completion(handle)
beeswax/impala_beeswax.py:361: in wait_for_completion
    raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
E   ImpalaBeeswaxException: ImpalaBeeswaxException:
E    Query aborted:Memory limit exceeded: ParquetColumnReader::ReadDataPage() failed to allocate 65533 bytes for decompressed data.
E   HDFS_SCAN_NODE (id=0) could not allocate 64.00 KB without exceeding limit.
E   Error occurred on backend impala-boost-static-burst-slave-el7-1aa3.vpc.cloudera.com:22001 by fragment f9423526590ed30b:732f1db100000001
E   Memory left in process limit: 11.14 GB
E   Memory left in query limit: 22.16 KB
E   Query(f9423526590ed30b:732f1db100000000): Limit=200.00 MB Reservation=160.00 MB ReservationLimit=160.00 MB OtherMemory=39.98 MB Total=199.98 MB Peak=199.98 MB
E     Fragment f9423526590ed30b:732f1db100000007: Reservation=134.00 MB OtherMemory=30.22 MB Total=164.22 MB Peak=164.22 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       AGGREGATION_NODE (id=3): Total=4.00 KB Peak=4.00 KB
E         Exprs: Total=4.00 KB Peak=4.00 KB
E       HASH_JOIN_NODE (id=2): Reservation=132.00 MB OtherMemory=198.25 KB Total=132.19 MB Peak=132.19 MB
E         Exprs: Total=25.12 KB Peak=25.12 KB
E         Hash Join Builder (join_node_id=2): Total=157.12 KB Peak=157.12 KB
E           Hash Join Builder (join_node_id=2) Exprs: Total=149.12 KB Peak=149.12 KB
E       EXCHANGE_NODE (id=4): Reservation=14.91 MB OtherMemory=67.76 KB Total=14.97 MB Peak=14.97 MB
E         KrpcDeferredRpcs: Total=67.76 KB Peak=67.76 KB
E       EXCHANGE_NODE (id=5): Reservation=15.03 MB OtherMemory=0 Total=15.03 MB Peak=15.03 MB
E         KrpcDeferredRpcs: Total=0 Peak=45.12 KB
E       KrpcDataStreamSender (dst_id=6): Total=16.00 KB Peak=16.00 KB
E     Fragment f9423526590ed30b:732f1db100000001: Reservation=26.00 MB OtherMemory=9.75 MB Total=35.75 MB Peak=35.75 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       HDFS_SCAN_NODE (id=0): Reservation=24.00 MB OtherMemory=9.30 MB Total=33.30 MB Peak=33.30 MB
E         Exprs: Total=260.00 KB Peak=260.00 KB
E       KrpcDataStreamSender (dst_id=4): Total=426.57 KB Peak=458.57 KB
E         KrpcDataStreamSender (dst_id=4) Exprs: Total=256.00 KB Peak=256.00 KB
E     Fragment f9423526590ed30b:732f1db100000004: Reservation=0 OtherMemory=0 Total=0 Peak=29.59 MB
E       HDFS_SCAN_NODE (id=1): Reservation=0 OtherMemory=0 Total=0 Peak=29.32 MB
E       KrpcDataStreamSender (dst_id=5): Total=0 Peak=266.57 KB
E   
E   Memory limit exceeded: ParquetColumnReader::ReadDataPage() failed to allocate 65533 bytes for decompressed data.
E   HDFS_SCAN_NODE (id=0) could not allocate 64.00 KB without exceeding limit.
E   Error occurred on backend impala-boost-static-burst-slave-el7-1aa3.vpc.cloudera.com:22001 by fragment f9423526590ed30b:732f1db100000001
E   Memory left in process limit: 11.14 GB
E   Memory left in query limit: 22.16 KB
E   Query(f9423526590ed30b:732f1db100000000): Limit=200.00 MB Reservation=160.00 MB ReservationLimit=160.00 MB OtherMemory=39.98 MB Total=199.98 MB Peak=199.98 MB
E     Fragment f9423526590ed30b:732f1db100000007: Reservation=134.00 MB OtherMemory=30.22 MB Total=164.22 MB Peak=164.22 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       AGGREGATION_NODE (id=3): Total=4.00 KB Peak=4.00 KB
E         Exprs: Total=4.00 KB Peak=4.00 KB
E       HASH_JOIN_NODE (id=2): Reservation=132.00 MB OtherMemory=198.25 KB Total=132.19 MB Peak=132.19 MB
E         Exprs: Total=25.12 KB Peak=25.12 KB
E         Hash Join Builder (join_node_id=2): Total=157.12 KB Peak=157.12 KB
E           Hash Join Builder (join_node_id=2) Exprs: Total=149.12 KB Peak=149.12 KB
E       EXCHANGE_NODE (id=4): Reservation=14.91 MB OtherMemory=67.76 KB Total=14.97 MB Peak=14.97 MB
E         KrpcDeferredRpcs: Total=67.76 KB Peak=67.76 KB
E       EXCHANGE_NODE (id=5): Reservation=15.03 MB OtherMemory=0 Total=15.03 MB Peak=15.03 MB
E         KrpcDeferredRpcs: Total=0 Peak=45.12 KB
E       KrpcDataStreamSender (dst_id=6): Total=16.00 KB Peak=16.00 KB
E     Fragment f9423526590ed30b:732f1db100000001: Reservation=26.00 MB OtherMemory=9.75 MB Total=35.75 MB Peak=35.75 MB
E       Runtime Filter Bank: Reservation=2.00 MB ReservationLimit=2.00 MB OtherMemory=0 Total=2.00 MB Peak=2.00 MB
E       HDFS_SCAN_NODE (id=0): Reservation=24.00 MB OtherMemory=9.30 MB Total=33.30 MB Peak=33.30 MB
E         Exprs: Total=260.00 KB Peak=260.00 KB
E       KrpcDataStreamSender (dst_id=4): Total=426.57 KB Peak=458.57 KB
E         KrpcDataStreamSender (dst_id=4) Exprs: Total=256.00 KB Peak=256.00 KB
E     Fragment f9423526590ed30b:732f1db100000004: Reservation=0 OtherMemory=0 Total=0 Peak=29.59 MB
E       HDFS_SCAN_NODE (id=1): Reservation=0 OtherMemory=0 Total=0 Peak=29.32 MB
E       KrpcDataStreamSender (dst_id=5): Total=0 Peak=266.57 KB
{code}
"	IMPALA	Resolved	2	1	8925	build-failure
13054056	S3 : Orphaned HDFS files after table location is altered	"Repro 
* Create partitioned table (Default FS is HDFS)
* Insert 100 rows data into partitioned table
* Alter table set location -> S3
* Insert overwrite table limit 0
* Select count(*) on table returns 100 rows where 0 is expected


Full repro 
{code}
create table orders_part_2 (O_ORDERKEY BIGINT,
 O_CUSTKEY BIGINT,
 O_ORDERSTATUS STRING,
 O_TOTALPRICE decimal(12,2),
 O_ORDERPRIORITY STRING,
 O_CLERK STRING,
 O_SHIPPRIORITY BIGINT,
 O_COMMENT STRING)
 partitioned by (O_ORDERDATE string)
 stored as parquet;

 insert into  etl_s3_parquet.orders_part_2 partition(o_orderdate) select O_ORDERKEY,O_CUSTKEY ,  O_ORDERSTATUS ,  O_TOTALPRICE ,  O_ORDERPRIORITY,  O_CLERK ,  O_SHIPPRIORITY,  O_COMMENT, O_ORDERDATE from  tpch_300_parquet_s3.orders limit 10000;
Query: insert into  etl_s3_parquet.orders_part_2 partition(o_orderdate) select O_ORDERKEY,O_CUSTKEY ,  O_ORDERSTATUS ,  O_TOTALPRICE ,  O_ORDERPRIORITY,  O_CLERK ,  O_SHIPPRIORITY,  O_COMMENT, O_ORDERDATE from  tpch_300_parquet_s3.orders limit 10000
WARNINGS: Cancelled (1 of 12 similar)

Inserted 10000 row(s) in 66.39s


alter table orders_part_2 set location 's3a://cloudera-impala-perf-w2/etl_s3_parquet.db/orders_part_2';

[impala-s3-scale-2.vpc.cloudera.com:21000] > select count(*) from orders_part_2;
Query: select count(*) from orders_part_2
+----------+
| count(*) |
+----------+
| 10000    |
+----------+
Fetched 1 row(s) in 1.20s

insert overwrite table  etl_s3_parquet.orders_part_2 partition(o_orderdate) select O_ORDERKEY,O_CUSTKEY ,  O_ORDERSTATUS ,  O_TOTALPRICE ,  O_ORDERPRIORITY,  O_CLERK ,  O_SHIPPRIORITY,  O_COMMENT, O_ORDERDATE from  tpch_300_parquet_s3.orders limit 0;
Query: insert overwrite table  etl_s3_parquet.orders_part_2 partition(o_orderdate) select O_ORDERKEY,O_CUSTKEY ,  O_ORDERSTATUS ,  O_TOTALPRICE ,  O_ORDERPRIORITY,  O_CLERK ,  O_SHIPPRIORITY,  O_COMMENT, O_ORDERDATE from  tpch_300_parquet_s3.orders limit 0
Inserted 0 row(s) in 0.28s

[impala-s3-scale-2.vpc.cloudera.com:21000] > select count(*) from etl_s3_parquet.orders_part_2;
Query: select count(*) from etl_s3_parquet.orders_part_2
+----------+
| count(*) |
+----------+
| 10000    |
+----------+
Fetched 1 row(s) in 1.29s
{code}"	IMPALA	Resolved	2	1	8925	function, hdfs, s3
13053279	S3: Use block size from FileStatus rather than default filesystem default block size	"Previously the file block size for a s3a file was 0 when derived from getFileStatus(). (see HADOOP-11584). We used the filesystem's default block size as a workaround for this. (in file catalog/HdfsTable.java)

However, now since HADOOP-11584 is fixed, we can get the block size from getFileStatus."	IMPALA	Resolved	4	4	8925	s3
13053500	Add a use_ssl option to compute_table_stats.py	The compute_table_stats.py script does not have a _'use_ssl'_ option. Due to this, the compute stats job that is run as a part of the nightly build on the secure perf cluster fails if SSL is configured on the cluster.	IMPALA	Resolved	3	4	8925	usability
13054397	Disable cache pool reader thread when HDFS isn't running	"When running Impala on s3 (without hdfs started) the logs every minute produce this error:
E0622 17:11:31.581317 24069 CatalogServiceCatalog.java:197] Error loading cache pools: 
Java exception follows:
java.lang.IllegalStateException
at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
at com.cloudera.impala.common.FileSystemUtil.getDistributedFileSystem(FileSystemUtil.java:329)
at com.cloudera.impala.catalog.CatalogServiceCatalog$CachePoolReader.run(CatalogServiceCatalog.java:190)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
I believe that this is because the catalog is checking for hdfs every minute: http://github.mtv.cloudera.com/CDH/Impala/blob/cdh5-trunk/fe/src/main/java/com/cloudera/impala/catalog/CatalogServiceCatalog.java#L163"	IMPALA	Resolved	3	1	8925	s3
13055060	Divide-by-zero in RuntimeProfile::SummaryStatsCounter::SetStats	"{{counter.total_num_values}} is 0 in the following code:

{code}
void RuntimeProfile::SummaryStatsCounter::SetStats(const TSummaryStatsCounter& counter) {
  lock_guard<SpinLock> l(lock_);
  unit_ = counter.unit;
  sum_ = counter.sum;
  total_num_values_ = counter.total_num_values;
  min_ = counter.min_value;
  max_ = counter.max_value;

  value_.Store(sum_ / total_num_values_);
}
{code}

{code}
#0  0x0000003bb7e328e5 in raise () from /lib64/libc.so.6
#1  0x0000003bb7e340c5 in abort () from /lib64/libc.so.6
#2  0x00007f169c39ec55 in os::abort(bool) () from /opt/toolchain/sun-jdk-64bit-1.7.0.75/jre/lib/amd64/server/libjvm.so
#3  0x00007f169c520cd7 in VMError::report_and_die() () from /opt/toolchain/sun-jdk-64bit-1.7.0.75/jre/lib/amd64/server/libjvm.so
#4  0x00007f169c3a3b6f in JVM_handle_linux_signal () from /opt/toolchain/sun-jdk-64bit-1.7.0.75/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  0x00000000015b469d in impala::RuntimeProfile::SummaryStatsCounter::SetStats (this=0x9aa55c0, counter=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/util/runtime-profile.cc:1047
#7  0x00000000015addc6 in impala::RuntimeProfile::Update (this=0x16fb8a80, nodes=..., idx=0x7f15d3dbfabc) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/util/runtime-profile.cc:307
#8  0x00000000015adfdf in impala::RuntimeProfile::Update (this=0xb41fb80, nodes=..., idx=0x7f15d3dbfabc) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/util/runtime-profile.cc:328
#9  0x00000000015ad342 in impala::RuntimeProfile::Update (this=0xb41fb80, thrift_profile=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/util/runtime-profile.cc:224
#10 0x000000000198c3e6 in impala::Coordinator::UpdateFragmentExecStatus (this=0x10e3f200, params=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/runtime/coordinator.cc:1454
#11 0x0000000001467961 in impala::ImpalaServer::ReportExecStatus (this=0x855d200, return_val=..., params=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/service/impala-server.cc:1093
#12 0x0000000001476f81 in impala::ImpalaInternalService::ReportExecStatus (this=0x8dd9120, return_val=..., params=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/service/impala-internal-service.h:55
#13 0x00000000018fc9a4 in impala::ImpalaInternalServiceProcessor::process_ReportExecStatus (this=0x8487480, seqid=0, iprot=0x8f89110, oprot=0x8f89140, callContext=0x8f8b040) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:1451
#14 0x00000000018fbf8a in impala::ImpalaInternalServiceProcessor::dispatchCall (this=0x8487480, iprot=0x8f89110, oprot=0x8f89140, fname=..., seqid=0, callContext=0x8f8b040) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/generated-sources/gen-cpp/ImpalaInternalService.cpp:1370
#15 0x000000000115ac9c in apache::thrift::TDispatchProcessor::process (this=0x8487480, in=..., out=..., connectionContext=0x8f8b040) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/thrift-0.9.0-p8/include/thrift/TDispatchProcessor.h:121
#16 0x0000000001322869 in apache::thrift::server::TAcceptQueueServer::Task::run (this=0x8efbb00) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/rpc/TAcceptQueueServer.cpp:76
#17 0x000000000131e947 in impala::ThriftThread::RunRunnable (this=0x8f8a1c0, runnable=..., promise=0x7f16482b6460) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/rpc/thrift-thread.cc:64
#18 0x0000000001320073 in boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator() (this=0x7fb83c0, p=0x8f8a1c0, a1=..., a2=0x7f16482b6460) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/bind/mem_fn_template.hpp:280
#19 0x000000000131ff09 in boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0> (this=0x7fb83d0, f=..., a=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:392
#20 0x000000000131fc55 in boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator() (this=0x7fb83c0) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#21 0x000000000131fb68 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >, void>::invoke (function_obj_ptr=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:153
#22 0x000000000132d4d4 in boost::function0<void>::operator() (this=0x7f15d3dc0c40) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/function/function_template.hpp:767
#23 0x00000000015d6253 in impala::Thread::SuperviseThread (name=..., category=..., functor=..., thread_started=0x7f16482b6250) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/repos/Impala/be/src/util/thread.cc:317
#24 0x00000000015dd22c in boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> >::operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(const std::basic_string<char, std::char_traits<char>, std::allocator<char> > &, const std::basic_string<char, std::char_traits<char>, std::allocator<char> > &, boost::function<void()>, impala::Promise<long> *), boost::_bi::list0 &, int) (this=0x7c329c0, f=@0x7c329b8, a=...) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind.hpp:457
#25 0x00000000015dd16f in boost::_bi::bind_t<void, void (*)(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > >::operator()(void) (this=0x7c329b8) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/bind/bind_template.hpp:20
#26 0x00000000015dd0ca in boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list4<boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >, boost::_bi::value<boost::function<void()> >, boost::_bi::value<impala::Promise<long int>*> > > >::run(void) (this=0x7c32800) at /data/jenkins/workspace/impala-umbrella-build-and-test-isilon/Impala-Toolchain/boost-1.57.0/include/boost/thread/detail/thread.hpp:116
{code}"	IMPALA	Resolved	1	1	8925	broken-build
13054405	Make ASF repo tests pass in a fresh install and when switching from cloudera Impala	Now that IMPALA-3223 is mostly working, we need to make sure the tests pass in both a fresh install and in a repo that also has a github.com/cloudera/Impala remote.	IMPALA	Resolved	2	7	8925	asf, asf-milestone-1
13053416	Query hang up if remote impalad hosts shut down	"I test impala2.3 in a 5 hosts clusterand 3 of them running impalad. Sometimes when I shut down 2 impalad hosts, the query hang up. This situation is rarely seen. By checking the impalad log and tcp connection information (through lsof), I found that when I shut down the 2 remote impalad hosts, the local impalad, i.e. the impalad accepting the query request, disconnected tcp connection with one of the 2 remote impalad, but still had tcp connection with the other one of the 2 remote impalad, and the query hang up. Every time the query hang up, the execution state is 'STARTED', and the last event is 'Ready to start remote fragments', and I cannot cancel the query. 

BTW, I modified default tcp keepalive parameters, include setting net.ipv4.tcp_keepalive_time=30, net.ipv4.tcp_keepalive_probes=3 and  net.ipv4.tcp_keepalive_intvl=10. This means if the tcp server is unreachable, keepalive settings guarantee the tcp client disconnecting the tcp connection actively after 30+3*10=60 seconds, but it seems it do not.

Following is the log related to the hang up query.

{noformat}
I1223 19:15:36.448956 23603 coordinator.cc:315] Exec() query_id=1542be5811b01f41:4624e416aa592b8c
I1223 19:15:36.449033 23603 plan-fragment-executor.cc:85] Prepare(): query_id=1542be5811b01f41:4624e416aa592b8c instance_id=1542be5811b01f41:4624e416aa592b8d
I1223 19:15:36.449177 23603 plan-fragment-executor.cc:193] descriptor table for fragment=1542be5811b01f41:4624e416aa592b8d
tuples:
Tuple(id=0 size=24 slots=[Slot(id=0 type=STRING col_path=[3] offset=8 null=(offset=0 mask=2) slot_idx=1 field_idx=-1), Slot(id=1 type=INT col_path=[1] offset=4 null=(offset=0 mask=1) slot_idx=0 field_idx=-1)] tuple_path=[])
I1223 19:15:36.449282 23603 coordinator.cc:391] starting 3 backends for query 1542be5811b01f41:4624e416aa592b8c
I1223 19:15:36.450311 24554 fragment-mgr.cc:36] ExecPlanFragment() instance_id=1542be5811b01f41:4624e416aa592b8f coord=vm3:22000 backend#=1
I1223 19:15:36.450402 24554 plan-fragment-executor.cc:85] Prepare(): query_id=1542be5811b01f41:4624e416aa592b8c instance_id=1542be5811b01f41:4624e416aa592b8f
I1223 19:15:36.450562 24554 plan-fragment-executor.cc:193] descriptor table for fragment=1542be5811b01f41:4624e416aa592b8f
tuples:
Tuple(id=0 size=24 slots=[Slot(id=0 type=STRING col_path=[3] offset=8 null=(offset=0 mask=2) slot_idx=1 field_idx=-1), Slot(id=1 type=INT col_path=[1] offset=4 null=(offset=0 mask=1) slot_idx=0 field_idx=-1)] tuple_path=[])
I1223 19:15:36.700852 21520 plan-fragment-executor.cc:303] Open(): instance_id=1542be5811b01f41:4624e416aa592b8f
I1223 19:16:15.860250 20878 thrift-util.cc:109] TSocket::read() recv() <Host: ::ffff:192.168.7.115 Port: 45152>Connection reset by peer
I1223 19:16:15.860384 20878 thrift-util.cc:109] TThreadedServer client died: ECONNRESET
I1223 19:16:16.463649 20879 thrift-util.cc:109] TSocket::read() recv() <Host: ::ffff:192.168.7.114 Port: 49091>Connection reset by peer
I1223 19:16:16.463825 20879 thrift-util.cc:109] TThreadedServer client died: ECONNRESET
I1223 19:19:35.522938 22979 status.cc:112] Cancelled from Impala's debug web interface
    @           0x788a33  impala::Status::Status()
    @           0x9e34ea  impala::ImpalaServer::CancelQueryUrlCallback()
    @           0xae1bd1  impala::Webserver::RenderUrlWithTemplate()
    @           0xae2a61  impala::Webserver::BeginRequestCallback()
    @           0xaf2e03  handle_request
    @           0xaf45a7  process_new_connection
    @           0xaf4dd8  worker_thread
    @       0x31aa6079d1  (unknown)
    @       0x31a9ee8b6d  (unknown)
I1223 19:19:35.522980 22979 impala-server.cc:862] UnregisterQuery(): query_id=1542be5811b01f41:4624e416aa592b8c
I1223 19:19:35.523000 22979 impala-server.cc:943] Cancel(): query_id=1542be5811b01f41:4624e416aa592b8c
I1223 19:19:35.575162 22979 status.cc:112] Query not yet running
    @           0x788a33  impala::Status::Status()
    @           0x9ba69f  impala::ImpalaServer::CancelInternal()
    @           0x9c2a17  impala::ImpalaServer::UnregisterQuery()
    @           0x9e3510  impala::ImpalaServer::CancelQueryUrlCallback()
    @           0xae1bd1  impala::Webserver::RenderUrlWithTemplate()
    @           0xae2a61  impala::Webserver::BeginRequestCallback()
    @           0xaf2e03  handle_request
    @           0xaf45a7  process_new_connection
    @           0xaf4dd8  worker_thread
    @       0x31aa6079d1  (unknown)
    @       0x31a9ee8b6d  (unknown)
{noformat}"	IMPALA	Resolved	4	1	8925	hang
13053103	investigate parquet multiple row-groups performance	We need to get perf numbers for the multiple row groups patch.  Please work with Yanpei to get these numbers as soon as possible and post them here.	IMPALA	Resolved	4	1	8925	performance
13052727	numRows incorrect when table has TINYINT partitions	"Expected behaviour (note the SMALLINT partition):

{noformat}
[localhost:21000] > CREATE TABLE working (a String) PARTITIONED BY (b SMALLINT);
[localhost:21000] > INSERT INTO working (a, b) VALUES (""A"",1);
[localhost:21000] > INSERT INTO working (a, b) VALUES (""B"",1);
[localhost:21000] > INSERT INTO working (a, b) VALUES (""C"",2);
[localhost:21000] > COMPUTE STATS working;
[localhost:21000] > SHOW TABLE STATS working;
Query: show TABLE STATS working
+-------+-------+--------+------+--------------+-------------------+--------+-------------------+
| b     | #Rows | #Files | Size | Bytes Cached | Cache Replication | Format | Incremental stats |
+-------+-------+--------+------+--------------+-------------------+--------+-------------------+
| 1     | 2     | 2      | 4B   | NOT CACHED   | NOT CACHED        | TEXT   | false             |
| 2     | 1     | 1      | 2B   | NOT CACHED   | NOT CACHED        | TEXT   | false             |
| Total | 3     | 3      | 6B   | 0B           |                   |        |                   |
+-------+-------+--------+------+--------------+-------------------+--------+-------------------+
{noformat}


Now the same steps with a TINYINT partition type:

{noformat}
[localhost:21000] > CREATE TABLE broken (a String) PARTITIONED BY (b TINYINT);
[localhost:21000] > INSERT INTO broken (a, b) VALUES (""A"",1);
[localhost:21000] > INSERT INTO broken (a, b) VALUES (""B"",1);
[localhost:21000] > INSERT INTO broken (a, b) VALUES (""C"",2);
[localhost:21000] > COMPUTE STATS broken;
[localhost:21000] > SHOW TABLE STATS broken;
Query: show TABLE STATS broken
+-------+-------+--------+------+--------------+-------------------+--------+-------------------+
| b     | #Rows | #Files | Size | Bytes Cached | Cache Replication | Format | Incremental stats |
+-------+-------+--------+------+--------------+-------------------+--------+-------------------+
| 1     | 0     | 2      | 4B   | NOT CACHED   | NOT CACHED        | TEXT   | false             |
| 2     | 0     | 1      | 2B   | NOT CACHED   | NOT CACHED        | TEXT   | false             |
| Total | 3     | 3      | 6B   | 0B           |                   |        |                   |
+-------+-------+--------+------+--------------+-------------------+--------+-------------------+
{noformat}

Notice that all the partitions have numRows=0. The incorrect number of rows can negatively impact the query planning."	IMPALA	Resolved	2	1	8925	compute-stats, performance, ramp-up
13053843	Ensure that tests and dataloading can be run efficiently w/o Cloudera infra	We need to confirm that Impala's dataloading phase and test suite can be run _efficiently_ without any access to Cloudera infrastructure. It's one thing to ensure that they can run, but we must also make sure that it's easy and cheap to do so, particularly the dataloading. We can't require new contributors to run a three-hour data load before they can run tests :)	IMPALA	Resolved	4	7	8925	asf, asf-milestone-2
13053760	Support inserts to a multi-partitioned table that lives on multiple filesystems	Currently our INSERT logic assumes that the table we're writing to lives only on one filesystem. However, the table could have different partitions on different filesystems. Adjust the logic in Coordinator::FinalizeSuccessfulInsert() so that this becomes possible.	IMPALA	Resolved	3	7	8925	s3, supportability
13053316	Handle exceptions thrown by TSSLSocket in TSaslTransport.cpp	We don't handle exceptions in TSaslTransport if thrown by the thrift transport protocols which thereby lead to IMPALA-2598.	IMPALA	Resolved	3	1	8925	impala, security
13051173	Cancelling INSERT queries causes  file handle leak	"Cancelling INSERT queries seems to result in a memory leak based on the value reported by the 'impala-server.mem-pool.total-bytes' metric. When this metric is ignored it also appears we are leaking HDFS file handles - the 'impala-server.num-files-open-for-insert' never goes back to zero.


{code}
query_test/test_cancellation.py:135: TestCancellationSerial.test_cancel_insert[table_format: text/none | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'batch_size': 0, 'num_nodes': 0} | query_type: CTAS | cancel_delay: 1 | action: None | query: select l_returnflag from lineitem] FAILED

============================================================================================================ FAILURES ============================================================================================================
 TestCancellationSerial.test_cancel_insert[table_format: text/none | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'batch_size': 0, 'num_nodes': 0} | query_type: CTAS | cancel_delay: 1 | action: None | query: select l_returnflag from lineitem] 
query_test/test_cancellation.py:139: in test_cancel_insert
>     metric_verifier.verify_metrics_are_zero(timeout=10)
verifiers/test_verify_metrics.py:42: in verify_metrics_are_zero
>       self.__wait_for_metric_value(metric, 0)
verifiers/test_verify_metrics.py:52: in __wait_for_metric_value
>         metric_name, expected_value, timeout)
common/impala_service.py:77: in wait_for_metric_value
>         (metric_name, expected_value, timeout)
E     AssertionError: Metric value impala-server.mem-pool.total-bytes did not reach value 0 in 60s
---------------------------------------------------------------------------------------------------------- Captured log ----------------------------------------------------------------------------------------------------------
impala_service.py           61 INFO     Getting metric: impala-server.backends.client-cache.clients-in-use from localhost:25000
impala_service.py           69 INFO     Metric 'impala-server.backends.client-cache.clients-in-use' has reach desired value: 0
impala_service.py           61 INFO     Getting metric: impala-server.io-mgr.num-open-files from localhost:25000
impala_service.py           69 INFO     Metric 'impala-server.io-mgr.num-open-files' has reach desired value: 0
impala_service.py           61 INFO     Getting metric: impala-server.hash-table.total-bytes from localhost:25000
impala_service.py           69 INFO     Metric 'impala-server.hash-table.total-bytes' has reach desired value: 0
impala_service.py           61 INFO     Getting metric: impala-server.io-mgr.num-open-files from localhost:25000
impala_service.py           69 INFO     Metric 'impala-server.io-mgr.num-open-files' has reach desired value: 0
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 681984
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 785408
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 681984
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 707584
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 681984
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 23152
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 23152
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 23152
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 23152
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 23152
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 23152
impala_service.py           74 INFO     Sleeping 1s before next retry.
impala_service.py           61 INFO     Getting metric: impala-server.mem-pool.total-bytes from localhost:25000
impala_service.py           73 INFO     Waiting for metric value 'impala-server.mem-pool.total-bytes'=0. Current value: 23152
impala_service.py           74 INFO     Sleeping 1s before next retry.
{code}"	IMPALA	Resolved	2	1	8925	resource-management
13054511	Impala-shell prints incorrect address for coordinator	"When running impala-shell on local coordinator the incorrect IP address is printed. 
Host name is more likely to work

{code}
Fetched 0 row(s) in 0.07s
Query: insert overwrite table customer select * from tpcds_10000_text.customer
Query submitted at: 2016-07-20 18:41:33 (Coordinator: http://0.0.0.0:25000)
Query progress can be monitored at: http://0.0.0.0:25000/query_plan?query_id=4746846e5fd0ea34:12390b0794aa9893
Inserted 65000000 row(s) in 15.80s
Query: drop table if exists customer_address
Query submitted at: 2016-07-20 18:41:48 (Coordinator: http://0.0.0.0:25000)
Query progress can be monitored at: http://0.0.0.0:25000/query_plan?query_id=1f46697924d5b51b:c1528b654683d83
Query: create table customer_address like tpcds_10000_text.customer_address stored as parquetfile
Query submitted at: 2016-07-20 18:41:48 (Coordinator: http://0.0.0.0:25000)
Query progress can be monitored at: http://0.0.0.0:25000/query_plan?query_id=3a4b0d39b143dbb3:1f4ffbedc85b381
{code}

Verbose output can't be disabled and it is on default  

{code}
  -V, --verbose         Verbose output [default: True]

impala-shell -c -i d2401 --verbose=false
Usage: impala_shell.py [options]

impala_shell.py: error: --verbose option does not take a value
{code}"	IMPALA	Resolved	3	1	8925	supportability
13054160	crash in DiskIoMgr::GetFreeBuffer	"Impala becomes unresponsive, causing a full outage for our reporting system. Impalad logs show various errors as follows:

{noformat}
Time	Log Level	Source	Log Message
May 16, 3:20:59.953 PM	ERROR	logging.cc:120	
stderr will be logged to this file.
May 16, 3:27:38.652 PM	ERROR	data-stream-sender.cc:261	
channel send status: 
Sender timed out waiting for receiver fragment instance: a045d3abcd1bed4f:a116563837c8ebda
May 16, 3:27:39.738 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:44.784 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:46.055 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:46.371 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:46.906 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:51.003 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:52.479 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-master-1-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:54.227 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:54.298 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-master-1-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:54.305 PM	ERROR	data-stream-sender.cc:261	
channel send status: 
Sender timed out waiting for receiver fragment instance: 7446d34d9eefb9f:436f61bcd3c78eb3
May 16, 3:27:55.022 PM	ERROR	data-stream-sender.cc:261	
channel send status: 
Sender timed out waiting for receiver fragment instance: b44445bcb782b5a4:5cef13b26af20aa1
May 16, 3:27:55.646 PM	ERROR	data-stream-sender.cc:261	
channel send status: 
Sender timed out waiting for receiver fragment instance: 3249c6494703ecc7:3b9186fbed00dc
May 16, 3:27:56.042 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:56.277 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-master-1-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:56.749 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-worker-6-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:57.126 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-master-1-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:27:59.030 PM	ERROR	data-stream-sender.cc:261	
channel send status: Couldn't open transport for ux-reporting-engine-master-1-prod-us-east-1a:22000 (connect() failed: Connection timed out)
May 16, 3:28:03.573 PM	ERROR	data-stream-sender.cc:261	
channel send status: 
Sender timed out waiting for receiver fragment instance: c64b22033608d609:10e070d34aa299dd
{noformat}"	IMPALA	Resolved	2	1	8925	crash, hang
13054104	test_ddl.py failure on LocalFS run	"On LocalFS runs, we usually skip any test that uses the hdfs_client because the hdfs_client cannot access the local filesystem.

However, we've had a bug in test_ddl.py for a long time where even though we skipped tests which used the hdfs_client, the setup_method() and teardown_method() used the hdfs_client.

{code:java}
  def setup_method(self, method):
    self._cleanup()

  def teardown_method(self, method):
    self._cleanup()

  def _cleanup(self):
    map(self.cleanup_db, self.TEST_DBS)
    # Cleanup the test table HDFS dirs between test runs so there are no errors the next
    # time a table is created with the same location. This also helps remove any stale
    # data from the last test run.
    for dir_ in ['part_data', 't1_tmp1', 't_part_tmp']:
      self.hdfs_client.delete_file_dir('test-warehouse/%s' % dir_, recursive=True)
{code}

The only reason this never failed before was because of the way we used to check if files existed in hdfs_client.delete_file_dir():

{code:java}
  def delete_file_dir(self, path, recursive=False):
    """"""Deletes a file or a dir if it exists.
    Overrides the superclass's method by providing delete if exists semantics. This takes
    the burden of stat'ing the file away from the caller.
    """"""
    try:
      self.get_file_dir_status(path)
    except Exception as e:
      return True
    return super(PyWebHdfsClientWithChmod, self).delete_file_dir(path,
        recursive=recursive)
{code}

Since we only caught a generic exception, this function always returned 'True' and passed. It actually threw a ConnectionError exception.

As a part of IMPALA-1878, I added a hdfs_client.exists() function which did the right thing by only catching 'FileNotFound' Exception.

This change exposed this bug on localFS runs causing them to fail."	IMPALA	Resolved	1	1	8925	broken-build, test-infra
13053216	Pseudo-random sleep before acquiring kerberos ticket possibly not really pseudo-random.	"According to the code in SaslAuthProvider::RunKinit(), before acquiring a new key, we sleep for the following amount of time:
_max(keberos_reinit_interval - *random(0 to 5 minutes)*, 60)_ seconds

Looking at the logs from a secure cluster run, we observed that every impalad slept for the *same* amount of time which means that the pseudo-randomization code doesn't really achieve pseudo-randomness.

We suspect that it's because the _generator_ is not seeded during creation.

The whole point of adding the pseudo-randomization factor was to avoid impalad's from storming the KDC for a new ticket at the same time. So, this could have caused some of the earlier ""Cannot contact any KDC for realm 'xyz'"" errors. But it's hard to tell as it's not an easily reproducible error.

Need to confirm on a secure cluster. Will post an update once I do."	IMPALA	Resolved	3	1	8925	authentication, kerberos
13266892	Parallelise flush in data stream sender	"The data stream sender currently does a synchronous RPC to close each channel https://github.com/apache/impala/blob/d4648e8/be/src/runtime/krpc-data-stream-sender.cc#L565.

This is suboptimal because it serializes the network round-trips and takes sum(RTT) over all the destinations in the best case, where no data needs to be flushed or  2 * sum(RTT) in the worst case if all channels need to flush data.

If the RPCs were done asynchronously and overlapped with each other, we could get this down to 2 * max(RTT).

I'm including this as a subtask of multi-threading because this is going to scale poorly as the number of fragment instances increases."	IMPALA	Resolved	3	7	9911	perf, scalability
13132266	Change Mempool memory allocation size to be <1MB to avoid allocating from CentralFreeList	"While[~tlipcon] was investigating KRPCcontention he noticed thatMemPool::Allocate is doing 1MB allocations,which issomewhat of an anti-pattern with tcmalloc.
During the tests MemPool was doing several thousand 1MB allocs per secondand those have to do a full scan of the tcmalloc span linked list, which is very slow and only gets slower
1040384 bytes on the other hand is constant time.

It is not clear if a Power of 2 allocation size would help, worth experimenting with 512KB and1040384 bytes.
{code}

/// The maximum size of chunk that should be allocated. Allocations larger than this
/// size will get their own individual chunk.
static const int MAX_CHUNK_SIZE = 8192*127

{code}



{code}

#0 0x0000000002097407 in base::internal::SpinLockDelay(int volatile*, int, int) ()
#1 0x00000000020e2049 in SpinLock::SlowLock() ()
#2 0x0000000002124348 in tcmalloc::CentralFreeList::Populate() ()
#3 0x0000000002124458 in tcmalloc::CentralFreeList::FetchFromOneSpansSafe(int, void**, void**) ()
#4 0x00000000021244e8 in tcmalloc::CentralFreeList::RemoveRange(void**, void**, int) ()
#5 0x0000000002131ee5 in tcmalloc::ThreadCache::FetchFromCentralCache(unsigned int, int, void* (*)(unsigned long)) ()
#6 0x0000000000b2879a in impala::MemPool::FindChunk(long, bool) ()
#7 0x0000000000b364f6 in impala::MemPool::Allocate(long) ()
#8 0x0000000000b36674 in impala::FreePool::Allocate(long) ()
#9 0x0000000000b353db in impala::RowBatch::Deserialize(kudu::Slice const&, kudu::Slice const&, long, bool, impala::FreePool*) ()
#10 0x0000000000b35795 in impala::RowBatch::RowBatch(impala::RowDescriptor const*, impala::RowBatchHeaderPB const&, kudu::Slice const&, kudu::Slice const&, impala::FreePool*) ()
#11 0x0000000000b1644f in impala::KrpcDataStreamRecvr::SenderQueue::AddBatchWork(long, impala::RowBatchHeaderPB const&, kudu::Slice const&, kudu::Slice const&, boost::unique_lock<impala::SpinLock>*) ()
#12 0x0000000000b19135 in impala::KrpcDataStreamRecvr::SenderQueue::AddBatch(impala::TransmitDataRequestPB const*, kudu::rpc::RpcContext*) ()
#13 0x0000000000b0ee30 in impala::KrpcDataStreamMgr::AddData(impala::TransmitDataRequestPB const*, kudu::rpc::RpcContext*) ()
#14 0x0000000001187035 in kudu::rpc::GeneratedServiceIf::Handle(kudu::rpc::InboundCall*) ()
#15 0x00000000011bc1cd in impala::ImpalaServicePool::RunThread(long) ()

{code}



Also it appears that the thread above was a victim of thread below, yet allocations <1MB will make MemPool::Allocate content less over theCentralFreeList lock.

{code}

#0 0x0000003173ae5407 in madvise () from /lib64/libc.so.6
#1 0x0000000002131cca in TCMalloc_SystemRelease(void*, unsigned long) ()
#2 0x000000000212f26a in tcmalloc::PageHeap::DecommitSpan(tcmalloc::Span*) ()
#3 0x000000000212f505 in tcmalloc::PageHeap::MergeIntoFreeList(tcmalloc::Span*) ()
#4 0x000000000212f864 in tcmalloc::PageHeap::Delete(tcmalloc::Span*) ()
#5 0x0000000002123cf7 in tcmalloc::CentralFreeList::ReleaseToSpans(void*) ()
#6 0x0000000002123d9b in tcmalloc::CentralFreeList::ReleaseListToSpans(void*) ()
#7 0x0000000002124067 in tcmalloc::CentralFreeList::InsertRange(void*, void*, int) ()
#8 0x00000000021320a4 in tcmalloc::ThreadCache::ReleaseToCentralCache(tcmalloc::ThreadCache::FreeList*, unsigned int, int) ()
#9 0x0000000002132575 in tcmalloc::ThreadCache::ListTooLong(tcmalloc::ThreadCache::FreeList*, unsigned int) ()
#10 0x0000000000b276e0 in impala::MemPool::FreeAll() ()
#11 0x0000000000b34655 in impala::RowBatch::Reset() ()
#12 0x0000000000fe882f in impala::PartitionedAggregationNode::GetRowsStreaming(impala::RuntimeState*, impala::RowBatch*) ()
#13 0x0000000000fe9771 in impala::PartitionedAggregationNode::GetNext(impala::RuntimeState*, impala::RowBatch*, bool*) ()
#14 0x0000000000b78352 in impala::FragmentInstanceState::ExecInternal() ()
#15 0x0000000000b7adc2 in impala::FragmentInstanceState::Exec() ()
#16 0x0000000000b6a0da in impala::QueryState::ExecFInstance(impala::FragmentInstanceState*) ()

{code}"	IMPALA	Resolved	3	4	9911	performance
13234034	Test failure in TestAdmissionControllerStress.test_mem_limit	"We had a recent test failure in test_mem_limit: 

{noformat}
custom_cluster/test_admission_controller.py:1504: in test_mem_limit
    {'request_pool': self.pool_name, 'mem_limit': query_mem_limit})
custom_cluster/test_admission_controller.py:1365: in run_admission_test
    assert metric_deltas['dequeued'] == 0,\
E   AssertionError: Queued queries should not run until others are made to finish
E   assert 3 == 0
{noformat}

{noformat}
assert 7 >= 10  +  where 10 = min((30 - 5), 10)

Stacktrace

custom_cluster/test_admission_controller.py:1504: in test_mem_limit
    {'request_pool': self.pool_name, 'mem_limit': query_mem_limit})
custom_cluster/test_admission_controller.py:1384: in run_admission_test
    assert queries_page_num_queued >=\
E   assert 7 >= 10
E    +  where 10 = min((30 - 5), 10)
{noformat}


It looks like this was a release build.


 [^test1.txt] was master
 [^test2.txt] was 2.12 with a bunch of cloudera patches"	IMPALA	Resolved	2	1	9911	flaky
13063386	Impala may hang on empty row batch exchange	"Hang reproduction:

Start impala with the following stress timeouts (the same as those in test_exchange_delays):
{code}
bin/start-impala-cluster.py --impalad_args=--stress_datastream_recvr_delay_ms=10000 --impalad_args=--datastream_sender_timeout_ms=5000
{code}

Do a join where 0 rows will be exchanged:
{code}
select count(*) from tpch.lineitem inner join tpch.orders on l_orderkey=o_orderkey where o_orderkey < 0
{code}

When the data-stream-sender sends 0 rows, it sends it with params.row_batch.num_rows=0 and params.eos=true.

The receiver hits the following case:
https://github.com/apache/incubator-impala/blob/master/be/src/service/impala-server.cc#L1133
It ultimately times out from FindRecvrOrWait() after datastream_sender_timeout_ms.
https://github.com/apache/incubator-impala/blob/0d0c93ec8c4949940ec113192731f2adb66a0c5e/be/src/runtime/data-stream-mgr.cc#L123

Due to stress_datastream_recvr_delay_ms, the exchange node is Prepare()'d even later.
https://github.com/apache/incubator-impala/blob/a50c344077f6c9bbea3d3cbaa2e9146ba20ac9a9/be/src/exec/exchange-node.cc#L74

Although this is a testing flag, this behavior is reasonable to expect in a real world setting under heavy stress, since the stress_datastream_recvr_delay_ms flag was added into testing after some users experienced nodes receiving data from DataStreamSenders and timing out after datastream_sender_timeout_ms, and before the ExchangeNodes were created. Thus, the CloseSender() function is called before the exchange node is created.

After stress_datastream_recvr_delay_ms, the ExchangeNode is created and it calls CreateRecvr().
https://github.com/apache/incubator-impala/blob/a50c344077f6c9bbea3d3cbaa2e9146ba20ac9a9/be/src/exec/exchange-node.cc#L80

Then ExchangeNode::Open() calls DataStreamRecvr::CreateMerger() which makes a few threads wait on DataStreamRecvr::SenderQueue::GetBatch().
https://github.com/apache/incubator-impala/blob/0d0c93ec8c4949940ec113192731f2adb66a0c5e/be/src/runtime/data-stream-recvr.cc#L124

However, since the DataStreamSender has already received the timeout error from earlier, the receiver threads in GetBatch() will wait indefinitely until the query is cancelled by the user."	IMPALA	Resolved	2	1	9911	hang
13196687	Crash in repeat() constructing strings > 1GB	"{noformat}
select repeat('x', 1024 * 1024 * 1024 * 1024);
{noformat}

{noformat}
#0  0x0000000004551aa3 in google_breakpad::ExceptionHandler::SignalHandler(int, siginfo_t*, void*) ()
#1  0x00007f72cdc0f362 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#2  0x00007f72cdc138b9 in JVM_handle_linux_signal ()
   from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#3  0x00007f72cdc06f78 in ?? () from /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/libjvm.so
#4  <signal handler called>
#5  0x00007f72cab8a06c in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#6  0x00000000030e19da in impala::StringFunctions::Repeat (context=0x721f800, str=..., n=...)
    at be/src/exprs/string-functions-ir.cc:100
#7  0x0000000003145c32 in impala::ScalarFnCall::InterpretEval<impala_udf::StringVal> (
    this=0xaf2cb40, eval=0xd26ccc0, row=0x0) at be/src/exprs/scalar-fn-call.cc:485
#8  0x000000000312bd85 in impala::ScalarFnCall::GetStringVal (this=0xaf2cb40, eval=0xd26ccc0,
    row=0x0) at be/src/exprs/scalar-fn-call.cc:599
#9  0x00000000030da116 in impala::ScalarExprEvaluator::GetValue (this=0xd26ccc0, expr=..., row=0x0)
    at be/src/exprs/scalar-expr-evaluator.cc:299
#10 0x00000000030d9da5 in impala::ScalarExprEvaluator::GetValue (this=0xd26ccc0, row=0x0)
    at be/src/exprs/scalar-expr-evaluator.cc:250
#11 0x0000000001fc59bc in Java_org_apache_impala_service_FeSupport_NativeEvalExprsWithoutRow (
    env=0xd25b1e0, caller_class=0x7f724d523280, thrift_expr_batch=0x7f724d523298,
    thrift_query_ctx_bytes=0x7f724d523290) at be/src/service/fe-support.cc:237
{noformat}"	IMPALA	Resolved	2	1	9911	crash
13053098	DCHECK for memory leak in PHJ	"{code}
Log file created at: 2015/10/05 12:21:21
Running on machine: tarmstrong-box
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
F1005 12:21:21.494573  6057 exec-node.cc:190] Check failed: mem_tracker()->consumption() == 0 (65536 vs. 0) Leaked memory.
Fragment d8487a58dc42eab0:e23c15ddc2f16ec5: Consumption=124.00 KB
  AGGREGATION_NODE (id=31): Consumption=36.00 KB
  HASH_JOIN_NODE (id=30): Consumption=0
  HASH_JOIN_NODE (id=29): Consumption=0
  HASH_JOIN_NODE (id=28): Consumption=64.00 KB
  HDFS_SCAN_NODE (id=24): Consumption=0
  EXCHANGE_NODE (id=88): Consumption=0
  EXCHANGE_NODE (id=89): Consumption=0
  DataStreamRecvr: Consumption=0
  EXCHANGE_NODE (id=90): Consumption=0
  DataStreamRecvr: Consumption=0
  DataStreamSender: Consumption=16.00 KB
{code}

I hit the crash after running these two queries in a loop for several minutes. I had two concurrent clients running the same queries.
{code}
use tpcds_parquet;

select  *
from
 (select count(*) h8_30_to_9
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 8
     and time_dim.t_minute >= 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s1,
 (select count(*) h9_to_9_30
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 9
     and time_dim.t_minute < 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s2,
 (select count(*) h9_30_to_10
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 9
     and time_dim.t_minute >= 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s3,
 (select count(*) h10_to_10_30
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 10
     and time_dim.t_minute < 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s4,
 (select count(*) h10_30_to_11
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 10
     and time_dim.t_minute >= 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s5,
 (select count(*) h11_to_11_30
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 11
     and time_dim.t_minute < 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s6,
 (select count(*) h11_30_to_12
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 11
     and time_dim.t_minute >= 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s7,
 (select count(*) h12_to_12_30
 from store_sales, household_demographics , time_dim, store
 where ss_sold_time_sk = time_dim.t_time_sk
     and ss_hdemo_sk = household_demographics.hd_demo_sk
     and ss_store_sk = s_store_sk
     and time_dim.t_hour = 12
     and time_dim.t_minute < 30
     and ((household_demographics.hd_dep_count = 1 and household_demographics.hd_vehicle_count<=1+2) or
          (household_demographics.hd_dep_count = 0 and household_demographics.hd_vehicle_count<=0+2) or
          (household_demographics.hd_dep_count = 2 and household_demographics.hd_vehicle_count<=2+2))
     and store.s_store_name = 'ese') s8;





select promotions,total,cast(promotions as decimal(15,4))/cast(total as decimal(15,4))*100
from
  (select sum(ss_ext_sales_price) promotions
   from  store_sales
        ,store
        ,promotion
        ,date_dim
        ,customer
        ,customer_address
        ,item
   where ss_sold_date_sk = d_date_sk
   and   ss_store_sk = s_store_sk
   and   ss_promo_sk = p_promo_sk
   and   ss_customer_sk= c_customer_sk
   and   ca_address_sk = c_current_addr_sk
   and   ss_item_sk = i_item_sk
   and   ca_gmt_offset = -5
   and   i_category = 'Books'
   and   (p_channel_dmail = 'Y' or p_channel_email = 'Y' or p_channel_tv = 'Y')
   and   s_gmt_offset = -5
   and   d_year = 2000
   and   d_moy  = 11) promotional_sales,
  (select sum(ss_ext_sales_price) total
   from  store_sales
        ,store
        ,date_dim
        ,customer
        ,customer_address
        ,item
   where ss_sold_date_sk = d_date_sk
   and   ss_store_sk = s_store_sk
   and   ss_customer_sk= c_customer_sk
   and   ca_address_sk = c_current_addr_sk
   and   ss_item_sk = i_item_sk
   and   ca_gmt_offset = -5
   and   i_category = 'Books'
   and   s_gmt_offset = -5
   and   d_year = 2000
   and   d_moy  = 11) all_sales
order by promotions, total;

{code}"	IMPALA	Resolved	2	1	9911	crash
13230747	Admit memory not set in backend descriptor for coordinator-only nodes	"Whenconfiguring admission control with dedicated coordinator daemons, queriesin pools with memory limits fail with theadmission rejections like thefollowing:
{noformat}
Rejected query from pool root.default: request memory needed 3.00 GB per node is greater than memory available for admission 0 of coord1.example.com:22000. Use the MEM_LIMIT query option to indicate how much memory is required per node.{noformat}
Tracing thisin the code leads us to line 576 of {{admission-controller.cc}} and therefore to suspect that the local {{TBackendDescriptor}} ({{local_backend_descriptor_}}) for the coordinator node in{{scheduler.cc}} never has {{admit_mem_limit}} set, and thus ends up with the default value of 0.

The issue goes away if NO_SPECIALIZATION is used instead of COORDINATOR_ONLY."	IMPALA	Resolved	2	1	9911	admission-control, resource-management
13055450	Clients can violate BufferPool invariants by calling ReservationTracker methods directly.	"If a client unpins some pages, then calls ReservationTracker::DecreaseReservation(), it can leave too many dirty unpinned pages in memory.

Similarly, a client can cause mischief by calling AllocateFrom() and ReleaseTo() or by creating child ReservationTrackers."	IMPALA	Resolved	3	7	9911	resource-management
13054198	Impala appears to be leaking memory in stress test	"Impala seems to be leaking memory in the stress test. I've noticed that the allocated memory has been gradually creeping up. One of the daemons is now sitting at this while no queries are running:

MALLOC:    14364384496 (13698.9 MiB) Bytes in use by application

Whereas it was a couple of gigabytes lower an hour or two ago."	IMPALA	Resolved	1	1	9911	resource-management
13162138	Selective Avro scans of wide tables use more memory than necessary	"We have some anecdotal evidence that this got worse somewhere between 2.7 and 2.10, but it may be because of a change in timing rather than a direct consequence of a code change. The scan uses ~500MB of memory, but should probably be running with less.

Analysis revealed that most of the memory was from the tuple data buffers, which were multiple mb because they had space for 1024 wide rows.

There are a couple of things we could do:
* Directly improve Avro selective scans to avoid transferring excessive memory (i.e. something analogous to what IMPALA-4923 did for Parquet)
* Tweak the row batch queueing to reduce unnecessary queueing. If the consumer is running slow, lots of batches pile up in the queue. We saw the regression on a system with 24 disks, so the queue was ~280 entries, which is excessive.

Repro is as follows:
{code}
-- In Hive
create table wide_250_cols (
  col0 STRING, col1 STRING, col2 STRING, col3 STRING, col4 STRING, col5 STRING,
  col6 STRING, col7 STRING, col8 STRING, col9 STRING, col10 STRING, col11 STRING,
  col12 STRING, col13 STRING, col14 STRING, col15 STRING, col16 STRING, col17 STRING,
  col18 STRING, col19 STRING, col20 STRING, col21 STRING, col22 STRING, col23 STRING,
  col24 STRING, col25 STRING, col26 STRING, col27 STRING, col28 STRING, col29 STRING,
  col30 STRING, col31 STRING, col32 STRING, col33 STRING, col34 STRING, col35 STRING,
  col36 STRING, col37 STRING, col38 STRING, col39 STRING, col40 STRING, col41 STRING,
  col42 STRING, col43 STRING, col44 STRING, col45 STRING, col46 STRING, col47 STRING,
  col48 STRING, col49 STRING, col50 STRING, col51 STRING, col52 STRING, col53 STRING,
  col54 STRING, col55 STRING, col56 STRING, col57 STRING, col58 STRING, col59 STRING,
  col60 STRING, col61 STRING, col62 STRING, col63 STRING, col64 STRING, col65 STRING,
  col66 STRING, col67 STRING, col68 STRING, col69 STRING, col70 STRING, col71 STRING,
  col72 STRING, col73 STRING, col74 STRING, col75 STRING, col76 STRING, col77 STRING,
  col78 STRING, col79 STRING, col80 STRING, col81 STRING, col82 STRING, col83 STRING,
  col84 STRING, col85 STRING, col86 STRING, col87 STRING, col88 STRING, col89 STRING,
  col90 STRING, col91 STRING, col92 STRING, col93 STRING, col94 STRING, col95 STRING,
  col96 STRING, col97 STRING, col98 STRING, col99 STRING, col100 STRING, col101 STRING,
  col102 STRING, col103 STRING, col104 STRING, col105 STRING, col106 STRING, col107 STRING,
  col108 STRING, col109 STRING, col110 STRING, col111 STRING, col112 STRING, col113 STRING,
  col114 STRING, col115 STRING, col116 STRING, col117 STRING, col118 STRING, col119 STRING,
  col120 STRING, col121 STRING, col122 STRING, col123 STRING, col124 STRING, col125 STRING,
  col126 STRING, col127 STRING, col128 STRING, col129 STRING, col130 STRING, col131 STRING,
  col132 STRING, col133 STRING, col134 STRING, col135 STRING, col136 STRING, col137 STRING,
  col138 STRING, col139 STRING, col140 STRING, col141 STRING, col142 STRING, col143 STRING,
  col144 STRING, col145 STRING, col146 STRING, col147 STRING, col148 STRING, col149 STRING,
  col150 STRING, col151 STRING, col152 STRING, col153 STRING, col154 STRING, col155 STRING,
  col156 STRING, col157 STRING, col158 STRING, col159 STRING, col160 STRING, col161 STRING,
  col162 STRING, col163 STRING, col164 STRING, col165 STRING, col166 STRING, col167 STRING,
  col168 STRING, col169 STRING, col170 STRING, col171 STRING, col172 STRING, col173 STRING,
  col174 STRING, col175 STRING, col176 STRING, col177 STRING, col178 STRING, col179 STRING,
  col180 STRING, col181 STRING, col182 STRING, col183 STRING, col184 STRING, col185 STRING,
  col186 STRING, col187 STRING, col188 STRING, col189 STRING, col190 STRING, col191 STRING,
  col192 STRING, col193 STRING, col194 STRING, col195 STRING, col196 STRING, col197 STRING,
  col198 STRING, col199 STRING, col200 STRING, col201 STRING, col202 STRING, col203 STRING,
  col204 STRING, col205 STRING, col206 STRING, col207 STRING, col208 STRING, col209 STRING,
  col210 STRING, col211 STRING, col212 STRING, col213 STRING, col214 STRING, col215 STRING,
  col216 STRING, col217 STRING, col218 STRING, col219 STRING, col220 STRING, col221 STRING,
  col222 STRING, col223 STRING, col224 STRING, col225 STRING, col226 STRING, col227 STRING,
  col228 STRING, col229 STRING, col230 STRING, col231 STRING, col232 STRING, col233 STRING,
  col234 STRING, col235 STRING, col236 STRING, col237 STRING, col238 STRING, col239 STRING,
  col240 STRING, col241 STRING, col242 STRING, col243 STRING, col244 STRING, col245 STRING,
  col246 STRING, col247 STRING, col248 STRING, col249 STRING
) stored as avro;

-- In Hive
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET mapred.output.compression.type=BLOCK;
SET hive.exec.compress.output=true;
SET avro.output.codec=snappy;

insert into wide_250_cols
select
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment
 
from tpch.lineitem;

-- In Impala
invalidate metadata wide_250_cols;
set num_scanner_threads=0; set batch_size=0; set live_summary=1; use default;
select * from wide_250_cols where col225 = 'foo' order by col244; summary;
/*
+---------------------+--------+----------+----------+-------+------------+-----------+---------------+-----------------------+
| Operator        	| #Hosts | Avg Time | Max Time | #Rows | Est. #Rows | Peak Mem  | Est. Peak Mem | Detail            	|
+---------------------+--------+----------+----------+-------+------------+-----------+---------------+-----------------------+
| 02:MERGING-EXCHANGE | 1  	| 0ns  	| 0ns  	| 0 	| -1     	| 0 B   	| 0 B       	| UNPARTITIONED     	|
| 01:SORT         	| 1  	| 0ns  	| 0ns  	| 0 	| -1     	| 24.52 MB  | 12.00 MB  	|                   	|
| 00:SCAN HDFS    	| 1  	| 68.63s   | 68.63s   | 0 	| -1     	| 433.95 MB | 704.00 MB 	| default.wide_250_cols |
+---------------------+--------+----------+----------+-------+------------+-----------+---------------+-----------------------+
- Setting batch_size reduces peak memory
- Setting num_scanner_threads to lower value reduces peak memory
- Setting --max_row_batches lower reduces peak memory.
- Setting --max_row_batches higher increase peak memory.
- Logging the batches added to the queue shows they have 3.94MB of memory in the MemPool
  and sometimes 1 8mb I/O buffer.
4 STRING, col15 STRING, col16 STRING, col17 STRING,
  col18 STRING, col19 STRING, col20 STRING, col21 STRING, col22 STRING, col23 STRING,
  col24 STRING, col25 STRING, col26 STRING, col27 STRING, col28 STRING, col29 STRING,
  col30 STRING, col31 STRING, col32 STRING, col33 STRING, col34 STRING, col35 STRING,
  col36 STRING, col37 STRING, col38 STRING, col39 STRING, col40 STRING, col41 STRING,
  col42 STRING, col43 STRING, col44 STRING, col45 STRING, col46 STRING, col47 STRING,
  col48 STRING, col49 STRING, col50 STRING, col51 STRING, col52 STRING, col53 STRING,
  col54 STRING, col55 STRING, col56 STRING, col57 STRING, col58 STRING, col59 STRING,
  col60 STRING, col61 STRING, col62 STRING, col63 STRING, col64 STRING, col65 STRING,
  col66 STRING, col67 STRING, col68 STRING, col69 STRING, col70 STRING, col71 STRING,
  col72 STRING, col73 STRING, col74 STRING, col75 STRING, col76 STRING, col77 STRING,
  col78 STRING, col79 STRING, col80 STRING, col81 STRING, col82 STRING, col83 STRING,
  col84 STRING, col85 STRING, col86 STRING, col87 STRING, col88 STRING, col89 STRING,
  col90 STRING, col91 STRING, col92 STRING, col93 STRING, col94 STRING, col95 STRING,
  col96 STRING, col97 STRING, col98 STRING, col99 STRING, col100 STRING, col101 STRING,
  col102 STRING, col103 STRING, col104 STRING, col105 STRING, col106 STRING, col107 STRING,
  col108 STRING, col109 STRING, col110 STRING, col111 STRING, col112 STRING, col113 STRING,
  col114 STRING, col115 STRING, col116 STRING, col117 STRING, col118 STRING, col119 STRING,
  col120 STRING, col121 STRING, col122 STRING, col123 STRING, col124 STRING, col125 STRING,
  col126 STRING, col127 STRING, col128 STRING, col129 STRING, col130 STRING, col131 STRING,
  col132 STRING, col133 STRING, col134 STRING, col135 STRING, col136 STRING, col137 STRING,
  col138 STRING, col139 STRING, col140 STRING, col141 STRING, col142 STRING, col143 STRING,
  col144 STRING, col145 STRING, col146 STRING, col147 STRING, col148 STRING, col149 STRING,
  col150 STRING, col151 STRING, col152 STRING, col153 STRING, col154 STRING, col155 STRING,
  col156 STRING, col157 STRING, col158 STRING, col159 STRING, col160 STRING, col161 STRING,
  col162 STRING, col163 STRING, col164 STRING, col165 STRING, col166 STRING, col167 STRING,
  col168 STRING, col169 STRING, col170 STRING, col171 STRING, col172 STRING, col173 STRING,
  col174 STRING, col175 STRING, col176 STRING, col177 STRING, col178 STRING, col179 STRING,
  col180 STRING, col181 STRING, col182 STRING, col183 STRING, col184 STRING, col185 STRING,
  col186 STRING, col187 STRING, col188 STRING, col189 STRING, col190 STRING, col191 STRING,
  col192 STRING, col193 STRING, col194 STRING, col195 STRING, col196 STRING, col197 STRING,
  col198 STRING, col199 STRING, col200 STRING, col201 STRING, col202 STRING, col203 STRING,
  col204 STRING, col205 STRING, col206 STRING, col207 STRING, col208 STRING, col209 STRING,
  col210 STRING, col211 STRING, col212 STRING, col213 STRING, col214 STRING, col215 STRING,
  col216 STRING, col217 STRING, col218 STRING, col219 STRING, col220 STRING, col221 STRING,
  col222 STRING, col223 STRING, col224 STRING, col225 STRING, col226 STRING, col227 STRING,
  col228 STRING, col229 STRING, col230 STRING, col231 STRING, col232 STRING, col233 STRING,
  col234 STRING, col235 STRING, col236 STRING, col237 STRING, col238 STRING, col239 STRING,
  col240 STRING, col241 STRING, col242 STRING, col243 STRING, col244 STRING, col245 STRING,
  col246 STRING, col247 STRING, col248 STRING, col249 STRING
) stored as avro;

-- In Hive
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET mapred.output.compression.type=BLOCK;
insert into wide_250_cols
select
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment, l_comment,
  l_comment, l_comment, l_comment, l_comment, l_comment
 
from tpch.lineitem;

-- In Impala
invalidate metadata wide_250_cols;
set num_scanner_threads=0; set batch_size=0; set live_summary=1; use default;
select * from wide_250_cols where col225 = 'foo' order by col244; summary;
/*
+---------------------+--------+----------+----------+-------+------------+-----------+---------------+-----------------------+
| Operator        	| #Hosts | Avg Time | Max Time | #Rows | Est. #Rows | Peak Mem  | Est. Peak Mem | Detail            	|
+---------------------+--------+----------+----------+-------+------------+-----------+---------------+-----------------------+
| 02:MERGING-EXCHANGE | 1  	| 0ns  	| 0ns  	| 0 	| -1     	| 0 B   	| 0 B       	| UNPARTITIONED     	|
| 01:SORT         	| 1  	| 0ns  	| 0ns  	| 0 	| -1     	| 24.52 MB  | 12.00 MB  	|                   	|
| 00:SCAN HDFS    	| 1  	| 68.63s   | 68.63s   | 0 	| -1     	| 433.95 MB | 704.00 MB 	| default.wide_250_cols |
+---------------------+--------+----------+----------+-------+------------+-----------+---------------+-----------------------+
- Setting batch_size reduces peak memory
- Setting num_scanner_threads to lower value reduces peak memory
- Setting --max_row_batches lower reduces peak memory.
- Setting --max_row_batches higher increase peak memory.
- Logging the batches added to the queue shows they have 3.94MB of memory in the MemPool
  and sometimes 1 8mb I/O buffer.
{code}"	IMPALA	Resolved	3	4	9911	avro, resource-management
13103032	Q35a and Q48 test files don't match standard qualification queries	"Results of q35a (in ttq.218.out) disagree with same query run on Postgres, Vertica and Netezza.

Results of q48 disagree with expected TPC-DS result data.  Q48 was passing for a long time, then this _intermittent_ failure began occurring at least by 10/5/2017, 16:17PT on Jenkins--see attached pre-review-consoleText.

https://github.com/gregrahn/tpcds-kit/tree/master/answer_sets"	IMPALA	Resolved	3	1	9911	TPCDS
13121116	disk-io-mgr-test hangs	In an exhaustive test build, disk-io-mgr-test hangs for 22 hours. The commit is c656181826848d71c6d53c51321769968660ddf7 (IMPALA-4927: Impala should handle invalid input from Sentry). IMPALA-6121 is merged before this commit and might be related.	IMPALA	Resolved	1	1	9911	broken-build
13209083	Clean up config files in docker containers	"Currently the docker containers include a bunch of config files copied indiscriminately from the dev environment. Mostly these aren't valid for a production container and it's expected that the real config files will be mounted at /opt/impala/conf.

We should instead include a more reasonable set of default configs (e.g. for admission control), plus placeholders for other config files that may need to be overridden with site-specific configs."	IMPALA	Resolved	3	7	9911	docker
13111076	TestHashJoinTimer failed on local filesystem and ASAN builds	"{noformat}
04:26:05  TestHashJoinTimer.test_hash_join_timer[test cases: ['select /*+straight_join*/ count(*) from (select distinct * from functional.alltypes where int_col >= sleep(5)) a cross join  functional.alltypes b where a.id > b.id and b.id=99', 'NESTED LOOP JOIN'] | exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 0, 'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0} | table_format: text/none] 
04:26:05 query_test/test_hash_join_timer.py:140: in test_hash_join_timer
04:26:05     assert (asyn_build), ""Join is not prepared asynchronously""
04:26:05 E   UnboundLocalError: local variable 'asyn_build' referenced before assignment
{noformat}

Saw this at commit: 2187d36... IMPALA-6045: Make build scripts more friendly to Ubuntu 16.04

The test seems fundamentally flaky, since the behaviour will change if there are enough other threads were running simultaneously and taking up threads that the query couldn't start an async build thread. It's worth thinking about how we can make this less flaky without losing coverage."	IMPALA	Resolved	2	1	9911	flaky
13053892	BufferedTupleStream::PrepareForRead() does not handle pin failure correctly	"PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:

{code}
      bool current_pinned;
      RETURN_IF_ERROR((*it)->Pin(&current_pinned));
      if (!current_pinned) {
        DCHECK(got_buffer != NULL) << ""Should have reserved enough blocks"";
        *got_buffer = false;
        return Status::OK();
      }
{code}"	IMPALA	Resolved	2	1	9911	resource-management
13140804	"test_compute_stats_tablesample failing with ""Cancelled"""	"https://jenkins.impala.io/job/ubuntu-16.04-from-scratch/1417
{noformat}
03:24:03  TestStatsExtrapolation.test_compute_stats_tablesample[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: text/none] 
03:24:03 [gw9] linux2 -- Python 2.7.12 /home/ubuntu/Impala/bin/../infra/python/env/bin/python
03:24:03 metadata/test_stats_extrapolation.py:108: in test_compute_stats_tablesample
03:24:03     self.__run_sampling_test(no_column_tbl, columns, part_test_tbl_base, 10, 7)
03:24:03 metadata/test_stats_extrapolation.py:131: in __run_sampling_test
03:24:03     .format(tbl, cols, perc, seed))
03:24:03 common/impala_connection.py:160: in execute
03:24:03     return self.__beeswax_client.execute(sql_stmt, user=user)
03:24:03 beeswax/impala_beeswax.py:173: in execute
03:24:03     handle = self.__execute_query(query_string.strip(), user=user)
03:24:03 beeswax/impala_beeswax.py:341: in __execute_query
03:24:03     self.wait_for_completion(handle)
03:24:03 beeswax/impala_beeswax.py:361: in wait_for_completion
03:24:03     raise ImpalaBeeswaxException(""Query aborted:"" + error_log, None)
03:24:03 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
03:24:03 E    Query aborted:Cancelled
03:24:03 ---------------------------- Captured stderr setup -----------------------------
03:24:03 -- connecting to: localhost:21000
03:24:03 SET sync_ddl=False;
03:24:03 -- executing against localhost:21000
03:24:03 DROP DATABASE IF EXISTS `test_compute_stats_tablesample_4f26fb11` CASCADE;
03:24:03 
03:24:03 SET sync_ddl=False;
03:24:03 -- executing against localhost:21000
03:24:03 CREATE DATABASE `test_compute_stats_tablesample_4f26fb11`;
03:24:03 
03:24:03 MainThread: Created database ""test_compute_stats_tablesample_4f26fb11"" for test ID ""metadata/test_stats_extrapolation.py::TestStatsExtrapolation::()::test_compute_stats_tablesample[exec_option: {'batch_size': 0, 'num_nodes': 0, 'disable_codegen_rows_threshold': 5000, 'disable_codegen': False, 'abort_on_error': 1, 'debug_action': None, 'exec_single_node_rows_threshold': 0} | table_format: text/none]""
03:24:03 ----------------------------- Captured stderr call -----------------------------
03:24:03 -- executing against localhost:21000
03:24:03 set compute_stats_min_sample_size=0;
03:24:03 
03:24:03 -- executing against localhost:21000
03:24:03 use functional;
03:24:03 
03:24:03 -- executing against localhost:21000
03:24:03 describe formatted functional.alltypes;
03:24:03 
03:24:03 -- executing against localhost:21000
03:24:03 create external table test_compute_stats_tablesample_4f26fb11.alltypes like functional.alltypes location 'hdfs://localhost:20500/test-warehouse/alltypes';
03:24:03 
03:24:03 -- executing against localhost:21000
03:24:03 alter table test_compute_stats_tablesample_4f26fb11.alltypes recover partitions;
03:24:03 
03:24:03 -- executing against localhost:21000
03:24:03 compute stats test_compute_stats_tablesample_4f26fb11.no_columns() tablesample system (10) repeatable (7);
03:24:03 
03:24:03 ======= 1 failed, 1817 passed, 64 skipped, 41 xfailed in 2456.05 seconds =======
{noformat}

This was probably introduced by one of my DiskIoMgr patches - I fixed failure with a similar symptoms earlier, which was caused by a race between scanner threads and the main thread issuing ranges, leading to an DiskIoMgr method returning CANCELLED. However, I'm not sure that a similar race is possible in the mt_dop > 1 scan node."	IMPALA	Resolved	1	1	9911	broken-build
13081927	Race in fragment instance teardown can lead to use-after-free in MemTracker::AnyLimitExceeded()	"There seems to be a race when queries are being cancelled and the client tries to fetch results at the same time. I used the following loop to provoke a use-after-free detected by asan:

{noformat}
while [ $? -eq 0 ]; do impala-py.test tests/query_test/test_scanners.py::TestParquet::test_corrupt_files --exploration_strategy=exhaustive -n8; done
{noformat}

To run this I had to make the following change to {{query_test/test_scanners.py

{noformat}
-  def test_corrupt_files(self, vector):
+  @pytest.mark.parametrize('multiplier', xrange(32))
+  def test_corrupt_files(self, vector, multiplier):
{noformat}

This it the logging produced by asan:
{noformat}
Log file created at: 2017/06/22 11:15:36
Running on machine: lv-desktop
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
E0622 11:15:36.630154 11764 logging.cc:124] stderr will be logged to this file.
=================================================================
==11764==ERROR: AddressSanitizer: heap-use-after-free on address 0x6120003d9bf8 at pc 0x7ff0abd21b24 bp 0x7fee287f49e0 sp 0x7fee287f49d8
READ of size 8 at 0x6120003d9bf8 thread T193
    #0 0x7ff0abd21b23 in __gnu_cxx::__normal_iterator<impala::MemTracker**, std::vector<impala::MemTracker*, std::allocator<impala::MemTracker*> > >::__normal_iterator(impala::MemTracker** const&) /opt/Impala-Toolchain/gcc-4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/../../../../include/c++/4.9.2/bits/stl_iterator.h:729:20
    #1 0x7ff0abd21a3b in std::vector<impala::MemTracker*, std::allocator<impala::MemTracker*> >::end() /opt/Impala-Toolchain/gcc-4.9.2/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/../../../../include/c++/4.9.2/bits/stl_vector.h:566:16
    #2 0x7ff0ab202faa in impala::MemTracker::AnyLimitExceeded() /home/lv/i3/be/src/runtime/mem-tracker.h:224:21
    #3 0x7ff0ab2bb394 in impala::RuntimeState::CheckQueryState() /home/lv/i3/be/src/runtime/runtime-state.cc:244:30
    #4 0x7ff0aa9c0de4 in impala::PlanRootSink::GetNext(impala::RuntimeState*, impala::QueryResultSet*, int, bool*) /home/lv/i3/be/src/exec/plan-root-sink.cc:163:10
    #5 0x7ff0ab0eecda in impala::Coordinator::GetNext(impala::QueryResultSet*, int, bool*) /home/lv/i3/be/src/runtime/coordinator.cc:876:19
    #6 0x7ff0a94c8b34 in impala::ClientRequestState::FetchRowsInternal(int, impala::QueryResultSet*) /home/lv/i3/be/src/service/client-request-state.cc:787:21
    #7 0x7ff0a94c81f4 in impala::ClientRequestState::FetchRows(int, impala::QueryResultSet*) /home/lv/i3/be/src/service/client-request-state.cc:692:28
    #8 0x7ff0a94ae4c9 in impala::ImpalaServer::FetchInternal(impala::TUniqueId const&, bool, int, beeswax::Results*) /home/lv/i3/be/src/service/impala-beeswax-server.cc:525:25
    #9 0x7ff0a94ad9d6 in impala::ImpalaServer::fetch(beeswax::Results&, beeswax::QueryHandle const&, bool, int) /home/lv/i3/be/src/service/impala-beeswax-server.cc:171:19
    #10 0x7ff0a89127c6 in beeswax::BeeswaxServiceProcessor::process_fetch(int, apache::thrift::protocol::TProtocol*, apache::thrift::protocol::TProtocol*, void*) /home/lv/i3/be/generated-sources/gen-cpp/BeeswaxService.cpp:3150:5
    #11 0x7ff0a890fff9 in beeswax::BeeswaxServiceProcessor::dispatchCall(apache::thrift::protocol::TProtocol*, apache::thrift::protocol::TProtocol*, std::string const&, int, void*) /home/lv/i3/be/generated-sources/gen-cpp/BeeswaxService.cpp:2952:3
    #12 0x7ff0a88dceca in impala::ImpalaServiceProcessor::dispatchCall(apache::thrift::protocol::TProtocol*, apache::thrift::protocol::TProtocol*, std::string const&, int, void*) /home/lv/i3/be/generated-sources/gen-cpp/ImpalaService.cpp:1673:12
    #13 0x7ff0abdd873a in apache::thrift::TDispatchProcessor::process(boost::shared_ptr<apache::thrift::protocol::TProtocol>, boost::shared_ptr<apache::thrift::protocol::TProtocol>, void*) /opt/Impala-Toolchain/thrift-0.9.0-p8/include/thrift/TDispatchProcessor.h:121:12
    #14 0x16e6f3a in apache::thrift::server::TThreadPoolServer::Task::run() (/home/lv/i3/be/build/debug/service/impalad+0x16e6f3a)
    #15 0x16ca448 in apache::thrift::concurrency::ThreadManager::Worker::run() (/home/lv/i3/be/build/debug/service/impalad+0x16ca448)
    #16 0x7ff0a97cd239 in impala::ThriftThread::RunRunnable(boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*) /home/lv/i3/be/src/rpc/thrift-thread.cc:64:3
    #17 0x7ff0a97cf87c in boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>::operator()(impala::ThriftThread*, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*) const /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/mem_fn_template.hpp:280:16
    #18 0x7ff0a97cf6c3 in void boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> >::operator()<boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list0>(boost::_bi::type<void>, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>&, boost::_bi::list0&, int) /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind.hpp:392:9
    #19 0x7ff0a97cf567 in boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::ThriftThread, boost::shared_ptr<apache::thrift::concurrency::Runnable>, impala::Promise<unsigned long>*>, boost::_bi::list3<boost::_bi::value<impala::ThriftThread*>, boost::_bi::value<boost::shared_ptr<apache::thrift::concurrency::Runnable> >, boost::_bi::value<impala::Promise<unsigned long>*> > >::operator()() /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind_template.hpp:20:16
    #20 0x7ff0ab8aa6d2 in boost::function0<void>::operator()() const /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/function/function_template.hpp:766:14
    #21 0x7ff0ab8a718d in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) /home/lv/i3/be/src/util/thread.cc:322:3
    #22 0x7ff0ab8b1d8a in void boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind.hpp:457:9
    #23 0x7ff0ab8b1c17 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind_template.hpp:20:16
    #24 0x94ff09 in thread_proxy (/home/lv/i3/be/build/debug/service/impalad+0x94ff09)
    #25 0x7ff0a5182183 in start_thread /build/eglibc-MjiXCM/eglibc-2.19/nptl/pthread_create.c:312
    #26 0x7ff0a4c99bec in clone /build/eglibc-MjiXCM/eglibc-2.19/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:111

0x6120003d9bf8 is located 184 bytes inside of 296-byte region [0x6120003d9b40,0x6120003d9c68)
freed by thread T98430 here:
    #0 0x8512e0 in operator delete(void*) /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-ubuntu-14-04/toolchain/source/llvm/llvm-3.8.0.src-p1/projects/compiler-rt/lib/asan/asan_new_delete.cc:94
    #1 0x7ff0ab0951e2 in boost::scoped_ptr<impala::MemTracker>::reset(impala::MemTracker*) /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/smart_ptr/scoped_ptr.hpp:88:9
    #2 0x7ff0ab2bb9a5 in impala::RuntimeState::ReleaseResources() /home/lv/i3/be/src/runtime/runtime-state.cc:285:3
    #3 0x7ff0ab246a54 in impala::FragmentInstanceState::Close() /home/lv/i3/be/src/runtime/fragment-instance-state.cc:308:3
    #4 0x7ff0ab2434aa in impala::FragmentInstanceState::Exec() /home/lv/i3/be/src/runtime/fragment-instance-state.cc:95:3
    #5 0x7ff0ab27cf04 in impala::QueryState::ExecFInstance(impala::FragmentInstanceState*) /home/lv/i3/be/src/runtime/query-state.cc:330:19
    #6 0x7ff0ab8aa6d2 in boost::function0<void>::operator()() const /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/function/function_template.hpp:766:14
    #7 0x7ff0ab8a718d in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) /home/lv/i3/be/src/util/thread.cc:322:3
    #8 0x7ff0ab8b1d8a in void boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind.hpp:457:9
    #9 0x7ff0ab8b1c17 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind_template.hpp:20:16
    #10 0x94ff09 in thread_proxy (/home/lv/i3/be/build/debug/service/impalad+0x94ff09)

previously allocated by thread T98430 here:
    #0 0x850ce0 in operator new(unsigned long) /data/jenkins/workspace/verify-impala-toolchain-package-build/label/ec2-package-ubuntu-14-04/toolchain/source/llvm/llvm-3.8.0.src-p1/projects/compiler-rt/lib/asan/asan_new_delete.cc:62
    #1 0x7ff0ab2b8f78 in impala::RuntimeState::Init() /home/lv/i3/be/src/runtime/runtime-state.cc:124:31
    #2 0x7ff0ab2b8912 in impala::RuntimeState::RuntimeState(impala::QueryState*, impala::TPlanFragmentCtx const&, impala::TPlanFragmentInstanceCtx const&, impala::ExecEnv*) /home/lv/i3/be/src/runtime/runtime-state.cc:86:3
    #3 0x7ff0ab243a82 in impala::FragmentInstanceState::Prepare() /home/lv/i3/be/src/runtime/fragment-instance-state.cc:119:40
    #4 0x7ff0ab24326c in impala::FragmentInstanceState::Exec() /home/lv/i3/be/src/runtime/fragment-instance-state.cc:73:19
    #5 0x7ff0ab27cf04 in impala::QueryState::ExecFInstance(impala::FragmentInstanceState*) /home/lv/i3/be/src/runtime/query-state.cc:330:19
    #6 0x7ff0ab8aa6d2 in boost::function0<void>::operator()() const /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/function/function_template.hpp:766:14
    #7 0x7ff0ab8a718d in impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*) /home/lv/i3/be/src/util/thread.cc:322:3
    #8 0x7ff0ab8b1d8a in void boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> >::operator()<void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0>(boost::_bi::type<void>, void (*&)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list0&, int) /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind.hpp:457:9
    #9 0x7ff0ab8b1c17 in boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void ()>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void ()> >, boost::_bi::value<impala::Promise<long>*> > >::operator()() /opt/Impala-Toolchain/boost-1.57.0-p2/include/boost/bind/bind_template.hpp:20:16
    #10 0x94ff09 in thread_proxy (/home/lv/i3/be/build/debug/service/impalad+0x94ff09)
{noformat}

"	IMPALA	Resolved	2	1	9911	crash
13185862	incorrect HS2 null handling introduced by IMPALA-7477	"[~boroknagyz] reported this issue with the HS2 endpoint:



Reproduction:
{noformat}
create table null_table (int_field int, float_field float, double_field double, string_field string);
insert into table null_table values (1, 3.14, 3.14, 'abc'), (2, 4.12, 4.12, 'def'), (NULL, NULL, NULL, NULL);
{noformat}
From JDBC client (only tried Hive JDBC client)
{noformat}
select * from null_table;
0 | 0.0 | 0.0 | null
2 | 4.12 | 4.12 | 'def'
0 | 0.0 | 0.0 |
{noformat}

The bug is with handling of nulls in the conversion functions, specifically output_row_idx isn't incremented.
"	IMPALA	Resolved	1	1	9911	correctness
13053249	Link LLVM bytecode into impalad binary	"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained.

These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead.

Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time."	IMPALA	Resolved	3	4	9911	test-infra, usability
13187705	Erasure coding builds still failing because of default query options	"Two tests fail because the default query options they set were clobbered by the custom cluster test infra:

*TestSetAndUnset.test_set_and_unset

*TestAdmissionController.test_set_request_pool
{noformat}
hs2/hs2_test_suite.py:48: in add_session
    fn(self)
custom_cluster/test_set_and_unset.py:44: in test_set_and_unset
    assert ""DEBUG_ACTION\tcustom\tDEVELOPMENT"" in result.data, ""baseline""
E   AssertionError: baseline
E   assert 'DEBUG_ACTION\tcustom\tDEVELOPMENT' in ['ABORT_ON_ERROR\t0\tREGULAR', 'ALLOW_ERASURE_CODED_FILES\t1\tDEVELOPMENT', 'APPX_COUNT_DISTINCT\t0\tADVANCED', 'BATCH_SIZE\t0\tDEVELOPMENT', 'BUFFER_POOL_LIMIT\t\tADVANCED', 'COMPRESSION_CODEC\t\tREGULAR', ...]
E    +  where ['ABORT_ON_ERROR\t0\tREGULAR', 'ALLOW_ERASURE_CODED_FILES\t1\tDEVELOPMENT', 'APPX_COUNT_DISTINCT\t0\tADVANCED', 'BATCH_SIZE\t0\tDEVELOPMENT', 'BUFFER_POOL_LIMIT\t\tADVANCED', 'COMPRESSION_CODEC\t\tREGULAR', ...] = <tests.beeswax.impala_beeswax.ImpalaBeeswaxResult object at 0x4d9a6d0>.data

hs2/hs2_test_suite.py:48: in add_session
    fn(self)
custom_cluster/test_admission_controller.py:317: in test_set_request_pool
    ['MEM_LIMIT=200000000', 'REQUEST_POOL=root.queueB'])
custom_cluster/test_admission_controller.py:224: in __check_query_options
    assert False, ""Expected query options %s, got %s."" % (expected, actual)
E   AssertionError: Expected query options MEM_LIMIT=200000000,REQUEST_POOL=root.queueB, got allow_erasure_coded_files=1,request_pool=root.queueb.
E   assert False{noformat}"	IMPALA	Resolved	1	1	9911	broken-build
13054111	Stress test: codegened spilling joins return incorrect results	"The stress test started reporting some queries are returning incorrect results in some cases. The rate is very small, tens out of 10,000 queries. The symptoms seem a lot like IMPALA-2987, so I tried to provide some of the information that's included in that bug.

See for example for this build:
http://sandbox.jenkins.cloudera.com/job/Impala-Stress-Test-Physical/486/

This is the console log containing one of the failed queries:

{noformat}
00:38:57 Process Process-88:
00:38:57 Traceback (most recent call last):
00:38:57   File ""/usr/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap
00:38:57     self.run()
00:38:57   File ""/usr/lib/python2.7/multiprocessing/process.py"", line 114, in run
00:38:57     self._target(*self._args, **self._kwargs)
00:38:57   File ""tests/stress/concurrent_select.py"", line 625, in _start_single_runner
00:38:57     % (query.result_hash, report.result_hash, query.sql))
00:38:57 Exception: Result hash mismatch; expected 3440873535184108466, got 3440873535183883670
00:38:57 Query: select
00:38:57   p_brand,
00:38:57   p_type,
00:38:57   p_size,
00:38:57   count(distinct ps_suppkey) as supplier_cnt
00:38:57 from
00:38:57   partsupp,
00:38:57   part
00:38:57 where
00:38:57   p_partkey = ps_partkey
00:38:57   and p_brand <> 'Brand#45'
00:38:57   and p_type not like 'MEDIUM POLISHED%'
00:38:57   and p_size in (49, 14, 23, 45, 19, 3, 36, 9)
00:38:57   and ps_suppkey not in (
00:38:57     select
00:38:57       s_suppkey
00:38:57     from
00:38:57       supplier
00:38:57     where
00:38:57       s_comment like '%Customer%Complaints%'
00:38:57   )
00:38:57 group by
00:38:57   p_brand,
00:38:57   p_type,
00:38:57   p_size
00:38:57 order by
00:38:57   supplier_cnt desc,
00:38:57   p_brand,
00:38:57   p_type,
00:38:57   p_size
00:38:57 00:38:57 3153 139801718388480 INFO:concurrent_select[374]:Checking for crashes
00:38:57 00:38:57 3153 139801718388480 INFO:concurrent_select[377]:No crashes detected
{noformat}

Using the saved artifacts from the Jenkins job (results.zip) we see that corresponds to query ID {{447bd5594405d60:f8b5fcc30b7b2eb6}}. We do this by grepping the result in the directory containing all the queries and results. The filename is the query ID.

I grabbed the logs and query profiles off the cluster and put them on impala desktop in a directory with this bug ID.  I can then run a small script in the dev user's homedir to get the query profile (~dev/print_profile).

The incorrect query's profile is:

{noformat}
Operator              #Hosts   Avg Time   Max Time   #Rows  Est. #Rows   Peak Mem  Est. Peak Mem  Detail
---------------------------------------------------------------------------------------------------------------------------------
12:MERGING-EXCHANGE        1   51.893ms   51.893ms  27.84K     188.75K          0        -1.00 B  UNPARTITIONED
07:SORT                    8   49.124ms  216.409ms  27.84K     188.75K   24.02 MB       16.00 MB    
06:AGGREGATE               8    1s876ms    2s426ms  27.84K     188.75K    2.52 MB       10.00 MB  FINALIZE
11:AGGREGATE               8    9s396ms   12s008ms  11.82M       3.20M  266.11 MB      257.14 MB    
10:EXCHANGE                8  373.182ms    1s089ms  11.83M       3.20M          0              0  HASH(p_brand,p_type,p_size)
05:AGGREGATE               8    3s818ms    4s954ms  11.83M       3.20M   12.58 MB      257.14 MB  STREAMING
04:HASH JOIN               8    4s531ms    5s881ms  11.83M       3.20M   10.20 MB        9.07 MB  NULL AWARE LEFT ANTI JOIN, ...
|--09:EXCHANGE             8  406.771us    2.317ms     479     100.00K          0              0  BROADCAST
|  02:SCAN HDFS            7    4s091ms    5s725ms     479     100.00K   16.15 MB       48.00 MB  tpch_100_parquet.supplier
03:HASH JOIN               8   23s811ms   27s344ms  11.83M       3.20M  266.17 MB       64.29 MB  INNER JOIN, BROADCAST
|--08:EXCHANGE             8    3s183ms    3s883ms   2.97M     800.00K          0              0  BROADCAST
|  01:SCAN HDFS            7    3s915ms    4s865ms   2.97M     800.00K   96.45 MB      352.00 MB  tpch_100_parquet.part
00:SCAN HDFS               8    4s051ms    6s761ms  80.00M      80.00M   39.94 MB      176.00 MB  tpch_100_parquet.partsupp
{noformat}

Compare this to a profile of the same query that returned correct results. There are many such runs since we re-run the queries. Here's {{134425b2e619c627:48b806a7130653b4}}:

{noformat}
Operator              #Hosts   Avg Time   Max Time   #Rows  Est. #Rows   Peak Mem  Est. Peak Mem  Detail
---------------------------------------------------------------------------------------------------------------------------------
12:MERGING-EXCHANGE        1   27.808ms   27.808ms  27.84K     188.75K          0        -1.00 B  UNPARTITIONED
07:SORT                    8   13.794ms   21.751ms  27.84K     188.75K   24.02 MB       16.00 MB    
06:AGGREGATE               8    1s321ms    1s714ms  27.84K     188.75K    2.52 MB       10.00 MB  FINALIZE
11:AGGREGATE               8    1s860ms    2s663ms  11.88M       3.20M  171.10 MB      257.14 MB    
10:EXCHANGE                8  717.939ms    1s620ms  11.88M       3.20M          0              0  HASH(p_brand,p_type,p_size)
05:AGGREGATE               8    3s784ms    5s524ms  11.88M       3.20M   12.71 MB      257.14 MB  STREAMING
04:HASH JOIN               8  234.658ms  325.862ms  11.88M       3.20M    2.69 MB        9.07 MB  NULL AWARE LEFT ANTI JOIN, ...
|--09:EXCHANGE             8  158.408us  333.299us     479     100.00K          0              0  BROADCAST
|  02:SCAN HDFS            7    1s807ms    2s859ms     479     100.00K   16.15 MB       48.00 MB  tpch_100_parquet.supplier
03:HASH JOIN               8    3s331ms    3s921ms  11.89M       3.20M  394.08 MB       64.29 MB  INNER JOIN, BROADCAST
|--08:EXCHANGE             8    2s387ms    6s663ms   2.97M     800.00K          0              0  BROADCAST
|  01:SCAN HDFS            7    1s452ms    2s243ms   2.97M     800.00K   96.28 MB      352.00 MB  tpch_100_parquet.part
00:SCAN HDFS               8    1s084ms    2s400ms  80.00M      80.00M   39.99 MB      176.00 MB  tpch_100_parquet.partsupp 
{noformat}

What follows is the full list of query IDs that were reported as having hash mismatches:

{noformat}
447bd5594405d60:f8b5fcc30b7b2eb6
5642c03e84fec850:e4c4e7d4b57e5291
bf4ada8ece00f3b4:31e11d41904adfa4
2d4e8ec89dc9f072:67cf3c6843ed7bd
a46cf7d01964dff:9e9eabb44ca2ca9a
af4490619ef34246:f08c4695612c4c85
7443d268a0ba7201:3893ffcc845a78d
e34dfe2fb3ff9f33:c0d18ac4f45b978b
f4ac95f838ad22e:1f9a0a66dafa46a1
7d4713f6b51241b1:a8c7c8d00ea6a49c
{noformat}

It's interesting to note these 10 failures are from the same 2 queries. Some fail on the query above. The other query is this:

{noformat}
select
  c_custkey,
  c_name,
  sum(l_extendedprice * (1 - l_discount)) as revenue,
  c_acctbal,
  n_name,
  c_address,
  c_phone,
  c_comment
from
  customer,
  orders,
  lineitem,
  nation
where
  c_custkey = o_custkey
  and l_orderkey = o_orderkey
  and o_orderdate >= '1993-10-01'
  and o_orderdate < '1994-01-01'
  and l_returnflag = 'R'
  and c_nationkey = n_nationkey
group by
  c_custkey,
  c_name,
  c_acctbal,
  c_phone,
  n_name,
  c_address,
  c_comment
order by
  revenue desc
limit 20
{noformat}

I'll note that this started on April 21 but the test framework didn't fail the job. IMPALA-3487 filed to change that. What brought this to my attention was a crash that happened yesterday in the same job. That's IMPALA-3485. 

I think everything someone needs to look into this further is in the appropriate directory on impala-desktop."	IMPALA	Resolved	1	1	9911	correctness, stress
13133122	Impala Catalog generates a core file / mini dump when the HMS is not available	"Synopsis:
 =========
 Impala Catalog generates a core file / mini dump when the HMS is not available

Problem:
 ========
Catalog server created multiple Catalog core files. During the investigation it was determine that the cause of the core files was because the Hive Meta Store was not available and the option ""Enable Core Dump"" was enabled when starting the Impala service.

Below is the back trace of the core file:

#0 0x00007f72e93ee5d7 in raise () from /root/191729/slib/lib64/libc.so.6
 #1 0x00007f72e93efcc8 in abort () from /root/191729/slib/lib64/libc.so.6
 #2 0x0000000001ba5754 in google::DumpStackTraceAndExit() ()
 #3 0x0000000001b9c1cd in google::LogMessage::Fail() ()
 #4 0x0000000001b9da72 in google::LogMessage::SendToLog() ()
 #5 0x0000000001b9bba7 in google::LogMessage::Flush() ()
 #6 0x0000000001b9f16e in google::LogMessageFatal::~LogMessageFatal() ()
 #7 0x000000000083067e in impala::Catalog::(GetCatalogVersion (this=0x0, version=0x7ffc2aa6b750) at /usr/src/debug/impala-2.10.0-cdh5.13.1/be/src/catalog/catalog.cc:88
 #8 0x00000000008143c9 in impala::CatalogServer::Start() () at /usr/src/debug/impala-2.10.0-cdh5.13.1/be/src/catalog/catalog-server.cc:175

The corresponding entries in the Catalog server log show the following fatal error:

F0111 09:48:05.017491 14571 catalog.cc:76] java.lang.IllegalStateException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
 at org.apache.impala.catalog.MetaStoreClientPool$MetaStoreClient.<init>(MetaStoreClientPool.java:99)
 at org.apache.impala.catalog.MetaStoreClientPool$MetaStoreClient.<init>(MetaStoreClientPool.java:72)
 at org.apache.impala.catalog.MetaStoreClientPool.initClients(MetaStoreClientPool.java:168)
 at org.apache.impala.catalog.Catalog.<init>(Catalog.java:103)
 at org.apache.impala.catalog.CatalogServiceCatalog.<init>(CatalogServiceCatalog.java:163)
 at org.apache.impala.service.JniCatalog.<init>(JniCatalog.java:104)
 Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient

Iwas able to reproduce this issue. When the option ""Enable Core Dump"" is enable and the Hive Meta Store is not available, the system generates a core file. If the option ""Enable Core Dump"" is disabled, the system generates a mini dump.

Crashing due to an error is not expected. Impala should fail in a more user friendly way.

Reproduction case:
 ==================
 1) Enable the option ""Enable Core Dump"" for the Impala service in CM.
 2) Stop Hive and Impala services.
 3) Start Impala Catalog server"	IMPALA	Resolved	2	1	9911	supportability
13065234	Impala does not start under ASAN	"Impala does not start under ASAN anymore. Our ASAN build is hitting this, and I can reproduce locally.

Here's the stack:
{code}
Stack: [0x00007f5d20999000,0x00007f5d2119a000],  sp=0x00007f5d211963d0,  free space=8180k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [impalad+0xc718bb]  base::subtle::NoBarrier_CompareAndSwap(int volatile*, int, int)+0x1b
C  [impalad+0xc71875]  base::SpinLock::Lock()+0x15
C  [impalad+0xf417d5]  impala::SimpleMetric<unsigned long, (impala::TMetricKind::type)0>::value()+0x95
C  [impalad+0x126e494]  impala::SumGauge<unsigned long>::CalculateValue()+0x124
C  [impalad+0xf41808]  impala::SimpleMetric<unsigned long, (impala::TMetricKind::type)0>::value()+0xc8
C  [impalad+0x126ce93]  impala::SimpleMetric<unsigned long, (impala::TMetricKind::type)0>::ToLegacyJson(rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*)+0xb3
C  [impalad+0x127093a]  impala::MetricGroup::CMCompatibleCallback(std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*)+0x4ca
C  [impalad+0x12750bd]  void boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::MetricGroup, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*>, boost::_bi::list3<boost::_bi::value<impala::MetricGroup*>, boost::arg<1>, boost::arg<2> > >::operator()<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > >, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*>(std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*&)+0xad
C  [impalad+0x1274e30]  boost::detail::function::void_function_obj_invoker2<boost::_bi::bind_t<void, boost::_mfi::mf2<void, impala::MetricGroup, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*>, boost::_bi::list3<boost::_bi::value<impala::MetricGroup*>, boost::arg<1>, boost::arg<2> > >, void, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*>::invoke(boost::detail::function::function_buffer&, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*)+0xb0
C  [impalad+0x13083a6]  boost::function2<void, std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*>::operator()(std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, rapidjson::GenericDocument<rapidjson::UTF8<char>, rapidjson::MemoryPoolAllocator<rapidjson::CrtAllocator> >*) const+0xc6
C  [impalad+0x1306c6f]  impala::Webserver::RenderUrlWithTemplate(std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > const&, impala::Webserver::UrlHandler const&, std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >*, impala::ContentType*)+0x28f
C  [impalad+0x130648f]  impala::Webserver::BeginRequestCallback(sq_connection*, sq_request_info*)+0x50f
C  [impalad+0x1322ed0]  boost::_bi::bind_t<impala::ThriftClientImpl*, boost::_mfi::mf4<impala::ThriftClientImpl*, impala::ClientCache<impala::ImpalaBackendClient>, impala::TNetworkAddress const&, void**, std::string, bool>, boost::_bi::list_av_5<impala::ClientCache<impala::ImpalaBackendClient>*, boost::arg<1>, boost::arg<2>, std::string, bool>::type> boost::bind<impala::ThriftClientImpl*, boost::_mfi::mf4<impala::ThriftClientImpl*, impala::ClientCache<impala::ImpalaBackendClient>, impala::TNetworkAddress const&, void**, std::string, bool>, impala::ClientCache<impala::ImpalaBackendClient>*, boost::arg<1>, boost::arg<2>, std::string, bool>(boost::_mfi::mf4<impala::ThriftClientImpl*, impala::ClientCache<impala::ImpalaBackendClient>, impala::TNetworkAddress const&, void**, std::string, bool>, impala::ClientCache<impala::ImpalaBackendClient>*, boost::arg<1>, boost::arg<2>, std::string, bool)+0x190
C  [impalad+0x132290e]  impala::_TGetHadoopConfigResponse__isset::_TGetHadoopConfigResponse__isset()+0x2e
C  [impalad+0x1322347]  __gnu_cxx::new_allocator<std::pair<impala::TUniqueId const, impala::QueryState*> >::~new_allocator()+0x7
{code}"	IMPALA	Resolved	1	1	9911	broken-build
13054213	Mislabeled cache levels on Debug webpage	"{code}
  L1 Cache: 32.00 KB (Line: 64.00 B)
  L1 Cache: 256.00 KB (Line: 64.00 B)
  L1 Cache: 30.00 MB (Line: 64.00 B)
{code}"	IMPALA	Resolved	3	1	9911	usability
13065660	Negative byte values are not printed with the correct unit	"It seems like the logic to decide on the unit does not work correctly for negative values, e.g. it prints ""-7369392.00 B"" instead of showing MB

{code}
[localhost:21000] > select * from tpch.lineitem;
Query: select * from tpch.lineitem
Query submitted at: 2017-04-20 12:04:22 (Coordinator: http://localhost:25000)
Query progress can be monitored at: http://localhost:25000/query_plan?query_id=6048492f67282f78:ef0f2bd400000000
WARNINGS: Memory limit exceeded: Failed to allocate tuple buffer
HDFS_SCAN_NODE (id=0) could not allocate 190.00 KB without exceeding limit.
Error occurred on backend localhost:22000 by fragment 6048492f67282f78:ef0f2bd400000003
Memory left in process limit: 8.24 GB
Memory left in query limit: -7369392.00 B
Query(6048492f67282f78:ef0f2bd400000000): memory limit exceeded. Limit=1.00 MB Total=8.03 MB Peak=8.03 MB
  Fragment 6048492f67282f78:ef0f2bd400000000: Total=8.00 KB Peak=8.00 KB
    EXCHANGE_NODE (id=1): Total=0 Peak=0
    DataStreamRecvr: Total=0 Peak=0
    PLAN_ROOT_SINK: Total=0 Peak=0
    CodeGen: Total=0 Peak=0
  Block Manager: Total=0 Peak=0
  Fragment 6048492f67282f78:ef0f2bd400000003: Total=8.02 MB Peak=8.02 MB
    HDFS_SCAN_NODE (id=0): Total=8.01 MB Peak=8.01 MB
    DataStreamSender (dst_id=1): Total=688.00 B Peak=688.00 B
    CodeGen: Total=0 Peak=0
{code}"	IMPALA	Resolved	4	1	9911	newbie, supportability
13203500	Run impalad minicluster processes inside docker containers	"I'm looking on some docker-related tasks, e.g. IMPALA-7941 and it'd be cool to be able to start up a minicluster with each impala process inside its own container. This is distinct from test-with-docker which runs the whole minicluster inside a container and is much closer to how containerised services are deployed in production.

I'm imagining this would be a start-impala-cluster.py flag."	IMPALA	Resolved	3	2	9911	docker
13053758	High contention for spinlock in MemTracker::GcMemory()	"Mostafa reported that Impala could get stuck spinning on a spinlock in GcMemory():

It seems that the spinning is allocations which makes lots of requests to the MemTracker.
__sched_yield has a CPI of 10 which is really high.

{code}
Instructions Retired
1 of 61: 32.4% (84688982810 of 261749989945)

libc-2.12.so ! __sched_yield - [unknown source file]
libc-2.12.so ! [Unknown stack frame(s)] - [unknown source file]
impalad ! impala::MemTracker::LimitExceeded + 0x4f - mem-tracker.h:268
impalad ! impala::MemTracker::AnyLimitExceeded + 0x15 - mem-tracker.h:255
impalad ! impala::RuntimeState::CheckQueryState + 0x1b - runtime-state.cc:287
impalad ! impala::HdfsScanner::CommitRows + 0xce - hdfs-scanner.cc:182
impalad ! impala::HdfsParquetScanner::AssembleRows<(bool)1, (bool)0> + 0x252 - hdfs-parquet-scanner.cc:1657
impalad ! impala::HdfsParquetScanner::ProcessSplit + 0x31f - hdfs-parquet-scanner.cc:1535
impalad ! impala::HdfsScanNode::ProcessSplit + 0x365 - hdfs-scan-node.cc:1183
impalad ! impala::HdfsScanNode::ScannerThread + 0xc8d - hdfs-scan-node.cc:1063
impalad ! boost::function0<void>::operator() + 0x1a - function_template.hpp:767
impalad ! impala::Thread::SuperviseThread + 0x20c - thread.cc:316
impalad ! operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> + 0x5a - bind.hpp:457
impalad ! boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>>::run + 0x19 - thread.hpp:116
impalad ! thread_proxy + 0xd9 - [unknown source file]
libpthread-2.12.so ! start_thread + 0xd0 - [unknown source file]
libc-2.12.so ! clone + 0x6c - [unknown source file]
{code}

{code}
Instructions Retired
2 of 61: 13.5% (35412538191 of 261749989945)

libc-2.12.so ! __sched_yield - [unknown source file]
libc-2.12.so ! [Unknown stack frame(s)] - [unknown source file]
impalad ! impala::MemTracker::LimitExceeded + 0x4f - mem-tracker.h:268
impalad ! impala::MemTracker::AnyLimitExceeded + 0x15 - mem-tracker.h:255
impalad ! impala::RuntimeState::CheckQueryState + 0x1b - runtime-state.cc:287
impalad ! impala::HdfsScanner::CommitRows + 0xce - hdfs-scanner.cc:182
impalad ! impala::HdfsParquetScanner::AssembleRows<(bool)0, (bool)0> + 0x82e - hdfs-parquet-scanner.cc:1657
impalad ! impala::HdfsParquetScanner::ProcessSplit + 0x4d2 - hdfs-parquet-scanner.cc:1535
impalad ! impala::HdfsScanNode::ProcessSplit + 0x365 - hdfs-scan-node.cc:1183
impalad ! impala::HdfsScanNode::ScannerThread + 0xc8d - hdfs-scan-node.cc:1063
impalad ! boost::function0<void>::operator() + 0x1a - function_template.hpp:767
impalad ! impala::Thread::SuperviseThread + 0x20c - thread.cc:316
impalad ! operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> + 0x5a - bind.hpp:457
impalad ! boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>>::run + 0x19 - thread.hpp:116
impalad ! thread_proxy + 0xd9 - [unknown source file]
libpthread-2.12.so ! start_thread + 0xd0 - [unknown source file]
libc-2.12.so ! clone + 0x6c - [unknown source file]
{code}

{code}
Instructions Retired
4 of 61: 6.0% (15807873279 of 261749989945)

libc-2.12.so ! __sched_yield - [unknown source file]
libc-2.12.so ! [Unknown stack frame(s)] - [unknown source file]
impalad ! impala::MemTracker::TryConsume + 0x1c3 - mem-tracker.h:184
impalad ! impala::MemPool::FindChunk + 0x140 - mem-pool.cc:129
impalad ! Allocate<true> + 0x57 - mem-pool.h:215
impalad ! impala::MemPool::TryAllocate - mem-pool.h:99
impalad ! CollectionValueBuilder + 0x31 - collection-value-builder.h:37
impalad ! impala::HdfsParquetScanner::CollectionColumnReader::ReadSlot - hdfs-parquet-scanner.cc:1359
impalad ! impala::HdfsParquetScanner::CollectionColumnReader::ReadValue + 0x4c - hdfs-parquet-scanner.cc:1333
impalad ! ReadRow<true> + 0x7d - hdfs-parquet-scanner.cc:1701
impalad ! impala::HdfsParquetScanner::AssembleRows<(bool)1, (bool)1> + 0x100 - hdfs-parquet-scanner.cc:1610
impalad ! impala::HdfsParquetScanner::CollectionColumnReader::ReadSlot + 0x10c - hdfs-parquet-scanner.cc:1363
impalad ! impala::HdfsParquetScanner::CollectionColumnReader::ReadValue + 0x4c - hdfs-parquet-scanner.cc:1333
impalad ! ReadRow<true> + 0x82 - hdfs-parquet-scanner.cc:1701
impalad ! impala::HdfsParquetScanner::AssembleRows<(bool)1, (bool)0> + 0xf0 - hdfs-parquet-scanner.cc:1593
impalad ! impala::HdfsParquetScanner::ProcessSplit + 0x31f - hdfs-parquet-scanner.cc:1535
impalad ! impala::HdfsScanNode::ProcessSplit + 0x365 - hdfs-scan-node.cc:1183
impalad ! impala::HdfsScanNode::ScannerThread + 0xc8d - hdfs-scan-node.cc:1063
impalad ! boost::function0<void>::operator() + 0x1a - function_template.hpp:767
impalad ! impala::Thread::SuperviseThread + 0x20c - thread.cc:316
impalad ! operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> + 0x5a - bind.hpp:457
impalad ! boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>>::run + 0x19 - thread.hpp:116
impalad ! thread_proxy + 0xd9 - [unknown source file]
libpthread-2.12.so ! start_thread + 0xd0 - [unknown source file]
libc-2.12.so ! clone + 0x6c - [unknown source file]
{code}

{code}
Instructions Retired
2 of 49: 4.5% (662091353 of 14656153166)

vmlinux ! thread_return - [unknown source file]
vmlinux ! sys_sched_yield + 0x54 - [unknown source file]
vmlinux ! system_call_fastpath + 0x15 - [unknown source file]
libc-2.12.so ! __sched_yield + 0x6 - [unknown source file]
impalad ! impala::SpinLock::SlowAcquire + 0x40 - spinlock.cc:25
impalad ! lock_guard + 0x4 - lock_guard.hpp:38
impalad ! impala::MemTracker::GcMemory + 0x19 - mem-tracker.cc:301
impalad ! impala::MemTracker::LimitExceeded + 0x60 - mem-tracker.h:268
impalad ! impala::DiskIoMgr::GetNextRequestRange + 0x90 - disk-io-mgr.cc:786
impalad ! impala::DiskIoMgr::WorkLoop + 0x41 - disk-io-mgr.cc:987
impalad ! boost::function0<void>::operator() + 0x1a - function_template.hpp:767
impalad ! impala::Thread::SuperviseThread + 0x20c - thread.cc:316
impalad ! operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> + 0x5a - bind.hpp:457
impalad ! boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>>::run + 0x19 - thread.hpp:116
impalad ! thread_proxy + 0xd9 - [unknown source file]
libpthread-2.12.so ! start_thread + 0xd0 - [unknown source file]
libc-2.12.so ! clone + 0x6c - [unknown source file]
{code}"	IMPALA	Resolved	2	1	9911	perf, resource-management
13158984	Order by expressions in Analytical functions are not materialized causing slowdown	"Order by expressions in Analytical functions are not materialized and cause queries to run much slower.

The rewrite for the query below is 20x faster, profiles attached.

Repro 
{code}
select *
FROM
  (
    SELECT
      o.*,
      ROW_NUMBER() OVER(ORDER BY evt_ts DESC) AS rn
    FROM
      (
        SELECT
          l_orderkey,l_partkey,l_linenumber,l_quantity, cast (l_shipdate as string) evt_ts
        FROM
          lineitem
        WHERE
          l_shipdate  BETWEEN '1992-01-01 00:00:00' AND '1992-01-15 00:00:00'
      ) o
  ) r
WHERE
  rn BETWEEN 1 AND 101
ORDER BY rn;
{code}

Workaround 
{code}
select *
FROM
  (
    SELECT
      o.*,
      ROW_NUMBER() OVER(ORDER BY evt_ts DESC) AS rn
    FROM
      (
        SELECT
          l_orderkey,l_partkey,l_linenumber,l_quantity, cast (l_shipdate as string) evt_ts
        FROM
          lineitem
        WHERE
          l_shipdate  BETWEEN '1992-01-01 00:00:00' AND '1992-01-15 00:00:00'
          union all 
                  SELECT
          l_orderkey,l_partkey,l_linenumber,l_quantity, cast (l_shipdate as string) evt_ts
        FROM
          lineitem limit 0
        
      ) o
  ) r
WHERE
  rn BETWEEN 1 AND 101
ORDER BY rn;
{code}"	IMPALA	Resolved	3	4	9911	performance
13052917	Crash: impala::RowBatch::SerializeInternal (Nested Types)	"DB: tpch_nested_parquet
This query causes a crash every time it is executed.

Query:
{code}
SELECT
t1.p_type
FROM part t1
INNER JOIN supplier t2 ON (t2.s_nationkey) = (t1.p_size)
, t2.s_partsupps t3
{code}

Stack Trace:
{code}
__GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56	../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
__GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
__GI_abort () at abort.c:89
google::DumpStackTraceAndExit () at src/utilities.cc:147
google::LogMessage::Fail () at src/logging.cc:1315
google::LogMessage::SendToLog (this=0x7fb4f5081d80) at src/logging.cc:1269
google::LogMessage::Flush (this=this@entry=0x7fb4f5081d80) at src/logging.cc:1138
google::LogMessageFatal::~LogMessageFatal (this=0x7fb4f5081d80, __in_chrg=) at src/logging.cc:1836
impala::RowBatch::SerializeInternal (this=0xf128f00, size=46, distinct_tuples=0x7fb4f5081e30, output_batch=0xc67a048) at /home/dev/Impala/be/src/runtime/row-batch.cc:254
impala::RowBatch::Serialize (this=0xf128f00, output_batch=0xc67a048, full_dedup=true) at /home/dev/Impala/be/src/runtime/row-batch.cc:165
impala::RowBatch::Serialize (this=0xf128f00, output_batch=0xc67a048) at /home/dev/Impala/be/src/runtime/row-batch.cc:141
impala::DataStreamSender::SerializeBatch (this=0xc67a000, src=0xf128f00, dest=0xc67a048, num_receivers=1) at /home/dev/Impala/be/src/runtime/data-stream-sender.cc:463
impala::DataStreamSender::Send (this=0xc67a000, state=0xc7e4900, batch=0xf128f00, eos=false) at /home/dev/Impala/be/src/runtime/data-stream-sender.cc:411
impala::PlanFragmentExecutor::OpenInternal (this=0xd75e628) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:355
impala::PlanFragmentExecutor::Open (this=0xd75e628) at /home/dev/Impala/be/src/runtime/plan-fragment-executor.cc:320
impala::FragmentMgr::FragmentExecState::Exec (this=0xd75e400) at /home/dev/Impala/be/src/service/fragment-exec-state.cc:50
impala::FragmentMgr::FragmentExecThread (this=0x7cc4e40, exec_state=0xd75e400) at /home/dev/Impala/be/src/service/fragment-mgr.cc:70
boost::_mfi::mf1::operator() (this=0xf21e420, p=0x7cc4e40, a1=0xd75e400) at /usr/include/boost/bind/mem_fn_template.hpp:165
boost::_bi::list2, boost::_bi::value >::operator(), boost::_bi::list0> (this=0xf21e430, f=..., a=...) at /usr/include/boost/bind/bind.hpp:313
boost::_bi::bind_t, boost::_bi::list2, boost::_bi::value > >::operator() (this=0xf21e420) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::function::void_function_obj_invoker0, boost::_bi::list2, boost::_bi::value > >, void>::invoke (function_obj_ptr=...) at /usr/include/boost/function/function_template.hpp:153
boost::function0::operator() (this=0x7fb4f5082e00) at /usr/include/boost/function/function_template.hpp:767
impala::Thread::SuperviseThread(std::string const&, std::string const&, boost::function, impala::Promise*) (name=..., category=..., functor=..., thread_started=0x7fb4f6103ea0) at /home/dev/Impala/be/src/util/thread.cc:314
boost::_bi::list4, boost::_bi::value, boost::_bi::value >, boost::_bi::value*> >::operator(), impala::Promise*), boost::_bi::list0>(boost::_bi::type, void (*&)(std::string const&, std::string const&, boost::function, impala::Promise*), boost::_bi::list0&, int) (this=0xdcc47c0, f=@0xdcc47b8: 0x144876e , impala::Promise*)>, a=...) at /usr/include/boost/bind/bind.hpp:457
boost::_bi::bind_t, impala::Promise*), boost::_bi::list4, boost::_bi::value, boost::_bi::value >, boost::_bi::value*> > >::operator()() (this=0xdcc47b8) at /usr/include/boost/bind/bind_template.hpp:20
boost::detail::thread_data, impala::Promise*), boost::_bi::list4, boost::_bi::value, boost::_bi::value >, boost::_bi::value*> > > >::run() (this=0xdcc4600) at /usr/include/boost/thread/detail/thread.hpp:117
?? () from /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0
start_thread (arg=0x7fb4f5083700) at pthread_create.c:312
clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
{code}

impalad.Fatal:
{code}
F0904 13:19:40.303438 38236 row-batch.cc:254] Check failed: offset <= size (70 vs. 46)
{code}

EDIT: Another query that causes a similar crash:
{code}
SELECT
  t3.r_regionkey, t2.p_size, t1.p_retailprice
FROM
  part t1 INNER JOIN part t2 ON (t2.p_brand) = (t1.p_container)
  RIGHT JOIN region t3 ON ((t3.r_regionkey) = (t1.p_partkey)) AND ((t3.r_name) = (t1.p_mfgr)), 
  t3.r_nations t4
{code}"	IMPALA	Resolved	1	1	9911	crash, nested_types, query_generator
13053334	impala-shell breaks on non-ascii characters in the resultset	"(Shell build version: Impala Shell v2.2.0-cdh5.4.7 (8b8d376) built on Thu Sep 17 02:00:38 PDT 2015)
[host:21000] > insert into sometable values ('rvztr tkrfrgp');
Query: insert into sometable values ('rvztr tkrfrgp')
Inserted 1 row(s) in 6.84s
[host:21000] > select * from sometable;
Query: select * from sometable
Unknown Exception : 'ascii' codec can't encode character u'\xc1' in position 83: ordinal not in range(128)
[Not connected] >


This is very similar to IMPALA-1130, IMPALA-489, IMPALA-738, the difference is that here the resultset contains the offending char.
With the -B option the result is printed correctly."	IMPALA	Resolved	4	1	9911	impala-shell, ramp-up, shell
13128305	Queries don't make progress due to what seems like a memory reservation deadlock while running the stress tests 	"Queries stopped making progress, many of the fragment threads are trying to increase or decrease memory reservation and non of those threads is making progress.

Did some quick analysis on the threads and I couldn't find any thread making progress, so this might be a deadlock. 
cat stress_debug_without_krpc_vd1304.halxg.cloudera.com_1.txt  | grep 0x0000000001b01006 -B 4 | awk '{print $4}' | sort -nr | uniq -c | sort -nr
   1312 impala::SpinLock::lock()
   1312 impala::ReservationTracker::IncreaseReservationInternalLocked(long,
   1312 boost::lock_guard<impala::SpinLock>::lock_guard(impala::SpinLock&)
   1312 base::SpinLock::SlowLock()
   1312 base::SpinLock::Lock()
   1311
cat stress_debug_without_krpc_vd1304.halxg.cloudera.com_1.txt  | grep 0x0000000001b017c6 -B 4 | awk '{print $4}' | sort -nr | uniq -c | sort -nr
    688 impala::ReservationTracker::DecreaseReservation(long,
    688 impala::ReservationTracker::DecreaseReservationLocked(long,
    400 impala::SpinLock::lock()
    400 boost::lock_guard<impala::SpinLock>::lock_guard(impala::SpinLock&)
    400 base::SpinLock::Lock()
    399

{code}
#0  0x0000000003bd6944 in sys_futex ()
#1  0x0000000003bd6a85 in base::internal::SpinLockDelay(int volatile*, int, int) ()
#2  0x0000000003bd6835 in base::SpinLock::SlowLock() ()
#3  0x00000000015f75fd in base::SpinLock::Lock() ()
#4  0x00000000015f7672 in impala::SpinLock::lock() ()
#5  0x00000000015f8d4c in boost::lock_guard<impala::SpinLock>::lock_guard(impala::SpinLock&) ()
#6  0x0000000001b015bf in impala::ReservationTracker::DecreaseReservation(long, bool) ()
#7  0x0000000001b017c6 in impala::ReservationTracker::DecreaseReservationLocked(long, bool) ()
#8  0x0000000001b015d6 in impala::ReservationTracker::DecreaseReservation(long, bool) ()
#9  0x0000000001b017c6 in impala::ReservationTracker::DecreaseReservationLocked(long, bool) ()
#10 0x0000000001b015d6 in impala::ReservationTracker::DecreaseReservation(long, bool) ()
#11 0x00000000018aabf0 in impala::ReservationTracker::DecreaseReservation(long) ()
#12 0x00000000018aaaee in impala::InitialReservations::Return(impala::BufferPool::ClientHandle*, long) ()
#13 0x0000000001b5e8e9 in impala::ExecNode::Close(impala::RuntimeState*) ()
#14 0x000000000293ef2c in impala::BlockingJoinNode::Close(impala::RuntimeState*) ()
#15 0x00000000028d639f in impala::PartitionedHashJoinNode::Close(impala::RuntimeState*) ()
#16 0x00000000018a51aa in impala::FragmentInstanceState::Close() ()
#17 0x00000000018a24b8 in impala::FragmentInstanceState::Exec() ()
#18 0x000000000188afe6 in impala::QueryState::ExecFInstance(impala::FragmentInstanceState*) ()
#19 0x0000000001889886 in impala::QueryState::StartFInstances()::{lambda()#1}::operator()() const ()
#20 0x000000000188bc25 in boost::detail::function::void_function_obj_invoker0<impala::QueryState::StartFInstances()::{lambda()#1}, void>::invoke(boost::detail::function::function_buffer&) ()
{code}

{code}
#0  0x0000000003bd6944 in sys_futex ()
#1  0x0000000003bd6a85 in base::internal::SpinLockDelay(int volatile*, int, int) ()
#2  0x0000000003bd6835 in base::SpinLock::SlowLock() ()
#3  0x00000000015f75fd in base::SpinLock::Lock() ()
#4  0x00000000015f7672 in impala::SpinLock::lock() ()
#5  0x00000000015f8d4c in boost::lock_guard<impala::SpinLock>::lock_guard(impala::SpinLock&) ()
#6  0x0000000001b01006 in impala::ReservationTracker::IncreaseReservationInternalLocked(long, bool, bool, impala::Status*) ()
#7  0x0000000001b01031 in impala::ReservationTracker::IncreaseReservationInternalLocked(long, bool, bool, impala::Status*) ()
#8  0x0000000001b01031 in impala::ReservationTracker::IncreaseReservationInternalLocked(long, bool, bool, impala::Status*) ()
#9  0x0000000001b006f5 in impala::ReservationTracker::IncreaseReservationToFit(long, impala::Status*) ()
#10 0x0000000001af738e in impala::BufferPool::ClientHandle::IncreaseReservationToFit(long) ()
#11 0x0000000002c66574 in impala::BufferedTupleStream::AdvanceWritePage(long, bool*) ()
#12 0x0000000002c692d9 in impala::BufferedTupleStream::AddRowCustomBeginSlow(long, impala::Status*) ()
#13 0x0000000002c69111 in impala::BufferedTupleStream::AddRowSlow(impala::TupleRow*, impala::Status*) ()
#14 0x0000000002c69b5e in impala::BufferedTupleStream::AddRow(impala::TupleRow*, impala::Status*) ()
#15 0x00007f059f628148 in impala::PhjBuilder::ProcessBuildBatch ()
#16 0x000000000295e10c in impala::PhjBuilder::Send(impala::RuntimeState*, impala::RowBatch*) ()
#17 0x0000000002941fda in impala::Status impala::BlockingJoinNode::SendBuildInputToSink<false>(impala::RuntimeState*, impala::DataSink*) ()
#18 0x000000000293fe59 in impala::BlockingJoinNode::ProcessBuildInputAndOpenProbe(impala::RuntimeState*, impala::DataSink*) ()
#19 0x00000000028d58cf in impala::PartitionedHashJoinNode::Open(impala::RuntimeState*) ()
{code}"	IMPALA	Resolved	2	1	9911	hang
13055006	Avro scanner crashes if the file schema has invalid decimal precision or scale	"Haven't seen this before; may be a flaky test.

{noformat}
04:26:14.562 ==================================== ERRORS ====================================
04:26:14.562  ERROR at teardown of TestScannersFuzzing.test_fuzz_decimal_tbl[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: avro/snap/block] 
04:26:14.562 [gw2] linux2 -- Python 2.6.6 /data/jenkins/workspace/impala-umbrella-build-and-test-s3/repos/Impala/bin/../infra/python/env/bin/python
04:26:14.562 conftest.py:265: in cleanup
04:26:14.562     request.instance.execute_query_expect_success(request.instance.client, ""use default"")
04:26:14.562 common/impala_test_suite.py:418: in wrapper
04:26:14.562     return function(*args, **kwargs)
04:26:14.562 common/impala_test_suite.py:425: in execute_query_expect_success
04:26:14.562     result = cls.__execute_query(impalad_client, query, query_options)
04:26:14.562 common/impala_test_suite.py:511: in __execute_query
04:26:14.562     return impalad_client.execute(query, user=user)
04:26:14.562 common/impala_connection.py:160: in execute
04:26:14.562     return self.__beeswax_client.execute(sql_stmt, user=user)
04:26:14.562 beeswax/impala_beeswax.py:173: in execute
04:26:14.562     handle = self.__execute_query(query_string.strip(), user=user)
04:26:14.562 beeswax/impala_beeswax.py:337: in __execute_query
04:26:14.562     handle = self.execute_query_async(query_string, user=user)
04:26:14.562 beeswax/impala_beeswax.py:333: in execute_query_async
04:26:14.562     return self.__do_rpc(lambda: self.imp_service.query(query,))
04:26:14.562 beeswax/impala_beeswax.py:465: in __do_rpc
04:26:14.562     raise ImpalaBeeswaxException(self.__build_error_message(u), u)
04:26:14.562 E   ImpalaBeeswaxException: ImpalaBeeswaxException:
04:26:14.562 E    INNER EXCEPTION: <class 'socket.error'>
04:26:14.562 E    MESSAGE: [Errno 32] Broken pipe
{noformat}"	IMPALA	Resolved	1	1	9911	avro, broken-build, crash, flaky
13052210	Remove workarounds for DCHECK in sorter.cc:549 (var_len_blocks[0]->Pin() fails to pin), when running parallel test	"I was running a parallel cancellation test and hit the DCHECK in L549 below. 
I see that there are 7 var_len_blocks, but the Sorter::MinBuffersRequired is 6. It seems that (1) there were some large (varlen) strings which caused to reserve at least one additional var-len block; and (2) because of the parallel tests it failed to allocate the 7th block, which was not reserved. As a result we hit the DCHECK at L549.

A temporary fix is to not DCHECK but return OOM in cases where the number of var-len blocks is larger than the minimum reserved.

{code}
 527Status Sorter::Run::PrepareRead() {
...
 541  if (fixed_len_blocks_.size() > 0) {
 542    bool pinned;
 543    RETURN_IF_ERROR(fixed_len_blocks_[0]->Pin(&pinned));
 544    DCHECK(pinned);
 545  }
 546  if (has_var_len_slots_ && var_len_blocks_.size() > 0) {
 547    bool pinned;
 548    RETURN_IF_ERROR(var_len_blocks_[0]->Pin(&pinned));
 549=>  DCHECK(pinned);
 550  }
{code}"	IMPALA	Resolved	2	1	9911	resource-management
13315511	Add support for single IN in disjunction	"A simple case of IMPALA-5226 with a simple rewrite is an IN predicate. We can directly rewrite it into a left outer join
{noformat}
WHERE  <condition> OR (x IN (select y ...))
{noformat}

Can be transformed into
{noformat}
LEFT OUTER JOIN (select distinct y ...) v on x = v.y
...
WHERE <condition> OR v.y IS NOT NULL
{noformat}

This pattern appears in TPC-DS query 45."	IMPALA	Resolved	3	7	9911	tpcds
13055368	Hit DCHECK in sorter for spilling query with scratch limit	"I hit a DCHECK when running the following query on the test minicluster:
{code}
set max_block_mgr_memory=64m;
set scratch_limit=24m;
select o_orderdate, o_custkey, o_comment
from tpch.orders
order by o_orderdate;
{code}

{code}
F0109 16:56:25.251039 27202 exec-node.cc:204] Check failed: mem_tracker()->consumption() == 0 (8388608 vs. 0) Leaked memory.
Fragment 784ce2a2e1ebd300:31263e8000000002: Total=8.00 MB Peak=115.47 MB
  SORT_NODE (id=1): Total=8.00 MB Peak=72.13 MB
  HDFS_SCAN_NODE (id=0): Total=0 Peak=43.49 MB
  CodeGen: Total=4.36 KB Peak=450.00 KB
*** Check failure stack trace: ***
    @          0x282ae6d  google::LogMessage::Fail()
    @          0x282d796  google::LogMessage::SendToLog()
    @          0x282a98d  google::LogMessage::Flush()
    @          0x282e23e  google::LogMessageFatal::~LogMessageFatal()
    @          0x165e469  impala::ExecNode::Close()
    @          0x17a0ddf  impala::SortNode::Close()
    @          0x19f00a5  impala::PlanFragmentExecutor::Close()
    @          0x19e7b65  impala::FragmentInstanceState::Exec()
    @          0x19f31df  impala::QueryExecMgr::ExecFInstance()
    @          0x19f672a  boost::_mfi::mf1<>::operator()()
    @          0x19f65cd  boost::_bi::list2<>::operator()<>()
    @          0x19f6291  boost::_bi::bind_t<>::operator()()
    @          0x19f5d68  boost::detail::function::void_function_obj_invoker0<>::invoke()
    @          0x134c6ae  boost::function0<>::operator()()
    @          0x15f5d23  impala::Thread::SuperviseThread()
    @          0x15fccfc  boost::_bi::list4<>::operator()<>()
    @          0x15fcc3f  boost::_bi::bind_t<>::operator()()
    @          0x15fcb9a  boost::detail::thread_data<>::run()
    @          0x1a5c45a  thread_proxy
    @     0x7f1ccc53170a  start_thread
    @     0x7f1ccc26782d  (unknown)
Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30000,server=y,suspend=n 
Wrote minidump to /home/tarmstrong/Impala/incubator-impala/logs/cluster/minidumps/impalad/13b81910-07d0-3765-4f496d56-3ed53e84.dmp
{code}

This only affects debug builds - on a release build the memory should eventually be cleaned up."	IMPALA	Resolved	1	1	9911	crash, resource-management
13055160	test_codegen_mem_limit  is flaky	"The test seems to exceed memory limit at another location so it doesn't consistently fail due to  exceeding memory limit during codegen.
The exhaustive RHEL7 build hit the following failure.

{noformat}

Error Message

query_test/test_query_mem_limit.py:123: in test_codegen_mem_limit     self.run_test_case('QueryTest/codegen-mem-limit', vector) common/impala_test_suite.py:324: in run_test_case     self.__verify_exceptions(test_section['CATCH'], str(e), use_db) common/impala_test_suite.py:215: in __verify_exceptions     (expected_str, actual_str) E   AssertionError: Unexpected exception string. Expected: Codegen failed to reserve E   Not found in actual: ImpalaBeeswaxException: Query aborted:Memory limit exceededFailed to allocate tuple bufferMemory Limit Exceeded by fragment: a44539d48545d15b:2  HDFS_SCAN_NODE (id=0) could not allocate 90.00 KB without exceeding limit.Query(a44539d48545d15b:ec0ea61000000000): Limit=100.00 KB Total=64.42 KB Peak=64.42 KB  Fragment a44539d48545d15b:2: Total=64.42 KB Peak=64.42 KB    HDFS_SCAN_NODE (id=0): Total=55.00 KB Peak=55.00 KB      Exprs: Total=8.00 KB Peak=8.00 KB    DataStreamSender (dst_id=1): Total=1.42 KB Peak=1.42 KB  Block Manager: Total=0 Peak=0
Stacktrace

query_test/test_query_mem_limit.py:123: in test_codegen_mem_limit
    self.run_test_case('QueryTest/codegen-mem-limit', vector)
common/impala_test_suite.py:324: in run_test_case
    self.__verify_exceptions(test_section['CATCH'], str(e), use_db)
common/impala_test_suite.py:215: in __verify_exceptions
    (expected_str, actual_str)
E   AssertionError: Unexpected exception string. Expected: Codegen failed to reserve
E   Not found in actual: ImpalaBeeswaxException: Query aborted:Memory limit exceededFailed to allocate tuple bufferMemory Limit Exceeded by fragment: a44539d48545d15b:2  HDFS_SCAN_NODE (id=0) could not allocate 90.00 KB without exceeding limit.Query(a44539d48545d15b:ec0ea61000000000): Limit=100.00 KB Total=64.42 KB Peak=64.42 KB  Fragment a44539d48545d15b:2: Total=64.42 KB Peak=64.42 KB    HDFS_SCAN_NODE (id=0): Total=55.00 KB Peak=55.00 KB      Exprs: Total=8.00 KB Peak=8.00 KB    DataStreamSender (dst_id=1): Total=1.42 KB Peak=1.42 KB  Block Manager: Total=0 Peak=0
Standard Error

-- connecting to: localhost:21000
-- executing against localhost:21000
use functional_parquet;

SET disable_codegen=True;
SET abort_on_error=1;
SET exec_single_node_rows_threshold=0;
SET batch_size=0;
SET num_nodes=0;
-- executing against localhost:21000
set mem_limit=100k;

-- executing against localhost:21000

select *
from alltypes
where substr(string_col, 1) = """";

-- executing against localhost:21000
SET MEM_LIMIT=0;;

{noformat}"	IMPALA	Resolved	1	1	9911	broken-build
13055173	Memory corruption of nested collection with MT_DOP > 0.	"A query that explodes and aggregates a nested collection appears to hang with mt_dop>0.

The reason is that the the in-memory collection slot has a ""garbage"" size that leads to infinite unnesting. It looks like the memory gets corrupted at some point, but I have not been able to make out where exactly yet.

Start Impala like this:
{code}
bin/start-impala-cluster.py -s 1 --impalad_args=""--default_query_options=mt_dop=1""
{code}

Query to repro:
{code}
select id, cnt from functional_parquet.complextypestbl t,
(select count(item) cnt from t.int_array) v;
{code}

Relevant code snippet from unnest-node.cc
{code}
  Tuple* tuple = containing_subplan_->current_input_row_->GetTuple(coll_tuple_idx_);
  if (tuple != NULL) {
    // Retrieve the collection value to be unnested directly from the tuple. We purposely
    // ignore the null bit of the slot because we may have set it in a previous Open() of
    // this same unnest node for projection.
    coll_value_ = reinterpret_cast<const CollectionValue*>(
        tuple->GetSlot(coll_slot_desc_->tuple_offset()));
    // Projection: Set the slot containing the collection value to NULL.
    tuple->SetNull(coll_slot_desc_->null_indicator_offset());
  } else {
    coll_value_ = &EMPTY_COLLECTION_VALUE;
    DCHECK_EQ(coll_value_->num_tuples, 0);
  }

At this point coll_value_->num_tuples appears to be garbage.
{code}"	IMPALA	Resolved	2	1	9911	crash, hang
13269427	Update to impyla 0.16.1 is not Python 2.6 compatible	"Centos 6 builds (which use Python 2.6) tripped up when starting Impala:
{noformat}
12:05:52 Traceback (most recent call last):
12:05:52   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-centos6-shard1/repos/Impala/bin/start-impala-cluster.py"", line 38, in <module>
12:05:52     from tests.common.impala_cluster import (ImpalaCluster, DEFAULT_BEESWAX_PORT,
12:05:52   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-centos6-shard1/repos/Impala/tests/common/impala_cluster.py"", line 35, in <module>
12:05:52     from tests.common.impala_service import (
12:05:52   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-centos6-shard1/repos/Impala/tests/common/impala_service.py"", line 29, in <module>
12:05:52     from tests.common.impala_connection import create_connection, create_ldap_connection
12:05:52   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-centos6-shard1/repos/Impala/tests/common/impala_connection.py"", line 26, in <module>
12:05:52     import impala.dbapi as impyla
12:05:52   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-centos6-shard1/repos/Impala/infra/python/env/lib/python2.6/site-packages/impala/dbapi.py"", line 28, in <module>
12:05:52     import impala.hiveserver2 as hs2
12:05:52   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-centos6-shard1/repos/Impala/infra/python/env/lib/python2.6/site-packages/impala/hiveserver2.py"", line 30, in <module>
12:05:52     from impala.interface import Connection, Cursor, _bind_parameters
12:05:52   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-centos6-shard1/repos/Impala/infra/python/env/lib/python2.6/site-packages/impala/interface.py"", line 229
12:05:52     if paramstyle in {'named', 'pyformat'}:
12:05:52                              ^
12:05:52 SyntaxError: invalid syntax{noformat}
That seems to be this line in impyla:

[https://github.com/cloudera/impyla/blob/master/impala/interface.py#L229]

It is the usage of a set literal that causes problems. The set literal could be replaced with a list.
{noformat}
Python 2.7.12 (default, Oct  8 2019, 14:14:10) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> 'a' in {'a', 'b'}
True
>>> 'c' in {'a', 'b'}
False
>>> 'a' in ('a', 'b')
True
>>> 'c' in ('a', 'b')
False
{noformat}"	IMPALA	Resolved	3	1	9911	broken-build
13145491	Better estimate of per-column compressed data size for low-NDV columns.	"In the previous IMPALA-4835 patch, we assumed that the ""ideal"" memory per Parquet column was 3 * 8MB, except when the total size of the file capped the total amount of memory we might use. This is often an overestimate, particular for smaller files, files with large numbers of columns, and highly compressible data.

We could do something smarter for Parquet given file sizes, per-partition row count, and column NDV. We can estimate row count per file by dividing the row count by the file size and estimate bytes per value with two methods:
* For fixed width types, estimating bytes per value based on the type width. We don't know what the physical parquet type is necessarily, but it seems reasonable to estimate based on the type declared in the table.
* log2(ndv) / 8, assuming that dictionary compression or general-purpose compression will kick in.

See https://docs.google.com/document/d/1kR0zfevNNUJom3sH1XmposacVZ-QALan7NSwnR5CkSA/edit#heading=h.a2b8e8h5a6en for some analysis. 
I looked at encoded lineitem data and saw that many of the scanned columns were 3-4MB in size and that we could have estimated an ideal size < 24MB per column based on the above heuristics."	IMPALA	Resolved	3	7	9911	resource-management
13055175	Crash in HdfsParquetScanner destructor with mt_dop > 0	"I was investigating IMPALA-4554 and attached a debugger to the running process. This caused the client to timeout and the query to get cancelled, at which point it hit this DCHECK:

{code}
Running on machine: tarmstrong-box
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
E1129 14:37:27.831372 27312 logging.cc:121] stderr will be logged to this file.
F1129 14:43:36.144094 27659 mem-pool.cc:69] Check failed: chunks_.empty() Must call FreeAll() or AcquireData() for this pool
*** Check failure stack trace: ***
    @          0x2ae2e46  google::DumpStackTraceAndExit()
    @          0x2adc2bd  google::LogMessage::Fail()
    @          0x2adebe6  google::LogMessage::SendToLog()
    @          0x2adbddd  google::LogMessage::Flush()
    @          0x2adf68e  google::LogMessageFatal::~LogMessageFatal()
    @          0x13d830d  impala::MemPool::~MemPool()
    @          0x15e7806  boost::checked_delete<>()
    @          0x17ef1a3  impala::HdfsParquetScanner::~HdfsParquetScanner()
    @          0x17ef22e  impala::HdfsParquetScanner::~HdfsParquetScanner()
    @          0x178bb63  boost::scoped_ptr<>::reset()
    @          0x17a73b9  impala::HdfsScanNodeMt::Close()
    @          0x173b447  impala::ExecNode::Close()
    @          0x1c84766  impala::PlanFragmentExecutor::Close()
    @          0x158b6ff  impala::FragmentMgr::FragmentExecState::Exec()
    @          0x157e376  impala::FragmentMgr::FragmentThread()
    @          0x15839fe  boost::_mfi::mf1<>::operator()()
    @          0x1583858  boost::_bi::list2<>::operator()<>()
    @          0x1583708  boost::_bi::bind_t<>::operator()()
    @          0x12e91f3  boost::function0<>::operator()()
    @          0x16b0846  impala::Thread::SuperviseThread()
    @          0x16b962b  boost::_bi::list4<>::operator()<>()
    @          0x16b94b8  boost::_bi::bind_t<>::operator()()
    @          0x1cf97ca  thread_proxy
    @     0x7f04552c170a  start_thread
    @     0x7f0454de182d  (unknown)
Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30000,server=y,suspend=n 
Wrote minidump to /home/tarmstrong/Impala/incubator-impala/logs/cluster/minidumps/impalad/1c9f5990-c9a3-f768-19a81f12-79c73058.dmp
{code}"	IMPALA	Resolved	1	1	9911	crash
13053954	Factor out Impala-lzo build logic	The Impala-lzo build depends on the Impala build scripts and headers, but we don't need to do an entire Impala build to produce the Impala-lzo .so. Factor out the build logic from buildall.sh so it can be built independently if needed.	IMPALA	Resolved	3	1	9911	build
13053816	Replace BufferedBlockMgr with new buffer pool	"We want to replace BufferedBlockMgr, a query-wide buffer pool, with a new BufferPool that is shared between all queries. The goals are:

* Support for guaranteed reservations: i.e. if a reservation is granted, the buffer pool will fulfil it (unless the OS is unable to fulfill the buffer pool's memory requirements).
* Simplified interaction between reservations and pins (if you have a reservation, you can pin, if you don't, you can't)
* Support for increasing reservations (up to a planner-specified limit).
* Support for smaller buffer sizes with similar performance (so we can reduce minimum memory requirement to execute spill-to-disk algorithm)
* Support for larger buffers to support wide rows
* Reduced reliance on TCMalloc, which isn't suited to management of large buffers (e.g. see IMPALA-2800)
* Better transfer model for buffer pool pages, so we can implement transfer-of-ownership consistently for row batches (instead of mixing transfer with MarkNeedsToReturn())."	IMPALA	Resolved	1	4	9911	resource-management
13052648	select trunc(dt) as dt ... group by 1 causes AnalysisException: select list expression not produced by aggregation output	"Running:
{code}select trunc(dt, 'DD') as dt, count( *) from test1 group by 1;{code}
...where the query creates a column with the same name as in the original table, then groups by the created column with 'group by 1' triggers AnalysisException.

Changing 'as dt' to 'as dt1' fixes the problem. This doesn't seem to happen with simple arithmetic ('id+1 as id').


Full testcase follows:
{code}
--drop table test1;
create table test1 (dt timestamp);
insert into test1 (dt) values ('2015-05-01');
insert into test1 (dt) values ('2015-05-02');
select * from test1;

select trunc(dt, 'DD') as dt, count( *) from test1 group by 1;
{code}

The complete text of the error message:

{quote}
ERROR: AnalysisException: select list expression not produced by aggregation output (missing from GROUP BY clause?): trunc(dt, 'DD')
{quote}

MySQL seems to handle a similar case fine: http://sqlfiddle.com/#!9/5b35d/1/0

There were no obvious duplicates. I can't test with a recent version easily, sorry about that."	IMPALA	Resolved	3	1	9911	planner, ramp-up, sql-language
13316813	Inlining functions in Sorter::Partition() gives a significant speedup.	"In TPC-H q67, the sort gets ~9% faster just from this small change.

{noformat}
AFTER
Fetched 100 row(s) in 3.27s
[localhost:21000] tpcds_parquet> summary;
+-----------------+--------+-------+----------+----------+---------+------------+-----------+---------------+---------------------------+
| Operator        | #Hosts | #Inst | Avg Time | Max Time | #Rows   | Est. #Rows | Peak Mem  | Est. Peak Mem | Detail                    |
+-----------------+--------+-------+----------+----------+---------+------------+-----------+---------------+---------------------------+
| F00:ROOT        | 1      | 1     | 402.93us | 402.93us |         |            | 0 B       | 0 B           |                           |
| 11:TOP-N        | 1      | 1     | 9.85ms   | 9.85ms   | 100     | 100        | 7.04 MB   | 9.38 KB       |                           |
| 10:ANALYTIC     | 1      | 1     | 102.50ms | 102.50ms | 514.59K | 15.09M     | 11.02 MB  | 4.00 MB       |                           |
| 09:SORT         | 1      | 1     | 304.39ms | 304.39ms | 514.59K | 15.09M     | 80.02 MB  | 38.00 MB      |                           |
| 08:AGGREGATE    | 1      | 1     | 178.87ms | 178.87ms | 514.59K | 15.09M     | 114.16 MB | 1.42 GB       | FINALIZE                  |
| 07:AGGREGATE    | 1      | 1     | 794.68ms | 794.68ms | 514.59K | 15.09M     | 257.67 MB | 16.98 GB      | FINALIZE                  |
| 06:HASH JOIN    | 1      | 1     | 33.95ms  | 33.95ms  | 535.97K | 2.88M      | 3.33 MB   | 2.88 MB       | INNER JOIN                |
| |--03:SCAN HDFS | 1      | 1     | 2.77ms   | 2.77ms   | 18.00K  | 18.00K     | 2.69 MB   | 80.00 MB      | tpcds_parquet.item        |
| 05:HASH JOIN    | 1      | 1     | 8.02ms   | 8.02ms   | 535.97K | 2.88M      | 2.05 MB   | 1.94 MB       | INNER JOIN                |
| |--02:SCAN HDFS | 1      | 1     | 1.18ms   | 1.18ms   | 12      | 12         | 69.00 KB  | 32.00 MB      | tpcds_parquet.store       |
| 04:HASH JOIN    | 1      | 1     | 10.22ms  | 10.22ms  | 535.97K | 2.88M      | 2.01 MB   | 1.94 MB       | INNER JOIN                |
| |--01:SCAN HDFS | 1      | 1     | 2.62ms   | 2.62ms   | 365     | 7.30K      | 1.33 MB   | 80.00 MB      | tpcds_parquet.date_dim    |
| 00:SCAN HDFS    | 1      | 1     | 9.06ms   | 9.06ms   | 535.97K | 2.88M      | 14.37 MB  | 64.00 MB      | tpcds_parquet.store_sales |
+-----------------+--------+-------+----------+----------+---------+------------+-----------+---------------+---------------------------+


BEFORE
Fetched 100 row(s) in 3.33s
[localhost:21000] tpcds_parquet> summary;
+-----------------+--------+-------+----------+----------+---su------+------------+-----------+---------------+---------------------------+
| Operator        | #Hosts | #Inst | Avg Time | Max Time | #Rows   | Est. #Rows | Peak Mem  | Est. Peak Mem | Detail                    |
+-----------------+--------+-------+----------+----------+---------+------------+-----------+---------------+---------------------------+
| F00:ROOT        | 1      | 1     | 414.22us | 414.22us |         |            | 0 B       | 0 B           |                           |
| 11:TOP-N        | 1      | 1     | 10.03ms  | 10.03ms  | 100     | 100        | 7.04 MB   | 9.38 KB       |                           |
| 10:ANALYTIC     | 1      | 1     | 101.26ms | 101.26ms | 514.59K | 15.09M     | 11.02 MB  | 4.00 MB       |                           |
| 09:SORT         | 1      | 1     | 334.83ms | 334.83ms | 514.59K | 15.09M     | 80.02 MB  | 38.00 MB      |                           |
| 08:AGGREGATE    | 1      | 1     | 221.69ms | 221.69ms | 514.59K | 15.09M     | 114.16 MB | 1.42 GB       | FINALIZE                  |
| 07:AGGREGATE    | 1      | 1     | 811.38ms | 811.38ms | 514.59K | 15.09M     | 257.68 MB | 16.98 GB      | FINALIZE                  |
| 06:HASH JOIN    | 1      | 1     | 30.35ms  | 30.35ms  | 535.97K | 2.88M      | 3.33 MB   | 2.88 MB       | INNER JOIN                |
| |--03:SCAN HDFS | 1      | 1     | 2.71ms   | 2.71ms   | 18.00K  | 18.00K     | 2.63 MB   | 80.00 MB      | tpcds_parquet.item        |
| 05:HASH JOIN    | 1      | 1     | 7.48ms   | 7.48ms   | 535.97K | 2.88M      | 2.07 MB   | 1.94 MB       | INNER JOIN                |
| |--02:SCAN HDFS | 1      | 1     | 869.04us | 869.04us | 12      | 12         | 69.00 KB  | 32.00 MB      | tpcds_parquet.store       |
| 04:HASH JOIN    | 1      | 1     | 9.77ms   | 9.77ms   | 535.97K | 2.88M      | 2.01 MB   | 1.94 MB       | INNER JOIN                |
| |--01:SCAN HDFS | 1      | 1     | 2.83ms   | 2.83ms   | 365     | 7.30K      | 1.33 MB   | 80.00 MB      | tpcds_parquet.date_dim    |
| 00:SCAN HDFS    | 1      | 1     | 8.31ms   | 8.31ms   | 535.97K | 2.88M      | 14.35 MB  | 64.00 MB      | tpcds_parquet.store_sales |
+-----------------+--------+-------+----------+----------+---------+------------+-----------+---------------+---------------------------+
{noformat}"	IMPALA	Resolved	3	4	9911	performance
13262729	S3 tests failing with FileNotFoundException getVersionMarkerItem on ../VERSION	"I've seen this happen several times now, S3 tests intermittently fail with an error such as:
{code:java}
Query aborted:InternalException: Error adding partitions E   CAUSED BY: MetaException: java.io.IOException: Got exception: java.io.FileNotFoundException getVersionMarkerItem on ../VERSION: com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException: Requested resource not found (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: 8T9IS939MDI7ASOB0IJCC34J3NVV4KQNSO5AEMVJF66Q9ASUAAJG) {code}"	IMPALA	Resolved	2	1	9911	broken-build, flaky
13052956	Check failed: slot_desc->type().type == TYPE_STRING (15 vs. 10)	"The query below runs fine on the stress test cluster when no mem limit is set but if the mem limit is 192M, then it crashes. (It ran 3 times in a row without a mem limit.)

This is using the randomness database on c2120.hal.cloudera.com.

{noformat}
Query: select (t2.int_col) * (t1.smallint_col_8) AS int_col,
COALESCE(t1.bigint_col_1, t1.bigint_col_1, t1.smallint_col_8) AS int_col_1
FROM table_15 t1
INNER JOIN (
SELECT
COALESCE(393, NULL, LEAD(-232, 51) OVER (ORDER BY t1.varchar0155_col_9 DESC, t1.char0130_col_15)) AS int_col
FROM table_17 t1
WHERE
(t1.decimal0402_col_2) != (t1.tinyint_col_8)
) t2 ON (t2.int_col) = (t1.tinyint_col_7)
Socket error 104: Connection reset by peer
{noformat}

Fatal log

{noformat}
F0914 13:52:49.012533 10937 sorter.cc:680] Check failed: slot_desc->type().type == TYPE_STRING (15 vs. 10)
{noformat}

Backtrace

{noformat}
#6  0x0000000002047f6d in google::LogMessageFatal::~LogMessageFatal (this=0x7f8e4018b450, __in_chrg=<value optimized out>) at src/logging.cc:1836
#7  0x00000000016f0349 in impala::Sorter::Run::GetNext<true> (this=0x9f50680, output_batch=0x22fbf0e0, eos=0x7f8e4018b5ef)
    at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/sorter.cc:680
#8  0x00000000016ea8cd in impala::Sorter::Run::GetNextBatch (this=0x9f50680, output_batch=0x94e36e0) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/sorter.cc:605
#9  0x00000000016f4d37 in boost::_mfi::mf1<impala::Status, impala::Sorter::Run, impala::RowBatch**>::operator() (this=0x94e36c8, p=0x9f50680, a1=0x94e36e0)
    at /opt/toolchain/boost-pic-1.55.0/include/boost/bind/mem_fn_template.hpp:165
#10 0x00000000016f4a4e in boost::_bi::list2<boost::_bi::value<impala::Sorter::Run*>, boost::arg<1> >::operator()<impala::Status, boost::_mfi::mf1<impala::Status, impala::Sorter::Run, impala::RowBatch**>, boost::_bi::list1<impala::RowBatch**&> > (this=0x94e36d8, f=..., a=...) at /opt/toolchain/boost-pic-1.55.0/include/boost/bind/bind.hpp:303
#11 0x00000000016f484e in boost::_bi::bind_t<impala::Status, boost::_mfi::mf1<impala::Status, impala::Sorter::Run, impala::RowBatch**>, boost::_bi::list2<boost::_bi::value<impala::Sorter::Run*>, boost::arg<1> > >::operator()<impala::RowBatch**> (this=0x94e36c8, a1=@0x7f8e4018b740) at /opt/toolchain/boost-pic-1.55.0/include/boost/bind/bind_template.hpp:32
#12 0x00000000016f4628 in boost::detail::function::function_obj_invoker1<boost::_bi::bind_t<impala::Status, boost::_mfi::mf1<impala::Status, impala::Sorter::Run, impala::RowBatch**>, boost::_bi::list2<boost::_bi::value<impala::Sorter::Run*>, boost::arg<1> > >, impala::Status, impala::RowBatch**>::invoke (function_obj_ptr=..., a0=0x94e36e0)
    at /opt/toolchain/boost-pic-1.55.0/include/boost/function/function_template.hpp:132
#13 0x00000000015677f0 in boost::function1<impala::Status, impala::RowBatch**>::operator() (this=0x94e36c0, a0=0x94e36e0)
    at /opt/toolchain/boost-pic-1.55.0/include/boost/function/function_template.hpp:767
#14 0x00000000015671cf in impala::SortedRunMerger::BatchedRowSupplier::Init (this=0x94e36c0, done=0x7f8e4018b837)
    at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/sorted-run-merger.cc:45
#15 0x000000000156657a in impala::SortedRunMerger::Prepare(const std::vector<boost::function<impala::Status(impala::RowBatch**)>, std::allocator<boost::function<impala::Status(impala::RowBatch**)> > > &) (this=0xb7a52b0, input_runs=Traceback (most recent call last):
  File ""/usr/lib64/../share/gdb/python/libstdcxx/v6/printers.py"", line 192, in display_hint
    itype0  = self.val.type.template_argument(0)
RuntimeError: syntax error, near `**)>, std::allocator<boost::function<'
std::vector of length 2, capacity 2 = {...}) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/sorted-run-merger.cc:133
#16 0x00000000016eeb96 in impala::Sorter::CreateMerger (this=0x22f5d180, num_runs=2) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/sorter.cc:1194
#17 0x00000000016ed3f9 in impala::Sorter::InputDone (this=0x22f5d180) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/sorter.cc:1023
#18 0x000000000168ccc2 in impala::SortNode::SortInput (this=0x2300f600, state=0xbe6e400) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/sort-node.cc:160
#19 0x000000000168bfb7 in impala::SortNode::Open (this=0x2300f600, state=0xbe6e400) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/sort-node.cc:76
#20 0x0000000001558dd8 in impala::PlanFragmentExecutor::OpenInternal (this=0xbe6ef28) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/plan-fragment-executor.cc:334
#21 0x0000000001558c7e in impala::PlanFragmentExecutor::Open (this=0xbe6ef28) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/plan-fragment-executor.cc:320
#22 0x000000000132d828 in impala::FragmentMgr::FragmentExecState::Exec (this=0xbe6ed00) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/service/fragment-exec-state.cc:50
#23 0x0000000001325e46 in impala::FragmentMgr::FragmentExecThread (this=0x83dcfc0, exec_state=0xbe6ed00) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/service/fragment-mgr.cc:70
#24 0x0000000001329836 in boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>::operator() (this=0xb76e820, p=0x83dcfc0, a1=0xbe6ed00)
    at /opt/toolchain/boost-pic-1.55.0/include/boost/bind/mem_fn_template.hpp:165
#25 0x00000000013295ef in boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> >::operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list0> (this=0xb76e830, f=..., a=...) at /opt/toolchain/boost-pic-1.55.0/include/boost/bind/bind.hpp:313
#26 0x0000000001328ed7 in boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >::operator() (this=0xb76e820) at /opt/toolchain/boost-pic-1.55.0/include/boost/bind/bind_template.hpp:20
#27 0x0000000001328825 in boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::FragmentMgr::FragmentExecState*>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::FragmentMgr::FragmentExecState*> > >, void>::invoke (function_obj_ptr=...)
    at /opt/toolchain/boost-pic-1.55.0/include/boost/function/function_template.hpp:153
#28 0x00000000011f603d in boost::function0<void>::operator() (this=0x7f8e4018ccc0) at /opt/toolchain/boost-pic-1.55.0/include/boost/function/function_template.hpp:767
#29 0x000000000140ecf2 in impala::Thread::SuperviseThread (name=""exec-plan-fragment"", category=""impala-server"", functor=..., thread_started=0x7f8e337f4b80)
    at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/util/thread.cc:314
{noformat}"	IMPALA	Resolved	3	1	9911	query_generator
13054274	Permission upon insert are wrong in hive warehouse table files	"Found an issue with permissions on warehouse.
The Warehouse /user/hive/warehouse was set to owner hive:hive with 771 permissions recursively. User was granted write privilege on table (tbl-1) on database (db-1).
Initially all grants were done with beeline.
Next the user switched to impala-shell and inserted some data into tbl-1. The permissions on the new hdfs file was the following:
ownership :  impala:hive
permissions:  751 i.e. read and execute on group.

The user cannot use insert overwrite via beeline sine the group hive has read only permissions.

The documentation: http://www.cloudera.com/documentation/enterprise/latest/topics/impala_insert.html has the following:

Related startup options:

By default, if an INSERT statement creates any new subdirectories underneath a partitioned table, those subdirectories are assigned default HDFS permissions for the impala user. To make each subdirectory have the same permissions as its parent directory in HDFS, specify the --insert_inherit_permissions startup option for the impalad daemon."	IMPALA	Resolved	4	1	9911	security
13202684	test_shutdown_executor fails with timeout waiting for query target state	"On a recent S3 test run test_shutdown_executor hit a timeout waiting for a query to reach state FINISHED. Instead the query stays at state 5 (EXCEPTION).

{noformat}
12:51:11 __________________ TestShutdownCommand.test_shutdown_executor __________________
12:51:11 custom_cluster/test_restart_services.py:209: in test_shutdown_executor
12:51:11     assert self.__fetch_and_get_num_backends(QUERY, before_shutdown_handle) == 3
12:51:11 custom_cluster/test_restart_services.py:356: in __fetch_and_get_num_backends
12:51:11     self.client.QUERY_STATES['FINISHED'], timeout=20)
12:51:11 common/impala_service.py:267: in wait_for_query_state
12:51:11     target_state, query_state)
12:51:11 E   AssertionError: Did not reach query state in time target=4 actual=5
{noformat}

From the logs I can see that the query fails because one of the executors becomes unreachable:

{noformat}
I1204 12:31:39.954125  5609 impala-server.cc:1792] Query a34c3a84775e5599:b2b25eb900000000: Failed due to unreachable impalad(s): jenkins-worker:22001
{noformat}

The query was {{select count\(*) from functional_parquet.alltypes where sleep(1) = bool_col}}. 

It seems that the query took longer than expected and was still running when the executor shut down.

I can reproduce by adding a sleep to the test:
{noformat}
diff --git a/tests/custom_cluster/test_restart_services.py b/tests/custom_cluster/test_restart_services.py
index e441cbc..32bc8a1 100644
--- a/tests/custom_cluster/test_restart_services.py
+++ b/tests/custom_cluster/test_restart_services.py
@@ -206,7 +206,7 @@ class TestShutdownCommand(CustomClusterTestSuite, HS2TestSuite):
     after_shutdown_handle = self.__exec_and_wait_until_running(QUERY)
 
     # Finish executing the first query before the backend exits.
-    assert self.__fetch_and_get_num_backends(QUERY, before_shutdown_handle) == 3
+    assert self.__fetch_and_get_num_backends(QUERY, before_shutdown_handle, delay=5) == 3
 
     # Wait for the impalad to exit, then start it back up and run another query, which
     # should be scheduled on it again.
@@ -349,11 +349,14 @@ class TestShutdownCommand(CustomClusterTestSuite, HS2TestSuite):
                 self.client.QUERY_STATES['RUNNING'], timeout=20)
     return handle
 
-  def __fetch_and_get_num_backends(self, query, handle):
+  def __fetch_and_get_num_backends(self, query, handle, delay=0):
     """"""Fetch the results of 'query' from the beeswax handle 'handle', close the
     query and return the number of backends obtained from the profile.""""""
     self.impalad_test_service.wait_for_query_state(self.client, handle,
                 self.client.QUERY_STATES['FINISHED'], timeout=20)
+    if delay > 0:
+      LOG.info(""sleeping for {0}"".format(delay))
+      time.sleep(delay)
     self.client.fetch(query, handle)
     profile = self.client.get_runtime_profile(handle)
     self.client.close_query(handle)
{noformat}"	IMPALA	Resolved	2	1	9911	broken-build
13054518	Certain join types fail to spill after they start probing if there are matches stored in the hash table	"The fix for IMPALA-1488 disabled spilling for certain hash join types (e.g. RIGHT OUTER JOIN, FULL OUTER JOIN) if there were any matches in the hash table. The underlying cause of this (if I understand correctly) is that we can spill partitions after we've built the hash table and started probing.

As part of IMPALA-3567 I need to fix the underlying issue so that we don't spill additional build partitions while probing. As a consequence of that work, it should be possible to disable the IMPALA-1488 workaround."	IMPALA	Resolved	4	1	9911	resource-management
13053706	Report which memory limit was exceeded	It would be helpful if memory limit exceeded messages report whether they hit a query limit or a process limit to aid with diagnosing the problem.	IMPALA	Resolved	3	4	9911	resource-management
13140073	Come up with reliable regression test for IMPALA-2376	"We don't have a good regression test for IMPALA-2376. With IMPALA-4835 and other changes it's increasingly difficult to get the query to OOM at the intended place. We probably need to construct some special test data with a very large nested collection to trigger this bug.

This is based on the TODO:
{noformat}
====
---- QUERY
# IMPALA-2376
# Set memory limit low enough to get the below query to consistently fail.
# TODO: change query and/or table so we can consistently check for the following error:
# Failed to allocate buffer for collection '...'.
set mem_limit=4m;
select max(cnt) from customer c,
(select count(l_returnflag) cnt from c.c_orders.o_lineitems) v;
---- TYPES
BIGINT
---- CATCH
Memory limit exceeded
{noformat}"	IMPALA	Resolved	3	6	9911	resource-management
13173007	 tests_statestore.py fails at assert len(args.topic_deltas[topic_name].topic_entries) == 1	"Assigning to [~tarmstrong@cloudera.com] given he recently did some Statestore related changes. Please feel free to reassign.

{noformat}
statestore/test_statestore.py:568: in test_update_with_clear_entries_flag
    .wait_for_update(topic_name, 2)
statestore/test_statestore.py:301: in wait_for_update
    self.check_thread_exceptions()
statestore/test_statestore.py:239: in check_thread_exceptions
    if self.exception is not None: raise self.exception
E   assert 2 == 1
E    +  where 2 = len([TTopicItem(deleted=False, value='bar0', key='old0'), TTopicItem(deleted=False, value='bar1', key='old1')])
E    +    where [TTopicItem(deleted=False, value='bar0', key='old0'), TTopicItem(deleted=False, value='bar1', key='old1')] = TTopicDelta(min_subscriber_topic_version=None, clear_topic_entries=None, from_version=0, topic_name='test_topic_255472...pic_entries=[TTopicItem(deleted=False, value='bar0', key='old0'), TTopicItem(deleted=False, value='bar1', key='old1')]).topic_entries

Traceback (most recent call last):
  File ""/data0/jenkins/workspace/impala-asf-master-core-local/repos/Impala/tests/statestore/test_statestore.py"", line 207, in UpdateState
    response = self.update_cb(self, args)
  File ""/data0/jenkins/workspace/impala-asf-master-core-local/repos/Impala/tests/statestore/test_statestore.py"", line 546, in check_entries
    assert len(args.topic_deltas[topic_name].topic_entries) == 1
{noformat}"	IMPALA	Resolved	2	1	9911	broken-build
13164859	session-expiry-test failed - unable to open ThriftServer port	"{noformat}
ThriftServer 'backend' (on port: 52436) exited due to TException: Could not bind: Transport endpoint is not connected
{noformat}

This is similar to IMPALA-5499 but is on CentOS 7:
{noformat}
# lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.4.1708 (Core) 
Release:	7.4.1708
Codename:	Core
{noformat}

It's in the ephemeral port range:
{noformat}
# cat /proc/sys/net/ipv4/ip_local_port_range
32768	60999
{noformat}"	IMPALA	Resolved	2	1	9911	flaky
13094905	COMPUTE STATS uses MT_DOP=4 by default	"Now that IMPALA-3905 has been completely addressed we should run COMPUTE STATS with MT_DOP=4 by default, regardless of file format. The motivation is consistency and speeding up COMPUTE STATS in most cases.

This task is a continuation of IMPALA-4572."	IMPALA	Resolved	3	7	9911	compute-stats
13161977	Failed test: query_test.test_scanners.TestScannerReservation.test_scanners	"Possibly flaky test:
{code:java}
Stacktrace
query_test/test_scanners.py:1064: in test_scanners
    self.run_test_case('QueryTest/scanner-reservation', vector)
common/impala_test_suite.py:451: in run_test_case
    verify_runtime_profile(test_section['RUNTIME_PROFILE'], result.runtime_profile)
common/test_result_verifier.py:590: in verify_runtime_profile
    actual))
E   AssertionError: Did not find matches for lines in runtime profile:
E   EXPECTED LINES:
E   row_regex:.*InitialRangeActualReservation.*Avg: 4.00 MB.*{code}

{noformat}
11:27:13 -- executing against localhost:21000
11:27:13 set debug_action=""-1:OPEN:SET_DENY_RESERVATION_PROBABILITY@1.0"";
11:27:13 
11:27:13 -- executing against localhost:21000
11:27:13 
11:27:13 select count(*)
11:27:13 from tpch.customer;
11:27:13 
11:27:13 -- executing against localhost:21000
11:27:13 SET DEBUG_ACTION="""";
11:27:13 
11:27:13 -- executing against localhost:21000
11:27:13 select min(l_comment)
11:27:13 from tpch_parquet.lineitem;
{noformat}

{noformat}

11:27:13 E              - SpilledPartitions: 0 (0)
11:27:13 E           HDFS_SCAN_NODE (id=0):(Total: 2s295ms, non-child: 2s295ms, % non-child: 100.00%)
11:27:13 E             Hdfs split stats (<volume id>:<# splits>/<split lengths>): -1:8/193.99 MB 
11:27:13 E             ExecOption: PARQUET Codegen Enabled, Codegen enabled: 8 out of 8
11:27:13 E             Hdfs Read Thread Concurrency Bucket: 0:80% 1:20% 2:0% 3:0% 4:0% 5:0% 6:0% 
11:27:13 E             File Formats: PARQUET/NONE:5 PARQUET/SNAPPY:3 
11:27:13 E             BytesRead(500.000ms): 0, 21.31 MB, 21.60 MB, 47.94 MB, 56.23 MB, 74.46 MB
11:27:13 E              - FooterProcessingTime: (Avg: 3.624ms ; Min: 999.979us ; Max: 9.999ms ; Number of samples: 8)
11:27:13 E              - InitialRangeActualReservation: (Avg: 21.50 MB (22544384) ; Min: 4.00 MB (4194304) ; Max: 24.00 MB (25165824) ; Number of samples: 8)
11:27:13 E              - InitialRangeIdealReservation: (Avg: 128.00 KB (131072) ; Min: 128.00 KB (131072) ; Max: 128.00 KB (131072) ; Number of samples: 8)
11:27:13 E              - ParquetRowGroupActualReservation: (Avg: 24.00 MB (25165824) ; Min: 24.00 MB (25165824) ; Max: 24.00 MB (25165824) ; Number of samples: 3)
11:27:13 E              - ParquetRowGroupIdealReservation: (Avg: 24.00 MB (25165824) ; Min: 24.00 MB (25165824) ; Max: 24.00 MB (25165824) ; Number of samples: 3)
11:27:13 E              - AverageHdfsReadThreadConcurrency: 0.20 
11:27:13 E              - AverageScannerThreadConcurrency: 1.00 
11:27:13 E              - BytesRead: 74.55 MB (78175787)
11:27:13 E              - BytesReadDataNodeCache: 0
11:27:13 E              - BytesReadLocal: 0
11:27:13 E              - BytesReadRemoteUnexpected: 0
11:27:13 E              - BytesReadShortCircuit: 0
11:27:13 E              - CachedFileHandlesHitCount: 0 (0)
11:27:13 E              - CachedFileHandlesMissCount: 11 (11)
11:27:13 E              - CollectionItemsRead: 0 (0)
11:27:13 E              - DecompressionTime: 345.992ms
11:27:13 E              - MaxCompressedTextFileLength: 0
11:27:13 E              - NumColumns: 1 (1)
11:27:13 E              - NumDictFilteredRowGroups: 0 (0)
11:27:13 E              - NumDisksAccessed: 2 (2)
11:27:13 E              - NumRowGroups: 3 (3)
11:27:13 E              - NumScannerThreadReservationsDenied: 0 (0)
11:27:13 E              - NumScannerThreadsStarted: 1 (1)
11:27:13 E              - NumScannersWithNoReads: 5 (5)
11:27:13 E              - NumStatsFilteredRowGroups: 0 (0)
11:27:13 E              - PeakMemoryUsage: 26.40 MB (27684346)
11:27:13 E              - PerReadThreadRawHdfsThroughput: 140.94 MB/sec
11:27:13 E              - RemoteScanRanges: 0 (0)
11:27:13 E              - RowBatchQueueGetWaitTime: 2s179ms
11:27:13 E              - RowBatchQueuePutWaitTime: 0.000ns
11:27:13 E              - RowsRead: 6.00M (6001215)
11:27:13 E              - RowsReturned: 6.00M (6001215)
11:27:13 E              - RowsReturnedRate: 2.61 M/sec
11:27:13 E              - ScanRangesComplete: 8 (8)
11:27:13 E              - ScannerThreadsInvoluntaryContextSwitches: 6.62K (6622)
11:27:13 E              - ScannerThreadsTotalWallClockTime: 2s509ms
11:27:13 E                - MaterializeTupleTime(*): 980.979ms
11:27:13 E                - ScannerThreadsSysTime: 181.972ms
11:27:13 E                - ScannerThreadsUserTime: 965.853ms
11:27:13 E              - ScannerThreadsVoluntaryContextSwitches: 19 (19)
11:27:13 E              - TotalRawHdfsOpenFileTime(*): 21.999ms
11:27:13 E              - TotalRawHdfsReadTime(*): 528.988ms
11:27:13 E              - TotalReadThroughput: 24.82 MB/sec
11:27:13 E             Buffer pool:
11:27:13 E                - AllocTime: 0.000ns
11:27:13 E                - CumulativeAllocationBytes: 73.00 MB (76546048)
11:27:13 E                - CumulativeAllocations: 17 (17)
11:27:13 E                - PeakReservation: 24.00 MB (25165824)
11:27:13 E                - PeakUnpinnedBytes: 0
11:27:13 E                - PeakUsedReservation: 24.00 MB (25165824)
11:27:13 E                - ReadIoBytes: 0
11:27:13 E                - ReadIoOps: 0 (0)
11:27:13 E                - ReadIoWaitTime: 0.000ns
11:27:13 E                - WriteIoBytes: 0
11:27:13 E                - WriteIoOps: 0 (0)
11:27:13 E                - WriteIoWaitTime: 0.000ns
{noformat}"	IMPALA	Resolved	1	1	9911	broken-build, test-failure
13053133	buffered-block-mgr.cc:891] Check failed: Validate()	"After ~4 hrs and ~10k queries a stress run crashed saying

{noformat}
impala-stress-5.vpc.cloudera.com crashed:
F1008 18:38:58.361202 10472 buffered-block-mgr.cc:891] Check failed: Validate() 
Buffered block mgr
  Num writes outstanding: 0
  Num free io buffers: 5
  Num unpinned blocks: 35
  Num available buffers: -6
  Total pinned buffers: 454
  Unfullfilled reserved buffers: 334
  Remaining memory: 2422471680 (#blocks=288)
  Block write threshold: 4
[...skipped...]
#6  0x000000000205f4ed in google::LogMessageFatal::~LogMessageFatal (this=0x7f8fdcb03280, __in_chrg=<value optimized out>) at src/logging.cc:1836
#7  0x0000000001231ba7 in impala::BufferedBlockMgr::DeleteBlock (this=0x7cb7c00, block=0x568c06c0) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/buffered-block-mgr.cc:891
#8  0x000000000122993e in impala::BufferedBlockMgr::Block::Delete (this=0x568c06c0) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/buffered-block-mgr.cc:133
#9  0x0000000001574135 in impala::BufferedTupleStream::Close (this=0xc773b560) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/buffered-tuple-stream.cc:159
#10 0x000000000168d012 in impala::PartitionedHashJoinNode::BuildHashTables (this=0x6cc0fb00, state=0x1b3346d00) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:1237
#11 0x000000000168757d in impala::PartitionedHashJoinNode::ProcessBuildInput (this=0x6cc0fb00, state=0x1b3346d00, level=1) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:666
#12 0x000000000168891d in impala::PartitionedHashJoinNode::PrepareNextPartition (this=0x6cc0fb00, state=0x1b3346d00) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:781
#13 0x000000000168a253 in impala::PartitionedHashJoinNode::GetNext (this=0x6cc0fb00, state=0x1b3346d00, out_batch=0x1d8dd5860, eos=0x6cc11be1) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:941
#14 0x000000000168787f in impala::PartitionedHashJoinNode::NextProbeRowBatch (this=0x6cc11a80, state=0x1b3346d00, out_batch=0xec5932c0) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:692
#15 0x0000000001689f47 in impala::PartitionedHashJoinNode::GetNext (this=0x6cc11a80, state=0x1b3346d00, out_batch=0xec5932c0, eos=0x3d0165361) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:916
#16 0x000000000168787f in impala::PartitionedHashJoinNode::NextProbeRowBatch (this=0x3d0165200, state=0x1b3346d00, out_batch=0x6c4574a0) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:692
#17 0x0000000001689f47 in impala::PartitionedHashJoinNode::GetNext (this=0x3d0165200, state=0x1b3346d00, out_batch=0x6c4574a0, eos=0x3d0167761) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:916
#18 0x000000000168787f in impala::PartitionedHashJoinNode::NextProbeRowBatch (this=0x3d0167600, state=0x1b3346d00, out_batch=0x7f8fdcb04b20) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:692
#19 0x0000000001689f47 in impala::PartitionedHashJoinNode::GetNext (this=0x3d0167600, state=0x1b3346d00, out_batch=0x7f8fdcb04b20, eos=0x7f8fdcb04d3f) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-hash-join-node.cc:916
#20 0x000000000166eba7 in impala::PartitionedAggregationNode::Open (this=0x28844c600, state=0x1b3346d00) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/exec/partitioned-aggregation-node.cc:240
#21 0x000000000155fc4b in impala::PlanFragmentExecutor::OpenInternal (this=0x1e2d7dd28) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/plan-fragment-executor.cc:334
#22 0x000000000155faec in impala::PlanFragmentExecutor::Open (this=0x1e2d7dd28) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/runtime/plan-fragment-executor.cc:320
#23 0x0000000001333060 in impala::FragmentMgr::FragmentExecState::Exec (this=0x1e2d7db00) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/service/fragment-exec-state.cc:50
#24 0x000000000132b67e in impala::FragmentMgr::FragmentExecThread (this=0x5b92ea0, exec_state=0x1e2d7db00) at /usr/src/debug/impala-2.3.0-cdh5.5.0-SNAPSHOT/be/src/service/fragment-mgr.cc:70
{noformat}

I'll post the location of the logs/core dump when it is available. The cluster will be up until tomorrow if anyone wants to look now. The crashed node was impala-stress-5.vpc.cloudera.com."	IMPALA	Resolved	1	1	9911	stress
13055184	Incorrect results under ASAN with mt_dop > 0 and old aggs/joins	"There are 7300 rows in alltypes but count(*) returns varying numbers.

{code}
[tarmstrong-box.ca.cloudera.com:21000] > use functional_parquet; set mt_dop=1; select count(*) from alltypes; summary;
Query: use functional_parquet
MT_DOP set to 1
Query: select count(*) from alltypes
Query submitted at: 2016-11-30 22:31:17 (Coordinator: http://tarmstrong-box:25000)
Query progress can be monitored at: http://tarmstrong-box:25000/query_plan?query_id=3346df982e7ba933:b9cbaacf00000000
+----------+
| count(*) |
+----------+
| 900      |
+----------+
Fetched 1 row(s) in 0.13s
+--------------+--------+----------+----------+-------+------------+----------+---------------+-----------------------------+
| Operator     | #Hosts | Avg Time | Max Time | #Rows | Est. #Rows | Peak Mem | Est. Peak Mem | Detail                      |
+--------------+--------+----------+----------+-------+------------+----------+---------------+-----------------------------+
| 03:AGGREGATE | 1      | 762.35us | 762.35us | 1     | 1          | 8.04 MB  | -1 B          | FINALIZE                    |
| 02:EXCHANGE  | 1      | 56.20us  | 56.20us  | 3     | 1          | 0 B      | -1 B          | UNPARTITIONED               |
| 01:AGGREGATE | 3      | 541.97us | 571.22us | 3     | 1          | 8.02 MB  | 10.00 MB      |                             |
| 00:SCAN HDFS | 3      | 6.00ms   | 7.60ms   | 900   | 7.30K      | 16.00 KB | 0 B           | functional_parquet.alltypes |
+--------------+--------+----------+----------+-------+------------+----------+---------------+-----------------------------+
{code}"	IMPALA	Resolved	1	1	9911	correctness, multithreading
13232081	bootstrap_toolchain.py failed: 'ascii' codec can't encode character u'\u2018' in position 742: ordinal not in range(128)	"{noformat}
18:38:17 2019-05-06 18:38:17,093 Thread-203 INFO: Downloading https://native-toolchain.s3.amazonaws.com/build/cdh_components/1055188/tarballs/hadoop-3.0.0-cdh6.x-SNAPSHOT.tar.gz to /data/jenkins/workspace/impala-asf-master-exhaustive-release/Impala-Toolchain/cdh_components-1055188/hadoop-3.0.0-cdh6.x-SNAPSHOT.tar.gz (attempt 1)
18:38:17 2019-05-06 18:38:17,093 Thread-204 INFO: Downloading https://native-toolchain.s3.amazonaws.com/build/cdh_components/1055188/tarballs/sentry-2.1.0-cdh6.x-SNAPSHOT.tar.gz to /data/jenkins/workspace/impala-asf-master-exhaustive-release/Impala-Toolchain/cdh_components-1055188/sentry-2.1.0-cdh6.x-SNAPSHOT.tar.gz (attempt 1)
18:38:17 2019-05-06 18:38:17,093 Thread-202 INFO: Downloading https://native-toolchain.s3.amazonaws.com/build/cdh_components/1055188/tarballs/hbase-2.1.0-cdh6.x-SNAPSHOT.tar.gz to /data/jenkins/workspace/impala-asf-master-exhaustive-release/Impala-Toolchain/cdh_components-1055188/hbase-2.1.0-cdh6.x-SNAPSHOT.tar.gz (attempt 1)
18:38:17 2019-05-06 18:38:17,094 Thread-205 INFO: Downloading https://native-toolchain.s3.amazonaws.com/build/cdh_components/1055188/tarballs/hive-2.1.1-cdh6.x-SNAPSHOT.tar.gz to /data/jenkins/workspace/impala-asf-master-exhaustive-release/Impala-Toolchain/cdh_components-1055188/hive-2.1.1-cdh6.x-SNAPSHOT.tar.gz (attempt 1)
18:38:21 2019-05-06 18:38:21,430 Thread-205 INFO: Extracting hive-2.1.1-cdh6.x-SNAPSHOT.tar.gz
18:38:23 2019-05-06 18:38:23,031 Thread-203 INFO: Extracting hadoop-3.0.0-cdh6.x-SNAPSHOT.tar.gz
18:38:24 2019-05-06 18:38:24,012 Thread-205 INFO: Downloading https://native-toolchain.s3.amazonaws.com/build/cdh_components/1055188/tarballs/kudu-1.10.0-cdh6.x-SNAPSHOT-redhat7.tar.gz to /data/jenkins/workspace/impala-asf-master-exhaustive-release/Impala-Toolchain/cdh_components-1055188/kudu-1.10.0-cdh6.x-SNAPSHOT-redhat7.tar.gz (attempt 1)
18:38:37 2019-05-06 18:38:37,805 Thread-202 INFO: Extracting hbase-2.1.0-cdh6.x-SNAPSHOT.tar.gz
18:38:38 2019-05-06 18:38:38,786 Thread-205 INFO: Extracting kudu-1.10.0-cdh6.x-SNAPSHOT-redhat7.tar.gz
18:43:33 Traceback (most recent call last):
18:43:33   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-release/repos/Impala/bin/bootstrap_toolchain.py"", line 564, in <module>
18:43:33     download_cdh_components(toolchain_root, cdh_components, download_path_prefix)
18:43:33   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-release/repos/Impala/bin/bootstrap_toolchain.py"", line 437, in download_cdh_components
18:43:33     execute_many(download, cdh_components)
18:43:33   File ""/data/jenkins/workspace/impala-asf-master-exhaustive-release/repos/Impala/bin/bootstrap_toolchain.py"", line 399, in execute_many
18:43:33     return pool.map(f, args, 1)
18:43:33   File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 250, in map
18:43:33     return self.map_async(func, iterable, chunksize).get()
18:43:33   File ""/usr/lib64/python2.7/multiprocessing/pool.py"", line 554, in get
18:43:33     raise self._value
18:43:33 UnicodeEncodeError: 'ascii' codec can't encode character u'\u2018' in position 742: ordinal not in range(128)
{noformat}

Unfortunately I can see where the original exception came from. I'll see if I can add something that would print the backtrace."	IMPALA	Resolved	2	1	9911	flaky
13064526	Clean up TCMalloc memory maintenance logic	"As mentioned in IMPALA-2800, TCMalloc's default behaviour so that it does not keep unmapped memory around. This means that our various ReleaseMemory()/ReleaseToSystem() calls may not have any effect.

We should verify this and then remove any logic that is no longer needed."	IMPALA	Resolved	3	4	9911	resource-management
13195276	Disabling ORC scanner can cause query hang	"{noformat}
$ start-impala-cluster.py --impalad_args=--enable_orc_scanner=false
> select * from functional_orc_def.alltypes;
{noformat}

The error in the impalad logs is:
{noformat}
E1030 17:36:12.035167  5934 ImpaladCatalog.java:217] Error adding catalog object: null
Java exception follows:
java.lang.NullPointerException
        at org.apache.impala.catalog.Table.fromThrift(Table.java:286)
        at org.apache.impala.catalog.ImpaladCatalog.addTable(ImpaladCatalog.java:397)
        at org.apache.impala.catalog.ImpaladCatalog.addCatalogObject(ImpaladCatalog.java:284)
        at org.apache.impala.catalog.ImpaladCatalog.updateCatalog(ImpaladCatalog.java:215)
        at org.apache.impala.service.FeCatalogManager$CatalogdImpl.updateCatalogCache(FeCatalogManager.java:105)
        at org.apache.impala.service.Frontend.updateCatalogCache(Frontend.java:264)
        at org.apache.impala.service.JniFrontend.updateCatalogCache(JniFrontend.java:187)
{noformat}"	IMPALA	Resolved	1	1	9911	hang
13123363	Various crashes and incorrect results on CPUs with AVX512	"M5 and C5 instances use a different hypervisor than M4 and C4. In EC2 C5 and M5 instances, data loading fails. An interesting snippet from the end of an impalad log:

{noformat}
I1207 04:12:07.922456 19933 coordinator.cc:99] Exec() query_id=944ead2f178cf67e:1755131f00000000 stmt=CREATE TABLE tmp_orders_string AS 
          SELECT STRAIGHT_JOIN
            o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate,
            o_orderpriority, o_clerk, o_shippriority, o_comment,
            GROUP_CONCAT(
              CONCAT(
                CAST(l_partkey AS STRING), '\005',
                CAST(l_suppkey AS STRING), '\005',
                CAST(l_linenumber AS STRING), '\005',
                CAST(l_quantity AS STRING), '\005',
                CAST(l_extendedprice AS STRING), '\005',
                CAST(l_discount AS STRING), '\005',
                CAST(l_tax AS STRING), '\005',
                CAST(l_returnflag AS STRING), '\005',
                CAST(l_linestatus AS STRING), '\005',
                CAST(l_shipdate AS STRING), '\005',
                CAST(l_commitdate AS STRING), '\005',
                CAST(l_receiptdate AS STRING), '\005',
                CAST(l_shipinstruct AS STRING), '\005',
                CAST(l_shipmode AS STRING), '\005',
                CAST(l_comment AS STRING)
              ), '\004'
            ) AS lineitems_string
          FROM tpch_parquet.lineitem
          INNER JOIN [SHUFFLE] tpch_parquet.orders ON o_orderkey = l_orderkey
          WHERE o_orderkey % 1 = 0
          GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9
...
F1207 04:12:08.972215 19953 partitioned-hash-join-node.cc:291] Check failed: probe_batch_pos_ == probe_batch_->num_rows() || probe_batch_pos_ == -1 
{noformat}

The error log shows:

{noformat}
F1207 04:12:08.972215 19953 partitioned-hash-join-node.cc:291] Check failed: probe_batch_pos_ == probe_batch_->num_rows() || probe_batch_pos_ == -1 
*** Check failure stack trace: ***
    @          0x3bdcefd  google::LogMessage::Fail()
    @          0x3bde7a2  google::LogMessage::SendToLog()
    @          0x3bdc8d7  google::LogMessage::Flush()
    @          0x3bdfe9e  google::LogMessageFatal::~LogMessageFatal()
    @          0x28bd4db  impala::PartitionedHashJoinNode::NextProbeRowBatch()
    @          0x28c1741  impala::PartitionedHashJoinNode::GetNext()
    @          0x289f71f  impala::PartitionedAggregationNode::GetRowsStreaming()
    @          0x289d8d5  impala::PartitionedAggregationNode::GetNext()
    @          0x1891d1c  impala::FragmentInstanceState::ExecInternal()
    @          0x188f629  impala::FragmentInstanceState::Exec()
    @          0x1878c0a  impala::QueryState::ExecFInstance()
    @          0x18774cc  _ZZN6impala10QueryState15StartFInstancesEvENKUlvE_clEv
    @          0x1879849  _ZN5boost6detail8function26void_function_obj_invoker0IZN6impala10QueryState15StartFInstancesEvEUlvE_vE6invokeERNS1_15function_bufferE
    @          0x17c64ba  boost::function0<>::operator()()
    @          0x1abb5a1  impala::Thread::SuperviseThread()
    @          0x1ac412c  boost::_bi::list4<>::operator()<>()
    @          0x1ac406f  boost::_bi::bind_t<>::operator()()
    @          0x1ac4032  boost::detail::thread_data<>::run()
    @          0x2d668ca  thread_proxy
    @     0x7fe9287146ba  start_thread
    @     0x7fe92844a3dd  clone
Picked up JAVA_TOOL_OPTIONS: -agentlib:jdwp=transport=dt_socket,address=30002,server=y,suspend=n 
{noformat}

To reproduce this, start a M5.4xlarge with 250GB space

{noformat}
sudo apt-get update
sudo apt-get install --yes git
git init ~/Impala
pushd ~/Impala
git fetch https://github.com/apache/impala master
git checkout FETCH_HEAD
./bin/bootstrap_development.sh | tee -a $(mktemp -p ~)
{noformat}

You might need to fiddle with the default security group; I'm not sure. You can test on an M4.4xlarge, since the above script should work there."	IMPALA	Resolved	1	1	9911	correctness, crash
13052950	ASAN Crashes In Join Tests	"The following tests fail:

{code}
query_test.test_udfs.TestUdfs.test_ir_functions[exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none]
query_test.test_join_queries.TestJoinQueries.test_single_node_nested_loop_joins[batch_size: 0 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none]
query_test.test_join_queries.TestTPCHJoinQueries.test_outer_joins[batch_size: 0 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none]
query_test.test_limit.TestLimit.test_limit[query: select * from lineitem limit %d | limit_value: 1 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/none]
query_test.test_limit.TestLimit.test_limit[query: select * from lineitem limit %d | limit_value: 5 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: text/gzip/block]
query_test.test_limit.TestLimit.test_limit[query: select * from lineitem limit %d | limit_value: 10 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: seq/gzip/block]
query_test.test_limit.TestLimit.test_limit[query: select * from lineitem limit %d | limit_value: 5000 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: seq/snap/block]
query_test.test_limit.TestLimit.test_limit[query: select * from lineitem limit %d | limit_value: 5000 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: rc/none]
query_test.test_limit.TestLimit.test_limit[query: select * from lineitem limit %d | limit_value: 10 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: avro/none]
query_test.test_limit.TestLimit.test_limit[query: select * from lineitem limit %d | limit_value: 5 | exec_option: {'disable_codegen': False, 'abort_on_error': 1, 'exec_single_node_rows_threshold': 0, 'batch_size': 0, 'num_nodes': 0} | table_format: avro/snap/block]
{code}"	IMPALA	Resolved	3	1	9911	broken-build
13270511	Hash table lookup should be read-only	"For IMPALA-9156, we need concurrent lookups to the hash table to be thread safe. We are pretty close to that, except a few stats are maintained in HashTable and would be modified concurrently from multiple threads.

We should modify those  places to update the stats in HashTableCtx instead."	IMPALA	Resolved	3	4	9911	multithreading
13339214	test_shell_interactive.TestImpalaShellInteractive AssertionError: alter query should be closed	"shell.test_shell_interactive.TestImpalaShellInteractive.test_ddl_queries_are_closed[table_format_and_file_extension: ('textfile', '.txt') | protocol: beeswax](from pytest)
Failing for the past 1 build (Since[!https://master-02.jenkins.cloudera.com/static/4975bb2b/images/16x16/red.png! #196|https://master-02.jenkins.cloudera.com/view/Impala/view/Evergreen-asf-master/job/impala-asf-master-core-s3-data-cache/lastCompletedBuild/])
[Took 22 sec.|https://master-02.jenkins.cloudera.com/view/Impala/view/Evergreen-asf-master/job/impala-asf-master-core-s3-data-cache/lastCompletedBuild/testReport/shell.test_shell_interactive/TestImpalaShellInteractive/test_ddl_queries_are_closed_table_format_and_file_extension____textfile_____txt_____protocol__beeswax_/history]

[!https://master-02.jenkins.cloudera.com/static/4975bb2b/images/16x16/notepad.png! add description|https://master-02.jenkins.cloudera.com/view/Impala/view/Evergreen-asf-master/job/impala-asf-master-core-s3-data-cache/lastCompletedBuild/testReport/shell.test_shell_interactive/TestImpalaShellInteractive/test_ddl_queries_are_closed_table_format_and_file_extension____textfile_____txt_____protocol__beeswax_/editDescription]
h3. Error Message

AssertionError: alter query should be closed assert <bound method ImpaladService.wait_for_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x7f49541b2150>>(0) + where <bound method ImpaladService.wait_for_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x7f49541b2150>> = <tests.common.impala_service.ImpaladService object at 0x7f49541b2150>.wait_for_num_in_flight_queries
h3. Stacktrace

/data/jenkins/workspace/impala-asf-master-core-s3-data-cache/repos/Impala/tests/shell/test_shell_interactive.py:467: in test_ddl_queries_are_closed assert impalad.wait_for_num_in_flight_queries(0), MSG % 'alter' E AssertionError: alter query should be closed E assert <bound method ImpaladService.wait_for_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x7f49541b2150>>(0) E + where <bound method ImpaladService.wait_for_num_in_flight_queries of <tests.common.impala_service.ImpaladService object at 0x7f49541b2150>> = <tests.common.impala_service.ImpaladService object at 0x7f49541b2150>.wait_for_num_in_flight_queries"	IMPALA	Resolved	3	1	9911	broken-build, flaky
13185858	"Incorrect results when querying primary = ""\"""" in Kudu and HBase"	"Version string from catalogd web ui:
{noformat}
catalogd version 3.1.0-cdh6.x-SNAPSHOT RELEASE (build 8baac7f5849b6bacb02fedeb9b3fe2b2ee9450ee)
{noformat}
A reproduction script for the impala-shell:
{noformat}
create table test(name string, primary key(name) ) stored as kudu;

insert into test values (""\"""");
-- Modified 1 row(s), 0 row error(s) in 4.01s

-- row found in full table scan
select * from test;
-- Fetched 1 row(s) in 0.15s

-- row not found on = predicate (pushed to kudu)
select * from test where name=""\"""";
-- Fetched 0 row(s) in 0.13s

-- row found when predicate cannot be pushed to kudu
select * from test where name like ""\"""";
-- Fetched 1 row(s) in 0.13s
{noformat}
This was originally reported asKUDU-2575. I tried to reproduce directly against Kudu using the python client but got the expected result.

From the plan and profile, Impala is pushing down the predicate, but Kudu is not being scanned, possibly because the Kudu client short-circuits the scan as having no results based on the predicate Impala pushes down.
{noformat}
00:SCAN KUDU [default.test]
   kudu predicates: name = '""'
   mem-estimate=0B mem-reservation=0B thread-reservation=1
   tuple-ids=0 row-size=15B cardinality=unavailable
   in pipelines: 00(GETNEXT)
{noformat}
{noformat}
KUDU_SCAN_NODE (id=0)
          - AverageScannerThreadConcurrency: 0.00 (0.0)
          - InactiveTotalTime: 0ns (0)
          - KuduRemoteScanTokens: 0 (0)
          - MaterializeTupleTime(*): 0ns (0)
          - NumScannerThreadMemUnavailable: 0 (0)
          - NumScannerThreadsStarted: 1 (1)
          - PeakMemoryUsage: 24.0 KiB (24576)
          - PeakScannerThreadConcurrency: 1 (1)
          - RowBatchBytesEnqueued: 16.0 KiB (16384)
          - RowBatchQueueGetWaitTime: 0ns (0)
          - RowBatchQueuePeakMemoryUsage: 0 B (0)
          - RowBatchQueuePutWaitTime: 0ns (0)
          - RowBatchesEnqueued: 1 (1)
          - RowsRead: 0 (0)
===>  - RowsReturned: 0 (0)
          - RowsReturnedRate: 0 per second (0)
          - ScanRangesComplete: 1 (1)
          - ScannerThreadsInvoluntaryContextSwitches: 0 (0)
          - ScannerThreadsTotalWallClockTime: 0ns (0)
            - ScannerThreadsSysTime: 158.00us (158000)
            - ScannerThreadsUserTime: 0ns (0)
          - ScannerThreadsVoluntaryContextSwitches: 2 (2)
===>  - TotalKuduScanRoundTrips: 0 (0)
          - TotalTime: 1ms (1999972)
{noformat}
I also confirmed Kudu sees no scan from Impala for this query using the /scans page of the tablet servers.

Full profile attached."	IMPALA	Resolved	1	1	9911	correctness, kudu
13086646	"Remove ""unlimited"" process mem_limit option"	"The ability to set an ""unlimited"" process mem limit is unnecessary (since you can configure the process limit to be arbitrarily large) and causes some complications in the code. It is also not tested automatically. We should remove this support in 2.11 after we officially deprecate it in 2.10 (IMPALA-5652)."	IMPALA	Resolved	4	3	9911	resource-management
13053395	ASAN build calls llvm-symbolizer with invalid arguments on build machines.	"I get the following error on the build machines: it looks like it's using llvm-symbolizer from llvm3.3 but I think ASAN is built with a later version of llvm - maybe this is the problem.

This means it's difficult to get a useful stack trace.

{code}
==30832==ERROR: AddressSanitizer: heap-use-after-free on address 0x60300000bdb8 at pc 0x0000024988dc bp 0x7f339e6d9880 sp 0x7f339e6d9878
READ of size 1 at 0x60300000bdb8 thread T9942
llvm-symbolizer: Unknown command line argument '--default-arch=x86_64'.  Try: '/opt/toolchain/llvm-3.3/bin/llvm-symbolizer -help'
llvm-symbolizer: Did you mean '-demangle=x86_64'?
==30832==WARNING: Can't read from symbolizer at fd 346
llvm-symbolizer: Unknown command line argument '--default-arch=x86_64'.  Try: '/opt/toolchain/llvm-3.3/bin/llvm-symbolizer -help'
llvm-symbolizer: Did you mean '-demangle=x86_64'?
==30832==WARNING: Can't read from symbolizer at fd 346
llvm-symbolizer: Unknown command line argument '--default-arch=x86_64'.  Try: '/opt/toolchain/llvm-3.3/bin/llvm-symbolizer -help'
llvm-symbolizer: Did you mean '-demangle=x86_64'?
==30832==WARNING: Can't read from symbolizer at fd 346
llvm-symbolizer: Unknown command line argument '--default-arch=x86_64'.  Try: '/opt/toolchain/llvm-3.3/bin/llvm-symbolizer -help'
llvm-symbolizer: Did you mean '-demangle=x86_64'?
==30832==WARNING: Can't read from symbolizer at fd 8
==30832==WARNING: Failed to use and restart external symbolizer!
    #0 0x24988db  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x24988db)
    #1 0x2497a4a  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x2497a4a)
    #2 0x24982e8  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x24982e8)
    #3 0xf5349e  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0xf5349e)
    #4 0x1610da2  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x1610da2)
    #5 0xf5c4b7  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0xf5c4b7)
    #6 0x183231f  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x183231f)
    #7 0x183247e  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x183247e)
    #8 0x13eb067  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x13eb067)
    #9 0x13eac51  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x13eac51)
    #10 0x1595eb5  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x1595eb5)
    #11 0x1594a80  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x1594a80)
    #12 0x1594018  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x1594018)
    #13 0x158a34d  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x158a34d)
    #14 0x158a08b  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x158a08b)
    #15 0x188d64e  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x188d64e)
    #16 0x188a8a8  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x188a8a8)
    #17 0x18891a8  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x18891a8)
    #18 0x1890717  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x1890717)
    #19 0x1386f32  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x1386f32)
    #20 0x16a4f66  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x16a4f66)
    #21 0x16b172a  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x16b172a)
    #22 0x16b15b7  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x16b15b7)
    #23 0x1b043e9  (/data/jenkins/workspace/impala-master-64bit-PRIVATE/repos/Impala/be/build/debug/service/impalad+0x1b043e9)
    #24 0x344a407850  (/lib64/libpthread.so.0+0x344a407850)
    #25 0x344a0e894c  (/lib64/libc.so.6+0x344a0e894c)

llvm-symbolizer: Unknown command line argument '--default-arch=x86_64'.  Try: '/opt/toolchain/llvm-3.3/bin/llvm-symbolizer -help'
llvm-symbolizer: Did you mean '-demangle=x86_64'?
{code}"	IMPALA	Resolved	3	1	9911	test-infra
13164583	Fix detection and handling of Device Mapper volumes	"I've heard a couple of instances of Impala doing the wrong thing when multiple scratch directories are present on a single DeviceMapper logical volume. E.g. in the following setup, we only use one directory as a disk.
{noformat}
>> mount list
/dev/mapper/xx01-lv_data01 on /data01 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx02-lv_data02 on /data02 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx03-lv_data03 on /data03 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx04-lv_data04 on /data04 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx05-lv_data05 on /data05 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx06-lv_data06 on /data06 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx07-lv_data07 on /data07 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx08-lv_data08 on /data08 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx09-lv_data09 on /data09 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx10-lv_data10 on /data10 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx11-lv_data11 on /data11 type ext4 (rw,relatime,data=ordered)
/dev/mapper/xx12-lv_data12 on /data12 type ext4 (rw,relatime,data=ordered)

Scratch dirs are:
/data01/impala/impalad,/data02/impala/impalad,/data03/impala/impalad,/data04/impala/impalad,/data05/impala/impalad,/data06/impala/impalad,/data07/impala/impalad,/data08/impala/impalad,/data09/impala/impalad,/data10/impala/impalad,/data11/impala/impalad,/data12/impala/impalad


~~~ tmp-file-mgr.cc:122] Using scratch directory /data01/impala/impalad/impala-scratch on disk13. 
{noformat}

A workaround is to set --allow_multiple_scratch_dirs_per_device=true

There are a few questions here:
# Does the deduplication logic even make sense to have?
# Do we need to fix how we treat Device mapper volumes?"	IMPALA	Resolved	3	1	9911	resource-management
13053487	ExprCodegenTest failed to load IR file	"http://sandbox.jenkins.cloudera.com/view/Impala/view/Nightly-Builds/job/impala-cdh5-exhaustive-release/21/testReport/(root)/ExprCodegenTest/TestInlineConstants/

{noformat}
/data/jenkins/workspace/impala-cdh5-exhaustive-release/repos/Impala/be/src/exprs/expr-codegen-test.cc:245
Value of: status.ok()
  Actual: false
Expected: true
{noformat}

Relevant code snippet:
{noformat}
  // Get TestGetConstant() IR function
  stringstream test_udf_file;
  test_udf_file << getenv(""IMPALA_HOME"") << ""/be/build/debug/exprs/expr-codegen-test.ll"";
  scoped_ptr<LlvmCodeGen> codegen;
  status = LlvmCodeGen::LoadFromFile(&pool, test_udf_file.str(), ""test"", &codegen);
  ASSERT_TRUE(status.ok());
{noformat}

Unfortunately we do not capture the BE test logs, see IMPALA-2808."	IMPALA	Resolved	3	7	9911	codegen, test-infra
13053499	Build failure due to exception in test_redaction.py. Failed to kill an impalad within 10 seconds.	"Tim, assigning to you since you changed that code last. Maybe the 10 seconds timeout is too aggressive?

See:
http://sandbox.jenkins.cloudera.com/view/Impala/view/Build/job/impala-master-cdh5-trunk/

{code}
22:30:22 =================================== FAILURES ===================================
22:30:22 _________________________ TestRedaction.test_bad_rules _________________________
22:30:22 
22:30:22 self = <test_redaction.TestRedaction testMethod=test_bad_rules>
22:30:22 
22:30:22     @pytest.mark.execute_serially
22:30:22     def test_bad_rules(self):
22:30:22       '''Check that the server fails to start if the redaction rules are bad.'''
22:30:22       startup_options = dict()
22:30:22       self.assert_server_fails_to_start('{ ""version"": 100 }', startup_options,
22:30:22 >         'Error parsing redaction rules; only version 1 is supported')
22:30:22 
22:30:22 custom_cluster/test_redaction.py:204: 
22:30:22 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
22:30:22 
22:30:22 self = <test_redaction.TestRedaction testMethod=test_bad_rules>
22:30:22 rules = '{ ""version"": 100 }', start_options = {}
22:30:22 expected_error_message = 'Error parsing redaction rules; only version 1 is supported'
22:30:22 
22:30:22     def assert_server_fails_to_start(self, rules, start_options, expected_error_message):
22:30:22       try:
22:30:22         self.start_cluster_using_rules(rules, **start_options)
22:30:22         self.fail('Cluster should not have started but did')
22:30:22       except Exception:
22:30:22         if self.cluster.impalads:
22:30:22 >         raise Exception(""No impalads should have started"")
22:30:22 E         Exception: No impalads should have started
22:30:22 
22:30:22 custom_cluster/test_redaction.py:118: Exception
22:30:22 ----------------------------- Captured stderr call -----------------------------
22:30:22 MainThread: tmp_dir is /tmp/tmpvfmKfP
22:30:22 Traceback (most recent call last):
22:30:22   File ""/data/jenkins/workspace/impala-master-cdh5-trunk/repos/Impala/bin/start-impala-cluster.py"", line 328, in <module>
22:30:22     kill_cluster_processes(force=options.force_kill)
22:30:22   File ""/data/jenkins/workspace/impala-master-cdh5-trunk/repos/Impala/bin/start-impala-cluster.py"", line 126, in kill_cluster_processes
22:30:22     kill_matching_processes(binaries, force)
22:30:22   File ""/data/jenkins/workspace/impala-master-cdh5-trunk/repos/Impala/bin/start-impala-cluster.py"", line 146, in kill_matching_processes
22:30:22     process.pid, KILL_TIMEOUT_IN_SECONDS))
22:30:22 RuntimeError: Unable to kill impalad (pid 21258) after 10 seconds.
22:30:22 MainThread: Found 1 impalad/0 statestored/0 catalogd process(es)
22:30:22  generated xml file: /data/jenkins/workspace/impala-master-cdh5-trunk/repos/Impala/tests/custom_cluster/results/TEST-impala-custom-cluster.xml 
{code}"	IMPALA	Resolved	1	1	9911	broken-build
13164713	TestDescribeTableResults started failing because of OwnerType	"{noformat}
23:27:32   AuthorizationTest.TestDescribeTableResults:1963->verifyOutputWithOptionalData:1988 Size difference. Expected: [# col_name, data_type, comment, , NULL, NULL, id, int, NULL, bool_col, boolean, NULL, tinyint_col, tinyint, NULL, smallint_col, smallint, NULL, int_col, int, NULL, bigint_col, bigint, NULL, float_col, float, NULL, double_col, double, NULL, date_string_col, string, NULL, string_col, string, NULL, timestamp_col, timestamp, NULL, , NULL, NULL, # Partition Information, NULL, NULL, # col_name, data_type, comment, , NULL, NULL, year, int, NULL, month, int, NULL, day, int, NULL, , NULL, NULL, # Detailed Table Information, NULL, NULL, Database:, functional, NULL, Owner:, *, NULL, CreateTime:, *, NULL, LastAccessTime:, UNKNOWN, NULL, Retention:, 0, NULL, Location:, hdfs://localhost:20500/test-warehouse/alltypesagg, NULL, Table Type:, EXTERNAL_TABLE, NULL, Table Parameters:, NULL, NULL, , DO_NOT_UPDATE_STATS, true, , EXTERNAL, TRUE, , STATS_GENERATED, TASK, , impala.lastComputeStatsTime, *, , numRows, 11000, , totalSize, 834279, , transient_lastDdlTime, *, , NULL, NULL, # Storage Information, NULL, NULL, SerDe Library:, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, NULL, InputFormat:, org.apache.hadoop.mapred.TextInputFormat, NULL, OutputFormat:, org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, NULL, Compressed:, No, NULL, Num Buckets:, 0, NULL, Bucket Columns:, [], NULL, Sort Columns:, [], NULL, Storage Desc Params:, NULL, NULL, , escape.delim, \\, , field.delim, ,, , serialization.format, ,] Actual: [# col_name, data_type, comment, , NULL, NULL, id, int, NULL, bool_col, boolean, NULL, tinyint_col, tinyint, NULL, smallint_col, smallint, NULL, int_col, int, NULL, bigint_col, bigint, NULL, float_col, float, NULL, double_col, double, NULL, date_string_col, string, NULL, string_col, string, NULL, timestamp_col, timestamp, NULL, , NULL, NULL, # Partition Information, NULL, NULL, # col_name, data_type, comment, , NULL, NULL, year, int, NULL, month, int, NULL, day, int, NULL, , NULL, NULL, # Detailed Table Information, NULL, NULL, Database:, functional, NULL, OwnerType:, USER, NULL, Owner:, jenkins, NULL, CreateTime:, Wed Jun 06 21:50:33 PDT 2018, NULL, LastAccessTime:, UNKNOWN, NULL, Retention:, 0, NULL, Location:, hdfs://localhost:20500/test-warehouse/alltypesagg, NULL, Table Type:, EXTERNAL_TABLE, NULL, Table Parameters:, NULL, NULL, , DO_NOT_UPDATE_STATS, true, , EXTERNAL, TRUE, , STATS_GENERATED, TASK, , impala.lastComputeStatsTime, 1528349431, , numRows, 11000, , totalSize, 834279, , transient_lastDdlTime, 1528347033, , NULL, NULL, # Storage Information, NULL, NULL, SerDe Library:, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, NULL, InputFormat:, org.apache.hadoop.mapred.TextInputFormat, NULL, OutputFormat:, org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, NULL, Compressed:, No, NULL, Num Buckets:, 0, NULL, Bucket Columns:, [], NULL, Sort Columns:, [], NULL, Storage Desc Params:, NULL, NULL, , escape.delim, \\, , field.delim, ,, , serialization.format, ,] expected:<150> but was:<153>
23:27:32   AuthorizationTest.TestDescribeTableResults:1963->verifyOutputWithOptionalData:1988 Size difference. Expected: [# col_name, data_type, comment, , NULL, NULL, id, int, NULL, bool_col, boolean, NULL, tinyint_col, tinyint, NULL, smallint_col, smallint, NULL, int_col, int, NULL, bigint_col, bigint, NULL, float_col, float, NULL, double_col, double, NULL, date_string_col, string, NULL, string_col, string, NULL, timestamp_col, timestamp, NULL, , NULL, NULL, # Partition Information, NULL, NULL, # col_name, data_type, comment, , NULL, NULL, year, int, NULL, month, int, NULL, day, int, NULL, , NULL, NULL, # Detailed Table Information, NULL, NULL, Database:, functional, NULL, Owner:, *, NULL, CreateTime:, *, NULL, LastAccessTime:, UNKNOWN, NULL, Retention:, 0, NULL, Location:, hdfs://localhost:20500/test-warehouse/alltypesagg, NULL, Table Type:, EXTERNAL_TABLE, NULL, Table Parameters:, NULL, NULL, , DO_NOT_UPDATE_STATS, true, , EXTERNAL, TRUE, , STATS_GENERATED, TASK, , impala.lastComputeStatsTime, *, , numRows, 11000, , totalSize, 834279, , transient_lastDdlTime, *, , NULL, NULL, # Storage Information, NULL, NULL, SerDe Library:, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, NULL, InputFormat:, org.apache.hadoop.mapred.TextInputFormat, NULL, OutputFormat:, org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, NULL, Compressed:, No, NULL, Num Buckets:, 0, NULL, Bucket Columns:, [], NULL, Sort Columns:, [], NULL, Storage Desc Params:, NULL, NULL, , escape.delim, \\, , field.delim, ,, , serialization.format, ,] Actual: [# col_name, data_type, comment, , NULL, NULL, id, int, NULL, bool_col, boolean, NULL, tinyint_col, tinyint, NULL, smallint_col, smallint, NULL, int_col, int, NULL, bigint_col, bigint, NULL, float_col, float, NULL, double_col, double, NULL, date_string_col, string, NULL, string_col, string, NULL, timestamp_col, timestamp, NULL, , NULL, NULL, # Partition Information, NULL, NULL, # col_name, data_type, comment, , NULL, NULL, year, int, NULL, month, int, NULL, day, int, NULL, , NULL, NULL, # Detailed Table Information, NULL, NULL, Database:, functional, NULL, OwnerType:, USER, NULL, Owner:, jenkins, NULL, CreateTime:, Wed Jun 06 21:50:33 PDT 2018, NULL, LastAccessTime:, UNKNOWN, NULL, Retention:, 0, NULL, Location:, hdfs://localhost:20500/test-warehouse/alltypesagg, NULL, Table Type:, EXTERNAL_TABLE, NULL, Table Parameters:, NULL, NULL, , DO_NOT_UPDATE_STATS, true, , EXTERNAL, TRUE, , STATS_GENERATED, TASK, , impala.lastComputeStatsTime, 1528349431, , numRows, 11000, , totalSize, 834279, , transient_lastDdlTime, 1528347033, , NULL, NULL, # Storage Information, NULL, NULL, SerDe Library:, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, NULL, InputFormat:, org.apache.hadoop.mapred.TextInputFormat, NULL, OutputFormat:, org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, NULL, Compressed:, No, NULL, Num Buckets:, 0, NULL, Bucket Columns:, [], NULL, Sort Columns:, [], NULL, Storage Desc Params:, NULL, NULL, , escape.delim, \\, , field.delim, ,, , serialization.format, ,] expected:<150> but was:<153>
{noformat}

The newly-added fields are:
{noformat}
 OwnerType:, USER, NULL,
{noformat}

I suspect HIVE-19374 is somehow related."	IMPALA	Resolved	1	1	9911	broken-build
13071315	Consider always copying-out Disk I/O buffers instead of attaching to RowBatches	"IMPALA-4835 would be greatly simplified if we don't have to attach disk I/O buffers to RowBatches and handle the resultant complexity.

Disk I/O buffers currently need to be attached to RowBatches if the row batches directly reference var-len data in the buffer. The cases when this can occur are as follows:
* The column being read contains strings
* The string data is not dictionary encoded in Parquet (since we copy out the dictionary data in Parquet)
* The string data is not compressed with a general-purpose compression algorithm (GZip, snappy, etc).

This includes the following cases: plain-encoded strings in uncompressed Parquet; any strings in uncompressed text, RCFile, Avro, or sequence file.

In those cases the copy avoidance could provide some performance benefits. However it's unclear that any of those file formats are/should be used in performance-critical use cases, because the storage density of uncompressed strings is almost always terrible.

We should evaluate the performance impact of the additional copies, but I suspect that it is not severe and does not impact any important use cases."	IMPALA	Resolved	3	7	9911	resource-management
13065161	Try using TCMalloc + Huge Pages for buffers	"As well as mmap() + huge pages, we could support TCMalloc + huge pages. I believe TCMalloc will support this well because:
* We can allocate huge-page-aligned memory via posix_memalign()
* TCMalloc, by default, always decommits large allocations upon freeing them. So if we undo the HUGEPAGE madvise() before handing the pages back to TCMalloc, then TCMalloc will decommit the huge pages backing the allocation, and we won't get TCMalloc's page heap into a weird state.

This could give us the perf benefits of huge pages without some of the headaches associated with using mmap() directly."	IMPALA	Resolved	3	4	9911	perf, resource-management
13053596	LLVM : High spinlock contention during concurrency	"When hitting the same coordinator with high concurrency there appears to be high CPU utilization in the kernel, these are the LLVM call stacks originating to the Spin lock. 
The contention goes away when Codegen is disabled. 

{code}
CPU Time
1 of 101: 45.3% (166.658s of 368.193s)

libpthread.so.0 ! _L_unlock_657 - [unknown source file]
impalad ! llvm::sys::MutexImpl::release + 0x10 - [unknown source file]
impalad ! llvm::PassRegistry::getPassInfo + 0x150 - [unknown source file]
impalad ! llvm::PMTopLevelManager::findAnalysisPass + 0xa1 - [unknown source file]
impalad ! llvm::PMDataManager::verifyPreservedAnalysis + 0x62 - [unknown source file]
impalad ! llvm::FPPassManager::runOnFunction + 0x1a6 - [unknown source file]
impalad ! (anonymous namespace)::CGPassManager::runOnModule + 0x653 - [unknown source file]
impalad ! llvm::MPPassManager::runOnModule + 0x2d7 - [unknown source file]
impalad ! llvm::PassManagerImpl::run + 0xe0 - [unknown source file]
impalad ! impala::LlvmCodeGen::OptimizeModule + 0x376 - llvm-codegen.cc:740
impalad ! impala::LlvmCodeGen::FinalizeModule + 0x15b - llvm-codegen.cc:666
impalad ! impala::PlanFragmentExecutor::OptimizeLlvmModule + 0x51 - scoped_ptr.hpp:284
impalad ! impala::PlanFragmentExecutor::Open + 0x26b - plan-fragment-executor.cc:326
impalad ! impala::FragmentMgr::FragmentExecState::Exec + 0x17 - fragment-exec-state.cc:51
impalad ! impala::FragmentMgr::FragmentThread + 0x271 - fragment-mgr.cc:86
impalad ! boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>::operator() + 0x42 - mem_fn_template.hpp:165
impalad ! operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list0> - bind.hpp:313
impalad ! boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId>>>, void>::invoke + 0x7 - function_template.hpp:153
impalad ! boost::function0<void>::operator() + 0x1a - function_template.hpp:767
impalad ! impala::Thread::SuperviseThread + 0x20c - thread.cc:316
impalad ! operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> + 0x5a - bind.hpp:457
impalad ! boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>>::run + 0x19 - thread.hpp:116
impalad ! thread_proxy + 0xd9 - [unknown source file]
libpthread.so.0 ! start_thread + 0xd0 - [unknown source file]
libc.so.6 ! clone + 0x6c - [unknown source file]
{code}

And 
{code}
CPU Time
1 of 101: 46.5% (112.156s of 241.416s)

libpthread.so.0 ! _L_lock_892 - [unknown source file]
impalad ! llvm::sys::MutexImpl::acquire + 0x10 - [unknown source file]
impalad ! llvm::PassRegistry::getPassInfo + 0x197 - [unknown source file]
impalad ! llvm::PMTopLevelManager::findAnalysisPass + 0xa1 - [unknown source file]
impalad ! llvm::PMDataManager::verifyPreservedAnalysis + 0x62 - [unknown source file]
impalad ! llvm::FPPassManager::runOnFunction + 0x1a6 - [unknown source file]
impalad ! (anonymous namespace)::CGPassManager::runOnModule + 0x653 - [unknown source file]
impalad ! llvm::MPPassManager::runOnModule + 0x2d7 - [unknown source file]
impalad ! llvm::PassManagerImpl::run + 0xe0 - [unknown source file]
impalad ! impala::LlvmCodeGen::OptimizeModule + 0x376 - llvm-codegen.cc:740
impalad ! impala::LlvmCodeGen::FinalizeModule + 0x15b - llvm-codegen.cc:666
impalad ! impala::PlanFragmentExecutor::OptimizeLlvmModule + 0x51 - scoped_ptr.hpp:284
impalad ! impala::PlanFragmentExecutor::Open + 0x26b - plan-fragment-executor.cc:326
impalad ! impala::FragmentMgr::FragmentExecState::Exec + 0x17 - fragment-exec-state.cc:51
impalad ! impala::FragmentMgr::FragmentThread + 0x271 - fragment-mgr.cc:86
impalad ! boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>::operator() + 0x42 - mem_fn_template.hpp:165
impalad ! operator()<boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list0> - bind.hpp:313
impalad ! boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::function::void_function_obj_invoker0<boost::_bi::bind_t<void, boost::_mfi::mf1<void, impala::FragmentMgr, impala::TUniqueId>, boost::_bi::list2<boost::_bi::value<impala::FragmentMgr*>, boost::_bi::value<impala::TUniqueId>>>, void>::invoke + 0x7 - function_template.hpp:153
impalad ! boost::function0<void>::operator() + 0x1a - function_template.hpp:767
impalad ! impala::Thread::SuperviseThread + 0x20c - thread.cc:316
impalad ! operator()<void (*)(const std::basic_string<char>&, const std::basic_string<char>&, boost::function<void()>, impala::Promise<long int>*), boost::_bi::list0> + 0x5a - bind.hpp:457
impalad ! boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>::operator() - bind_template.hpp:20
impalad ! boost::detail::thread_data<boost::_bi::bind_t<void, void (*)(std::string const&, std::string const&, boost::function<void (void)>, impala::Promise<long>*), boost::_bi::list4<boost::_bi::value<std::string>, boost::_bi::value<std::string>, boost::_bi::value<boost::function<void (void)>>, boost::_bi::value<impala::Promise<long>*>>>>::run + 0x19 - thread.hpp:116
impalad ! thread_proxy + 0xd9 - [unknown source file]
libpthread.so.0 ! start_thread + 0xd0 - [unknown source file]
libc.so.6 ! clone + 0x6c - [unknown source file]
{code}

And from perf 
|Samples||Process ||Module||Function|
|46.41%   |impalad|  [kernel.kallsyms]|            [k] _spin_lock                          |
|11.56%   |impalad|  perf-51814.map|               [.] 0x00007f3b29293079                   |      
 |5.03%   |impalad|  impalad|               [.] impala::HdfsParquetScanner::ScalarColumnReader<long, true>::ReadNonRepeatedValue(impa|
 |2.44%   |impalad|  impalad|               [.] bool impala::HdfsParquetScanner::AssembleRows<false, false>(impala::TupleDescriptor c|
 |1.93%   |impalad|  impalad|               [.] bool impala::RleDecoder::Get<int>(int*)                  |
 |1.33%   |impalad|  [kernel.kallsyms]|            [k] update_curr                         |
 |1.07%   |impalad|  impalad|               [.] bool impala::HdfsParquetScanner::BaseScalarColumnReader::NextLevels<false>()         |
 |0.88%   |impalad|  [kernel.kallsyms]|            [k] _spin_lock_irqsave                         |


This is the query used
{code}
select count(*) from supplier_2g_1 /* +SCHEDULE_REMOTE,RANDOM_REPLICA */, nation where s_nationkey = n_nationkey group by s_nationkey;
{code}"	IMPALA	Resolved	1	1	9911	codegen, performance
13337473	Fix analytic limit pushdown when predicates are present	This is to fix case 1 of the parent JIRA.	IMPALA	Resolved	1	7	9911	correctness
13055297	Set up query mem tracker in QueryState	We should hang the query MemTracker off QueryState instead of relying on a static map. This will be required for IMPALA-3200 in order to set up the query's reservation in the QueryState	IMPALA	Resolved	3	7	9911	resource-management
13137528	Impala should expose when the last row is fetched	"This is exposed indirectly via the ""waiting to be closed"" list in the Impala debug server. However, there is no way to tell from the profile or query state that the last row was fetched by the client.

It is useful for this to be exposed to management tools since queries where all rows have been fetched have released resources and could be closed."	IMPALA	Resolved	3	4	9911	admission-control, observability
13099095	Parquet scanner does not free local allocations in filter contexts	"This problem can occur if runtime filter expressions that are evaluated in the scan allocate temporary memory - ""local allocations"". These accumulate for each scan range and are only 
freed upon scan range completion. 

A contrived query that exhibits the problem is the following. If I continue adding upper() and lower() to the expression the memory consumption of the scan node will continue to grow - up to 100MB for each extra function call!

{code}
set runtime_filter_wait_time_ms=1000000;
select straight_join count(*) from tpch_parquet.lineitem l1 join tpch_parquet.lineitem l2 on upper(lower(upper(lower(l1.l_comment)))) = concat(l2.l_comment, 'foo');
summary;
{code}

I think other conjuncts in the scanner may be affected by the same problem, e.g. the min_max conjuncts."	IMPALA	Resolved	3	1	9911	resource-management
13054490	Add support for caching PyPi packages	As part of the ASF migration we're moving away from checking python packages into the repository, and instead downloading them via PyPi. We don't want to hit the PyPi servers for every build, so instead we should support a local cache or repository.	IMPALA	Resolved	4	1	9911	asf
13088454	Eagerly release reservation in blocking nodes	"The new buffer pool code releases memory from aggregations at a later point than the old code: the old code will release the memory partition-by-partition as the output rows are emitted.

This is particularly useful with pre/merge aggregations since the preaggregation will be shrinking at the same time as the merge is growing.

With the new code, if all remaining partitions are in memory we can safely release reservation before we move to the next partition. E.g. in NextPartition() we can have something like:

{code}
  if (spilled_partitions_.empty()) ReleaseUnusedReservation();
{code}

where ReleaseUnusedReservation() will release any unused reservation above the minimum back to the query reservation.


A similar problem exists for sorts."	IMPALA	Resolved	3	7	9911	resource-management
