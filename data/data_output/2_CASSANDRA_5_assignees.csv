id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
12553420	Preserve commitlog size cap when recycling segments at startup	"1. Create a single node cluster, use default configuration, use cassandra.bat to start the server:

2. run the following commands in cli:
{code}
create keyspace toto;
use toto;
create column family titi;
truncate titi;
{code}

3. the node dies with this error:
{code}
ERROR 23:23:02,118 Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Map failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:202)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:159)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(Unknown Source)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:119)
        ... 5 more
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        ... 7 more
 INFO 23:23:02,122 Stop listening to thrift clients
 INFO 23:23:02,123 Waiting for messaging service to quiesce
 INFO 23:23:02,125 MessagingService shutting down server thread.
{code}"	CASSANDRA	Resolved	10003	1	5408	commitlog
12525557	truncate can still result in data being replayed after a restart	Our first stab at fixing this was CASSANDRA-2950.	CASSANDRA	Resolved	10002	1	5408	commitlog
12529885	Problem SliceByNamesReadCommand on super column family after flush operation	"I'm having a problem with doing a multiget_slice on a super column family
after its first flush. Updates to the column values work properly, but
trying to retrieve the updated values using a multiget_slice operation fail
to get the updated values. Instead they return the values from before the
flush. The problem is not apparent with standard column families.

I've seen this problem in Cassandra v1.0.0 and v1.0.1. The problem
is not present in Cassandra v0.7.6.

Steps to reproduce:

   1. Create one or more super column entries
   2. Verify the sub column values can be updated and that you can retrieve
   the new values
   3. Use nodetool to flush the column family or restart cassandra
   4. Update the sub column values
   5. Verify they have been updated using cassandra-cli
   6. Verify you *DO NOT* get the updated values when doing a
   multiget_slice; instead you get the old values from before the flush

You can get the most recent value by doing a flush followed by a major
compaction. However, future updates are not retrieved properly either.

With debug turned on, it looks like the multiget_slice query uses the
following command/consistency level:
SliceByNamesReadCommand(table='test_cassandra', key=666f6f,
columnParent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', columns=[foo,])/QUORUM.

Cassandra-cli uses the following command/consistency level for a get_slice:
SliceFromReadCommand(table='test_cassandra', key='666f6f',
column_parent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', start='', finish='', reversed=false,
count=1000000)/QUORUM

Notice the test program gets 'bar2' for the column values and cassandra-cli
gets 'bar3' for the column values:

tcpdump from test program using hector-core:1.0-1

16:46:07.424562 IP iam.47158 > iam.9160: Flags [P.], seq 55:138, ack 30,
win 257, options [nop,nop,TS val 27474096 ecr 27474095], length 83
E....#@.@.PK.........6#.....].8......{.....
..8...8.........multiget_slice................foo..........test................foo.........
16:46:07.424575 IP iam.9160 > iam.47158: Flags [.], ack 138, win 256,
options [nop,nop,TS val 27474096 ecr 27474096], length 0
E..4..@.@.<.........#..6].8..........(.....
..8...8.
16:46:07.428771 IP iam.9160 > iam.47158: Flags [P.], seq 30:173, ack 138,
win 256, options [nop,nop,TS val 27474097 ecr 27474096], length 143
@.@.<&........#..6].8................
............foo...............foo...............foo1.......bar2
........6h........foo2.......bar2
........I.....


tcpdump of cassandra-cli:

16:30:55.945123 IP iam.47134 > iam.9160: Flags [P.], seq 370:479, ack 5310,
win 387, options [nop,nop,TS val 27246226 ecr 27241207], length 109
E.....@.@.9q..........#..n.X\
.............
................get_range_slices..............test.........................................................d.........
16:30:55.945152 IP iam.9160 > iam.47134: Flags [.], ack 479, win 256,
options [nop,nop,TS val 27246226 ecr 27246226], length 0
E..4..@.@."".........#...\
...n.......(.....
........
16:30:55.949245 IP iam.9160 > iam.47134: Flags [P.], seq 5310:5461, ack
479, win 256, options [nop,nop,TS val 27246227 ecr 27246226], length 151
E.....@.@.""V........#...\
...n.............
....................get_range_slices...................foo..................foo...............foo1.......bar3
........&.........foo2.......bar3
........: ....."	CASSANDRA	Resolved	10000	1	5408	supercolumns
12498670	EOFException during name query	"As reported by Jonas Borgstrom on the mailing list:

{quote}
While testing the new 0.7.1 release I got the following exception:

ERROR [ReadStage:11] 2011-02-15 16:39:18,105
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.io.IOError: java.io.EOFException
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:75)
       at
org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
       at
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
       at
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1274)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1166)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
       at org.apache.cassandra.db.Table.getRow(Table.java:384)
       at
org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:60)
       at
org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:473)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
       at java.io.DataInputStream.readInt(DataInputStream.java:392)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:48)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
       at
org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
       ... 12 more

{quote}"	CASSANDRA	Resolved	10002	1	5408	EOF
12525723	Add compaction_thread_priority back	"In CASSANDRA-3104, this was removed with the following reasoning:

bq. compaction_throughput_mb_per_sec is a more effective throttle on compaction.

This turns out to be false in the majority of deployments.  In many (if not most) situations, compaction is actually CPU bound, not IO bound, so multithreaded compaction is generally helpful, but the priority needs to be lowered in order to prevent it from stealing CPU used for reads/writes.

Compaction is always CPU bound on both real hardware (sw raid0 with two SATA disks) and on a rackspace cloud server (though my understanding is they are back by a raid10 array underneath) however I suspect even a single drive is fast enough to handle the ~20MB/s that compaction is currently performing when unthrottled."	CASSANDRA	Resolved	10003	4	5408	compaction
12653784	Change timestamps used in CAS ballot proposals to be more resilient to clock skew	"The current time is used to generate the timeuuid used for CAS ballots proposals with the logic that if a newer proposal exists then the current one needs to complete that and re-propose. The problem is that if a machine has clock skew and drifts into the future it will propose with a large timestamp (which will get accepted) but then subsequent proposals with lower (but correct) timestamps will not be able to proceed. This will prevent CAS write operations and also reads at serializable consistency level. 

The work around is to initially propose with current time (current behavior) but if the proposal fails due to a larger existing one re-propose (after completing the existing if necessary) with the max of (currentTime, mostRecent+1, proposed+1).

Since small drift is normal between different nodes in the same datacenter this can happen even if NTP is working properly and a write hits one node and a subsequent serialized read hits another. In the case of NTP config issues (or OS bugs with time esp around DST) the unavailability window could be much larger.  

"	CASSANDRA	Resolved	10003	4	5408	LWT
12533568	Hints are not replayed unless node was marked down	If B drops a write from A because it is overwhelmed (but not dead), A will hint the write.  But it will never get notified that B is back up (since it was never down), so it will never attempt hint delivery.	CASSANDRA	Resolved	10002	1	5408	hintedhandoff, jmx
12518911	AssertionError on nodetool cleanup	"While doing a cleanup I got the following AssertionError, I have tried a scrub and a major compaction before the cleanup which has not helped.

ST:

 INFO 18:49:58,540 Scrubbing SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db')
 INFO 18:49:58,834 Scrub of SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db') complete: 4 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:58,913 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db')
 INFO 18:49:59,218 Scrub of SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db') complete: 1 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,256 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db')
 INFO 18:49:59,323 Scrub of SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db') complete: 34 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,416 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db')
 INFO 18:50:50,137 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db') complete: 91735 rows in new sstable and 32 empty (tombstoned) rows dropped
 INFO 18:50:50,137 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db')
 INFO 18:50:53,075 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db') complete: 27940 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:50:53,089 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db')

 INFO 18:51:10,302 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db') complete: 70815 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:53:05,420 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5078-Data.db')
 INFO 18:53:13,266 Cleaned up to /vol/cassandra/data/SpiderServices/Content2-tmp-g-5079-Data.db.  198,705,176 to 198,705,176 (~100% of original) bytes for 27,940 keys.  Time: 7,846ms.
 INFO 18:53:13,267 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5077-Data.db')
ERROR 18:53:33,913 Fatal exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:132)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:866)
	at org.apache.cassandra.db.compaction.CompactionManager.access$500(CompactionManager.java:65)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:204)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)"	CASSANDRA	Resolved	10002	1	5408	exception, nodetool
12653785	NPE in net.OutputTcpConnection when tracing is enabled	"I get multiple NullPointerException when trying to trace INSERT statements.

To reproduce:
{code}
$ ccm create -v git:trunk
$ ccm populate -n 3
$ ccm start
$ ccm node1 cqlsh < 5668_npe_ddl.cql
$ ccm node1 cqlsh < 5668_npe_insert.cql
{code}

And see many exceptions like this in the logs of node1:
{code}
ERROR [WRITE-/127.0.0.3] 2013-06-19 14:54:35,885 OutboundTcpConnection.java (line 197) error writing to /127.0.0.3
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:182)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:144)
{code}


This is similar to CASSANDRA-5658 and is the reason that npe_ddl and npe_insert are separate files."	CASSANDRA	Resolved	10002	1	5408	pull-request-available, qa-resolved
12649106	ArrayIndexOutOfBoundsException in LeveledManifest	"The following stack trace was in the system.log:

{quote}
ERROR [CompactionExecutor:2] 2013-05-22 16:19:32,402 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:2,1,main]
 java.lang.ArrayIndexOutOfBoundsException: 5
	at org.apache.cassandra.db.compaction.LeveledManifest.skipLevels(LeveledManifest.java:176)
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:215)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:155)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:410)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:223)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:991)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:230)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
{quote}"	CASSANDRA	Resolved	10003	1	5408	compaction
12540203	snapshot-before-compaction snapshots entire keyspace	Should only snapshot the CF being compacted	CASSANDRA	Resolved	10003	1	5408	compaction
12641685	Include fatal errors in trace events	This would help tracking down which query is causing errors.	CASSANDRA	Resolved	10003	1	5408	qa-resolved
12685087	Endless L0 LCS compactions	"I have first described the problem here: http://stackoverflow.com/questions/20589324/cassandra-2-0-3-endless-compactions-with-no-traffic

I think I have really abused my system with the traffic (mix of reads, heavy updates and some deletes). Now after stopping the traffic I see the compactions that are going on endlessly for over 4 days.

For a specific CF I have about 4700 sstable data files right now.  The compaction estimates are logged as ""[3312, 4, 0, 0, 0, 0, 0, 0, 0]"". sstable_size_in_mb=256.  3214 files are about 256Mb (+/1 few megs), other files are smaller or much smaller than that. No sstables are larger than 256Mb. What I observe is that LCS picks 32 sstables from L0 and compacts them into 32 sstables of approximately the same size. So, what my system is doing for last 4 days (no traffic at all) is compacting groups of 32 sstables into groups of 32 sstables without any changes. Seems like a bug to me regardless of what did I do to get the system into this state...
"	CASSANDRA	Resolved	10002	1	5408	compaction, lcs
12547839	Check SSTable range before running cleanup	"Before running a cleanup compaction on an SSTable we should check the range to see if the SSTable falls into the range we want to remove. If it doesn't we can just mark the SSTable as compacted and be done with it, if it does, we can no-op.

Will not help with STCS, but for LCS, and perhaps some others we may see a benefit here after topology changes."	CASSANDRA	Resolved	10003	4	5408	compaction
12532941	Assertion error during bootstraping cassandra	" I have a 3 node cassandra cluster. I have RF set to 3 and do reads
and writes using QUORUM.

Here is my initial ring configuration

[root@CAP4-CNode1 ~]# /root/cassandra/bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load
Owns    Token

       113427455640312821154458202477256070484
10.19.104.11    datacenter1 rack1       Up     Normal  1.66 GB
33.33%  0
10.19.104.12    datacenter1 rack1       Up     Normal  1.06 GB
33.33%  56713727820156410577229101238628035242
10.19.104.13    datacenter1 rack1       Up     Normal  1.61 GB
33.33%  113427455640312821154458202477256070484

I want to add 10.19.104.14 to the cluster.

I edited the 10.19.104.14 cassandra.yaml file and set the token to
127605887595351923798765477786913079296 and set auto_bootstrap to
true.

When I started cassandra I am getting Assertion Error.  

thanks
Ramesh




[root@CAP4-CNode4 cassandra]#  INFO 10:29:46,093 Logging initialized
 INFO 10:29:46,099 JVM vendor/version: Java HotSpot(TM) 64-Bit Server
VM/1.6.0_25
 INFO 10:29:46,100 Heap size: 8304721920/8304721920
 INFO 10:29:46,100 Classpath:
bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/apache-cassandra-1.0.2.jar:bin/../lib/apache-cassandra-clientutil-1.0.2.jar:bin/../lib/apache-cassandra-thrift-1.0.2.jar:bin/../lib/avro-1.4.0-fixes.jar:bin/../lib/avro-1.4.0-sources-fixes.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:bin/../lib/guava-r08.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/jna.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.6.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/mx4j-examples.jar:bin/../lib/mx4j-impl.jar:bin/../lib/mx4j.jar:bin/../lib/mx4j-jmx.jar:bin/../lib/mx4j-remote.jar:bin/../lib/mx4j-rimpl.jar:bin/../lib/mx4j-rjmx.jar:bin/../lib/mx4j-tools.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.6.1.jar:bin/../lib/slf4j-log4j12-1.6.1.jar:bin/../lib/snakeyaml-1.6.jar:bin/../lib/snappy-java-1.0.4.1.jar:bin/../lib/jamm-0.2.5.jar
 INFO 10:29:48,713 JNA mlockall successful
 INFO 10:29:48,726 Loading settings from
file:/root/apache-cassandra-1.0.2/conf/cassandra.yaml
 INFO 10:29:48,883 DiskAccessMode 'auto' determined to be mmap,
indexAccessMode is mmap
 INFO 10:29:48,898 Global memtable threshold is enabled at 2640MB
 INFO 10:29:49,203 Couldn't detect any schema definitions in local storage.
 INFO 10:29:49,204 Found table data in data directories. Consider
using the CLI to define your schema.
 INFO 10:29:49,220 Creating new commitlog segment
/var/lib/cassandra/commitlog/CommitLog-1321979389220.log
 INFO 10:29:49,227 No commitlog files found; skipping replay
 INFO 10:29:49,230 Cassandra version: 1.0.2
 INFO 10:29:49,230 Thrift API version: 19.18.0
 INFO 10:29:49,230 Loading persisted ring state
 INFO 10:29:49,235 Starting up server gossip
 INFO 10:29:49,259 Enqueuing flush of
Memtable-LocationInfo@122130810(192/240 serialized/live bytes, 4 ops)
 INFO 10:29:49,260 Writing Memtable-LocationInfo@122130810(192/240
serialized/live bytes, 4 ops)
 INFO 10:29:49,317 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db (300 bytes)
 INFO 10:29:49,340 Starting Messaging Service on port 7000
 INFO 10:29:49,349 JOINING: waiting for ring and schema information
 INFO 10:29:50,759 Applying migration
4b0e20f0-1511-11e1-0000-c11bc95834d7 Add keyspace: MSA, rep
strategy:SimpleStrategy{}, durable_writes: true
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Migrations@1507565381(6744/8430 serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Writing Memtable-Migrations@1507565381(6744/8430
serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Schema@1498835564(2889/3611 serialized/live bytes, 3 ops)
 INFO 10:29:50,776 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-1-Data.db (6808 bytes)
 INFO 10:29:50,777 Writing Memtable-Schema@1498835564(2889/3611
serialized/live bytes, 3 ops)
 INFO 10:29:50,797 Completed flushing
/var/lib/cassandra/data/system/Schema-h-1-Data.db (3039 bytes)
 INFO 10:29:50,814 Applying migration
4b6f2cb0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1639d811[cfId=1000,ksName=MSA,cfName=modseq,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2f984f7d,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,815 Enqueuing flush of
Memtable-Migrations@948613108(7482/9352 serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Writing Memtable-Migrations@948613108(7482/9352
serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Enqueuing flush of
Memtable-Schema@421910828(3294/4117 serialized/live bytes, 3 ops)
 INFO 10:29:50,831 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-2-Data.db (7546 bytes)
 INFO 10:29:50,832 Writing Memtable-Schema@421910828(3294/4117
serialized/live bytes, 3 ops)
 INFO 10:29:50,846 Completed flushing
/var/lib/cassandra/data/system/Schema-h-2-Data.db (3444 bytes)
 INFO 10:29:50,854 Applying migration
4b8c9fc0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1bd97d0d[cfId=1001,ksName=MSA,cfName=msgid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@63a0eec3,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,855 Enqueuing flush of
Memtable-Migrations@1520138062(7750/9687 serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Writing Memtable-Migrations@1520138062(7750/9687
serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Enqueuing flush of
Memtable-Schema@347459675(3630/4537 serialized/live bytes, 3 ops)
 INFO 10:29:50,878 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-3-Data.db (7814 bytes)
 INFO 10:29:50,879 Writing Memtable-Schema@347459675(3630/4537
serialized/live bytes, 3 ops)
 INFO 10:29:50,894 Completed flushing
/var/lib/cassandra/data/system/Schema-h-3-Data.db (3780 bytes)
 INFO 10:29:50,900 Applying migration
4ba1ae60-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@6a095b8a[cfId=1002,ksName=MSA,cfName=participants,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@c58f769,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,900 Enqueuing flush of
Memtable-Migrations@618337492(8194/10242 serialized/live bytes, 1 ops)
 INFO 10:29:50,901 Writing Memtable-Migrations@618337492(8194/10242
serialized/live bytes, 1 ops)
 INFO 10:29:50,902 Enqueuing flush of
Memtable-Schema@724860211(4020/5025 serialized/live bytes, 3 ops)
 INFO 10:29:50,917 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-4-Data.db (8258 bytes)
 INFO 10:29:50,918 Writing Memtable-Schema@724860211(4020/5025
serialized/live bytes, 3 ops)
 INFO 10:29:50,925 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-3-Data.db')]
 INFO 10:29:50,934 Completed flushing
/var/lib/cassandra/data/system/Schema-h-4-Data.db (4170 bytes)
 INFO 10:29:50,935 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-3-Data.db')]
 INFO 10:29:50,940 Applying migration
4bb4e840-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@318c69a9[cfId=1003,ksName=MSA,cfName=subinfo,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=5000.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=14400,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@796cefa8,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Migrations@1682081063(8618/10772 serialized/live bytes, 1
ops)
 INFO 10:29:50,941 Writing Memtable-Migrations@1682081063(8618/10772
serialized/live bytes, 1 ops)
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Schema@1083461053(4427/5533 serialized/live bytes, 3 ops)
 INFO 10:29:50,977 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-5-Data.db (8682 bytes)
 INFO 10:29:50,978 Writing Memtable-Schema@1083461053(4427/5533
serialized/live bytes, 3 ops)
 INFO 10:29:50,991 Compacted to
[/var/lib/cassandra/data/system/Schema-h-5-Data.db,].  14,433 to
14,106 (~97% of original) bytes for 5 keys at 0.269051MB/s.  Time:
50ms.
 INFO 10:29:50,995 Completed flushing
/var/lib/cassandra/data/system/Schema-h-7-Data.db (4577 bytes)
 INFO 10:29:51,000 Applying migration
4bc6e9a0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@20b00ec2[cfId=1004,ksName=MSA,cfName=transactions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@698f352,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,001 Enqueuing flush of
Memtable-Migrations@596545504(9027/11283 serialized/live bytes, 1 ops)
 INFO 10:29:51,002 Writing Memtable-Migrations@596545504(9027/11283
serialized/live bytes, 1 ops)
 INFO 10:29:51,003 Enqueuing flush of
Memtable-Schema@1686621532(4835/6043 serialized/live bytes, 3 ops)
 INFO 10:29:51,029 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-7-Data.db (9091 bytes)
 INFO 10:29:51,029 Writing Memtable-Schema@1686621532(4835/6043
serialized/live bytes, 3 ops)
 INFO 10:29:51,031 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-6-Data.db,].  30,426 to
30,234 (~99% of original) bytes for 1 keys at 0.272013MB/s.  Time:
106ms.
 INFO 10:29:51,044 Completed flushing
/var/lib/cassandra/data/system/Schema-h-8-Data.db (4985 bytes)
 INFO 10:29:51,049 Applying migration
4bd76460-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@4ab4faeb[cfId=1005,ksName=MSA,cfName=uid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1500000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2fc5809e,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,050 Enqueuing flush of
Memtable-Migrations@1333730706(9421/11776 serialized/live bytes, 1
ops)
 INFO 10:29:51,050 Writing Memtable-Migrations@1333730706(9421/11776
serialized/live bytes, 1 ops)
 INFO 10:29:51,051 Enqueuing flush of
Memtable-Schema@577668356(5236/6545 serialized/live bytes, 3 ops)
 INFO 10:29:51,065 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-9-Data.db (9485 bytes)
 INFO 10:29:51,066 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-6-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-7-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-5-Data.db')]
 INFO 10:29:51,066 Writing Memtable-Schema@577668356(5236/6545
serialized/live bytes, 3 ops)
 INFO 10:29:51,081 Completed flushing
/var/lib/cassandra/data/system/Schema-h-9-Data.db (5386 bytes)
 INFO 10:29:51,083 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-5-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-8-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-7-Data.db')]
 INFO 10:29:51,114 Compacted to
[/var/lib/cassandra/data/system/Schema-h-10-Data.db,].  29,054 to
28,727 (~98% of original) bytes for 8 keys at 0.913207MB/s.  Time:
30ms.
 INFO 10:29:51,144 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-10-Data.db,].  57,492 to
57,300 (~99% of original) bytes for 1 keys at 0.700584MB/s.  Time:
78ms.
 INFO 10:29:51,410 Node /10.19.104.13 is now part of the cluster
 INFO 10:29:51,412 InetAddress /10.19.104.13 is now UP
 INFO 10:29:51,414 Enqueuing flush of
Memtable-LocationInfo@709342045(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,415 Writing Memtable-LocationInfo@709342045(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,428 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db (89 bytes)
 INFO 10:29:51,439 Node /10.19.104.12 is now part of the cluster
 INFO 10:29:51,439 InetAddress /10.19.104.12 is now UP
 INFO 10:29:51,441 Enqueuing flush of
Memtable-LocationInfo@1292444743(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,441 Writing Memtable-LocationInfo@1292444743(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,455 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db (89 bytes)
 INFO 10:29:51,456 Node /10.19.104.11 is now part of the cluster
 INFO 10:29:51,457 InetAddress /10.19.104.11 is now UP
 INFO 10:29:51,459 Enqueuing flush of
Memtable-LocationInfo@1891328597(20/25 serialized/live bytes, 1 ops)
 INFO 10:29:51,459 Writing Memtable-LocationInfo@1891328597(20/25
serialized/live bytes, 1 ops)
 INFO 10:29:51,471 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db (74 bytes)
 INFO 10:29:51,473 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db')]
 INFO 10:29:51,497 Compacted to
[/var/lib/cassandra/data/system/LocationInfo-h-5-Data.db,].  552 to
444 (~80% of original) bytes for 3 keys at 0.018410MB/s.  Time: 23ms.
 INFO 10:30:19,349 JOINING: getting bootstrap token
 INFO 10:30:19,352 Enqueuing flush of
Memtable-LocationInfo@225265367(36/45 serialized/live bytes, 1 ops)
 INFO 10:30:19,353 Writing Memtable-LocationInfo@225265367(36/45
serialized/live bytes, 1 ops)
 INFO 10:30:19,364 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-7-Data.db (87 bytes)
 INFO 10:30:19,374 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:30:49,375 JOINING: Starting to bootstrap...
ERROR 10:31:13,444 Fatal exception in thread Thread[Thread-49,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
       at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
       at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
       at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:922)
       at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
       at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:102)
       at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
       at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)"	CASSANDRA	Resolved	10002	1	5408	compaction
12529794	local writes timing out cause attempt to hint to self	"As reported by Ramash Natarajan on the mailing list:

{noformat}
We have a 8 node cassandra cluster running 1.0.1. After running a load
test for a day we are seeing this exception in system.log file.

ERROR [EXPIRING-MAP-TIMER-1] 2011-11-01 13:20:45,350
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[EXPIRING-MAP-TIMER-1,5,main]
java.lang.AssertionError: /10.19.102.12
       at org.apache.cassandra.service.StorageProxy.scheduleLocalHint(StorageProxy.java:339)
       at org.apache.cassandra.net.MessagingService.scheduleMutationHint(MessagingService.java:201)
       at org.apache.cassandra.net.MessagingService.access$500(MessagingService.java:64)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:175)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:152)
       at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:89)
       at java.util.TimerThread.mainLoop(Timer.java:512)
       at java.util.TimerThread.run(Timer.java:462)
{noformat}"	CASSANDRA	Resolved	10002	1	5408	hintedhandoff
12527792	NPE in hinted handoff	"I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more


this could possibly be related to #3291
"	CASSANDRA	Resolved	10003	1	5408	hintedhandoff
12526275	dropping index causes some inflight mutations to fail	"dropping index causes some inflight mutations to fail. hector on client side didnt throw any exception

 INFO [MigrationStage:1] 2011-10-07 23:11:53,742 Migration.java (line 119) Applying migration fb1a8540-f128-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@786669[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@8bb33c,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=461 lim=464 cap=466],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=null, index_name='null'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:11:53,805 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:11:53,820 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:53,820 Memtable.java (line 237) Writing Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-14-Data.db (7924 bytes)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 237) Writing Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [CompactionExecutor:4] 2011-10-07 23:11:55,008 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-11-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-12-Data.db')]
 INFO [FlushWriter:3] 2011-10-07 23:11:56,430 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-14-Data.db (3470 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:11:56,446 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-12-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-11-Data.db')]
ERROR [MutationStage:56] 2011-10-07 23:11:56,508 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:56,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:51] 2011-10-07 23:11:56,539 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:51,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:38] 2011-10-07 23:11:56,633 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:38,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:57] 2011-10-07 23:11:56,664 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:57,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
"	CASSANDRA	Resolved	10003	1	5408	indexing
12513477	Allow map/reduce to use server-side query filters	"Currently, when running a MapReduce job against data in a Cassandra data store, it reads through all the data for a particular ColumnFamily.  This could be optimized to only read through those rows that have to do with the query.
"	CASSANDRA	Resolved	10000	2	5408	hadoop
12527269	Allow one leveled compaction task to kick off another	"Leveled compaction wants to prevent multiple tasks from running at once, but this check also defeats the ""kick off another compaction if there is more work to do"" code in CompactionTask.  So currently LCS relies completely on the every-five-minutes compaction check, which is not enough to keep up with heavy insert load."	CASSANDRA	Resolved	10003	4	5408	compaction, lcs
12643007	Promote row-level tombstones to index file	The idea behind promoted indexes (CASSANDRA-2319) was we could skip a seek to the row header by keeping the column index in the index file.  But, we skip writing the row-level tombstone to the index file unless it also has some column data.  So unless we read the tombstone from the data file (where it is guaranteed to exist) we can return incorrect results.	CASSANDRA	Resolved	10002	4	5408	qa-resolved
12544447	Hints Should Be Dropped When Missing CFid Implies Deleted ColumnFamily	If hints have accumulated for a CF that has been deleted, Hinted Handoff repeatedly fails until manual intervention removes those hints. For 1.0.7, UnserializableColumnFamilyException is thrown only when a CFid is unknown on the sending node. As discussed on #cassandra-dev, if the schema is in agreement, the affected hint(s) should be deleted to avoid indefinite repeat failures.	CASSANDRA	Resolved	10002	1	5408	datastax_qa
12597658	Include metadata for system keyspace itself in schema_* tables	"The `system.schema_keyspaces`, `system.schema_columnfamilies`, and `system.schema_columns` virtual tables allow clients to query schema and layout information through CQL. This will be invaluable when users start to make more use of the CQL-only protocol (CASSANDRA-2478), since there will be no other way to determine certain information about available columnfamilies, keyspaces, or show metadata about them.

However, the system keyspace itself, and all the columnfamilies in it, are not represented in the schema_* tables:

{noformat}
cqlsh> select * from system.schema_keyspaces where ""keyspace"" = 'system';
cqlsh> 
cqlsh> select * from system.schema_columnfamilies where ""keyspace"" = 'system';
cqlsh> 
cqlsh> select * from system.schema_columns where ""keyspace"" = 'system';
cqlsh> 
{noformat}

It would be greatly helpful to clients which do more introspection than the minimum (say, for example, cqlsh) to be able to get information on the structure and availability of schema-definition tables."	CASSANDRA	Resolved	10003	4	5408	cql, cql3
12498027	"Add ""reduce memory usage because I tuned things poorly"" feature"	Users frequently create too many columnfamilies, set the memtable thresholds too high (or adjust throughput while ignoring operations), and/or set caching thresholds too high.  Then their server OOMs and they tell their friends Cassandra sucks.	CASSANDRA	Resolved	10003	4	5408	ponies
12667496	Paxos replay of in progress update is incorrect	When we replay {{inProgress}}, we need to refresh it with the newly prepared ballot, or it will be (correctly) rejected.	CASSANDRA	Resolved	10002	1	5408	LWT
12525107	Error during multi-threaded compaction in 0.8	"I'm running 0.8.6 plus the multi-threaded compaction patch in issue 2901.  I'm getting an error compacting last night:


Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:278)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1856)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1447)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getCompactedRow(ParallelCompactionIterable.java:211)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:185)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:146)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:105)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:92)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:573)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:507)
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:320)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
"	CASSANDRA	Resolved	10002	1	5408	compaction
12537700	Unsustainable Thread Accumulation in ParallelCompactionIterable.Reducer ThreadPoolExecutor	"With multithreaded compaction enabled, it looks like Reducer creates a new thread pool for every compaction.  These pools seem to just sit around - i.e. ""executor.shutdown()"" never gets called and the Threads live forever waiting for tasks that will never come.  For instance...


Name: CompactionReducer:1
State: TIMED_WAITING on java.util.concurrent.SynchronousQueue$TransferStack@72938aea
Total blocked: 0  Total waited: 1

Stack trace: 
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1043)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1103)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
java.lang.Thread.run(Thread.java:722)
"	CASSANDRA	Resolved	10003	1	5408	compaction, memory_leak, threading, threads
12523442	LeveledCompactionStrategy is too complacent	"As the title says, it barely does anything.  I inserted 50G worth of data with 1G heap and 99% overwrite ratio, and it only compacted twice:

{noformat}
 INFO [CompactionExecutor:1] 2011-09-16 22:29:54,572 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-1-Data.db')]
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,606 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db,].  12,595,811 to 12,595,811 (~100% of original) bytes for 40,501 keys at 3.058122MBPS.  Time: 3,928ms.
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,607 CompactionTask.java (line 222) CF Total Bytes Compacted: 12,595,811
 INFO [CompactionExecutor:3] 2011-09-16 22:29:58,889 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-3-Data.db')]
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,900 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-7-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-9-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-11-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-12-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-14-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-15-Data.db,].  28,374,396 to 28,374,396 (~100% of original) bytes for 91,236 keys at 3.380379MBPS.  Time: 8,005ms.
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,901 CompactionTask.java (line 222) CF Total Bytes Compacted: 40,970,207
{noformat}

Resulting in the following levels:

{noformat}
L0: 4965
L1: 6
L2: 0
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

This is obviously going to result in extremely poor read performance."	CASSANDRA	Resolved	10002	1	5408	compaction
12536553	Multiple threads can attempt hint handoff to the same target	"HintedHandOffManager attempts to prevent multiple threads sending hints to the same target with the queuedDeliveries set, but the code is buggy.  If two handoffs *do* occur concurrently, the second thread can use an arbitrarily large amount of memory skipping tombstones when it starts paging from the beginning of the hint row, looking for the first live hint.  (This is not a problem with a single thread, since it always pages starting with the last-seen hint column name, effectively skipping the tombstones.  Then it compacts when it's done.)

Technically this bug is present in all older Cassandra releases, but it only causes problems in 1.0.x since the hint rows tend to be much larger (since there is one hint per write containing the entire mutation, instead of just one per row consisting of just the key)."	CASSANDRA	Resolved	10003	1	5408	hintedhandoff
12526261	remove more copies from read/write network path	RowMutation.serializedBuffer and ReadVerbHandler both do an extra copy of the serialized data. We can avoid this be pre-computing the serialized size and allocating an appropriate buffer.	CASSANDRA	Resolved	10003	4	5408	network
12640117	incremental backups race	"incremental backups does not mark things referenced or compacting, so it could get compacted away before createLinks runs.  Occasionally you can see this happen during ColumnFamilyStoreTest.  (Since it runs on the background tasks stage, it does not fail the test.)

{noformat}
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ja-8-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1066)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
{noformat}"	CASSANDRA	Resolved	10003	1	5408	compaction
12646032	ColumnFamilyInputFormat demands OrderPreservingPartitioner when specifying InputRange with tokens	"When ColumnFamilyInputFormat starts getting splits (via getSplits(...) [ColumnFamilyInputFormat.java:101]) it checks to see if a `jobKeyRange` has been set.  If it has been set it attempts to set the `jobRange`.  However the if block (ColumnFamilyInputFormat.java:124) looks to see if the `jobKeyRange` has tokens but asserts that the OrderPreservingPartitioner must be in use.

This if block should be looking for keys (not tokens).  Code further down (ColumnFamilyInputFormat.java:147) already manages the range if tokens are used but can never be reached."	CASSANDRA	Resolved	10002	1	5408	hadoop
12615825	Add blocking force compaction (and anything else) calls to NodeProbe	There are times when I'd like to get feedback about when compactions complete.  For example, if I'm deleting data from cassandra and want to know when it is 100% removed from cassandra (tombstones collected and all).  This is completely trivial to implement based on the existing code (the method called by the non-blocking version returns a future, so you could just wait on that, potentially with a timeout).	CASSANDRA	Resolved	10003	4	5408	lhf
12551356	Generate Digest file for compressed SSTables	We use the generated *Digest.sha1-files to verify backups, would be nice if they were generated for compressed sstables as well.	CASSANDRA	Resolved	10003	4	5408	performance
12504840	CQL: create keyspace does not the replication factor argument and allows invalid sql to pass thru	"I ran the following SQL in cqlsh and immediately received a socket closed error.  After that point, I couldn't run nodetool, and then got an exception starting up the cluster.

Please Note:  The following syntax is valid in 0.74 but invalid in 0.8.
The 0.8 cassandra-cli errors on the following statement, so the resolution of the bug is to have cqlsh block this incorrect syntax.

{code}
create keyspace testcli with replication_factor=1
and placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy';
{code}

{code}
CREATE KEYSPACE testcql with replication_factor = 1 and strategy_class = 'org.apache.cassandra.locator.SimpleStrategy';	
{code}


{code}
ERROR 01:29:26,989 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
{code}"	CASSANDRA	Resolved	10000	1	5408	cql
12497179	Eliminate excess comparator creation	"Despite the singleton status of each AbstractType, we end up creating at least one new comparator per query. By making more of the ""wrapper"" comparators that exist in the codebase members of AbstractType, we could cut down on the ""new Comparator"" spam."	CASSANDRA	Resolved	10003	4	5408	abstract_types, gc
12633090	Hinted Handoff Throttle based on cluster size	"For a 12-node EC2 m1.xlarge cluster, restarting a node causes it to get completely overloaded with the default 2-thread, 1024KB setting in 1.2.x. This seemed to be a smaller problem when it was 6-nodes, but still required us to abort handoffs. The old defaults in 1.1.x were WAY more conservative. I've dropped this way down to 128KB on our production cluster which is really conservative, but appears to have solved it. The default seems way too high on any cluster that is non-trivial in size.

After putting some thought to this, it seems that this should really be based on cluster size, making the throttle a ""target"" for how much write load a single node can swallow. As the cluster grows, the amount of hints that can be delivered by each other node in the cluster goes down, so the throttle should self-adjust to take that into account."	CASSANDRA	Resolved	10003	4	5408	lhf
12596836	cleanup uses global partitioner to estimate ranges in index sstables	"Introduced in CASSANDRA-1404, CleanupTest is showing this on trunk (on stderr, so test doesn't actually fail):

{noformat}
    [junit] java.lang.ClassCastException: org.apache.cassandra.dht.Token$KeyBound cannot be cast to org.apache.cassandra.dht.Token
    [junit]     at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:24)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:386)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:383)
    [junit]     at java.util.Arrays.mergeSort(Arrays.java:1270)
    [junit]     at java.util.Arrays.sort(Arrays.java:1210)
    [junit]     at java.util.Collections.sort(Collections.java:159)
    [junit]     at org.apache.cassandra.dht.Range.normalize(Range.java:382)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:570)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:549)
    [junit]     at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:111)
    [junit]     at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:136)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't happen on the 1.1 branch (less robust test?) but the problem is still there."	CASSANDRA	Resolved	10003	1	5408	compaction
12676860	AE in PrecompactedRow.update(PrecompactedRow.java:171)	"Getting this AE on destination nodes during repair:

ERROR [ValidationExecutor:78] 2013-10-31 04:35:31,243 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:78,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.update(PrecompactedRow.java:171)
        at org.apache.cassandra.repair.Validator.rowHash(Validator.java:198)
        at org.apache.cassandra.repair.Validator.add(Validator.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:799)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
"	CASSANDRA	Resolved	10003	1	5408	repair
12525766	Cancelling index build throws assert error	"Canceling index build throws this, but checking log there was no compaction running in background.

INFO 08:46:41,343 Writing Memtable-IndexInfo@9480253(34/42 serialized/live byte
s, 1 ops)
ERROR 08:46:41,343 Fatal exception in thread Thread[CompactionExecutor:3,5,main]

java.lang.AssertionError
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates
(SecondaryIndexManager.java:397)
        at org.apache.cassandra.db.Table.indexRow(Table.java:534)
        at org.apache.cassandra.db.index.SecondaryIndexBuilder.build(SecondaryIn
dexBuilder.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$7.run(Compaction
Manager.java:856)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 08:46:41,531 Completed flushing \var\lib\cassandra\data\system\IndexInfo-h
-1-Data.db (88 bytes)"	CASSANDRA	Resolved	10003	1	5408	indexing
12610366	Clean out STREAM_STAGE vestiges	Currently it appears as though bulk loading operations don't run in any stage. Seems like they should be running in STREAM_STAGE.	CASSANDRA	Resolved	10003	1	5408	streaming
12560470	Data insertion fails because of commitlog rename failure	"h3. Configuration
Cassandra server configuration:
{noformat}heap size: 4 GB
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: ""xxx.xxx.xxx.10,xxx.xxx.xxx.11""
listen_address: xxx.xxx.xxx.10
rpc_address: 0.0.0.0
rpc_port: 9160
rpc_timeout_in_ms: 20000
endpoint_snitch: PropertyFileSnitch{noformat}

cassandra-topology.properties
{noformat}xxx.xxx.xxx.10=datacenter1:rack1
xxx.xxx.xxx.11=datacenter1:rack1
default=datacenter1:rack1{noformat}

Ring configuration:
{noformat}Address         DC          Rack        Status State   Load            Effective-Ownership Token
                                                                                           85070591730234615865843651857942052864
xxx.xxx.xxx.10  datacenter1 rack1       Up     Normal  23,11 kB        100,00%             0
xxx.xxx.xxx.11  datacenter1 rack1       Up     Normal  23,25 kB        100,00%             85070591730234615865843651857942052864{noformat}

h3.Problem
I have ctreated keyspace and column family using CLI commands:
{noformat}create keyspace testks with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options = {datacenter1:2};
use testks;
create column family testcf;{noformat}

Then I started my Java application, which inserts 50 000 000 rows to created column family using Hector client. Client is connected to node 1.
After about 30 seconds (160 000 rows were inserted) Cassandra server on node 1 throws an exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-13 10:26:38,393 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}
	
Then, few seconds later Cassandra server on node 2 throws the same exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-14 10:26:44,005 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}

After that, my application cannot insert any more data. Hector gets TimedOutException from Thrift:
{noformat}Thread-4 HConnectionManager.java 306 2012-06-14 10:26:56,034 HConnectionManager  operateWithFailover 	 WARN  	 %Could not fullfill request on this host CassandraClient<xxx.xxx.xxx.10:9160-10> 
Thread-4 HConnectionManager.java 307 2012-06-14 10:26:56,034 HConnectionManager operateWithFailover 	 WARN  	 %Exception:  
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:35)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:264)
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecuteOperation(ExecutingKeyspace.java:97)
	at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)
	at patrycjusz.nosqltest.db.cassandra.CassandraHectorDbAdapter.commitTransaction(CassandraDbAdapter.java:63)
	at patrycjusz.nosqltest.DbTest.insertData(DbTest.java:459)
	at patrycjusz.nosqltest.gui.InsertPanel.executeTask(NePanel.java:154)
	at patrycjusz.nosqltest.gui.InsertPanel$1.run(NePanel.java:141)
	at java.lang.Thread.run(Unknown Source)
Caused by: TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:20269)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:922)
	at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:908)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:246)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:243)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:103)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:258)
	... 8 more{noformat}"	CASSANDRA	Resolved	10002	1	5408	commitlog
12607241	Truncate operation doesn't delete rows from HintsColumnFamily.	"Steps to reproduce:
1. Start writing of data to some column family, let name it 'MyCF'
2. Stop 1 node
3. Wait some time (until some data will be collected in HintsColumnFamily)
4. Start node (HintedHandoff will be started automatically for 'MyCF')
5. Run 'truncate' command for 'MyCF' column family from command from cli
6. Wait until truncate will be finished
7. You will see that 'MyCF' is not empty because HintedHandoff is copying data 

So, I suggest to clean HintsColumnFamily (for truncated column family) before we had started to discard sstables. 
I think it should be done in CompactionManager#submitTrucate() method. I can try to create patch but I need to know right way of cleaning HintsColumnFamily. Could you clarify it?
"	CASSANDRA	Resolved	10003	1	5408	hintedhandoff, truncate
12610368	StackOverflowError in CompactionExecutor thread	"Seeing the following error:


Exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.StackOverflowError
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)

"	CASSANDRA	Resolved	10002	1	5408	compaction
12558177	SizeTieredCompactionStrategy.getBuckets is quadradic in the number of sstables	getBuckets first sorts the sstables by size (N log N) then adds each sstable to a bucket (N**2 in the worst case of all sstables the same size, because we use the bucket's contents as a hash key).	CASSANDRA	Resolved	10003	1	5408	compaction
12559479	clean up messagingservice protocol limitations	"Weaknesses of the existing protocol:

- information asymmetry: node A can know what version node B expects, but not vice versa (see CASSANDRA-4101)
- delayed information: node A will often not know what version node B expects, until after first contacting node B -- forcing it to throw that first message away and retry for the next one
- protocol cannot handle both cross-dc forwarding and broadcast_address != socket address (see bottom of CASSANDRA-4099)
- version is partly global, partly per-connection, and partly per-message, resulting in some interesting hacks (CASSANDRA-3166) and difficulty layering more sophisticated OutputStreams on the socket (CASSANDRA-3127, CASSANDRA-4139)"	CASSANDRA	Resolved	10002	1	5408	jmx
12536283	Changing compaction strategy from Leveled to SizeTiered logs millions of messages about nothing to compact	"When column family compaction strategy is changed from Leveled to SizeTiered and there're Leveled compaction tasks pending, Cassandra starting to flood in logs with thousands per sec messages:

Nothing to compact in ColumnFamily1.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)

As a result, log disk is full and system is down."	CASSANDRA	Resolved	10002	1	5408	compaction
12527620	correct dropped messages logging	"CASSANDRA-3004 switched MessagingService back to logging only ""recent"" dropped messages instead of server lifetime totals, but the log message was not updated."	CASSANDRA	Resolved	10003	1	5408	logging
12535015	Hinted Handoff - related OOM	"One of our nodes had collected alot of hints for another node, so when the dead node came back and the row mutations were read back from disk, the node died with an OOM-exception (and kept dying after restart, even with increased heap (from 8G to 12G)). The heap dump contained alot of SuperColumns and our application does not use those (but HH does). 

I'm guessing that each mutation is big so that PAGE_SIZE*<mutation_size> does not fit in memory (will check this tomorrow)

A simple fix (if my assumption above is correct) would be to reduce the PAGE_SIZE in HintedHandOffManager.java to something like 10 (or even 1?) to reduce the memory pressure. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every *mutation* anyway (not every page), thoughts?

If anyone runs in to the same problem, I got the node started again by simply removing the HintsColumnFamily* files."	CASSANDRA	Resolved	10002	1	5408	hintedhandoff
12643123	Backport row-level bloom filter removal to 1.2	"With the possible presence of range tombstones, it is erroneous to skip checking for a given column in SSTableNamesIterator because the bloom filter says it is not there.
"	CASSANDRA	Resolved	10003	4	5408	qa-resolved
12643966	Reduce memory consumption of IndexSummary	"I am evaluating cassandra for a use case with many tiny rows which would result in a node with 1-3TB of storage having billions of rows. Before loading that much data I am hitting GC issues and when looking at the heap dump I noticed that 70+% of the memory was used by IndexSummaries. 

The two major issues seem to be:

1) that the positions are stored as an ArrayList<Long> which results in each position taking 24 bytes (class + flags + 8 byte long). This might make sense when the file is initially written but once it has been serialized it would be a lot more memory efficient to just have an long[] (really a int[] would be fine unless 2GB sstables are allowed).

2) The DecoratedKey for a byte[16] key takes 195 bytes -- this is for the overhead of the ByteBuffer in the key and overhead in the token.

To somewhat ""work around"" the problem I have increased index_sample but will this many rows that didn't really help starts to have diminishing returns. 


NOTE: This heap dump was from linux with a 64bit oracle vm. 
"	CASSANDRA	Resolved	10002	4	5408	qa-resolved
12603683	Mutation response(WriteResponse.java) could be smaller and not contain keyspace and key	"In the mutation response, WriteResponse.java object is send back to the co-ordinator. This object has keyspace and key in it which is not required. It is not being used at the co-ordiantor. 

This wastes IO specially in case of WAN links between DC. Also since response from each node in multi-DC deployments goes back to the co-ordinator in another DC makes it even worse. 

It also becomes worse if the the keyspace and key are of large size and the data is small. In that case, a node which is not the co-ordinator and purely receiving mutations, the outbound n/w bandwidth could be half of incoming bandwidth.  "	CASSANDRA	Resolved	10003	4	5408	network
12533699	CQL CF creation skips most of the validation code	Most validation is done by ThriftValidation.validateCfDef, which we call from QP when creating an index but not on CF creation.	CASSANDRA	Resolved	10003	1	5408	cql
12605495	StackOverflowError in LeveledCompactionStrategy$LeveledScanner.computeNext	"while running nodetool repair, the following was logged in system.log:


ERROR [ValidationExecutor:2] 2012-08-30 10:58:19,490 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.StackOverflowError
        at sun.nio.cs.UTF_8.updatePositions(UTF_8.java:76)
        at sun.nio.cs.UTF_8$Encoder.encodeArrayLoop(UTF_8.java:411)
        at sun.nio.cs.UTF_8$Encoder.encodeLoop(UTF_8.java:466)
        at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:561)
        at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:258)
        at java.lang.StringCoding.encode(StringCoding.java:290)
        at java.lang.String.getBytes(String.java:954)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:64)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:46)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1007)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
        at org.apache.cassandra.io.sstable.SSTableBoundedScanner.<init>(SSTableBoundedScanner.java:41)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:869)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:247)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
.

(about 900 lines deleted)
.


        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:202)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:90)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:47)
        at org.apache.cassandra.db.compaction.CompactionIterable.iterator(CompactionIterable.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:703)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:442)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
"	CASSANDRA	Resolved	10003	1	5408	compaction
12551799	Single-pass compaction for LCR	LazilyCompactedRow reads all data twice to compact a row which is obviously inefficient. The main reason we do that is to compute the row header. However, CASSANDRA-2319 have removed the main part of that row header. What remains is the size in bytes and the number of columns, but it should be relatively simple to remove those, which would then remove the need for the two-phase compaction.	CASSANDRA	Resolved	10002	4	5408	compaction
12539978	remove deprecated KsDef.replication_factor field	KsDef.replication_factor is superceded by KsDef.strategy_options, but we've been keeping special-case code around to populate the old r_f field for SimpleStrategy so that pre-0.8 clients can still create and introspect the schema.  Time to clean that up.	CASSANDRA	Resolved	10003	4	5408	thrift
12504618	"Eagerly re-write data at read time (""superseding / defragmenting"")"	"Oncdsed. This basic approach would improve read performance considerably, but would cause a lot of duplicate data to be written, and would make compaction's work more necessary.

Augmenting the basic idea, if when we superseded data in a file we marked it as superseded somehow, the next compaction that touched that file could remove the data. Since our file format is immutable, the values that a particular sstable superseded could be recorded in a component of that sstable. If we always supersede at the ""block"" level (as defined by CASSANDRA-674 or CASSANDRA-47), then the list of superseded blocks could be represented using a generation number and a bitmap of block numbers. Since 2498 would already allow for sstables to be eliminated due to timestamps, this information would probably only be used at compaction time (by loading all superseding information in the system for the sstables that are being compacted).

Initially described on [1608|https://issues.apache.org/jira/secure/EditComment!default.jspa?id=12477095&commentId=12920353]."	CASSANDRA	Resolved	10002	2	5408	compaction, performance
12524448	inherent deadlock situation in commitLog flush?	"after my system ran for a while, it consitently goes into frozen state where all the mutations stage threads are waiting
on the switchlock,

the reason is that the switchlock is held by commit log, as shown by the following thread dump:



""COMMIT-LOG-WRITER"" prio=10 tid=0x00000000010df000 nid=0x32d3 waiting on condition [0x00007f2d81557000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f3579eec060> (a java.util.concurrent.FutureTask$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:248)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.getContext(CommitLog.java:386)
        at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:650)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.commitlog.CommitLog.createNewSegment(CommitLog.java:573)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:81)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:596)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)


we can clearly see that the COMMIT-LOG-WRITER thread is running the regular appender , but the appender itself calls getContext(), which again submits a new Callable to be executed, and waits on the Callable. but the new Callable is never going to be executed since the executor has only *one* thread.


I believe this is a deterministic bug.



"	CASSANDRA	Resolved	10000	1	5408	commitlog
12523482	cassandra-cli use micro second timestamp, but CQL use milli second	"cassandra-cli set micro second timestamp by FBUtilities.timestampMicros. But CQL insert or update operation set milli second timestamp by AbstractModification.getTimestamp.

If you register data by cassandra-cli, you can't update data by CQL. Because CQL timestamp is judged as past time."	CASSANDRA	Resolved	10002	1	5408	cql
12633306	rename AntiEntropyService	"Maybe ActiveRepairService or ManualRepairService.  ""AntiEntropy"" doesn't help anyone find it who knows they want to find the ""repair"" code."	CASSANDRA	Resolved	10003	3	5408	repair
12526215	Improve CompactionTask extensibility	CompactionTask is still fairly coupled to SizeTieredCompaction, including some ugly casting.	CASSANDRA	Resolved	10003	4	5408	compaction
12519731	Fix count()	count() has been broken since it was introduced in CASSANDRA-1704.	CASSANDRA	Resolved	10002	7	5408	cql
12560663	Small SSTable Segments Can Hurt Leveling Process	"This concerns:

static int MAX_COMPACTING_L0 = 32;

Repair can create very small SSTable segments. We should consider moving to a threshold that takes into account the size of the files brought into compaction rather than the number of files for this and similar situations. Bringing the small files from L0 to L1 magnifies the issue.

If there are too many very small files in L0 perhaps even an intermediate compaction would even reduce the magnifying effect of a L0 to L1 compaction.

"	CASSANDRA	Resolved	10003	4	5408	compaction, lcs
12712423	o.a.c.db.ColumnFamilyTest.testDigest failing in 2.1	bisecting...	CASSANDRA	Resolved	10003	4	5408	qa-resolved
12522759	Compaction fails to occur	Compaction just stops running at some point.  To repro, insert like 20M rows with a 1G heap and you'll get around 1k sstables.  Restarting doesn't help, you have to invoke a major to get anything to happen.	CASSANDRA	Resolved	10002	1	5408	compaction
12520595	Secondary index still does minor compacting after deleting index	We deleted all of our secondary indexes.  A couple of days later I was watching compactionstats on one of the nodes and it was in the process of minor compacting one of the deleted secondary indexes.  I double checked the keyspace definitions on the CLI and there were no secondary indexes defined.	CASSANDRA	Resolved	10003	1	5408	compaction
12623764	Support CAS	"""Strong"" consistency is not enough to prevent race conditions.  The classic example is user account creation: we want to ensure usernames are unique, so we only want to signal account creation success if nobody else has created the account yet.  But naive read-then-write allows clients to race and both think they have a green light to create.
"	CASSANDRA	Resolved	10002	2	5408	LWT
12612999	BulkLoader throws NPE at start up	"BulkLoader in trunk throws below exception at start up and exit abnormally.

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:180)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:148)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:96)
	at java.io.File.list(File.java:1010)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:117)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:63)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.CacheService.initRowCache(CacheService.java:154)
	at org.apache.cassandra.service.CacheService.<init>(CacheService.java:102)
	at org.apache.cassandra.service.CacheService.<clinit>(CacheService.java:83)
	... 8 more
{code}

This comes from CASSANDRA-4732, which moved keyCache in SSTableReader initialization at instance creation. This causes access to CacheService that did not happen for v1.1 and ends up NPE because BulkLoader does not load cassandra.yaml."	CASSANDRA	Resolved	10002	1	5408	bulkloader
12607257	cql version race condition with rpc_server_type: sync	"If clients connect to a cassandra cluster configured with rpc_server_type: sync with heterogeneous cql versions (2 and 3), the cql version used for execution on the server changes seemingly randomly.
It's due to the fact that CustomTThreadPoolServer.java does not set the remoteSocket anytime, or does not clear the cql version in the ThreadLocal clientState object.
When CassandraServer.java calls state() it gets the ThreadLocal object clientState, which has its cqlversion already changed by a previous socket that was using the same thread.


The easiest fix is probably to do a SocketSessionManagementService.instance.set when accepting a new client and SocketSessionManagementService.instance.remove when the client is closed, but if you really want to use the ThreadLocal clientState and not alloc/destroy a ClientState everytime, then you should clear this clientState on accept of a new client.

The problem can be reproduced with cqlsh -3 on one side and a client that does not set the cql version, expecting to get version 2 by default, but actually gettingv v2/v3 depending on which thread it connects to.

The problem does not happen with other rpc_server_types, nor with clients that set their cql version at connection."	CASSANDRA	Resolved	10003	1	5408	features
12557067	kick off background compaction when min/max changed	When the threshold changes, we may be eligible for a compaction immediately (without waiting for a flush to trigger the eligibility check).	CASSANDRA	Resolved	10003	1	5408	compaction
12618265	Add a latency histogram option to stress	Averages just aren't always enough and it should be easy to wire our existing histogram utils into stress.	CASSANDRA	Resolved	10003	4	5408	stress
12678906	Make gossip tolerate slow Gossip tasks	"Currently if a single gossip task bogs down the gossip Stage, Gossip will mark everyone down because it hasn't seen updates from them (since they are all queued behind the slow one).

This means that full GCs can cause gossip ""flapping"" as well as any actually problematic tasks such as recomputing pending ranges."	CASSANDRA	Resolved	10003	1	5408	gossip
12609349	NPE with some load of writes, but possible snitch setting issue for a cluster	"The following errors are showing under height load

ERROR [MutationStage:8294] 2012-09-25 22:01:47,628 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:8294,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.locator.PropertyFileSnitch.getDatacenter(PropertyFileSnitch.java:104)
	at com.datastax.bdp.snitch.DseDelegateSnitch.getDatacenter(DseDelegateSnitch.java:69)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:93)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1984)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1972)
	at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:262)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:248)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:505)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:56)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


ERROR [MutationStage:13164] 2012-09-25 22:19:06,486 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13164,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [Thrift:12] 2012-09-25 22:19:07,433 Cassandra.java (line 3462) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [Thrift:16] 2012-09-25 22:19:07,437 Cassandra.java (line 2999) Internal error processing get


java.lang.NullPointerException
 INFO [GossipStage:280] 2012-09-26 00:15:15,371 Gossiper.java (line 818) InetAddress /172.16.233.208 is now dead.
ERROR [GossipStage:280] 2012-09-26 00:15:15,372 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:280,5,main]
j

ERROR [MutationStage:40529] 2012-09-26 00:15:21,527 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:40529,5,main]
java.lang.NullPointerException
 INFO [GossipStage:281] 2012-09-26 00:15:23,013 Gossiper.java (line 818) InetAddress /172.16.232.159 is now dead.
ERROR [GossipStage:281] 2012-09-26 00:15:23,014 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:281,5,main]
"	CASSANDRA	Resolved	10003	1	5408	snitch
12539274	hadoop word count example is unable to output to cassandra with default settings	"{noformat}
12/01/21 06:03:16 WARN mapred.LocalJobRunner: job_local_0001
java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.newPartitioner(FBUtilities.java:407)
        at org.apache.cassandra.hadoop.ConfigHelper.getOutputPartitioner(ConfigHelper.java:384)
        at org.apache.cassandra.client.RingCache.<init>(RingCache.java:58)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:99)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:93)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:62)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:553)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
{noformat}

(Output to filesystem still works.)"	CASSANDRA	Resolved	10003	1	5408	hadoop
12513118	Starting 0.8.1 after upgrade from 0.7.6-2 fails	"After upgrading the binaries to 0.8.1 I get an exception when starting cassandra:

{noformat}
[root@bserv2 local]#  INFO 12:51:04,512 Logging initialized
 INFO 12:51:04,523 Heap size: 8329887744/8329887744
 INFO 12:51:04,524 JNA not found. Native methods will be disabled.
 INFO 12:51:04,531 Loading settings from file:/usr/local/apache-cassandra-0.8.1/conf/cassandra.yaml
 INFO 12:51:04,621 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 12:51:04,707 Global memtable threshold is enabled at 2648MB
 INFO 12:51:04,708 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,713 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,714 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,716 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,717 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,719 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,770 reading saved cache /vm1/cassandraDB/saved_caches/system-IndexInfo-KeyCache
 INFO 12:51:04,776 Opening /vm1/cassandraDB/data/system/IndexInfo-f-9
 INFO 12:51:04,792 reading saved cache /vm1/cassandraDB/saved_caches/system-Schema-KeyCache
 INFO 12:51:04,794 Opening /vm1/cassandraDB/data/system/Schema-f-194
 INFO 12:51:04,797 Opening /vm1/cassandraDB/data/system/Schema-f-195
 INFO 12:51:04,802 Opening /vm1/cassandraDB/data/system/Schema-f-193
 INFO 12:51:04,811 Opening /vm1/cassandraDB/data/system/Migrations-f-193
 INFO 12:51:04,814 reading saved cache /vm1/cassandraDB/saved_caches/system-LocationInfo-KeyCache
 INFO 12:51:04,815 Opening /vm1/cassandraDB/data/system/LocationInfo-f-292
 INFO 12:51:04,843 Loading schema version 586e70fd-a332-11e0-828e-34b74a661156
ERROR 12:51:04,996 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
{noformat}

It seems this has something to do with indexes, and I do have a CF with an index on it, but it is not used.
I can try and remove the index with 0.7.x binaries, but I will wait a bit to see if anyone needs it to reproduce the bug."	CASSANDRA	Resolved	10002	1	5408	exception, index, starting
12525396	creating column family sets durable_writes to true	"[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *false*
    Options: [datacenter1:1]
  Column Families:
[default@rapidshare] create column family t1;
1ba19300-ebfa-11e0-0000-34912694d0bf
Waiting for schema agreement...
... schemas agree across the cluster
[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *true*
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: t1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.028124999999999997/1440/6 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []"	CASSANDRA	Resolved	10003	1	5408	schema
12530617	Hinted handoff not working after rolling upgrade from 0.8.7 to 1.0.2	"While testing rolling upgrades from 0.8.7 to 1.0.2 on a test cluster I've noticed that hinted hand-off didn't always work properly. Hints generated on an upgraded node does not seem to be delivered to other newly upgraded nodes once they rejoin the ring. They only way I've found to get a node to deliver its hints is to restart it.

Here's some steps to reproduce this issue:

1. Install cassandra 0.8.7 on node1 and node2 using default settings.
2. Create keyspace foo with {replication_factor: 2}. Create column family bar
3. Shutdown node2 
4. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
5. Start node2 and verify that hinted handoff is performed and HintsColumnFamily becomes empty again.

6. Upgrade and restart node1
7. Shutdown node2 
8. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
9. Upgrade and start node2
10. Notice that hinted handoff is *not* performed when ""node2"" comes back. (Only if node1 is restarted)
"	CASSANDRA	Resolved	10002	1	5408	hintedhandoff
12522664	Disabling hinted handoff counterintuitively continues to log handoff messages	"In order to test a theory, we tried to disable hinted handoff on our cluster.  We updated all of the yaml files and then restarted all the nodes in our cluster.  However we continue to get messages such as this in the logs after restarting:
{quote}
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:45,025 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:41:45,026 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.3.4.5
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.3.4.5
{quote}

Also looking at the System.HintsColumnFamily in jmx there is activity there such as pending tasks that come and go."	CASSANDRA	Resolved	10003	1	5408	HintedHandoff
12556268	concurrent modif ex when repair is run on LCS	"came across this, will try to figure a way to systematically reprod this. But the problem is the sstable list in the manifest is changing as the repair is triggered:

{panel}
Exception in thread ""main"" java.util.ConcurrentModificationException 
 at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
 at java.util.AbstractList$Itr.next(Unknown Source)
 at org.apache.cassandra.io.sstable.SSTable.getTotalBytes(SSTable.java:250)
 at org.apache.cassandra.db.compaction.LeveledManifest.getEstimatedTasks(LeveledManifest.java:435)
 at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getEstimatedRemainingTasks(LeveledCompactionStrategy.java:128)
 at org.apache.cassandra.db.compaction.CompactionManager.getPendingTasks(CompactionManager.java:1063)
 at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
 at com.sun.jmx.mbeanserver.PerInterface.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(Unknown Source)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(Unknown Source)
 at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
 at sun.rmi.transport.Transport$1.run(Unknown Source)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
 at java.lang.Thread.run(Unknown Source)
{panel}

maybe we could change the list to a copyOnArrayList? just a suggestion, haven't investigated much yet:

{code:title=LeveledManifest.java}
generations[i] = new ArrayList<SSTableReader>();
{code}"	CASSANDRA	Resolved	10003	1	5408	compaction, lcs
12525770	Fail to delete -Index files if index is currently building	"If there is index building in progress, following errors are thrown if cassandra is trying to delete *-Index.db files. There is no problem with deleting -Data or -Filter.. files. CF is using leveled compaction but it is probably not related.

ERROR [NonPeriodicTasks:1] 2011-10-05 09:13:03,702 AbstractCassandraDaemon.java
(line 133) Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: java.io.IOException: Failed to delete C:\var\lib\cas
sandra\data\test\sipdb-h-772-Index.db
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:3
4)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)"	CASSANDRA	Resolved	10003	1	5408	compaction, indexing
12735390	When compaction is interrupted, it leaves locked, undeletable files	"During tests of new features of 2.1 like: 
- incremental repairs 
- leveled compaction
I interrupted a compaction, which left the following ERROR in the _system.log_
{quote}
org.apache.cassandra.db.compaction.CompactionInterruptedException: Compaction interrupted: Compaction@152e6e70-1975-11e4-ba09-61f0d75c60c6(xx, xxx, 378505918/1993581634)bytes
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:174)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_09]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) ~[na:1.7.0_09]
	at java.util.concurrent.FutureTask.run(FutureTask.java:166) ~[na:1.7.0_09]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) ~[na:1.7.0_09]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) [na:1.7.0_09]
	at java.lang.Thread.run(Thread.java:722) [na:1.7.0_09]
{quote}

Right after that, a cascade of reoccurring errors was being emitted till the restart:
{quote}
ERROR [NonPeriodicTasks:1] 2014-08-19 13:38:41,258 SSTableDeletingTask.java:81 - Unable to delete /grid/data04/cassandra/data/xx/xxx-152e6e70197511e4ba0961f0d75c60c6/xx-xxx-ka-55058-Data.db (it will be removed on server restart; we'll also retry after GC)
{quote}
which made this node blinking (noted from the other nodes gossiper log entries).
After restart, the node is healthy and fully operational."	CASSANDRA	Resolved	10002	1	6295	compaction
12755467	SStablesplit behavior changed	"The dtest sstablesplit_test.py has begun failing due to an incorrect number of sstables being created after running sstablesplit.

http://cassci.datastax.com/job/cassandra-2.1_dtest/559/changes#detail1
is the run where the failure began.

In 2.1.x, the test expects 7 sstables to be created after split, but instead 12 are being created. All of the data is there, and the sstables add up to the expected size, so this simply may be a change in default behavior. The test runs sstablesplit without the --size argument, and the default has not changed, so it is unexpected that the behavior would change in a minor point release."	CASSANDRA	Resolved	10003	1	6295	qa-resolved
13174881	[dtest] test_sstableofflinerelevel - offline_tools_test.TestOfflineTools	"consistently failing dtest on 3.0 (no other branches). Output from pytest:

{noformat}
        output, _, rc = node1.run_sstableofflinerelevel(""keyspace1"", ""standard1"")
>       assert re.search(""L0=1"", output)
E       AssertionError: assert None
E        +  where None = <function search at 0x7f99afffbe18>('L0=1', 'New leveling: \nL0=0\nL1 10\n')
E        +    where <function search at 0x7f99afffbe18> = re.search

offline_tools_test.py:160: AssertionError
{noformat}
"	CASSANDRA	Resolved	10003	1	6295	dtest
13176472	Create fqltool replay command	"Make it possible to replay the full query logs from CASSANDRA-13983 against one or several clusters. The goal is to be able to compare different runs of production traffic against different versions/configurations of Cassandra.

* It should be possible to take logs from several machines and replay them in ""order"" by the timestamps recorded
* Record the results from each run to be able to compare different runs (against different clusters/versions/etc)
* If {{fqltool replay}} is run against 2 or more clusters, the results should be compared as we go"	CASSANDRA	Resolved	10002	2	6295	fqltool
12613911	Move manifest into sstable metadata	Now that we have a metadata component it would be better to keep sstable level there, than in a separate manifest.  With information per-sstable we don't need to do a full re-level if there is corruption.	CASSANDRA	Resolved	10003	4	6295	lcs, qa-resolved
13133040	[DTEST] repair_tests/repair_test.py:TestRepair.simple_sequential_repair_test	Getting all rows from a node times out.	CASSANDRA	Resolved	10002	1	6295	dtest
12951245	Cancelled compaction leading to infinite loop in compaction strategy getNextBackgroundTask	"Our test is basically running *nodetool repair* on specific keyspaces (such as keyspace1) and the test is also triggering *nodetool compact keyspace1 standard1* in the background. 
And so it looks like running major compactions & repairs lead to that issue when using *LCS*.


Below is an excerpt from the *thread dump* (the rest is attached)
{code}
""CompactionExecutor:2"" #33 daemon prio=1 os_prio=4 tid=0x00007f5363e64f10 nid=0x3c4e waiting for monitor entry [0x00007f53340d8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:252)
	- waiting to lock <0x00000006c9362c80> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
	at org.apache.cassandra.db.lifecycle.Tracker.notifySSTableRepairedStatusChanged(Tracker.java:434)
	at org.apache.cassandra.db.compaction.CompactionManager.performAnticompaction(CompactionManager.java:550)
	at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:465)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- <0x00000006c9362ca8> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""CompactionExecutor:1"" #32 daemon prio=1 os_prio=4 tid=0x00007f5363e618b0 nid=0x3c4d runnable [0x00007f5334119000]
   java.lang.Thread.State: RUNNABLE
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:650)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at com.google.common.collect.Iterators.addAll(Iterators.java:361)
	at com.google.common.collect.Iterables.addAll(Iterables.java:354)
	at org.apache.cassandra.db.compaction.LeveledManifest.getCandidatesFor(LeveledManifest.java:589)
	at org.apache.cassandra.db.compaction.LeveledManifest.getCompactionCandidates(LeveledManifest.java:349)
	- locked <0x00000006d0f7a6a8> (a org.apache.cassandra.db.compaction.LeveledManifest)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:98)
	- locked <0x00000006d0f7a568> (a org.apache.cassandra.db.compaction.LeveledCompactionStrategy)
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:95)
	- locked <0x00000006c9362c80> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:257)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

*CPU usage is at 100%*
{code}
top -p 15386
top - 12:12:40 up  1:28,  1 user,  load average: 1.08, 1.11, 1.16
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.3 us,  0.0 sy, 12.4 ni, 87.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  16433792 total,  8947336 used,  7486456 free,    89552 buffers
KiB Swap:        0 total,        0 used,        0 free.  3326796 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
15386 automat+  20   0 7891448 5.004g 290184 S 102.9 31.9  80:07.06 java

{code}

*ttop shows that the compaction thread consumes all the CPU*
{code}
$ java -jar sjk.jar ttop -p 15386
Monitoring threads ...

2016-03-17T12:17:13.514+0000 Process summary
  process cpu=126.34%
  application cpu=102.81% (user=102.46% sys=0.35%)
  other: cpu=23.53%
  heap allocation rate 375mb/s
[000002] user= 0.00% sys= 0.00% alloc=     0b/s - Reference Handler
[000003] user= 0.00% sys= 0.00% alloc=     0b/s - Finalizer
[000005] user= 0.00% sys= 0.00% alloc=     0b/s - Signal Dispatcher
[000012] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Accept-7199
[000013] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Accept-0
[000015] user= 0.00% sys= 0.00% alloc=   476b/s - AsyncAppender-Worker-ASYNCDEBUGLOG
[000016] user= 0.00% sys= 0.05% alloc=  1070b/s - ScheduledTasks:1
[000017] user= 0.00% sys= 0.00% alloc=    33b/s - EXPIRING-MAP-REAPER:1
[000018] user= 0.00% sys= 0.02% alloc=  1932b/s - Background_Reporter:1
[000022] user= 0.00% sys= 0.00% alloc=     0b/s - MemtablePostFlush:1
[000023] user= 0.00% sys= 0.00% alloc=     0b/s - MemtableReclaimMemory:1
[000026] user= 0.00% sys= 0.00% alloc=     0b/s - SlabPoolCleaner
[000027] user= 0.00% sys= 0.00% alloc=     0b/s - PERIODIC-COMMIT-LOG-SYNCER
[000028] user= 0.00% sys= 0.00% alloc=     0b/s - COMMIT-LOG-ALLOCATOR
[000029] user= 0.00% sys= 0.01% alloc=  7086b/s - OptionalTasks:1
[000030] user= 0.00% sys= 0.00% alloc=     0b/s - Reference-Reaper:1
[000031] user= 0.00% sys= 0.00% alloc=     0b/s - Strong-Reference-Leak-Detector:1
[000032] user=99.45% sys= 0.07% alloc=  374mb/s - CompactionExecutor:1
[000033] user= 0.00% sys= 0.00% alloc=     0b/s - CompactionExecutor:2
[000036] user= 0.00% sys= 0.00% alloc=     0b/s - NonPeriodicTasks:1
[000037] user= 0.00% sys= 0.00% alloc=     0b/s - LocalPool-Cleaner:1
[000041] user= 0.00% sys= 0.00% alloc=     0b/s - IndexSummaryManager:1
[000043] user= 0.00% sys= 0.01% alloc=  2705b/s - GossipTasks:1
[000044] user= 0.00% sys= 0.00% alloc=     0b/s - ACCEPT-/10.200.182.146
[000045] user= 0.00% sys= 0.01% alloc=  2283b/s - BatchlogTasks:1
[000055] user= 0.00% sys= 0.02% alloc=  9494b/s - GossipStage:1
[000056] user= 0.00% sys= 0.00% alloc=     0b/s - AntiEntropyStage:1
[000057] user= 0.00% sys= 0.00% alloc=     0b/s - MigrationStage:1
[000058] user= 0.00% sys= 0.00% alloc=     0b/s - MiscStage:1
[000067] user= 0.00% sys= 0.02% alloc=  2445b/s - MessagingService-Incoming-/10.200.182.144
[000068] user= 0.00% sys= 0.01% alloc=   968b/s - MessagingService-Outgoing-/10.200.182.144
[000069] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.144
[000070] user= 0.00% sys= 0.02% alloc=   512b/s - MessagingService-Outgoing-/10.200.182.144
[000072] user= 0.00% sys= 0.00% alloc=     0b/s - NanoTimeToCurrentTimeMillis updater
[000073] user= 0.00% sys= 0.02% alloc=  3113b/s - MessagingService-Incoming-/10.200.182.144
[000074] user= 0.00% sys= 0.00% alloc=     0b/s - PendingRangeCalculator:1
[000075] user= 0.00% sys= 0.41% alloc=   66kb/s - SharedPool-Worker-1
[000076] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-2
[000077] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-3
[000078] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-4
[000079] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-5
[000080] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-6
[000081] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-7
[000082] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-8
[000084] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-2
[000085] user= 0.00% sys= 0.00% alloc=   181b/s - HintsWriteExecutor:1
[000091] user= 0.00% sys= 0.00% alloc=     0b/s - PO-thread-0
[000092] user= 0.00% sys= 0.00% alloc=     0b/s - NodeHealthPlugin-Scheduler-thread-0
[000093] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-1
[000094] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-2
[000097] user= 0.00% sys= 0.00% alloc=     0b/s - Lease RemoteMessageServer acceptor-2-1
[000104] user= 0.00% sys= 0.00% alloc=     0b/s - RemoteMessageClient worker-4-1
[000120] user= 0.00% sys= 0.00% alloc=     0b/s - RemoteMessageClient connection limiter - 0
[000121] user= 0.00% sys= 0.00% alloc=     0b/s - threadDeathWatcher-5-1
[000122] user= 0.00% sys= 0.00% alloc=     0b/s - PO-thread scheduler
[000123] user= 0.00% sys= 0.00% alloc=     0b/s - JOB-TRACKER
[000124] user= 0.00% sys= 0.01% alloc=  1276b/s - TASK-TRACKER
[000127] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-1
[000128] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-2
[000129] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-3
[000130] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-4
[000131] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-5
[000132] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-6
[000133] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-7
[000134] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-8
[000135] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-9
[000136] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-10
[000137] user= 0.19% sys=-0.18% alloc=     0b/s - epollEventLoopGroup-6-11
[000138] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-12
[000139] user= 0.19% sys=-0.19% alloc=     0b/s - epollEventLoopGroup-6-13
[000140] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-14
[000141] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-15
[000142] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-16
[000143] user= 0.19% sys=-0.04% alloc=   13kb/s - Thread-7
[000144] user= 0.00% sys= 0.00% alloc=     0b/s - taskCleanup
[000145] user= 0.00% sys= 0.00% alloc=     0b/s - DseGossipStateUpdater
[000146] user= 0.00% sys= 0.00% alloc=     0b/s - DestroyJavaVM
[000149] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-10
[000150] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-11
[000151] user= 0.00% sys= 0.00% alloc=     0b/s - Directory/File cleanup thread
[000153] user= 0.00% sys= 0.00% alloc=     0b/s - pool-15-thread-1
[000190] user= 0.00% sys= 0.00% alloc=     0b/s - pool-18-thread-1
[000215] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-3
[000217] user= 0.00% sys= 0.00% alloc=     0b/s - RMI Scheduler(0)
[000220] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Connection(335)-10.200.182.146
[000222] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-4
[000223] user= 0.00% sys= 0.00% alloc=     0b/s - taskCleanup
[000224] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-69
[000225] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-70
[000227] user= 0.00% sys= 0.00% alloc=     0b/s - pool-19-thread-1
[000254] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-9
[000255] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-11
[000256] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-10
[000269] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-13
[000270] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-12
[000272] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-14
[000273] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-15
[000274] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-18
[000275] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-19
[000276] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-20
[000277] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-17
[000278] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-16
[000279] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-22
[000280] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-21
[000281] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-23
[000282] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-24
[000283] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-25
[000284] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-26
[000285] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-27
[000286] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-28
[000287] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-29
[000288] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-30
[000289] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-31
[000290] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-32
[000296] user= 0.00% sys= 0.00% alloc=  1970b/s - pool-2-thread-1
[000297] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-33
[000298] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-34
[000302] user= 0.00% sys= 0.01% alloc=  1576b/s - MessagingService-Incoming-/10.200.182.145
[000303] user= 0.00% sys= 0.00% alloc=   451b/s - MessagingService-Outgoing-/10.200.182.145
[000304] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.145
[000305] user= 0.00% sys= 0.01% alloc=   206b/s - MessagingService-Outgoing-/10.200.182.145
[000308] user= 0.00% sys= 0.00% alloc=   424b/s - MessagingService-Incoming-/10.200.182.145
[000314] user= 0.00% sys= 0.00% alloc=     0b/s - StreamingTransferTaskTimeouts:1
[000324] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000325] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000326] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000328] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.146
[000329] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-35
[000330] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-37
[000331] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-36
[000332] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-39
[000333] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-38
[000334] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-42
[000335] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-41
[000336] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-40
[000337] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-46
[000338] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-44
[000339] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-45
[000340] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-43
[000341] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-47
[000342] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-48
[000343] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-50
[000344] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-49
[000375] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:1
[000376] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:2
[000406] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.145
[000408] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.146
[000409] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.144
[000415] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:3
[000418] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:4
[000435] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:5
[000438] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:6
[000439] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:7
[000444] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:8
[000687] user= 0.00% sys= 0.00% alloc=     0b/s - JMX server connection timeout 687
[000688] user= 2.44% sys= 0.16% alloc= 1380kb/s - RMI TCP Connection(401)-10.200.182.146
[000694] user= 0.00% sys= 0.00% alloc=     0b/s - Attach Listener
[000726] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Connection(400)-10.200.182.146
[000743] user=-0.00% sys=-0.16% alloc=-109800b/s - MemtableFlushWriter:112
[000745] user= 0.00% sys= 0.00% alloc=     0b/s - MemtableFlushWriter:113
[000746] user= 0.00% sys= 0.03% alloc=  4295b/s - JMX server connection timeout 746
{code}

"	CASSANDRA	Resolved	10002	1	6295	lcs
12744024	Run LCS for both repaired and unrepaired data	"If a user has leveled compaction configured, we should run that for both the unrepaired and the repaired data. I think this would make things a lot easier for end users

It would simplify migration to incremental repairs as well, if a user runs incremental repair on its nice leveled unrepaired data, we wont need to drop it all to L0, instead we can just start moving sstables from the unrepaired leveling straight into the repaired leveling

Idea could be to have two instances of LeveledCompactionStrategy and move sstables between the instances after an incremental repair run (and let LCS be totally oblivious to whether it handles repaired or unrepaired data). Same should probably apply to any compaction strategy, run two instances and remove all repaired/unrepaired logic from the strategy itself."	CASSANDRA	Resolved	10002	1	6295	compaction, lcs
12954704	dtest failure in compaction_test.TestCompaction_with_LeveledCompactionStrategy.bloomfilter_size_test	"The final assertion {{self.assertGreaterEqual(bfSize, min_bf_size)}} is failing with {{44728 not greater than or equal to 50000}} on 2.1, pretty consistently.

example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/439/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/bloomfilter_size_test

Failed on CassCI build cassandra-2.1_dtest #439"	CASSANDRA	Resolved	10002	4	6295	dtest
12928082	disk_balance_decommission_test is failing on trunk	"http://cassci.datastax.com/job/trunk_dtest/891/testReport/junit/disk_balance_test/TestDiskBalance/disk_balance_decommission_test/

{code}
======================================================================
FAIL: disk_balance_decommission_test (disk_balance_test.TestDiskBalance)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 74, in disk_balance_decommission_test
    self.assert_balanced(node)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 127, in assert_balanced
    assert_almost_equal(*sums, error=0.2, error_message=node.name)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/assertions.py"", line 65, in assert_almost_equal
    assert vmin > vmax * (1.0 - error) or vmin == vmax, ""values not within %.2f%% of the max: %s (%s)"" % (error * 100, args, kwargs['error_message'])
AssertionError: values not within 20.00% of the max: (482095, 477840, 612940) (node2)
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-SLbi3e
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 1 test in 121.295s

FAILED (failures=1)
{code}"	CASSANDRA	Resolved	10002	1	6295	dtest
12759729	Standalone Scrubber broken for LCS	After CASSANDRA-8004, the compaction strategy for a column family will not be instanceof LeveledCompactionStrategy (StandaloneScrubber.java:100), so we don't check the manifest.	CASSANDRA	Resolved	10003	1	6295	lcs
12920407	Fix the way we replace sstables after anticompaction	"We have a bug when we replace sstables after anticompaction, we keep adding duplicates which causes leveled compaction to fail after. Reason being that LCS does not keep its sstables in a {{Set}}, so after first compaction, we will keep around removed sstables in the leveled manifest and that will put LCS in an infinite loop as it tries to mark non-existing sstables as compacting
"	CASSANDRA	Resolved	10002	1	6295	lcs
12637053	Add binary protocol support for bind variables to non-prepared statements	"Currently, the binary protocol allows requests as ""string"" or ""[prepared statement] id + bind vars"".  Allowing ""string + bind vars"" as well would simplify life for users with one-off statements and not have to choose between adding boilerplate for PS, and having to manually escape parameters, which is particularly painful for binary data."	CASSANDRA	Resolved	10003	3	6295	cql, protocol
12932643	Make it clear what timestamp_resolution is used for with DTCS	"We have had a few cases lately where users misunderstand what timestamp_resolution does, we should;

* make the option not autocomplete in cqlsh
* update documentation
* log a warning"	CASSANDRA	Resolved	10002	4	6295	dtcs
12786278	Tombstoned SSTables are not removed past max_sstable_age_days when using DTCS	When using DTCS, tombstoned sstables past max_sstable_age_days are not removed by minor compactions. I was able to reproduce this manually and also wrote a dtest (currently failing) which reproduces this issue: [dtcs_deletion_test|https://github.com/riptano/cassandra-dtest/blob/master/compaction_test.py#L115] in compaction_test.py. I tried applying the patch in CASSANDRA-8359 but found that the test still fails with the same issue.	CASSANDRA	Resolved	10002	1	6295	compaction, dtcs, test
13175351	Flaky dtest: nodetool_test.TestNodetool.test_describecluster_more_information_three_datacenters	"@jay zhuang observed nodetool_test.TestNodetool.test_describecluster_more_information_three_datacenters being flaky in Apache Jenkins. I ran locally and got a different flaky behavior:

{noformat}
        out_node1_dc3, err, _ = node1_dc3.nodetool('describecluster')
        assert 0 == len(err), err
>       assert out_node1_dc1 == out_node1_dc3
E       AssertionError: assert 'Cluster Info...1=3, dc3=1}\n' == 'Cluster Infor...1=3, dc3=1}\n'
E           Cluster Information:
E           	Name: test
E           	Snitch: org.apache.cassandra.locator.PropertyFileSnitch
E           	DynamicEndPointSnitch: enabled
E           	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
E           	Schema versions:
E           		fc9ec7cd-80ba-3f27-87af-fc0bafcf7a03: [127.0.0.6, 127.0.0.5, 127.0.0.4, 127.0.0.3, 127.0.0.2, 127.0.0.1]...
E         
E         ...Full output truncated (26 lines hidden), use '-vv' to show


09:58:14,357 ccm DEBUG Log-watching thread exiting.
===Flaky Test Report===

test_describecluster_more_information_three_datacenters failed and was not selected for rerun.
	<class 'AssertionError'>
	assert 'Cluster Info...1=3, dc3=1}\n' == 'Cluster Infor...1=3, dc3=1}\n'
    Cluster Information:
    	Name: test
    	Snitch: org.apache.cassandra.locator.PropertyFileSnitch
    	DynamicEndPointSnitch: enabled
    	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
    	Schema versions:
    		fc9ec7cd-80ba-3f27-87af-fc0bafcf7a03: [127.0.0.6, 127.0.0.5, 127.0.0.4, 127.0.0.3, 127.0.0.2, 127.0.0.1]...
  
  ...Full output truncated (26 lines hidden), use '-vv' to show
	[<TracebackEntry /opt/orig/1/opt/dev/cassandra-dtest/nodetool_test.py:373>]

===End Flaky Test Report===
{noformat}

As this test is for a patch that was introduced for 4.0, this dtest (should) only be failing on trunk."	CASSANDRA	Resolved	10003	3	6295	dtest
12722195	After cleanup we can end up with non-compacting high level sstables	"If we run cleanup (or increase sstable size) on a node with LCS, we could end up with a bunch of sstables in higher levels that are ""never"" compacted."	CASSANDRA	Resolved	10003	1	6295	lcs
12837353	Make sstableofflinerelevel print stats before relevel	"The current version of sstableofflinerelevel prints the new level hierarchy. While ""nodetool cfstats ..."" will tell the current hierarchy it would be nice to have ""sstableofflinerelevel"" output the current level histograms for easy comparison of what changes will be made. Especially since sstableofflinerelevel needs to run when node isn't running and ""nodetool cfstats ..."" doesn't work because of that."	CASSANDRA	Resolved	10003	4	6295	lcs, lhf
12905450	RangeAwareCompaction	"Broken out from CASSANDRA-6696, we should split sstables based on ranges during compaction.

Requirements;
* dont create tiny sstables - keep them bunched together until a single vnode is big enough (configurable how big that is)
* make it possible to run existing compaction strategies on the per-range sstables

We should probably add a global compaction strategy parameter that states whether this should be enabled or not."	CASSANDRA	Open	10002	2	6295	compaction, lcs, vnodes
12664817	Add existing sstables to leveled manifest on startup	we need to add all sstables to the leveled manifest on startup, looks like this was introduced in 6968f68cd7c	CASSANDRA	Resolved	10002	1	6295	lcs
12949031	Fix bloom filter sizing with LCS	Since CASSANDRA-7272 we most often over allocate the bloom filter size with LCS	CASSANDRA	Resolved	10002	1	6295	lcs
13326258	Don't adjust nodeCount when setting node id topology in in-jvm dtests	We update the node count when setting the node id topology in in-jvm dtests, this should only happen if node count is smaller than the node id topology, otherwise bootstrap tests error out.	CASSANDRA	Resolved	10003	4	6295	pull-request-available
12772443	Don't check for overlap with sstables that have had their start positions moved in LCS	"When picking compaction candidates in LCS, we check that we won't cause any overlap in the higher level. Problem is that we compare the files that have had their start positions moved meaning we can cause overlap. We need to also include the tmplink files when checking this.

Note that in 2.1 overlap is not as big problem as earlier, if adding an sstable would cause overlap, we send it back to L0 instead, meaning we do a bit more compaction but we never actually have overlap."	CASSANDRA	Resolved	10002	1	6295	lcs
12862172	Do STCS in DTCS-windows	"To avoid constant recompaction of files in big ( > max threshold) DTCS windows, we should do STCS of those files.

Patch here: https://github.com/krummas/cassandra/commits/marcuse/dtcs_stcs"	CASSANDRA	Resolved	10002	7	6295	dtcs
12836815	DateTieredCompactionStrategy fails to combine SSTables correctly when TTL is used.	"DateTieredCompaction works correctly when data is dumped for a certain time period in short SSTables in time manner and then compacted together. However, if TTL is applied to the data columns the DTCS fails to compact files correctly in timely manner. In our opinion the problem is caused by two issues:

A) During the DateTieredCompaction process the getFullyExpiredSStables is called twice. First from the DateTieredCompactionStrategy class and second time from the CompactionTask class. On the first time the target is to find out fully expired SStables that are not overlapping with any non-fully expired SSTables. That works correctly. When the getFullyExpiredSSTables is called second time from CompactionTask class the selection of fully expired SSTables is modified compared to the first selection.

B) The minimum timestamp of the new SSTables created by combining together fully expired SSTable and files from the most interesting bucket is not correct.

These two issues together cause problems for the DTCS process when it combines together SSTables having overlap in time and TTL for the column. This is demonstrated by generating test data first without compactions and showing the timely distribution of files. When the compaction is enabled the DCTS combines files together, but the end result is not something to be expected. This is demonstrated in the file motivation_jira.txt

Attachments contain following material:

- Motivation_jira.txt: Practical examples how the DTCS behaves with TTL
- Explanation_jira.txt: gives more details, explains test cases and demonstrates the problems in the compaction process
- Logfile file for the compactions in the first test case (compaction_stage_test01_jira.log)
- Logfile file for the compactions in the seconnd test case (compaction_stage_test02_jira.log)
- source code zip file for version 2.1.5 with additional comment statements (src_2.1.5_with_debug.zip)
- Python script to generate test data (datagen.py)
- Python script to read metadata from SStables (cassandra_sstable_metadata_reader.py)
- Python script to generate timeline representation of SSTables (cassandra_sstable_timespan_graph.py)
"	CASSANDRA	Resolved	10002	1	6295	dtcs
12996296	dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.bloomfilter_size_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/669/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/bloomfilter_size_test

Failed on CassCI build cassandra-2.2_dtest build #669
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 147, in bloomfilter_size_test
    self.assertLessEqual(bfSize, size_factor * max_bf_size)
  File ""/usr/lib/python2.7/unittest/case.py"", line 936, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""125456 not less than or equal to 0\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-XJ96MC\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: bloom filter size is: 125456\ndtest: DEBUG: size factor = 0\n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	4	6295	dtest
12761143	Constant compaction under LCS	"It appears that tables configured with LCS will completely re-compact themselves over some period of time after upgrading from 2.0 to 2.1 (2.0.11 -> 2.1.2, specifically). It starts out with <10 pending tasks for an hour or so, then starts building up, now with 50-100 tasks pending across the cluster after 12 hours. These nodes are under heavy write load, but were easily able to keep up in 2.0 (they rarely had >5 pending compaction tasks), so I don't think it's LCS in 2.1 actually being worse, just perhaps some different LCS behavior that causes the layout of tables from 2.0 to prompt the compactor to reorganize them?

The nodes flushed ~11MB SSTables under 2.0. They're currently flushing ~36MB SSTables due to the improved memtable setup in 2.1. Before I upgraded the entire cluster to 2.1, I noticed the problem and tried several variations on the flush size, thinking perhaps the larger tables in L0 were causing some kind of cascading compactions. Even if they're sized roughly like the 2.0 flushes were, same behavior occurs. I also tried both enabling & disabling STCS in L0 with no real change other than L0 began to back up faster, so I left the STCS in L0 enabled.

Tables are configured with 32MB sstable_size_in_mb, which was found to be an improvement on the 160MB table size for compaction performance. Maybe this is wrong now? Otherwise, the tables are configured with defaults. Compaction has been unthrottled to help them catch-up. The compaction threads stay very busy, with the cluster-wide CPU at 45% ""nice"" time. No nodes have completely caught up yet. I'll update JIRA with status about their progress if anything interesting happens.

From a node around 12 hours ago, around an hour after the upgrade, with 19 pending compaction tasks:
SSTables in each level: [6/4, 10, 105/100, 268, 0, 0, 0, 0, 0]
SSTables in each level: [6/4, 10, 106/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [1, 16/10, 105/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [5/4, 10, 103/100, 272, 0, 0, 0, 0, 0]
SSTables in each level: [4, 11/10, 105/100, 270, 0, 0, 0, 0, 0]
SSTables in each level: [1, 12/10, 105/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [1, 14/10, 104/100, 267, 0, 0, 0, 0, 0]
SSTables in each level: [9/4, 10, 103/100, 265, 0, 0, 0, 0, 0]

Recently, with 41 pending compaction tasks:
SSTables in each level: [4, 13/10, 106/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [4, 12/10, 106/100, 273, 0, 0, 0, 0, 0]
SSTables in each level: [5/4, 11/10, 106/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [4, 12/10, 103/100, 275, 0, 0, 0, 0, 0]
SSTables in each level: [2, 13/10, 106/100, 273, 0, 0, 0, 0, 0]
SSTables in each level: [3, 10, 104/100, 275, 0, 0, 0, 0, 0]
SSTables in each level: [6/4, 11/10, 103/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [4, 16/10, 105/100, 264, 0, 0, 0, 0, 0]

More information about the use case: writes are roughly uniform across these tables. The data is ""sharded"" across these 8 tables by key to improve compaction parallelism. Each node receives up to 75,000 writes/sec sustained at peak, and a small number of reads. This is a pre-production cluster that's being warmed up with new data, so the low volume of reads (~100/sec per node) is just from automatic sampled data checks, otherwise we'd just use STCS :)"	CASSANDRA	Resolved	10002	1	6295	lcs
12953601	dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_repairedset_test	"sstable_repairedset_test is failing on 2.1 and 2.2, but not on trunk.

In the final assertion, after running sstablemetadata on both nodes, we see unrepaired sstables, when we expect all sstables to be repaired.

example failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/220/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test

Failed on CassCI build cassandra-2.1_novnode_dtest #220"	CASSANDRA	Resolved	10002	4	6295	dtest
13129753	"[DTEST] [TRUNK] TestTopology.movement_test is flaky; fails assert ""values not within 16.00% of the max: (851.41, 713.26)"""	"DTest* TestTopology.test_movement* is flaky. All of the testing so far (and thus all of the current known observed failures) have been when running against trunk. When the test fails, it always due to the assert_almost_equal assert.

{code}
AssertionError: values not within 16.00% of the max: (851.41, 713.26) ()
{code}

The following CircleCI runs are 2 examples with dtests runs that failed due to this test failing it's assert:
[https://circleci.com/gh/mkjellman/cassandra/487]
[https://circleci.com/gh/mkjellman/cassandra/526]

*p.s.* assert_almost_equal has a comment ""@params error Optional margin of error. Default 0.16"". I don't see any obvious notes for why the default is this magical 16% number. It looks like it was committed as part of a big bulk commit by Sean McCarthy (who I can't find on JIRA). If anyone has any history on the magic 16% allowed delta please share!
"	CASSANDRA	Resolved	10002	1	6295	dtest
12972142	dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x.select_key_in_test	"example failure:

http://cassci.datastax.com/job/upgrade_tests-all/47/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_3_x_To_indev_3_x/select_key_in_test

Failed on CassCI build upgrade_tests-all #47

Attached logs for test failure.

{code}
ERROR [CompactionExecutor:2] 2016-05-21 23:10:35,678 CassandraDaemon.java:195 - Exception in thread Thread[CompactionExecutor:2,1,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[apache-cassandra-3.5.jar:3.5]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1364) ~[na:1.8.0_51]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[apache-cassandra-3.5.jar:3.5]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_51]
	at org.apache.cassandra.db.compaction.CompactionManager.submitBackground(CompactionManager.java:184) ~[apache-cassandra-3.5.jar:3.5]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:270) ~[apache-cassandra-3.5.jar:3.5]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
{code}"	CASSANDRA	Resolved	10002	1	6295	dtest
12932522	consistent_reads_after_move_test is failing on trunk	"The novnode dtest {{consistent_bootstrap_test.TestBootstrapConsistency.consistent_reads_after_move_test}} is failing on trunk. See an example failure [here|http://cassci.datastax.com/job/trunk_novnode_dtest/274/testReport/consistent_bootstrap_test/TestBootstrapConsistency/consistent_reads_after_move_test/].

On trunk I am getting an OOM of one of my C* nodes [node3], which is what causes the nodetool move to fail. Logs are attached."	CASSANDRA	Resolved	10000	1	6295	dtest
12693358	Droppable tombstones are not being removed from LCS table despite being above 20%	"JMX is showing that one of our CQL3 LCS tables has a droppable tombstone ratio above 20% and increasing (currently at 28%).  Compactions are not falling behind and we are using the OOTB setting for this feature so I would expect not to go above 20% (will attach screen shot from JMX).   Table description:

CREATE TABLE global_user (
  user_id timeuuid,
  app_id int,
  type text,
  name text,
  extra_param map<text, text>,
  last timestamp,
  paid boolean,
  sku_time map<text, timestamp>,
  values map<timestamp, float>,
  PRIMARY KEY (user_id, app_id, type, name)
) WITH
  bloom_filter_fp_chance=0.100000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=86400 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'sstable_size_in_mb': '160', 'class': 'LeveledCompactionStrategy'} AND
  compression={'chunk_length_kb': '8', 'crc_check_chance': '0.1', 'sstable_compression': 'LZ4Compressor'}; "	CASSANDRA	Resolved	10002	1	6295	qa-resolved
13309380	Add bytebuddy support for in-jvm dtests	Old python dtests support byteman, but that is quite horrible to work with, [bytebuddy|https://bytebuddy.net/#/] is much better, so we should add support for that in the in-jvm dtests.	CASSANDRA	Resolved	10002	4	6295	pull-request-available
12963227	(2.1) dtest failure in bootstrap_test.TestBootstrap.test_cleanup	"This test was originally waiting on CASSANDRA-11179, which I recently removed the 'require' annotation from (since 11179 is committed). Not sure why failing on 2.1 now, perhaps didn't get committed.

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/339/testReport/bootstrap_test/TestBootstrap/test_cleanup

Failed on CassCI build cassandra-2.1_offheap_dtest #339"	CASSANDRA	Resolved	10002	4	6295	dtest
13019612	dtest failure in disk_balance_test.TestDiskBalance.disk_balance_stress_test	"example failure:

http://cassci.datastax.com/job/trunk_dtest/1418/testReport/disk_balance_test/TestDiskBalance/disk_balance_stress_test

{noformat}
Error Message

'float' object has no attribute '2f'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-lxr8Vr
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/disk_balance_test.py"", line 31, in disk_balance_stress_test
    self.assert_balanced(node)
  File ""/home/automaton/cassandra-dtest/disk_balance_test.py"", line 120, in assert_balanced
    assert_almost_equal(*sums, error=0.1, error_message=node.name)
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 187, in assert_almost_equal
    assert vmin > vmax * (1.0 - error) or vmin == vmax, ""values not within {.2f}% of the max: {} ({})"".format(error * 100, args, error_message)
""'float' object has no attribute '2f'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-lxr8Vr\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
{noformat}"	CASSANDRA	Resolved	10002	1	6295	dtest, test-failure
13265329	Use multiple data directories in the in-jvm dtests	We should default to using 3 data directories when running the in-jvm dtests.	CASSANDRA	Resolved	10002	4	6295	pull-request-available
12765749	AssertionErrors after activating unchecked_tombstone_compaction with leveled compaction	"During our upgrade of Cassandra from version 2.0.7 to 2.1.2 we experienced a serious problem regarding the setting unchecked_tombstone_compaction in combination with leveled compaction strategy.

In order to prevent tombstone-threshold-warnings we activated the setting for a specific table after the upgrade. Some time after that we observed new errors in our log files:
{code}
INFO  [CompactionExecutor:184] 2014-12-11 12:36:06,597 CompactionTask.java:136 - Compacting [SSTableReader(path='/data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1848-Data.db'), SSTableReader(path='/
data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1847-Data.db'), SSTableReader(path='/data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1845-Data.db'), SSTableReader
(path='/data/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ka-1846-Data.db')]
ERROR [CompactionExecutor:183] 2014-12-11 12:36:06,613 CassandraDaemon.java:153 - Exception in thread Thread[CompactionExecutor:183,1,main]
java.lang.AssertionError: /data/cassandra/data/metrigo_prod/new_user_data/metrigo_prod-new_user_data-tmplink-ka-705732-Data.db
        at org.apache.cassandra.io.sstable.SSTableReader.getApproximateKeyCount(SSTableReader.java:243) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:146) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:75) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:232) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_45]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
        {code}
Obviously that error aborted the compaction and after some time the number of pending compactions became very high on every node. Of course, this in turn had a negative impact on several other metrics.

After reverting the setting we had to restart all nodes. After that compactions could finish again and the pending compactions could be worked off."	CASSANDRA	Resolved	10002	1	6295	lcs
12970593	If repair fails no way to run repair again	"I have a test that disables gossip and runs repair at the same time. 

{quote}
WARN  [RMI TCP Connection(15)-54.67.121.105] 2016-05-17 16:57:21,775 StorageService.java:384 - Stopping gossip by operator request
INFO  [RMI TCP Connection(15)-54.67.121.105] 2016-05-17 16:57:21,775 Gossiper.java:1463 - Announcing shutdown
INFO  [RMI TCP Connection(15)-54.67.121.105] 2016-05-17 16:57:21,776 StorageService.java:1999 - Node /172.31.31.1 state jump to shutdown
INFO  [HANDSHAKE-/172.31.17.32] 2016-05-17 16:57:21,895 OutboundTcpConnection.java:514 - Handshaking version with /172.31.17.32
INFO  [HANDSHAKE-/172.31.24.76] 2016-05-17 16:57:21,895 OutboundTcpConnection.java:514 - Handshaking version with /172.31.24.76
INFO  [Thread-25] 2016-05-17 16:57:21,925 RepairRunnable.java:125 - Starting repair command #1, repairing keyspace keyspace1 with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 3)
INFO  [Thread-26] 2016-05-17 16:57:21,953 RepairRunnable.java:125 - Starting repair command #2, repairing keyspace stresscql with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 3)
INFO  [Thread-27] 2016-05-17 16:57:21,967 RepairRunnable.java:125 - Starting repair command #3, repairing keyspace system_traces with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 2)
{quote}

This ends up failing:

{quote}
16:54:44.844 INFO  serverGroup-node-1-574 - STDOUT: [2016-05-17 16:57:21,933] Starting repair command #1, repairing keyspace keyspace1 with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 3)
[2016-05-17 16:57:21,943] Did not get positive replies from all endpoints. List of failed endpoint(s): [172.31.24.76, 172.31.17.32]
[2016-05-17 16:57:21,945] null
{quote}

Subsequent calls to repair with all nodes up still fails:

{quote}
ERROR [ValidationExecutor:3] 2016-05-17 18:58:53,460 CompactionManager.java:1193 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:3] 2016-05-17 18:58:53,460 Validator.java:261 - Failed creating a merkle tree for [repair #66425f10-1c61-11e6-83b2-0b1fff7a067d on keyspace1/standard1, 
{quote}"	CASSANDRA	Resolved	10002	1	6295	fallout
12965715	Incremental repair fails with vnodes+lcs+multi-dc	"Produced on 2.1.12

We are seeing incremental repair fail with an error regarding creating multiple repair sessions on overlapping sstables. This is happening in the following setup

* 6 nodes
* 2 Datacenters
* Vnodes enabled
* Leveled compaction on the relevant tables

When STCS is used instead, we don't hit an issue. This is slightly related to https://issues.apache.org/jira/browse/CASSANDRA-11461, except in this case OpsCenter repair service is running all repairs sequentially. Let me know what other information we can provide. "	CASSANDRA	Resolved	10002	1	6295	lcs
12665490	Improve the way we pick L0 compaction candidates	"We could improve the way we pick compaction candidates in level 0 in LCS.

The most common way for us to get behind on compaction is after repairs, we should exploit the fact that the streamed sstables are most often very narrow in range since the other nodes in the ring will have a similar sstable-range-distribution. We should in theory be able to do 10 concurrent compactions involving L1 - ie, partition L0 in buckets defined by the sstables in L1 to only keep one L1 SSTable busy for every compaction (be it L1 to L2 or L0 to L1).

we will need some heuristics on when to select candidates from the buckets and when to do it the old way (since L0 sstables can span several L1 sstables)"	CASSANDRA	Resolved	10002	4	6295	compaction, lcs
13019606	testall failure in org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest-compression	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_testall/602/testReport/org.apache.cassandra.db.compaction/NeverPurgeTest/minorNeverPurgeTombstonesTest_compression

{noformat}
Error Message

Memory was freed by Thread[NonPeriodicTasks:1,5,main]
Stacktrace

junit.framework.AssertionFailedError: Memory was freed by Thread[NonPeriodicTasks:1,5,main]
	at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:103)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:260)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:223)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBufferMmap(CompressedRandomAccessReader.java:168)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:226)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:303)
	at org.apache.cassandra.io.util.AbstractDataInput.readInt(AbstractDataInput.java:202)
	at org.apache.cassandra.io.util.AbstractDataInput.readLong(AbstractDataInput.java:264)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:131)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:169)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.verifyContainsTombstones(NeverPurgeTest.java:114)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest(NeverPurgeTest.java:85)
Standard Output

WARN  20:06:47 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:47 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:49 You a
...[truncated 2456 chars]...
 this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
WARN  20:06:56 You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!
{noformat}"	CASSANDRA	Resolved	10002	4	6295	test-failure
12750639	Compactions stop completely because of RuntimeException in CompactionExecutor	"I have a cluster that is recovering from being overloaded with writes.  I am using the workaround from CASSANDRA-6621 to prevent the STCS fallback (which is killing the cluster - see CASSANDRA-7949). 

I have observed that after one or more exceptions like this

{code}
ERROR [CompactionExecutor:4087] 2014-10-26 22:50:05,016 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:4087,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(425124616570337476, 0010000000001111000000000000033523da00001000000000033523da000000001111000000001000000000
00004000000000000000000100) >= current key DecoratedKey(-8778432288598355336, 0010000000001111000000000000040c7a8f00001000000000040c7a8f000000001111000000001000000000
00004000000000000000000100) writing into /cassandra-data/disk2/myks/mytable/myks-mytable-tmp-jb-130379-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

the node completely stops the compactions and I end up in the state like this:

{code}
# nodetool compactionstats
pending tasks: 1288
          compaction type        keyspace           table       completed           total      unit  progress
Active compaction remaining time :        n/a
{code}

The node recovers if restarted and starts compactions - until getting more exceptions like this.
"	CASSANDRA	Resolved	10002	1	6295	lcs
13014242	testall failure in org.apache.cassandra.db.compaction.CompactionsCQLTest.testTriggerMinorCompactionDTCS-compression	"example failure:
http://cassci.datastax.com/job/trunk_testall/1243/testReport/org.apache.cassandra.db.compaction/CompactionsCQLTest/testTriggerMinorCompactionDTCS_compression/

{code}
Error Message

No minor compaction triggered in 5000ms
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: No minor compaction triggered in 5000ms
	at org.apache.cassandra.db.compaction.CompactionsCQLTest.waitForMinor(CompactionsCQLTest.java:247)
	at org.apache.cassandra.db.compaction.CompactionsCQLTest.testTriggerMinorCompactionDTCS(CompactionsCQLTest.java:72)
{code}

Related failure:
http://cassci.datastax.com/job/cassandra-3.X_testall/47/testReport/org.apache.cassandra.db.compaction/CompactionsCQLTest/testTriggerMinorCompactionDTCS/"	CASSANDRA	Resolved	10002	1	6295	test-failure
12751061	CQL Spec needs to be updated with DateTieredCompactionStrategy	The {{CREATE TABLE}} section of the CQL Specification isn't up to date for the latest {{DateTieredCompactionStrategy}} that has been added in 2.0.11 and 2.1.1. We need to cover all its options just like it's done for the other strategies.	CASSANDRA	Resolved	10003	3	6295	dtcs
12707312	sstable_generation_loading_test sstableloader_compression_* dtests fail in 1.2, 2.0, and 2.1	"This looks like something fundamentally wrong with the sstable_generation_loading_test.py dtest - 1.2 and 2.0 look like:
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-sJDYKB
ok
remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-gRTcgb
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99.9th,elapsed_time
10000,1000,1000,11.5,80.3,280.6,5
END
ok
sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-xneocV
Testing sstableloader with pre_compression=Deflate and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-_iJ1tD
Testing sstableloader with pre_compression=Deflate and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-FZAci9
Testing sstableloader with pre_compression=Deflate and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-AsoepN
Testing sstableloader with pre_compression=None and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Cfiwh8
Testing sstableloader with pre_compression=None and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-tTuQQg
Testing sstableloader with pre_compression=None and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-lA0rXi
Testing sstableloader with pre_compression=Snappy and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-jJ5iub
Testing sstableloader with pre_compression=Snappy and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-JTMBKg
Testing sstableloader with pre_compression=Snappy and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL

======================================================================
FAIL: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 108, in sstableloader_compression_deflate_to_deflate_test
    self.load_sstable_with_configuration('Deflate', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 102, in sstableloader_compression_deflate_to_none_test
    self.load_sstable_with_configuration('Deflate', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 105, in sstableloader_compression_deflate_to_snappy_test
    self.load_sstable_with_configuration('Deflate', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 90, in sstableloader_compression_none_to_deflate_test
    self.load_sstable_with_configuration(None, 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 84, in sstableloader_compression_none_to_none_test
    self.load_sstable_with_configuration(None, None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 87, in sstableloader_compression_none_to_snappy_test
    self.load_sstable_with_configuration(None, 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 99, in sstableloader_compression_snappy_to_deflate_test
    self.load_sstable_with_configuration('Snappy', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 93, in sstableloader_compression_snappy_to_none_test
    self.load_sstable_with_configuration('Snappy', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 96, in sstableloader_compression_snappy_to_snappy_test
    self.load_sstable_with_configuration('Snappy', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

----------------------------------------------------------------------
Ran 11 tests in 336.704s

FAILED (failures=9)
{noformat}

2.1 looks like:
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-gPkstl
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Vx3Bsj
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Ap0VwZ
Testing sstableloader with pre_compression=Deflate and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Nsy44b
Testing sstableloader with pre_compression=Deflate and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-77_xIH
Testing sstableloader with pre_compression=Deflate and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-x3esB0
Testing sstableloader with pre_compression=None and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-HgJni9
Testing sstableloader with pre_compression=None and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-QkUEf5
Testing sstableloader with pre_compression=None and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-mgv9zG
Testing sstableloader with pre_compression=Snappy and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-e0mEVq
Testing sstableloader with pre_compression=Snappy and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-x3D6Bu
Testing sstableloader with pre_compression=Snappy and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR

======================================================================
ERROR: incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 25, in incompressible_data_in_compressed_table_test
    cluster.populate(1).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 55, in remove_index_file_test
    cluster.populate(1).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 108, in sstableloader_compression_deflate_to_deflate_test
    self.load_sstable_with_configuration('Deflate', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 102, in sstableloader_compression_deflate_to_none_test
    self.load_sstable_with_configuration('Deflate', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 105, in sstableloader_compression_deflate_to_snappy_test
    self.load_sstable_with_configuration('Deflate', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 90, in sstableloader_compression_none_to_deflate_test
    self.load_sstable_with_configuration(None, 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 84, in sstableloader_compression_none_to_none_test
    self.load_sstable_with_configuration(None, None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 87, in sstableloader_compression_none_to_snappy_test
    self.load_sstable_with_configuration(None, 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 99, in sstableloader_compression_snappy_to_deflate_test
    self.load_sstable_with_configuration('Snappy', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 93, in sstableloader_compression_snappy_to_none_test
    self.load_sstable_with_configuration('Snappy', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 96, in sstableloader_compression_snappy_to_snappy_test
    self.load_sstable_with_configuration('Snappy', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

----------------------------------------------------------------------
Ran 11 tests in 56.599s

FAILED (errors=11)
{noformat}

2.1 last ccm node1 log (node2 is the same):
{noformat}
INFO  [main] 2014-04-08 19:20:33,339 CassandraDaemon.java:102 - Hostname: hana.12.am
INFO  [main] 2014-04-08 19:20:33,396 YamlConfigurationLoader.java:80 - Loading settings from file:/tmp/dtest-x3D6Bu/test/node1/conf/cassandra.yaml
INFO  [main] 2014-04-08 19:20:33,514 YamlConfigurationLoader.java:123 - Node configuration:[authenticator=AllowAllAuthenticator; authorizer=AllowAllAuthorizer; auto_bootstrap=false; auto_snapshot=true; batchlog_replay_throttle_in_kb=1024; cas_contention_timeout_in_ms=1000; client_encryption_options=<REDACTED>; cluster_name=test; column_index_size_in_kb=64; commitlog_directory=/tmp/dtest-x3D6Bu/test/node1/commitlogs; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_period_in_ms=10000; compaction_preheat_key_cache=true; compaction_throughput_mb_per_sec=16; concurrent_counter_writes=32; concurrent_reads=32; concurrent_writes=32; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; cross_node_timeout=false; data_file_directories=[/tmp/dtest-x3D6Bu/test/node1/data]; disk_failure_policy=stop; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; endpoint_snitch=SimpleSnitch; flush_directory=/tmp/dtest-x3D6Bu/test/node1/flush; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; in_memory_compaction_limit_in_mb=64; incremental_backups=false; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=-9223372036854775808; inter_dc_tcp_nodelay=false; internode_compression=all; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=127.0.0.1; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=0.4; native_transport_port=9042; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_validity_in_ms=2000; phi_convict_threshold=5; preheat_kernel_page_cache=false; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=10000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_timeout_in_ms=10000; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=127.0.0.1; rpc_keepalive=true; rpc_port=9160; rpc_server_type=sync; saved_caches_directory=/tmp/dtest-x3D6Bu/test/node1/saved_caches; seed_provider=[{class_name=org.apache.cassandra.locator.SimpleSeedProvider, parameters=[{seeds=127.0.0.1}]}]; server_encryption_options=<REDACTED>; snapshot_before_compaction=false; ssl_storage_port=7001; start_native_transport=true; start_rpc=true; storage_port=7000; thrift_framed_transport_size_in_mb=15; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=10000; write_request_timeout_in_ms=10000]
INFO  [main] 2014-04-08 19:20:33,740 DatabaseDescriptor.java:197 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2014-04-08 19:20:33,746 DatabaseDescriptor.java:285 - Global memtable on-heap threshold is enabled at 249MB
INFO  [main] 2014-04-08 19:20:33,747 DatabaseDescriptor.java:289 - Global memtable off-heap threshold is enabled at 249MB
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:113 - JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_51
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:141 - Heap size: 1046937600/1046937600
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:143 - Code Cache Non-heap memory: init = 2555904(2496K) used = 675712(659K) committed = 2555904(2496K) max = 50331648(49152K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - Par Eden Space Heap memory: init = 214827008(209792K) used = 81668768(79754K) committed = 214827008(209792K) max = 214827008(209792K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - Par Survivor Space Heap memory: init = 26804224(26176K) used = 0(0K) committed = 26804224(26176K) max = 26804224(26176K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - CMS Old Gen Heap memory: init = 805306368(786432K) used = 0(0K) committed = 805306368(786432K) max = 805306368(786432K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - CMS Perm Gen Non-heap memory: init = 21757952(21248K) used = 16716776(16324K) committed = 21757952(21248K) max = 85983232(83968K)
INFO  [main] 2014-04-08 19:20:34,143 CassandraDaemon.java:144 - Classpath: /home/mshuler/git/cassandra/build/cobertura/classes:/tmp/dtest-x3D6Bu/test/node1/conf:/home/mshuler/git/cassandra/build/classes/main:/home/mshuler/git/cassandra/build/classes/thrift:/home/mshuler/git/cassandra/lib/airline-0.6.jar:/home/mshuler/git/cassandra/lib/antlr-3.2.jar:/home/mshuler/git/cassandra/lib/commons-cli-1.1.jar:/home/mshuler/git/cassandra/lib/commons-codec-1.2.jar:/home/mshuler/git/cassandra/lib/commons-lang3-3.1.jar:/home/mshuler/git/cassandra/lib/commons-math3-3.2.jar:/home/mshuler/git/cassandra/lib/compress-lzf-0.8.4.jar:/home/mshuler/git/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/home/mshuler/git/cassandra/lib/disruptor-3.0.1.jar:/home/mshuler/git/cassandra/lib/guava-16.0.jar:/home/mshuler/git/cassandra/lib/high-scale-lib-1.1.2.jar:/home/mshuler/git/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar:/home/mshuler/git/cassandra/lib/javax.inject.jar:/home/mshuler/git/cassandra/lib/jbcrypt-0.3m.jar:/home/mshuler/git/cassandra/lib/jline-1.0.jar:/home/mshuler/git/cassandra/lib/jna-4.0.0.jar:/home/mshuler/git/cassandra/lib/json-simple-1.1.jar:/home/mshuler/git/cassandra/lib/libthrift-0.9.1.jar:/home/mshuler/git/cassandra/lib/logback-classic-1.1.2.jar:/home/mshuler/git/cassandra/lib/logback-core-1.1.12.jar:/home/mshuler/git/cassandra/lib/lz4-1.2.0.jar:/home/mshuler/git/cassandra/lib/metrics-core-2.2.0.jar:/home/mshuler/git/cassandra/lib/netty-all-4.0.17.Final.jar:/home/mshuler/git/cassandra/lib/reporter-config-2.1.0.jar:/home/mshuler/git/cassandra/lib/slf4j-api-1.7.2.jar:/home/mshuler/git/cassandra/lib/snakeyaml-1.11.jar:/home/mshuler/git/cassandra/lib/snappy-java-1.0.5.jar:/home/mshuler/git/cassandra/lib/stream-2.5.2.jar:/home/mshuler/git/cassandra/lib/super-csv-2.1.0.jar:/home/mshuler/git/cassandra/lib/thrift-server-0.3.3.jar:/home/mshuler/.m2/repository/net/sourceforge/cobertura/cobertura/1.9.4.1/cobertura-1.9.4.1.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar
WARN  [main] 2014-04-08 19:20:34,204 CLibrary.java:130 - Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
INFO  [main] 2014-04-08 19:20:34,227 CacheService.java:111 - Initializing key cache with capacity of 49 MBs.
INFO  [main] 2014-04-08 19:20:34,237 CacheService.java:133 - Initializing row cache with capacity of 0 MBs
INFO  [main] 2014-04-08 19:20:34,244 CacheService.java:150 - Initializing counter cache with capacity of 24 MBs
INFO  [main] 2014-04-08 19:20:34,246 CacheService.java:161 - Scheduling counter cache save to every 7200 seconds (going to save all keys).
INFO  [main] 2014-04-08 19:20:34,360 ColumnFamilyStore.java:283 - Initializing system.schema_triggers
INFO  [main] 2014-04-08 19:20:35,608 ColumnFamilyStore.java:283 - Initializing system.compaction_history
INFO  [main] 2014-04-08 19:20:35,618 ColumnFamilyStore.java:283 - Initializing system.batchlog
INFO  [main] 2014-04-08 19:20:35,624 ColumnFamilyStore.java:283 - Initializing system.sstable_activity
INFO  [main] 2014-04-08 19:20:35,630 ColumnFamilyStore.java:283 - Initializing system.peer_events
INFO  [main] 2014-04-08 19:20:35,643 ColumnFamilyStore.java:283 - Initializing system.compactions_in_progress
INFO  [main] 2014-04-08 19:20:35,652 ColumnFamilyStore.java:283 - Initializing system.hints
INFO  [main] 2014-04-08 19:20:35,656 ColumnFamilyStore.java:283 - Initializing system.schema_keyspaces
INFO  [main] 2014-04-08 19:20:35,660 ColumnFamilyStore.java:283 - Initializing system.range_xfers
INFO  [main] 2014-04-08 19:20:35,664 ColumnFamilyStore.java:283 - Initializing system.schema_columnfamilies
INFO  [main] 2014-04-08 19:20:35,669 ColumnFamilyStore.java:283 - Initializing system.NodeIdInfo
INFO  [main] 2014-04-08 19:20:35,677 ColumnFamilyStore.java:283 - Initializing system.paxos
INFO  [main] 2014-04-08 19:20:35,682 ColumnFamilyStore.java:283 - Initializing system.schema_usertypes
INFO  [main] 2014-04-08 19:20:35,686 ColumnFamilyStore.java:283 - Initializing system.schema_columns
INFO  [main] 2014-04-08 19:20:35,691 ColumnFamilyStore.java:283 - Initializing system.IndexInfo
INFO  [main] 2014-04-08 19:20:35,695 ColumnFamilyStore.java:283 - Initializing system.peers
INFO  [main] 2014-04-08 19:20:35,700 ColumnFamilyStore.java:283 - Initializing system.local
INFO  [main] 2014-04-08 19:20:35,820 DatabaseDescriptor.java:587 - Couldn't detect any schema definitions in local storage.
INFO  [main] 2014-04-08 19:20:35,821 DatabaseDescriptor.java:592 - To create keyspaces and column families, see 'help create' in cqlsh.
INFO  [main] 2014-04-08 19:20:35,903 ColumnFamilyStore.java:853 - Enqueuing flush of local: 1109 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:35,921 Memtable.java:344 - Writing Memtable-local@1878619947(220 serialized bytes, 5 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:36,021 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-1-Data.db (171 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=403)
INFO  [main] 2014-04-08 19:20:36,038 CommitLog.java:108 - No commitlog files found; skipping replay
INFO  [main] 2014-04-08 19:20:36,535 StorageService.java:510 - Cassandra version: 2.1.0-beta1-SNAPSHOT
INFO  [main] 2014-04-08 19:20:36,535 StorageService.java:511 - Thrift API version: 19.39.0
INFO  [main] 2014-04-08 19:20:36,559 StorageService.java:512 - CQL supported versions: 2.0.0,3.1.5 (default: 3.1.5)
INFO  [main] 2014-04-08 19:20:36,595 IndexSummaryManager.java:99 - Initializing index summary manager with a memory pool size of 49 MB and a resize interval of 60 minutes
INFO  [main] 2014-04-08 19:20:36,607 StorageService.java:537 - Loading persisted ring state
INFO  [main] 2014-04-08 19:20:36,679 StorageService.java:858 - Saved tokens not found. Using configuration value: [-9223372036854775808]
INFO  [main] 2014-04-08 19:20:36,701 MigrationManager.java:206 - Create new Keyspace: KSMetaData{name=system_traces, strategyClass=SimpleStrategy, strategyOptions={replication_factor=2}, cfMetaData={sessions=org.apache.cassandra.config.CFMetaData@7892cf41[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(706172616d6574657273:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type))),comment=traced sessions,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=duration, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=parameters, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=started_at, type=org.apache.cassandra.db.marshal.TimestampType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=request, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=coordinator, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}], events=org.apache.cassandra.config.CFMetaData@3a437a40[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.UTF8Type),comment=,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=source, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=thread, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=activity, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=14 cap=14]=ColumnDefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=event_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]}, durableWrites=true, userTypes=org.apache.cassandra.config.UTMetaData@240f1da2}
INFO  [MigrationStage:1] 2014-04-08 19:20:36,826 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 990 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:36,834 Memtable.java:344 - Writing Memtable-schema_keyspaces@2107075397(251 serialized bytes, 7 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:36,915 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-1-Data.db (216 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:36,920 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 164066 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:36,921 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1732384250(30202 serialized bytes, 514 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:37,019 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-Data.db (6720 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:37,033 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 288881 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:37,034 Memtable.java:344 - Writing Memtable-schema_columns@985819426(46627 serialized bytes, 904 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:37,140 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-1-Data.db (10835 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:37,351 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@40e9da6[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(706172616d6574657273:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type))),comment=traced sessions,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=duration, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=parameters, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=started_at, type=org.apache.cassandra.db.marshal.TimestampType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=request, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=coordinator, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 19:20:37,357 ColumnFamilyStore.java:283 - Initializing system_traces.sessions
INFO  [MigrationStage:1] 2014-04-08 19:20:37,358 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@5b8fff5e[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.UTF8Type),comment=,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=source, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=thread, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=activity, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=14 cap=14]=ColumnDefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=event_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 19:20:37,363 ColumnFamilyStore.java:283 - Initializing system_traces.events
ERROR [MigrationStage:1] 2014-04-08 19:20:37,426 CassandraDaemon.java:166 - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340) ~[main/:na]
        at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380) ~[main/:na]
        at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR [main] 2014-04-08 19:20:37,427 CassandraDaemon.java:471 - Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207) ~[main/:na]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543) [main/:na]
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.get(FutureTask.java:188) ~[na:1.7.0_51]
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407) ~[main/:na]
        ... 8 common frames omitted
Caused by: java.lang.AssertionError: null
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340) ~[main/:na]
        at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380) ~[main/:na]
        at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
ERROR [StorageServiceShutdownHook] 2014-04-08 19:20:37,437 CassandraDaemon.java:166 - Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.gms.Gossiper.stop(Gossiper.java:1270) ~[main/:na]
        at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:581) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
{noformat}"	CASSANDRA	Resolved	10002	4	6295	qa-resolved
12923645	sstableutil_test.py:SSTableUtilTest.abortedcompaction_test flapping on 3.0	"{{sstableutil_test.py:SSTableUtilTest.abortedcompaction_test}} flaps on 3.0:

http://cassci.datastax.com/job/cassandra-3.0_dtest/438/testReport/junit/sstableutil_test/SSTableUtilTest/abortedcompaction_test/

It also flaps on the CassCI job running without vnodes:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/110/testReport/junit/sstableutil_test/SSTableUtilTest/abortedcompaction_test/history/

"	CASSANDRA	Resolved	10002	7	6295	dtest
12751400	Overlapping sstables in L1+	"Seems we have a bug that can create overlapping sstables in L1:

{code}
WARN [main] 2014-10-28 04:09:42,295 LeveledManifest.java (line 164) At level 2, SSTableReader(path='<sstable>') [DecoratedKey(2838397575996053472, 00
10000000001111000000000000066059b200001000000000066059b200000000111100000000100000000000004000000000000000000100), DecoratedKey(5516674013223138308, 001000000000111100000000000000ff2d160000100000000000ff2d1600000
000111100000000100000000000004000000000000000000100)] overlaps SSTableReader(path='<sstable>') [DecoratedKey(2839992722300822584, 00100000000011110000
0000000000229ad20000100000000000229ad200000000111100000000100000000000004000000000000000000100), DecoratedKey(5532836928694021724, 0010000000001111000000000000034b05a600001000000000034b05a600000000111100000000100
000000000004000000000000000000100)].  This could be caused by a bug in Cassandra 1.1.0 .. 1.1.3 or due to the fact that you have dropped sstables from another node into the data directory. Sending back to L0.  If
 you didn't drop in sstables, and have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable
{code}

Which might manifest itself during compaction with this exception:
{code}
ERROR [CompactionExecutor:3152] 2014-10-28 00:24:06,134 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:3152,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(5516674013223138308, 001000000000111100000000000000ff2d160000100000000000ff2d1600000000111100000000100000000000004000000000000000000100) >= current key DecoratedKey(2839992722300822584, 001000000000111100000000000000229ad20000100000000000229ad200000000111100000000100000000000004000000000000000000100) writing into <sstable>
{code}
since we use LeveledScanner when compacting (the backing sstable scanner might go beyond the start of the next sstable scanner)"	CASSANDRA	Resolved	10002	1	6295	lcs
12712386	sstablemetadata command should print some more stuff	It would be nice if the sstablemetadata command printed out some more of the stuff we track.  Like the Min/Max column names and the min/max token in the file.	CASSANDRA	Resolved	10003	4	6295	lhf
13097635	CircleCI fix - only collect the xml file from containers where it exists	"Followup from CASSANDRA-13775 - my fix with {{ant eclipse-warnings}} obviously does not work since it doesn't generate any xml files

Push a new fix here: https://github.com/krummas/cassandra/commits/marcuse/fix_circle_3.0 which only collects the xml file from the first 3 containers
Test running here: https://circleci.com/gh/krummas/cassandra/86"	CASSANDRA	Resolved	10002	1	6295	CI
12862261	Make DTCS work well with old data	"Operational tasks become incredibly expensive if you keep around a long timespan of data with DTCS - with default settings and 1 year of data, the oldest window covers about 180 days. Bootstrapping a node with vnodes with this data layout will force cassandra to compact very many sstables in this window.

We should probably put a cap on how big the biggest windows can get. We could probably default this to something sane based on max_sstable_age (ie, say we can reasonably handle 1000 sstables per node, then we can calculate how big the windows should be to allow that)"	CASSANDRA	Resolved	10002	7	6295	dtcs
12754861	"Create a tool that given a bunch of sstables creates a ""decent"" sstable leveling"	"In old versions of cassandra (i.e. not trunk/3.0), when bootstrapping a new node, you will end up with a ton of files in L0 and it might be extremely painful to get LCS to compact into a new leveling

We could probably exploit the fact that we have many non-overlapping sstables in L0, and offline-bump those sstables into higher levels. It does not need to be perfect, just get the majority of the data into L1+ without creating overlaps.

So, suggestion is to create an offline tool that looks at the range each sstable covers and tries to bump it as high as possible in the leveling."	CASSANDRA	Resolved	10002	4	6295	lcs
13152896	Fix setting min/max compaction threshold with LCS	To be able to actually set max/min_threshold in compaction options we need to remove it from the options map when validating.	CASSANDRA	Resolved	10002	1	6295	lcs
12755926	LeveledCompactionStrategy should split large files across data directories when compacting	"Because we fall back to STCS for L0 when LCS gets behind, the sstables in L0 can get quite large during sustained periods of heavy writes.  This can result in large imbalances between data volumes when using JBOD support.  

Eventually these large files get broken up as L0 sstables are moved up into higher levels; however, because LCS only chooses a single volume on which to write all of the sstables created during a single compaction, the imbalance is persisted."	CASSANDRA	Resolved	10002	4	6295	lcs
12691271	Add flag to disable STCS in L0	"The initial discussion started in (closed) CASSANDRA-5371. I've rewritten my last comment here...

After streaming (e.g. during boostrap) Cassandra places all sstables at L0. At the end of the process we end up with huge number of sstables at the lowest level. 

Currently, Cassandra falls back to STCS until the number of sstables at L0 reaches the reasonable level (32 or something).

I'm not sure if falling back to STCS is the best way to handle this particular situation. I've read the comment in the code and I'm aware why it is a good thing to do if we have to many sstables at L0 as a result of too many random inserts. We have a lot of sstables, each of them covers the whole ring, there's simply no better option.

However, after the bootstrap situation looks a bit different. The loaded sstables already have very small ranges! We just have to tidy up a bit and everything should be OK. STCS ignores that completely and after a while we have a bit less sstables but each of them covers the whole ring instead of just a small part. I believe that in that case letting LCS do the job is a better option that allowing STCS mix everything up before.

Is there a way to disable STCS fallback? I'd like to test that scenario in practice during our next bootstrap...

Does Cassandra really have to put streamed sstables at L0? The only thing we have to assure is that sstables at any given level do not overlap. If we stream different regions from different nodes how can we get any overlaps?

"	CASSANDRA	Resolved	10003	4	6295	compaction, lcs, streaming
12749501	Opening results early with leveled compactions broken	"CASSANDRA-8034 notifies the listeners whenever we replace an sstable to make sure we have track the right instance.

Problem is though that when we open early and finish a compaction, we try to re-add the same sstable to the manifest which drops it to level 0 since it overlaps with the one that is already there"	CASSANDRA	Resolved	10000	1	6295	lcs
12522905	Repair: compare all trees together (for a given range/cf) instead of by pair in isolation	"Currently, repair compare merkle trees by pair, in isolation of any other tree. What that means concretely is that if I have three node A, B and C (RF=3) with A and B in sync, but C having some range r inconsitent with both A and B (since those are consistent), we will do the following transfer of r: A -> C, C -> A, B -> C, C -> B.

The fact that we do both A -> C and C -> A is fine, because we cannot know which one is more to date from A or C. However, the transfer B -> C is useless provided we do A -> C if A and B are in sync. Not doing that transfer will be a 25% improvement in that case. With RF=5 and only one node inconsistent with all the others, that almost a 40% improvement, etc...

Given that this situation of one node not in sync while the others are is probably fairly common (one node died so it is behind), this could be a fair improvement over what is transferred. In the case where we use repair to rebuild completely a node, this will be a dramatic improvement, because it will avoid the rebuilded node to get RF times the data it should get."	CASSANDRA	Resolved	10003	4	6295	repair
12823670	Disable single-sstable tombstone compactions for DTCS	"We should probably disable tombstone compactions by default for DTCS for these reasons:

# users should not do deletes with DTCS
# the only way we should get rid of data is by TTL - and then we don't want to trigger a single sstable compaction whenever an sstable is 20%+ expired, we want to drop the whole thing when it is fully expired"	CASSANDRA	Resolved	10002	1	6295	dtcs
12992973	dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.bloomfilter_size_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/19/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/bloomfilter_size_test

500352 not less than or equal to 150000"	CASSANDRA	Resolved	10002	1	6295	dtest
13325867	Make sure we don't throw any uncaught exceptions during in-jvm dtests	We should assert that we don't throw any uncaught exceptions when running in-jvm dtests	CASSANDRA	Resolved	10002	4	6295	pull-request-available
12840061	DTCS configuration proposals for handling consequences of repairs	"This is a document bringing up some issues when DTCS is used to compact time series data in a three node cluster. The DTCS is currently configured with a few parameters that are making the configuration fairly simple, but might cause problems in certain special cases like recovering from the flood of small SSTables due to repair operation. We are suggesting some ideas that might be a starting point for further discussions. Following sections are containing:

- Description of the cassandra setup
- Feeding process of the data
- Failure testing
- Issues caused by the repair operations for the DTCS
- Proposal for the DTCS configuration parameters

Attachments are included to support the discussion and there is a separate section giving explanation for those.

Cassandra setup and data model

- Cluster is composed from three nodes running Cassandra 2.1.2. Replication factor is two and read and write consistency levels are ONE.
- Data is time series data. Data is saved so that one row contains a certain time span of data for a given metric ( 20 days in this case). The row key contains information about the start time of the time span and metrix name. Column name gives the offset from the beginning of time span. Column time stamp is set to correspond time stamp when adding together the timestamp from the row key and the offset (the actual time stamp of data point). Data model is analog to KairosDB implementation.
- Average sampling rate is 10 seconds varying significantly from metric to metric.
- 100 000 metrics are fed to the Cassandra.
- max_sstable_age_days is set to 5 days (objective is to keep SStable files in manageable size, around 50 GB)
- TTL is not in use in the test.

Procedure for the failure test.

- Data is first dumped to Cassandra for 11 days and the data dumping is stopped so that DTCS will have a change to finish all compactions. Data is dumped with ""fake timestamps"" so that column time stamp is set when data is written to Cassandra.
- One of the nodes is taken down and new data is dumped on top of the earlier data covering couple of hours worth of data (faked time stamps).
- Dumping is stopped and the node is kept down for few hours.
- Node is taken up and the ""nodetool repair"" is applied on the node that was down.

Consequences

- Repair operation will lead to massive amount of new SStables far back in the history. New SStables are covering similar time spans than the files that were created by DTCS before the shutdown of one of the nodes.
- To be able to compact the small files the max_sstable_age_days should be increased to allow compaction to handle the files. However, the in a practical case the time window will increase so large that generated files will be huge that is not desirable. The compaction also combines together one very large file with a bunch of small files in several phases that is not effective. Generating really large files may also lead to out of disc space problems.
- See the list of time graphs later in the document.

Improvement proposals for the DTCS configuration

Below is a list of desired properties for the configuration. Current parameters are mentioned if available.

- Initial window size (currently:base_time_seconds)
- The amount of similar size windows for the bucketing (currently: min_threshold)
- The multiplier for the window size when increased (currently: min_threshold). This we would like to be independent from the min_threshold parameter so that you could actually control the rate how fast the window size is increased.
- Maximum length of the time window inside which the files are assigned for a certain bucket (not currently defined). This means that expansion of time window length is restricted. When the limit is reached the window size will be same all the way back in the history (e.g. one week)
- The maximum horizon in which SStables are candidates for buckets (currently: max_sstable_age_days)
- Maximum file size of SStable allowed to be in a set of files to be compacted (not possible currently). Preventing out of disk space situations.
- Optional strategies to select the most interesting bucket:
    - Minimum amount of SStables in the time window before it is a candidate for the most interesting bucket (currently: min_threshold for the most recent window, otherwise two). Being able set this value independently would allow to put most of the efforts on those areas where a large amount of small files should be compacted together instead of few new files.
    - Optionally, the criteria for the most interesting bucket could be set: e.g. select the window with most files to be compacted.
    - Inside the bucket when the amount of files is limited by max_threshold, the compaction would select first small files instead of one huge file and a bunch of small files.

The above set of parameters allows to recover from repair operations producing large amount of small SStables.

- Maximum length of the time window for compactions would keep the compacted SStable size in reasonable range and would allow to extend the horizon far back in the history
- Combining small files together instead of combining one huge file with e.g. 31 small files again and again is more disk efficient

In addition to the previous advantages the above parameters would also allow:

- Dumping of more data in the history (e.g. new metrics) by assigning the correct timestamp for the column (fake time stamp) and proper compaction of new and existing SStables.
- Expiring reasonable size SStable with TTL even if the compactions would be intermittently executed far back in the history. In this case the new data has to fed with TTL calculated dynamically.
- Note: Being able to give the absolute time stamp for the column expiry time would be beneficial when data is dumped back in the history. This is the case when you move data from some legacy system to Cassandra with faked time stamps and would like to keep the data only a certain time period.  Currently the absolute time stamp is calculated by Cassandra from the system time and given TTL. TTL has to be calculated dynamically based on the current time and desired expiry moment making things more complex.

One interesting question is that why those duplicate SStable files are created? The duplication problem could not be produced when the data was dumped with following spec:

- 100 metrics
- 20 days of data in one row
- one year of data
- max_sstable_age_days = 15
 - memtable_offheap_space_in_mb was decreased so that small SStables were created (to create something to be compacted)
 - One node was taken down and one more day of data was dumped on top of the earlier data
- ""nodetool repair -pr"" was executed on each node => duplicates were checked in each step => no duplicates
- ""nodetool repair"" was executed on a node that was down => no duplicates were generated


--------------------------------------------------------------------------------------------------------------------

ATTACHMENTS


Time graphs of content of SSTables from different phases of the test run:

*************************************************************

Fields in the below time graphs are following:

- Order number from the  SSTable file name
- Minimum column timestamp in the SSTable file
- Timespan representation graphically
- Maximum column time stamp in SStable
- The size of the SStable in megabytes

Time graphs after dumping the 11 days of data and letting all compactions to run through

node0_20150621_1646_time_graph.txt
node1_20150621_1646_time_graph.txt
node2_20150621_1646_time_graph.txt (error: same as for node1, but the behavior is same)

Time graphs after taking one node down (node2) and dumping couple of hours of mode data

node0_20150621_2320_time_graph.txt
node1_20150621_2320_time_graph.txt
node2_20150621_2320_time_graph.txt 

Time graphs when the repair operation has finished and compactions are done. Compactions will naturally handle only the files inside the max_sstable_age_days range.

==> Now there is a large amount of small files covering pretty much same areas as the original SStables

node0_20150623_1526_time_graph.txt
node1_20150623_1526_time_graph.txt
node2_20150623_1526_time_graph.txt

-----------
Trend from the SStable count as a function of time on each node.
sstable_counts.jpg

Vertical lines:

1) Clearing the database and dumping the 11 days worth of data
2) Stopping the dumping and letting compactions run
3) Taking one node down (top bottom one in figure) and dumping few hours of new data on top of earlier data
4) Starting the repair operation
5) Repair operation finished


--------
Nodetool status prints before and after repair operation
nodetool status infos.txt

--------------
Tracing compactions

Log files were parsed to demonstrate the creation of new small SStables and the combination of one large file with a bunch of small ones. This is done from the time range where the max_sstable_age_days is able to reach (in this case 5 days). The hierarchy of the files is shown in the file ""sstable_compaction_trace.txt"". The first flushed file can be found from the line 10.

Each line represents either a flushed SStable or the SStable created by DTCS. For flushed files the timestamp indicates the time period the file represents. For compacted files (marked with C) the first timestamp represents the moment when the compaction was done (wall clock). Time stamps are faked when written to the database. The size of the file is the last field. The first field with number in parenthesis shows the level of the file. Top level files marked with (0) are those that don't have any predecessors and should be found from the disk also.

SStables that are created by the repair operation are not mentioned in the log files so they are handled as phantom files. The existence of file can be concluded from the predecessor list of compacted SStable. Those are marked with None,None in timestamps.

In the file ""sstable_compaction_trace_snipped.txt"" is one portion that shows the compaction hierarchy for the small files originating from the repair operation. max_threshold is in the default value of 32. In each step 31 tiny files are compacted together with 46 GB file.


sstable_compaction_trace.txt
sstable_compaction_trace_snipped.txt"	CASSANDRA	Resolved	10002	4	6295	compaction, dtcs
12632213	Improve LeveledScanner work estimation	See https://issues.apache.org/jira/browse/CASSANDRA-5222?focusedCommentId=13577420&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13577420	CASSANDRA	Resolved	10002	4	6295	compaction, lcs
12928084	disk_balance_bootstrap_test is failing on trunk	"http://cassci.datastax.com/job/trunk_dtest/891/testReport/junit/disk_balance_test/TestDiskBalance/disk_balance_bootstrap_test/

{code}
======================================================================
FAIL: disk_balance_bootstrap_test (disk_balance_test.TestDiskBalance)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 51, in disk_balance_bootstrap_test
    self.assert_balanced(node)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/disk_balance_test.py"", line 127, in assert_balanced
    assert_almost_equal(*sums, error=0.2, error_message=node.name)
  File ""/home/aboudreault/git/cstar/cassandra-dtest/assertions.py"", line 65, in assert_almost_equal
    assert vmin > vmax * (1.0 - error) or vmin == vmax, ""values not within %.2f%% of the max: %s (%s)"" % (error * 100, args, kwargs['error_message'])
AssertionError: values not within 20.00% of the max: (529955, 386060, 473640) (node4)
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-nNoQzp
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 1 test in 114.862s

FAILED (failures=1)

{code}"	CASSANDRA	Resolved	10002	1	6295	dtest
13108030	Infinite compaction of L0 SSTables in JBOD	"I recently upgraded from 2.2.6 to 3.11.0.

I am seeing Cassandra loop infinitely compacting the same data over and over. Attaching logs.

It is compacting two tables, one on /srv/disk10, the other on /srv/disk1. It does create new SSTables but immediately recompacts again. Note that I am not inserting anything at the moment, there is no flushing happening on this table (Memtable switch count has not changed).

My theory is that it somehow thinks those should be compaction candidates. But they shouldn't be, they are on different disks and I ran nodetool relocatesstables as well as nodetool compact. So, it tries to compact them together, but the compaction results in the exact same 2 SSTables on the 2 disks, because the keys are split by data disk.

This is pretty serious, because all our nodes right now are consuming CPU doing this for multiple tables, it seems."	CASSANDRA	Resolved	10002	1	6295	jbod-aware-compaction
12748313	Stopping a node during compaction can make already written files stay around	"In leveled compaction we generally create many files during compaction, in 2.0 we left the ones we had written as -tmp- files, in 2.1 we close and open the readers, removing the -tmp- markers.

This means that any ongoing compactions will leave the resulting files around if we restart. Note that stop:ing the compaction will cause an exception and that makes us call abort() on the SSTableRewriter which removes the files.

Guess a fix could be to keep the -tmp- marker and make -tmplink- files until we are actually done with the compaction."	CASSANDRA	Resolved	10002	1	6295	triaged
12705654	upgradesstables does not maintain levels for existing SSTables	"Initially ran into this issue on a DSE 3.2 (C* 1.2) to DSE 4.0 (C* 2.0) upgrade, and then I was able to reproduce it when testing an upgrade from C* 2.0.5 to C* 2.1-beta so the problem still exists in the latest code.

Basically after you've upgraded to the new version and run ""nodetool upgradesstables"" on a CF/table that has been using LCS, then all of the non-L0 SSTables will be changed to L0 in the upgraded SSTables. In other words, they don't maintain their level and will have to go through the compaction again. The problem is that if you've got thousands of non-L0 SSTables before the upgrade, then all of these files showing up in L0 will push the system to do STCS and start to build some huge L0 tables. If a user doesn't budget enough free space (for example, if they used the recommended guideline and only budgeted 10% of free space because LCS is in use), then this STCS in L0 effect will have them run out of space."	CASSANDRA	Resolved	10002	1	6295	lcs
12824437	for DateTieredCompactionStrategy, TIMESTAMP_RESOLUTION_KEY sets wrong msxSSTableAge value if RESOLUTION is other than MILLISECONDS	"I was trying to set 'timestamp_resolution' to MINUTES/HOURS/DAYS. it turned out maxSSTableAge was set as wrong value. In the code,
{code}
    public DateTieredCompactionStrategyOptions(Map<String, String> options)
    {
        String optionValue = options.get(TIMESTAMP_RESOLUTION_KEY);
        TimeUnit timestampResolution = optionValue == null ? DEFAULT_TIMESTAMP_RESOLUTION : TimeUnit.valueOf(optionValue);
        optionValue = options.get(MAX_SSTABLE_AGE_KEY);
        double fractionalDays = optionValue == null ? DEFAULT_MAX_SSTABLE_AGE_DAYS : Double.parseDouble(optionValue);
        maxSSTableAge = Math.round(fractionalDays * timestampResolution.convert(1, TimeUnit.DAYS));
 ...   }{code}

maxSSTableAge will be set as the value in ""timestamp_resolution"" unit, such as , with the following settings,
        'timestamp_resolution':'HOURS',
        'max_sstable_age_days':'7',
        'base_time_seconds':'3600'
and I get: 
maxSSTableAge=168,  baseTime=1

while in the following routine, it expect maxSSTableAge as milliseconds
    static Iterable<SSTableReader> filterOldSSTables(List<SSTableReader> sstables, long maxSSTableAge, long now)


"	CASSANDRA	Resolved	10002	1	6295	dtcs
12945616	dtest failure in disk_balance_test.TestDiskBalance.disk_balance_bootstrap_test	"example failure:

http://cassci.datastax.com/job/trunk_dtest/1011/testReport/disk_balance_test/TestDiskBalance/disk_balance_bootstrap_test

Failed on CassCI build trunk_dtest #1011

This looks likely to be a test issue, perhaps we need to relax the assertion here a bit:
{noformat}
values not within 20.00% of the max: (474650, 382235, 513385) (node1)
{noformat}

This is flaky with several flaps in the last few weeks."	CASSANDRA	Resolved	10002	4	6295	dtest
12823744	Max sstable size in leveled manifest is an int, creating large sstables overflows this and breaks LCS	"nodetool compactionstats
pending tasks: -222222228

I can see negative numbers in 'pending tasks' on all 8 nodes
it looks like -222222228 + real number of pending tasks
for example -222222128 for 100 real pending tasks

"	CASSANDRA	Resolved	10002	1	6295	lcs
12832787	SSTable leak after stress and repair	"I have a dtest that fails intermittently because of SSTable leaks. The test logic leading to the error is:

- create a 5-node cluster
- insert 5000 records with {{stress}}, RF=3 at CL=ONE
- run {{flush}} on all nodes 
- run {{repair}} on a single node.

The leak is detected on a different node than {{repair}} was run on.

The failing test is [here|https://github.com/mambocab/cassandra-dtest/blob/CASSANDRA-5839-squash/repair_test.py#L317]. The relevant error his [here|https://gist.github.com/mambocab/8aab7b03496e0b279bd3#file-node2-log-L256], along with the errors from the entire 5-node cluster. In these logs, the {{repair}} was run on {{node1}} and the leak was found on {{node2}}.

I can bisect, but I thought I'd get the ball rolling in case someone knows where to look."	CASSANDRA	Resolved	10002	1	6295	stress
12767650	sstablesplit fails *randomly* with Data component is missing	"I'm experiencing an issue related to sstablesplit. I would like to understand if I am doing something wrong or there is an issue in the split process. The process fails randomly with the following exception:
{code}
ERROR 02:17:36 Error in ThreadPoolExecutor
java.lang.AssertionError: Data component is missing for sstable./tools/bin/../../data/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-16
{code}

See attached output.log file. The process never stops after this exception and I've also seen the dataset growing indefinitely (number of sstables).  

* I have not been able to reproduce the issue with a single sstablesplit command. ie, specifying all files with glob matching.
* I can reproduce the bug if I call multiple sstablesplit one file at the time (the way ccm does)

Here is the test case file to reproduce the bug:

https://drive.google.com/file/d/0BwZ_GPM33j6KdVh0NTdkOWV2R1E/view?usp=sharing

1. Download the split_issue.tar.gz file. It includes latest cassandra-2.1 branch binaries.
2. Extract it
3. CD inside the use case directory
4. Download the dataset (2G) just to be sure we have the same thing, and place it in the working directory.
   https://docs.google.com/uc?id=0BwZ_GPM33j6KV3ViNnpPcVFndUU&export=download
5. The first time, run ./test.sh. This will setup and run a test.
6. The next times, you can only run ./test --no-setup . This will only reset the dataset as its initial state and re-run the test. You might have to run the tests some times before experiencing it... but I'm always able with only 2-3 runs.
"	CASSANDRA	Resolved	10002	1	6295	qa-resolved
12999380	For LCS, single SSTable up-level is handled inefficiently	"I'm using the latest trunk (as of August 2016, which probably is going to be 3.10) to run some experiments on LeveledCompactionStrategy and noticed this inefficiency.

The test data is generated using cassandra-stress default parameters (keyspace1.standard1), so as you can imagine, it consists of a ton of newly inserted partitions that will never merge in compactions, which is probably the worst kind of workload for LCS (however, I'll detail later why this scenario should not be ignored as a corner case; for now, let's just assume we still want to handle this scenario efficiently).

After the compaction test is done, I scrubbed debug.log for patterns that match  the ""Compacted"" summary so that I can see how long each individual compaction took and how many bytes they processed. The search pattern is like the following:

{noformat}
grep 'Compacted.*standard1' debug.log
{noformat}

Interestingly, I noticed a lot of the finished compactions are marked as having *only one* SSTable involved. With the workload mentioned above, the ""single SSTable"" compactions actually consist of the majority of all compactions (as shown below), so its efficiency can affect the overall compaction throughput quite a bit.

{noformat}
automaton@0ce59d338-1:~/cassandra-trunk/logs$ grep 'Compacted.*standard1' debug.log-test1 | wc -l
243
automaton@0ce59d338-1:~/cassandra-trunk/logs$ grep 'Compacted.*standard1' debug.log-test1 | grep "") 1 sstable"" | wc -l
218
{noformat}

By looking at the code, it appears that there's a way to directly edit the level of a particular SSTable like the following:

{code}
sstable.descriptor.getMetadataSerializer().mutateLevel(sstable.descriptor, targetLevel);
sstable.reloadSSTableMetadata();
{code}

To be exact, I summed up the time spent for these single-SSTable compactions (the total data size is 60GB) and found that if each compaction only needs to spend 100ms for only the metadata change (instead of the 10+ second they're doing now), it can already achieve 22.75% saving on total compaction time.

Compared to what we have now (reading the whole single-SSTable from old level and writing out the same single-SSTable at the new level), the only difference I could think of by using this approach is that the new SSTable will have the same file name (sequence number) as the old one's, which could break some assumptions on some other part of the code. However, not having to go through the full read/write IO, and not having to bear the overhead of cleaning up the old file, creating the new file, creating more churns in heap and file buffer, it seems the benefits outweigh the inconvenience. So I'd argue this JIRA belongs to LHF and should be made available in 3.0.x as well.

As mentioned in the 2nd paragraph, I'm also going to address why this kind of all-new-partition workload should not be ignored as a corner case. Basically, for the main use case of LCS where you need to frequently merge partitions to optimize read and eliminate tombstones and expired data sooner, LCS can be perfectly happy and efficiently perform the partition merge and tombstone elimination for a long time. However, as soon as the node becomes a bit unhealthy for various reasons (could be a bad disk so it's missing a whole bunch of mutations and need repair, could be the user chooses to ingest way more data than it usually takes and exceeds its capability, or god-forbidden, some DBA chooses to run offline sstablelevelreset), you will have to handle this kind of ""all-new-partition with a lot of SSTables in L0"" scenario, and once all L0 SSTables finally gets up-leveled to L1, you will likely see a lot of such single-SSTable compactions, which is the situation this JIRA is intended to address.

Actually, when I think more about this, to make this kind of single SSTable up-level more efficient will not only help the all-new-partition scenario, but also help in general any time when there is a big backlog of L0 SSTables due to too many flushes or excessive repair streaming with vnode. In those situations, by default STCS_in_L0 will be triggered, and you will end up getting a bunch of much bigger L0 SSTables after STCS is done. When it's time to up-level those much bigger L0 SSTables most likely they will overlap among themselves and you will add them all into your compaction session (along with all overlapped L1 SSTables). For these much bigger L0 SSTables, they have gone through a few rounds of STCS compactions, so if there's partition merge that needs to be done because fragments of the same partition are dispersed in smaller L0 SSTables earlier, after those STCS rounds, what you end up having in those much bigger L0 SSTables (generated by STCS) will not have much more opportunity for partition merge to happen, so we're in a scenario very similar to L0 data ""consists of a ton of newly inserted partitions that will never merge in compactions"" mentioned earlier."	CASSANDRA	Resolved	10002	4	6295	compaction, lcs, performance
12758861	Add option to set max_sstable_age in fractional days in DTCS	Using days as the unit for max_sstable_age in DTCS might be too much, add option to set it in seconds	CASSANDRA	Resolved	10002	1	6295	doc-impacting, dtcs
12939988	Parallel cleanup can lead to disk space exhaustion	"In CASSANDRA-5547, we made cleanup (among other things) run in parallel across multiple sstables.  There have been reports on IRC of this leading to disk space exhaustion, because multiple sstables are (almost entirely) rewritten at the same time.  This seems particularly problematic because cleanup is frequently run after a cluster is expanded due to low disk space.

I'm not really familiar with how we perform free disk space checks now, but it sounds like we can make some improvements here.  It would be good to reduce the concurrency of cleanup operations if there isn't enough free disk space to support this."	CASSANDRA	Resolved	10002	4	6295	doc-impacting
13021172	Increase error margin in SplitterTest	"SplitterTest is a randomized test - it generates random tokens and splits the ranges in equal parts. Since it is random we sometimes get very big vnodes right where we want a split and that makes the split unbalanced

Bumping the error margin a bit will avoid these false positives."	CASSANDRA	Resolved	10002	1	6295	lhf
13173740	[DTEST] fix write_failures_test.py::TestWriteFailures::test_thrift	"seems it needs a {{WITH COMPACT STORAGE}} to avoid failing like this:
{code}
write_failures_test.py::TestWriteFailures::test_thrift swapoff: Not superuser.
01:23:57,245 ccm DEBUG Log-watching thread starting.

INTERNALERROR> Traceback (most recent call last):
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 178, in wrap_session
INTERNALERROR>     session.exitstatus = doit(config, session) or 0
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 215, in _main
INTERNALERROR>     config.hook.pytest_runtestloop(session=session)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 236, in pytest_runtestloop
INTERNALERROR>     item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 81, in pytest_runtest_protocol
INTERNALERROR>     self.runner.pytest_runtest_protocol(item, nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 64, in pytest_runtest_protocol
INTERNALERROR>     runtestprotocol(item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 79, in runtestprotocol
INTERNALERROR>     reports.append(call_and_report(item, ""call"", log))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 120, in call_and_report
INTERNALERROR>     report = hook.pytest_runtest_makereport(item=item, call=call)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 196, in _multicall
INTERNALERROR>     gen.send(outcome)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/skipping.py"", line 123, in pytest_runtest_makereport
INTERNALERROR>     rep = outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 331, in pytest_runtest_makereport
INTERNALERROR>     longrepr = item.repr_failure(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 675, in repr_failure
INTERNALERROR>     return self._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 668, in _repr_failure_py
INTERNALERROR>     return super(FunctionMixin, self)._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/nodes.py"", line 295, in _repr_failure_py
INTERNALERROR>     tbfilter=tbfilter,
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 476, in getrepr
INTERNALERROR>     return fmt.repr_excinfo(self)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 717, in repr_excinfo
INTERNALERROR>     reprtraceback = self.repr_traceback(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 664, in repr_traceback
INTERNALERROR>     reprentry = self.repr_traceback_entry(entry, einfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 624, in repr_traceback_entry
INTERNALERROR>     s = self.get_source(source, line_index, excinfo, short=short)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 568, in get_source
INTERNALERROR>     lines.extend(self.get_exconly(excinfo, indent=indent, markall=True))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 575, in get_exconly
INTERNALERROR>     exlines = excinfo.exconly(tryshort=True).split(""\n"")
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 426, in exconly
INTERNALERROR>     lines = format_exception_only(self.type, self.value)
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 136, in format_exception_only
INTERNALERROR>     return list(TracebackException(etype, value, None).format_exception_only())
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 462, in __init__
INTERNALERROR>     _seen.add(exc_value)
INTERNALERROR> TypeError: unhashable type: 'InvalidRequestException'
{code}"	CASSANDRA	Resolved	10002	1	6295	dtest
13325605	Avoid marking shutting down nodes as up after receiving gossip shutdown message	"We have two recent failures for this test on trunk: 

1.) https://app.circleci.com/pipelines/github/maedhroz/cassandra/102/workflows/37ed8dab-9da4-4730-a883-20b7a99d88b4/jobs/518/tests (CASSANDRA-15909)
2.) https://app.circleci.com/pipelines/github/jolynch/cassandra/6/workflows/41e080e0-d7ff-4256-899e-b4010c6ef5ab/jobs/716/tests (CASSANDRA-15379)

The test expects there to be mismatches and then read repair executed on a following SELECT, but either those mismatches arent there, read repair isnt happening, or both."	CASSANDRA	Resolved	10002	1	6295	dtest, incremental_repair, repair
12667829	Remove leveled manifest json migration code	"We should remove the json leveled manifest migration code from 2.1

this will require users to atleast start 2.0 before upgrading to 2.1 (manifest is migrated on startup)."	CASSANDRA	Resolved	10003	4	6295	lcs
12724070	Send source sstable level when bootstrapping or replacing nodes	When replacing or bootstrapping a new node we can keep the source sstable level to avoid doing alot of compaction after bootstrap	CASSANDRA	Resolved	10002	4	6295	lcs
13161703	Add option to sanity check tombstones on reads/compaction	We should add an option to do a quick sanity check of tombstones on reads + compaction. It should either log the error or throw an exception.	CASSANDRA	Resolved	10003	4	6295	pull-request-available
12903904	Failure to start up Cassandra when temporary compaction files are not all renamed after kill/crash (FSReadError)	"We have seen an issue intermittently but repeatedly over the last few months where, after exiting the Cassandra process, it fails to start with an FSReadError (stack trace below). The FSReadError refers to a 'statistics' file for a  that doesn't exist, though a corresponding temporary file does exist (eg. there is no /media/data/cassandraDB/data/clusteradmin/singleton_token-01a92ed069b511e59b2c53679a538c14/clusteradmin-singleton_token-ka-9-Statistics.db file, but there is a /media/data/cassandraDB/data/clusteradmin/singleton_token-01a92ed069b511e59b2c53679a538c14/clusteradmin-singleton_token-tmp-ka-9-Statistics.db file.)

We tracked down the issue to the fact that the process exited with leftover compactions and some of the 'tmp' files for the SSTable had been renamed to final files, but not all of them - the issue happens if the 'Statistics' file is not renamed but others are. The scenario we've seen on the last two occurrences involves the 'CompressionInfo' file being a final file while all other files for the SSTable generation were left with 'tmp' names.

When this occurs, Cassandra cannot start until the file issue is resolved; we've worked around it by deleting the SSTable files from the same generation, both final and tmp, which at least allows Cassandra to start. Renaming all files to either tmp or final names would also work.

We've done some debugging in Cassandra and have been unable to cause the issue without renaming the files manually. The rename code at SSTableWriter.rename() looks like it could result in this if the process exits in the middle of the rename, but in every occurrence we've debugged through, the Set of components is ordered and Statistics is the first file renamed.

However the comments in SSTableWriter.rename() suggest that the 'Data' file is meant to be used as meaning the files were completely renamed. The method ColumnFamilyStore. removeUnfinishedCompactionLeftovers(), however, will proceed assuming the compaction is complete if any of the component files has a final name, and will skip temporary files when reading the list. If the 'Statistics' file is temporary then it won't be read, and the defaults does not include a list of ancestors, leading to the NullPointerException.

It appears that ColumnFamilyStore. removeUnfinishedCompactionLeftovers() should perhaps either ensure that all 'tmp' files are properly renamed before it uses them, or skip SSTable files that don't have either the 'Data' or 'Statistics' file in final form.

Stack trace: 
{code}
FSReadError in Failed to remove unfinished compaction leftovers (file: /media/data/cassandraDB/data/clusteradmin/singleton_token-01a92ed069b511e59b2c53679a538c14/clusteradmin-singleton_token-ka-9-Statistics.db).  See log for details.
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:617)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:302)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:536)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:625)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:609)
        ... 3 more
Exception encountered during startup: java.lang.NullPointerException
{code}
"	CASSANDRA	Resolved	10002	1	6295	compaction, triage
12951608	dtest failure in materialized_views_test.TestMaterializedViews.complex_mv_select_statements_test	"We've got a single flap on the 3.0 novnode job:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/189/testReport/materialized_views_test/TestMaterializedViews/complex_mv_select_statements_test

{code}

Error Message

Expected [[1, 0, 1, 0], [1, 1, 1, 0], [1, 2, 1, 0]] from SELECT a, b, c, d FROM mv, but got [[1, 0, 1, 0], [1, 1, 1, 0]]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-d0ZZ9_
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Creating keyspace
dtest: DEBUG: Testing MV primary key: ((a, b), c)
dtest: DEBUG: Testing MV primary key: ((b, a), c)
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 1193, in complex_mv_select_statements_test
    cl=ConsistencyLevel.QUORUM
  File ""/home/automaton/cassandra-dtest/assertions.py"", line 67, in assert_all
    assert list_res == expected, ""Expected %s from %s, but got %s"" % (expected, query, list_res)
""Expected [[1, 0, 1, 0], [1, 1, 1, 0], [1, 2, 1, 0]] from SELECT a, b, c, d FROM mv, but got [[1, 0, 1, 0], [1, 1, 1, 0]]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-d0ZZ9_\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Creating keyspace\ndtest: DEBUG: Testing MV primary key: ((a, b), c)\ndtest: DEBUG: Testing MV primary key: ((b, a), c)\n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
13013500	dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_2_x_To_indev_3_0_x.boolean_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/64/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_2_x_To_indev_3_0_x/boolean_test

{code}
Error Message

Problem starting node node1 due to [Errno 2] No such file or directory: '/tmp/dtest-QXmxBV/test/node1/cassandra.pid'
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 2206, in boolean_test
    for is_upgraded, cursor in self.do_upgrade(cursor):
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 153, in do_upgrade
    node1.start(wait_for_binary_proto=True, wait_other_notice=True)
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 648, in start
    self._update_pid(process)
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1780, in _update_pid
    raise NodeError('Problem starting node %s due to %s' % (self.name, e), process)
{code}"	CASSANDRA	Open	10002	4	8105	dtest, test-failure
13004628	dtest failure in repair_tests.repair_test.TestRepairDataSystemTable.repair_parent_table_test	"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/478/testReport/repair_tests.repair_test/TestRepairDataSystemTable/repair_parent_table_test

{code}
stderr: WARN  02:48:18,362 No schema agreement from live replicas after 10 s. The schema may not be up to date on some nodes.
java.lang.RuntimeException: Encountered exception creating schema
	at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpacesNative(SettingsSchema.java:101)
	at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpaces(SettingsSchema.java:69)
	at org.apache.cassandra.stress.settings.StressSettings.maybeCreateKeyspaces(StressSettings.java:228)
	at org.apache.cassandra.stress.StressAction.run(StressAction.java:59)
	at org.apache.cassandra.stress.Stress.run(Stress.java:143)
	at org.apache.cassandra.stress.Stress.main(Stress.java:62)
Caused by: com.datastax.driver.core.exceptions.InvalidQueryException: Keyspace 'keyspace1' does not exist
	at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:50)
	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)
	at org.apache.cassandra.stress.util.JavaDriverClient.execute(JavaDriverClient.java:183)
	at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpacesNative(SettingsSchema.java:86)
	... 5 more
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 320, in run
    self.setUp()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 32, in wrapped_setUp
    orig_setUp(obj, *args, **kwargs)
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 1082, in setUp
    self.node1.stress(stress_options=['write', 'n=5K', 'no-warmup', 'cl=ONE', '-schema', 'replication(factor=3)'])
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 1256, in stress
    return handle_external_tool_process(p, ['stress'] + stress_options)
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 1985, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}

There are no logs."	CASSANDRA	Resolved	10002	4	8105	dtest
13004610	dtest failure in replace_address_test.TestReplaceAddress.insert_data_during_replace_same_address_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testReport/replace_address_test/TestReplaceAddress/insert_data_during_replace_same_address_test

{code}
Error Message

13 Sep 2016 05:42:44 [replacement] Missing: ['Writes will not be forwarded to this node during replacement']:
INFO  [main] 2016-09-13 05:41:07,618 YamlConfigura.....
See system.log for remainder

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 422, in insert_data_during_replace_same_address_test
    self._test_insert_data_during_replace(same_address=True)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 217, in _test_insert_data_during_replace
    self._do_replace(same_address=same_address, extra_jvm_args=[""-Dcassandra.write_survey=true""])
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 112, in _do_replace
    timeout=60)
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 450, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
13005445	dtest failure in nose.failure.Failure.runTest	"example failure:

http://cassci.datastax.com/job/trunk_dtest-skipped-with-require/434/testReport/nose.failure/Failure/runTest

{code}
Stacktrace

Traceback (most recent call last):
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/usr/local/lib/python2.7/dist-packages/nose/loader.py"", line 418, in loadTestsFromName
    addr.filename, addr.module)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/paging_test.py"", line 18, in <module>
    from upgrade_base import UpgradeTester
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 33, in <module>
    class UpgradeTester(Tester):
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 47, in UpgradeTester
    if LooseVersion(CASSANDRA_VERSION_FROM_BUILD) < '2.2':
  File ""/usr/lib/python2.7/distutils/version.py"", line 265, in __init__
    self.parse(vstring)
  File ""/usr/lib/python2.7/distutils/version.py"", line 274, in parse
    self.component_re.split(vstring))
TypeError: expected string or buffer
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12995237	dtest failure in bootstrap_test.TestBootstrap.local_quorum_bootstrap_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/666/testReport/bootstrap_test/TestBootstrap/local_quorum_bootstrap_test"	CASSANDRA	Resolved	10002	4	8105	dtest
12986893	dtest failure in snapshot_test.TestArchiveCommitlog.dont_test_archive_commitlog	"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/409/testReport/snapshot_test/TestArchiveCommitlog/dont_test_archive_commitlog

Failed on CassCI build trunk_novnode_dtest #409

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 168, in dont_test_archive_commitlog
    self.run_archive_commitlog(restore_point_in_time=False, restore_archived_commitlog=False)
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 279, in run_archive_commitlog
    set())
  File ""/usr/lib/python2.7/unittest/case.py"", line 522, in assertNotEqual
    raise self.failureException(msg)
""set([]) == set([])
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12730632	cqlsh should automatically disable tracing when selecting from system_traces	Nobody needs to trace their traces while they're tracing.	CASSANDRA	Resolved	10003	4	8105	lhf, qa-resolved
12724501	Dtest: Windows - various cqlsh_tests errors	"Have a few windows-specific failures in this test.

{code:title=test_eat_glass}
======================================================================
ERROR: test_eat_glass (cqlsh_tests.TestCqlsh)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 158, in test_eat_glass
    """""".encode(""utf-8""))
  File ""build\bdist.win32\egg\ccmlib\node.py"", line 613, in run_cqlsh
    p.stdin.write(cmd + ';\n')
IOError: [Errno 22] Invalid argument
{code}

{code:title=test_simple_insert}
======================================================================
ERROR: test_simple_insert (cqlsh_tests.TestCqlsh)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 35, in test_simple_insert
    cursor.execute(""select id, value from simple.simple"");
  File ""c:\src\cassandra-dbapi2\cql\cursor.py"", line 80, in execute
    response = self.get_response(prepared_q, cl)
  File ""c:\src\cassandra-dbapi2\cql\thrifteries.py"", line 77, in get_response
    return self.handle_cql_execution_errors(doquery, compressed_q, compress, cl)
  File ""c:\src\cassandra-dbapi2\cql\thrifteries.py"", line 98, in handle_cql_execution_errors
    raise cql.ProgrammingError(""Bad Request: %s"" % ire.why)
ProgrammingError: Bad Request: Keyspace simple does not exist
{code}

{code:title=test_with_empty_values}
======================================================================
ERROR: test_with_empty_values (cqlsh_tests.TestCqlsh)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 347, in test_with_empty_values
    output = self.run_cqlsh(node1, ""select intcol, bigintcol, varintcol from CASSANDRA_7196.has_all_types where num in (0, 1, 2, 3, 4)"")
  File ""C:\src\cassandra-dtest\cqlsh_tests.py"", line 373, in run_cqlsh
    p = subprocess.Popen([ cli ] + args, env=env, stdin=subprocess.PIPE, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
  File ""C:\Python27\lib\subprocess.py"", line 710, in __init__
    errread, errwrite)
  File ""C:\Python27\lib\subprocess.py"", line 958, in _execute_child
    startupinfo)
WindowsError: [Error 193] %1 is not a valid Win32 application
{code}"	CASSANDRA	Resolved	10003	4	8105	Windows, qa-resolved
12950855	dtest failure in repair_tests.repair_test.TestRepair.partitioner_range_repair_test	"This new(ish) test failed on the 2.1 novnodes job. I don't know if it's a flap, or if it never would have passed; the test is relatively new, and I think this may be one of the first commits to 2.1 since it was committed. Here's the failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/216/testReport/repair_tests.repair_test/TestRepair/partitioner_range_repair_test

Failed on CassCI build cassandra-2.1_novnode_dtest #216. Assigning [~philipthompson] for initial triaging, since he wrote this test."	CASSANDRA	Resolved	10002	4	8105	dtest
12855904	Bring cqlsh into PEP8 compliance	In order for us to begin running flake8 against cqlsh.py in CI, it would be helpful if it were already PEP8 complaint, with the exception of using 120 character lines.	CASSANDRA	Resolved	10003	1	8105	cqlsh
13001682	dtest failure in repair_tests.repair_test.TestRepair.nonexistent_table_repair_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/50/testReport/repair_tests.repair_test/TestRepair/nonexistent_table_repair_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 210, in nonexistent_table_repair_test
    self.assertFalse(t.isAlive(), 'Repair thread on inexistent table is still running')
  File ""/usr/lib/python2.7/unittest/case.py"", line 416, in assertFalse
    raise self.failureException(msg)
""Repair thread on inexistent table is still running
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12995962	dtest failure in bootstrap_test.TestBootstrap.local_quorum_bootstrap_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/31/testReport/bootstrap_test/TestBootstrap/local_quorum_bootstrap_test"	CASSANDRA	Resolved	10002	4	8105	dtest
13004603	dtest failure in thrift_tests.TestMutations.test_range_tombstone_eoc_0	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testReport/thrift_tests/TestMutations/test_range_tombstone_eoc_0

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/thrift_tests.py"", line 2588, in test_range_tombstone_eoc_0
    self.assertEquals(2, len(ret))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
'2 != 0
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
13006548	dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x.conditional_update_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/9/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_0_x_To_indev_2_1_x/conditional_update_test

{code}
Error Message

<Error from server: code=2000 [Syntax error in CQL query] message=""line 1:35 no viable alternative at input 'IN'"">
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 3028, in conditional_update_test
    assert_one(cursor, ""DELETE FROM test WHERE k = 0 IF v1 IN (null)"", [True])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 128, in assert_one
    res = session.execute(simple_query)
  File ""cassandra/cluster.py"", line 1998, in cassandra.cluster.Session.execute (cassandra/cluster.c:34869)
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""cassandra/cluster.py"", line 3781, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:73073)
    raise self._final_exception
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12964481	dtest failure in upgrade_crc_check_chance_test.TestCrcCheckChanceUpgrade.crc_check_chance_upgrade_test	"single failure, unsure if test or cassandra issue:
{noformat}
ERROR [BatchlogTasks:1] 2016-05-02 20:23:08,488 CassandraDaemon.java:195 - Exception in thread Thread[BatchlogTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:122) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.utils.JVMStabilityInspector
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_45]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_45]
	... 8 common frames omitted
ERROR [BatchlogTasks:1] 2016-05-02 20:23:08,490 CassandraDaemon.java:195 - Exception in thread Thread[BatchlogTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:199) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.handleOrLog(DebuggableThreadPoolExecutor.java:244) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logExceptionsAfterExecute(DebuggableThreadPoolExecutor.java:227) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:89) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1150) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.utils.JVMStabilityInspector
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_45]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_45]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_45]
	... 7 common frames omitted
ERROR [OptionalTasks:1] 2016-05-02 20:23:08,577 CassandraDaemon.java:195 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:122) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
ERROR [OptionalTasks:1] 2016-05-02 20:23:08,588 CassandraDaemon.java:195 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NoClassDefFoundError: org/apache/cassandra/utils/JVMStabilityInspector
	at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:199) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.handleOrLog(DebuggableThreadPoolExecutor.java:244) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logExceptionsAfterExecute(DebuggableThreadPoolExecutor.java:227) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:89) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1150) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
WARN  [OptionalTasks:2] 2016-05-02 20:23:08,588 CassandraRoleManager.java:344 - CassandraRoleManager skipped default role setup: some nodes were not ready
{noformat}
http://cassci.datastax.com/job/trunk_dtest/1190/testReport/upgrade_crc_check_chance_test/TestCrcCheckChanceUpgrade/crc_check_chance_upgrade_test

Failed on CassCI build trunk_dtest #1190"	CASSANDRA	Resolved	10002	4	8105	dtest
12712444	dtests should reuse existing clusters where possible	"Many dtests don't require special setup, specifically the cql tests. We can reuse the clusters we setup across multiple tests to save time.

Suggestion: only share clusters across a single test suite, and don't share the cluster by default, turn it on per-class. If we share them more broadly than that we may get into weird states because some tests definitely do mess with the cluster setup."	CASSANDRA	Resolved	10003	4	8105	qa-resolved
12737546	Archive Commitlog Tests Failings	"Four of the snapshot_test.py:TestArchiveCommitlog tests are failing on 2.1.0 and 2.1-HEAD:
http://cassci.datastax.com/job/cassandra-2.1.0_dtest/lastCompletedBuild/testReport/snapshot_test/

The tests restore archived commit logs and check how many rows exist after restoring. They are passing on 2.0-HEAD."	CASSANDRA	Resolved	10002	4	8105	qa-resolved
13021601	dtest failure in batch_test.TestBatch.logged_batch_doesnt_throw_uae_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.X_dtest/37/testReport/batch_test/TestBatch/logged_batch_doesnt_throw_uae_test

{noformat}
Error Message

Error from server: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={'required_replicas': 3, 'alive_replicas': 2, 'consistency': 'ALL'}
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-Ysb5Cf
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
dtest: DEBUG: Creating schema...
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 193, in logged_batch_doesnt_throw_uae_test
    cl=ConsistencyLevel.ALL)
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 164, in assert_all
    res = session.execute(simple_query)
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
'Error from server: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={\'required_replicas\': 3, \'alive_replicas\': 2, \'consistency\': \'ALL\'}\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-Ysb5Cf\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.policies: INFO: Using datacenter \'datacenter1\' for DCAwareRoundRobinPolicy (via host \'127.0.0.1\'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ndtest: DEBUG: Creating schema...\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\n--------------------- >> end captured logging << ---------------------'
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest, test-failure
12995667	dtest failure in cql_tracing_test.TestCqlTracing.tracing_simple_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/381/testReport/cql_tracing_test/TestCqlTracing/tracing_simple_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 102, in tracing_simple_test
    self.trace(session)
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 74, in trace
    self.assertIn('/127.0.0.1', out)
  File ""/usr/lib/python2.7/unittest/case.py"", line 803, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
'\'/127.0.0.1\' not found in ""Consistency level set to ALL.
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12998874	dtest failure in cql_tests.SlowQueryTester.disable_slow_query_log_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/680/testReport/cql_tests/SlowQueryTester/disable_slow_query_log_test

http://cassci.datastax.com/job/cassandra-2.2_dtest/680/testReport/cql_tests/SlowQueryTester/local_query_test/

{code}
Error Message

Error starting node1.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-rDW1JQ
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cql_tests.py"", line 945, in disable_slow_query_log_test
    ""-Dcassandra.test.read_iteration_delay_ms=50""])
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 414, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node1.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-rDW1JQ\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
Standard Output

[node1 ERROR] org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [slow_query_log_timeout_in_ms] from your cassandra.yaml
	at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:146)
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:113)
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:85)
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:135)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:119)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:507)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:641)
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
13017692	dtest failure in materialized_views_test.TestMaterializedViews.populate_mv_after_insert_wide_rows_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/316/testReport/materialized_views_test/TestMaterializedViews/populate_mv_after_insert_wide_rows_test

{code}
Error Message

Expected [[0, 0]] from SELECT * FROM t_by_v WHERE id = 0 AND v = 0, but got []
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 211, in populate_mv_after_insert_wide_rows_test
    assert_one(session, ""SELECT * FROM t_by_v WHERE id = {} AND v = {}"".format(i, j), [j, i])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 130, in assert_one
    assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest, test-failure
12945545	dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1.select_distinct_with_deletions_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/155/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1/select_distinct_with_deletions_test

Failed on CassCI build cassandra-3.0_novnode_dtest #155

looks to be an assertion error, so could be test code or possibly cassandra:
{noformat}
9 != 8
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest
12941537	repair_tests.incremental_repair_test.TestIncRepair.sstable_repairedset_test failing	"recent occurence:
http://cassci.datastax.com/job/cassandra-2.1_dtest/427/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test/

last 2 runs failed:
http://cassci.datastax.com/job/cassandra-2.1_dtest/427/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_repairedset_test/history/"	CASSANDRA	Resolved	10002	4	8105	dtest
12996274	dtest failure in consistency_test.TestAvailability.test_network_topology_strategy	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/669/testReport/consistency_test/TestAvailability/test_network_topology_strategy

Failed on CassCI build cassandra-2.2_dtest build #669
{code}

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 319, in test_network_topology_strategy
    self._start_cluster()
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 96, in _start_cluster
    cluster.start(wait_for_binary_proto=True, wait_other_notice=True)
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 414, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node9.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-an_vc5\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12721750	Node enables vnodes when bounced	"According to cassandra.yaml, in the information for the num_tokens setting, ""Specifying initial_token will override this setting."" So if exactly one initial token is set, then vnodes are disabled, regardless of if or what num_tokens are set to. This behavior is inconsistent when a node is started, versus if it has been bounced.

From a fresh checkout of C*, if I build, then edit cassandra.yaml so that:

num_tokens: 256
initial_token: -9223372036854775808

then run bin/cassandra, C* will start correctly. I can run bin/nodetool ring and see that the node has exactly one token and it is what I set in initial_token. If I gracefully shutdown C*, then restart the node, running bin/nodetool ring shows that the node now has vnodes enabled and has 256 tokens.

I have been able to reproduce this locally on OSX using 2.0.8, 2.1 rc1, and trunk. I have not yet tested in Linux or Windows to see if it occurs there."	CASSANDRA	Resolved	10003	4	8105	qa-resolved
12989184	dtest failure in upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_1_x_To_indev_3_x.bootstrap_test	"example failure:

http://cassci.datastax.com/job/upgrade_tests-all/59/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_1_x_To_indev_3_x/bootstrap_test

Failed on CassCI build upgrade_tests-all #59

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 707, in bootstrap_test
    self.upgrade_scenario(after_upgrade_call=(self._bootstrap_new_node,))
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 383, in upgrade_scenario
    call()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 688, in _bootstrap_new_node
    nnode.start(use_jna=True, wait_other_notice=True, wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 634, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 481, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""13 Jul 2016 02:23:05 [node2] Missing: ['127.0.0.4.* now UP']:\nINFO  [HANDSHAKE-/127.0.0.4] 2016-07-13 02:21:00,2.....\nSee system.log for remainder
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12948874	dtest failure in consistency_test.TestConsistency.quorum_available_during_failure_test	"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/68/testReport/consistency_test/TestConsistency/quorum_available_during_failure_test

Failed on CassCI build trunk_offheap_dtest #68

This seems to be failing after merging this CCM PR:

https://github.com/pcmanus/ccm/pull/461

I'm not sure why it would fail with that error-checking code but not without it. On this test run, it ran after these two tests:

{code}
test_simple_strategy_each_quorum (consistency_test.TestAvailability) ... ok
test_simple_strategy_each_quorum_counters (consistency_test.TestAccuracy) ... ok
{code}

so, maybe processes are hanging around after one of those tests. [~philipthompson] can you have a first look here?"	CASSANDRA	Resolved	10002	4	8105	dtest
12707271	auth_test system_auth_ks_is_alterable_test dtest hangs in 2.1 and 2.0	"This test hangs forever. When I hit ctl-c after running the test, then the ccm nodes actually continue running - I think ccm is looking for log lines that never occur until the test is killed(?).
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; ENABLE_VNODES=true PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 auth_test.py:TestAuth.system_auth_ks_is_alterable_test
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
system_auth_ks_is_alterable_test (auth_test.TestAuth) ... cluster ccm directory: /tmp/dtest-O3AAJr
^C
{noformat}
Search for (hanging here) below - I typed this prior to hitting ctl-c. Then the nodes start running again and I see ""Listening for thrift clients"" later on.
{noformat}
mshuler@hana:~$ tail -f /tmp/dtest-O3AAJr/test/node*/logs/system.log
==> /tmp/dtest-O3AAJr/test/node1/logs/system.log <==
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,599 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1792243696(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:2] 2014-04-08 16:45:12,603 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node1/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  14,454 bytes to 11,603 (~80% of original) in 105ms = 0.105386MB/s.  7 total partitions merged to 3.  Partition merge counts were {1:1, 2:1, 4:1, }
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,668 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504671, position=193292)
INFO  [MigrationStage:1] 2014-04-08 16:45:12,669 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6806 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,670 Memtable.java:344 - Writing Memtable-schema_columns@352928691(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:1] 2014-04-08 16:45:12,672 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node1/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  710 bytes to 233 (~32% of original) in 70ms = 0.003174MB/s.  6 total partitions merged to 3.  Partition merge counts were {1:2, 4:1, }
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,721 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504671, position=193830)
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,566 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.
INFO  [NonPeriodicTasks:1] 2014-04-08 16:45:20,570 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'
INFO  [NonPeriodicTasks:1] 2014-04-08 16:45:21,806 Auth.java:232 - Created default superuser 'cassandra'

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,214 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1004 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,215 Memtable.java:344 - Writing Memtable-schema_keyspaces@781373873(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,295 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,296 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 34190 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,297 Memtable.java:344 - Writing Memtable-schema_columnfamilies@2077216447(5746 serialized bytes, 108 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,369 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db (2088 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,370 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 37408 (0%) on-heap, 0 (0%) off-heap
INFO  [CompactionExecutor:4] 2014-04-08 16:45:12,371 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db')]
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,371 Memtable.java:344 - Writing Memtable-schema_columns@2003573271(5173 serialized bytes, 119 ops, 0%/0% of on/off-heap limit)
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,248 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,654 Memtable.java:344 - Writing Memtable-schema_keyspaces@789549279(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,788 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=193958)
INFO  [InternalResponseStage:1] 2014-04-08 16:45:11,789 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 34192 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:11,790 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1695849916(5746 serialized bytes, 108 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:11,898 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db (2087 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=194091)
INFO  [InternalResponseStage:1] 2014-04-08 16:45:11,899 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 37410 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,900 Memtable.java:344 - Writing Memtable-schema_columns@2090391222(5173 serialized bytes, 119 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,998 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db (1700 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=194091)
INFO  [main] 2014-04-08 16:45:18,654 CassandraDaemon.java:533 - No gossip backlog; proceeding
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,131 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.

(hanging here)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [NonPeriodicTasks:1] 2014-04-08 16:49:02,863 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [NonPeriodicTasks:1] 2014-04-08 16:49:02,888 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:02,919 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db (1699 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:02,922 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db')]

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [InternalResponseStage:3] 2014-04-08 16:49:02,959 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1006 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:02,960 Memtable.java:344 - Writing Memtable-schema_keyspaces@44265998(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:02,970 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 501 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:02,972 Memtable.java:344 - Writing Memtable-schema_keyspaces@603519674(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [main] 2014-04-08 16:49:03,029 Server.java:159 - Starting listening for CQL clients on /127.0.0.3:9042...

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [CompactionExecutor:4] 2014-04-08 16:49:03,064 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-13,].  13,907 bytes to 7,643 (~54% of original) in 213ms = 0.034220MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,069 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [InternalResponseStage:3] 2014-04-08 16:49:03,071 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 42850 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,071 Memtable.java:344 - Writing Memtable-schema_columnfamilies@666219518(7373 serialized bytes, 135 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,087 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=247475)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,090 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  16,411 bytes to 11,314 (~68% of original) in 165ms = 0.065393MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }
INFO  [CompactionExecutor:6] 2014-04-08 16:49:03,090 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-13-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db')]
INFO  [MigrationStage:1] 2014-04-08 16:49:03,091 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8799 (0%) on-heap, 0 (0%) off-heap

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [main] 2014-04-08 16:49:03,091 ThriftServer.java:119 - Binding thrift service to /127.0.0.3:9160

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,091 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1821762369(1609 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [Thread-8] 2014-04-08 16:49:03,098 ThriftServer.java:136 - Listening for thrift clients...
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,166 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db (2536 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [InternalResponseStage:3] 2014-04-08 16:49:03,167 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 44033 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,168 Memtable.java:344 - Writing Memtable-schema_columns@1052245443(6187 serialized bytes, 140 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:8] 2014-04-08 16:49:03,168 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-9-Data.db')]

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,203 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (949 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=248454)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,204 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6833 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,205 Memtable.java:344 - Writing Memtable-schema_columns@1698728310(1041 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:6] 2014-04-08 16:49:03,218 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  750 bytes to 233 (~31% of original) in 125ms = 0.001778MB/s.  8 total partitions merged to 3.  Partition merge counts were {1:1, 3:1, 4:1, }

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,278 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db (1990 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,279 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db')]

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,310 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (431 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=248454)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [CompactionExecutor:8] 2014-04-08 16:49:03,313 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-13,].  14,821 bytes to 8,115 (~54% of original) in 140ms = 0.055279MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }
INFO  [MigrationStage:1] 2014-04-08 16:49:03,313 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 502 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,315 Memtable.java:344 - Writing Memtable-schema_keyspaces@901379409(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,332 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 501 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,333 Memtable.java:344 - Writing Memtable-schema_keyspaces@1181313624(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,392 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=217732)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,397 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8818 (0%) on-heap, 0 (0%) off-heap
INFO  [CompactionExecutor:7] 2014-04-08 16:49:03,397 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-13-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db')]
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,400 Memtable.java:344 - Writing Memtable-schema_columnfamilies@2077765501(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,405 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  16,987 bytes to 11,600 (~68% of original) in 123ms = 0.089940MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,418 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-18-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252763)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,419 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8817 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,420 Memtable.java:344 - Writing Memtable-schema_columnfamilies@495883972(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,483 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=217732)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,484 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6807 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,485 Memtable.java:344 - Writing Memtable-schema_columns@365627698(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,519 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-15-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252896)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,520 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6806 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,521 Memtable.java:344 - Writing Memtable-schema_columns@1830789468(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [CompactionExecutor:7] 2014-04-08 16:49:03,523 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  750 bytes to 233 (~31% of original) in 122ms = 0.001821MB/s.  8 total partitions merged to 3.  Partition merge counts were {1:1, 3:1, 4:1, }
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,562 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=218351)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,581 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-15-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252896)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,586 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 502 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,586 Memtable.java:344 - Writing Memtable-schema_keyspaces@543512535(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,604 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@1327e4ff[cfId=2d324e48-3275-3517-8dd5-9a2c5b0856c5,ksName=system_auth,cfName=permissions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(7065726d697373696f6e73:org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type))),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,gcGraceSeconds=7776000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UTF8Type,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=permissions, type=org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=username, type=org.apache.cassandra.db.marshal.UTF8Type, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=resource, type=org.apache.cassandra.db.marshal.UTF8Type, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=0,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 16:49:03,610 ColumnFamilyStore.java:283 - Initializing system_auth.permissions
INFO  [main] 2014-04-08 16:49:03,622 CassandraDaemon.java:501 - Waiting for gossip to settle before accepting client requests...
INFO  [InternalResponseStage:5] 2014-04-08 16:49:03,625 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1004 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,626 Memtable.java:344 - Writing Memtable-schema_keyspaces@2064761560(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,664 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-18-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=222660)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,665 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8818 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,666 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1464552123(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
<...>
{noformat}"	CASSANDRA	Resolved	10002	4	8105	qa-resolved
12992303	dtest failure in secondary_indexes_test.TestSecondaryIndexes.test_query_indexes_with_vnodes	"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/347/testReport/secondary_indexes_test/TestSecondaryIndexes/test_query_indexes_with_vnodes

{code}
Standard Output

Unexpected error in node2 log, error: 
ERROR [ReadStage-1] 2016-07-20 04:58:27,391 MessageDeliveryTask.java:74 - The secondary index 'composites_index' is not yet available
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12997262	dtest failure in compaction_test.TestCompaction_with_SizeTieredCompactionStrategy.bloomfilter_size_test	"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/lastCompletedBuild/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/bloomfilter_size_test"	CASSANDRA	Resolved	10002	4	8105	dtest
12960056	dtest failure in user_types_test.TestUserTypes.test_nested_user_types	"This is a single flap:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/217/testReport/user_types_test/TestUserTypes/test_nested_user_types

Failed on CassCI build cassandra-2.2_dtest_win32 #217

{code}
Error Message

Lists differ: [None] != [[u'test', u'test2']]

First differing element 0:
None
[u'test', u'test2']

- [None]
+ [[u'test', u'test2']]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: d:\temp\dtest-vgkgwi
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""C:\tools\python2\lib\unittest\case.py"", line 329, in run
    testMethod()
  File ""D:\jenkins\workspace\cassandra-2.2_dtest_win32\cassandra-dtest\user_types_test.py"", line 289, in test_nested_user_types
    self.assertEqual(listify(primary_item), [[u'test', u'test2']])
  File ""C:\tools\python2\lib\unittest\case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""C:\tools\python2\lib\unittest\case.py"", line 742, in assertListEqual
    self.assertSequenceEqual(list1, list2, msg, seq_type=list)
  File ""C:\tools\python2\lib\unittest\case.py"", line 724, in assertSequenceEqual
    self.fail(msg)
  File ""C:\tools\python2\lib\unittest\case.py"", line 410, in fail
    raise self.failureException(msg)
""Lists differ: [None] != [[u'test', u'test2']]\n\nFirst differing element 0:\nNone\n[u'test', u'test2']\n\n- [None]\n+ [[u'test', u'test2']]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: d:\\temp\\dtest-vgkgwi\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
Standard Error

Started: node1 with pid: 4328
Started: node3 with pid: 7568
Started: node2 with pid: 7504
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest, windows
12944327	dtest failure in bootstrap_test.TestBootstrap.simple_bootstrap_test_nodata	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/526/testReport/bootstrap_test/TestBootstrap/simple_bootstrap_test_nodata

Failed on CassCI build cassandra-2.2_dtest #526"	CASSANDRA	Resolved	10002	4	8105	dtest
12780574	COPY command has inherent 128KB field size limit	"In using the COPY command as follows:
{{cqlsh -e ""COPY test.test1mb(pkey, ccol, data) FROM 'in/data1MB/data1MB_9.csv'""}}
the following error is thrown:
{{<stdin>:1:field larger than field limit (131072)}}

The data file contains a field that is greater than 128KB (it's more like almost 1MB).

A work-around (thanks to [~jjordan] and [~thobbs] is to modify the cqlsh script and add the line
{{csv.field_size_limit(1000000000)}}
anywhere after the line
{{import csv}}"	CASSANDRA	Resolved	10002	1	8105	cqlsh
12965767	dtest failure in pushed_notifications_test.TestPushedNotifications.move_single_node_test	"one recent failure (no vnode job)

{noformat}
'MOVED_NODE' != u'NEW_NODE'
{noformat}

http://cassci.datastax.com/job/trunk_novnode_dtest/366/testReport/pushed_notifications_test/TestPushedNotifications/move_single_node_test

Failed on CassCI build trunk_novnode_dtest #366"	CASSANDRA	Resolved	10002	4	8105	dtest
12944001	dtest failure in replication_test.SnitchConfigurationUpdateTest.test_failed_snitch_update_property_file_snitch	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/301/testReport/replication_test/SnitchConfigurationUpdateTest/test_failed_snitch_update_property_file_snitch

Failed on CassCI build cassandra-2.1_offheap_dtest #301
Failed on CassCI build cassandra-2.1_offheap_dtest #286

Looks to be a timeout issue on both historical failures:
{noformat}
Ran out of time waiting for topology to change on node 0
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest
13008285	dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test_not_intersecting_all_ranges	"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/406/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test_not_intersecting_all_ranges

{code}
Error Message

Subprocess sstablemetadata on keyspace: keyspace1, column_family: None exited with non-zero status; exit status: 1; 
stdout: 
usage: Usage: sstablemetadata [--gc_grace_seconds n] <sstable filenames>
Dump contents of given SSTable to standard output in JSON format.
    --gc_grace_seconds <arg>   The gc_grace_seconds to use when
                               calculating droppable tombstones
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 366, in sstable_marking_test_not_intersecting_all_ranges
    for out in (node.run_sstablemetadata(keyspace='keyspace1').stdout for node in cluster.nodelist()):
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 366, in <genexpr>
    for out in (node.run_sstablemetadata(keyspace='keyspace1').stdout for node in cluster.nodelist()):
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1021, in run_sstablemetadata
    return handle_external_tool_process(p, ""sstablemetadata on keyspace: {}, column_family: {}"".format(keyspace, column_families))
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 1983, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12753195	parse_for_table_meta errors out on queries with undefined grammars	CASSANDRA-6910 introduced changes to the cqlsh function {{parse_for_table_meta}} that cause an error to be thrown whenever a query does not have its grammar defined in cql3handling.py. However, this is affecting queries that are legitimate cql syntax and are accepted by Cassandra, but aren't defined in cqlsh. 	CASSANDRA	Resolved	10002	1	8105	cqlsh
12683140	nodetool getendpoints doesn't validate key arity	"I have a complex row key.

$ create table b (x int, s text, ((x,s)) primary key);

In cqlsh I cannot fill row key partially:

{noformat}
$ insert into b (x) values(4);
Bad Request: Missing mandatory PRIMARY KEY part s
{noformat}

But nodetool can find hosts by incomplete key
{noformat}
$ nodetool -h cas3 getendpoints anti_portal b 12
192.168.4.4
192.168.4.5
192.168.4.6
{noformat}

No error is reported.

I found that columns are separated by "":"".
And If I pass to many elements then the error happens.

{noformat}
$ nodetool -h cas3 getendpoints anit_portal b 12:dd:dd
Exception in thread ""main"" org.apache.cassandra.serializers.MarshalException: unable to make int from '12:dd:dd'
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:69)
    at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2495)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
    at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
    at sun.rmi.transport.Transport$1.run(Transport.java:177)
    at sun.rmi.transport.Transport$1.run(Transport.java:174)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NumberFormatException: For input string: ""12:dd:dd""
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Integer.parseInt(Integer.java:492)
    at java.lang.Integer.parseInt(Integer.java:527)
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:65)
    ... 36 more
{noformat}

I think showing huge stack trace is not proper behavior.
Error message should be printer if arity of passed key and table key are not equal."	CASSANDRA	Resolved	10003	1	8105	lhf
12997598	dtest failure in pending_range_test.TestPendingRangeMovements.pending_range_test	"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/456/testReport/pending_range_test/TestPendingRangeMovements/pending_range_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/pending_range_test.py"", line 63, in pending_range_test
    self.assertRegexpMatches(out, '127\.0\.0\.1.*?Down.*?Moving')
  File ""/usr/lib/python2.7/unittest/case.py"", line 1002, in assertRegexpMatches
    raise self.failureException(msg)
""Regexp didn't match: '127\\\\.0\\\\.0\\\\.1.*?Down.*?Moving' not found in '\\nDatacenter: datacenter1\\n==========\\nAddress    Rack        Status State   Load            Owns                Token                                       \\n                                                                          5534023222112865484                         \\n127.0.0.2  rack1       Up     Normal  200.01 KiB      73.44%              -5534023222112865485                        \\n127.0.0.3  rack1       Up     Normal  162.37 KiB      80.00%              -1844674407370955162                        \\n127.0.0.1  rack1       Down   Normal  147.05 KiB      66.56%              -634023222112864484                         \\n127.0.0.4  rack1       Up     Normal  164.85 KiB      40.00%              1844674407370955161                         \\n127.0.0.5  rack1       Up     Normal  164.99 KiB      40.00%              5534023222112865484  
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12982042	dtest failure in paging_test.TestPagingData.static_columns_paging_test	"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/261/testReport/paging_test/TestPagingData/static_columns_paging_test

Failed on CassCI build trunk_offheap_dtest #261

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 715, in static_columns_paging_test
    self.assertEqual(16, len(results))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""16 != 6
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12996610	dtest failure in repair_tests.repair_test.TestRepair.test_multiple_concurrent_repairs	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/318/testReport/repair_tests.repair_test/TestRepair/test_multiple_concurrent_repairs

{code}
Error Message

ERROR 15:43:07 Error creating pool to /127.0.0.3:9042
com.datastax.driver.core.TransportException: [/127.0.0.3:9042] Cannot connect
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
Caused by: java.net.ConnectException: Connection refused: /127.0.0.3:9042
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]
	at com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	... 6 common frames omitted
ERROR 15:43:07 Error creating pool to /127.0.0.1:9042
com.datastax.driver.core.TransportException: [/127.0.0.1:9042] Cannot connect
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
Caused by: java.net.ConnectException: Connection refused: /127.0.0.1:9042
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]
	at com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	at com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]
	... 6 common frames omitted
Failed to connect over JMX; not collecting these stats

-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-b0pWB9
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------

{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/repair_test.py"", line 886, in test_multiple_concurrent_repairs
    self.assertTrue(len(stderr) == 0, stderr)
  File ""/usr/lib/python2.7/unittest/case.py"", line 422, in assertTrue
    raise self.failureException(msg)
""ERROR 15:43:07 Error creating pool to /127.0.0.3:9042\ncom.datastax.driver.core.TransportException: [/127.0.0.3:9042] Cannot connect\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]\nCaused by: java.net.ConnectException: Connection refused: /127.0.0.3:9042\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]\n\tat com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\t... 6 common frames omitted\nERROR 15:43:07 Error creating pool to /127.0.0.1:9042\ncom.datastax.driver.core.TransportException: [/127.0.0.1:9042] Cannot connect\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:156) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.driver.core.Connection$1.operationComplete(Connection.java:139) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]\nCaused by: java.net.ConnectException: Connection refused: /127.0.0.1:9042\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.7.0_80]\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744) ~[na:1.7.0_80]\n\tat com.datastax.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) ~[cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\tat com.datastax.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281) [cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:na]\n\t... 6 common frames omitted\nFailed to connect over JMX; not collecting these stats\n\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-b0pWB9\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""

{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12951333	dtest failure in topology_test.TestTopology.crash_during_decommission_test	"This has flapped once on 3.0:

http://cassci.datastax.com/job/cassandra-3.0_dtest/614/testReport/topology_test/TestTopology/crash_during_decommission_test

and more frequently on trunk:

http://cassci.datastax.com/job/trunk_dtest/1057/testReport/junit/topology_test/TestTopology/crash_during_decommission_test/

Message and trace:

{code}
Error Message

15 Mar 2016 10:48:55 [node1] Missing: ['127.0.0.2.* now UP']:
.....
See system.log for remainder
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-EJfTo3
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Status as reported by node 127.0.0.2
dtest: DEBUG: Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  112.85 KB  32           64.1%             d7d836bd-a0ff-474c-b04c-2c00bd2fc636  rack1
UN  127.0.0.2  127.25 KB  32           61.7%             68c5a8c4-8986-40df-afb2-ee0849814618  rack1
UN  127.0.0.3  112.68 KB  32           74.2%             6dd52315-53f4-444d-bace-a8a0d7c2b34e  rack1


dtest: DEBUG: Restarting node2
dtest: DEBUG: 
dtest: DEBUG: 
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 253, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/topology_test.py"", line 230, in crash_during_decommission_test
    node2.start(wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 597, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 417, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""15 Mar 2016 10:48:55 [node1] Missing: ['127.0.0.2.* now UP']:\n.....\nSee system.log for remainder\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-EJfTo3\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Status as reported by node 127.0.0.2\ndtest: DEBUG: Datacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  127.0.0.1  112.85 KB  32           64.1%             d7d836bd-a0ff-474c-b04c-2c00bd2fc636  rack1\nUN  127.0.0.2  127.25 KB  32           61.7%             68c5a8c4-8986-40df-afb2-ee0849814618  rack1\nUN  127.0.0.3  112.68 KB  32           74.2%             6dd52315-53f4-444d-bace-a8a0d7c2b34e  rack1\n\n\ndtest: DEBUG: Restarting node2\ndtest: DEBUG: \ndtest: DEBUG: \n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12958505	dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test_not_intersecting_all_ranges	"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/344/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test_not_intersecting_all_ranges

Failed on CassCI build trunk_novnode_dtest #344

Test does not appear to deal with single-token cluster testing correctly:
{noformat}
Error Message

Error starting node1.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-I164Fa
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None, 'phi_convict_threshold': 5, 'start_rpc': 'true'}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 369, in sstable_marking_test_not_intersecting_all_ranges
    cluster.populate(4, use_vnodes=True).start()
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 360, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node1.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-I164Fa\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None, 'phi_convict_threshold': 5, 'start_rpc': 'true'}\n--------------------- >> end captured logging << ---------------------""
Standard Output

[node1 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node3 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node2 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node4 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest
12953635	dtest failure in consistency_test.TestAccuracy.test_network_topology_strategy_users	"This test and consistency_test.TestAvailability.test_network_topology_strategy have begun failing now that we dropped the instance size we run CI with. The tests should be altered to reflect the constrained resources. They are ambitious for dtests, regardless.

example failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/221/testReport/consistency_test/TestAccuracy/test_network_topology_strategy_users

Failed on CassCI build cassandra-2.1_novnode_dtest #221"	CASSANDRA	Resolved	10002	4	8105	dtest
12945014	dtest failure in materialized_views_test.TestMaterializedViews.view_tombstone_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/564/testReport/materialized_views_test/TestMaterializedViews/view_tombstone_test

Failed on CassCI build cassandra-3.0_dtest #564

intermittent failure, error:
{noformat}
Expected [[1, 1, 'b', 3.0]] from SELECT * FROM t_by_v WHERE v = 1, but got []
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest
12941589	replication_test.ReplicationTest.network_topology_test flaps	"Test intermittently failing with set comparison errors that differ from one failure to the next. Looks a bit more stable recently since #203 failed, but probably worth keeping an eye on, and check if there's a problem with the test code.

most recent failure:
http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/203/testReport/replication_test/ReplicationTest/network_topology_test/"	CASSANDRA	Resolved	10002	4	8105	dtest
12713401	Update ccm for windows launch script changes from #7001	The new .bat launch script changes will require changes to ccm and dtests so we can get windows dtests running on cassci.	CASSANDRA	Resolved	10002	4	8105	Windows, qa-resolved
12998216	dtest failure in auth_test.TestAuth.conditional_create_drop_user_test	"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/458/testReport/auth_test/TestAuth/conditional_create_drop_user_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/auth_test.py"", line 348, in conditional_create_drop_user_test
    self.prepare()
  File ""/home/automaton/cassandra-dtest/auth_test.py"", line 978, in prepare
    n = self.wait_for_any_log(self.cluster.nodelist(), 'Created default superuser', 25)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 760, in wait_for_any_log
    found = node.grep_log(pattern, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 347, in grep_log
    with open(os.path.join(self.get_path(), 'logs', filename)) as f:
""[Errno 2] No such file or directory: '/tmp/dtest-XmnSYI/test/node1/logs/system.log'
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
13001679	dtest failure in materialized_views_test.TestMaterializedViews.add_dc_after_mv_simple_replication_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testReport/materialized_views_test/TestMaterializedViews/add_dc_after_mv_simple_replication_test/

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 389, in add_dc_after_mv_simple_replication_test
    self._add_dc_after_mv_test(1)
  File ""/home/automaton/cassandra-dtest/materialized_views_test.py"", line 363, in _add_dc_after_mv_test
    node5.start(jvm_args=[""-Dcassandra.migration_task_wait_in_seconds={}"".format(MIGRATION_WAIT)])
  File ""/home/automaton/ccm/ccmlib/node.py"", line 636, in start
    self._update_pid(process)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 1770, in _update_pid
    raise NodeError('Problem starting node %s due to %s' % (self.name, e), process)
""Problem starting node node5 due to [Errno 2] No such file or directory: '/tmp/dtest-S4bmF0/test/node5/cassandra.pid'
{code}

Related failure:
http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testReport/materialized_views_test/TestMaterializedViews/add_dc_after_mv_network_replication_test/"	CASSANDRA	Resolved	10002	4	8105	dtest
12998217	dtest failure in topology_test.TestTopology.crash_during_decommission_test	"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/376/testReport/topology_test/TestTopology/crash_during_decommission_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 673, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
""Unexpected error in log, see stdout
{code}

{code}
Standard Output

Unexpected error in node1 log, error: 
ERROR [RMI TCP Connection(2)-127.0.0.1] 2016-08-18 02:15:31,444 StorageService.java:3719 - Error while decommissioning node 
org.apache.cassandra.streaming.StreamException: Stream failed
	at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:215) ~[main/:na]
	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:191) ~[main/:na]
	at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:448) ~[main/:na]
	at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:551) ~[main/:na]
	at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:249) ~[main/:na]
	at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:263) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12970865	dtest failure in topology_test.TestTopology.simple_decommission_test	"example failure:

http://cassci.datastax.com/job/trunk_dtest/1223/testReport/topology_test/TestTopology/simple_decommission_test

Failed on CassCI build trunk_dtest #1223

The problem is that node3 detected node2 as down before the stop call was made, so the wait_other_notice check fails. The fix here is almost certainly as simple as just changing that line to {{node2.stop()}}"	CASSANDRA	Resolved	10002	4	8105	dtest
12948418	dtest failure in compaction_test.TestCompaction_with_LeveledCompactionStrategy.data_size_test	"This dtest has failed once:

http://cassci.datastax.com/job/cassandra-3.0_dtest/597/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/data_size_test

Here's the history for this test:

http://cassci.datastax.com/job/cassandra-3.0_dtest/597/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/data_size_test/history/

It failed at this line:

https://github.com/riptano/cassandra-dtest/blob/88a74d7/compaction_test.py#L86

Basically, it ran compaction over the default stress tables, but timed out waiting to see the line {{Compacted }} in the log."	CASSANDRA	Resolved	10002	4	8105	dtest
13004611	dtest failure in upgrade_tests.storage_engine_upgrade_test.TestBootstrapAfterUpgrade.upgrade_with_range_tombstone_eoc_0_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/31/testReport/upgrade_tests.storage_engine_upgrade_test/TestBootstrapAfterUpgrade/upgrade_with_range_tombstone_eoc_0_test

{code}
Error Message

Expected [] to have length 2, but instead is of length 0

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/storage_engine_upgrade_test.py"", line 421, in upgrade_with_range_tombstone_eoc_0_test
    assert_length_equal(ret, 2)
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 249, in assert_length_equal
    expected_length, len(object_with_length)))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
{code}

Related failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/31/testReport/upgrade_tests.storage_engine_upgrade_test/TestStorageEngineUpgrade/upgrade_with_range_tombstone_eoc_0_test/"	CASSANDRA	Resolved	10002	4	8105	dtest
13004607	dtest failure in replace_address_test.TestReplaceAddress.insert_data_during_replace_different_address_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testReport/replace_address_test/TestReplaceAddress/insert_data_during_replace_different_address_test

{code}
Error Message

Subprocess ['nodetool', '-h', 'localhost', '-p', '7400', ['join']] exited with non-zero status; exit status: 1; 
stdout: nodetool: This node has already joined the ring.

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 430, in insert_data_during_replace_different_address_test
    self._test_insert_data_during_replace(same_address=False)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 228, in _test_insert_data_during_replace
    self.replacement_node.nodetool(""join"")
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 756, in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])
  File ""/home/automaton/src/ccm/ccmlib/node.py"", line 1985, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12724679	COPY FROM doesn't accept diacritical characters	The current version of the COPY command chokes on diacritical characters such as    and . It would be great if the COPY command would support these characters.	CASSANDRA	Resolved	10003	4	8105	cqlsh
12958509	dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_non_prepared_statements	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/329/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_non_prepared_statements

Failed on CassCI build cassandra-2.1_offheap_dtest #329

{noformat}
Error Message

'int' object has no attribute 'on_read_timeout'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-LNfFyy
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Running stress without any user profile
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2325, in test_bulk_round_trip_non_prepared_statements
    copy_from_options={'PREPAREDSTATEMENTS': False})
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2283, in _test_bulk_round_trip
    num_records = create_records()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2258, in create_records
    ret = rows_to_list(self.session.execute(count_statement))[0][0]
  File ""cassandra/cluster.py"", line 1581, in cassandra.cluster.Session.execute (cassandra/cluster.c:27046)
    return self.execute_async(query, parameters, trace, custom_payload, timeout).result()
  File ""cassandra/cluster.py"", line 3145, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:59905)
    raise self._final_exception
""'int' object has no attribute 'on_read_timeout'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-LNfFyy\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Running stress without any user profile\n--------------------- >> end captured logging << ---------------------""
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest
12902784	Fix pep8 compliance in cqlsh.py	"cqlsh has fallen out of pep8 compliance:

http://cassci.datastax.com/view/trunk/job/cassandra-3.0_dtest/lastCompletedBuild/testReport/cqlsh_tests.cqlsh_tests/TestCqlsh/test_pep8_compliance/

[~philipthompson] has said off-JIRA that he'll fix it."	CASSANDRA	Resolved	10002	7	8105	cqlsh
12988240	dtest failure in thrift_tests.TestMutations.test_describe_keyspace	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/492/testReport/thrift_tests/TestMutations/test_describe_keyspace

Failed on CassCI build cassandra-2.1_dtest #492

{code}
Stacktrace

Traceback (most recent call last):
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/thrift_tests.py"", line 1507, in test_describe_keyspace
    assert len(kspaces) == 4, [x.name for x in kspaces]  # ['Keyspace2', 'Keyspace1', 'system', 'system_traces']
AssertionError: ['Keyspace2', 'system', 'Keyspace1', 'ValidKsForUpdate', 'system_traces']
{code}

Related failures:
http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/304/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/cassandra-3.0_dtest/767/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/264/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/trunk_dtest/1301/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/trunk_novnode_dtest/421/testReport/thrift_tests/TestMutations/test_describe_keyspace/

http://cassci.datastax.com/job/cassandra-3.9_dtest/6/testReport/thrift_tests/TestMutations/test_describe_keyspace/"	CASSANDRA	Resolved	10002	4	8105	dtest
13005923	dtest failure in json_tools_test.TestJson.json_tools_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/508/testReport/json_tools_test/TestJson/json_tools_test/

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/json_tools_test.py"", line 23, in json_tools_test
    debug(""Version: "" + cluster.version())
""cannot concatenate 'str' and 'instance' objects
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12732683	dtest cql_tests.py:TestCQL.cql3_insert_thrift_test fails intermittently	"This test fails about 20-25% of the time - ran about 10 times through looping the test, and it typically fails on the 4th or 5th test.
{noformat}
(master)mshuler@hana:~/git/cassandra-dtest$ ../loop_dtest.sh ""cql_tests.py:TestCQL.cql3_insert_thrift_test""
<...>

==== Run #4 ====
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
cql3_insert_thrift_test (cql_tests.TestCQL) ... cluster ccm directory: /tmp/dtest-Drwunj
[node1 ERROR] 
[node1 ERROR] 
FAIL
removing ccm cluster test at: /tmp/dtest-Drwunj

======================================================================
FAIL: cql3_insert_thrift_test (cql_tests.TestCQL)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/cql_tests.py"", line 1627, in cql3_insert_thrift_test
    assert res == [ [2, 4, 200] ], res
AssertionError: []

----------------------------------------------------------------------
Ran 1 test in 7.192s
{noformat}

loop_dtest.sh:
{noformat}
#!/bin/bash
if [ ${1} ]; then
    export MAX_HEAP_SIZE=""1G""
    export HEAP_NEWSIZE=""256M""
    export PRINT_DEBUG=true
    COUNT=0
    while true; do
        echo
        echo ""==== Run #$COUNT ====""
        nosetests --nocapture --nologcapture --verbosity=3 ${1}
        if [ $? -ne 0 ]; then
            exit 1
        fi
        ((COUNT++))
        sleep 0.5
    done
    unset MAX_HEAP_SIZE HEAP_NEWSIZE PRINT_DEBUG
else
    echo ""  ${0} needs a test to run..""
    exit 255
fi
{noformat}

I find no ERROR/WARN log entries from the failed test - attached node log anyway."	CASSANDRA	Resolved	10003	4	8105	qa-resolved
12985805	dtest failure in compaction_test.TestCompaction_with_DateTieredCompactionStrategy.large_compaction_warning_test	"example failure:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_DateTieredCompactionStrategy/large_compaction_warning_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/compaction_test.py"", line 330, in large_compaction_warning_test
    node.watch_log_for('{} large partition ks/large:user \({}\)'.format(verb, sizematcher), from_mark=mark, timeout=180)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 448, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""28 Jun 2016 15:16:51 [node1] Missing: ['Writing large partition ks/large:user \\\\(\\\\d+ bytes\\\\)']:\nINFO  [Native-Transport-Requests-5] 2016-06-28 15:.....\nSee system.log for remainder
{code}

Related failures:

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_LeveledCompactionStrategy/large_compaction_warning_test/

http://cassci.datastax.com/job/trunk_dtest/1290/testReport/compaction_test/TestCompaction_with_SizeTieredCompactionStrategy/large_compaction_warning_test/

Failed on CassCI build trunk_dtest #1290"	CASSANDRA	Resolved	10002	4	8105	dtest
12921817	pushed_notifications_test.py:TestPushedNotifications.restart_node_test flapping on C* 2.1	"This test flaps on CassCI on 2.1. [~aboudreault] Do I remember correctly that you did some work on these tests in the past few months? If so, could you have a look and see if there's some assumption the test makes that don't hold for 2.1?

Oddly, it fails frequently under JDK8:

http://cassci.datastax.com/job/cassandra-2.1_dtest_jdk8/lastCompletedBuild/testReport/pushed_notifications_test/TestPushedNotifications/restart_node_test/history/

but less frequently on JDK7:

http://cassci.datastax.com/job/cassandra-2.1_dtest/lastCompletedBuild/testReport/pushed_notifications_test/TestPushedNotifications/restart_node_test/history/
"	CASSANDRA	Resolved	10002	7	8105	dtest
13001685	dtest failure in cql_tracing_test.TestCqlTracing.tracing_default_impl_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/50/testReport/cql_tracing_test/TestCqlTracing/tracing_default_impl_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cql_tracing_test.py"", line 163, in tracing_default_impl_test
    errs[0][1])
'list index out of range
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12944664	(windows) dtest failure in replace_address_test.TestReplaceAddress.replace_with_reset_resume_state_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/165/testReport/replace_address_test/TestReplaceAddress/replace_with_reset_resume_state_test

Failed on CassCI build cassandra-2.2_dtest_win32 #165

2 flaps of this test in recent history, looks like a possible test issue, perhaps with invalid yaml at startup (somehow).
"	CASSANDRA	Resolved	10002	4	8105	dtest, windows
12997291	dtest failure in TestMutations:setUpClass.TestMutations:setUpClass	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/290/testReport/junit/(root)/TestMutations_setUpClass/TestMutations_setUpClass/"	CASSANDRA	Resolved	10002	4	8105	dtest
12934503	upgrade_supercolumns_test dtests failing on 2.1	"This tests in this module fail [here|https://github.com/riptano/cassandra-dtest/blob/18647a3e167f127795e2fe63d73305dddf103716/upgrade_supercolumns_test.py#L213] and [here|https://github.com/riptano/cassandra-dtest/blob/529cd71ad5ac4c2f28ccb5560ddc068f604c7b28/upgrade_supercolumns_test.py#L106] when a call to {{start}} with {{wait_other_notice=True}} times out. It happens consistently on the upgrade path from cassandra-2.1 to 2.2. I haven't seen clear evidence as to whether this is a test failure or a C* bug, so I'll mark it as a test error for the TE team to debug.

I don't have a CassCI link for this failure - the changes to the tests haven't been merged yet.

EDIT: changing the title of this ticket since there are multiple similar failures. The failing tests are

{code}
upgrade_supercolumns_test.py:TestSCUpgrade.upgrade_with_counters_test failing
upgrade_supercolumns_test.py:TestSCUpgrade.upgrade_with_index_creation_test
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12958506	dtest failure in secondary_indexes_test.TestSecondaryIndexes.test_query_indexes_with_vnodes	"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/344/testReport/secondary_indexes_test/TestSecondaryIndexes/test_query_indexes_with_vnodes

Failed on CassCI build trunk_novnode_dtest #344

Test does not appear to configure single-token cluster correctly:
{noformat}
Error Message

Error starting node1.
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4pEIhy
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/secondary_indexes_test.py"", line 436, in test_query_indexes_with_vnodes
    cluster.populate(2, use_vnodes=True).start()
  File ""/home/automaton/ccm/ccmlib/cluster.py"", line 360, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
""Error starting node1.\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4pEIhy\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------""
Standard Output

[node1 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
[node2 ERROR] Invalid yaml. Those properties [num_tokens] are not valid
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest
12955942	dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_default	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/327/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_default

Failed on CassCI build cassandra-2.1_offheap_dtest #327

Looks like a test error in dtest.py FlakyRetryPolicy() getting in an odd state. This test runs OK for me locally.

{noformat}
Error Message

'int' object has no attribute 'on_read_timeout'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-SrTCbF
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-SrTCbF
dtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-SrTCbF] directory
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-smdNhH
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Running stress without any user profile
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 829, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2306, in test_bulk_round_trip_default
    self._test_bulk_round_trip(nodes=3, partitioner=""murmur3"", num_operations=100000)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2277, in _test_bulk_round_trip
    num_records = create_records()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2252, in create_records
    ret = rows_to_list(self.session.execute(count_statement))[0][0]
  File ""cassandra/cluster.py"", line 1581, in cassandra.cluster.Session.execute (cassandra/cluster.c:27107)
    return self.execute_async(query, parameters, trace, custom_payload, timeout).result()
  File ""cassandra/cluster.py"", line 3145, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:60227)
    raise self._final_exception
""'int' object has no attribute 'on_read_timeout'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-SrTCbF\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-SrTCbF\ndtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-SrTCbF] directory\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-smdNhH\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Running stress without any user profile\n--------------------- >> end captured logging << ---------------------""
{noformat}"	CASSANDRA	Resolved	10002	4	8105	dtest
12997623	dtest failure in upgrade_tests.repair_test.TestUpgradeRepair.repair_after_upgrade_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/24/testReport/upgrade_tests.repair_test/TestUpgradeRepair/repair_after_upgrade_test"	CASSANDRA	Resolved	10002	4	8105	dtest
12739899	Implement -f functionality in stop-server.bat	"Stop-server.bat lists ""-f"" as an argument but does not handle it inside of stop-server.ps1. "	CASSANDRA	Resolved	10003	4	8105	Windows, qa-resolved
12992950	dtest failure in nose.failure.Failure.runTest	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_cqlsh_tests/17/testReport/nose.failure/Failure/runTest

{code}
Stack Trace
Traceback (most recent call last):
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/usr/local/lib/python2.7/dist-packages/nose/loader.py"", line 418, in loadTestsFromName
    addr.filename, addr.module)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/usr/local/lib/python2.7/dist-packages/nose/importer.py"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/__init__.py"", line 17, in <module>
    from .cassconnect import create_test_db, remove_test_db
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/cassconnect.py"", line 22, in <module>
    from .basecase import cql, cqlsh, cqlshlog, TEST_HOST, TEST_PORT, rundir
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/basecase.py"", line 46, in <module>
    import cqlsh
  File ""/home/automaton/cassandra/pylib/cqlshlib/test/cqlsh.py"", line 109, in <module>
    from cassandra.cluster import Cluster, PagedResult
ImportError: cannot import name PagedResult
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12985134	dtest failure in rebuild_test.TestRebuild.rebuild_ranges_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest/647/testReport/rebuild_test/TestRebuild/rebuild_ranges_test

Failed on CassCI build cassandra-2.2_dtest #647"	CASSANDRA	Resolved	10002	4	8105	dtest
12997284	dtest failure in batch_test.TestBatch.logged_batch_compatibility_3_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/792/testReport/batch_test/TestBatch/logged_batch_compatibility_3_test

http://cassci.datastax.com/job/cassandra-3.0_dtest/792/testReport/batch_test/TestBatch/logged_batch_compatibility_2_test/

http://cassci.datastax.com/job/cassandra-3.0_dtest/792/testReport/batch_test/TestBatch/logged_batch_compatibility_5_test/

{code}
Error Message

('Unable to connect to any servers', {'127.0.0.1': DriverException('ProtocolError returned from server while using explicitly set client protocol_version 4',)})
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-Ob2TdU
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Testing with 1 node(s) at version 'git:cassandra-2.1', 2 node(s) at current version
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set cassandra dir for node1 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set cassandra dir for node2 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1
ccm: INFO: Fetching Cassandra updates...
dtest: DEBUG: Set cassandra dir for node3 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1
--------------------- >> end captured logging << ---------------------
{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 290, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 318, in logged_batch_compatibility_3_test
    self._logged_batch_compatibility_test(0, 2, 'git:cassandra-2.1', 1)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 340, in _logged_batch_compatibility_test
    session = self.prepare_mixed(coordinator_idx, current_nodes, previous_version, previous_nodes)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 417, in prepare_mixed
    self.prepare(previous_nodes + current_nodes, compression, previous_version)
  File ""/home/automaton/cassandra-dtest/batch_test.py"", line 383, in prepare
    session = self.patient_cql_connection(node1)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 545, in patient_cql_connection
    bypassed_exception=NoHostAvailable
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 219, in retry_till_success
    return fun(*args, **kwargs)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 478, in cql_connection
    protocol_version, port=port, ssl_opts=ssl_opts)
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 506, in _create_session
    session = cluster.connect()
  File ""cassandra/cluster.py"", line 1156, in cassandra.cluster.Cluster.connect (cassandra/cluster.c:17387)
    with self._lock:
  File ""cassandra/cluster.py"", line 1189, in cassandra.cluster.Cluster.connect (cassandra/cluster.c:17208)
    raise
  File ""cassandra/cluster.py"", line 1176, in cassandra.cluster.Cluster.connect (cassandra/cluster.c:16911)
    self.control_connection.connect()
  File ""cassandra/cluster.py"", line 2521, in cassandra.cluster.ControlConnection.connect (cassandra/cluster.c:45182)
    self._set_new_connection(self._reconnect_internal())
  File ""cassandra/cluster.py"", line 2558, in cassandra.cluster.ControlConnection._reconnect_internal (cassandra/cluster.c:46079)
    raise NoHostAvailable(""Unable to connect to any servers"", errors)
""('Unable to connect to any servers', {'127.0.0.1': DriverException('ProtocolError returned from server while using explicitly set client protocol_version 4',)})\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-Ob2TdU\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Testing with 1 node(s) at version 'git:cassandra-2.1', 2 node(s) at current version\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set cassandra dir for node1 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set cassandra dir for node2 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Set cassandra dir for node3 to /home/automaton/.ccm/repository/gitCOLONcassandra-2.1\n--------------------- >> end captured logging << ---------------------""
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:cassandra-2.1
http://git-wip-us.apache.org/repos/asf/cassandra.git git:cassandra-2.1
http://git-wip-us.apache.org/repos/asf/cassandra.git git:cassandra-2.1
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12997928	dtest failure in rebuild_test.TestRebuild.simple_rebuild_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/382/testReport/rebuild_test/TestRebuild/simple_rebuild_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/rebuild_test.py"", line 75, in simple_rebuild_test
    session.execute(""ALTER KEYSPACE system_auth WITH REPLICATION = {'class':'NetworkTopologyStrategy', 'dc1':1, 'dc2':1};"")
  File ""cassandra/cluster.py"", line 1972, in cassandra.cluster.Session.execute (cassandra/cluster.c:34423)
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile).result()
  File ""cassandra/cluster.py"", line 3665, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:70216)
    raise self._final_exception
'Error from server: code=2200 [Invalid query] message=""Unknown keyspace system_auth""
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12960065	dtest failure in replace_address_test.TestReplaceAddress.replace_first_boot_test	"This looks like a timeout kind of flap. It's flapped once. Example failure:

http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/344/testReport/replace_address_test/TestReplaceAddress/replace_first_boot_test

Failed on CassCI build cassandra-2.2_offheap_dtest #344 - 2.2.6-tentative

{code}
Error Message

15 Apr 2016 16:23:41 [node3] Missing: ['127.0.0.4.* now UP']:
INFO  [main] 2016-04-15 16:21:32,345 Config.java:4.....
See system.log for remainder
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4i5qkE
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'start_rpc': 'true'}
dtest: DEBUG: Starting cluster with 3 nodes.
dtest: DEBUG: 32
dtest: DEBUG: Inserting Data...
dtest: DEBUG: Stopping node 3.
dtest: DEBUG: Testing node stoppage (query should fail).
dtest: DEBUG: Retrying read after timeout. Attempt #0
dtest: DEBUG: Retrying read after timeout. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Starting node 4 to replace node 3
dtest: DEBUG: Verifying querying works again.
dtest: DEBUG: Verifying tokens migrated sucessfully
dtest: DEBUG: ('WARN  [main] 2016-04-15 16:21:21,068 TokenMetadata.java:196 - Token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4\n', <_sre.SRE_Match object at 0x7fd21c0e2370>)
dtest: DEBUG: Try to restart node 3 (should fail)
dtest: DEBUG: [('WARN  [GossipStage:1] 2016-04-15 16:21:22,942 StorageService.java:1962 - Host ID collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner\n', <_sre.SRE_Match object at 0x7fd1f83555e0>)]
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 212, in replace_first_boot_test
    node4.start(wait_for_binary_proto=True)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 610, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 457, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 425, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""15 Apr 2016 16:23:41 [node3] Missing: ['127.0.0.4.* now UP']:\nINFO  [main] 2016-04-15 16:21:32,345 Config.java:4.....\nSee system.log for remainder\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-4i5qkE\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: Starting cluster with 3 nodes.\ndtest: DEBUG: 32\ndtest: DEBUG: Inserting Data...\ndtest: DEBUG: Stopping node 3.\ndtest: DEBUG: Testing node stoppage (query should fail).\ndtest: DEBUG: Retrying read after timeout. Attempt #0\ndtest: DEBUG: Retrying read after timeout. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Starting node 4 to replace node 3\ndtest: DEBUG: Verifying querying works again.\ndtest: DEBUG: Verifying tokens migrated sucessfully\ndtest: DEBUG: ('WARN  [main] 2016-04-15 16:21:21,068 TokenMetadata.java:196 - Token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4\\n', <_sre.SRE_Match object at 0x7fd21c0e2370>)\ndtest: DEBUG: Try to restart node 3 (should fail)\ndtest: DEBUG: [('WARN  [GossipStage:1] 2016-04-15 16:21:22,942 StorageService.java:1962 - Host ID collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner\\n', <_sre.SRE_Match object at 0x7fd1f83555e0>)]\n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12710662	cqlsh: DESCRIBE is not case-insensitive	"Keyspaces which are named starting with capital letters (and perhaps other things) sometimes require double quotes and sometimes do not.

For example, describe works without quotes:

cqlsh> describe keyspace ProductGenomeLocal;

CREATE KEYSPACE ""ProductGenomeLocal"" WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '3'
};

USE ""ProductGenomeLocal"";
[...]

But use will not:

cqlsh> use ProductGenomeLocal;
Bad Request: Keyspace 'productgenomelocal' does not exist

It seems that qoutes should only really be necessary when there's spaces or other symbols that need to be quoted. 

At the least, the acceptance or failures of quotes should be consistent.

Other minor annoyance: tab expansion works in use and describe with quotes, but will not work in either without quotes.
"	CASSANDRA	Resolved	10003	1	8105	cqlsh
12951315	dtest failure in replace_address_test.TestReplaceAddress.resumable_replace_test	"example failures:

http://cassci.datastax.com/job/cassandra-2.2_dtest/lastCompletedBuild/testReport/replace_address_test/TestReplaceAddress/resumable_replace_test/history/

http://cassci.datastax.com/job/cassandra-3.0_dtest/612/testReport/replace_address_test/TestReplaceAddress/resumable_replace_test/

Seems to fail hard with the following error:

{code}
16 Mar 2016 18:42:00 [node1] Missing: ['127.0.0.4.* now UP']:
INFO  [HANDSHAKE-/127.0.0.4] 2016-03-16 18:40:03,1.....
See system.log for remainder
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-JdjudW
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'start_rpc': 'true'}
dtest: DEBUG: Starting node 4 to replace node 3
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 253, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/replace_address_test.py"", line 253, in resumable_replace_test
    node4.start(jvm_args=[""-Dcassandra.replace_address_first_boot=127.0.0.3""])
  File ""/home/automaton/ccm/ccmlib/node.py"", line 597, in start
    node.watch_log_for_alive(self, from_mark=mark)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 449, in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
  File ""/home/automaton/ccm/ccmlib/node.py"", line 417, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""16 Mar 2016 18:42:00 [node1] Missing: ['127.0.0.4.* now UP']:\nINFO  [HANDSHAKE-/127.0.0.4] 2016-03-16 18:40:03,1.....\nSee system.log for remainder\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-JdjudW\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: Starting node 4 to replace node 3\n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	4	8105	dtest
12783395	Run stress nightly against trunk in a way that validates	"Stress has some very basic validation functionality when used without workload profiles. It found a bug on trunk when I first ran it so it has value even though the validation is basic.

As a beachhead for the kind of blackbox validation that we are missing we can start by running stress nightly or 24/7 in some rotation.

There should be two jobs. One job has inverted success criteria (C* should lose some data) and the job should only ""pass"" if the failure is detected. This is just to prove that the harness reports failure if failure occurs.

Another would be the real job that runs stress, parses and parses the output for reports of missing data.

This job is the first pass and basis of what we can point to when a developer makes a change, implements a feature, or fixes a bug, and say ""go add validation to this job.""

Follow on tickets to link to this
* Test multiple configurations
* Get stress to validate more query functionality and APIs (counters, LWT, batches)
* Parse logs and fail tests on error level logs (great way to improve log messages over time)
* ?

I am going to hold off on creating a ton of issues until we have a basic version of the job running."	CASSANDRA	Resolved	10002	3	8105	LWT, monthly-release
12962805	dtest failure in pushed_notifications_test.TestPushedNotifications.move_single_node_test	"single test flap, so could be a fluke. happened on the trunk no-vnode test:

http://cassci.datastax.com/job/trunk_novnode_dtest/354/testReport/pushed_notifications_test/TestPushedNotifications/move_single_node_test

Failed on CassCI build trunk_novnode_dtest #354"	CASSANDRA	Resolved	10002	4	8105	dtest
12775794	LOCAL_QUORUM writes returns wrong message	"We have two DC3, each with 7 nodes.
Here is the keyspace setup:

 create keyspace test
 with placement_strategy = 'NetworkTopologyStrategy'
 and strategy_options = {DC2 : 3, DC1 : 3}
 and durable_writes = true;

We brought down two nodes in DC2 for maintenance. We only write to DC1 using local_quroum (using datastax JavaClient)
But we see this errors in the log:
Cassandra timeout during write query at consistency LOCAL_QUORUM (4 replica were required but only 3 acknowledged the write
why does it say 4 replica were required? and Why would it give error back to client since local_quorum should succeed.

Here are the output from nodetool status

Note: Ownership information does not include topology; for complete information, specify a keyspace
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.2.0.1  10.92 GB   256     7.9%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC206
UN  10.2.0.2   6.17 GB    256     8.0%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC106
UN  10.2.0.3  6.63 GB    256     7.3%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC107
DL  10.2.0.4  1.54 GB    256     7.7%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC107
UN  10.2.0.5  6.02 GB    256     6.6%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC106
UJ  10.2.0.6   3.68 GB    256     ?      XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC205
UN  10.2.0.7  7.22 GB    256     7.7%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC205
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.1.0.1   6.04 GB    256     8.6%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC10
UN  10.1.0.2   7.55 GB    256     7.4%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC8
UN  10.1.0.3   5.83 GB    256     7.0%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC9
UN  10.1.0.4    7.34 GB    256     7.9%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC6
UN  10.1.0.5   7.57 GB    256     8.0%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC7
UN  10.1.0.6   5.31 GB    256     7.3%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX  RAC10
UN  10.1.0.7   5.47 GB    256     8.6%   XXXXXXXXXXXXXXXXXXXXXXXXXXXX RAC9

I did a cql trace on the query and here is the trace, and it does say 
   Write timeout; received 3 of 4 required replies | 17:27:52,831 |  10.1.0.1 |        2002873

at the end. I guess that is where the client gets the error from. But the rows was inserted to Cassandra correctly. And I traced read with local_quorum and it behaves correctly and the reads don't go to DC2. The problem is only with writes on local_quorum.
{code}
Tracing session: 5a789fb0-b70d-11e4-8fca-99bff9c19890

 activity                                                                                                                                    | timestamp    | source      | source_elapsed
---------------------------------------------------------------------------------------------------------------------------------------------+--------------+-------------+----------------
                                                                                                                          execute_cql3_query | 17:27:50,828 |  10.1.0.1 |              0
 Parsing insert into test (user_id, created, event_data, event_id)values ( 123456789 , 9eab8950-b70c-11e4-8fca-99bff9c19891, 'test', '16'); | 17:27:50,828 |  10.1.0.1 |             39
                                                                                                                         Preparing statement | 17:27:50,828 |  10.1.0.1 |            135
                                                                                                           Message received from /10.1.0.1 | 17:27:50,829 |  10.1.0.5 |             25
                                                                                                              Sending message to /10.1.0.5 | 17:27:50,829 |  10.1.0.1 |            421
                                                                                                   Executing single-partition query on users | 17:27:50,829 |  10.1.0.5 |            177
                                                                                                                Acquiring sstable references | 17:27:50,829 |  10.1.0.5 |            191
                                                                                                                 Merging memtable tombstones | 17:27:50,830 |  10.1.0.5 |            208
                                                                                                           Message received from /10.1.0.5 | 17:27:50,830 |  10.1.0.1 |           1461
                                                                                                           Message received from /10.1.0.1 | 17:27:50,830 |  10.1.0.2 |             14
                                                                                                                 Key cache hit for sstable 1 | 17:27:50,830 |  10.1.0.5 |            254
                                                                                                        Processing response from /10.1.0.5 | 17:27:50,830 |  10.1.0.1 |           1514
                                                                                                   Executing single-partition query on users | 17:27:50,830 |  10.1.0.2 |             78
                                                                                                 Seeking to partition beginning in data file | 17:27:50,830 |  10.1.0.5 |            264
                                                                                                              Sending message to /10.1.0.2 | 17:27:50,830 |  10.1.0.1 |           1517
                                                                                                                Acquiring sstable references | 17:27:50,830 |  10.1.0.2 |             85
                                                                   Skipped 0/1 non-slice-intersecting sstables, included 0 due to tombstones | 17:27:50,830 |  10.1.0.5 |            453
                                                                                                           Determining replicas for mutation | 17:27:50,830 |  10.1.0.1 |           1746
                                                                                                                 Merging memtable tombstones | 17:27:50,830 |  10.1.0.2 |             97
                                                                                                  Merging data from memtables and 1 sstables | 17:27:50,830 |  10.1.0.5 |            476
                                                                                                           Message received from /10.1.0.2 | 17:27:50,830 |  10.1.0.1 |           2369
                                                                                                                 Key cache hit for sstable 2 | 17:27:50,830 |  10.1.0.2 |            120
                                                                                                          Read 1 live and 0 tombstoned cells | 17:27:50,830 |  10.1.0.5 |            506
                                                                                                 Seeking to partition beginning in data file | 17:27:50,830 |  10.1.0.2 |            123
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,830 |  10.1.0.5 |            570
                                                                   Skipped 0/1 non-slice-intersecting sstables, included 0 due to tombstones | 17:27:50,830 |  10.1.0.2 |            288
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,830 |  10.1.0.5 |            617
                                                                                                  Merging data from memtables and 1 sstables | 17:27:50,830 |  10.1.0.2 |            297
                                                                                                          Read 1 live and 0 tombstoned cells | 17:27:50,830 |  10.1.0.2 |            319
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,830 |  10.1.0.2 |            362
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,830 |  10.1.0.2 |            420
                                                                                                           Message received from /10.1.0.1 | 17:27:50,831 |   10.1.0.4 |             17
                                                                                                              Sending message to /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           2435
                                                                                                           Message received from /10.1.0.1 | 17:27:50,831 |  10.1.0.2 |              8
                                                                                                              Acquiring switchLock read lock | 17:27:50,831 |   10.1.0.4 |             61
                                                                                                        Processing response from /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           2488
                                                                                                              Acquiring switchLock read lock | 17:27:50,831 |  10.1.0.2 |             44
                                                                                                                      Appending to commitlog | 17:27:50,831 |   10.1.0.4 |             78
                                                                                    Not hinting /10.2.0.4 which has been down 364809650ms | 17:27:50,831 |  10.1.0.1 |           2503
                                                                                                                      Appending to commitlog | 17:27:50,831 |  10.1.0.2 |             62
                                                                                                                    Adding to event memtable | 17:27:50,831 |   10.1.0.4 |             96
                                                                                                               Sending message to /10.1.0.4 | 17:27:50,831 |  10.1.0.1 |           2557
                                                                                                                    Adding to event memtable | 17:27:50,831 |  10.1.0.2 |             80
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,831 |   10.1.0.4 |            177
                                                                                                              Acquiring switchLock read lock | 17:27:50,831 |  10.1.0.1 |           2669
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,831 |  10.1.0.2 |            160
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,831 |   10.1.0.4 |            333
                                                                                                                      Appending to commitlog | 17:27:50,831 |  10.1.0.1 |           2682
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,831 |  10.1.0.2 |            266
                                                                                                                    Adding to event memtable | 17:27:50,831 |  10.1.0.1 |           2720
                                                                                                             Sending message to /10.2.0.5 | 17:27:50,831 |  10.1.0.1 |           2758
                                                                                                           Message received from /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           2989
                                                                                                        Processing response from /10.1.0.2 | 17:27:50,831 |  10.1.0.1 |           3024
                                                                                                            Message received from /10.1.0.4 | 17:27:50,832 |  10.1.0.1 |           3764
                                                                                                         Processing response from /10.1.0.4 | 17:27:50,832 |  10.1.0.1 |           3805
                                                                                                           Message received from /10.1.0.1 | 17:27:50,841 | 10.2.0.5 |             24
                                                                                                   Enqueuing forwarded write to /10.2.0.7 | 17:27:50,841 | 10.2.0.5 |            255
                                                                                                   Enqueuing forwarded write to /10.2.0.3 | 17:27:50,841 | 10.2.0.5 |            283
                                                                                                    Enqueuing forwarded write to /10.2.0.6 | 17:27:50,841 | 10.2.0.5 |            307
                                                                                                              Acquiring switchLock read lock | 17:27:50,841 | 10.2.0.5 |            362
                                                                                                                      Appending to commitlog | 17:27:50,841 | 10.2.0.5 |            380
                                                                                                             Sending message to /10.2.0.7 | 17:27:50,841 | 10.2.0.5 |            382
                                                                                                                    Adding to event memtable | 17:27:50,841 | 10.2.0.5 |            411
                                                                                                              Sending message to /10.2.0.6 | 17:27:50,841 | 10.2.0.5 |            429
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,841 | 10.2.0.5 |            484
                                                                                                             Sending message to /10.2.0.3 | 17:27:50,841 | 10.2.0.5 |            561
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,841 | 10.2.0.5 |            625
                                                                                                              Acquiring switchLock read lock | 17:27:50,842 |  10.2.0.6 |           1031
                                                                                                              Acquiring switchLock read lock | 17:27:50,842 | 10.2.0.7 |            178
                                                                                                                      Appending to commitlog | 17:27:50,842 |  10.2.0.6 |           1066
                                                                                                                      Appending to commitlog | 17:27:50,842 | 10.2.0.7 |            196
                                                                                                                    Adding to event memtable | 17:27:50,842 |  10.2.0.6 |           1118
                                                                                                                    Adding to event memtable | 17:27:50,842 | 10.2.0.7 |            231
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,842 |  10.2.0.6 |           1181
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,842 | 10.2.0.7 |            286
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,842 | 10.2.0.7 |            421
                                                                                                              Acquiring switchLock read lock | 17:27:50,843 | 10.2.0.3 |           1216
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,843 |  10.2.0.6 |           1382
                                                                                                                      Appending to commitlog | 17:27:50,843 | 10.2.0.3 |           1239
                                                                                                                    Adding to event memtable | 17:27:50,843 | 10.2.0.3 |           1313
                                                                                                           Enqueuing response to /10.1.0.1 | 17:27:50,843 | 10.2.0.3 |           1407
                                                                                                              Sending message to /10.1.0.1 | 17:27:50,843 | 10.2.0.3 |           1631
                                                                                                          Message received from /10.2.0.5 | 17:27:50,851 |  10.1.0.1 |          23333
                                                                                                       Processing response from /10.2.0.5 | 17:27:50,851 |  10.1.0.1 |          23380
                                                                                                          Message received from /10.2.0.7 | 17:27:50,852 |  10.1.0.1 |          23908
                                                                                                       Processing response from /10.2.0.7 | 17:27:50,852 |  10.1.0.1 |          23953
                                                                                                           Message received from /10.2.0.6 | 17:27:50,853 |  10.1.0.1 |          25143
                                                                                                        Processing response from /10.2.0.6 | 17:27:50,853 |  10.1.0.1 |          25178
                                                                                                          Message received from /10.2.0.3 | 17:27:50,854 |  10.1.0.1 |          25478
                                                                                                       Processing response from /10.2.0.3 | 17:27:50,854 |  10.1.0.1 |          25516
                                                                                             Write timeout; received 3 of 4 required replies | 17:27:52,831 |  10.1.0.1 |        2002873
                                                                                                                            Request complete | 17:27:52,833 |  10.1.0.1 |        2005989
{code}
cqlsh:XXXX> CONSISTENCY
Current consistency level is LOCAL_QUORUM."	CASSANDRA	Resolved	10002	1	9435	qa-resolved
12552639	Make identifier and value grammar for CQL3 stricter	"The current grammar for CQL3 allows:
# uuid and integer constants as identifiers
# identifier as value (aka term in the grammar)

I think both of those should be removed.

For 1, mostly because this feels useless and slightly complicates the grammar which is annoying for the documentation of CQL3 for instance (note that this doesn't mean forbidding integer or uuid as identifier, but means they have to be double-quoted when used as such).
For 2, I think that allowing identifier as value is actually misleading, typically if you write things like {{SELECT foo WHERE foo=foo}}. It suggests we support JOIN when we do not.

Also, if both are done, then one will always be able to distinguish between identifier and value even without any context, which is a nice property."	CASSANDRA	Resolved	10002	4	9435	cql3
12981178	Syncing most recent commit in CAS across replicas can cause all CAS queries in the CQL partition to fail	"We update the most recent commit on requiredParticipant replicas if out of sync during the prepare round in beginAndRepairPaxos method. We keep doing this in a loop till the requiredParticipant replicas have the same most recent commit or we hit timeout. 

Say we have 3 machines A,B and C and gc grace on the table is 10 days. We do a CAS write at time 0 and it went to A and B but not to C.  C will get the hint later but will not update the most recent commit in paxos table. This is how CAS hints work. 
In the paxos table whose gc_grace=0, most_recent_commit in A and B will be inserted with timestamp 0 and with a TTL of 10 days. After 10 days, this insert will become a tombstone at time 0 till it is compacted away since gc_grace=0.

Do a CAS read after say 1 day on the same CQL partition and this time prepare phase involved A and C. most_recent_commit on C for this CQL partition is empty. A sends the most_recent_commit to C with a timestamp of 0 and with a TTL of 10 days. This most_recent_commit on C will expire on 11th day since it is inserted after 1 day. 

most_recent_commit are now in sync on A,B and C, however A and B most_recent_commit will expire on 10th day whereas for C it will expire on 11th day since it was inserted one day later. 

Do another CAS read after 10days when most_recent_commit on A and B have expired and is treated as tombstones till compacted. In this CAS read, say A and C are involved in prepare phase. most_recent_commit will not match between them since it is expired in A and is still there on C. This will cause most_recent_commit to be applied to A with a timestamp of 0 and TTL of 10 days. If A has not compacted away the original most_recent_commit which has expired, this new write to most_recent_commit wont be visible on reads since there is a tombstone with same timestamp(Delete wins over data with same timestamp). 

Another round of prepare will follow and again A would say it does not know about most_recent_write(covered by original write which is not a tombstone) and C will again try to send the write to A. This can keep going on till the request timeouts or only A and B are involved in the prepare phase. 

When As original most_recent_commit which is now a tombstone is compacted, all the inserts which it was covering will come live. This will in turn again get played to another replica. This ping pong can keep going on for a long time. 

The issue is that most_recent_commit is expiring at different times across replicas. When they get replayed to a replica to make it in sync, we again set the TTL from that point.  
During the CAS read which timed out, most_recent_commit was being sent to another replica in a loop. Even in successful requests, it will try to loop for a couple of times if involving A and C and then when the replicas which respond are A and B, it will succeed. So this will have impact on latencies as well. 

These timeouts gets worse when a machine is down as no progress can be made as the machine with unexpired commit is always involved in the CAS prepare round. Also with range movements, the new machine gaining range has empty most recent commit and gets the commit at a later time causing same issue. 

Repro steps:
1. Paxos TTL is max(3 hours, gc_grace) as defined in SystemKeyspace.paxosTtl(). Change this method to not put a minimum TTL of 3 hours. 
Method  SystemKeyspace.paxosTtl() will look like return metadata.getGcGraceSeconds();   instead of return Math.max(3 * 3600, metadata.getGcGraceSeconds());
We are doing this so that we dont need to wait for 3 hours. 

Create a 3 node cluster with the code change suggested above with machines A,B and C
CREATE KEYSPACE  test WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };
use test;
CREATE TABLE users (a int PRIMARY KEY,b int);
alter table users WITH gc_grace_seconds=120;
consistency QUORUM;
bring down machine C
INSERT INTO users (user_name, password ) VALUES ( 1,1) IF NOT EXISTS;
Nodetool flush on machine A and B
Bring up the down machine B 
consistency SERIAL;
tracing on;
wait 80 seconds
Bring up machine C
select * from users where user_name = 1;
Wait 40 seconds 
select * from users where user_name = 1;  //All queries from this point forward will timeout. 

One of the potential fixes could be to set the TTL based on the remaining time left on another replicas. This will be TTL-timestamp of write. This timestamp is calculated from ballot which uses server time."	CASSANDRA	Resolved	10002	1	9435	LWT
12637842	Transposed KS/CF arguments	"*Reproduction*
Using https://github.com/joaquincasares/java-driver's integrationtests branch, run `mvn test` from the root directory.

*Issue*
The test will fail due to https://github.com/joaquincasares/java-driver/blob/integrationtests/driver-core/src/main/java/com/datastax/driver/core/ResultSetFuture.java being swapped here:
{CODE}
case ALREADY_EXISTS:
    org.apache.cassandra.exceptions.AlreadyExistsException aee = (org.apache.cassandra.exceptions.AlreadyExistsException)te;
    return new AlreadyExistsException(aee.ksName, aee.cfName);
{CODE}

*Error*
{CODE}
repeatSchemaDefinition(com.datastax.driver.core.ExceptionsTest)  Time elapsed: 0.501 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<Table repeatschema[ks.repeatschemacf] already exists> but was:<Table repeatschema[cf.repeatschemaks] already exists>
{CODE}"	CASSANDRA	Resolved	10003	1	9435	datastax_qa
12552706	Apparent data loss using super columns and row cache via ConcurrentLinkedHashCacheProvider	"Tested on a vanilla single-node cassandra 1.0.9 installation.

When using super columns along with row caching via ConcurrentLinkedHashCacheProvider (default if no JNA available, or explicitly configured even if JNA available), there's what appears as transient data loss.

Given this script executed in cassandra-cli:
{quote}
create keyspace Test;
use Test;

create column family Users with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type' and rows_cached=75000 and row_cache_provider='ConcurrentLinkedHashCacheProvider';

set Users['mina']['attrs']['name'] = 'Mina';
get Users['mina'];

set Users['mina']['attrs']['country'] = 'Canada';
get Users['mina'];

set Users['mina']['attrs']['region'] = 'Quebec';
get Users['mina'];
{quote}

The output from the 3 gets above is as follows:

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

It's clear that the second and third set commands (country, region) are missing in the returned results.

If the row cache is explicitly invalidated (in a second terminal, via `nodetool -h localhost invalidaterowcache Test Users`), the missing data springs to life on next 'get':
{quote}
[default@Test] get Users['mina'];
=> (super_column=attrs,
     (column=country, value=Canada, timestamp=1335377839592000)
     (column=name, value=Mina, timestamp=1335377788441000)
     (column=region, value=Quebec, timestamp=1335377871353000))
Returned 1 results.
{quote}

From cursory checks, this does not to appear to happen with regular columns, nor with JNA enabled + SerializingCacheProvider.

"	CASSANDRA	Resolved	10002	1	9435	ConcurrentLinkedHashCacheProvider, cache, supercolumns
12536532	Add Support for Composite Secondary Indexes	"CASSANDRA-2474 and CASSANDRA-3647 add the ability to transpose wide rows differently, for efficiency and functionality secondary index api needs to be altered to allow composite indexes.  

I think this will require the IndexManager api to have a maybeIndex(ByteBuffer column) method that SS can call and implement a PerRowSecondaryIndex per column, break the composite into parts and index specific bits, also including the base rowkey.

Then a search against a TRANSPOSED row or DOCUMENT will be possible.

 "	CASSANDRA	Resolved	10002	7	9435	cql3, secondary_index
12610588	(CQL3) data type not in lowercase are not handled correctly.	Seems that we accept {{int}} but we don't accept {{INT}} (that is, the parser accepts it, but we fail later to recognize it).	CASSANDRA	Resolved	10003	1	9435	cql3
12558112	CQL3: dates are not handled correctly in slices 	"Our timestamp type allows to input timestamp as dates like '2012-06-06'. However, those don't work as expected in slice queries, as for instance:
{noformat}
SELECT * FROM timeline
  WHERE k = ...
  AND time > '2012-06-06'
  AND time <= '2012-06-09'
{noformat}
will return timestamps from '2012-06-06' and not those from '2012-06-09'. The reason being of course that we always translate a date the same way, using 0 for whichever part is not precised.

A reasonably simple fix could be to add a new fromString(String s, boolean gt) method to AbstractType that is used when the the string should be interpreted in an inequality (the boolean gt would then say which kind of inequality).
"	CASSANDRA	Resolved	10002	1	9435	cql3
12994675	dtest failure in read_repair_test.TestReadRepair.test_gcable_tombstone_resurrection_on_range_slice_query	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_jdk8/291/testReport/read_repair_test/TestReadRepair/test_gcable_tombstone_resurrection_on_range_slice_query"	CASSANDRA	Resolved	10002	4	9435	dtest
12703505	Fix UnsupportedOperationException on CAS timeout	This is due to CASSANDRA-6595: we call blockFor() for the paxos consistency, but we never added SERIAL/LOCAL_SERIAL in that method. Attaching trivial patch to add it.	CASSANDRA	Resolved	10003	1	9435	LWT
12916743	Allow literal value as parameter of UDF & UDA	"I have defined the following UDF

{code:sql}
CREATE OR REPLACE FUNCTION  maxOf(current int, testValue int) RETURNS NULL ON NULL INPUT 
RETURNS int 
LANGUAGE java 
AS  'return Math.max(current,testValue);'

CREATE TABLE maxValue(id int primary key, val int);
INSERT INTO maxValue(id, val) VALUES(1, 100);

SELECT maxOf(val, 101) FROM maxValue WHERE id=1;
{code}

I got the following error message:

{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:19 no viable alternative at input '101' (SELECT maxOf(val1, [101]...)"">
{code}

 It would be nice to allow literal value as parameter of UDF and UDA too.

 I was thinking about an use-case for an UDA groupBy() function where the end user can *inject* at runtime a literal value to select which aggregation he want to display, something similar to GROUP BY ... HAVING <filter clause>"	CASSANDRA	Resolved	10003	4	9435	CQL3, UDF, doc-impacting
12600923	cql3: defining more than one pk should be invalid	"dtests caught this on trunk:

{noformat}
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 277, in create_invalid_test
    assert_invalid(cursor, ""CREATE TABLE test (key1 text PRIMARY KEY, key2 text PRIMARY KEY)"")
  File ""/var/lib/buildbot/cassandra-dtest/assertions.py"", line 31, in assert_invalid
    assert False, ""Expecting query to be invalid""
AssertionError: Expecting query to be invalid
{noformat}"	CASSANDRA	Resolved	10002	1	9435	cql3
12597608	Add cursor API/auto paging to the native CQL protocol	"The goal here would be to use a query paging mechanism to the CQL native protocol. Typically the client/server with that would look something like this:
{noformat}
C sends query to S.
S sends N first rows matching the query + flag saying the response is not complete
C requests the next N rows
S sends N next rows + flag saying whether there is more
C requests the next N rows
...
S sends last rows + flag saying there is no more result
{noformat}

The clear goal is for user to not have to worry about limiting queries and doing manual paging."	CASSANDRA	Resolved	10002	2	9435	cql, protocol
12539699	CQL support for changing row key type in ALTER TABLE	There is currently no way to alter the key_validation_class from CQL. jbellis suggested that this could be done by being able to ALTER the type of the KEY alias.	CASSANDRA	Resolved	10002	4	9435	cql
12597524	Assertion with LCS compaction	"As instructed in CASSANDRA-4321 I have raised this issue as a continuation of that issue as it appears the problem still exists.

I have repeatedly run sstablescrub across all my nodes after the 1.1.2 upgrade until sstablescrub shows no errors.  The exceptions described in CASSANDRA-4321 do not occur as frequently now but the integrity check still throws exceptions on a number of nodes.  Once those exceptions occur compactionstats shows a large number of pending tasks with no progression afterwards.

{code}
ERROR [CompactionExecutor:150] 2012-07-05 04:26:15,570 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:150,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}
"	CASSANDRA	Resolved	10002	1	9435	lcs
12714754	Tuple type	"For CASSANDRA-6875 we need to be able to talk about tuples values and types (for prepared variables). Since we need it there, clients will need to support them anyway and so I think it would be a lot cleaner to start supporting those more generally. Besides, having tuples is a relatively simple and natural extension to what we have. I'll note in particular that tuple have a close relationship to user type in the sense that a tuple will be really just like an anonymous with no name for the fields and in particular a tuple value will be the same than a user type value.

The syntax would simply look like that:
{noformat}
CREATE TABLE foo (
    k int PRIMARY KEY,
    v tuple<int, text, float>
)

INSERT INTO foo(k, v) VALUES(0, (3, 'bar', 2.1));
{noformat}
We can also add projections in selects if we want:
{noformat}
SELECT v[0], v[2] FROM foo WHERE k = 0;
{noformat}
but that can come later (after all, we still don't have projections for collections and it's not a big deal)."	CASSANDRA	Resolved	10002	2	9435	cql3
12716401	BatchlogManagerTest unit test failing in 2.1 & trunk	"Commit 1147ee3 for CASSANDRA-6975 introduced a regression for BatchlogManagerTest in 2.1 and trunk:

{noformat}
    [junit] Testsuite: org.apache.cassandra.db.BatchlogManagerTest
    [junit] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 45.795 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] WARN  18:43:57 Changing /127.0.0.1's host ID from 1c3d6470-e2aa-11e3-beb0-9b2001e5c823 to 32f21350-e2aa-11e3-beb0-9b2001e5c823
    [junit] WARN  18:43:57 Changing /127.0.0.1's host ID from 1c3d6470-e2aa-11e3-beb0-9b2001e5c823 to 32f21350-e2aa-11e3-beb0-9b2001e5c823
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testReplay(org.apache.cassandra.db.BatchlogManagerTest):  Caused an ERROR
    [junit] Error validating SELECT count(*) FROM %s.%s
    [junit] java.lang.RuntimeException: Error validating SELECT count(*) FROM %s.%s
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:275)
    [junit]     at org.apache.cassandra.db.BatchlogManager.countAllBatches(BatchlogManager.java:102)
    [junit]     at org.apache.cassandra.db.BatchlogManagerTest.testReplay(BatchlogManagerTest.java:60)
    [junit] Caused by: org.apache.cassandra.exceptions.SyntaxException: line 1:24 no viable alternative at character '%'
    [junit]     at org.apache.cassandra.cql3.CqlLexer.throwLastRecognitionError(CqlLexer.java:201)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:455)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:430)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:211)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:252)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:262)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.BatchlogManagerTest FAILED
{noformat}

http://cassci.datastax.com/job/cassandra-2.1_utest/291/testReport/org.apache.cassandra.db/BatchlogManagerTest/testReplay/
http://cassci.datastax.com/job/trunk_utest/627/testReport/junit/org.apache.cassandra.db/BatchlogManagerTest/testReplay/"	CASSANDRA	Resolved	10003	1	9435	qa-resolved
12701241	Allow filtering on primary key expressions in 2i queries	"We allow

{code}
SELECT a, d FROM t.t WHERE b = 'b1' AND a = 'a14521'
{code}

and

{code}
SELECT a, d FROM t.t WHERE b = 'b1' AND token(a)  > token( 'a14521')
{code}

but not
{code}
SELECT a, d FROM t.t WHERE b = 'b1' AND a  > 'a14521'
{code}

(given an index on {{b}}, with primary key {{a}})

we allow combining other predicates with an indexed one and filtering those in a nested loop; we should allow the same for primary keys"	CASSANDRA	Open	10003	2	9435	indexes
12514422	Simplified classes to write SSTables (for bulk loading usage)	"sstableloader only stream existing sstables. If you need to load data existing in another form (json, csv, whatnot), you need to first write the sstable(s) to load. The recommended way to do this is either to use json2sstable or to modify it if your input is not json. Modifying json2sstable is however more involved than it needs to be, you'll need at least some basic understanding of a bunch of internal classes (DecoratedKey, ColumnFamily, SuperColumn, ...). Even for json input, you can use json2sstable only if your json actually conform to what is expected and even then, good luck to someone that want to add counters.

This ticket proposes to add a simple interface to write sstables. "	CASSANDRA	Resolved	10003	2	9435	bulkloader
12497480	Add description to nodetool commands	The help of nodetool is not very pretty, in particular there is no description of proposed commands	CASSANDRA	Resolved	10003	4	9435	nodetool
12537925	Clean up isMarkedForDelete / getLocalDeletionTime	As explained in CASSANDRA-3579, isMarkedForDelete() depends on the current system clock so it can change during a two-pass compaction.  Suggested fix is to replace iMFD + gLDT with a getExpirationTime method, so comparison with the compaction's gcBefore will remain constant.	CASSANDRA	Resolved	10002	4	9435	compaction
12551531	cql3 ALTER TABLE foo WITH default_validation=int has no effect	"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY) WITH default_validation=timestamp;
ALTER TABLE test WITH default_validation=int;
{noformat}

does not actually change the default validation type of the CF. It does under cql2.

No error is thrown. Some properties *can* be successfully changed using ALTER WITH, such as comment and gc_grace_seconds, but I haven't tested all of them. It seems probable that default_validation is the only problematic one, since it's the only (changeable) property which accepts CQL typenames."	CASSANDRA	Resolved	10003	1	9435	cql3
12668469	CAS should distinguish promised and accepted ballots	"Currently, we only keep 1) the most recent promise we've made and 2) the last update we've accepted. But we don't keep the ballot at which that last update was accepted. And because a node always promise to newer ballot, this means an already committed update can be replayed even after another update has been committed. Re-committing a value is fine, but only as long as we've not start a new round yet.

Concretely, we can have the following case (with 3 nodes A, B and C) with the current implementation:
* A proposer P1 prepare and propose a value X at ballot t1. It is accepted by all nodes.
* A proposer P2 propose at t2 (wanting to commit a new value Y). If say A and B receive the commit of P1 before the propose of P2 but C receives those in the reverse order, we'll current have the following states:
{noformat}
A: in-progress = (t2, _), mrc = (t1, X)
B: in-progress = (t2, _), mrc = (t1, X)
C: in-progress = (t2, X), mrc = (t1, X)
{noformat}
Because C has received the t1 commit after promising t2, it won't have removed X during t1 commit (but note that the problem is not during commit, that example still stand if C never receive any commit message).
* Now, based on the promise of A and B, P2 will propose Y at t2 (C don't see this propose in particular, not before he promise on t3 below at least). A and B accepts, P2 will send a commit for Y.
* In the meantime a proposer P3 submit a prepare at t3 (for some other irrelevant value) which reaches C before it receives P2 propose&commit. That prepare reaches A and B too, but after the P2 commit. At that point the state will be:
{noformat}
A: in-progress = (t3, _), mrc = (t2, Y)
B: in-progress = (t3, _), mrc = (t2, Y)
C: in-progress = (t3, X), mrc = (t2, Y)
{noformat}
In particular, C still has X as update because each time it got a commit, it has promised to a more recent ballot and thus skipped the delete. The value is still X because it has received the P2 propose after having promised t3 and has thus refused it.
* P3 gets back the promise of say C and A. Both response has t3 as in-progress ballot (and it is more recent than any mrc) but C comes with value X. So P3 will replay X. Assuming no more contention this replay will succeed and X will be committed at t3.

At the end of that example, we've comitted X, Y and then X again, even though only P1 has ever proposed X.

I believe the correct fix is to keep the ballot of when an update is accepted (instead of using the most recent promised ballot). That way, in the example above, P3 would receive from C a promise on t3, but would know that X was accepted at t1. And so P3 would be able to ignore X since the mrc of A will tell him it's an obsolete value.
"	CASSANDRA	Resolved	10002	1	9435	LWT
12497550	Fix the read race condition in CFStore for counters 	"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
"	CASSANDRA	Resolved	10002	1	9435	counters
12513838	Randomize (to some extend) the choice of the first replica for counter increment	"Right now, we choose the first replica for a counter increments based solely on what the snitch returns. If the clients requests are well balanced over the cluster and the snitch not ill configured, this should not be a problem, but this is probably too strong an assumption to make.

The goal of this ticket is to change this to choose a random replica in the current data center instead."	CASSANDRA	Resolved	10003	4	9435	counters
12733053	altering a table to add a static column bypasses clustering column requirement check	"cqlsh:test_ks> create TABLE foo ( bar int, primary key (bar));
cqlsh:test_ks> alter table foo add bar2 text static;
cqlsh:test_ks> describe table foo;

CREATE TABLE foo (
  bar int,
  bar2 text static,
  PRIMARY KEY ((bar))
) 

cqlsh:test_ks> select * from foo;
TSocket read 0 bytes


ERROR [Thrift:12] 2014-08-09 15:08:22,518 CassandraDaemon.java (line 199) Exception in thread Thread[Thrift:12,5,main]
java.lang.AssertionError
	at org.apache.cassandra.config.CFMetaData.getStaticColumnNameBuilder(CFMetaData.java:2142)
	at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:454)
	at org.apache.cassandra.cql3.statements.SelectStatement.getRangeCommand(SelectStatement.java:360)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:206)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:61)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:158)

"	CASSANDRA	Resolved	10002	1	9435	cql
12924894	[Regression] Error when removing list element with UPDATE statement	"Steps to reproduce:

{code:sql}
CREATE TABLE simple(
  id int PRIMARY KEY,
  int_list list<int>
);

INSERT INTO simple(id, int_list) VALUES(10, [1,2,3]);
SELECT * FROM simple;

 id | int_list
----+-----------
 10 | [1, 2, 3]

UPDATE simple SET int_list[0]=null WHERE id=10;
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.AssertionError"">
{code}

 Per CQL semantics, setting a column to NULL == deleting it.

 When using debugger, below is the Java stack trace on server side:

{noformat}
 ERROR o.apache.cassandra.transport.Message - Unexpected exception during request; channel = [id: 0x6dbc33bd, /192.168.51.1:57723 => /192.168.51.1:9473]
java.lang.AssertionError: null
	at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:49) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:88) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.UpdateParameters.addTombstone(UpdateParameters.java:141) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.UpdateParameters.addTombstone(UpdateParameters.java:136) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.Lists$SetterByIndex.execute(Lists.java:362) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:94) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:666) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:606) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:413) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:401) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:472) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:449) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:130) ~[cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [cassandra-all-3.1.1.jar:3.1.1]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_60-ea]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [cassandra-all-3.1.1.jar:3.1.1]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [cassandra-all-3.1.1.jar:3.1.1]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60-ea]
{noformat}

The root cause seems to be located at *org.apache.cassandra.cql3.Lists:362* :

{code:java}
            CellPath elementPath = existingRow.getComplexColumnData(column).getCellByIndex(idx).path();
            if (value == null)
            {
                params.addTombstone(column);
            }
            else if (value != ByteBufferUtil.UNSET_BYTE_BUFFER)
            {
                params.addCell(column, elementPath, value);
            }
{code}

 In the if block, it seems we do not pass the CellPath as it should be and it makes the asertion _assert column.isComplex() == (path != null);_ fails at 
*org.apache.cassandra.db.rows:49*

{code:java}
    public BufferCell(ColumnDefinition column, long timestamp, int ttl, int localDeletionTime, ByteBuffer value, CellPath path)
    {
        super(column);
        assert column.isComplex() == (path != null);
        ....
   }
{code}

 Another remark about the code block in *org.apache.cassandra.cql3.Lists:362*, there is an *if/else if* but there is no final *else* block to catch all other alternatives, is it *intended* or just an oversight ?"	CASSANDRA	Resolved	10002	1	9435	regression
12712320	Followup to 6914: null handling, duplicate column in resultSet and cleanup	"The patch for CASSANDRA-6914 left a few stuffs not properly handled:
# A condition like {{IF m['foo'] = null}} is not handled and throw a NPE.
# It's using ByteBuffer.equals() to compare 2 collection values which is generally incorrect (the actual comparator should be used).
# If 2 conditions on 2 elements of the same collection were provided and the CAS failed, then the collection was duplicated in the resultSet.
# The ColumnCondition.WithVariables was generally a bit inefficient/ugly: it can lead to bind multiple times the same terms which is unnecessary. It's cleaner to directly create a condition with bound values."	CASSANDRA	Resolved	10003	1	9435	LWT
12507136	Cli should be able to specify a limit for get_slice	"The cli doesn't allow
{noformat}
get cf['k'] limit 5;
{noformat}
but should. Actually it should probably allow
{noformat}
get cf['k']['c':'g'] limit 10;
{noformat}"	CASSANDRA	Resolved	10003	4	9435	cli
12544647	Explore not returning range ghosts	"This ticket proposes to remove range ghosts in CQL3.
The basic argument is that range ghosts confuses users a lot and don't add any value since range ghost don't allow to distinguish between the two following case:
* the row is deleted
* the row is not deleted but don't have data for the provided filter"	CASSANDRA	Resolved	10002	7	9435	cql3
12674276	NPE in system.log	"I wrote a stresstest to test C* and my code that uses CAS heavily. I see strange exception messages in logs:
{noformat}
ERROR [MutationStage:320] 2013-10-17 13:59:10,710 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:320,5,main]
java.lang.NullPointerException
ERROR [MutationStage:328] 2013-10-17 13:59:10,718 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:328,5,main]
java.lang.NullPointerException
ERROR [MutationStage:327] 2013-10-17 13:59:10,732 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:327,5,main]
java.lang.NullPointerException
ERROR [MutationStage:325] 2013-10-17 13:59:10,750 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:325,5,main]
java.lang.NullPointerException
ERROR [MutationStage:326] 2013-10-17 13:59:10,762 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:326,5,main]
java.lang.NullPointerException
ERROR [MutationStage:330] 2013-10-17 13:59:10,768 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:330,5,main]
java.lang.NullPointerException
ERROR [MutationStage:331] 2013-10-17 13:59:10,775 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:331,5,main]
java.lang.NullPointerException
ERROR [MutationStage:334] 2013-10-17 13:59:10,789 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:334,5,main]
java.lang.NullPointerException
ERROR [MutationStage:329] 2013-10-17 13:59:10,803 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:329,5,main]
java.lang.NullPointerException
ERROR [MutationStage:335] 2013-10-17 13:59:10,812 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:335,5,main]
java.lang.NullPointerException
ERROR [MutationStage:333] 2013-10-17 13:59:10,826 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:333,5,main]
java.lang.NullPointerException
ERROR [MutationStage:332] 2013-10-17 13:59:10,834 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:332,5,main]
java.lang.NullPointerException
ERROR [MutationStage:337] 2013-10-17 13:59:10,842 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:337,5,main]
java.lang.NullPointerException
ERROR [MutationStage:336] 2013-10-17 13:59:10,859 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:336,5,main]
java.lang.NullPointerException
ERROR [MutationStage:338] 2013-10-17 13:59:10,870 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:338,5,main]
java.lang.NullPointerException
ERROR [MutationStage:339] 2013-10-17 13:59:10,884 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:339,5,main]
java.lang.NullPointerException
ERROR [MutationStage:341] 2013-10-17 13:59:10,894 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:341,5,main]
java.lang.NullPointerException
ERROR [MutationStage:340] 2013-10-17 13:59:10,910 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:340,5,main]
java.lang.NullPointerException
ERROR [MutationStage:344] 2013-10-17 13:59:10,920 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:344,5,main]
java.lang.NullPointerException
{noformat}"	CASSANDRA	Resolved	10002	1	9435	npe, nullpointerexception
12552645	Minor CQL3 fixes	"The goal of this ticket is to be the home for a number of minor fixes/improvements in CQL3 that I didn't felt warranted a ticket each. It includes 4 patches:
* The first one fixes the grammar for float constants, so as to not recognize 3.-3, but to actually allow 3. (i.e, with radix point but with the fractional part left blank)
* The second one correctly detect the (invalid) case where a table is created with COMPACT STORAGE but without any 'clustering keys'.
* The third one fixes COUNT, first by making sure both COUNT(*) and COUNT(1) are correctly recognized and also by ""processing"" the internal row before counting, are there isn't a 1-to-1 correspondence between internal rows and CQL rows in CQL3. The grammar change in this patch actually rely on CASSANDRA-4184
* The fourth and last patch disallows the counter type for keys (i.e. any column part of the PRIMARY KEY) as it is completely non-sensical and will only led to confusion.
"	CASSANDRA	Resolved	10003	1	9435	cql3
12600870	Using 'key' as primary key throws exception when using CQL2	"When I run following CQL on trunk, throws exception (only in CQL2). This statement used to work and I think something is broken after CASSANDRA-4179.

{code}
CREATE TABLE Standard1 (key ascii PRIMARY KEY, c0 ascii);
{code}

Exception is:

{code}
ERROR [Thrift:1] 2012-07-31 09:54:02,585 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:166)
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:123)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:49)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:45)
        at org.apache.cassandra.cql3.CFDefinition.getKeyId(CFDefinition.java:167)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:81)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1382)
        at org.apache.cassandra.config.CFMetaData.keyAliases(CFMetaData.java:235)
        at org.apache.cassandra.cql.CreateColumnFamilyStatement.getCFMetaData(CreateColumnFamilyStatement.java:170)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:692)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:846)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}"	CASSANDRA	Resolved	10003	1	9435	cql
12532195	CounterColumnFamily Compaction error (ArrayIndexOutOfBoundsException)	"On a single node, I'm seeing the following error when trying to compact a CounterColumnFamily. This appears to have started with version 1.0.3.

nodetool -h localhost compact TRProd MetricsAllTime
Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:250)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1471)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1523)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.cassandra.utils.ByteBufferUtil.arrayCopy(ByteBufferUtil.java:292)
	at org.apache.cassandra.db.context.CounterContext$ContextState.copyTo(CounterContext.java:792)
	at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:709)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:260)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:306)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:271)
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:86)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:102)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:133)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:116)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:277)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more"	CASSANDRA	Resolved	10002	1	9435	compaction
12539666	need forked language document	The language doc ({{doc/cql/CQL.textile}}) needs to be forked for CQLv3 and updated accordingly.	CASSANDRA	Resolved	10002	7	9435	cql
12901262	Specialize MultiCBuilder when building a single clustering	"{{MultiCBuilder}} is used to build the {{Clustering}} and {{Slice.Bound}} used by  queries. As the name implies, it's able to build multiple {{Clustering}}/{{Slice.Bound}} for when we have {{IN}}, but most queries don't use {{IN}} and in this (frequent) case, {{MultiCBuilder}} creates quite a bit more objects that would be necessary (it creates 2 lists for its {{elementsList}}, then a {{CBuilder}} and a {{BTreeSet.Builder}} (even though we know the resulting set will have only one element in this case)). Without being huge, this does show up as non entirely negligible when profiling some simple stress.

We can easily know if the query has a {{IN}} and so we can know when only a single {{Clustering}}/{{Slice.Bound}} is built, and we can specialize the implementation in that case to be less wasteful."	CASSANDRA	Resolved	10003	4	9435	performance
12595294	CQL3: allow definition with only a PK	"Currently, in CQL3 and contrarily to SQL, one cannot define a table having only a PK but no other columns. Related to that, a CQL always needs at least one column outside of the PK to be inserted to exist. All that may force people to add 'fake' value that they don't really need.

The goal of this ticket is to lift that limitation and allow table definition to have only a PK, and to have CQL rows exist even if only the PK has been inserted (in other words, to have CQL rows behave like SQL rows).

Following CASSANDRA-4329, one way to do that with the sparse-composite encoding CQL3 uses would be to insert as marker of the CQL row presence a CQL column with an empty name (the underlying column name won't be empty though since it's a composite). The drawback though is that we will need to insert that marker with every insert to the CQL row (in other word, we'll add a slight overhead to the size of each write). The pros is that if we have such marker for the CQL row presence, we will be able to reoptimize back queries that select only a few columns (since following CASSANDRA-3982 we query all columns of a CQL row every time).
"	CASSANDRA	Resolved	10002	4	9435	cql3
12669202	'Internal application error' on SELECT .. WHERE col1=val AND col2 IN (1,2)	"Query with error: SELECT * FROM user WHERE login='nsv' AND st IN ('1','2') ALLOW FILTERING;

Query works:
SELECT * FROM user WHERE login='nsv' AND st IN ('1') ALLOW FILTERING;
-- Single item inside IN

Table definition: 
CREATE COLUMNFAMILY user (
     KEY uuid PRIMARY KEY,
     name text,
     avatar text,
     email text,
     phone text,
     login text,
     pw text,
     st text
);

From /var/log/cassandra/output.log:
ERROR 11:58:52,454 Internal error processing execute_cql3_query
java.lang.AssertionError
	at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:749)
	at org.apache.cassandra.cql3.statements.SelectStatement.getRangeCommand(SelectStatement.java:303)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:155)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:108)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1920)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4372)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4356)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

"	CASSANDRA	Resolved	10002	1	9435	cql3
12539665	KEY IN (...) queries do not work	{{...KEY IN (...)}} queries fail due to faulty validation.  A pull request for cassandra-dtest was opened that demonstrates this: https://github.com/riptano/cassandra-dtest/pull/2	CASSANDRA	Resolved	10002	7	9435	cql
12995126	List Append order is wrong	"""INSERT INTO collection_type(key,normal_column,list_column) VALUES ('k','value',[ '#293847','#323442' ]);""

""UPDATE collection_type SET list_column=list_column+'#611987' WHERE key='k`;""

Using 2.1.7.1 java driver to run Update query, the output is: '#611987', '#293847','#323442'

Using DevCenter 1.3.1 to execute Update query, result is in correct order: '#293847','#323442', '#611987'

The error happened in 3 node cluster. In local, one node is working properly.
(all Cassandra 2.1.13. )

Is it related to internal message processing?"	CASSANDRA	Open	10002	1	9435	remove-reopen
12652558	CQL support for updating multiple rows in a partition using CAS	This is currently supported via Thrift but not via CQL. 	CASSANDRA	Resolved	10003	4	9435	LWT, cql3
12702597	Unintended update with conditional statement	"After updated to 2.0.6, I have encountered the strange behavior of conditional updates.

When I executed CQL like UPDATE test SET value = ? WHERE id = ? IF value = ? in concurrent, sometimes cassandra returns true even if value is not satisfied the condition.

I have attached the program which reproduce this issue. The program works fine in cassandra 2.0.5. But it seems that resets values while execution in 2.0.6."	CASSANDRA	Resolved	10002	1	9435	LWT, qa-resolved
12535723	Support collection (list, set, and map) value types in CQL	Composite columns introduce the ability to have arbitrarily nested data in a Cassandra row.  We should expose this through CQL.	CASSANDRA	Resolved	10002	2	9435	cql
12553679	COMPACT STORAGE should not require a value to be aliased	It's legitimate to only need the column name in a schema, e.g., system.NodeIdInfo.	CASSANDRA	Resolved	10002	1	9435	cql3
12985848	dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_next_2_1_x_To_head_trunk.select_with_alias_test	"example failure:

http://cassci.datastax.com/job/upgrade_tests-all-custom_branch_runs/37/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_next_2_1_x_To_head_trunk/select_with_alias_test

Failed on CassCI build upgrade_tests-all-custom_branch_runs #37

This is just a problem with different error messages across C* versions. Someone needs to do the legwork of figuring out what is required where, and filtering. The query is failing correctly."	CASSANDRA	Resolved	10002	4	9435	dtest
12721816	Native Protocol V3 CREATE Response	"Native protocol v3 changes the EVENT (opcode 12) SCHEMA_CHANGE to include the target type that changed : <change><target><keyspace><name>.

The RESULT (opcode 8) SCHEMA_CHANGE has the old layout (<change><keyspace><table>.

Is this difference intentional or does the protocol spec needs change for RESULT/SCHEMA_CHANGE?"	CASSANDRA	Resolved	10002	3	9435	protocol
12529516	Avoid large array allocation for compressed chunk offsets	"For each compressed file we keep the chunk offsets in memory (a long[]). The size of this array is directly proportional to the sstable file and the chunk_length_kb used, but say for a 64GB sstable, we're talking ~8MB in memory by default.

Without being absolutely huge, this probably makes the life of the GC harder than necessary for the same reasons than CASSANDRA-2466, and this ticket proposes the same solution, i.e. to break down those big array into smaller ones to ease fragmentation.

Note that this is only a concern for size tiered compaction. But until leveled compaction is battle tested, the default and we know nobody uses size tiered anymore, it's probably worth making the optimization."	CASSANDRA	Resolved	10003	4	9435	compression
12552685	CQL3: make some keywords unreserved	CQL has quite a few keywords. Currently all of them are reserved, but this is not always necessary. PostreSQL for instance distinguish between reserved keywords and non-reserved ones, and allow things like {{key}}, {{timestamp}} or {{type}} as identifiers. I suggest we do the same as convenience for the user.	CASSANDRA	Resolved	10003	4	9435	cql3
12659025	Thrift cas() method crashes if input columns are not sorted.	"CassandraServer#cas() use UnsortedColumns for the ""updates"", which might result later to a
{noformat}
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:117)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:119)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:96)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:91)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:139)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:128)
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:175)
        at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:135)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82)
{noformat}"	CASSANDRA	Resolved	10002	1	9435	LWT
12735784	UDF cleanups (#7395 follow-up)	"The current code for UDF is largely not reusing the pre-existing mechanics/code for native/hardcoded functions. I don't see a good reason for that but I do see downsides: it's more code to maintain and makes it much easier to have inconsitent behavior between hard-coded and user-defined function. More concretely, {{UDFRegistery/UDFFunctionOverloads}} fundamentally do the same thing than {{Functions}}, we should just merge both. I'm also not sure there is a need for both {{UFMetadata}} and {{UDFunction}} since {{UFMetadata}} really only store infos on a given function (contrarly to what the javadoc pretends).  I suggest we consolidate all this to cleanup the code, but also as a way to fix 2 problems that the UDF code has but that the existing code for ""native"" functions don't:
* if there is multiple overloads of a function, the UDF code picks the first version whose argument types are compatible with the concrete arguments provided. This is broken for bind markers: we don't know the type of markers and so the first function match may not at all be what the user want. The only sensible choice is to detect that type of ambiguity and reject the query, asking the user to explicitly type-cast their bind marker (which is what the code for hard-coded function does).
* the UDF code builds a function signature using the CQL type names of the arguments and use that to distinguish multiple overrides in the schema. This means in particular that {{f(v text)}} and {{f(v varchar)}} are considered distinct, which is wrong since CQL considers {{varchar}} as a simple alias of {{text}}. And in fact, the function resolution does consider them aliases leading to seemingly broken behavior.

There is a few other small problems that I'm proposing to fix while doing this cleanup:
* Function creation only use the function name when checking if the function exists, which is not enough since we allow multiple over-loadings. You can bypass the check by using ""OR REPLACE"" but that's obviously broken.
* {{IF NOT EXISTS}} for function creation is broken.
* The code allows to replace a function (with {{OR REPLACE}}) by a new function with an incompatible return type. Imo that's dodgy and we should refuse it (users can still drop and re-create the method if they really want).
"	CASSANDRA	Resolved	10002	1	9435	cql
12539160	CQL 3.0	"This ticket is a reformulation/generalization of CASSANDRA-2474. The core change of CQL 3.0 is to introduce the new syntaxes that were discussed in CASSANDRA-2474 that allow to:
# Provide a better/more native support for wide rows, using the idea of transposed vie.
# The generalization to composite columns.

The attached text file create_cf_syntaxes.txt recall the new syntaxes introduced.

The changes proposed above allow (and strongly suggest in some cases) a number of other changes to the language that this ticket proposes to explore/implement (more details coming in the comments)."	CASSANDRA	Resolved	10002	2	9435	cql
12525291	Bootstrap is broken in 1.0.0-rc1	"The commit of #3219 introduced two bugs: the condition to bootstrap is that there *are* non-system tables instead, a _not_ is missing, and the setToken() was wrongly push up into the ""I'm not bootstrapping"" block so a boostrapping node was left in the joining state."	CASSANDRA	Resolved	10000	1	9435	bootstrap
12950195	ERROR [CompactionExecutor] CassandraDaemon.java Exception in thread	"Hey. Please help me with a problem. Recently I updated to 3.3.0 and this problem appeared in the logs.

ERROR [CompactionExecutor:2458] 2016-03-10 12:41:15,127 CassandraDaemon.java:195 - Exception in thread Thread[CompactionExecutor:2458,1,main]
java.lang.AssertionError: null
at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:49) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:88) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BufferCell.tombstone(BufferCell.java:83) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BufferCell.purge(BufferCell.java:175) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.lambda$purge$107(ComplexColumnData.java:165) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$68/1224572667.apply(Unknown Source) ~[na:na]
at org.apache.cassandra.utils.btree.BTree$FiltrationTracker.apply(BTree.java:650) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:693) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:668) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.transformAndFilter(ComplexColumnData.java:170) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.purge(ComplexColumnData.java:165) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.ComplexColumnData.purge(ComplexColumnData.java:43) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow.lambda$purge$102(BTreeRow.java:333) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow$$Lambda$67/1968133513.apply(Unknown Source) ~[na:na]
at org.apache.cassandra.utils.btree.BTree$FiltrationTracker.apply(BTree.java:650) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:693) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.btree.BTree.transformAndFilter(BTree.java:668) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow.transformAndFilter(BTreeRow.java:338) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.rows.BTreeRow.purge(BTreeRow.java:333) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.partitions.PurgeFunction.applyToRow(PurgeFunction.java:88) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:116) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:38) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:64) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:24) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:76) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:177) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:78) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[apache-cassandra-3.3.0.jar:3.3.0]
at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264) ~[apache-cassandra-3.3.0.jar:3.3.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]"	CASSANDRA	Resolved	10002	1	9435	error
12529483	CompressionMetadata is not shared across threads, we create a new one for each read	"The CompressionMetada holds the compressed block offsets in memory. Without being absolutely huge, this is still of non-negligible size as soon as you have a bit of data in the DB. Reallocating this for each read is a very bad idea.

Note that this only affect range queries, since ""normal"" queries uses CompressedSegmentedFile that does reuse a unique CompressionMetadata instance.

( Background: http://thread.gmane.org/gmane.comp.db.cassandra.user/21362 )"	CASSANDRA	Resolved	10002	1	9435	compression
12556274	CQL3 range query with secondary index fails	"This query fails:
select * from indextest where setid = 0 and row < 1;
when there's a secondary index on 'setid'; row isn't the primary key.

{code:title=CQL3}
bin$ ./cqlsh --cql3
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> create table indextest (id int primary key, row int, setid int);
cqlsh:warehouse1> create index indextest_setid_idx on indextest (setid);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (0, 0, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (1, 1, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (2, 2, 0);
cqlsh:warehouse1> select * from indextest where setid = 0;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
  2 |   2 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row = 1;
 id | row | setid
----+-----+-------
  1 |   1 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
TSocket read 0 bytes
{code}

{code:title=Error message}
ERROR 13:36:23,544 Error occurred during processing of message.
java.lang.NullPointerException
  at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:546)
  at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:253)
  at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:132)
  at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
  at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
  at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
  at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:680)
{code}

Works fine in CQL2:
{code:title=CQL2}
bin$ ./cqlsh_uuid --cql2
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
 id | row | setid
----+-----+-------
  0 |   0 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 2;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
{code}"	CASSANDRA	Resolved	10003	1	9435	cql3, index
13027048	dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x.static_columns_with_distinct_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.X_dtest_upgrade/28/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/static_columns_with_distinct_test

{code}
Error Message

<Error from server: code=0000 [Server error] message=""java.io.IOError: java.io.IOException: Corrupt empty row found in unfiltered partition"">
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 46, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py"", line 4010, in static_columns_with_distinct_test
    rows = list(cursor.execute(""SELECT DISTINCT k, s1 FROM test2""))
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
{code}{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:7eac22dd41cb09e6d64fb5ac48b2cca3c8840cc8
Unexpected error in node2 log, error: 
ERROR [Native-Transport-Requests-2] 2016-12-08 03:20:04,861 Message.java:617 - Unexpected exception during request; channel = [id: 0xf4c13f2c, L:/127.0.0.2:9042 - R:/127.0.0.1:52112]
java.io.IOError: java.io.IOException: Corrupt empty row found in unfiltered partition
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:224) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:212) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:779) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:741) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:408) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:273) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:232) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:76) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:219) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:204) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.9.jar:3.9]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_51]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
Caused by: java.io.IOException: Corrupt empty row found in unfiltered partition
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:430) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:219) ~[apache-cassandra-3.9.jar:3.9]
	... 23 common frames omitted
{code}

Related failures: (~25)
http://cassci.datastax.com/job/cassandra-3.X_dtest_upgrade/28/testReport/"	CASSANDRA	Resolved	10000	1	9435	dtest, test-failure
12510951	Repair hangs if a neighbor has nothing to send 	"This is actually a streaming problem. If a StreamOutSession has nothing to transfer (i.e, no sstables have the requested ranges), it will not even initiate the transfer and simply close the session right away. The problem is that if the session was initiated by a remote end (through a StreamRequestMessage), the remote end will never be notified and never run his callback.
"	CASSANDRA	Resolved	10003	1	9435	repair, streaming
12703187	Map element is not allowed in CAS condition with DELETE/UPDATE query	"{code}
CREATE TABLE test (id int, data map<text,text>, PRIMARY KEY(id));

INSERT INTO test (id, data) VALUES (1,{'a':'1'});

DELETE FROM test WHERE id=1 IF data['a']=null;
Bad Request: line 1:40 missing EOF at '='

UPDATE test SET data['b']='2' WHERE id=1 IF data['a']='1';
Bad Request: line 1:53 missing EOF at '='
{code}
These queries was successfuly executed with cassandra 2.0.5, but don't work in 2.0.6 release"	CASSANDRA	Resolved	10002	1	9435	LWT
12513837	Avoids having replicate on write tasks stacking up at CL.ONE	"The counter design involves a read on the first replica during a write. At CL.ONE, this read is not involved in the latency of the operation (the write is acknowledged before). This means it is fairly easy to insert too quickly at CL.ONE and have the replicate on write tasks falling behind. The goal of this ticket is to protect against that.

An option could be to bound the replicate on write task queue so that write start to block once we have too much of those in the queue. Another option could be to drop the oldest tasks when they are too old, but it's probably a more unsafe option."	CASSANDRA	Resolved	10002	4	9435	counters
12553104	CQL3: improve experience with time uuid	"This ticket proposes to add a timeuuid type to CQL3. I know that the uuid type does support version 1 UUID (which is fine), but my rational is that time series is a very common use case for Cassandra. But when modeling time series, it seems to me that you'd almost always want to use time uuids rather than timestamps to avoid having to care about collision. In those case, using a timeuuid type would imo have the following advantages over simply uuid:
# the type convey the idea that this is really a date (but need to avoid collision). In other words, the 'time' in timeuuid has a documentation purpose.
# it validates that you do only insert a UUID v1. Inserting non-time based UUID when you really care about the time ordering is a important mistake, it's nice to validate this doesn't happen (it's one of the goal of the type after all)
# it'll allow to parse date values (which TimeUUIDType already does). Since timeuuid is really a date, it's useful and convenient to allow '2012-04-27 11:32:02' as a value.

I'll note that imho there really is no reason not to at least allow 3) and even if there is strong opposition to adding a new timeuuid type (though I don't see why that would be a big deal) we could add the parsing of date to uuid. But I do think personally that 1) and 2) are equally important and warrant the addition of timeuuid (and it'll feel less random to parse date as timeuuid than to do it for uuid).
"	CASSANDRA	Resolved	10003	4	9435	cql3
12515424	Batch mutation of counters in multiple supercolumns throws an exception during replication.	"Steps to reproduce:
* Perform a batch mutation of more than one counter in more than one super-column in the same column-family.
* The following exception is thrown during replication:

DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 CounterMutationVerbHandler.java (line 52) Applying forwarded CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:8@1311696312648,]),SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:8@1311696312648,]),])]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 StorageProxy.java (line 432) insert writing local & replicate CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[cf1]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,654 Table.java (line 398) applying mutation of row 4ae71336e44bf9bf
ERROR [ReplicateOnWriteStage:125] 2011-07-26 17:05:12,655 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[ReplicateOnWriteStage:125,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:123)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:120)
        at org.apache.cassandra.service.StorageProxy$5$1.runMayThrow(StorageProxy.java:455)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
"	CASSANDRA	Resolved	10000	1	9435	batch_mutate, counters, supercolumns
12533701	Cache saving broken on windows	CASSANDRA-1740 broke cache saving on Windows.	CASSANDRA	Resolved	10003	1	9435	windows
12986267	CAS Reads Inconsistencies 	"While looking at the CAS code in Cassandra, I found a potential issue with CAS Reads. Here is how it can happen with RF=3

1) You issue a CAS Write and it fails in the propose phase. A machine replies true to a propose and saves the commit in accepted filed. The other two machines B and C does not get to the accept phase. 

Current state is that machine A has this commit in paxos table as accepted but not committed and B and C does not. 

2) Issue a CAS Read and it goes to only B and C. You wont be able to read the value written in step 1. This step is as if nothing is inflight. 

3) Issue another CAS Read and it goes to A and B. Now we will discover that there is something inflight from A and will propose and commit it with the current ballot. Now we can read the value written in step 1 as part of this CAS read.

If we skip step 3 and instead run step 4, we will never learn about value written in step 1. 

4. Issue a CAS Write and it involves only B and C. This will succeed and commit a different value than step 1. Step 1 value will never be seen again and was never seen before. 



If you read the Lamport paxos made simple paper and read section 2.3. It talks about this issue which is how learners can find out if majority of the acceptors have accepted the proposal. 

In step 3, it is correct that we propose the value again since we dont know if it was accepted by majority of acceptors. When we ask majority of acceptors, and more than one acceptors but not majority has something in flight, we have no way of knowing if it is accepted by majority of acceptors. So this behavior is correct. 

However we need to fix step 2, since it caused reads to not be linearizable with respect to writes and other reads. In this case, we know that majority of acceptors have no inflight commit which means we have majority that nothing was accepted by majority. I think we should run a propose step here with empty commit and that will cause write written in step 1 to not be visible ever after. 

With this fix, we will either see data written in step 1 on next serial read or will never see it which is what we want. 
"	CASSANDRA	Resolved	10002	1	9435	LWT, pull-request-available
12551206	ORDER BY ... DESC reverses comparrison predicates in WHERE	"When issuing a cql select statement with an ORDER BY ... DESC clause the comparison predicates in the WHERE clause gets reversed. 

Example: (see also attached)

SELECT number FROM test WHERE number < 3 ORDER BY number DESC

returns the results expected of WHERE number > 3"	CASSANDRA	Resolved	10002	1	9435	cql3
12523364	Nodes started at the same time end up with the same token	"Since autoboostrap is defaulted to on when you start a cluster at once (http://screenr.com/5G6) you can end up with nodes being assigned the same token.

{code}
INFO 17:34:55,688 Node /67.23.43.14 is now part of the cluster
 INFO 17:34:55,698 InetAddress /67.23.43.14 is now UP
 INFO 17:34:55,698 Nodes /67.23.43.14 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /67.23.43.14
 INFO 17:34:55,698 Node /98.129.220.182 is now part of the cluster
 INFO 17:34:55,698 InetAddress /98.129.220.182 is now UP
 INFO 17:34:55,698 Nodes /98.129.220.182 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /98.129.220.182
{code}"	CASSANDRA	Resolved	10002	1	9435	bootstrap
12533605	CFMetaData conversions to Thrift/Native schema should be inverse one of the other	"In other word, it would probably be a good idea to have:
{noformat}
  cfm == CFMetadata.fromThrift(cfm.toThrift())
  cfm == CFMetadata.fromSchema(cfm.toSchema())
{noformat}
In particular, we could have unit tests to check that, which would avoid things like CASSANDRA-3558.

It is not the case today for thrift because of the keyAlias. For some reason, if the keyAlias is not set, we return with toThrift() the default alias. I don't think this serves any purpose though."	CASSANDRA	Resolved	10002	4	9435	avro, thrift
12544623	Consider providing error code with exceptions (and documenting them)	It could be a good idea to assign documented error code for the different exception raised. Currently, one may have to parse the exception string (say if one wants to know if its 'create keyspace' failed because the keyspace already exists versus other kind of exception), but it means we cannot improve the error message at the risk of breaking client code. Adding documented error codes with the message would avoid this.	CASSANDRA	Resolved	10002	7	9435	cql3
12557052	Can't specify certain keyspace properties in CQL	"A user using EC2MultiRegionSnitch, where the datacenter name has to match the AWS region names, will not be able to specify a keyspace's replica counts for those datacenters using CQL. AWS region names contain hyphens, which are not valid identifiers in CQL, and CQL keyspace/columnfamily properties must be identifiers or identifiers separated by colons.

Example:

{noformat}
CREATE KEYSPACE Foo
  WITH strategy_class = 'NetworkTopologyStrategy'
      AND strategy_options:""us-east""=1
      AND strategy_options:""us-west""=1;
{noformat}

(see http://mail-archives.apache.org/mod_mbox/cassandra-user/201205.mbox/browser for context)

..will not currently work, with or without the double quotes.

CQL should either allow hyphens in COMPIDENT, or allow quoted parts of a COMPIDENT token."	CASSANDRA	Resolved	10003	1	9435	cql, cql3
12605133	CQL queries using LIMIT sometimes missing results	"In certain conditions, CQL queries using LIMIT clauses are not being given all of the expected results (whether unset column values or missing rows).

Here are the condition sets I've been able to identify:

First mode: all rows are returned, but in the last row of results, all columns which are not part of the primary key receive no values, except for the first non-primary-key column. Conditions:

 * Table has a multi-component primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

Second mode: result has fewer rows than it should, lower than both the LIMIT and the actual number of matching rows. Conditions:

 * Table has a single-column primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

It would make sense to me that this would have started with CASSANDRA-4329, but bisecting indicates that this behavior started with commit 91bdf7fb4220b27e9566c6673bf5dbd14153017c, implementing CASSANDRA-3647.

Test case for the first failure mode:

{noformat}
DROP KEYSPACE test;

CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int,
    b int,
    c int,
    d int,
    e int,
    PRIMARY KEY (a, b)
);

INSERT INTO testcf (a, b, c, d, e) VALUES (1, 11, 111, 1111, 11111);
INSERT INTO testcf (a, b, c, d, e) VALUES (2, 22, 222, 2222, 22222);
INSERT INTO testcf (a, b, c, d, e) VALUES (3, 33, 333, 3333, 33333);
INSERT INTO testcf (a, b, c, d, e) VALUES (4, 44, 444, 4444, 44444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- columns d and e in result row are null
SELECT * FROM testcf LIMIT 2; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 3; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 4; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 5; -- results are correct (4 rows returned)
{noformat}

Test case for the second failure mode:

{noformat}
CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int primary key,
    b int,
    c int,
);

INSERT INTO testcf (a, b, c) VALUES (1, 11, 111);
INSERT INTO testcf (a, b, c) VALUES (2, 22, 222);
INSERT INTO testcf (a, b, c) VALUES (3, 33, 333);
INSERT INTO testcf (a, b, c) VALUES (4, 44, 444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- gives 1 row
SELECT * FROM testcf LIMIT 2; -- gives 1 row
SELECT * FROM testcf LIMIT 3; -- gives 2 rows
SELECT * FROM testcf LIMIT 4; -- gives 2 rows
SELECT * FROM testcf LIMIT 5; -- gives 3 rows
{noformat}"	CASSANDRA	Resolved	10002	1	9435	cql, cql3
12544061	Supercolumn serialization assertion failure	"As reported at http://mail-archives.apache.org/mod_mbox/cassandra-user/201202.mbox/%3CCADJL=w5kH5TEQXOwhTn5Jm3cmR4Rj=NfjcqLryXV7pLyASi95A@mail.gmail.com%3E,

{noformat}
ERROR 10:51:44,282 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 4690 to accomodate data size
of 2347 (predicted 2344) for RowMutation(keyspace='Player',
key='36336138643338652d366162302d343334392d383466302d356166643863353133356465',
modifications=[ColumnFamily(PlayerCity [SuperColumn(owneditem_1019
[]),SuperColumn(owneditem_1024 []),SuperColumn(owneditem_1026
[]),SuperColumn(owneditem_1074 []),SuperColumn(owneditem_1077
[]),SuperColumn(owneditem_1084 []),SuperColumn(owneditem_1094
[]),SuperColumn(owneditem_1130 []),SuperColumn(owneditem_1136
[]),SuperColumn(owneditem_1141 []),SuperColumn(owneditem_1142
[]),SuperColumn(owneditem_1145 []),SuperColumn(owneditem_1218
[636f6e6e6563746564:false:5@1329648704269002
,63757272656e744865616c7468:false:3@1329648704269006
,656e64436f6e737472756374696f6e54696d65:false:13@1329648704269007
,6964:false:4@1329648704269000,6974656d4964:false:15@1329648704269001
,6c61737444657374726f79656454696d65:false:1@1329648704269008
,6c61737454696d65436f6c6c6563746564:false:13@1329648704269005
,736b696e4964:false:7@1329648704269009,78:false:4@1329648704269003
,79:false:3@1329648704269004,]),SuperColumn(owneditem_133
[]),SuperColumn(owneditem_134 []),SuperColumn(owneditem_135
[]),SuperColumn(owneditem_141 []),SuperColumn(owneditem_147
[]),SuperColumn(owneditem_154 []),SuperColumn(owneditem_159
[]),SuperColumn(owneditem_171 []),SuperColumn(owneditem_253
[]),SuperColumn(owneditem_422 []),SuperColumn(owneditem_438
[]),SuperColumn(owneditem_515 []),SuperColumn(owneditem_521
[]),SuperColumn(owneditem_523 []),SuperColumn(owneditem_525
[]),SuperColumn(owneditem_562 []),SuperColumn(owneditem_61
[]),SuperColumn(owneditem_634 []),SuperColumn(owneditem_636
[]),SuperColumn(owneditem_71 []),SuperColumn(owneditem_712
[]),SuperColumn(owneditem_720 []),SuperColumn(owneditem_728
[]),SuperColumn(owneditem_787 []),SuperColumn(owneditem_797
[]),SuperColumn(owneditem_798 []),SuperColumn(owneditem_838
[]),SuperColumn(owneditem_842 []),SuperColumn(owneditem_847
[]),SuperColumn(owneditem_849 []),SuperColumn(owneditem_851
[]),SuperColumn(owneditem_852 []),SuperColumn(owneditem_853
[]),SuperColumn(owneditem_854 []),SuperColumn(owneditem_857
[]),SuperColumn(owneditem_858 []),SuperColumn(owneditem_874
[]),SuperColumn(owneditem_884 []),SuperColumn(owneditem_886
[]),SuperColumn(owneditem_908 []),SuperColumn(owneditem_91
[]),SuperColumn(owneditem_911 []),SuperColumn(owneditem_930
[]),SuperColumn(owneditem_934 []),SuperColumn(owneditem_937
[]),SuperColumn(owneditem_944 []),SuperColumn(owneditem_945
[]),SuperColumn(owneditem_962 []),SuperColumn(owneditem_963
[]),SuperColumn(owneditem_964 []),])])
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
"	CASSANDRA	Resolved	10002	1	9435	datastax_qa
12642448	isRunning flag set prematurely in org.apache.cassandra.transport.Server	"In org.apache.cassandra.transport.Server, the start() method sets the isRunning flag before calling the run() method. In the event of an initialization error like a port conflict an exception will be thrown at line 136 which is,

    Channel channel = bootstrap.bind(socket);

It seems like it might make more sense to set the isRunning flag after binding to the socket. I have a tool that deploys a node and then verifies it is ready to receive CQL requests. I do this via JMX. Unless I use a delay before making that check, the JMX call will return true even though there is a port conflict. 

"	CASSANDRA	Resolved	10003	1	9435	jmx, server
12844931	RangeTombstonListTest sometimes fails on trunk	"I've seen random failures with {{RangeTombstoneList.addAllRandomTest}}. The problem is 2 inequalities in {{RangeTombstoneList.insertFrom}} that should be inclusive rather than strict when we deal with boundaries between range. In practice, that makes us consider range like {{[3, 3)}} during addition, which is non-sensical.

Attaching patch as well as a test that reproduce (extracted from {{addAllRandomTest}} with a failing seed)."	CASSANDRA	Resolved	10002	4	9435	test
12552933	cql delete does not delete	"tested in 1.1 and trunk branch on a single node:
{panel}
cqlsh:test> create table testcf_old ( username varchar , id int , name varchar , stuff varchar, primary key(username,id,name)) with compact storage;
cqlsh:test> insert into testcf_old ( username , id , name , stuff ) values ('abc', 2, 'rst', 'some other bunch of craps');
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps

cqlsh:test> delete from testcf_old where username = 'abc' and id =2;
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps
{panel}

same also when not using compact:
{panel}
cqlsh:test> create table testcf ( username varchar , id int , name varchar , stuff varchar, primary key(username,id));
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps

cqlsh:test> delete from testcf where username = 'abc' and id =2;
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps
{panel}"	CASSANDRA	Resolved	10002	1	9435	cql3
12511343	Repair doesn't synchronize merkle tree creation properly	"Being a little slow, I just realized after having opened CASSANDRA-2811 and CASSANDRA-2815 that there is a more general problem with repair.

When a repair is started, it will send a number of merkle tree to its neighbor as well as himself and assume for correction that the building of those trees will be started on every node roughly at the same time (if not, we end up comparing data snapshot at different time and will thus mistakenly repair a lot of useless data). This is bogus for many reasons:
* Because validation compaction runs on the same executor that other compaction, the start of the validation on the different node is subject to other compactions. 0.8 mitigates this in a way by being multi-threaded (and thus there is less change to be blocked a long time by a long running compaction), but the compaction executor being bounded, its still a problem)
* if you run a nodetool repair without arguments, it will repair every CFs. As a consequence it will generate lots of merkle tree requests and all of those requests will be issued at the same time. Because even in 0.8 the compaction executor is bounded, some of those validations will end up being queued behind the first ones. Even assuming that the different validation are submitted in the same order on each node (which isn't guaranteed either), there is no guarantee that on all nodes, the first validation will take the same time, hence desynchronizing the queued ones.

Overall, it is important for the precision of repair that for a given CF and range (which is the unit at which trees are computed), we make sure that all node will start the validation at the same time (or, since we can't do magic, as close as possible).

One (reasonably simple) proposition to fix this would be to have repair schedule validation compactions across nodes one by one (i.e, one CF/range at a time), waiting for all nodes to return their tree before submitting the next request. Then on each node, we should make sure that the node will start the validation compaction as soon as requested. For that, we probably want to have a specific executor for validation compaction and:
* either we fail the whole repair whenever one node is not able to execute the validation compaction right away (because no thread are available right away).
* we simply tell the user that if he start too many repairs in parallel, he may start seeing some of those repairing more data than it should.
"	CASSANDRA	Resolved	10002	1	9435	repair
12659037	StorageProxy#cas() doesn't order columns names correctly when querying	"When querying columns for CAS, we build the SortedSet with:
{noformat}
new NamesQueryFilter(ImmutableSortedSet.copyOf(expected.getColumnNames())
{noformat}
but ImmutableSortedSet.copyOf() uses the natural order of keys unless a comparator is given, which is not what we want."	CASSANDRA	Resolved	10002	1	9435	LWT
12751333	Deleting columns breaks secondary index on clustering column	"Removing items from a set breaks index for field {{id}}:

{noformat}
cqlsh:cs> CREATE TABLE buckets (
      ...   tenant int,
      ...   id int,
      ...   items set<text>,
      ...   PRIMARY KEY (tenant, id)
      ... );
cqlsh:cs> CREATE INDEX buckets_ids ON buckets(id);
cqlsh:cs> INSERT INTO buckets (tenant, id, items) VALUES (1, 1, {'foo', 'bar'});
cqlsh:cs> SELECT * FROM buckets;

 tenant | id | items
--------+----+----------------
      1 |  1 | {'bar', 'foo'}

(1 rows)

cqlsh:cs> SELECT * FROM buckets WHERE id = 1;

 tenant | id | items
--------+----+----------------
      1 |  1 | {'bar', 'foo'}

(1 rows)

cqlsh:cs> UPDATE buckets SET items=items-{'foo'} WHERE tenant=1 AND id=1;
cqlsh:cs> SELECT * FROM buckets;

 tenant | id | items
--------+----+---------
      1 |  1 | {'bar'}

(1 rows)

cqlsh:cs> SELECT * FROM buckets WHERE id = 1;

(0 rows)
{noformat}

Re-building the index fixes the issue:

{noformat}
cqlsh:cs> DROP INDEX buckets_ids;
cqlsh:cs> CREATE INDEX buckets_ids ON buckets(id);
cqlsh:cs> SELECT * FROM buckets WHERE id = 1;

 tenant | id | items
--------+----+---------
      1 |  1 | {'bar'}

(1 rows)
{noformat}

Adding items does not cause similar failure, only delete. Also didn't test if other collections are also affected(?)"	CASSANDRA	Resolved	10000	1	9435	qa-resolved
12701462	Native protocol V3	"I think we need a V3 of the protocol for 2.1. The things that this could/should includes are:
# Adding an optional Serial CL for protocol batches (like we have for QUERY and EXECUTE). It was an oversight of V2 of not adding it, and now that we can batch conditional updates, it's definitively missing.
# Proper type codes for UDT. This is not *strictly* needed to be able to support UDT since currently a UDT will be sent as a ""custom type"" with his fully class name + arguments. But parsing that is no fun nor convenient for clients. It's also not particular space efficient (though that's probably not a huge concern since with prepared statement you can avoid sending the ResultSet metadata every time).
# Serialization format for collections. Currently the serialization format only allow for 65K elements, each of 65K bytes size at most. While collections are not meant to store large amount of data, having the limitation in the protocol serialization format is the wrong way to deal with that. Concretely, the current workaround for CASSANDRA-5428 is ugly. I'll note that the current serialization format is also an obstacle to supporting null inside collections (whether or not we want to support null there is a good question, but here again I'm not sure being limited by the serialization format is a good idea).
# CASSANDRA-6178: I continue to believe that in many case it makes somewhat more sense to have the default timestamp provided by the client (this is a necessary condition for true idempotent retries in particular). I'm absolutely fine making that optional and leaving server-side generated timestamps by default, but since client can already provide timestamp in query string anyway, I don't see a big deal in making it easier for client driver to control that without messing with the query string.
# Optional names for values in QUERY messages: it has been brought to my attention that while V2 allows to send a query string with values for a one-roundtrip bind-and-execute, a driver can't really support named bind marker with that feature properly without parsing the query. The proposition is thus to make it (optionally) possible to ship the name of the marker each value is supposed to be bound to.

I think that 1) and 2) are enough reason to make a V3 (even if there is disagreement on the rest that is).

3) is a little bit more involved tbh but I do think having the current limitations bolted in the protocol serialization format is wrong in the long run, and it turns out that due to UDT we will start storing serialized collections internally so if we want to lift said limitation in the serialization format, we should do it now and everywhere, as doing it afterwards will be a lot more painful.

4) and 5) are probably somewhat more minor, but at the same time, both are completely optional (a driver won't have to support those if he doesn't want). They are really just about making things more flexible for client drivers and they are not particularly hard to support so I don't see too many reasons not to include them.

Last but not least, I know that some may find it wrong to do a new protocol version with each major of C*, so let me state my view here: I fully agree that we shouldn't make an habit of that in the long run and that's definitively *not* my objective. However, it would be silly to expect that we could get everything right and forget nothing in the very first version. It shouldn't be surprising that we'll have to burn a few versions (and there might be a few more yet) before getting something more stable and complete and I think that delaying the addition of stuffs that are useful to create some fake notion of stability would be even more silly. On the bright side, the additions of this V3 are comparatively much more simple to implement for a client that those of V2 (in fact, for clients that want to support UDT, it will probably require less effort to add the changes for this new version than to try to support UDT without it), so I do think we make good progress on getting the protocol stabilized 
"	CASSANDRA	Resolved	10002	2	9435	qa-resolved
12724685	Throw exception on unknown UDT field	"Currently, the following code:
{noformat}
CREATE TYPE foo (f : int);
CREATE TABLE test (k int PRIMARY KEY, v foo);

INSERT INTO test (k, v) VALUES (0, { s : ?})
{noformat}
will crash, because the {{s}} field is not part of type {{foo}} and it's not caught. The consequence being that the metadata for the bindMarker ends up being {{null}} and some NPE is thrown. We should throw a proper exception instead."	CASSANDRA	Resolved	10003	1	9435	cql
12501263	Promote row index	"The row index contains entries for configurably sized blocks of a wide row. For a row of appreciable size, the row index ends up directing the third seek (1. index, 2. row index, 3. content) to nearby the first column of a scan.

Since the row index is always used for wide rows, and since it contains information that tells us whether or not the 3rd seek is necessary (the column range or name we are trying to slice may not exist in a given sstable), promoting the row index into the sstable index would allow us to drop the maximum number of seeks for wide rows back to 2, and, more importantly, would allow sstables to be eliminated using only the index.

An example usecase that benefits greatly from this change is time series data in wide rows, where data is appended to the beginning or end of the row. Our existing compaction strategy gets lucky and clusters the oldest data in the oldest sstables: for queries to recently appended data, we would be able to eliminate wide rows using only the sstable index, rather than needing to seek into the data file to determine that it isn't interesting. For narrow rows, this change would have no effect, as they will not reach the threshold for indexing anyway.

A first cut design for this change would look very similar to the file format design proposed on #674: http://wiki.apache.org/cassandra/FileFormatDesignDoc: row keys clustered, column names clustered, and offsets clustered and delta encoded."	CASSANDRA	Resolved	10002	4	9435	index, timeseries
12551750	Add more general support for composites (to row key, column value)	"Currently CQL3 have a nice syntax for using composites in the column name (it's more than that in fact, it creates a whole new abstraction but let's say I'm talking implementation here). There is however 2 other place where composites could be used (again implementation wise): the row key and the column value. This ticket proposes to explore which of those make sense for CQL3 and how.

For the row key, I really think that CQL support makes sense. It's very common (and useful) to want to stuff composite information in a row key. Sharding a time serie (CASSANDRA-4176) is probably the best example but there is other.

For the column value it is less clear. CQL3 makes it very transparent and convenient to store multiple related values into multiple columns so maybe composites in a column value is much less needed. I do still see two cases for which it could be handy:
# to save some disk/memory space, if you do know it makes no sense to insert/read two value separatly.
# if you want to enforce that two values should not be inserted separatly. I.e. to enforce a form of ""constraint"" to avoid programatic error.

Those are not widely useful things, but my reasoning is that if whatever syntax we come up for ""grouping"" row key in a composite trivially extends to column values, why not support it.


As for syntax I have 3 suggestions (that are just that, suggestions):
# If we only care about allowing grouping for row keys:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value text,
    PRIMARY KEY ((name, month), ts)
)
{noformat}
# A syntax that could work for both grouping in row key and colum value:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value1 text,
    value2 text,
    GROUP (name, month) as key,
    GROUP (value1, value2),
    PRIMARY KEY (key, ts)
)
{noformat}
# An alternative to the preceding one:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value1 text,
    value2 text,
    GROUP (name, month) as key,
    GROUP (value1, value2),
    PRIMARY KEY (key, ts)
) WITH GROUP (name, month) AS key
   AND GROUP (value1, value2)
{noformat}"	CASSANDRA	Resolved	10003	7	9435	cql3
12957218	cqlsh fails to format collections when using aliases	"Given is a simple table. Selecting the columns without an alias works fine. However, if the map is selected using an alias, cqlsh fails to format the value.

{code}
create keyspace foo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE foo.foo (id int primary key, m map<int, text>);
insert into foo.foo (id, m) VALUES ( 1, {1: 'one', 2: 'two', 3:'three'});
insert into foo.foo (id, m) VALUES ( 2, {1: '1one', 2: '2two', 3:'3three'});

cqlsh> select id, m from foo.foo;

 id | m
----+-------------------------------------
  1 |    {1: 'one', 2: 'two', 3: 'three'}
  2 | {1: '1one', 2: '2two', 3: '3three'}

(2 rows)
cqlsh> select id, m as ""weofjkewopf"" from foo.foo;

 id | weofjkewopf
----+-----------------------------------------------------------------------
  1 |    OrderedMapSerializedKey([(1, u'one'), (2, u'two'), (3, u'three')])
  2 | OrderedMapSerializedKey([(1, u'1one'), (2, u'2two'), (3, u'3three')])

(2 rows)
Failed to format value OrderedMapSerializedKey([(1, u'one'), (2, u'two'), (3, u'three')]) : 'NoneType' object has no attribute 'sub_types'
Failed to format value OrderedMapSerializedKey([(1, u'1one'), (2, u'2two'), (3, u'3three')]) : 'NoneType' object has no attribute 'sub_types'
{code}
"	CASSANDRA	Resolved	10003	1	9644	cqlsh
12969234	dtest failure in upgrade_tests.upgrade_through_versions_test.ProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD.rolling_upgrade_test	"No cassci link, as jenkins is failing to show failures for this test, but I'm seeing detected leaks.

Relevant log section:
{code}
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,616 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@41f74411) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@1328587359:/mnt/tmp/dtest-X5kTWw/test/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/la-12-big-Data.db was not released before the reference was garbage collected
DEBUG [Reference-Reaper:1] 2016-05-11 16:17:47,617 FileCacheService.java:177 - Invalidating cache for /mnt/tmp/dtest-X5kTWw/test/node1/data2/system/local-7ad54392bcdd35a684174e047860b377/tmplink-la-13-big-Data.db
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4b0f2f61) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@235776100:/mnt/tmp/dtest-X5kTWw/test/node1/data2/system/local-7ad54392bcdd35a684174e047860b377/tmplink-la-13-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@75a1b278) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1061799113:[Memory@[0..8), Memory@[0..50)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@d0f8066) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1847744299:/mnt/tmp/dtest-X5kTWw/test/node1/data2/system/local-7ad54392bcdd35a684174e047860b377/tmplink-la-13-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,617 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@16680ae5) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1744239469:/mnt/tmp/dtest-X5kTWw/test/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/la-12-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-05-11 16:17:47,618 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@160b4b45) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1785515424:[[OffHeapBitSet]] was not released before the reference was garbage collected
{code}

Logs are attached. node1 is the one experiencing the issue."	CASSANDRA	Resolved	10003	1	9644	dtest
12779092	Our default buffer size for (uncompressed) buffered reads should be smaller, and based on the expected record size	A large contributor to slower buffered reads than mmapped is likely that we read a full 64Kb at once, when average record sizes may be as low as 140 bytes on our stress tests. The TLB has only 128 entries on a modern core, and each read will touch 32 of these, meaning we are unlikely to almost ever be hitting the TLB, and will be incurring at least 30 unnecessary misses each time (as well as the other costs of larger than necessary accesses). When working with an SSD there is little to no benefit reading more than 4Kb at once, and in either case reading more data than we need is wasteful. So, I propose selecting a buffer size that is the next larger power of 2 than our average record size (with a minimum of 4Kb), so that we expect to read in one operation. I also propose that we create a pool of these buffers up-front, and that we ensure they are all exactly aligned to a virtual page, so that the source and target operations each touch exactly one virtual page per 4Kb of expected record size.	CASSANDRA	Resolved	10002	4	9644	benedict-to-commit
12963999	dtest failure in sstableutil_test.SSTableUtilTest.abortedcompaction_test	"example failure:

{noformat}
Lists differ: ['/mnt/tmp/dtest-hXZ_VA/test/n... != ['/mnt/tmp/dtest-hXZ_VA/test/n...

First differing element 16:
/mnt/tmp/dtest-hXZ_VA/test/node1/data2/keyspace1/standard1-483ee2700d5911e6b19a879d803a6aae/ma-3-big-CRC.db
/mnt/tmp/dtest-hXZ_VA/test/node1/data2/keyspace1/standard1-483ee2700d5911e6b19a879d803a6aae/ma-5-big-CRC.db

Diff is 5376 characters long. Set self.maxDiff to None to see it.
{noformat}

http://cassci.datastax.com/job/trunk_novnode_dtest/360/testReport/sstableutil_test/SSTableUtilTest/abortedcompaction_test

Failed on CassCI build trunk_novnode_dtest #360"	CASSANDRA	Resolved	10002	4	9644	dtest
12989861	dtest failure in write_failures_test.TestWriteFailures.test_paxos_any	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/10/testReport/write_failures_test/TestWriteFailures/test_paxos_any

and:

http://cassci.datastax.com/job/cassandra-3.9_dtest/10/testReport/write_failures_test/TestWriteFailures/test_mutation_v3/

Failed on CassCI build cassandra-3.9_dtest #10

"	CASSANDRA	Resolved	10002	1	9644	dtest
12958863	clqsh: COPY FROM throws TypeError with Cython extensions enabled	"Any COPY FROM command in cqlsh is throwing the following error:

""get_num_processes() takes no keyword arguments""

Example command: 

COPY inboxdata (to_user_id,to_user_network,created_time,attachments,from_user_id,from_user_name,from_user_network,id,message,to_user_name,updated_time) FROM 'inbox.csv';

Similar commands worked parfectly in the previous versions such as 3.0.4"	CASSANDRA	Resolved	10002	1	9644	cqlsh
12997278	dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x.bug_5732_test	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/16/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x/bug_5732_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 216, in tearDown
    super(UpgradeTester, self).tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 666, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
""Unexpected error in log, see stdout\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: Upgrade test beginning, setting CASSANDRA_VERSION to 2.1.15, and jdk to 8. (Prior values will be restored after test).\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-D8UF3i\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: [[Row(table_name=u'ks', index_name=u'test.testindex')], [Row(table_name=u'ks', index_name=u'test.testindex')]]\ndtest: DEBUG: upgrading node1 to git:91f7387e1f785b18321777311a5c3416af0663c2\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Querying upgraded node\ndtest: DEBUG: Querying old node\ndtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-D8UF3i\ndtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-D8UF3i] directory\n--------------------- >> end captured logging << ---------------------""
{code}

{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:91f7387e1f785b18321777311a5c3416af0663c2
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@73deb57f) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@2098812276:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7926de0f) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1009016655:[[OffHeapBitSet]] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3a5760f9) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@223486002:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@42cb4131) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1544265728:[Memory@[0..4), Memory@[0..a)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5dda43d0) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1100327913:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@59cfa823) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1480923322:[Memory@[0..4), Memory@[0..a)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,601 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@570e14a1) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@992487242:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,602 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1f021ebc) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1878148398:[Memory@[0..4), Memory@[0..e)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,604 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@48feef6d) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@848724815:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,605 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3dee8c5f) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@1078490617:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,614 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7f726f1a) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@2037913408:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,615 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@303df044) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@861514759:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5fbf0fc9) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1715786089:[[OffHeapBitSet]] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3924b235) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1197672578:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@600596e0) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@545967120:[[OffHeapBitSet]] was not released before the reference was garbage collected
{code}"	CASSANDRA	Resolved	10002	1	9644	dtest
12901713	cqlsh: Include sub-second precision in timestamps by default	"Query with >= timestamp works. But the exact timestamp value is not working.

{noformat}
NCHAN-M-D0LZ:bin nchan$ ./cqlsh
Connected to CCC Multi-Region Cassandra Cluster at <host>:<port>.
[cqlsh 5.0.1 | Cassandra 2.1.7 | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
cqlsh>
{noformat}

{panel:title=Schema|borderStyle=dashed|borderColor=#ccc|titleBGColor=#F7D6C1|bgColor=#FFFFCE}
cqlsh:ccc> desc COLUMNFAMILY ez_task_result ;

CREATE TABLE ccc.ez_task_result (
    submissionid text,
    ezid text,
    name text,
    time timestamp,
    analyzed_index_root text,
    ...
    ...
    PRIMARY KEY (submissionid, ezid, name, time)
{panel}

{panel:title=Working|borderStyle=dashed|borderColor=#ccc|titleBGColor=#F7D6C1|bgColor=#FFFFCE}
cqlsh:ccc> select submissionid, ezid, name, time, state, status, translated_criteria_status from ez_task_result where submissionid='760dd154670811e58c04005056bb6ff0' and ezid='760dd6de670811e594fc005056bb6ff0' and name='run-sanities' and time>='2015-09-29 20:54:23-0700';

 submissionid                     | ezid                             | name         | time                     | state     | status      | translated_criteria_status
----------------------------------+----------------------------------+--------------+--------------------------+-----------+-------------+----------------------------
 760dd154670811e58c04005056bb6ff0 | 760dd6de670811e594fc005056bb6ff0 | run-sanities | 2015-09-29 20:54:23-0700 | EXECUTING | IN_PROGRESS |       run-sanities started

(1 rows)
cqlsh:ccc>
{panel}
{panel:title=Not working|borderStyle=dashed|borderColor=#ccc|titleBGColor=#F7D6C1|bgColor=#FFFFCE}
cqlsh:ccc> select submissionid, ezid, name, time, state, status, translated_criteria_status from ez_task_result where submissionid='760dd154670811e58c04005056bb6ff0' and ezid='760dd6de670811e594fc005056bb6ff0' and name='run-sanities' and time='2015-09-29 20:54:23-0700';

 submissionid | ezid | name | time | analyzed_index_root | analyzed_log_path | clientid | end_time | jenkins_path | log_file_path | path_available | path_to_task | required_for_overall_status | start_time | state | status | translated_criteria_status | type
--------------+------+------+------+---------------------+-------------------+----------+----------+--------------+---------------+----------------+--------------+-----------------------------+------------+-------+--------+----------------------------+------

(0 rows)
cqlsh:ccc>
{panel}


"	CASSANDRA	Resolved	10002	4	9644	cqlsh
12984398	DESCRIBE INDEX: missing quotes for case-sensitive index name	"Create a custom index with a case-sensitive name.
The result of the DESCRIBE INDEX command does not have quotes around the index name. As a result, the index cannot be recreated with this output."	CASSANDRA	Resolved	10003	1	9644	cqlsh, lhf
12967925	cqlsh show sessions truncates time_elapsed values > 999999	"Output from show session in cqlsh:
{quote}
Submit hint for /10.255.227.20 [EXPIRING-MAP-REAPER:1] | 2016-05-11 15:57:53.730000 | 10.255.226.163 |         283246
{quote}
Output from select * from trace_events where session_id=(same as above):
{quote}
 1bbce5c0-1791-11e6-9598-3b9ec975a2e6 | 1ee37a20-1791-11e6-9598-3b9ec975a2e6 |                         Submit hint for /10.255.227.20 | 10.255.226.163 |        5283246 |                     EXPIRING-MAP-REAPER:1
{quote}
Notice that the 5 (seconds) part is being truncated in the output.
"	CASSANDRA	Resolved	10002	1	9644	lhf
12833870	ViewTest.testSSTablesInBounds fails in trunk	"Error:
{{0(true) 1(false) expected:<1> but was:<2>}}

http://cassci.datastax.com/job/trunk_testall/125/testReport/org.apache.cassandra.db.lifecycle/ViewTest/testSSTablesInBounds/"	CASSANDRA	Resolved	10002	4	9644	test-failure
12747344	cassandra-cli and cqlsh report two different values for a setting, partially update it and partially report it	"cassandra-cli updates and prints out a min_compaction_threshold that is not shown by cqlsh (it shows a different min_threshold attribute)

cqlsh updates ""both"" values but only shows one of them

{code}
cassandra-cli:
UPDATE COLUMN FAMILY foo WITH min_compaction_threshold = 8;

$ echo ""describe foo;"" | cassandra-cli -h `hostname` -k bar
      Compaction min/max thresholds: 8/32

$ echo ""describe table foo;"" | cqlsh -k bar `hostname`
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
{code}

{code}
cqlsh:
ALTER TABLE foo WITH compaction = {'class' : 'SizeTieredCompactionStrategy', 'min_threshold' : 16};

cassandra-cli:
      Compaction min/max thresholds: 16/32
      Compaction Strategy Options:
        min_threshold: 16
cqlsh:
  compaction={'min_threshold': '16', 'class': 'SizeTieredCompactionStrategy'} AND
{code}

{code}
cassandra-cli:
UPDATE COLUMN FAMILY foo WITH min_compaction_threshold = 8;

cassandra-cli:
      Compaction min/max thresholds: 8/32
      Compaction Strategy Options:
        min_threshold: 16

cqlsh:
  compaction={'min_threshold': '16', 'class': 'SizeTieredCompactionStrategy'} AND
{code}
"	CASSANDRA	Resolved	10003	1	9644	cli, cqlsh
12933073	C*2.1 cqlsh DESCRIBE KEYSPACE ( or TABLE ) returns 'NoneType' object has no attribute 'replace'	"C* 2.1 cqlsh DESCRIBE KEYSPACE ( or TABLE ) returns:

{code}
 'NoneType' object has no attribute 'replace' 
{code}

for thrift CF's originally created in C* 1.2.

Repro:

1. Create cf in cassandra-cli on C* 1.2.x  (1.2.9 was used here)

{code}
[default@ks1] CREATE COLUMN FAMILY t1
...	WITH column_type='Standard'
...	AND comparator='CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)'
...	AND default_validation_class='UTF8Type'
...	AND key_validation_class='UTF8Type'
...	AND read_repair_chance=0.1
...	AND dclocal_read_repair_chance=0.0
...	AND gc_grace=864000
...	AND min_compaction_threshold=4
...	AND max_compaction_threshold=32
...	AND replicate_on_write=true
...	AND compaction_strategy='LeveledCompactionStrategy' AND compaction_strategy_options={sstable_size_in_mb: 32}
...	AND caching='KEYS_ONLY'
...	AND compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:64};

qlsh> describe keyspace ks1;

CREATE KEYSPACE ks1 WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'datacenter1': '1'
};

USE ks1;

CREATE TABLE t1 (
  key text,
  column1 text,
  column2 text,
  value text,
  PRIMARY KEY (key, column1, column2)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.100000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'sstable_size_in_mb': '32', 'class': 'LeveledCompactionStrategy'} AND
  compression={'chunk_length_kb': '64', 'sstable_compression': 'SnappyCompressor'};


cqlsh> select keyspace_name, columnfamily_name,column_aliases,key_aliases from system.schema_columnfamilies where keyspace_name= 'ks1';

 keyspace_name | columnfamily_name | column_aliases | key_aliases
---------------+-------------------+----------------+-------------
           ks1 |                t1 |             [] |          []


2/ Upgrade -> C* 2.0.9 -> nodetool upgradesstables -a

At this stage , DESCRIBE in cqlsh is working

3/ Upgrade -> C* 2.1.12 -> nodetool upgradesstables -a

DESCRIBE now fails:

cqlsh> describe table ks1.t1;
'NoneType' object has no attribute 'replace'

cqlsh> describe keyspace ks1;
'NoneType' object has no attribute 'replace'
{code}

You can workaround by manually updating {{system.schema_columnfamilies}}

{code}
 UPDATE system.schema_columnfamilies SET column_aliases ='[""column1"",""column2""]' WHERE keyspace_name = 'ks1' AND columnfamily_name = 't1';
{code}

Once you exit and restart cqlsh, {{DESCRIBE}} is not working as per C* 1.2

{code}
cqlsh> describe keyspace ks1;

CREATE KEYSPACE ks1 WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1': '1'}  AND durable_writes = true;

CREATE TABLE ks1.t1 (
    key text,
    column1 text,
    column2 text,
    value text,
    PRIMARY KEY (key, column1, column2)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (column1 ASC, column2 ASC)
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'sstable_size_in_mb': '32', 'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'chunk_length_kb': '64', 'sstable_compression': 'org.apache.cassandra.io.compress.SnappyCompressor'}
    AND dclocal_read_repair_chance = 0.0
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.1
    AND speculative_retry = '99.0PERCENTILE';
{code}


"	CASSANDRA	Resolved	10002	1	9644	cqlsh
12953620	Make number of cores used by cqlsh COPY visible to testing code	"As per this conversation with [~Stefania]:

https://github.com/riptano/cassandra-dtest/pull/869#issuecomment-200597829

we don't currently have a way to verify that the test environment variable {{CQLSH_COPY_TEST_NUM_CORES}} actually affects the behavior of {{COPY}} in the intended way. If this were added, we could make our tests of the one-core edge case a little stricter."	CASSANDRA	Resolved	10003	4	9644	lhf
12768797	long-test LongLeveledCompactionStrategyTest flaps in 2.0	"LongLeveledCompactionStrategyTest periodically fails with:
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 54.412 sec
    [junit] 
    [junit] Testcase: testParallelLeveledCompaction(org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest):      Caused an ERROR
    [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit]     at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:413)
    [junit]     at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:402)
    [junit]     at org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest.testParallelLeveledCompaction(LongLeveledCompactionStrategyTest.java:97)
    [junit] Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit]     at java.util.concurrent.FutureTask.report(FutureTask.java:122)
    [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:188)
    [junit]     at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:409)
    [junit] Caused by: java.lang.RuntimeException: Last written key DecoratedKey(3133, 3133) >= current key DecoratedKey(313236, 313236) writing into build/test/cassandra/data/Keyspace1/StandardLeveled/Keyspace1-StandardLeveled-tmp-jb-304-Data.db
    [junit]     at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:143)
    [junit]     at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:166)
    [junit]     at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:167)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit]     at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
    [junit]     at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
    [junit]     at org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest$1.run(LongLeveledCompactionStrategyTest.java:87)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    [junit]     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    [junit]     at java.lang.Thread.run(Thread.java:745)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest FAILED
{noformat}

I would guess the failure is 10-20% of the time, looping over the test repeatedly.

----

On the 2.1 branch, the failure is different, so perhaps this could also be updated.
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.04 sec
    [junit] 
    [junit] Testcase: testParallelLeveledCompaction(org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest):      Caused an ERROR
    [junit] org.apache.cassandra.db.compaction.WrappingCompactionStrategy cannot be cast to org.apache.cassandra.db.compaction.LeveledCompactionStrategy
    [junit] java.lang.ClassCastException: org.apache.cassandra.db.compaction.WrappingCompactionStrategy cannot be cast to org.apache.cassandra.db.compaction.LeveledCompactionStrategy
    [junit]     at org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest.testParallelLeveledCompaction(LongLeveledCompactionStrategyTest.java:45)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongLeveledCompactionStrategyTest FAILED
{noformat}"	CASSANDRA	Resolved	10003	4	9644	test-failure
12779090	RandomAccessReader should share its FileChannel with all instances (via SegmentedFile)	There's no good reason to open a FileChannel for each \(Compressed\)\?RandomAccessReader, and this would simplify RandomAccessReader to just a thin wrapper.	CASSANDRA	Resolved	10002	4	9644	performance
12955796	Bug or not?: coordinator using SimpleSnitch may query other nodes for copies of local data 	"As [~Stefania] explains [in this JIRA comment|https://issues.apache.org/jira/browse/CASSANDRA-11225?focusedCommentId=15221059&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221059], {{SimpleSnitch}} does not implement {{IEndpointSnitch.sortByProximity(localhost, liveendpoints)}}, so a query for data on the coordinator may query other nodes. That seems like unnecessary work to me, and on that note, Stefania woonders [in this JIRA comment|https://issues.apache.org/jira/browse/CASSANDRA-11225?focusedCommentId=15223598&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15223598] - should this be considered a bug?

Stefania, I'm assigning you here -- could you find the right people to involve in this discussion?"	CASSANDRA	Resolved	10003	1	9644	doc-impacting
12781556	Fix SSTableRewriterTest on Windows	"Platform specific failures:
org.apache.cassandra.io.sstable.SSTableRewriterTest.testNumberOfFiles_truncate
org.apache.cassandra.io.sstable.SSTableRewriterTest.testSmallFiles
org.apache.cassandra.io.sstable.SSTableRewriterTest.testNumberOfFiles_dont_clean_readers
org.apache.cassandra.io.sstable.SSTableRewriterTest.testNumberOfFiles_finish_empty_new_writer
"	CASSANDRA	Resolved	10003	1	9644	Windows
12971376	cannot use cql since upgrading python to 2.7.11+	"OS: Debian GNU/Linux stretch/sid 
Kernel: 4.5.0-2-amd64 #1 SMP Debian 4.5.4-1 (2016-05-16) x86_64 GNU/Linux
Python version: 2.7.11+ (default, May  9 2016, 15:54:33)
[GCC 5.3.1 20160429]

cqlsh --version: cqlsh 5.0.1
cassandra -v: 3.5 (also occurs with 3.0.6)

Issue:
when running cqlsh, it returns the following error:

cqlsh -u dbarpt_usr01
Password: *****

Connection error: ('Unable to connect to any servers', {'odbasandbox1': TypeError('ref() does not take keyword arguments',)})

I cleared PYTHONPATH:

python -c ""import json; print dir(json); print json.__version__""
['JSONDecoder', 'JSONEncoder', '__all__', '__author__', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', '__version__', '_default_decoder', '_default_encoder', 'decoder', 'dump', 'dumps', 'encoder', 'load', 'loads', 'scanner']
2.0.9

Java based clients can connect to Cassandra with no issue. Just CQLSH and Python clients cannot.

nodetool status also works.

Thank you for your help.


"	CASSANDRA	Resolved	10002	1	9644	cqlsh
13058179	Cqlsh COPY fails importing Map<String,List<String>>, ParseError unhashable type list	"When importing data with the _COPY_ command into a column family that has a _map<text, frozen<list<text>>>_ field, I get a _unhashable type: 'list'_ error. Here is how to reproduce:

{code}
CREATE TABLE table1 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<list<text>>>
);

insert into table1 (col1, col2map) values (1, {'key': ['value1']});

cqlsh:ks> copy table1 to 'table1.csv';


table1.csv file content:
1,{'key': ['value1']}


cqlsh:ks> copy table1 from 'table1.csv';
...
Failed to import 1 rows: ParseError - Failed to parse {'key': ['value1']} : unhashable type: 'list',  given up without retries
Failed to process 1 rows; failed rows written to kv_table1.err
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.420 seconds (0 skipped).
{code}

But it works fine for Map<String, Set<String>>.

{code}
CREATE TABLE table2 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<set<text>>>
);

insert into table2 (col1, col2map) values (1, {'key': {'value1'}});

cqlsh:ks> copy table2 to 'table2.csv';


table2.csv file content:
1,{'key': {'value1'}}


cqlsh:ks> copy table2 from 'table2.csv';
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.417 seconds (0 skipped).
{code}

The exception seems to arrive in _convert_map_ function in _ImportConversion_ class inside _copyutil.py_."	CASSANDRA	Resolved	10002	1	9644	cqlsh
12831664	Metrics should use up to date nomenclature	"There are a number of exposed metrics that currently are named using the old nomenclature of columnfamily and rows (meaning partitions).
It would be good to audit all metrics and update any names to match what they actually represent; we should probably do that in a single sweep to avoid a confusing mixture of old and new terminology. 

As we'd need to do this in a major release, I've initially set the fixver for 3.0 beta1.
"	CASSANDRA	Resolved	10002	4	9644	jmx
12838097	Allow an initial connection timeout to be set in cqlsh	"[PYTHON-206|https://datastax-oss.atlassian.net/browse/PYTHON-206] introduced the ability to change the initial connection timeout on connections from the default of 5s.

This change was introduced because some auth providers (kerberos) can take longer than 5s to complete a first time negotiation for a connection. 

cqlsh should allow this setting to be changed. "	CASSANDRA	Resolved	10002	4	9644	cqlsh
12964484	[windows] dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_reading_with_skip_and_max_rows	"looks to be an assertion problem, so could be test or cassandra related:

e.g.:
{noformat}
10000 != 331
{noformat}

http://cassci.datastax.com/job/trunk_dtest_win32/404/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_reading_with_skip_and_max_rows

Failed on CassCI build trunk_dtest_win32 #404"	CASSANDRA	Resolved	10002	4	9644	dtest, windows
12956245	dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_reading_max_parse_errors	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/197/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_reading_max_parse_errors

Failed on CassCI build cassandra-3.0_novnode_dtest #197

{noformat}
Error Message

False is not true
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-c2AJlu
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Importing csv file /mnt/tmp/tmp2O43PH with 10 max parse errors
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 943, in test_reading_max_parse_errors
    self.assertTrue(num_rows_imported < (num_rows / 2))  # less than the maximum number of valid rows in the csv
  File ""/usr/lib/python2.7/unittest/case.py"", line 422, in assertTrue
    raise self.failureException(msg)
""False is not true\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-c2AJlu\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Importing csv file /mnt/tmp/tmp2O43PH with 10 max parse errors\n--------------------- >> end captured logging << ---------------------""
Standard Output

(EE)  Using CQL driver: <module 'cassandra' from '/home/automaton/cassandra/bin/../lib/cassandra-driver-internal-only-3.0.0-6af642d.zip/cassandra-driver-3.0.0-6af642d/cassandra/__init__.py'>(EE)  Using connect timeout: 5 seconds(EE)  Using 'utf-8' encoding(EE)  <stdin>:2:Failed to import 2500 rows: ParseError - could not convert string to float: abc,  given up without retries(EE)  <stdin>:2:Exceeded maximum number of parse errors 10(EE)  <stdin>:2:Failed to process 2500 rows; failed rows written to import_ks_testmaxparseerrors.err(EE)  <stdin>:2:Exceeded maximum number of parse errors 10(EE)  
{noformat}"	CASSANDRA	Resolved	10002	4	9644	dtest
12983808	dtest failure in consistency_test.TestAccuracy.test_simple_strategy_counters	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/484/testReport/consistency_test/TestAccuracy/test_simple_strategy_counters

Failed on CassCI build cassandra-2.1_dtest #484

{code}
Standard Error

Traceback (most recent call last):
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 514, in run
    valid_fcn(v)
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 497, in validate_counters
    check_all_sessions(s, n, c)
  File ""/home/automaton/cassandra-dtest/consistency_test.py"", line 490, in check_all_sessions
    ""value of %s at key %d, instead got these values: %s"" % (write_nodes, val, n, results)
AssertionError: Failed to read value from sufficient number of nodes, required 2 nodes to have a counter value of 1 at key 200, instead got these values: [0, 0, 1]
{code}"	CASSANDRA	Resolved	10002	4	9644	dtest
12859554	decommissioned_wiped_node_can_join_test fails on Jenkins	"This test passes locally but reliably fails on Jenkins. It seems after we restart node4, it is unable to Gossip with other nodes:

{code}
INFO  [HANDSHAKE-/127.0.0.2] 2015-08-27 06:50:42,778 OutboundTcpConnection.java:494 - Handshaking version with /127.0.0.2
INFO  [HANDSHAKE-/127.0.0.1] 2015-08-27 06:50:42,778 OutboundTcpConnection.java:494 - Handshaking version with /127.0.0.1
INFO  [HANDSHAKE-/127.0.0.3] 2015-08-27 06:50:42,778 OutboundTcpConnection.java:494 - Handshaking version with /127.0.0.3
ERROR [main] 2015-08-27 06:51:13,785 CassandraDaemon.java:635 - Exception encountered during startup
java.lang.RuntimeException: Unable to gossip with any seeds
        at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1342) ~[main/:na]
        at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:518) ~[main/:na]
        at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:763) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:687) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:570) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:320) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:516) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:622) [main/:na]
WARN  [StorageServiceShutdownHook] 2015-08-27 06:51:13,799 Gossiper.java:1453 - No local state or state is in silent shutdown, not announcing shutdown
{code}

It seems both the addresses and port number of the seeds are correct so I don't think the problem is the Amazon private addresses but I might be wrong. 

It's also worth noting that the first time the node starts up without problems. The problem only occurs during a restart."	CASSANDRA	Resolved	10002	7	9644	CI
12936865	cqlsh_copy_test failing on 2.1	"See [here|http://cassci.datastax.com/job/cassandra-2.1_dtest/415/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_datetimeformat_round_trip/] for a failure of {{cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_datetimeformat_round_trip}} on 2.1 against the SHA 165f586e6f5e7.

The test is seeing different timestamps than expected."	CASSANDRA	Resolved	10002	1	9644	dtest
12735865	enable describe on indices	"Describe index should be supported, right now, the only way is to export the schema and find what it really is before updating/dropping the index.

verified in 
[cqlsh 3.1.8 | Cassandra 1.2.18.1 | CQL spec 3.0.0 | Thrift protocol 19.36.2]
"	CASSANDRA	Resolved	10003	4	9644	doc-impacting
12742613	Changes to index_interval table properties revert after subsequent modifications	"It appears that if you want to increase the sampling in *-Summary.db files, you would change the default for {{index_interval}} table property from the {{128}} default value to {{256}} on a given CQL {{TABLE}}.

However, if you {{ALTER TABLE}} after setting the value, {{index_interval}} returns to the default, {{128}}. This is unexpected behavior. I would expect the value for {{index_interval}} to not be affected by subsequent {{ALTER TABLE}} statements.

As noted in Environment, this was seen with a 2.0.9-SNAPSHOT built w/ `ccm`. 

If I just use a table from one of DataStax documentation tutorials (musicdb as mdb):

{noformat}
cqlsh:mdb> DESC TABLE songs;

CREATE TABLE songs (
  id uuid,
  album text,
  artist text,
  data blob,
  reviews list<text>,
  tags set<text>,
  title text,
  venue map<timestamp, text>,
  PRIMARY KEY ((id))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{noformat}

We've got {{128}} as expected.

We alter it:

{noformat}
cqlsh:mdb> ALTER TABLE songs WITH index_interval = 256; 
{noformat}

And the change appears: 

{noformat}
cqlsh:mdb> DESC TABLE songs;

CREATE TABLE songs (
  id uuid,
  album text,
  artist text,
  data blob,
  reviews list<text>,
  tags set<text>,
  title text,
  venue map<timestamp, text>,
  PRIMARY KEY ((id))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=256 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{noformat}

But if do another {{ALTER TABLE}}, say, change the caching or comment, the {{index_interval}} will revert back to {{128}}.

{noformat}
cqlsh:mdb> ALTER TABLE songs WITH caching = 'none'; 
cqlsh:mdb> DESC TABLE songs; 

CREATE TABLE songs (
  id uuid,
  album text,
  artist text,
  data blob,
  reviews list<text>,
  tags set<text>,
  title text,
  venue map<timestamp, text>,
  PRIMARY KEY ((id))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='NONE' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{noformat}

It should be {{index_interval=256}}.

I know that 2.1 will replace {{index_interval}}. 

I have not confirmed any behavior with {{min_index_interval}} nor {{max_index_interval}} (which is described in resolved #6379). 
"	CASSANDRA	Resolved	10002	1	9644	cql3, metadata
12833127	Need to set TTL with COPY command	"I can import a chunk of data into Cassandra table with COPY command like:

COPY my_table (name, address) FROM my_file.csv WITH option='value' ... ;

But I am not able to specify a finite TTL in COPY command with ""USING TTL 3600"", for example. "	CASSANDRA	Resolved	10002	7	9644	cqlsh
12997924	dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_non_prepared_statements	"example failure:

http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/447/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_non_prepared_statements

{code}
Error Message

100000 != 96848
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-BryYNs
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Running stress without any user profile
dtest: DEBUG: Generated 100000 records
dtest: DEBUG: Exporting to csv file: /tmp/tmpREOhBZ
dtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpREOhBZ' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000
dtest: DEBUG: COPY TO took 0:00:04.598829 to export 100000 records
dtest: DEBUG: Truncating keyspace1.standard1...
dtest: DEBUG: Importing from csv file: /tmp/tmpREOhBZ
dtest: DEBUG: COPY keyspace1.standard1 FROM '/tmp/tmpREOhBZ' WITH PREPAREDSTATEMENTS = False
dtest: DEBUG: COPY FROM took 0:00:10.348123 to import 100000 records
dtest: DEBUG: Exporting to csv file: /tmp/tmpeXLPtz
dtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpeXLPtz' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000
dtest: DEBUG: COPY TO took 0:00:11.681829 to export 100000 records
--------------------- >> end captured logging << ---------------------
{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2482, in test_bulk_round_trip_non_prepared_statements
    copy_from_options={'PREPAREDSTATEMENTS': False})
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2461, in _test_bulk_round_trip
    sum(1 for _ in open(tempfile2.name)))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""100000 != 96848\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-BryYNs\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Running stress without any user profile\ndtest: DEBUG: Generated 100000 records\ndtest: DEBUG: Exporting to csv file: /tmp/tmpREOhBZ\ndtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpREOhBZ' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000\ndtest: DEBUG: COPY TO took 0:00:04.598829 to export 100000 records\ndtest: DEBUG: Truncating keyspace1.standard1...\ndtest: DEBUG: Importing from csv file: /tmp/tmpREOhBZ\ndtest: DEBUG: COPY keyspace1.standard1 FROM '/tmp/tmpREOhBZ' WITH PREPAREDSTATEMENTS = False\ndtest: DEBUG: COPY FROM took 0:00:10.348123 to import 100000 records\ndtest: DEBUG: Exporting to csv file: /tmp/tmpeXLPtz\ndtest: DEBUG: CONSISTENCY ALL; COPY keyspace1.standard1 TO '/tmp/tmpeXLPtz' WITH PAGETIMEOUT = 10 AND PAGESIZE = 1000\ndtest: DEBUG: COPY TO took 0:00:11.681829 to export 100000 records\n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	4	9644	dtest
12709760	Simplify (and unify) cleanup of compaction leftovers	"Currently we manage a list of in-progress compactions in a system table, which we use to cleanup incomplete compactions when we're done. The problem with this is that 1) it's a bit clunky (and leaves us in positions where we can unnecessarily cleanup completed files, or conversely not cleanup files that have been superceded); and 2) it's only used for a regular compaction - no other compaction types are guarded in the same way, so can result in duplication if we fail before deleting the replacements.

I'd like to see each sstable store in its metadata its direct ancestors, and on startup we simply delete any sstables that occur in the union of all ancestor sets. This way as soon as we finish writing we're capable of cleaning up any leftovers, so we never get duplication. It's also much easier to reason about."	CASSANDRA	Resolved	10003	4	9644	benedict-to-commit, compaction
12955019	dtest failure in materialized_views_test.TestMaterializedViews.base_replica_repair_test	"base_replica_repair_test has failed on trunk with the following exception in the log of node2:

{code}
ERROR [main] 2016-03-31 08:48:46,949 CassandraDaemon.java:708 - Exception encountered during startup
java.lang.RuntimeException: Failed to list files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:53) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:547) ~[main/:na]
        at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:725) ~[main/:na]
        at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:690) ~[main/:na]
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:567) ~[main/:na]
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:555) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:383) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:320) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:130) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:107) ~[main/:na]
        at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:139) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepareRestrictions(SelectStatement.java:864) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:811) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:799) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:505) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:242) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:286) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:294) ~[main/:na]
        at org.apache.cassandra.schema.SchemaKeyspace.query(SchemaKeyspace.java:1246) ~[main/:na]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:875) ~[main/:na]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:867) ~[main/:na]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:134) ~[main/:na]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:124) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:229) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:562) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:691) [main/:na]
Caused by: java.lang.RuntimeException: Failed to list directory files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985, inconsistent disk state for transaction [ma_txn_flush_58db56b0-f71d-11e5-bf68-03a01adb9f11.log in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.classifyFiles(LogAwareFileLister.java:149) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.classifyFiles(LogAwareFileLister.java:103) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$48/35984028.accept(Unknown Source) ~[na:na]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ~[na:1.8.0_45]
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[na:1.8.0_45]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) ~[na:1.8.0_45]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512) ~[na:1.8.0_45]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502) ~[na:1.8.0_45]
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) ~[na:1.8.0_45]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) ~[na:1.8.0_45]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_45]
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) ~[na:1.8.0_45]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:71) ~[main/:na]
        at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:49) ~[main/:na]
        ... 25 common frames omitted
{code}

example failure:

http://cassci.datastax.com/job/trunk_dtest/1092/testReport/materialized_views_test/TestMaterializedViews/base_replica_repair_test

Failed on CassCI build trunk_dtest #1092

I've attached the logs from the failure in build 1092."	CASSANDRA	Resolved	10002	1	9644	dtest
12975530	dtest failure in consistency_test.TestAvailability.test_network_topology_strategy_each_quorum	"example failure:

http://cassci.datastax.com/job/trunk_large_dtest/10/testReport/consistency_test/TestAvailability/test_network_topology_strategy_each_quorum

Failed on CassCI build trunk_large_dtest #10

Logs are attached.

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 719, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')

Standard Output

Unexpected error in node3 log, error: 
ERROR [SharedPool-Worker-1] 2016-06-03 14:25:27,460 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-2] 2016-06-03 14:25:27,460 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-3] 2016-06-03 14:25:27,462 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-2] 2016-06-03 14:25:27,464 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-3] 2016-06-03 14:25:27,464 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-1] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-4] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-5] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-7] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
ERROR [SharedPool-Worker-6] 2016-06-03 14:25:27,465 Keyspace.java:504 - Attempting to mutate non-existant table 03b14ad0-2997-11e6-b8c7-01c3aea11be7 (mytestks.users)
{code}"	CASSANDRA	Resolved	10002	1	9644	dtest
13001382	cqlsh lost the ability to have a request wait indefinitely	"In commit c7f0032912798b5e53b64d8391e3e3d7e4121165, when client_timeout became request_timeout, the logic was changed so that you can no longer use a timeout of None, despite the docs saying that you can:

https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshUsingCqlshrc.html#cqlshUsingCqlshrc__request-timeout"	CASSANDRA	Resolved	10003	1	9644	cqlsh, doc-impacting, lhf
13005008	cqlsh NoHostsAvailable/AuthenticationFailure when sourcing a file with COPY commands	"In {{cqlsh}}, with authentication enabled, when sourcing a file with {{COPY}} commands in it:
{noformat}
test.cql:2:Error for (None, None): Failed to connect to all replicas ['127.0.0.1'] for (None, None), errors: [""NoHostAvailable - ('Unable to connect to any servers', {'127.0.0.1': AuthenticationFailed('Remote end requires authentication.',)})""] (permanently given up after 0 rows and 5 attempts)
{noformat}

{{cqlsh}} creates a new {{Shell}} without passing all pertinent arguments. When {{copyutil}} creates new cluster connections, they are not initialized correctly.

This is only for the {{source}} command. As a workaround,  {{cqlsh -f <script>}}  works, since it does not create a new {{Shell}} instance.

Repro:

{code}
ccm create -v 3.7 -n 1 test
ccm updateconf ""authenticator: PasswordAuthenticator"" ""authorizer: CassandraAuthorizer""
ccm start

echo ""copy system.local to 'something';"" > test.cql

echo ""source 'test.cql'"" | ccm node1 cqlsh
{code}"	CASSANDRA	Resolved	10003	1	9644	cqlsh
12840391	Paxos ballot in StorageProxy could clash	"This code in {{StorageProxy.beginAndRepairPaxos()}} takes a timestamp in microseconds but divides it by 1000 before adding one. So if the summary is null, ballotMillis would be the same for up to 1000 possible state timestamp values:

{code}
    long currentTime = (state.getTimestamp() / 1000) + 1;
    long ballotMillis = summary == null
                                 ? currentTime
                                 : Math.max(currentTime, 1 +    UUIDGen.unixTimestamp(summary.mostRecentInProgressCommit.ballot));
    UUID ballot = UUIDGen.getTimeUUID(ballotMillis);
{code}

{{state.getTimestamp()}} returns the time in micro seconds and it ensures to add one microsecond to any previously used timestamp if the client sends the same or an older timestamp. 

Initially I used this code in {{ModificationStatement.casInternal()}}, introduced by CASSANDRA-9160 to support cas unit tests, but occasionally these tests were failing. It was only when I ensured uniqueness of the ballot that the tests started to pass reliably.

I wonder if we could ever have the same issue in StorageProxy?

cc [~jbellis] and [~slebresne] for CASSANDRA-7801"	CASSANDRA	Resolved	10003	1	9644	LWT
12862005	BATCH statement is broken in cqlsh	"BEGIN BATCH .... APPLY BATCH is not parsed correctly.

Steps:
{code}
CREATE KEYSPACE Excelsior  WITH REPLICATION={'class':'SimpleStrategy','replication_factor':1};
CREATE TABLE excelsior.data (id int primary key);
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0); APPLY BATCH ;
{code}
Error
{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 0:-1 mismatched input '<EOF>' expecting K_APPLY"">
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:0 no viable alternative at input 'APPLY' ([APPLY]...)"">
{code}
While 
{code}
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0)  APPLY BATCH ;
{code}
without *;* after insert works.

Consequently neither
{code}
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0);INSERT INTO excelsior.data (id) VALUES (0); APPLY BATCH ;
{code}
Error:
{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 0:-1 mismatched input '<EOF>' expecting K_APPLY"">
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:0 no viable alternative at input 'APPLY' ([APPLY]...)"">
{code}
nor
{code}
BEGIN BATCH INSERT INTO excelsior.data (id) VALUES (0);INSERT INTO excelsior.data (id) VALUES (0) APPLY BATCH ;
{code}
Error
{code}
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 0:-1 mismatched input '<EOF>' expecting K_APPLY"">
SyntaxException: <ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:43 missing EOF at 'APPLY' (...(id) VALUES (0) [APPLY] BATCH...)"">
{code}
works.

It was OK in 2.2.0 and 3.0 beta 1.
3.0-beta2-tentative also affected."	CASSANDRA	Resolved	10002	1	9644	cqlsh
12785472	Fix hinted_handoff_enabled yaml setting	"As discussed in CASSANDRA-6157, at the moment we have a single parameter {{hinted_handoff_enabled}} that can be either a boolean or a csv list of enabled data centers.

We should have a boolean global {{hinted_handoff_enabled}} param plus a separate yaml list for the HH DC blacklist - {{hinted_handoff_disabled_datacenters}} to be checked when the global flag is on.

Backward compatibility with the existing approach should be kept."	CASSANDRA	Resolved	10003	7	9644	doc-impacting
13013504	testall failure in org.apache.cassandra.db.ColumnFamilyStoreCQLHelperTest.testDynamicComposite	"example failure:
http://cassci.datastax.com/job/cassandra-3.0_testall/706/testReport/org.apache.cassandra.db/ColumnFamilyStoreCQLHelperTest/testDynamicComposite/

{code}
Stacktrace

junit.framework.AssertionFailedError: 
	at org.apache.cassandra.db.ColumnFamilyStoreCQLHelperTest.testDynamicComposite(ColumnFamilyStoreCQLHelperTest.java:636)
{code}"	CASSANDRA	Resolved	10002	1	9644	test-failure
12948253	Improve backoff policy for cqlsh COPY FROM	"Currently we have an exponential back-off policy in COPY FROM that kicks in when timeouts are received. However there are two limitations:

* it does not cover new requests and therefore we may not back-off sufficiently to give time to an overloaded server to recover
* the pause is performed in the receiving thread and therefore we may not process server messages quickly enough

There is a static throttling mechanism in rows per second from feeder to worker processes (the INGESTRATE) but the feeder has no idea of the load of each worker process. However it's easy to keep track of how many chunks a worker process has yet to read by introducing a bounded semaphore.

The idea is to move the back-off pauses to the worker processes main thread so as to include all messages, new and retries, not just the retries that timed out. The worker process will not read new chunks during the back-off pauses, and the feeder process can then look at the number of pending chunks before sending new chunks to a worker process.

[~aholmber], [~aweisberg] what do you think?  "	CASSANDRA	Resolved	10002	4	9644	doc-impacting
12856742	Windows dtest 3.0: ttl_test.py failures	"ttl_test.py:TestTTL.update_column_ttl_with_default_ttl_test2
ttl_test.py:TestTTL.update_multiple_columns_ttl_test
ttl_test.py:TestTTL.update_single_column_ttl_test

Errors locally are different than CI from yesterday. Yesterday on CI we have timeouts and general node hangs. Today on all 3 tests when run locally I see:
{noformat}
Traceback (most recent call last):
  File ""c:\src\cassandra-dtest\dtest.py"", line 532, in tearDown
    raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
AssertionError: Unexpected error in node1 node log: ['ERROR [main] 2015-08-17 16:53:43,120 NoSpamLogger.java:97 - This platform does not support atomic directory streams (SecureDirectoryStream); race conditions when loading sstable files could occurr']
{noformat}

This traces back to the commit for CASSANDRA-7066 today by [~Stefania] and [~benedict].  Stefania - care to take this ticket and also look further into whether or not we're going to have issues with 7066 on Windows? That error message certainly *sounds* like it's not a good thing."	CASSANDRA	Resolved	10002	7	9644	Windows
12666577	Allow JVM_OPTS to be passed to sstablescrub	"Can you add a feature request to pass JVM_OPTS to the sstablescrub script -- and other places where java is being called? (Among other things, this lets us run java stuff with ""-Djava.awt.headless=true"" on OS X so that Java processes don't pop up into the foreground -- i.e. we have a script that loops over all CFs and runs sstablescrub, and without that flag being passed in the OS X machine becomes pretty much unusable as it keeps switching focus to the java processes as they start.)
 
--- a/resources/cassandra/bin/sstablescrub
+++ b/resources/cassandra/bin/sstablescrub
@@ -70,7 +70,7 @@ if [ ""x$MAX_HEAP_SIZE"" = ""x"" ]; then
     MAX_HEAP_SIZE=""256M""
 fi
 
-$JAVA -ea -cp $CLASSPATH -Xmx$MAX_HEAP_SIZE \
+$JAVA $JVM_OPTS -ea -cp $CLASSPATH -Xmx$MAX_HEAP_SIZE \
         -Dlog4j.configuration=log4j-tools.properties \
         org.apache.cassandra.tools.StandaloneScrubber ""$@"""	CASSANDRA	Resolved	10002	2	9644	lhf
12864258	Stress should exit with non-zero status after failure	Currently, stress always exits with sucess status, even if after a failure. In order to be able to rely on stress exit status during dtests it would be nice if it exited with a non-zero status after failures.	CASSANDRA	Resolved	10003	4	9644	lhf, stress
12834943	Sstables created with MockSchema occasionally triggers assertions	"It's extremely hard to reproduce but occasionally some Jenkins tests fail as follows:

http://cassci.datastax.com/job/trunk_utest/229/testReport/org.apache.cassandra.db.lifecycle/TrackerTest/testAddInitialSSTables/

{code}
junit.framework.AssertionFailedError
	at org.apache.cassandra.utils.concurrent.Ref$State.assertNotReleased(Ref.java:157)
	at org.apache.cassandra.utils.concurrent.Ref.ref(Ref.java:113)
	at org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy.get(SSTableReader.java:2111)
	at org.apache.cassandra.io.sstable.format.SSTableReader$DescriptorTypeTidy.<init>(SSTableReader.java:1980)
	at org.apache.cassandra.io.sstable.format.SSTableReader$DescriptorTypeTidy.get(SSTableReader.java:2017)
	at org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier.setup(SSTableReader.java:1897)
	at org.apache.cassandra.io.sstable.format.SSTableReader.setup(SSTableReader.java:1842)
	at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:530)
	at org.apache.cassandra.MockSchema.sstable(MockSchema.java:128)
	at org.apache.cassandra.MockSchema.sstable(MockSchema.java:92)
	at org.apache.cassandra.MockSchema.sstable(MockSchema.java:87)
	at org.apache.cassandra.db.lifecycle.TrackerTest.testAddInitialSSTables(TrackerTest.java:146)
{code}

I propose not to reuse cfname in MockSchema.sstable() and in fact to get rid of the shared MockSchema.cfs which has caused other test problems, see CASSANDRA-9514."	CASSANDRA	Resolved	10002	4	9644	test-failure
12968662	dtest failure in upgrade_tests.upgrade_through_versions_test.ProtoV3Upgrade_AllVersions_Skips_3_0_x_EndsAt_Trunk_HEAD.rolling_upgrade_test	"Example failure:

http://cassci.datastax.com/view/Parameterized/job/upgrade_tests-all-custom_branch_runs/12/testReport/upgrade_tests.upgrade_through_versions_test/ProtoV3Upgrade_AllVersions_Skips_3_0_x_EndsAt_Trunk_HEAD/rolling_upgrade_test_2/

The test is seeing a corrupt hint sstable after upgrade from 2.2.5 to 3.6. Relevant stack trace is

{code}
ERROR [main] 2016-05-11 16:22:25,180 CassandraDaemon.java:727 - Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at org.apache.cassandra.hints.LegacyHintsMigrator.forceCompaction(LegacyHintsMigrator.java:119) ~[main/:na]
	at org.apache.cassandra.hints.LegacyHintsMigrator.compactLegacyHints(LegacyHintsMigrator.java:108) ~[main/:na]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrate(LegacyHintsMigrator.java:92) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:321) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:581) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:710) [main/:na]
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_51]
	at org.apache.cassandra.hints.LegacyHintsMigrator.forceCompaction(LegacyHintsMigrator.java:115) ~[main/:na]
	... 5 common frames omitted
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:351) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:265) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner.hasNext(BigTableScanner.java:245) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:463) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:150) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:72) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:182) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:82) ~[main/:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:805) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_51]
Caused by: java.io.EOFException: null
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:68) ~[main/:na]
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:60) ~[main/:na]
	at org.apache.cassandra.io.util.TrackedDataInputPlus.readFully(TrackedDataInputPlus.java:93) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:400) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:375) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:109) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:89) ~[main/:na]
	at org.apache.cassandra.io.sstable.IndexInfo$Serializer.deserialize(IndexInfo.java:135) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:651) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:577) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$LegacyShallowIndexedEntry.deserialize(RowIndexEntry.java:508) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$Serializer.deserialize(RowIndexEntry.java:321) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:310) ~[main/:na]
	... 19 common frames omitted
ERROR [CompactionExecutor:2] 2016-05-11 16:22:25,183 CassandraDaemon.java:213 - Exception in thread Thread[CompactionExecutor:2,1,main]
org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/tmp/dtest-X7IReF/test/node1/data2/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-3-big-Data.db
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:351) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:265) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner.hasNext(BigTableScanner.java:245) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:463) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:150) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:72) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:182) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:82) ~[main/:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:805) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
Caused by: java.io.EOFException: null
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:68) ~[main/:na]
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:60) ~[main/:na]
	at org.apache.cassandra.io.util.TrackedDataInputPlus.readFully(TrackedDataInputPlus.java:93) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:400) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:375) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:109) ~[main/:na]
	at org.apache.cassandra.db.Serializers$1.deserialize(Serializers.java:89) ~[main/:na]
	at org.apache.cassandra.io.sstable.IndexInfo$Serializer.deserialize(IndexInfo.java:135) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:651) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$IndexedEntry.<init>(RowIndexEntry.java:577) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$LegacyShallowIndexedEntry.deserialize(RowIndexEntry.java:508) ~[main/:na]
	at org.apache.cassandra.db.RowIndexEntry$Serializer.deserialize(RowIndexEntry.java:321) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:310) ~[main/:na]
	... 19 common frames omitted
{code}

Logs are attached"	CASSANDRA	Resolved	10002	1	9644	dtest
13016172	dtest failure in replication_test.SnitchConfigurationUpdateTest.test_cannot_restart_with_different_rack	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/280/testReport/replication_test/SnitchConfigurationUpdateTest/test_cannot_restart_with_different_rack

{code}
Error Message

Problem stopping node node1
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/replication_test.py"", line 630, in test_cannot_restart_with_different_rack
    node1.stop(wait_other_notice=True)
  File ""/usr/local/lib/python2.7/dist-packages/ccmlib/node.py"", line 727, in stop
    raise NodeError(""Problem stopping node %s"" % self.name)
{code}"	CASSANDRA	Resolved	10002	1	9644	dtest, test-failure
12823511	"""timestamp"" is considered as a reserved keyword in cqlsh completion"	"cqlsh seems to treat ""timestamp"" as a reserved keyword when used as an identifier:

{code}
cqlsh:ks1> create table t1 (int int primary key, ascii ascii, bigint bigint, blob blob, boolean boolean, date date, decimal decimal, double double, float float, inet inet, text text, time time, timestamp timestamp, timeuuid timeuuid, uuid uuid, varchar varchar, varint varint);
{code}

Leads to the following completion when building an {{INSERT}} statement:

{code}
cqlsh:ks1> insert into t1 (int, 
""timestamp"" ascii       bigint      blob        boolean     date        decimal     double      float       inet        text        time        timeuuid    uuid        varchar     varint
{code}

""timestamp"" is a keyword but not a reserved one and should therefore not be proposed as a quoted string. It looks like this error happens only for timestamp. Not a big deal of course, but it might be worth reviewing the keywords treated as reserved in cqlsh, especially with the many changes introduced in 3.0."	CASSANDRA	Resolved	10003	1	9644	cqlsh
12929250	cqlsh_copy_tests failing en mass when vnodes are disabled	"Check out [an example cassci failure|http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/186/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_list_data/] as well as the [full novnode report page|http://cassci.datastax.com/userContent/cstar_report/index.html?jobs=cassandra-2.1_novnode_dtest,cassandra-3.0_novnode_dtest,cassandra-2.2_novnode_dtest&show_known=true].

Many COPY TO tests are failing when the cluster only has one token. The message {{Found no ranges to query, check begin and end tokens: None - None}} is printed, and it appears to be coming from cqlsh, specfically in pylib/cqlshlib/copyutil.py"	CASSANDRA	Resolved	10002	1	9644	dtest
12901440	Fix cqlsh bugs	"This is followup to CASSANDRA-10289

The tests currently failing should be:

* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_create_columnfamily}}
** uses {{create_columnfamily_table_template}}. Stefania says ""the {{(}} after {{CREATE ... IF}} does not look valid to me.""
* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_create_table}}
** uses {{create_columnfamily_table_template}}, see above.
* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_delete}}
** Stefania says: ""I don't think keyspaces are a valid completion after {{DELETE a [}} and after {{DELETE FROM twenty_rows_composite_table USING TIMESTAMP 0 WHERE TOKEN(a) >=}}. From a quick analysis of {{cqlhandling.py}} I think it comes from {{<term>}}, which picks up {{<functionName>}}, which was changed to include {{ks.}} by CASSANDRA-7556.
* {{cqlshlib.test.test_cqlsh_completion.TestCqlshCompletion.test_complete_in_drop_keyspace}}
** Stefania says: ""the {{;}} after {{DROP KEYSPACE IF}} is not valid.
* {{cqlshlib.test.test_cqlsh_output.TestCqlshOutput.test_timestamp_output}}
** already documented with CASSANDRA-10313 and CASSANDRA-10397

I'm happy to break these out into separate tickets if necessary. 

To run the tests locally, I cd to {{cassandra/pylib/cqlshlib}} and run the following:

{code}
ccm create -n 1 --install-dir=../.. test
ccm start --wait-for-binary-proto
nosetests test 2>&1
ccm remove
{code}

This requires nose and ccm. Until CASSANDRA-10289 is resolved, you'll have to use my branch here: https://github.com/mambocab/cassandra/tree/fix-cqlsh-tests

Tests for this branch are run (non-continuously) here:

http://cassci.datastax.com/job/scratch_mambocab-fix_cqlsh/

Assigning [~Stefania] for now, since she's already looked at 10289, but feel free to reassign."	CASSANDRA	Resolved	10002	7	9644	cqlsh
12994919	dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_reading_with_multiple_files	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/783/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_reading_with_multiple_files"	CASSANDRA	Resolved	10002	4	9644	dtest
12895551	Windows dtest 3.0: replication_test fails on Windows	"Looks like there's a regex failure taking place: [link|https://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest_win32/69/testReport/junit/replication_test/SnitchConfigurationUpdateTest/test_rf_collapse_gossiping_property_file_snitch_multi_dc/]

Rack changes appear to be working but not being picked up by test checking."	CASSANDRA	Resolved	10002	7	9644	windows
13009153	cqlsh copy tests hang in case of no answer from the server or driver	"-If we bundle the driver to cqlsh using the 3.6.0 tag or cassandra_test head, some cqlsh copy tests hang, for example {{test_bulk_round_trip_blogposts}}. See CASSANDRA-12736 and CASSANDRA-11534 for some sample failures.-

If the driver fails to invoke a callback (either error or success), or if the server never answers to the driver, then the copy parent process will wait forever to receive an answer from child processes. We should put a cap to this. We should also use a very high timeout rather than None, so that the driver will notify us if there is no answer from the server."	CASSANDRA	Resolved	10002	1	9644	cqlsh
12955973	dtest failure in sstableutil_test.SSTableUtilTest.abortedcompaction_test	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/637/testReport/sstableutil_test/SSTableUtilTest/abortedcompaction_test

Failed on CassCI build cassandra-3.0_dtest #637

Next run passed, so this could be a flaky test.

{noformat}
Error Message

0 not greater than 0
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-gbo1Uc
dtest: DEBUG: Custom init_config not found. Setting defaults.
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'start_rpc': 'true'}
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: Comparing all files...
dtest: DEBUG: Comparing final files...
dtest: DEBUG: Comparing tmp files...
dtest: DEBUG: Comparing op logs...
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db
/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt

dtest: DEBUG: Got 40 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: About to invoke sstableutil...
dtest: DEBUG: Listing files...

dtest: DEBUG: Got 0 files
dtest: DEBUG: Comparing all files...
dtest: DEBUG: Comparing final files...
dtest: DEBUG: Comparing tmp files...
dtest: DEBUG: Comparing op logs...
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/sstableutil_test.py"", line 79, in abortedcompaction_test
    self.assertGreater(len(tmpfiles), 0)
  File ""/usr/lib/python2.7/unittest/case.py"", line 942, in assertGreater
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""0 not greater than 0\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-gbo1Uc\ndtest: DEBUG: Custom init_config not found. Setting defaults.\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'start_rpc': 'true'}\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: Comparing all files...\ndtest: DEBUG: Comparing final files...\ndtest: DEBUG: Comparing tmp files...\ndtest: DEBUG: Comparing op logs...\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data0/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-4-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data1/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-1-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-2-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-3-big-TOC.txt\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-CRC.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Data.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Digest.crc32\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Filter.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Index.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Statistics.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-Summary.db\n/mnt/tmp/dtest-gbo1Uc/test/node1/data2/keyspace1/standard1-688fdfd0f83611e5ad7e97459da1b606/ma-5-big-TOC.txt\n\ndtest: DEBUG: Got 40 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: About to invoke sstableutil...\ndtest: DEBUG: Listing files...\n\ndtest: DEBUG: Got 0 files\ndtest: DEBUG: Comparing all files...\ndtest: DEBUG: Comparing final files...\ndtest: DEBUG: Comparing tmp files...\ndtest: DEBUG: Comparing op logs...\n--------------------- >> end captured logging << ---------------------""
{noformat}"	CASSANDRA	Resolved	10002	4	9644	dtest
12902842	Fix sstableverify_test dtest	"The dtest for sstableverify is failing:

http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/lastCompletedBuild/testReport/offline_tools_test/TestOfflineTools/sstableverify_test/

It fails in the same way when I run it on OpenStack, so I don't think it's just a CassCI problem.

[~slebresne] Looks like you made changes to this test recently:

https://github.com/riptano/cassandra-dtest/commit/51ab085f21e01cc8e5ad88a277cb4a43abd3f880

Could you have a look at the failure? I'm assigning you for triage, but feel free to reassign."	CASSANDRA	Resolved	10002	7	9644	test
12956737	Implement streaming for bulk read requests	Allow clients to stream data from a C* host, bypassing the coordination layer and eliminating the need to query individual pages one by one.	CASSANDRA	Resolved	10002	7	9644	protocolv5
12980230	dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_copy_to_with_child_process_crashing	"example failure:

http://cassci.datastax.com/job/cassandra-2.1_offheap_dtest/360/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_copy_to_with_child_process_crashing

Failed on CassCI build cassandra-2.1_offheap_dtest #360

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 889, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2701, in test_copy_to_with_child_process_crashing
    self.assertIn('some records might be missing', err)
  File ""/usr/lib/python2.7/unittest/case.py"", line 803, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)

Error Message

'some records might be missing' not found in ''
{code}

Logs are attached."	CASSANDRA	Resolved	10002	4	9644	dtest
12992417	SSTablesIteratedTest.testDeletionOnOverlappingIndexedSSTable-compression is flaky	"[History|https://cassci.datastax.com/view/cassandra-3.9/job/cassandra-3.9_testall/lastCompletedBuild/testReport/org.apache.cassandra.cql3.validation.miscellaneous/SSTablesIteratedTest/testDeletionOnOverlappingIndexedSSTable_compression/history/]

Error Message
expected:<2> but was:<3>

Stacktrace
{noformat}
junit.framework.AssertionFailedError: expected:<2> but was:<3>
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.executeAndCheck(SSTablesIteratedTest.java:45)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnOverlappingIndexedSSTable(SSTablesIteratedTest.java:435)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnOverlappingIndexedSSTable(SSTablesIteratedTest.java:361)
{noformat}"	CASSANDRA	Resolved	10003	4	9644	unittest
12856787	Refuse to start and print txn log information in case of disk corruption	"Transaction logs were introduced by CASSANDRA-7066 and are read during start-up. In case of file system errors, such as disk corruption, we currently log a panic error and leave the sstable files and transaction logs as they are; this is to avoid rolling back a transaction (i.e. deleting files) by mistake.

We should instead look at the {{disk_failure_policy}} and refuse to start unless the failure policy is {{ignore}}. 

We should also consider stashing files that cannot be read during startup, either transaction logs or sstables, by moving them to a dedicated sub-folder. 

"	CASSANDRA	Resolved	10002	4	9644	doc-impacting
13015321	Extend native protocol flags and add supported versions to the SUPPORTED response	"We already use 7 bits for the flags of the QUERY message, and since they are encoded with a fixed size byte, we may be forced to change the structure of the message soon, and I'd like to do this in version 5 but without wasting bytes on the wire. Therefore, I propose to convert fixed flag's bytes to unsigned vints, as defined in CASSANDRA-9499. The only exception would be the flags in the frame, which should stay as fixed size.

Up to 7 bits, vints are encoded the same as bytes are, so no immediate change would be required in the drivers, although they should plan to support vint flags if supporting version 5. Moving forward, when a new flag is required for the QUERY message, and eventually when other flags reach 8 bits in other messages too, the flag's bitmaps would be automatically encoded with a size that is big enough to accommodate all flags, but no bigger than required. We can currently support up to 8 bytes with unsigned vints.

The downside is that drivers need to implement unsigned vint encoding for version 5, but this is already required by CASSANDRA-11873, and will most likely be required by CASSANDRA-11622 as well.

I would also like to add the list of versions to the SUPPORTED message, in order to simplify the handshake for drivers that prefer to send an OPTION message, rather than rely on receiving an error for an unsupported version in the STARTUP message. Said error should also contain the full list of supported versions, not just the min and max, for clarity, and because the latest version is now a beta version.

Finally, we currently store versions as integer constants in {{Server.java}}, and we still have a fair bit of hard-coded numbers in the code, especially in tests. I plan to clean this up by introducing a {{ProtocolVersion}} enum."	CASSANDRA	Resolved	10002	7	9644	protocolv5
12976900	cqlsh copyutil should get host metadata by connected address	"pylib.copyutil presently accesses cluster metadata using {{shell.hostname}} which could be an unresolved hostname.
https://github.com/apache/cassandra/blob/58d3b9a90461806d44dd85bf4aa928e575d5fb6c/pylib/cqlshlib/copyutil.py#L207

Cluster metadata normally refers to hosts in terms of numeric host address, not hostname. This works in the current integration because the driver allows hosts with unresolved names into metadata during the initial control connection. In a future version of the driver, that anomaly is removed, and no duplicate hosts-by-name are present in the metadata.

We will need to update copyutil to refer to hosts by address when accessing metadata. This can be accomplished by one of two methods presently:

# shell.conn.control_connection.host (gives the current connected host address)
# scan metadata.all_hosts() for the one that {{is_up}} and use host.address/host.datacenter"	CASSANDRA	Resolved	10003	1	9644	cqlsh
12991488	dtest failure in write_failures_test.TestWriteFailures.test_thrift	"example failure:

http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/14/testReport/write_failures_test/TestWriteFailures/test_thrift

Failure is
{code}
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,127 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-2-big-Data.db as it does not exist
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,334 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-15-big-Data.db as it does not exist
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,337 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-31-big-Data.db as it does not exist
Unexpected error in node3 log, error: 
ERROR [NonPeriodicTasks:1] 2016-07-20 07:09:52,339 LogTransaction.java:205 - Unable to delete /tmp/dtest-CSPEFG/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-18-big-Data.db as it does not exist
{code}"	CASSANDRA	Resolved	10002	1	9644	dtest
12679553	CAS not applied on rows containing an expired ttl column	"CREATE TABLE session (
  id text,
  usr text,
  valid int,
  PRIMARY KEY (id)
);
insert into session (id, usr) values ('abc', 'abc');
update session using ttl 1 set valid = 1 where id = 'abc';
(wait 1 sec)
And 
delete from session where id = 'DSYUCTCLSOEKVLAQWNWYLVQMEQGGXD' if usr ='demo';
Yields:
 [applied] | usr
-----------+-----
     False | abc
Rather than applying the delete.

Executing:
update session set valid = null where id = 'abc';
and again
delete from session where id = 'DSYUCTCLSOEKVLAQWNWYLVQMEQGGXD' if usr ='demo';
Positively deletes the row."	CASSANDRA	Resolved	10002	1	9644	LWT
12863719	cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_all_datatypes_read fails locally	"I get this failure on my box with TZ at GMT+08:
{code}
======================================================================
FAIL: test_all_datatypes_read (cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/stefi/git/cstar/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 674, in test_all_datatypes_read
    self.assertCsvResultEqual(self.tempfile.name, results)
  File ""/home/stefi/git/cstar/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 137, in assertCsvResultEqual
    raise e
AssertionError: Element counts were not equal:
First has 1, Second has 0:  ['ascii', '1099511627776', '0xbeef', 'True', '3.140000000000000124344978758017532527446746826171875', '2.444', '1.1', '127.0.0.1', '25', '\xe3\x83\xbd(\xc2\xb4\xe3\x83\xbc\xef\xbd\x80)\xe3\x83\x8e', '2005-07-14 12:30:00', '30757c2c-584a-11e5-b2d0-9cebe804ecbe', '2471e7de-41e4-478f-a402-e99ed779be76', 'asdf', '36893488147419103232']
First has 0, Second has 1:  ['ascii', '1099511627776', '0xbeef', 'True', '3.140000000000000124344978758017532527446746826171875', '2.444', '1.1', '127.0.0.1', '25', '\xe3\x83\xbd(\xc2\xb4\xe3\x83\xbc\xef\xbd\x80)\xe3\x83\x8e', '2005-07-14 04:30:00', '30757c2c-584a-11e5-b2d0-9cebe804ecbe', '2471e7de-41e4-478f-a402-e99ed779be76', 'asdf', '36893488147419103232']
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-8VAvBl
dtest: DEBUG: Importing from csv file: /tmp/tmpGwW8yB
dtest: WARNING: Mismatch at index: 10
dtest: WARNING: Value in csv: 2005-07-14 12:30:00
dtest: WARNING: Value in result: 2005-07-14 04:30:00
--------------------- >> end captured logging << ---------------------
{code}"	CASSANDRA	Resolved	10002	4	9644	cqlsh
12781304	Remove transient RandomAccessFile usage	"There are a few places within the code base where we use a RandomAccessFile transiently to either grab fd's or channels for other operations. This is prone to access violations on Windows (see CASSANDRA-4050 and CASSANDRA-8709) - while these usages don't appear to be causing issues at this time there's no reason to keep them. The less RandomAccessFile usage in the code-base the more stable we'll be on Windows.

[SSTableReader.dropPageCache|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L2021]
* Used to getFD, have FileChannel version

[FileUtils.truncate|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/util/FileUtils.java#L188]
* Used to get file channel for channel truncate call. Only use is in index file close so channel truncation down-only is acceptable.

[MMappedSegmentedFile.createSegments|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/util/MmappedSegmentedFile.java#L196]
* Used to get file channel for mapping.

Keeping these in a single ticket as all three should be fairly trivial refactors."	CASSANDRA	Resolved	10003	4	9644	Windows
12830209	cqlsh support for native protocol v4 features	"cqlsh/python-driver need to add support for all the new 2.2 CQL features:
- {{date}} and {{time}} types - CASSANDRA-7523 - not in cqlsh yet
- {{smallint}} and {{tinyint}} types - CASSANDRA-8951 - not in the driver yet
- client warnings - CASSANDRA-8930
- tracing improvements - CASSANDRA-7807"	CASSANDRA	Resolved	10002	1	9644	cqlsh
12905520	Make cqlsh tests work when authentication is configured	"cqlsh tests break if the runner has an authentication section in their ~/.cassandra/cqlshrc, because cqlsh changes the prompt and the tests scan output for a prompt. It manifests as read timeouts while waiting for a prompt in test/run_cqlsh.py.
[This pattern|https://github.com/mambocab/cassandra/blob/1c27f9be1ba8ea10dbe843d513e23de6238dede8/pylib/cqlshlib/test/run_cqlsh.py#L30] could be generalized to match the ""<username>@cqlsh..."" prompt that arises with this config."	CASSANDRA	Resolved	10003	4	9644	cqlsh, test
12841220	Improve batchlog write path	"Currently we allocate an on-heap {{ByteBuffer}} to serialize the batched mutations into, before sending it to a distant node, generating unnecessary garbage (potentially a lot of it).

With materialized views using the batchlog, it would be nice to optimise the write path:
- introduce a new verb ({{Batch}})
- introduce a new message ({{BatchMessage}}) that would encapsulate the mutations, expiration, and creation time (similar to {{HintMessage}} in CASSANDRA-6230)
- have MS serialize it directly instead of relying on an intermediate buffer

To avoid merely shifting the temp buffer to the receiving side(s) we should change the structure of the batchlog table to use a list or a map of individual mutations."	CASSANDRA	Resolved	10002	4	9644	performance
12849577	Potential race caused by async cleanup of transaction log files	"There seems to be a potential race in the cleanup of transaction log files, introduced in CASSANDRA-7066
It's pretty hard to trigger on trunk, but it's possible to hit it via {{o.a.c.db.SecondaryIndexTest#testCreateIndex}} 

That test creates an index, then removes it to check that the removal is correctly recorded, then adds the index again to assert that it gets rebuilt from the existing data. 
The removal causes the SSTables of the index CFS to be dropped, which is a transactional operation and so writes a transaction log. When the drop is completed and the last reference to an SSTable is released, the cleanup of the transaction log is scheduled on the periodic tasks executor. The issue is that re-creating the index re-creates the index CFS. When this happens, it's possible for the cleanup of the txn log to have not yet happened. If so, the initialization of the CFS attempts to read the log to identify any orphaned temporary files. The cleanup can happen between the finding the log file and reading it's contents, which results in a {{NoSuchFileException}}

{noformat}
[junit] java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit] java.lang.RuntimeException: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:620)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionFile.getTrackedFiles(TransactionLogs.java:190)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionData.getTemporaryFiles(TransactionLogs.java:338)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs.getTemporaryFiles(TransactionLogs.java:739)
[junit]     at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getTemporaryFiles(LifecycleTransaction.java:541)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.getFilter(Directories.java:652)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:641)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:606)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:351)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:313)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:511)
[junit]     at org.apache.cassandra.index.internal.CassandraIndexer.addIndexedColumn(CassandraIndexer.java:115)
[junit]     at org.apache.cassandra.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:265)
[junit]     at org.apache.cassandra.db.SecondaryIndexTest.testIndexCreate(SecondaryIndexTest.java:467)
[junit] Caused by: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
[junit]     at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:361)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:407)
[junit]     at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
[junit]     at java.nio.file.Files.newInputStream(Files.java:152)
[junit]     at java.nio.file.Files.newBufferedReader(Files.java:2784)
[junit]     at java.nio.file.Files.readAllLines(Files.java:3202)
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:616)
[junit] 
[junit] 
[junit] Test org.apache.cassandra.db.SecondaryIndexTest FAILED
{noformat}
"	CASSANDRA	Resolved	10002	1	9644	benedict-to-commit
12938255	"SOURCE command in CQLSH 3.2 requires that ""use keyspace"" is in the cql file that you are sourcing"	"a difference in behaviour between SOURCE command in CQLSH 3.1 and 3.2. 
In CQLSH 3.1 SOURCE will NOT require ""use keyspace"" in the cql file that you execute: the ""keyspace"" directive in the qlshrc file will work and the cql file will be executed.

In CQLSH 3.2.1, SOURCE command requires that ""use keyspace"" is in the cql file that you are sourcing, otherwise it throws this error:
""No keyspace has been specified. USE a keyspace, or explicitly specify keyspace.tablename"". 
The ""keyspace"" directive in cqlshrc is overridden by source command.

steps to reproduce:
create a file called select.cql in your home directory:
{noformat}
echo ""CONSISTENCY ONE;"" > select.cql
echo ""select * from tab;"" >> select.cql
{noformat}

in cqlsh:
{noformat}
create KEYSPACE kspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
create TABLE tab ( id int primary key);
insert into tab (id) VALUES ( 1);
{noformat}

Add this to cqlsgrc:
{noformat}
[authentication]
keyspace = kspace
{noformat}

Then exit cqlsh and rerun cqlsh using the cqlshrc just modified.
Note that you are in keyspace ""kspace"".
execute:
{noformat}
source 'select.cql' 
{noformat}

this will have different behaviour in CQLSH 3.2 and 3.1"	CASSANDRA	Resolved	10002	1	9644	lhf
12822794	Figure out a better default float precision rule for cqlsh	"We currently use a {{DEFAULT_FLOAT_PRECISION}} of 5 in cqlsh with formatting {{'%.*g' % (float_precision, val)}}.  In practice, this is way too low.  For example, 12345.5 will show up as 123456.  Since the float precision is used for cqlsh's COPY TO, it's particularly important that we maintain as much precision as is practical by default.

There are some other tricky considerations, though.  If the precision is too high, python will do something like this:

{noformat}
> '%.25g' % (12345.5555555555555555,)
'12345.55555555555474711582'
{noformat}

That's not terrible, but it would be nice to avoid if we can."	CASSANDRA	Resolved	10002	4	9644	cqlsh
12997931	dtest failure in cqlshlib.test.test_cqlsh_output.TestCqlshOutput.test_describe_keyspace_output	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_cqlsh_tests/29/testReport/cqlshlib.test.test_cqlsh_output/TestCqlshOutput/test_describe_keyspace_output

{code}
Error Message

errors={'127.0.0.1': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1

{code}

http://cassci.datastax.com/job/cassandra-3.0_cqlsh_tests/lastCompletedBuild/cython=no,label=ctool-lab/testReport/cqlshlib.test.test_cqlsh_output/TestCqlshOutput/test_describe_keyspace_output/"	CASSANDRA	Resolved	10002	1	9644	dtest
12932942	COPY FROM on large datasets: fix progress report and optimize performance part 4	"h5. Description

Running COPY from on a large dataset (20G divided in 20M records) revealed two issues:

* The progress report is incorrect, it is very slow until almost the end of the test at which point it catches up extremely quickly.

* The performance in rows per second is similar to running smaller tests with a smaller cluster locally (approx 35,000 rows per second). As a comparison, cassandra-stress manages 50,000 rows per second under the same set-up, therefore resulting 1.5 times faster. 

See attached file _copy_from_large_benchmark.txt_ for the benchmark details.

h5. Doc-impacting changes to COPY FROM options

* A new option was added: PREPAREDSTATEMENTS - it indicates if prepared statements should be used; it defaults to true.
* The default value of CHUNKSIZE changed from 1000 to 5000.
* The default value of MINBATCHSIZE changed from 2 to 10."	CASSANDRA	Resolved	10002	1	9644	doc-impacting
12944756	COPY TO should have higher double precision	"At the moment COPY TO uses the same float precision as cqlsh, which by default is 5 but it can be changed in cqlshrc. However, typically people want to preserve precision when exporting data and so this default is too low for COPY TO.

I suggest adding a new COPY FROM option to specify floating point precision with a much higher default value, for example 12."	CASSANDRA	Resolved	10002	4	9644	doc-impacting, lhf
12853915	Windows utest 3.0: ColumnFamilyStoreTest.testScrubDataDirectories failing	"{noformat}
    [junit] Testcase: testScrubDataDirectories(org.apache.cassandra.db.ColumnFamilyStoreTest):  Caused an ERROR
    [junit] java.nio.file.AccessDeniedException: build\test\cassandra\data;0\ColumnFamilyStoreTest1\Standard1-a0d5fa503f8b11e58dec831ef068609c\ma-7-big-Index.db
    [junit] FSWriteError in build\test\cassandra\data;0\ColumnFamilyStoreTest1\Standard1-a0d5fa503f8b11e58dec831ef068609c\ma-7-big-Index.db
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:135)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:152)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:147)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:556)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testScrubDataDirectories(ColumnFamilyStoreTest.java:533)
    [junit] Caused by: java.nio.file.AccessDeniedException: build\test\cassandra\data;0\ColumnFamilyStoreTest1\Standard1-a0d5fa503f8b11e58dec831ef068609c\ma-7-big-Index.db
    [junit]     at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
    [junit]     at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
    [junit]     at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    [junit]     at java.nio.file.Files.delete(Files.java:1126)
    [junit]     at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:129)
{noformat}

Test has never passed on Windows ([history|http://cassci.datastax.com/view/trunk/job/trunk_utest_win32/lastCompletedBuild/testReport/org.apache.cassandra.db/ColumnFamilyStoreTest/testScrubDataDirectories/history/]).

[~stefania_alborghetti]: Looks like this is your test. Care to take this?"	CASSANDRA	Resolved	10002	7	9644	Windows
12954731	dtest failure in cql_tracing_test.TestCqlTracing.tracing_unknown_impl_test	"Failing on the following assert, on trunk only: {{self.assertEqual(len(errs[0]), 1)}}

Is not failing consistently.

example failure:

http://cassci.datastax.com/job/trunk_dtest/1087/testReport/cql_tracing_test/TestCqlTracing/tracing_unknown_impl_test

Failed on CassCI build trunk_dtest #1087"	CASSANDRA	Resolved	10002	1	9644	dtest
12991974	SSTablesIteratedTest.testDeletionOnIndexedSSTableASC-compression failure	"Error Message
expected:<3> but was:<4>

Stacktrace
junit.framework.AssertionFailedError: expected:<3> but was:<4>
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.executeAndCheck(SSTablesIteratedTest.java:45)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnIndexedSSTableASC(SSTablesIteratedTest.java:348)
	at org.apache.cassandra.cql3.validation.miscellaneous.SSTablesIteratedTest.testDeletionOnIndexedSSTableASC(SSTablesIteratedTest.java:312)

[Failure|http://cassci.datastax.com/job/cassandra-3.9_testall/lastCompletedBuild/testReport/org.apache.cassandra.cql3.validation.miscellaneous/SSTablesIteratedTest/testDeletionOnIndexedSSTableASC_compression/]
"	CASSANDRA	Resolved	10002	4	9644	unittest
12851126	cqlsh should have DESCRIBE MATERIALIZED VIEW	cqlsh doesn't currently produce describe output that can be used to recreate a MV. Needs to add a new {{DESCRIBE MATERIALIZED VIEW}} command, and also add to {{DESCRIBE KEYSPACE}}.	CASSANDRA	Resolved	10002	4	9644	doc-impacting, materializedviews
12827319	COPY TO improvements	"COPY FROM has gotten a lot of love.  COPY TO not so much.  One obvious improvement could be to parallelize reading and writing (write one page of data while fetching the next).
"	CASSANDRA	Resolved	10003	4	9644	cqlsh
12996303	dtest failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_writing_with_token_boundaries	"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest/787/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_writing_with_token_boundaries

Failed on CassCI build cassandra-3.0_dtest build #787

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 1022, in test_writing_with_token_boundaries
    self._test_writing_with_token_boundaries(10000, None, 2000000000000000000)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 1059, in _test_writing_with_token_boundaries
    self.assertItemsEqual(csv_values, result)
  File ""/usr/lib/python2.7/unittest/case.py"", line 901, in assertItemsEqual
    self.fail(msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
""Element counts were not equal:\nFirst has 0, Second has 1:  ('130', -4364617663693876050L)\nFirst has 0, Second has 1:  ('1504', -4313346088993828066L)\nFirst has 0, Second has 1:  ('1657', -4298243044528711865L)\nFirst has 0, Second has 1:  ('1908', -4357762565998238890L)\nFirst has 0, Second has 1:  ('196', -4311842292754600676L)\nFirst has 0, Second has 1:  ('2069', -4364398944370882217L)\nFirst has 0, Second has 1:  ('2840', -4341639477649832153L)\nFirst has 0, Second has 1:  ('2887', -4318016824479819783L)\nFirst has 0, Second has 1:  ('2899', -4302748366908469185L)\nFirst has 0, Second has 1:  ('2928', -4320094196758787736L)\nFirst has 0, Second has 1:  ('2985', -4314356124534988584L)\nFirst has 0, Second has 1:  ('3684', -4338074463992249966L)\nFirst has 0, Second has 1:  ('371', -4314424123257001171L)\nFirst has 0, Second has 1:  ('3726', -4327342039280507889L)\nFirst has 0, Second has 1:  ('3767', -4314615789624913427L)\nFirst has 0, Second has 1:  ('3837', -4345782419910891107L)\nFirst has 0, Second has 1:  ('3917', -4288469607605675346L)\nFirst has 0, Second has 1:  ('4023', -4327319429102869913L)\nFirst has 0, Second has 1:  ('4340', -4364719196309290555L)\nFirst has 0, Second has 1:  ('4775', -4334399295585005795L)\nFirst has 0, Second has 1:  ('480', -4297721626756162038L)\nFirst has 0, Second has 1:  ('4927', -4363012199808638126L)\nFirst has 0, Second has 1:  ('5227', -4322405738833807588L)\nFirst has 0, Second has 1:  ('564', -4294201317243228473L)\nFirst has 0, Second has 1:  ('585', -4359001293509999319L)\nFirst has 0, Second has 1:  ('5869', -4350305245827564608L)\nFirst has 0, Second has 1:  ('6907', -4350623491924194304L)\nFirst has 0, Second has 1:  ('709', -4304008865600291097L)\nFirst has 0, Second has 1:  ('7415', -4315752378065264743L)\nFirst has 0, Second has 1:  ('7476', -4300546270541034340L)\nFirst has 0, Second has 1:  ('7805', -4344641724309508742L)\nFirst has 0, Second has 1:  ('7922', -4363605089028496367L)\nFirst has 0, Second has 1:  ('8026', -4319008002233878821L)\nFirst has 0, Second has 1:  ('8180', -4361912691055780971L)\nFirst has 0, Second has 1:  ('8371', -4309172311179179912L)\nFirst has 0, Second has 1:  ('8988', -4326093437666683541L)\nFirst has 0, Second has 1:  ('9492', -4347264403260361686L)\nFirst has 0, Second has 1:  ('9783', -4329297319597600121L)\nFirst has 0, Second has 1:  ('9911', -4320490295580904236L)\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-2v8O54\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: Exporting to csv file: /tmp/tmpDS47Vq\ndtest: DEBUG: Exporting to csv file: /tmp/tmpQvnl6e\ndtest: DEBUG: Exporting to csv file: /tmp/tmpfEEiAz\ndtest: DEBUG: Exporting to csv file: /tmp/tmpc7T8av\ndtest: DEBUG: Exporting to csv file: /tmp/tmplxBTNa\ndtest: DEBUG: Exporting to csv file: /tmp/tmpb5VYGq\n--------------------- >> end captured logging << ---------------------""
{code}"	CASSANDRA	Resolved	10002	1	9644	dtest
