id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13308032	It will throw Invalid lambda deserialization Exception when writing to elastic search with new format	"My job follows:
{code:java}
// 
create table csv( pageId VARCHAR, eventId VARCHAR, recvTime VARCHAR) with ( 'connector' = 'filesystem',
 'path' = '/Users/ohmeatball/Work/flink-sql-etl/data-generator/src/main/resources/user3.csv',
 'format' = 'csv'
 )
-----------------------------------------
CREATE TABLE es_table (
  aggId varchar ,
  pageId varchar ,
  ts varchar ,
  expoCnt int ,
  clkCnt int
) WITH (
'connector' = 'elasticsearch',
'hosts' = 'http://localhost:9200',
'index' = 'cli_test',
'document-id.key-delimiter' = '$',
'sink.bulk-flush.interval' = '1000',
'format' = 'json'
)
-----------------------------------------
INSERT INTO es_table
SELECT  pageId,eventId,cast(recvTime as varchar) as ts, 1, 1 from csv;
{code}
The full exception follows:
{code:java}
Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:471) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:459) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:155) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]Caused by: java.io.IOException: unexpected exception type at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1682) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1254) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
Caused by: java.lang.reflect.InvocationTargetExceptionCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreCaused by: java.lang.IllegalArgumentException: Invalid lambda deserialization at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink$Builder.$deserializeLambda$(ElasticsearchSink.java:80) ~[flink-sql-connector-elasticsearch7_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151]
 at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
{code}
Notice: everything works fine with former connector grammer."	FLINK	Closed	1	1	3568	pull-request-available
13187210	Add skip to next strategy	Add skip to next strategy, that should discard all partial matches that started with the same element as found match.	FLINK	Resolved	3	4	3568	pull-request-available
13391141	Add a global flag for enabling/disabling final checkpoints	We should have a feature toggle for the final checkpoint story.	FLINK	Closed	3	7	3568	pull-request-available
13340904	Avro Confluent Registry SQL format does not support adding nullable columns	"The {{AvroSchemaConverter#convertToSchema}} generates a union with ""null"" for nullable logical types, but it does not set the default value to null. In turn it makes it impossible to generate a backwards compatible schema from a DDL statement.

Example:
1. Create a table: {{CREATE TABLE t (id INT NOT NULL) WITH (/* avro confluent format*/)}}
2. Create a new table over the same topic or alter the old table with {{CREATE TABLE newT(id INT NOT NULL, optionalDescription STRING) WITH (/* avro confluent format */)}}
3. When reading from {{newT}} records inserted into {{t}} it will fail, because the {{optionalDescription}} has no default value."	FLINK	Closed	2	1	3568	pull-request-available
13248587	Include table examples in flink-dist	"We want to treat the table api as first-class API. We already included in the lib directory flink.
We should also include some examples of the table api in the distribution.

Before that we should strip all the dependency and just include the classes from  example module."	FLINK	Closed	2	4	3568	pull-request-available
13550488	SupportsReadingMetadata is not applied when loading a CompiledPlan	"If a few conditions are met, we can not apply ReadingMetadata interface:
# source overwrites:
 {code}
    @Override
    public boolean supportsMetadataProjection() {
        return false;
    }
 {code}
# source does not implement {{SupportsProjectionPushDown}}
# table has metadata columns e.g.
{code}
CREATE TABLE src (
  physical_name STRING,
  physical_sum INT,
  timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL
)
{code}
# we query the table {{SELECT * FROM src}}

It fails with:
{code}
Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)
{code}

The reason is {{SupportsReadingMetadataSpec}} is created only in the {{PushProjectIntoTableSourceScanRule}}, but the rule is not applied when 1 & 2"	FLINK	Closed	3	1	3568	pull-request-available
13437148	JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate failed on azure	"
{code:java}
2022-03-31T06:11:52.2333685Z Mar 31 06:11:52 [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 35.288 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase
2022-03-31T06:11:52.2336004Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate  Time elapsed: 15.008 s  <<< FAILURE!
2022-03-31T06:11:52.2336907Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2337353Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2337876Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2338631Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2339436Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2340599Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate(JobMasterStopWithSavepointITCase.java:136)
2022-03-31T06:11:52.2342251Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2342896Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2343608Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2344234Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2344873Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2345590Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2346498Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2347221Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2347922Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2348580Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2349222Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2349860Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2350502Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2351172Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2352095Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2352949Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2353643Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2354298Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2354909Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2355535Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2356505Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2357142Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2357771Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2358400Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2359014Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2359614Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2360221Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2371694Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2372907Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2373992Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2375195Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2376592Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2377778Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2379338Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2380786Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2382151Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2383487Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2384979Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2386341Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2387454Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2389081Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2390447Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2391930Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2393389Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2394759Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2395544Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2396673Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2397347Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2397932Z Mar 31 06:11:52 
2022-03-31T06:11:52.2398639Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend  Time elapsed: 15.004 s  <<< FAILURE!
2022-03-31T06:11:52.2399342Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2399793Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2400311Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2400837Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2401633Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2402751Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend(JobMasterStopWithSavepointITCase.java:130)
2022-03-31T06:11:52.2403623Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2404247Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2404961Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2405936Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2406676Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2407520Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2408242Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2409245Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2409940Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2410604Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2411358Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2412174Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2412786Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2413640Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2414856Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2416140Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2417502Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2418495Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2419110Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2419737Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2420361Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2420986Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2421601Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422359Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422969Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2423569Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2424331Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2424922Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2425464Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2426334Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2427379Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2428432Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2429538Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2430713Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2431900Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2433166Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2434372Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2435500Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2436771Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2437877Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2439206Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2440452Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2441694Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2442881Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2443999Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2445104Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2446367Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2447434Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2448170Z Mar 31 06:11:52 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5183"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13374114	Unnecessary entries in sql hbase-1.4 connector NOTICE file	"The NOTICE file for flink-sql-connector-hbase-1.4 lists dependencies that it does not bundle:

* commons-configuration:commons-configuration:1.7
* org.apache.hbase:hbase-prefix-tree:1.4.3
* org.apache.hbase:hbase-procedure:1.4.3"	FLINK	Closed	2	7	3568	pull-request-available
13296468	Support creating tables using other tables definition	We should be able to create a Table based on properties of other tables. This includes merging the properties and creating a new Table based on that.	FLINK	Closed	3	7	3568	pull-request-available
13364698	NullPointerException on restore in PojoSerializer	"As originally reported in [thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Schema-Evolution-Cannot-restore-from-savepoint-after-deleting-field-from-POJO-td42162.html], after removing a field from a class restore from savepoint fails with the following exception:

{code}
2021-03-10T20:51:30.406Z INFO  org.apache.flink.runtime.taskmanager.Task:960 … (6/8) (d630d5ff0d7ae4fbc428b151abebab52) switched from RUNNING to FAILED. java.lang.Exception: Exception while creating StreamOperatorStateContext.
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:253)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:901)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:415)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedCoProcessOperator_c535ac415eeb524d67c88f4a481077d2_(6/8) from any of the 1 provided restore options.
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:307)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:135)
        ... 6 common frames omitted
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
        at org.apache.flink.runtime.state.memory.MemoryStateBackend.createKeyedStateBackend(MemoryStateBackend.java:347)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:291)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
        ... 8 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.<init>(PojoSerializer.java:123)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:186)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:56)
        at org.apache.flink.api.common.typeutils.CompositeSerializer$PrecomputedParameters.precompute(CompositeSerializer.java:228)
        at org.apache.flink.api.common.typeutils.CompositeSerializer.<init>(CompositeSerializer.java:51)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializer.<init>(TtlStateFactory.java:250)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:359)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:330)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546)
        at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
        at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:505)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.snapshotsToRestoreSerializers(NestedSerializersSnapshotDelegate.java:225)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.getRestoredNestedSerializers(NestedSerializersSnapshotDelegate.java:83)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at org.apache.flink.runtime.state.StateSerializerProvider.previousSchemaSerializer(StateSerializerProvider.java:189)
        at org.apache.flink.runtime.state.StateSerializerProvider.currentSchemaSerializer(StateSerializerProvider.java:164)
        at org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.getStateSerializer(RegisteredKeyValueStateBackendMetaInfo.java:136)
        at org.apache.flink.runtime.state.heap.StateTable.getStateSerializer(StateTable.java:315)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:54)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:36)
        at org.apache.flink.runtime.state.heap.StateTable.<init>(StateTable.java:98)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.<init>(CopyOnWriteStateTable.java:49)
        at org.apache.flink.runtime.state.heap.AsyncSnapshotStrategySynchronicityBehavior.newStateTable(AsyncSnapshotStrategySynchronicityBehavior.java:41)
        at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.newStateTable(HeapSnapshotStrategy.java:243)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.createOrCheckStateForMetaInfo(HeapRestoreOperation.java:185)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:152)
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
        ... 12 common frames omitted
{code}
"	FLINK	Closed	1	1	3568	pull-request-available
13313439	Can not select fields with JdbcCatalog	"A query which selects fields from a table will fail if we set the PostgresCatalog as default.

Steps to reproduce:
 # Create postgres catalog and set it as default
 # Create any table (in any catalog)
 # Query that table with {{SELECT field FROM t}} (Important it must be a field name not '{{*}}'
 #  The query will fail

Stack trace:
{code}
org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement.
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:100) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:91) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:257) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:211) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. null
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
Caused by: java.lang.UnsupportedOperationException
	at org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.getFunction(AbstractJdbcCatalog.java:261) ~[?:?]
	at org.apache.flink.table.catalog.FunctionCatalog.resolvePreciseFunctionReference(FunctionCatalog.java:570) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lambda$resolveAmbiguousFunctionReference$2(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
	at org.apache.flink.table.catalog.FunctionCatalog.resolveAmbiguousFunctionReference(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:370) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:99) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:73) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.makeNullaryCall(SqlValidatorImpl.java:1754) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5987) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6154) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6140) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectExpr(SqlValidatorImpl.java:5574) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:452) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4255) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3523) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
{code}

The problem is that Calcite will try to check first if there is a built-in function with that name that allows calls without parenthesis. Therefore it will query the {{FunctionCatalog}} for that function. The logic in {{org.apache.flink.table.catalog.FunctionCatalog#lookupFunction}} is such that it will call {{JdbcCatalog#getFunction}} in the end, which in case of {{AbstractJdbcCatalog}} throws {{UnsupportedOperationException}}.
"	FLINK	Closed	1	1	3568	pull-request-available
13285020	Introduce a Java Expression DSL	Introduce the basic expressions. The new Java expression DSL should be feature equivalent to string based expression parser. It does not support calls with new inference stack yet.	FLINK	Closed	3	7	3568	pull-request-available
13317135	Docker e2e tests are failing on CI	"{code}
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=host&nocache=1&rm=1&session=z0y5c0io3wt7m3uqfb7zo7uds&shmsize=0&t=test_docker_embedded_job&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied
{code}
Will have to wait for [~rmetzger] to get back."	FLINK	Closed	3	4	3568	pull-request-available
13240421	Convert CatalogView to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	"Similar to [FLINK-12257] we should convert Flink's views to Calcite's views.

The tricky part is that we have to pass around the SqlParser somehow."	FLINK	Closed	3	7	3568	pull-request-available
13341804	Building flink-dist docker image does not work without python2	"The script {{common_docker.sh}} in function {{start_file_server}} tests existence of {{python3}}, but executes command using {{python}}:

{code}
    command -v python3 >/dev/null 2>&1
    if [[ $? -eq 0 ]]; then
      python ${TEST_INFRA_DIR}/python3_fileserver.py &
      return
    fi
{code}

The script {{python3_fileserver.py}} uses python2 {{SocketServer}} which does not exist in python3. It should use {{socketserver}}."	FLINK	Closed	2	1	3568	pull-request-available
13236795	Introduce Table API Planner interface	The planner interface is the bridge between base API and different planner modules. 	FLINK	Closed	3	4	3568	pull-request-available
13363745	DegreesWithExceptionITCase crash	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14422&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b

"	FLINK	Closed	1	7	3568	pull-request-available, test-stability
13141178	Scala shell broken for Flip6	"I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?

I followed the instructions from the Flink master branch so I did the following.
{code:java}
git clone https://github.com/apache/flink.git 
cd flink mvn clean package -DskipTests 
cd build-target
 ./bin/start-scala-shell.sh local{code}
{{And Here is the code I ran}}
{code:java}
val dataStream = senv.fromElements(1, 2, 3, 4)
dataStream.countWindowAll(2).sum(0).print()
senv.execute(""My streaming program""){code}
{{And I finally get this exception}}
{code:java}
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281){code}
 "	FLINK	Resolved	1	1	3568	pull-request-available
13583654	JSON_QUERY should return a well formatted nested objects/arrays for ARRAY<STRING>	"{code}
SELECT JSON_QUERY('{""items"": [{""itemId"":1234, ""count"":10}]}', '$.items' RETURNING ARRAY<STRING>)
{code}

returns

{code}
['{itemId=1234, count=10}']
{code}

but it should:

{code}
['{""itemId"":1234, ""count"":10}']
{code}

We should call jsonize for Collection types here: https://github.com/apache/flink/blob/f6f88135b3a5fa5616fe905346e5ab6dce084555/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/SqlJsonUtils.java#L268"	FLINK	Closed	3	1	3568	pull-request-available
13324973	Remove deprecated methods in ExecutionConfig	"We can remove:

- ExecutionConfig#isLatencyTrackingEnabled (deprecated in 1.7) 

Additionally, we should remove no-ops methods in ExecutionConfig.

    - ExecutionConfig#disable/enableSysoutLogging (deprecated in 1.10)
    - ExecutionConfig#set/isFailTaskOnCheckpointError (deprecated in 1.9) 

They are {{Public}}, however they became no-op operations, which can be argued already broke the stability guarantees."	FLINK	Closed	3	7	3568	pull-request-available
13556216	Implement type inference for reinterpret_cast function	https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/Reinterpret.scala	FLINK	Closed	3	7	3568	pull-request-available
13231976	Port utility methods for extracting fields information from TypeInformation	We need those methods in the api-module in order to create {{Table}} out of {{DataSet/Stream}}.	FLINK	Closed	3	4	3568	pull-request-available
13301012	flink legacy planner should not use CollectionEnvironment any more	"As discussed in https://github.com/apache/flink/pull/11794，{{CollectionEnvironment}} is not a good practice, as it is not going through all the steps that a regular user program would go. We should change the tests to use {{LocalEnvironment}}. 

commit ""Introduce CollectionPipelineExecutor for CollectionEnvironment ([c983ac9|https://github.com/apache/flink/commit/c983ac9c49b7b58394574efdde4f39e8d33a8582])""  should also be reverted at that moment."	FLINK	Closed	3	4	3568	pull-request-available
13380486	Performance regression on 25.05	"Tests such as:
* multiInputMapSink
* multiInputOneIdleMapSink
* readFileSplit

show regressions.

Regression in run for range: 80ad5b3b51-bb597ea-1621977169

It is most probably caused by: https://github.com/apache/flink/commit/ee9f9b227a7703c2688924070c4746a70bff3fd8"	FLINK	Closed	3	4	3568	pull-request-available
13572433	Parsing temporal table join throws cryptic exceptions	"1. Wrong expression type in {{AS OF}}:
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF 'o.rowtime' AS r "" +
      ""ON o.currency = r.currency
{code}

throws: 

{code}
java.lang.AssertionError: cannot convert CHAR literal to class org.apache.calcite.util.TimestampString
{code}

2. Not a simple table reference in {{AS OF}}
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF o.rowtime + INTERVAL '1' SECOND AS r "" +
      ""ON o.currency = r.currency
{code}

throws:
{code}
java.lang.AssertionError: no unique expression found for {id: o.rowtime, prefix: 1}; count is 0
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13434368	TimestampsAndWatermarksOperator should not propagate WatermarkStatus	The lifecycle/scope of WatermarkStatus is tightly coupled with watermarks. Upstream watermarks are cut off in the TimestampsAndWatermarksOperator and therefore watermark statuses should be cut off as well.	FLINK	Closed	3	1	3568	pull-request-available
13269264	Use higher granularity units in generated docs for Duration & MemorySize if possible	"It was mentioned on two occasions (https://github.com/apache/flink/pull/10216#discussion_r346866339, https://github.com/apache/flink/pull/10217/files#r347491314) that it would be better to use a higher granularity units if it is possible.

Right now for a default value of {{Duration.ofMinutes(1)}} the generated documentation will be printed as:

{{60000ms}}

but it would be better readable to print it as:

{{1min}}"	FLINK	Closed	3	4	3568	pull-request-available
13391350	InputStatus should not contain END_OF_RECOVERY	"We added the END_OF_RECOVERY enum value in order to support recovery of unaligned checkpoints with rescaling.

However the InputStatus is expose in a public interface via {{SourceReader}}. At the same time it is not a valid value which the {{SourceReader}} can return.

We should internally replace the InputStatus with an internal equivalent."	FLINK	Closed	3	1	3568	pull-request-available
13300004	Add open method to DeserializationSchema	"Additionally add support for it in connectors:
* Kafka
* PubSub
* RabbitMQ
* Kinesis"	FLINK	Closed	3	7	3568	pull-request-available
13263712	Update documentation regarding Temporary Objects	"* update references to deprecated methods
* describe the concept of temporary tables"	FLINK	Closed	3	7	3568	pull-request-available
13596954	Support LEAD/LAG functions in Table API	We should natively support LAG/LEAD functions in Table API.	FLINK	Open	3	4	3568	pull-request-available
13304842	TableEnvironment fromValues not work with map type and SQL	"{code:java}
Map<Integer, Integer> mapData = new HashMap<>();
      mapData.put(1, 1);
      mapData.put(2, 2);
      Row row = Row.of(mapData);
      tEnv().createTemporaryView(""values_t"", tEnv().fromValues(Collections.singletonList(row)));
      Iterator<Row> iter = tEnv().executeSql(""select * from values_t"").collect();

      List<Row> results = new ArrayList<>();
      while (iter.hasNext()) {
         results.add(iter.next());
      }
      System.out.println(results);
{code}
Not work, will occur exception:
{code:java}
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP f0) NOT NULL
converted type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP NOT NULL f0) NOT NULL
{code}
If change to {{Iterator<Row> iter = tEnv().from(""values_t"").execute().collect();}} will work."	FLINK	Closed	3	1	3568	pull-request-available
13201055	Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution	"If a time interval was specified with {{YEAR TO MONTH}} resolution like e.g.:
{code}
SELECT * 
FROM Mytable
GROUP BY 
    TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)
{code}
it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution."	FLINK	Closed	3	1	3568	pull-request-available
13320624	Create an uber jar when packaging flink-avro for sql client	"Currently users have to provide dependencies such as avro, jackson-core-asl, jackson-mapper-asl and joda-time in the job jar for DataStream jobs, or manually copy them into flink/lib in SQL jobs when using avro formatting.

It's better to generate an uber jar including these dependencies when packaging flink-avro module. "	FLINK	Closed	3	4	3568	pull-request-available
13238695	Port TableEnvironment to flink-api modules	"{{TableEnvironments}} should be a purely API class(es). Current implementation should be split into {{CatalogManager}} and {{Planner}}. The {{Planner}} should be discovered based on configuration. This will allow using either the legacy or the Blink planner.

This applies to the {{StreamTableEnvironment}}. The {{BatchTableEnvironment}} will be left as is. One will be able to use the new {{StreamTableEnvironment}} for stream processing with the legacy planner or stream and batch for the Blink {{Planner}}."	FLINK	Closed	3	4	3568	pull-request-available
13565276	Set ALWAYS ChainingStrategy in TemporalSort	Similarly to FLINK-27992 we should ALWAYS chaining strategy in TemporalSort operator	FLINK	Closed	3	1	3568	pull-request-available
13303573	BatchTableEnvironment#fromValues(Object... values) throws StackOverflowError 	"The Error can be reproduced by following code:
{code:java}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);
tEnv.fromValues(1L, 2L, 3L);
{code}
The Error is as following:
{code:java}
Exception in thread ""main"" java.lang.StackOverflowErrorException in thread ""main"" java.lang.StackOverflowError at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110) at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.expressions.ApiExpressionUtils.convertArray(ApiExpressionUtils.java:142) at org.apache.flink.table.expressions.ApiExpressionUtils.objectToExpression(ApiExpressionUtils.java:100) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1030) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163)
...{code}"	FLINK	Closed	3	1	3568	pull-request-available
13313068	Can not create a catalog from user jar	"The {{CREATE CATALOG}} statement does not work if the catalog implementation comes from the user classloader. The problem is that {{org.apache.flink.table.planner.operations.SqlToOperationConverter#convertCreateCatalog}} uses the {{SqlToOperationConverter}} classloader.

We should use {{Thread.currentThread().getContextClassloader()}} for now.

One of the ways to reproduce it is try to create e.g. a postgres catalog with the {{flink-connector-jdbc}} passed as an additional jar to {{sql--client}}"	FLINK	Closed	2	1	3568	pull-request-available
13383342	Deprecate/Remove StreamOperator#dispose method	"As discussed in the ML thread (e.g. https://lists.apache.org/thread.html/r34a05c77bddb2a7cb550c0b820d2a4aa8e1be882fc81bee501fb74e8%40%3Cdev.flink.apache.org%3E) we want to clean up the contract of {{close}} and {{dispose}} methods. 

We suggest introducing a new method finish(), deprecate or remove the dispose method and extract finish() part out of the close()."	FLINK	Closed	3	7	3568	pull-request-available
13297293	Improve literals conversion in ExpressionConverter	"There are couple of issues with the {{ExpressionResolver}} and literals conversion:
1. There is a lot of code duplication
2. Precision of certain types might get lost e.g. BINARY, CHAR"	FLINK	Closed	3	1	3568	pull-request-available
13370144	Test display last n exceptions/causes for job restarts in Web UI	This is the testing task for FLINK-6042. We should test whether the root causes for multiple restarts are properly displayed in the web UI.	FLINK	Closed	1	4	3568	pull-request-available, release-testing
13378788	Incompatible subtask mappings while resuming from unaligned checkpoints	"A user [reported|https://lists.apache.org/x/list.html?user@flink.apache.org:lte=1M:Flink%201.13.0%20reactive%20mode:%20Job%20stop%20and%20cannot%20restore%20from%20checkpoint] that he encountered an internal error while resuming during reactive mode. There isn't an immediate connection to reactive mode, so it's safe to assume that one rescaling case was not covered.

{noformat}
Caused by: java.lang.IllegalStateException: Incompatible subtask mappings: are multiple operators ingesting/producing intermediate results with varying degrees of parallelism?Found RescaleMappings{mappings=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179], [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]]} and RescaleMappings{mappings=[[0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98, 105, 112, 119, 126, 133, 140, 147, 154, 161, 168, 175, 182, 189, 196, 203], [1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99, 106, 113, 120, 127, 134, 141, 148, 155, 162, 169, 176, 183, 190, 197, 204], [2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86, 93, 100, 107, 114, 121, 128, 135, 142, 149, 156, 163, 170, 177, 184, 191, 198, 205], [3, 10, 17, 24, 31, 38, 45, 52, 59, 66, 73, 80, 87, 94, 101, 108, 115, 122, 129, 136, 143, 150, 157, 164, 171, 178, 185, 192, 199, 206], [4, 11, 18, 25, 32, 39, 46, 53, 60, 67, 74, 81, 88, 95, 102, 109, 116, 123, 130, 137, 144, 151, 158, 165, 172, 179, 186, 193, 200, 207], [5, 12, 19, 26, 33, 40, 47, 54, 61, 68, 75, 82, 89, 96, 103, 110, 117, 124, 131, 138, 145, 152, 159, 166, 173, 180, 187, 194, 201, 208], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97, 104, 111, 118, 125, 132, 139, 146, 153, 160, 167, 174, 181, 188, 195, 202, 209]]}.
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.checkSubtaskMapping(TaskStateAssignment.java:322) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.getInputMapping(TaskStateAssignment.java:306) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.reDistributeInputChannelStates(StateAssignmentOperation.java:409) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignAttemptState(StateAssignmentOperation.java:193) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignStates(StateAssignmentOperation.java:139) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1566) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1646) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphAndRestoreState(AdaptiveScheduler.java:986) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$createExecutionGraphAndRestoreStateAsync$25(AdaptiveScheduler.java:976) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.BackgroundTask.lambda$new$0(BackgroundTask.java:57) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.lang.Thread.run(Thread.java:834) ~[?:?]
{noformat}

Here it seems that the same gate gets input from a range-partitioned and a round-robin partitioned channel at the same time. During the implementation of FLINK-19801, we couldn't find such a case and optimized the implementation accordingly.

We have asked the user to provide his topology."	FLINK	Closed	1	1	3568	pull-request-available
13556214	Implement type inference for Over function	"https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/overOffsets.scala

Functions:
* OVER
* CURRENT_RANGE
* CURRENT_ROW
* UNBOUNDED_ROW
* UNBOUNDED_RANGE"	FLINK	Closed	3	7	3568	pull-request-available
13580266	BlockStatementGrouper uses lots of memory	"For deeply nested {{if else}} statements {{BlockStatementGrouper}} uses loads of memory and fails with OOM quickly.

When running JMs with around 400mb a query like:
{code}
select case when orderid = 0 then 1 when orderid = 1 then 2 when orderid
    = 2 then 3 when orderid = 3 then 4 when orderid = 4 then 5 when orderid = 5 then
    6 when orderid = 6 then 7 when orderid = 7 then 8 when orderid = 8 then 9 when
    orderid = 9 then 10 when orderid = 10 then 11 when orderid = 11 then 12 when orderid
    = 12 then 13 when orderid = 13 then 14 when orderid = 14 then 15 when orderid
    = 15 then 16 when orderid = 16 then 17 when orderid = 17 then 18 when orderid
    = 18 then 19 when orderid = 19 then 20 when orderid = 20 then 21 when orderid
    = 21 then 22 when orderid = 22 then 23 when orderid = 23 then 24 when orderid
    = 24 then 25 when orderid = 25 then 26 when orderid = 26 then 27 when orderid
    = 27 then 28 when orderid = 28 then 29 when orderid = 29 then 30 when orderid
    = 30 then 31 when orderid = 31 then 32 when orderid = 32 then 33 when orderid
    = 33 then 34 when orderid = 34 then 35 when orderid = 35 then 36 when orderid
    = 36 then 37 when orderid = 37 then 38 when orderid = 38 then 39 when orderid
    = 39 then 40 when orderid = 40 then 41 when orderid = 41 then 42 when orderid
    = 42 then 43 when orderid = 43 then 44 when orderid = 44 then 45 when orderid
    = 45 then 46 when orderid = 46 then 47 when orderid = 47 then 48 when orderid
    = 48 then 49 when orderid = 49 then 50 when orderid = 50 then 51 when orderid
    = 51 then 52 when orderid = 52 then 53 when orderid = 53 then 54 when orderid
    = 54 then 55 when orderid = 55 then 56 when orderid = 56 then 57 when orderid
    = 57 then 58 when orderid = 58 then 59 when orderid = 59 then 60 when orderid
    = 60 then 61 when orderid = 61 then 62 when orderid = 62 then 63 when orderid
    = 63 then 64 when orderid = 64 then 65 when orderid = 65 then 66 when orderid
    = 66 then 67 when orderid = 67 then 68 when orderid = 68 then 69 when orderid
    = 69 then 70 when orderid = 70 then 71 when orderid = 71 then 72 when orderid
    = 72 then 73 when orderid = 73 then 74 when orderid = 74 then 75 when orderid
    = 75 then 76 when orderid = 76 then 77 when orderid = 77 then 78 when orderid
    = 78 then 79 when orderid = 79 then 80 when orderid = 80 then 81 when orderid
    = 81 then 82 when orderid = 82 then 83 when orderid = 83 then 84 when orderid
    = 84 then 85 when orderid = 85 then 86 when orderid = 86 then 87 when orderid
    = 87 then 88 when orderid = 88 then 89 when orderid = 89 then 90 when orderid
    = 90 then 91 when orderid = 91 then 92 when orderid = 92 then 93 when orderid
    = 93 then 94 when orderid = 94 then 95 when orderid = 95 then 96 when orderid
    = 96 then 97 when orderid = 97 then 98 when orderid = 98 then 99 when orderid
    = 99 then 100 when orderid = 100 then 101 when orderid = 101 then 102 when orderid
    = 102 then 103 when orderid = 103 then 104 when orderid = 104 then 105 when orderid
    = 105 then 106 when orderid = 106 then 107 when orderid = 107 then 108 when orderid
    = 108 then 109 when orderid = 109 then 110 when orderid = 110 then 111 when orderid
    = 111 then 112 when orderid = 112 then 113 when orderid = 113 then 114 when orderid
    = 114 then 115 when orderid = 115 then 116 when orderid = 116 then 117 when orderid
    = 117 then 118 when orderid = 118 then 119 when orderid = 119 then 120 when orderid
    = 120 then 121 when orderid = 121 then 122 when orderid = 122 then 123 when orderid
    = 123 then 124 when orderid = 124 then 125 when orderid = 125 then 126 when orderid
    = 126 then 127 when orderid = 127 then 128 when orderid = 128 then 129 when orderid
    = 129 then 130 when orderid = 130 then 131 when orderid = 131 then 132 when orderid
    = 132 then 133 when orderid = 133 then 134 when orderid = 134 then 135 when orderid
    = 135 then 136 when orderid = 136 then 137 when orderid = 137 then 138 when orderid
    = 138 then 139 when orderid = 139 then 140 when orderid = 140 then 141 when orderid
    = 141 then 142 when orderid = 142 then 143 when orderid = 143 then 144 when orderid
    = 144 then 145 when orderid = 145 then 146 when orderid = 146 then 147 when orderid
    = 147 then 148 when orderid = 148 then 149 when orderid = 149 then 150 when orderid
    = 150 then 151 when orderid = 151 then 152 when orderid = 152 then 153 when orderid
    = 153 then 154 when orderid = 154 then 155 when orderid = 155 then 156 when orderid
    = 156 then 157 when orderid = 157 then 158 when orderid = 158 then 159 when orderid
    = 159 then 160 when orderid = 160 then 161 when orderid = 161 then 162 when orderid
    = 162 then 163 when orderid = 163 then 164 when orderid = 164 then 165 when orderid
    = 165 then 166 when orderid = 166 then 167 when orderid = 167 then 168 when orderid
    = 168 then 169 when orderid = 169 then 170 when orderid = 170 then 171 when orderid
    = 171 then 172 when orderid = 172 then 173 when orderid = 173 then 174 when orderid
    = 174 then 175 when orderid = 175 then 176 when orderid = 176 then 177 when orderid
    = 177 then 178 when orderid = 178 then 179 when orderid = 179 then 180 when orderid
    = 180 then 181 when orderid = 181 then 182 when orderid = 182 then 183 when orderid
    = 183 then 184 when orderid = 184 then 185 when orderid = 185 then 186 when orderid
    = 186 then 187 when orderid = 187 then 188 when orderid = 188 then 189 when orderid
    = 189 then 190 when orderid = 190 then 191 when orderid = 191 then 192 when orderid
    = 192 then 193 when orderid = 193 then 194 when orderid = 194 then 195 when orderid
    = 195 then 196 when orderid = 196 then 197 when orderid = 197 then 198 when orderid
    = 198 then 199 when orderid = 199 then 200 when orderid = 200 then 201 when orderid
    = 201 then 202 when orderid = 202 then 203 when orderid = 203 then 204 when orderid
    = 204 then 205 when orderid = 205 then 206 when orderid = 206 then 207 when orderid
    = 207 then 208 when orderid = 208 then 209 when orderid = 209 then 210 when orderid
    = 210 then 211 when orderid = 211 then 212 when orderid = 212 then 213 when orderid
    = 213 then 214 when orderid = 214 then 215 when orderid = 215 then 216 when orderid
    = 216 then 217 when orderid = 217 then 218 when orderid = 218 then 219 when orderid
    = 219 then 220 when orderid = 220 then 221 when orderid = 221 then 222 when orderid
    = 222 then 223 when orderid = 223 then 224 when orderid = 224 then 225 when orderid
    = 225 then 226 when orderid = 226 then 227 when orderid = 227 then 228 when orderid
    = 228 then 229 when orderid = 229 then 230 when orderid = 230 then 231 when orderid
    = 231 then 232 when orderid = 232 then 233 when orderid = 233 then 234 when orderid
    = 234 then 235 when orderid = 235 then 236 when orderid = 236 then 237 when orderid
    = 237 then 238 when orderid = 238 then 239 when orderid = 239 then 240 when orderid
    = 240 then 241 when orderid = 241 then 242 when orderid = 242 then 243 when orderid
    = 243 then 244 when orderid = 244 then 245 when orderid = 245 then 246 when orderid
    = 246 then 247 when orderid = 247 then 248 when orderid = 248 then 249 when orderid
    = 249 then 250 else 9999 end case_when_col from sample_data_1;
{code}

fails with an OOM. (Yes, I know the query can be simplified, but it shows the case)."	FLINK	Resolved	3	4	3568	pull-request-available
13387961	All records are processed in the close stage in ContinuousFileReaderOperatorBenchmark	The {{TARGET_COUNT_REACHED_LATCH}} is not correctly reset after the warmup iterations and thus subsequent runs process all records in the {{CLOSE}} stage of the {{ContinuousFileReaderOperator}} testing something different than anticipated.	FLINK	Closed	3	1	3568	pull-request-available
13296470	Support parsing LIKE clause in CREATE TABLE statement	We should support the CREATE TABLE ... LIKE syntax in SqlParser	FLINK	Closed	3	7	3568	pull-request-available
13553409	MATCH_RECOGNIZE AFTER MATCH clause can not be deserialised from a compiled plan	"{code}
        String sql =
                ""insert into MySink""
                        + "" SELECT * FROM\n""
                        + "" MyTable\n""
                        + ""   MATCH_RECOGNIZE(\n""
                        + ""   PARTITION BY vehicle_id\n""
                        + ""   ORDER BY `rowtime`\n""
                        + ""   MEASURES \n""
                        + ""       FIRST(A.`rowtime`) as startTime,\n""
                        + ""       LAST(A.`rowtime`) as endTime,\n""
                        + ""       FIRST(A.engine_temperature) as Initial_Temp,\n""
                        + ""       LAST(A.engine_temperature) as Final_Temp\n""
                        + ""   ONE ROW PER MATCH\n""
                        + ""   AFTER MATCH SKIP TO FIRST B\n""
                        + ""   PATTERN (A+ B)\n""
                        + ""   DEFINE\n""
                        + ""       A as LAST(A.engine_temperature,1) is NULL OR A.engine_temperature > LAST(A.engine_temperature,1),\n""
                        + ""       B as B.engine_temperature < LAST(A.engine_temperature)\n""
                        + ""   )MR;"";
        util.verifyJsonPlan(String.format(sql, afterClause));
{code}

fails with:

{code}
Could not resolve internal system function '$SKIP TO LAST$1'. This is a bug, please file an issue. (through reference chain: org.apache.flink.table.planner.plan.nodes.exec.serde.JsonPlanGraph[""nodes""]->java.util.ArrayList[3]->org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch[""matchSpec""]->org.apache.flink.table.planner.plan.nodes.exec.spec.MatchSpec[""after""])
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13324219	Remove deprecated RuntimeContext#getAllAccumulators	"We could  remove the deprecated:
{code}
RuntimeContext#getAllAcumullators
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13556206	Remove old expression stack leftovers for time functions	"Remove leftovers from https://issues.apache.org/jira/browse/FLINK-13785

There are some parts of the time functions that have not been removed e.g. https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/time.scala and some code in https://github.com/apache/flink/blob/b6000f6e589128ae1fd1e0e7d063a1b6ff1fcc20/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/PlannerExpressionConverter.scala"	FLINK	Closed	3	7	3568	pull-request-available
13360332	Document possible recommended usage of Bounded{One/Multi}Input.endInput and emphasize that they could be called multiple times	"It is too tempting to use these api, especially {{BoundedOneInput.endInput}}, to commit final result before FLIP-147 delivered. And this will cause re-commit after failover as [~gaoyunhaii] has pointed out in FLINK-21132.

I have [pointed|https://github.com/apache/iceberg/issues/2033#issuecomment-784153620] this out in [apache/iceberg#2033|https://github.com/apache/iceberg/issues/2033], please correct me if I was wrong.

cc [~aljoscha] [~pnowojski] [~roman_khachatryan]"	FLINK	Closed	10200	4	3568	auto-deprioritized-major, pull-request-available, stale-minor
13212945	Create CatalogManager to manage multiple catalogs and encapsulate Calcite schema	"Flink allows for more than one registered catalogs. {{CatalogManager}} class is the holding class to manage and encapsulate the catalogs and their interrelations with Calcite.

Following section describes how table resolution should work:

h4. PATH resolution:

First look into DEFAULT PATH: cat or cat.db. Then in the root. This is also the behavior of Calcite. We should mimic this behavior in Flink.
Example:

{noformat}
root:
  |- builtin
      |- default
          |- tab1
          |- tab2
      |- db1
          |- tab1
      |- clashing
          |- tab1
  |- cat1
      |- db1
          |- tab1
          |- tab2
  |- extCat1
      |- tab1
      |- clashing
          |- tab1
      |- extCat2
          |- tab1
          |- tab2
  |- clashing (ExternalCatalog)
      |- tab1
{noformat}
      
There is always a default catalog, initially set to builtin and default database initially set to default.

{noformat}
Assume 
default path = builtin then
  default.tab1 = builtin.default.tab1
  tab1 = error
  cat1.db1.tab1 = cat1.db1.tab1
  clashing.tab1 = builtin.clashing.tab1
default path = extCat1 then
  tab1 = extCat1.tab1
  clashing.tab1 = extCat1.clashing.tab1
default path = extCat1.extCat2 (do not support further nesting) then
  tab1 = extCat1.extCat2.tab1
  clashing.tab1 = clashing.tab1
{noformat}
  

h4. Structure in the Planner(Calcite-specific)

{noformat}
root: CatalogManagerSchema(CatalogManager)
     |- CatalogCalciteSchema(ReadableCatalog)
         |- DatabaseCalciteSchema (ReadableCatalog scoped to DB)
             |- Table
     |- ExternalCatalogSchema
         |- Table
         |- ExternalCatalogSchema
             |- Table
{noformat}

h4. Structure in the API
{noformat}    
CatalogManager:
    |- ReadableCatalog
      |- CatalogTable
    |- ExternalCatalog
      |- ExternalCatalog
      |- ExternalCatalogTable
{noformat}"	FLINK	Resolved	3	7	3568	pull-request-available
13194127	Add a switch to run_test to configure if logs should be checked for errors/excepions	After adding the switch, we should disable log checking for nightly-tests that currently fail (or fix the test).	FLINK	Closed	1	4	3568	pull-request-available
13574746	AggregateQueryOperations produces wrong asSerializableString representation	"A table API query:
{code}
        env.fromValues(1, 2, 3)
                .as(""number"")
                .select(col(""number"").count())
                .insertInto(TEST_TABLE_API)
{code}

produces

{code}
INSERT INTO `default`.`timo_eu_west_1`.`table_api_basic_api` SELECT `EXPR$0` FROM (
    SELECT (COUNT(`number`)) AS `EXPR$0` FROM (
        SELECT `f0` AS `number` FROM (
            SELECT `f0` FROM (VALUES 
                (1),
                (2),
                (3)
            ) VAL$0(`f0`)
        )
    )
    GROUP BY 
)
{code}

which is missing a grouping expression"	FLINK	Closed	3	1	3568	pull-request-available
13382184	Tasks are blocked while broadcasting stream status	"On a cluster I observed symptoms of tasks being blocked for long time, causing long delays with unaligned checkpointing. 99% of those cases were caused by `broadcastEmit` of the stream status

{noformat}
2021-06-04 14:41:44,049 ERROR org.apache.flink.runtime.io.network.buffer.LocalBufferPool   [] - Blocking wait [11059 ms] for an available buffer.
java.lang.Exception: Stracktracegenerator
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:323) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:290) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:338) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:314) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:246) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:142) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:67) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.writeStreamStatus(RecordWriterOutput.java:136) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.streamstatus.AnnouncedStatus.ensureActive(AnnouncedStatus.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:182) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.pollNext(IteratorSourceReader.java:98) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:294) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:69) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:422) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:680) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:635) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:619) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{noformat}

*I have seen this happening both in source and network tasks.* ~80% cases were in the source tasks

{{broadcastEmit}} can easily bypass our non blocking checks. There are two questions:
# why is the stream idling so much? It’s like almost every millisecond it’s broadcasting status
# should we optimise this? Broadcasting CBs and other events is not an issue, as those are events that do not request/require buffers"	FLINK	Closed	2	1	3568	pull-request-available
13316803	Kerberized YARN per-job on Docker test failed to download JDK 8u251	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
{code}
+ mkdir -p /usr/java/default
+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie
+ tar --strip-components=1 -xz -C /usr/java/default/

gzip: stdin: not in gzip format
tar: Child returned status 1
{code}"	FLINK	Closed	1	1	3568	pull-request-available
13192203	YarnConfigurationITCase.testFlinkContainerMemory test instability	"Test appeared to fail (by a narrow margin) without a reason randomly:

{noformat}
Failed tests: 
  YarnConfigurationITCase.testFlinkContainerMemory:182 
Expected: is a numeric value within <0.1> of <1.0>
     but: <0.8913043478260869> differed by <0.008695652173913077>
{noformat}

https://api.travis-ci.org/v3/job/442246057/log.txt"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13398595	Document FLIP-147 capabiliites and limitations	We should document how to enable the checkpointing after tasks finish as well as limitations we are aware of.	FLINK	Closed	2	7	3568	pull-request-available
13362233	 CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107

{code:java}
[ERROR] testAsyncCheckpointFailureTriggerJobFailed(org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase)  Time elapsed: 38.623 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:62)
	at org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed(CheckpointFailureManagerITCase.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
"	FLINK	Closed	3	1	3568	pull-request-available, test-stability
13228910	Convert CatalogTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	In FLINK-11476, we created CatalogManager to hook up planner with unified catalog APIs. What's missing there is, at the very last step, convert CatalogBaseTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs, like how {{ExternalTableUtil.fromExternalCatalogTable()}} works to convert the old {{ExternalCatalogTable}} to a Calcite table	FLINK	Closed	3	7	3568	pull-request-available
13409619	Remove scala suffix from respective benchmarks dependencies	With FLINK-24018 few dependencies lost its scala suffix. We should remove it in benchmark dependencies to test against newest artifacts.	FLINK	Closed	3	1	3568	pull-request-available
13404250	Add Flink 1.14 MigrationVersion	"Currently the largest MigrationVersion is 1.13. We need newer versions to add more serializer compatibility tests.
"	FLINK	Closed	3	4	3568	pull-request-available
13231708	Fix failling e2e test test_streaming_sql.sh	"https://travis-ci.org/apache/flink/jobs/535231108

{code}
==============================================================================
Running 'Streaming SQL end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-37856266807
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.9-SNAPSHOT-bin/flink-1.9-SNAPSHOT
Starting cluster.
Starting standalonesession daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Dispatcher REST endpoint is up.
[INFO] 1 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 2 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 3 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting execution of program
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(INTEGER NOT NULL correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
expression type is RecordType(INTEGER correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
set is rel#150:LogicalWindowAggregate.NONE(input=HepRelVertex#139,group={},correct=SUM($1),w$start=start('w$),w$end=end('w$),w$rowtime=rowtime('w$),w$proctime=proctime('w$))
expression is LogicalWindowAggregate#167
	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:57)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:234)
	at org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.onMatch(ExtendedAggregateExtractProjectRule.java:90)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:559)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:418)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:255)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:214)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:201)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlanner(TableEnvImpl.scala:282)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlannerSequentially(TableEnvImpl.scala:248)
	at org.apache.flink.table.api.TableEnvImpl.optimizeNormalizeLogicalPlan(TableEnvImpl.scala:204)
	at org.apache.flink.table.api.StreamTableEnvImpl.optimize(StreamTableEnvImpl.scala:738)
	at org.apache.flink.table.api.StreamTableEnvImpl.translate(StreamTableEnvImpl.scala:787)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:100)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:83)
	at org.apache.flink.sql.tests.StreamSQLTestProgram.main(StreamSQLTestProgram.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:269)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:742)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:272)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:204)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:983)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1056)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1056)
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13180226	WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow	"{{WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow}} failed on Travis.

https://api.travis-ci.org/v3/job/418629694/log.txt"	FLINK	Closed	2	1	3568	test-stability
13486345	BatchExecutionKeyedStateBackend is using incorrect ExecutionConfig when creating serializer	"{{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend#getOrCreateKeyedState}} is using freshly constructed {{ExecutionConfig}}, instead of the one configured by the user from the environment.


{code:java}
    public <N, S extends State, T> S getOrCreateKeyedState(
            TypeSerializer<N> namespaceSerializer, StateDescriptor<S, T> stateDescriptor)
            throws Exception {
        checkNotNull(namespaceSerializer, ""Namespace serializer"");
        checkNotNull(
                keySerializer,
                ""State key serializer has not been configured in the config. ""
                        + ""This operation cannot use partitioned state."");

        if (!stateDescriptor.isSerializerInitialized()) {
            stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
        }
{code}

The correct one could be obtained from {{env.getExecutionConfig()}} in {{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend#createKeyedStateBackend}} "	FLINK	Closed	4	1	3568	pull-request-available
13208537	KafkaITCase.testConcurrentProducerConsumerTopology times out on Travis	"The {{KafkaITCase.testConcurrentProducerConsumerTopology}} times out on Travis.

{code}
20:26:29.579 [ERROR] Errors: 
20:26:29.579 [ERROR]   KafkaITCase.testConcurrentProducerConsumerTopology:73->KafkaConsumerTestBase.runSimpleConcurrentProducerConsumerTopology:824->KafkaTestBase.deleteTestTopic:206->Object.wait:502->Object.wait:-2 Â» TestTimedOut
{code}

The solution might as simple as increasing the timeout.

https://api.travis-ci.org/v3/job/476975725/log.txt"	FLINK	Closed	2	4	3568	pull-request-available, test-stability
13031245	Fails AkkaRpcServiceTest#testTerminationFuture	"{code}
testTerminationFuture(org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest)  Time elapsed: 1.013 sec  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 1000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at akka.remote.Remoting.start(Remoting.scala:179)
	at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:634)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:142)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:119)
	at akka.actor.ActorSystem$.create(ActorSystem.scala:67)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
	at org.apache.flink.runtime.akka.AkkaUtils$.createDefaultActorSystem(AkkaUtils.scala:114)
	at org.apache.flink.runtime.akka.AkkaUtils.createDefaultActorSystem(AkkaUtils.scala)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.testTerminationFuture(AkkaRpcServiceTest.java:134)
{code} in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest while testing current master 1.2.0 branch "	FLINK	Closed	3	1	3568	test-stability
13371934	JobMasterStopWithSavepointITCase failed due to status is FAILING	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16405&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4472


{code:java}
[ERROR] Failures: 
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheSuspend:133->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate:138->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR] Errors: 
[ERROR]   JobMasterStopWithSavepointITCase.suspendWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished:103->stopWithSavepointNormalExecutionHelper:113->setUpJobGraph:307 » IllegalState
[ERROR]   JobMasterStopWithSavepointITCase.testRestartCheckpointCoordinatorIfStopWithSavepointFails:237 » IllegalState
[INFO] 
[ERROR] Tests run: 1645, Failures: 2, Errors: 2, Skipped: 51

{code}
"	FLINK	Resolved	2	1	3568	test-stability
13326530	Clean up the UnilateralSortMerger	"This is a preparation step for [FLIP-140|https://cwiki.apache.org/confluence/display/FLINK/FLIP-140%3A+Introduce+bounded+style+execution+for+keyed+streams]. The purpose of the task is two-folds:
* break down the implementation into a more composable pieces
* introduce a way to produce records in a push-based manner instead of pull-based with additional reading thread."	FLINK	Closed	3	7	3568	pull-request-available
13278176	Preserve logs from BashJavaUtils and make them part of TM logs	"In FLINK-13983 we introduced BashJavaUtils utility to call in taskmanager.sh before starting TM and calculate memory configuration for the JVM process of TM.

Ideally, it would be nice to preserve BashJavaUtils logs and make them part of the TM logs. Currently, logging for BashJavaUtils is configured from the class path and can differ from TM logging. Moreover TM logging can rewrite BashJavaUtils even if we align their loggings (e.g. log4j.appender.file.append=false in default log4j.properties  for Flink)."	FLINK	Closed	1	4	3568	pull-request-available
13597368	TIMESTAMPDIFF can not be string serialized	TIMESTAMPDIFF can not be properly string serialized, because TIMEPOINTUNIT can not be serialized.	FLINK	In Progress	3	1	3568	pull-request-available
13205141	Exception in code generation when ambiguous columns in MATCH_RECOGNIZE	"Query:
{code}
SELECT *
FROM Ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol, price
  ORDER BY proctime
  MEASURES
    A.symbol AS symbol,
    A.price AS price
  PATTERN (A)
  DEFINE
    A AS symbol = 'a'
) AS T
{code}

throws a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. 

I've also created a calcite ticket to fix it on calcite's side: [CALCITE-2747]"	FLINK	Closed	3	1	3568	pull-request-available
13185500	Savepoints should be counted as retained checkpoints	"This task is about reverting [FLINK-6328].

The problem is that you can get incorrect results with exactly-once sinks if there is a failure after taking a savepoint but before taking the next checkpoint because the savepoint will also have manifested side effects to the sink.
"	FLINK	Closed	3	1	3568	pull-request-available
13343946	Some Table examples are not built correctly	"Some examples were moved to the {{org.apache.flink.table.examples.scala.basics}} package but the pom.xml was not updated. This means the example jars are not built correctly and do not contain the classes.

Examples that I noticed:
* org.apache.flink.table.examples.scala.basics.StreamTableExample
* org.apache.flink.table.examples.scala.basics.TPCHQuery3Table

We should update the {{includes}} sections e.g.:

{code}
<execution>
	<id>StreamTableExample</id>
	<phase>package</phase>
	<goals>
		<goal>jar</goal>
	</goals>
	<configuration>
		<classifier>StreamTableExample</classifier>

<!--- The sections below should be updated -->

		<archive>
			<manifestEntries>
				<program-class>org.apache.flink.table.examples.scala.StreamTableExample</program-class>
			</manifestEntries>
		</archive>
		<includes>
			<include>org/apache/flink/table/examples/scala/StreamTableExample*</include>
		</includes>
	</configuration>
</execution>
{code}"	FLINK	Closed	2	1	3568	pull-request-available
13355441	Write savepoints in unified format from HeapStateBackend	The aim is to implement a {{HeapKeyValueStateIterator}} which can be used to produce a unified savepoint out of a HeapKeyedStateBackend	FLINK	Closed	3	7	3568	pull-request-available
13242119	Remove expressionBridge from QueryOperations factories	Expression bridge is used to create a schema of QueryOperation. This is no longer necessary with ResolvedExpressions in place.	FLINK	Closed	3	7	3568	pull-request-available
13415595	Document claim & no-claim mode	We should describe how the different restore modes work. It is important to go through the FLIP and include all {{NOTES}} in the written documentation	FLINK	Closed	3	7	3568	pull-request-available
13238698	Improve expression based TableSchema extraction from DataStream/DataSet	"We should improve the extraction of {{TableSchema}} from {{DataStream/DataSet}}. Currently it is split into a few stages:
# Extract types ignoring time attributes via {{FieldInfoUtils#getFieldInfo}}
# Extract the rowtime and proctime positions via {{StreamTableEnvImpl#validateAndExtractTimeAttributes}}
# Adjust the indices from #1 using information from #2

All that could happen in a single pass. This will also deal with the porting/removing of a few methods from {{StreamTableEnvImpl}}."	FLINK	Closed	3	7	3568	pull-request-available
13314702	Tests RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte & RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort fail locally	"The tests:
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte

fail locally (in IDE or from cmd with {{mvn clean install}}) with
{code}
java.lang.UnsatisfiedLinkError: org.rocksdb.ReadOptions.newReadOptions()J
	at org.rocksdb.ReadOptions.newReadOptions(Native Method)
	at org.rocksdb.ReadOptions.<init>(ReadOptions.java:16)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIterator(RocksKeyGroupsRocksSingleStateIteratorTest.java:78)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIteratorShort(RocksKeyGroupsRocksSingleStateIteratorTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13353481	SQLClientSchemaRegistryITCase unstable with InternalServerErrorException: Status 500	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12253&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2021-01-20T00:10:21.3510385Z Jan 20 00:10:21 
2021-01-20T00:10:21.3516246Z Jan 20 00:10:21 [ERROR] testWriting(org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase)  Time elapsed: 0.001 s  <<< ERROR!
2021-01-20T00:10:21.3517459Z Jan 20 00:10:21 java.lang.RuntimeException: Could not build the flink-dist image
2021-01-20T00:10:21.3518178Z Jan 20 00:10:21 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:281)
2021-01-20T00:10:21.3519176Z Jan 20 00:10:21 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.<init>(SQLClientSchemaRegistryITCase.java:88)
2021-01-20T00:10:21.3519873Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-01-20T00:10:21.3520537Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-01-20T00:10:21.3521390Z Jan 20 00:10:21 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-01-20T00:10:21.3522080Z Jan 20 00:10:21 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2021-01-20T00:10:21.3522730Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
2021-01-20T00:10:21.3523452Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
2021-01-20T00:10:21.3524237Z Jan 20 00:10:21 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-01-20T00:10:21.3524879Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
2021-01-20T00:10:21.3525527Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-01-20T00:10:21.3526157Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-01-20T00:10:21.3526754Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-01-20T00:10:21.3527316Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-01-20T00:10:21.3527884Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-01-20T00:10:21.3528462Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-01-20T00:10:21.3529491Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-01-20T00:10:21.3530220Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-01-20T00:10:21.3530970Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-01-20T00:10:21.3531649Z Jan 20 00:10:21 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-01-20T00:10:21.3532201Z Jan 20 00:10:21 	at java.lang.Thread.run(Thread.java:748)
2021-01-20T00:10:21.3533545Z Jan 20 00:10:21 Caused by: com.github.dockerjava.api.exception.InternalServerErrorException: Status 500: {""message"":""Get https://registry-1.docker.io/v2/testcontainers/ryuk/manifests/0.3.0: received unexpected HTTP status: 502 Bad Gateway""}
2021-01-20T00:10:21.3534353Z Jan 20 00:10:21 
2021-01-20T00:10:21.3534955Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:247)
2021-01-20T00:10:21.3536388Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$executeAndStream$1(DefaultInvocationBuilder.java:269)
2021-01-20T00:10:21.3537066Z Jan 20 00:10:21 	... 1 more
2021-01-20T00:10:21.3541323Z Jan 20 00:10:21 
{code}"	FLINK	Closed	2	1	3568	test-stability
13280523	SELECT 'ABC'; does not work in sql-client	"A query like {{SELECT 'abc';}} fails in sql-client with blink planner enabled with an error:
{code}
org.apache.flink.table.api.ValidationException: Type CHAR(3) of table field 'EXPR$0' does not match with the physical type STRING of the 'EXPR$0' field of the TableSink consumed type.
{code}

The reason is that those sinks do not properly support new type system. There is no good way to define schema and consumed data type so that they match. We should update the in-memory sinks in sql-client to work with the legacy type system for now until the retract and upsert sinks work properly with the new type system."	FLINK	Closed	1	1	3568	pull-request-available
13306910	SQL-CLI no exception stack	"If write a wrong DDL, only ""[ERROR] Unknown or invalid SQL statement"" message.

No exception stack in client and logs."	FLINK	Closed	1	1	4624	pull-request-available
13301480	Support SupportsProjectionPushDown in planner	"Support the {{SupportsProjectionPushDown}} interface for {{ScanTableSource}}.

"	FLINK	Closed	3	7	4624	pull-request-available
13229888	Supports sub-plan reuse	"Many query plans have duplicated sub-plan, and their computing logic is identical. So we could reuse duplicated sub-plans to reduce computing. 

Digest of RelNode is an identifier to distinguish RelNode, and the digest of sub-plan contains all inner-nodes' digests. we could use the digest of sub-plan to find duplicated sub-plan.

e.g. 
{code:sql}
WITH r AS (SELECT a FROM x LIMIT 10)
SELECT r1.a FROM r r1, r r2 WHERE r1.a = r2.a

the physical plan after sub-plan reuse:
Calc(select=[a])
+- HashJoin(joinType=[InnerJoin], where=[=(a, a0)], select=[a, a0], isBroadcast=[true], build=[right])
   :- Exchange(distribution=[hash[a]])
   :  +- Calc(select=[a], reuse_id=[1])
   :     +- Limit(offset=[0], fetch=[10], global=[true])
   :        +- Exchange(distribution=[single])
   :           +- Limit(offset=[0], fetch=[10], global=[false])
   :              +- TableSourceScan(table=[[x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
   +- Exchange(distribution=[broadcast])
      +- Reused(reference_id=[1])
{code}

sub-plan: Calc-Limit-Exchange-Limit-TableSourceScan is reused.

For batch job, reused node might lead to a deadlock when a HashJoin or NestedLoopJoin have same reused inputs. The probe side of HashJoin could start to read data only after build side has finished. If there is no full dam (DamBehavior#FULL_DAM) operators (e.g. Sort, HashAggregate) in probe side, the data will be blocked by probe side. In this case, we could set Exchange node (if it does not exist, add new one) as BATCH mode to break up the deadlock.(The Exchange node is a full dam operator now)

"	FLINK	Closed	3	2	4624	pull-request-available
13374969	IllegalArgumentException is thrown in WindowAttachedWindowingStrategy when two phase is enabled for distinct agg	"Caused by: java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.flink.table.planner.plan.logical.WindowAttachedWindowingStrategy.<init>(WindowAttachedWindowingStrategy.java:51)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)


The reason is the {{windowStart}} may be {{-1}} when two phase is enabled for distinct agg, see [TwoStageOptimizedWindowAggregateRule.java#L143|https://github.com/apache/flink/blob/a3363b91b144edfbae5ab114984ded622d3f8fbc/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/physical/stream/TwoStageOptimizedWindowAggregateRule.java#L143]"	FLINK	Closed	3	1	4624	pull-request-available
13290702	CheckpointCoordinatorFailureTest logs LinkageErrors	"This issue is in https://travis-ci.org/apache/flink/jobs/660152153?utm_medium=notification&utm_source=slack

Log output

{code:java}
2020-03-09 15:52:14,550 main ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:349)
	at org.apache.flink.util.TestLogger.<init>(TestLogger.java:36)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorFailureTest.<init>(CheckpointCoordinatorFailureTest.java:55)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTestInstance(PowerMockJUnit44RunnerDelegateImpl.java:197)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.createTest(PowerMockJUnit44RunnerDelegateImpl.java:182)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:204)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
"	FLINK	Closed	4	1	4624	pull-request-available
13366614	GroupAggregateJsonPlanTest.testDistinctAggCalls fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15083&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=6364


{code:java}

	at org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateJsonPlanTest.testDistinctAggCalls(GroupAggregateJsonPlanTest.java:148)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)

{code}
"	FLINK	Closed	3	1	4624	pull-request-available, test-stability
13244938	copy RelDecorrelator & FlinkFilterJoinRule to flink planner to fix CALCITE-3169 & CALCITE-3170	"[CALCITE-3169|https://issues.apache.org/jira/browse/CALCITE-3169] & [CALCITE-3170|https://issues.apache.org/jira/browse/CALCITE-3170] are not fixed in Calcite-1.20. 
{{RelDecorrelator}} & {{FlinkFilterJoinRule}} is copied from Calcite to blink planner to resolve those two bug. to make both planners available in one jar, {{RelDecorrelator}} & {{FlinkFilterJoinRule}} should also be copied to flink planner."	FLINK	Resolved	1	7	4624	pull-request-available
13218889	Introduce FlinkRelNode interface and FlinkConventions	"Flink RelNode inheritance structure can be defined as follow:

{code}
RelNode (Calcite)
|__ FlinkRelNode // base class for flink relnode
    |__ FlinkLogicalRel // base class for flink logical relnode
    |__ FlinkPhysicalRel // base class for flink physical relnode
        |__ BatchPhysicalRel // base class for batch physical relnode
        |__ StreamPhysicalRel // base class for stream physical relnode
{code}"	FLINK	Closed	3	2	4624	pull-request-available
13225249	Add support for generating optimized logical plan for join on batch	"Currently, there are 3 types of batch physical join nodes: {{BatchExecHashJoin}}, {{BatchExecSortMergeJoin}} and {{BatchExecNestedLoopJoin}}. This issue aims to add rules to convert logical join to appropriate physical join. 

a logical join
* can be converted to {{BatchExecHashJoin}} if there exists at least one equal-join condition and ShuffleHashJoin or BroadcastHashJoin are enabled, 
* can be converted to {{BatchExecSortMergeJoin}} if there exists at least one equal-join condition and SortMergeJoin is enabled,
* can be converted to {{BatchExecNestedLoopJoin}} if NestedLoopJoin is enabled or one of join input sides returns at most a single row.
"	FLINK	Closed	3	2	4624	pull-request-available
13244906	supports explain DAG plan in flink-python	update existing `explain` to support explain DAG plan in flink-python	FLINK	Closed	3	2	4624	pull-request-available
13231247	FlinkRelMetadataQuery does not compile with Scala 2.12	"{code}
10:57:51.770 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/plan/metadata/FlinkRelMetadataQuery.scala:52: error: value EMPTY in class RelMetadataQuery cannot be accessed in object org.apache.calcite.rel.metadata.RelMetadataQuery
10:57:51.770 [ERROR]  Access to protected value EMPTY not permitted because
10:57:51.770 [ERROR]  enclosing package metadata in package plan is not a subclass of
10:57:51.770 [ERROR]  class RelMetadataQuery in package metadata where target is defined
10:57:51.770 [ERROR]     this(RelMetadataQuery.THREAD_PROVIDERS.get, RelMetadataQuery.EMPTY)
{code}"	FLINK	Closed	1	1	4624	pull-request-available
13276207	INSERT INTO VALUES statement fails if a cast project is applied	"The following query will fail:


{code:scala}
  @Test
  def test(): Unit = {
    val sinkDDL =
      """"""
        |create table t2(
        |  a int,
        |  b string
        |) with (
        |  'connector' = 'COLLECTION'
        |)
      """""".stripMargin
    val query =
      """"""
        |insert into t2 select cast(a as int), cast(b as varchar) from (values (3, 'c')) T(a,b)
      """""".stripMargin
    tableEnv.sqlUpdate(sinkDDL)
    tableEnv.sqlUpdate(query)
    execJob(""testJob"")
  }
{code}


exception:


{code}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

LogicalSink(name=[`default_catalog`.`default_database`.`t2`], fields=[a, b])
+- LogicalProject(EXPR$0=[$0], EXPR$1=[CAST($1):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL])
   +- LogicalValues(type=[RecordType(INTEGER a, CHAR(1) b)], tuples=[[{ 3, _UTF-16LE'c' }]])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.
{code}


"	FLINK	Resolved	1	1	4624	pull-request-available
13441367	COALESCE('1', CAST(NULL as varchar)) throws expression type mismatch	"{code}
Flink SQL> SELECT
>     COALESCE('1', cast(NULL as varchar)),
>     COALESCE('4', cast(NULL as varchar), cast(NULL as varchar), cast(NULL as varchar));


Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
Caused by: java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$0, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$1) NOT NULL
expression type is RecordType(CHAR(1) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$0, CHAR(1) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$1) NOT NULL
set is rel#910:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#909,exprs=[COALESCE(_UTF-16LE'1', null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), COALESCE(_UTF-16LE'4', null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")])
expression is LogicalProject(EXPR$0=[_UTF-16LE'1'], EXPR$1=[_UTF-16LE'4'])
  LogicalValues(tuples=[[{ 0 }]])

	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:58)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.flink.table.planner.plan.rules.logical.RemoveUnreachableCoalesceArgumentsRule.onMatch(RemoveUnreachableCoalesceArgumentsRule.java:71)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:164)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:82)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:303)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:179)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:828)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:231)
	at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:561)
	at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:446)
	at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:373)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:330)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:281)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:229)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
{code}"	FLINK	Closed	2	1	4624	pull-request-available
13221872	Add support for generating optimized logical plan for 'select * from mytable'	"Add support for generating optimized logical plan for 'select * from mytable', including:
1. add {{BatchTableEnvironment}} 
2. introduce {{Optimizer}} interface and implements for batch and stream
3. add rules and programs for batch and stream
4. add {{registerTableSource}}, {{optimize}}, {{explain}} methods in {{TableEnvironment}}
5. add plan test infrastructure (is duplicated by https://issues.apache.org/jira/browse/FLINK-11685)
6. add {{RelTreeWriterImpl}} to make query plan easier to read (is duplicated by  https://issues.apache.org/jira/browse/FLINK-11680)"	FLINK	Closed	3	2	4624	pull-request-available
13366556	Rename DAGProcessor to ExecNodeGraphProcessor	in FLINK-21041, we have introduced ExecNodeGraph to wrap the ExecNode topology. DAGProcessor is used for transforming ExecNodeGraph. Their names do not match. I suggest to rename DAGProcessor to ExecNodeGraphProcessor.	FLINK	Closed	3	4	4624	pull-request-available
13219506	Introduce Flink metadata handlers	"Calcite has defined various metadata handlers(e.g. {{RowCoun}}, {{Selectivity}} and provided default implementation(e.g. {{RelMdRowCount}}, {{RelMdSelectivity}}). However, the default implementation can't completely meet our requirements, e.g. some of its logic is incomplete，and some RelNodes  are not considered.
There are two options to meet our requirements:
option 1. Extends from default implementation, overrides method to improve its logic, add new methods for new {{RelNode}}. The advantage of this option is we just need to focus on the additions and modifications. However, its shortcomings are also obvious: we have no control over the code of non-override methods in default implementation classes especially when upgrading the Calcite version.
option 2. Extends from metadata handler interfaces, reimplement all the logic. Its shortcomings are very obvious, however we can control all the code logic that's what we want.

so we choose option 2!

In this jira, all Flink metadata handles will be introduced, 

including calcite builtin metadata handlers:
{{FlinkRelMdPercentageOriginalRow}},
{{FlinkRelMdNonCumulativeCost}},
{{FlinkRelMdCumulativeCost}},
{{FlinkRelMdRowCount}},
{{FlinkRelMdSize}},
{{FlinkRelMdSelectivity}},
{{FlinkRelMdDistinctRowCoun}},
{{FlinkRelMdPopulationSize}},
{{FlinkRelMdColumnUniqueness}},
{{FlinkRelMdUniqueKeys}},
{{FlinkRelMdDistribution}},

and flink extented metadata handlers:
{{FlinkRelMdColumnInterval}},
{{FlinkRelMdFilteredColumnInterval}},
{{FlinkRelMdColumnNullCount}},
{{FlinkRelMdColumnOriginNullCount}},
{{FlinkRelMdUniqueGroups}},
{{FlinkRelMdModifiedMonotonicity}}
"	FLINK	Closed	3	2	4624	pull-request-available
13243599	PushFilterIntoTableSourceScanRuleTest.testWithUd fails on Travis	"{{PushFilterIntoTableSourceScanRuleTest.testWithUd}} fails on Travis with

{code}
06:22:11.290 [ERROR] Failures: 
06:22:11.290 [ERROR]   PushFilterIntoTableSourceScanRuleTest.testWithUdf:93 planAfter expected:<...ssions$utils$Func1$$[a39386268ffec8461452460bcbe089ad]($2), 32)])
   +- Lo...> but was:<...ssions$utils$Func1$$[8a89e0d5a022a06a00c7734a25295ff4]($2), 32)])
   +- Lo...>
{code}

https://api.travis-ci.org/v3/job/555252046/log.txt"	FLINK	Closed	2	1	4624	test-stability
13269857	can not be translated to StreamExecDeduplicate when PROCTIME() is defined in query	"CREATE TABLE user_log (
    user_id VARCHAR,
    item_id VARCHAR,
    category_id VARCHAR,
    behavior VARCHAR,
    ts TIMESTAMP
) WITH (
    'connector.type' = 'kafka',
    'connector.version' = 'universal',
    'connector.topic' = 'user_behavior',
    'connector.startup-mode' = 'earliest-offset',
    'connector.properties.0.key' = 'zookeeper.connect',
    'connector.properties.0.value' = 'localhost:2181',
    'connector.properties.1.key' = 'bootstrap.servers',
    'connector.properties.1.value' = 'localhost:9092',
    'update-mode' = 'append',
    'format.type' = 'json',
    'format.derive-schema' = 'true'
);

CREATE TABLE user_dist (
    dt VARCHAR,
    user_id VARCHAR,
    behavior VARCHAR
) WITH (
    'connector.type' = 'jdbc',
    'connector.url' = 'jdbc:mysql://localhost:3306/flink-test',
    'connector.table' = 'user_behavior_dup',
    'connector.username' = 'root',
    'connector.password' = ‘******',
    'connector.write.flush.max-rows' = '1'
);

INSERT INTO user_dist
SELECT
  dt,
  user_id,
  behavior
FROM (
   SELECT
      dt,
      user_id,
      behavior,
     ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum
   FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc
            from user_log) )
WHERE rownum = 1;

Exception in thread ""main"" org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has a full primary keys if it is updated.
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:114)
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)
at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54)
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)
at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:61)
at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.Iterator$class.foreach(Iterator.scala:891)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:60)
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:149)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:348)"	FLINK	Resolved	3	1	4624	pull-request-available
13246802	Port csv factories & descriptors from flink-table-planner to flink-csv	Blink planner does not define any csv factories & descriptors, so port them from flink-table-planner to flink-csv, and let both planners support them	FLINK	Resolved	2	7	4624	pull-request-available
13243112	Support lazy query transformation & execution on TableEnvironment	"in [FLINK-13081|https://issues.apache.org/jira/browse/FLINK-13081], {{explain}} method will be introduced to support explain multiple-sinks plan, this issue aims to introduce {{execute}} method into {{TableEnvironment}} to trigger the program execution. and in blink planner, queries will not be optimized immediately in {{insertInto}}/{{sqlUpdate}} methods, and will be optimized together in this method.

{code:java}
// Triggers the program execution
JobExecutionResult execute(String jobName) throws Exception;
{code}

there are two concerns about this method: 
1. which {{execute}} methods ({{TableEnvironment#execute}} or {{StreamExecutionEnvironment#execute}}) users should use?
2.  how to make sure users only use {{TableEnvironment#execute}} method if their code only depends on planner module instead of bridge module？"	FLINK	Closed	3	2	4624	pull-request-available
13217182	Add optimize program to organize optimization phases	"Currently, Flink organizes the optimization phases by different methods in Batch(Stream)TableEnvironment#optimize. However this is not easy to extend especially there are more than ten optimization stages in Blink. On the other hand, the methods are very similar, except the match order and rule sets for hep optimization phases, target traits and rule sets for volcano optimization phases.

Abstracts each optimization stage into a {{FlinkOptimizeProgram}} in Blink, defined as following:
{code}
/**
  * Likes [[org.apache.calcite.tools.Program]], FlinkOptimizeProgram transforms a relational
  * expression into another relational expression.
  */
trait FlinkOptimizeProgram[OC <: OptimizeContext] {
  def optimize(input: RelNode, context: OC): RelNode
}
{code}
{{FlinkOptimizeProgram}}'s subclasses include 
 1. {{FlinkRuleSetProgram}}, an abstract program can add/remove {{RuleSet}}, set target traits.
 2. {{FlinkHepRuleSetProgram}}, a subclass of {{FlinkRuleSetProgram}} which runs with {{HepPlanner}}.
 3. {{FlinkVolcanoProgram}}, a subclass of {{FlinkRuleSetProgram}} which runs with {{VolcanoPlanner}}.
 4. {{FlinkGroupProgram}}, a program contains a sequence of sub-programs as a group, programs in the group will be executed in sequence, and the group can be executed `iterations` times.
......

{{FlinkChainedPrograms}} is responsible for organizing all the programs, each program's optimize method will be called in sequence when {{FlinkChainedPrograms}}#optimize is called."	FLINK	Closed	3	2	4624	pull-request-available
13288623	Deprecate the methods in TableEnvironment proposed by FLIP-84	"In [FLIP-84|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=134745878], We propose to deprecate the following methods in TableEnvironment: 
{code:java}
void sqlUpdate(String sql)
void insertInto(String targetPath, Table table)
void execute(String jobName)
String explain(boolean extended)
Table fromTableSource(TableSource<?> source)
{code}
This issue aims to deprecate them.
"	FLINK	Closed	3	7	4624	pull-request-available
13241400	Remove dependencies on RelNode from TableImpl in blink planner	This issue aims to remove dependencies on RelNode from TableImpl in blink planner, just as [FLINK-12737|https://issues.apache.org/jira/browse/FLINK-12737] does.	FLINK	Closed	3	4	4624	pull-request-available
13335329	Introduce multi-input operator for batch	After the planner is ready for multi-input, we should introduce multi-input operator for batch.	FLINK	Closed	3	7	4624	pull-request-available
13233460	Introduce planner rules about semi/anti join	"This issue aims to introduce planner rules about semi/anti join, rules include:
1. {{FlinkSemiAntiJoinFilterTransposeRule}} that pushes semi/anti join down in a tree past a filter
2. {{FlinkSemiAntiJoinJoinTransposeRule}} that pushes semi/anti join down in a tree past a non semi/anti join
3. {{FlinkSemiAntiJoinProjectTransposeRule}} that push semi/anti join down in a tree past a project
4. {{ProjectSemiAntiJoinTransposeRule}} that pushes a project down in a tree past a semi/anti join

planner rules about non semi/anti join will be introduced in [FLINK-12509|https://issues.apache.org/jira/browse/FLINK-12509].
"	FLINK	Closed	3	2	4624	pull-request-available
13227868	Add support for generating optimized logical plan for join on stream	"This issue aims to supports generating optimized plan for join on stream. 
The query will be converted to window join if join condition contains window bounds, otherwise will be converted to normal join.

e.g.
Queries similar to the following should be window join:
{code:sql}
SELECT t1.a, t2.b FROM MyTable t1 JOIN MyTable2 t2 ON
    t1.a = t2.a AND
    t1.proctime BETWEEN t2.proctime - INTERVAL '1' HOUR AND t2.proctime + INTERVAL '1' HOUR
{code}
"	FLINK	Closed	3	2	4624	pull-request-available
13217139	Add table and column stats	"We define two structure mode to hold statistics


1. TableStats: statistics for table level, contains 2 elements:

rowCount: Long // the number of row count of table

colStats: Map[String, ColumnStats] // map each column to its ColumnStats


2. ColumnStats: statistics for column level, contains 6 elements:

ndv: Long // number of distinct values

nullCount: Long // number of null values

avgLen: Double // average length of column values

maxLen: Integer // max length of column values

max: Any // max value of column values

min: Any // min value of column values"	FLINK	Closed	3	2	4624	pull-request-available
13246224	should handle new JoinRelType(SEMI/ANTI) in switch case	Calcite 1.20 introduces {{SEMI}} & {{ANTI}} to {{JoinRelType}}, blink planner & flink planner should handle them in each switch case	FLINK	Resolved	3	1	4624	pull-request-available
13102217	Supports user defined optimization phase	Currently, the optimization phases are hardcode in {{BatchTableEnvironment}} and {{StreamTableEnvironment}}. It's better that user could define the optimization phases and the rules in each phase as needed.	FLINK	Closed	3	4	4624	pull-request-available
13346699	Introduce translateToExecNode method for FlinkPhysicalRel	Currently, we introduce ExecGraphGenerator to translate a graph of FlinkPhysicalRel to a ExecNode graph. While as more and more ExecNode supported, ExecGraphGenerator is hard to maintain. So we introduce FlinkPhysicalRel#translateToExecNode to create specific ExecNode in each physical RelNode, and ExecGraphGenerator is used to build the whole graph based on the translated ExecNode, including connecting the input/output nodes, handling the rel visitor, handle some special nodes (such as, Exchange), etc.	FLINK	Closed	3	7	4624	pull-request-available
13312618	TableResult#print can not print the result of unbounded stream query	In current implementation of PrintUtils, all result will be collected to local memory to compute column width first. this can works fine with batch query and bounded stream query. but for unbounded stream query, the result will be endless, so the result will be never printed. To solve this, we can use fix-length strategy, and print a row immediately once the row is accessed.	FLINK	Closed	1	1	4624	pull-request-available
13308861	Catalog does not exist in SQL Client	"Flink SQL> show catalogs;
default_catalog
hive

Flink SQL> use  catalog hive;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.catalog.exceptions.CatalogException: A catalog with name [`hive`] does not exist.


The reason is {{SqlCommandParser}} adds {{``}} for catalog name, which is unnecessary. "	FLINK	Closed	1	1	4624	pull-request-available
13434789	Failed to deserialize for match recognize	"Currently, the json deserialization logic is not tested, there de is a bug in {{JsonPlanTestBase}}#{{compileSqlAndExecutePlan}} method. The correct logic is the {{CompiledPlan}} should be converted to json string, and then the json string be deserialized to  {{CompiledPlan}} object. 

After correcting the logic, {{MatchRecognizeJsonPlanITCase}} will get the following exception:


{code:java}
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.loadPlan(TableEnvironmentImpl.java:714)
	at org.apache.flink.table.planner.utils.JsonPlanTestBase.compileSqlAndExecutePlan(JsonPlanTestBase.java:77)
	at org.apache.flink.table.planner.runtime.stream.jsonplan.MatchRecognizeJsonPlanITCase.testSimpleMatch(MatchRecognizeJsonPlanITCase.java:66)
{code}

 "	FLINK	Closed	3	1	4624	pull-request-available
13268318	extends max/min type in ColumnStats from Number to Comparable	Many tpc-ds queries have predicates on date, like {{d_date between '1999-02-01' and (cast('1999-02-01' as date) + INTERVAL '60' day)}}, It's very useful to find a better plan if the planner knows the max/min values of date. However, max/min in {{ColumnStats}} only support {{Number}} type currently. This issue aims to extend max/min type from {{Number}} to {{Comparable}}, and then {{Date}}, {{Time}}, {{Timestamp}} even {{String}} could be supported.	FLINK	Closed	3	7	4624	pull-request-available
13305364	proctime defined in ddl can't work with over window in Table api	"the following test will get {{org.apache.flink.table.api.ValidationException: Ordering must be defined on a time attribute.}}
{code:scala}
  @Test
  def testProcTimeTableSourceOverWindow(): Unit = {
    val ddl =
      s""""""
         |CREATE TABLE procTimeT (
         |  id int,
         |  val bigint,
         |  name varchar(32),
         |  proctime as PROCTIME()
         |) WITH (
         |  'connector' = 'projectable-values',
         |  'bounded' = 'false'
         |)
       """""".stripMargin
    util.tableEnv.executeSql(ddl)

    val t = util.tableEnv.from(""procTimeT"")
      .window(Over partitionBy 'id orderBy 'proctime preceding 2.hours as 'w)
      .select('id, 'name, 'val.sum over 'w as 'valSum)
      .filter('valSum > 100)
    util.verifyPlan(t)
  }
{code}

The reason is: the type of proctime is {{TIMESTAMP(3) NOT null}}, while {{LegacyTypeInfoDataTypeConverter}} does not handle the mapping between {{Types.LOCAL_DATE_TIME}} and {{DataTypes.TIMESTAMP(3)}} with not null. 
"	FLINK	Closed	1	1	4624	pull-request-available
13315894	Sql api cannot specify flink job name	"In Flink 1.11.0, {color:#172b4d}StreamTableEnvironment.executeSql(sql) {color}will explan and execute job Immediately, The job name will special as ""insert-into_sink-table-name"".  But we have Multiple sql job will insert into a same sink table, this is not very friendly. "	FLINK	Closed	2	4	4624	pull-request-available
13342017	flink-1.11.2 ContinuousFileMonitoringFunction cannot restore from failure	"流式消费Hive表，出现失败时，任务无法正常恢复，一直重启。

一直报错：The ContinuousFileMonitoringFunction has already restored from a previous Flink version.

 

{color:#FF0000}java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/rawdata/db/bw_hana/sapecc/hepecc_ekko_cut{color}
 at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1270) ~[hadoop-hdfs-2.6.0-cdh5.16.2.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1262) ~[hadoop-hdfs-2.6.0-cdh5.16.2.jar:?]
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0-cdh5.16.2.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1262) ~[hadoop-hdfs-2.6.0-cdh5.16.2.jar:?]
 at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.getFileStatus(HadoopFileSystem.java:85) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:588) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.getInputSplitsSortedByModTime(ContinuousFileMonit
 oringFunction.java:279) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.monitorDirAndForwardSplits(ContinuousFileMonitori
 ngFunction.java:251) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.run(ContinuousFileMonitoringFunction.java:215) ~[
 flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213) ~[flink-dist_2
 .11-1.11.2.jar:1.11.2]

 

 

2020-11-23 05:00:33,313 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Split Reader: HiveFileMonitoringFunction -> S
 ink: Sink(table=[default_catalog.default_database.kafka_hepecc_ekko_cut_json], fields=[mandt, ebeln, bukrs, bstyp, bsart, bsakz, loekz, statu
 , aedat, ernam, pincr, lponr, lifnr, spras, zterm, zbd1t, zbd2t, zbd3t, zbd1p, zbd2p, ekorg, ekgrp, waers, wkurs, kufix, bedat, kdatb, kdate,
 bwbdt, angdt, bnddt, gwldt, ausnr, angnr, ihran, ihrez, verkf, telf1, llief, kunnr, konnr, abgru, autlf, weakt, reswk, lblif, inco1, inco2, 
 ktwrt, submi, knumv, kalsm, stafo, lifre, exnum, unsez, logsy, upinc, stako, frggr, frgsx, frgke, frgzu, frgrl, lands, lphis, adrnr, stceg_l,
 stceg, absgr, addnr, kornr, memory, procstat, rlwrt, revno, scmproc, reason_code, memorytype, rettp, retpc, dptyp, dppct, dpamt, dpdat, msr_
 id, hierarchy_exists, threshold_exists, legal_contract, description, release_date, force_id, force_cnt, reloc_id, reloc_seq_id, source_logsys
 , auflg, yxcort, ysyb, ypsfs, yxqlx, yjplx, ylszj, yxdry, yxdrymc, ylbid1, yqybm, ysxpt_order, yy_write_if, fanpfg, yresfg, yrcofg, yretxt, y
 ...skipping...
 {color:#FF0000}java.lang.IllegalArgumentException: The ContinuousFileMonitoringFunction has already restored from a previous Flink version.{color}
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.initializeState(ContinuousFileMonitoringFunction.java:176) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:185) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:167) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:106) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:258) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:290) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:479) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:475) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:528) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
 at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_251]

 "	FLINK	Closed	1	1	4624	pull-request-available
13334946	Upgrade ICU4J to 67.1+	https://nvd.nist.gov/vuln/detail/CVE-2020-10531	FLINK	Closed	1	4	4624	pull-request-available
13243014	Supports explain DAG plan	"in flink planner, a query will be optimized while calling {{TableEnvironment#insertInto}} or {{TableEnvironment#sqlUpdate}}. however, if a job has multiple sinks (means {{insertInto}} or {{sqlUpdate}} will be called multiple times), the final job contains several independent sub-graphs. In most cases, there is duplicate computing in a multiple sinks job. so in blink planner, multiple sinks queries will be optimized together to avoid duplicate computing. a query will not be optimized in {{insertInto}} and {{sqlUpdate}}. instead, queries will be optimized before executing.
this issue aims to support above case.

this issue only introduces {{explain}} method into {{TableEnvironment}}, {{execute}} method will be introduced in [FLINK-13088|https://issues.apache.org/jira/browse/FLINK-13088]

{code:java}
// explain multiple-sinks plan
String explain(boolean extended);
{code}

to make sure the behavior of flink planner is same as before, a {{isLazyOptMode}} filed is added into {{EnvironmentSettings}}, which tells the table environment should optimize the query immediately in {{insertInto}}/{{sqlUpdate}} methods for flink planner({{isLazyOptMode}} is always false) or in execute method for blink planner({{isLazyOptMode}} is always true) .

"	FLINK	Closed	3	2	4624	pull-request-available
13234591	Introduce planner rules to remove redundant shuffle and collation	"{{Exchange}} and {{Sort}} is the most heavy operator, they are created in {{FlinkExpandConversionRule}} when some operators require its inputs to satisfy distribution trait or collation trait in planner rules. However, many operators could provide distribution or collation, e.g. {{BatchExecHashAggregate}} or {{BatchExecHashJoin}} could provide distribution on its shuffle keys, {{BatchExecSortMergeJoin}} could provide distribution and collation on its join keys. If the provided traits could satisfy the required traits, the {{Exchange}} or the {{Sort}} is redundant.
e.g. 
{code:sql}
schema:
x: a int, b bigint, c varchar
y: d int, e bigint, f varchar
t1: a1 int, b1 bigint, c1 varchar
t2: d1 int, e1 bigint, f1 varchar

sql:
select * from x join y on a = d and b = e join t1 on d = a1 and e = b1 left outer join t2 on a1 = d1 and b1 = e1

the physical plan after redundant Exchange and Sort are removed:
SortMergeJoin(joinType=[...], where=[AND(=(a1, d1), =(b1, e1))], leftSorted=[true], ...)
:- SortMergeJoin(joinType=[...], where=[AND(=(d, a1), =(e, b1))], leftSorted=[true], ...)
:  :- SortMergeJoin(joinType=[...], where=[AND(=(a, d), =(b, e))], ...)
:  :  :- Exchange(distribution=[hash[a, b]])
:  :  :  +- TableSourceScan(table=[[x]], ...)
:  :  +- Exchange(distribution=[hash[d, e]])
:  :     +- TableSourceScan(table=[[y]], ...)
:  +- Exchange(distribution=[hash[a1, b1]])
:     +- TableSourceScan(table=[[t1]], ...)
+- Exchange(distribution=[hash[d1, e1]])
   +- TableSourceScan(table=[[t2]], ...)
{code}

In above physical plan, the Exchanges between SortMergeJoins are redundant due to their shuffle keys are same, the Sorts in the top two SortMergeJoins' left hand side are redundant due to its input is sorted.
notes: after exchange removed, there maybe exist a sub-tree like localHashAggregate -> globalHashAggregate, the localHashAggregate should be removed due to localHashAggregate is redundant. so do localRank -> globalRank, localSortAggregate -> globalSortAggregate.

another situation is the shuffle and collation could be removed between multiple OVERs. e.g.
{code:sql}
schema:
MyTable: a int, b int, c varchar

sql:
SELECT
    COUNT(*) OVER (PARTITION BY c ORDER BY a),
    SUM(a) OVER (PARTITION BY b ORDER BY a),
    RANK() OVER (PARTITION BY c ORDER BY a, c),
    SUM(a) OVER (PARTITION BY b ORDER BY a),
    COUNT(*) OVER (PARTITION BY c ORDER BY c)
 FROM MyTable

the physical plan after redundant Exchange and Sort are removed:
Calc(select=[...])
+- OverAggregate(partitionBy=[c], orderBy=[c ASC], window#0=[COUNT(*)  ...])
   +- OverAggregate(partitionBy=[c], orderBy=[a ASC], window#0=[COUNT(*) ...], window#1=[RANK(*) ...], ...)
      +- Sort(orderBy=[c ASC, a ASC])
         +- Exchange(distribution=[hash[c]])
            +- OverAggregate(partitionBy=[b], orderBy=[a ASC], window#0=[COUNT(a), $SUM0(a) ...], ...)
               +- Sort(orderBy=[b ASC, a ASC])
                  +- Exchange(distribution=[hash[b]])
                     +- TableSourceScan(table=[[MyTable]], ...)
{code}
the {{Exchange}} and {{Sort}} between the top two OverAggregates are redundant due to their shuffle keys and sort keys are same."	FLINK	Closed	3	2	4624	pull-request-available
13217154	Add cost model for both batch and streaming	"Calcite's default cost model only contains ROWS, IO and CPU, and does not take IO and CPU into account when the cost is compared.

There are two improvements:

1. Add NETWORK and MEMORY to represents distribution cost and memory usage.

2. The optimization goal is to use minimal resources now, so the comparison order of factors is:
    (1). first compare CPU. Each operator will use CPU, so we think it's the most important factor.
    (2). then compare MEMORY, NETWORK and IO as a normalized value. Comparison order of them is not easy to decide, so convert them to CPU cost by different ratio.
    (3). finally compare ROWS. ROWS has been counted when calculating other factory.
         e.g. CPU of Sort = nLogN(ROWS) * number of sort keys, CPU of Filter = ROWS * condition cost on a row."	FLINK	Closed	3	2	4624	pull-request-available
13291466	Exception will be thrown when computing columnInterval relmetadata in some case	"Consider the following SQL

 
{code:java}
// a: INT, c: LONG
SELECT 
    c, SUM(a) 
FROM T 
WHERE a > 0.1 AND a < 1 
GROUP BY c{code}
 

Here the sql type of 0.1 is Decimal and 1 is Integer, and they are both in NUMERIC type family, and do not trigger type coercion, so the plan is:
{code:java}
FlinkLogicalAggregate(group=[{0}], EXPR$1=[SUM($1)])
+- FlinkLogicalCalc(select=[c, a], where=[AND(>(a, 0.1:DECIMAL(2, 1)), <(a, 1))])
   +- FlinkLogicalTableSourceScan(table=[[...]], fields=[a, b, c])
{code}
When we calculate the filtered column interval of calc, it'll lead to validation exception of `FiniteValueInterval`:

!image-2020-03-13-10-32-35-375.png!"	FLINK	Closed	2	1	4624	pull-request-available
13352426	Functions in ExplodeFunctionUtil should handle null data to avoid NPE	"The following test case will encounter NPE:

{code:scala}
    val t = tEnv.fromValues(
      DataTypes.ROW(
        DataTypes.FIELD(""a"", DataTypes.INT()),
        DataTypes.FIELD(""b"", DataTypes.ARRAY(DataTypes.STRING()))
      ),
      row(1, Array(""aa"", ""bb"", ""cc"")),
      row(2, null),
      row(3, Array(""dd""))
    )
    tEnv.registerTable(""T"", t)

    tEnv.executeSql(""SELECT a, s FROM T, UNNEST(T.b) as A (s)"").print()
{code}

Exception is 

{code:java}
Caused by: java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:32)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.flink.table.planner.plan.utils.ObjectExplodeTableFunc.eval(ExplodeFunctionUtil.scala:34)
{code}

The reason is functions in ExplodeFunctionUtil do not handle null data. Since 1.12, the bug is fixed, see https://issues.apache.org/jira/browse/FLINK-18528
"	FLINK	Closed	3	1	4624	pull-request-available
13342758	ColumnIntervalUtil#getColumnIntervalWithFilter does not consider the case when the predicate is a false constant	"To reproduce this bug, add the following test case to {{DeadlockBreakupTest.scala}}

{code:scala}
@Test
def testSubplanReuse_DeadlockCausedByReusingExchangeInAncestor(): Unit = {
  util.tableEnv.getConfig.getConfiguration.setBoolean(
    OptimizerConfigOptions.TABLE_OPTIMIZER_REUSE_SUB_PLAN_ENABLED, true)
  util.tableEnv.getConfig.getConfiguration.setBoolean(
    OptimizerConfigOptions.TABLE_OPTIMIZER_MULTIPLE_INPUT_ENABLED, false)
  util.tableEnv.getConfig.getConfiguration.setString(
    ExecutionConfigOptions.TABLE_EXEC_DISABLED_OPERATORS, ""NestedLoopJoin,SortMergeJoin"")
  val sqlQuery =
    """"""
      |WITH T1 AS (SELECT x1.*, x2.a AS k, x2.b AS v FROM x x1 LEFT JOIN x x2 ON x1.a = x2.a WHERE x2.b > 0)
      |SELECT x.a, T1.* FROM x LEFT JOIN T1 ON x.a = T1.k WHERE x.b > 0 AND T1.v = 0
      |"""""".stripMargin
  util.verifyPlan(sqlQuery)
}
{code}

And we'll get the exception stack
{code}
java.lang.RuntimeException: Error while applying rule FlinkLogicalJoinConverter(in:NONE,out:LOGICAL), args [rel#414:LogicalJoin.NONE.any.[](left=RelSubset#406,right=RelSubset#413,condition==($0, $4),joinType=inner)]

	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:86)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.getOptimizedPlan(TableTestBase.scala:431)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:348)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyPlan(TableTestBase.scala:271)
	at org.apache.flink.table.planner.plan.batch.sql.DeadlockBreakupTest.testSubplanReuse_DeadlockCausedByReusingExchangeInAncestor(DeadlockBreakupTest.scala:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.lang.RuntimeException: Error occurred while applying rule FlinkLogicalJoinConverter(in:NONE,out:LOGICAL)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:169)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
	... 54 more
Caused by: java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayBuffer.scala:48)
	at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:74)
	at scala.collection.mutable.ArrayBuffer.reduceLeft(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$.getColumnIntervalWithFilter(ColumnIntervalUtil.scala:219)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:181)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:801)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:156)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:801)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:114)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:309)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:306)
	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38)
	at scala.collection.IndexedSeqOptimized$class.exists(IndexedSeqOptimized.scala:46)
	at scala.collection.mutable.ArrayBuffer.exists(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getEquiInnerJoinRowCount(FlinkRelMdRowCount.scala:306)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:268)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:410)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalJoin.computeSelfCost(FlinkLogicalJoin.scala:64)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCost.getNonCumulativeCost(FlinkRelMdNonCumulativeCost.scala:41)
	at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost_$(Unknown Source)
	at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getNonCumulativeCost(RelMetadataQuery.java:288)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.getCost(VolcanoPlanner.java:705)
	at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements0(RelSubset.java:415)
	at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements(RelSubset.java:398)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1268)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1227)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
	... 58 more

{code}"	FLINK	Closed	3	1	4624	pull-request-available
13363765	Introduce ExecNodeTranslator	An ExecNodeTranslator is responsible for translating an ExecNode to Flink operator, it has two sub-classes now: SingleTransformationTranslator and MultipleTransformationTranslator. For SingleTransformationTranslator, we can set some info (such as parallelism) to the Transformation in the base class and will reduce many duplicate code	FLINK	Closed	3	7	4624	pull-request-available
13240102	Add travis profile for flink-table-planner-blink/flink-table-runtime-blink	The flink-table-planner-blink module and flink-table-runtime-blink module take almost 30 minutes, and that may cause libraries profile frequently hits timeouts; we can resolve this by moving flink-table-planner-blink and flink-table-runtime-blink into a separate profile.	FLINK	Closed	3	4	4624	pull-request-available
13299665	supports EXPLAIN statement in TableEnvironment#executeSql and Table#explain api	"[FLINK-16366|https://issues.apache.org/jira/browse/FLINK-16366] has introduced executeSql method in TableEnvironment, but EXPLAIN statement is not supported because [FLINK-17126|https://issues.apache.org/jira/browse/FLINK-17126] is not finished. This issue aims to support EXPLAIN statement  and introduce Table#explain api after FLINK-17126 finished.

"	FLINK	Closed	3	7	4624	pull-request-available
13344533	Refactor verifyPlan methods in TableTestBase	" Currently, we use {{verifyPlan}} method to verify the plan result for both {{RelNode}} plan and {{ExecNode}} plan, because their instances are the same. But once the implementation of {{RelNode}} and {{ExecNode}} are separated, we can't get {{ESTIMATED_COST}} and {{CHANGELOG_MODE}} on {{ExecNode}} plan. So in order to make those methods more clear, we will do the following refactoring:
1. replace {{planBefore}} with {{ast}} in xml file. {{ast}} is ""Abstract Syntax Tree"", corresponding to ""Abstract Syntax Tree"" item in the explain result; 
2. remove {{planAfter}}, introduce  {{optimized rel plan}} and {{optimized exec plan}}. {{optimized rel plan}}  is the optimized rel plan, and is similar to ""Optimized Physical Plan"" item in the explain result. but different from ""Optimized Physical Plan"", {{optimized rel plan}} can represent either optimized logical rel plan (for rule testing) or optimized physical rel plan (for changelog validation, etc). {{optimized exec plan}} is the optimized execution plan, corresponding to ""Optimized Execution Plan"" item in the explain result. see https://issues.apache.org/jira/browse/FLINK-20478 for more details about explain refactor
3. keep {{verifyPlan}} method, which will print {{ast}}, {{optimized rel plan}} and {{optimized exec plan}}. 
4. add {{verifyRelPlan}} method, which will print {{ast}}, {{optimized rel plan}}
5. add {{verifyExecPlan}} method, which will print {{ast}} and {{optimized exec plan}}. "	FLINK	Closed	3	7	4624	pull-request-available
13362522	When you insert multiple inserts with statementSet, you modify multiple inserts with OPTIONS('table-name '=' XXX '), but only the first one takes effect	"{code:java}
//代码占位符
StatementSet statementSet = tableEnvironment.createStatementSet();
String sql1 = ""insert into test select a,b,c from test_a_12342 /*+
OPTIONS('table-name'='test_a_1')*/"";
String sql2 = ""insert into test select a,b,c from test_a_12342 /*+
OPTIONS('table-name'='test_a_2')*/"";
statementSet.addInsertSql(sql1);
statementSet.addInsertSql(sql2);
statementSet.execute();
{code}
Sql code as above, in the final after the insert is put test_a_1 table data into the two times, and test_a_2 data did not insert, is excuse me this bug"	FLINK	Closed	2	1	4624	pull-request-available
13303799	TableEnvironmentITCase.testExecuteSqlAndToDataStream failed	"Here is the instance: https://dev.azure.com/imjark/Flink/_build/results?buildId=61&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=6855dd6e-a7b0-5fd1-158e-29fc186b16c8


{code:java}
[ERROR] Tests run: 26, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12 s <<< FAILURE! - in org.apache.flink.table.api.TableEnvironmentITCase
[ERROR] testExecuteSqlAndToDataStream[StreamTableEnvironment](org.apache.flink.table.api.TableEnvironmentITCase)  Time elapsed: 0.513 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.table.api.TableEnvironmentITCase.testExecuteSqlAndToDataStream(TableEnvironmentITCase.scala:343)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   TableEnvironmentITCase.testExecuteSqlAndToDataStream:343
[INFO] 
[ERROR] Tests run: 791, Failures: 1, Errors: 0, Skipped: 13

{code}
"	FLINK	Closed	1	1	4624	test-stability
13427078	Make implicit assumption of SQL local hash explicit	"If there are multiple consecutive and the same hash shuffles, SQL planner will change them except the first one to use forward partitioner, so that these operators can be chained to reduce unnecessary shuffles.

However, sometimes the local hash operators are not chained (e.g. multiple inputs), and  this kind of forward partitioners will turn into forward job edges. These forward edges still have the local keyBy assumption, so that they cannot be changed into rescale/rebalance edges, otherwise it can lead to incorrect results. This prevents the adaptive batch scheduler from determining parallelism for other forward edge downstream job vertices (see FLINK-25046).

When SQL planner optimizes the case of multiple consecutive the same groupBy, it should use {{ForwardForConsecutiveHashPartitioner}}, so that the runtime framework can further decide whether the partitioner can be changed to rescale or not."	FLINK	Closed	3	7	4624	pull-request-available
13246776	Correct package name after relocation	some scala classes's package name is not updated after [FLINK-13266|https://issues.apache.org/jira/browse/FLINK-13267], this issue aims to correct the package names	FLINK	Resolved	1	7	4624	pull-request-available
13342364	Rename table.optimizer.union-all-as-breakpoint-disabled to table.optimizer.union-all-as-breakpoint-enabled	{{table.optimizer.union-all-as-breakpoint-disabled}} is defined in {{RelNodeBlockPlanBuilder}}  and is an internal experimental config. While {{disabled}} and {{false}} (as default value) is very obscure.  I suggest to change {{table.optimizer.union-all-as-breakpoint-disabled}} to {{table.optimizer.union-all-as-breakpoint-enabled}} and use {{true}} as default value, which is easier to understand. As this config is an internal experimental config, we don't mark it as deprecated.	FLINK	Closed	3	4	4624	pull-request-available
13239679	Introduce planner rule to push projection into TableSource	This issue aims to support push projection into ProjectableTableSource or NestedFieldsProjectableTableSource to reduce output fields of a TableSource	FLINK	Closed	3	2	4624	pull-request-available
13288620	Remove deprecated method in StreamTableSink	"[FLIP-84|https://cwiki.apache.org/confluence/display/FLINK/FLIP-84%3A+Improve+%26+Refactor+API+of+TableEnvironment] proposes to unify the behavior of {{TableEnvironment}} and {{StreamTableEnvironment}}, and requires the {{StreamTableSink}} always returns {{DataStream}}. However
{{StreamTableSink.emitDataStream}} returns nothing and is deprecated since Flink 1.9, So we will remove it."	FLINK	Closed	3	7	4624	pull-request-available
13347472	Separate the implementation of batch group aggregate nodes	"Batch group aggregate nodes include:
BatchExecHashAggregate
BatchExecLocalHashAggregate
BatchExecSortAggregate
BatchExecLocalSortAggregate
BatchExecPythonGroupAggregate"	FLINK	Closed	3	7	4624	pull-request-available
13320399	Use TableEnvironment#executeSql to execute insert statement in sql client	Currently, sql client has a lot of logic to execute an insert job, which can be simplified through executeSql method.	FLINK	Closed	3	7	4624	pull-request-available
13235294	Introduce planner rules about aggregate	"This issue aims to introduce planner rules about aggregate, rules include:
1. {{AggregateCalcMergeRule}}, that recognizes {{Aggregate}} on top of a {{Calc}} and if possible aggregate through the calc or removes the calc
2. {{AggregateReduceGroupingRule}}, that reduces unless grouping columns
3. {{PruneAggregateCallRule}}, that removes unreferenced AggregateCall from Aggregate
4. {{FlinkAggregateRemoveRule}}, that is copied from Calcite's AggregateRemoveRule, and supports SUM, MIN, MAX, AUXILIARY_GROUP functions in non-empty group aggregate
5. {{FlinkAggregateJoinTransposeRule}}, that is copied from Calcite's AggregateJoinTransposeRule, and supports Left/Right outer join and aggregate with AUXILIARY_GROUP"	FLINK	Closed	3	2	4624	pull-request-available
13267696	Join condition could be simplified in logical phase	"currently the plan of tpcds q38.sql contains {{NestedLoopJoin}}, because it's join condition is {{CAST(AND(IS NOT DISTINCT FROM($2, $3), IS NOT DISTINCT FROM($1, $4), IS NOT DISTINCT FROM($0, $5))):BOOLEAN}}, and planner can't find equal join keys from the condition by {{Join#analyzeCondition}}.
{{SimplifyJoinConditionRule}} could solve this."	FLINK	Closed	3	7	4624	pull-request-available
13343489	Simplify type parameter of ExecNode	Currently, {{ExecNode#translateToPlan}} takes {{BatchPlanner}} or {{StreamPlanner}} as a parameter, so {{ExecNode}} has a type parameter {{E <: Planner}}, which indicates the node is a batch node or a streaming node. While in the future, a plan may contain both batch nodes and stream node. The type parameter can be removed, the implementation base class can cast the planner to expected planner.	FLINK	Closed	3	7	4624	pull-request-available
13299555	supports SELECT statement in TableEnvironment#executeSql and Table#execute api	[FLINK-16366|https://issues.apache.org/jira/browse/FLINK-16366] has introduced executeSql method in TableEnvironment, but INSERT statement is not supported because [FLINK-17126|https://issues.apache.org/jira/browse/FLINK-17126] is not finished and the design of [FLINK-14807|https://issues.apache.org/jira/browse/FLINK-14807] is discussing (the whole design of Table#collect and Table#execute are similar, although Table does not introduce collect method based on the conclusion of FLIP-84). This issue aims to support SELECT statement in TableEnvironment#executeSql and Table#execute api.	FLINK	Closed	3	7	4624	pull-request-available
13303954	TableEnvironmentITCase.testStatementSet fails on travis	"{code}
[ERROR] Failures: 
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
[ERROR]   TableEnvironmentITCase.testStatementSet:446 expected:<... fields=[first])
+- [TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- ]TableSourceScan(tabl...> but was:<... fields=[first])
+- [LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])

Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
+- Legacy]TableSourceScan(tabl...>
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=912&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=f83cd372-208c-5ec4-12a8-337462457129"	FLINK	Closed	1	1	4624	test-stability
13231930	Supports dag (multiple-sinks query) optimization	"Currently, Flink planner will optimize the plan in {{writeToSink}} method. If there are more than one sink in a query, each sink-tree will be optimized independent and the result execution plans are also completely independent. Actually, there is a high probability of duplicate computing for a multiple-sinks query. This issue aims to resolve the above problem. 
The basic idea of the solution is as following: 
1. lazy optimization: does not optimize the plan in {{writeToSink}} method, just puts the plan into a collection.
2. whole plan optimization and execution: a new {{execute}} method is added in {{TableEnvironment}}, this method will trigger whole plan optimization and execute the job.

The basic idea of dag (multiple-sinks query) optimization:
1. decompose the dag into different block, the leaf block is the common sub-plan
2. optimize each block from leaf block to root block, each block only needs to be optimized once
e.g. 
{code:scala}
val table = tableEnv.sqlQuery(""select * from (select a as a1, b as b1 from MyTable where a > 0) t1, (select b as b2, c as c2 from MyTable where c is not null) t2 where a1 = b2"")
tableEnv.registerTable(""TempTable"", table)

val table1 = tableEnv.sqlQuery(""select a1, b1 from TempTable where a1 >= 70"")
tableEnv.writeToSink(table1, Sink1)

val table2 = tableEnv.sqlQuery(""select a1, c2 from TempTable where a1 < 70"")
tableEnv.writeToSink(table2, Sink2)
{code}


 !image-2019-05-07-13-33-02-793.png! 

the above plan will be decomposed into 3 blocks, block1 is the input of block2 and block3. block2 and block3 will be optimized after block1 has been optimized."	FLINK	Closed	3	2	4624	pull-request-available
13439989	The plan for query with local sort is incorrect if adaptive batch scheduler is enabled	"Add the following test case in ForwardHashExchangeTest


{code:java}
  @Test
    public void testRankWithHashShuffle() {
        util.verifyExecPlan(
                ""SELECT * FROM (SELECT a, b, RANK() OVER(PARTITION BY a ORDER BY b) rk FROM T) WHERE rk <= 10"");
    }
{code}

The result plan is:

{code:java}
Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[true], select=[a, b, w0$o0])
+- Exchange(distribution=[forward])
   +- Sort(orderBy=[a ASC, b ASC])
      +- Exchange(distribution=[hash[a]])
         +- Rank(rankType=[RANK], rankRange=[rankStart=1, rankEnd=10], partitionBy=[a], orderBy=[b ASC], global=[false], select=[a, b])
            +- Sort(orderBy=[a ASC, b ASC])
                +- TableSourceScan(table=[[default_catalog, default_database, T, project=[a, b], metadata=[]]], fields=[a, b])
{code}

There should be an additional {{Exchange(distribution=[forward])}} node between local {{Rank}} and {{Sort}}, other wise if adaptive batch scheduler is enabled but operator chain is disabled, the result may be wrong. Because the parallelism for local {{Rank}} and {{Sort}} should be same, otherwise the adaptive batch scheduler may change their parallelism.

 Local sort agg has the similar problem.
"	FLINK	Closed	3	1	4624	pull-request-available
13288621	Correct the execution behavior of TableEnvironment and StreamTableEnvironment	"Both {{TableEnvironment.execute()}} and {{StreamExecutionEnvironment.execute}} can trigger a Flink table program execution. However if you use {{TableEnvironment}} to build a Flink table program, you must use {{TableEnvironment.execute()}} to trigger execution, because you can’t get the {{StreamExecutionEnvironment}} instance. If you use {{StreamTableEnvironment}} to build a Flink table program, you can use both to trigger execution. If you convert a table program to a {{DataStream}} program (using {{StreamExecutionEnvironment.toAppendStream/toRetractStream}}), you also can use both to trigger execution. So it’s hard to explain which `execute` method should be used.

To correct current messy trigger point, we propose that: for {{TableEnvironment}} and {{StreamTableEnvironment}}, you must use {{TableEnvironment.execute()}} to trigger table program execution, once you convert the table program to a {{DataStream}} program (through {{toAppendStream}} or {{toRetractStream}} method), you must use {{StreamExecutionEnvironment.execute}} to trigger the {{DataStream}} program.

please refer to [FLIP-84|https://cwiki.apache.org/confluence/display/FLINK/FLIP-84%3A+Improve+%26+Refactor+API+of+TableEnvironment] for more detail.
"	FLINK	Closed	3	7	4624	pull-request-available
13247978	CatalogTableStatisticsConverter & TreeNode should be in correct package	currently, {{CatalogTableStatisticsConverter}} is in {{org.apache.flink.table.util}}, {{TreeNode}} is in {{org.apache.flink.table.planner.plan}}. {{CatalogTableStatisticsConverter}} should be in {{org.apache.flink.table.planner.utils}}, {{TreeNode}} should be in {{org.apache.flink.table.planner.expressions}}.	FLINK	Closed	3	1	4624	pull-request-available
13350567	Separate the implementation of batch window aggregate nodes	"Batch window aggregate nodes include:
BatchExecHashWindowAggregate
BatchExecLocalHashWindowAggregate
BatchExecSortWindowAggregate
BatchExecLocalSortWindowAggregate
BatchExecPythonGroupWindowAggregate"	FLINK	Closed	3	7	4624	pull-request-available
13225795	Add support for generating optimized logical plan for simple group aggregate on stream	"This issue only involves simple stream group aggregate, complex group aggregate(including GROUP SETS and DISTINCT) will also be finished in new issue.

A logical aggregate will be converted to StreamExecGroupAggregate, and a StreamExecGroupAggregate will be written to two-stage aggregates (StreamExecLocalGroupAggregate and StreamExecGlobalGroupAggregate) if mini-batch is enabled and all aggregate functions are mergeable.

Retraction rules will also be ported, because multiple level aggregates will produce retraction message.
"	FLINK	Closed	3	2	4624	pull-request-available
13238497	Extracted creation & configuration of FrameworkConfig & RelBuilder to separate class in blink planner	just as commit ([e682395a|https://github.com/apache/flink/commit/e682395ae4e13caba0e2fdd98868f69ede9f3b3e]) in flink planner, do similar things in blink planner	FLINK	Closed	3	4	4624	pull-request-available
13271592	The digest of sub-plan reuse should contain retraction traits for stream physical nodes	"This bug is found in [FLINK-14946| https://issues.apache.org/jira/browse/FLINK-14946]:

The plan for the given sql in [FLINK-14946| https://issues.apache.org/jira/browse/FLINK-14946] is
 !image-2019-12-02-10-49-46-916.png! 

however, the plan after sub-plan reuse is:
 !image-2019-12-02-10-52-01-399.png! 

in the first picture, we could find that the accMode of two joins are different, but the two joins are reused in the second picture. 

The reason is the digest of sub-plan reuse does not contain retraction traits for stream physical nodes now.
"	FLINK	Closed	3	1	4624	pull-request-available
13269568	add local aggregate to solve data skew for ROLLUP/CUBE case	"Many tpc-ds queries have {{rollup}} keyword, which will be translated to multiple groups. 
for example:  {{group by rollup (channel, id)}} is equivalent {{group by (channel, id)}} +  {{group by (channel)}} +  {{group by ()}}. 
All data on empty group will be shuffled to a single node, It is a typical data skew case. If there is a local aggregate, the data size shuffled to the single node will be greatly reduced. However, currently the cost mode can't estimate the local aggregate's cost, and the plan with local aggregate may be chose even the query has {{rollup}} keyword.
we could add a rule based phase (after physical phase) to enforce local aggregate if it's input has empty group."	FLINK	Closed	3	7	4624	pull-request-available
13311786	Introduce TableResult#await method to wait for data ready	"Currently, {{TableEnvironment.executeSql()}}  method for INSERT statement returns TableResult once the job is submitted. Users must use {{tableResult.getJobClient.get().getJobExecutionResult(Thread.currentThread().getContextClassLoader).get()}} to wait the job finish. This API looks very ugly.
So this issue aims to introduce {{TableResult#await}} method, the code snippet looks like:

{code:java}
val tEnv = ...
// submit the job and wait job finish
tEnv.executeSql(""insert into ..."").await()
{code}

the suggested new methods are:

{code:java}
	/**
	 * Wait if necessary until the data is ready.
	 *
	 * <p>For select operation, this method will wait unit the first row can be accessed in local.
	 * For insert operation, this method will wait for the job to finish, because the result contains only one row.
	 * For other operations, this method will return immediately, because the result is ready in local.
	 *
	 * @throws ExecutionException if this future completed exceptionally
	 * @throws InterruptedException if the current thread was interrupted while waiting
	 */
	void await() throws InterruptedException, ExecutionException;

	/**
	 * Wait if necessary for at most the given time for the data to be ready.
	 *
	 * <p>For select operation, this method will wait unit the first row can be accessed in local.
	 * For insert operation, this method will wait for the job to finish, because the result contains only one row.
	 * For other operations, this method will return immediately, because the result is ready in local.
	 *
	 * @param timeout the maximum time to wait
	 * @param unit the time unit of the timeout argument
	 * @throws ExecutionException if this future completed exceptionally
	 * @throws InterruptedException if the current thread was interrupted while waiting
	 * @throws TimeoutException if the wait timed out
	 */
	void await(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;

{code}

"	FLINK	Closed	3	4	4624	pull-request-available
13272383	Remove XmlOutput util class in blink planner	{{org.apache.flink.table.planner.utils.XmlOutput}} is introduced in blink planner to fix a redundant line break for each CDATA section. This has been fixed in Calcite via 1745f752561be04ae34d1fa08593c2d3ba4470e8. 	FLINK	Closed	3	11500	4624	pull-request-available, stale-major
13230822	Add support for converting (NOT) IN/ (NOT) EXISTS to SemiJoin, and generating optimized logical plan	"This issue aims to convert IN/EXISTS to semi-join, and NOT IN/NOT EXISTS to anti-join.

In Calcite, [SemiJoin|https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/rel/core/SemiJoin.java] only represents semi-join, (could not represent anti-join) and requires equi join condition. Queries like `select * from left where left.a1 in (select right.a2 from right where left.b1 > right.b2)` and `select * from left where not exists (select * from right)` could not be converted to Calcite SemiJoin operator.

To solve the above problem, We need copy the {{SemiJoin}} class to Flink, and make the following changes:
1. make {{SemiJoin}} extending from {{Join}}, not from {{EquiJoin}}. (to support non-equi join condition) 
2. add {{isAnti}} field attribute to represent anti-join.

Currently, there are no rules to convert (NOT) IN/ (NOT) EXISTS to SemiJoin, so we need a whole new rule (named {{FlinkSubQueryRemoveRule}}) to meet our requirement."	FLINK	Closed	3	2	4624	pull-request-available
13266841	blink planner should also fetch catalog statistics for permanent CatalogTableImpl	currently, blink planner will fetch the catalog statistics and convert to {{TableStats}} for permanent catalog table in {{DatabaseCalciteSchema}}. However, only {{ConnectorCatalogTable}} is handled in {{convertPermanentTable}} method. This issue aims to support {{CatalogTableImpl}}	FLINK	Closed	3	7	4624	pull-request-available
13316045	use TableResult#collect to get select result for sql client	Currently, sql client has a lot of logic to handle the select result in sql client, which can be simplified through {{TableResult#collect}} method.	FLINK	Closed	3	7	4624	pull-request-available
13241010	Introduce join reorder planner rules in blink planner	"This issue aims to let blink planner support join reorder. {{LoptOptimizeJoinRule}} in Calcite could meet our requirement for now, so we could use directly this rule in blink planner. {{JoinToMultiJoinRule}} , {{ProjectMultiJoinMergeRule}} and {{FilterMultiJoinMergeRule}} should be also introduced to support {{LoptOptimizeJoinRule}}.

additionally, we add a new rule named {{RewriteMultiJoinConditionRule}} which could apply transitive closure on `MultiJoin` for equi-join predicates to create more optimization possibilities.

by default, join reorder is disabled, unless {{sql.optimizer.join-reorder.enabled}} is set as true."	FLINK	Closed	3	2	4624	pull-request-available
13428607	Explicitly set the partitioner for the sql operators whose shuffle and sort are removed	"After FLINK-25995 is finished, we have add an exchange (which will be converted to ForwardForConsecutiveHashPartitioner) for the nodes which do not need explicitly hash shuffle (which input has already hashed)

e.g.
{code:sql}
WITH r AS (SELECT * FROM T1, T2 WHERE a1 = a2 AND c1 LIKE 'He%') SELECT sum(b1) FROM r group by a1
{code}

the plan after FLINK-25995 is finished:
{code:java}
Calc(select=[EXPR$0])
+- HashAggregate(isMerge=[false], groupBy=[a1], select=[a1, SUM(b1) AS EXPR$0])
   +- Exchange(distribution=[keep_input_as_is[hash[a1]])
      +- Calc(select=[a1, b1])
         +- HashJoin(joinType=[InnerJoin], where=[(a1 = a2)], select=[a1, b1, a2], build=[left])
            :- Exchange(distribution=[hash[a1]])
            :  +- Calc(select=[a1, b1], where=[LIKE(c1, 'He%')])
            :     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[], project=[a1, b1, c1], metadata=[]]], fields=[a1, b1, c1])
            +- Exchange(distribution=[hash[a2]])
               +- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2])
{code}

but data between {{Calc}} and {{HashJoin}} may be out of order once their parallelism is different, so an {{Exchange(distribution=[keep_input_as_is[hash[a1]])}} should be added between them."	FLINK	Closed	3	1	4624	pull-request-available
13044693	support partition pruning on Table API & SQL	"Many data sources are partitionable storage, e.g. HDFS, Druid. And many queries just need to read a small subset of the total data. We can use partition information to prune or skip over files irrelevant to the user’s queries. Both query optimization time and execution time can be reduced obviously, especially for a large partitioned table.
"	FLINK	Closed	3	2	4624	pull-request-available
13317346	Simplify the methods of Executor interface in sql client	"After {{TableEnvironment#executeSql}} is introduced, many methods in {{Executor}} interface can be replaced with {{TableEnvironment#executeSql}}. Those methods include:
listCatalogs, listDatabases, createTable, dropTable, listTables, listFunctions, useCatalog, useDatabase, getTableSchema (use {{DESCRIBE xx}})
"	FLINK	Closed	3	7	4624	pull-request-available
13385379	Fix the issue that the InternalRow as arguments in Python UDAF	"The problem is reported from
https://stackoverflow.com/questions/68026832/pyflink-udaf-internalrow-vs-row

In release-1.14, we have reconstructed the coders and fixed this problem. So this problem only appeared in 1.13
"	FLINK	Closed	3	1	5059	pull-request-available
13355361	Introduce InternalTimerService In PyFlink	Introduce InternalTimerService that allows to specify a key and a namespace to which timers should be scoped.	FLINK	Resolved	3	7	5059	pull-request-available
13404411	PyFlink YARN per-job on Docker test fails on Azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24669&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=23186
{code}
Sep 30 18:20:22 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
Sep 30 18:20:22 	at java.security.AccessController.doPrivileged(Native Method)
Sep 30 18:20:22 	at javax.security.auth.Subject.doAs(Subject.java:422)
Sep 30 18:20:22 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
Sep 30 18:20:22 
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
Sep 30 18:20:22 	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)
Sep 30 18:20:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 30 18:20:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 30 18:20:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 30 18:20:22 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
Sep 30 18:20:22 	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)
Sep 30 18:20:22 	... 17 more

{code}"	FLINK	Resolved	2	1	5059	stale-major, test-stability
13263915	Support primitive data types in Python user-defined functions	This jira is a sub-task of FLINK-14388. In this jira, only primitive types are dedicated to be supported for python UDF. 	FLINK	Closed	3	2	5059	pull-request-available
13400910	Remove unnecessary info in Loopback mode	If the job runs in loopback mode, it will print unnecessary info `apache_beam.typehints.native_type_compatibility - INFO - Using Any for unsupported type: typing.Sequence[~T]` in the console. We need to remove this confusing info.	FLINK	Closed	3	4	5059	pull-request-available
13264789	UDF cannot be in the join condition in blink planner	"Currently, UDF cannot be in the join condition in blink planner, for the following example:

val util = batchTestUtil()
val left = util.addTableSource[(Int, Int, String)](""Table3"",'a, 'b, 'c)
val right = util.addTableSource[(Int, Int, Int, String, Long)](""Table5"", 'd, 'e, 'f, 'g, 'h)
util.addFunction(""Func"", Func0)
val result = left
 .leftOuterJoin(right, ""a === d && Func(a) === a + d"")
 .select(""a, d"")
util.verifyExplain(result)

The following exception will be thrown:

java.util.NoSuchElementException: No value presentjava.util.NoSuchElementException: No value present
 at java.util.Optional.get(Optional.java:135) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:475) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.lambda$visit$0(QueryOperationConverter.java:470) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:472) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.lambda$visit$0(QueryOperationConverter.java:470) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:472) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:228) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:139) at org.apache.flink.table.operations.JoinQueryOperation.accept(JoinQueryOperation.java:128) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:136) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:116) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:61) at org.apache.flink.table.operations.JoinQueryOperation.accept(JoinQueryOperation.java:128) at org.apache.flink.table.planner.plan.QueryOperationConverter.lambda$defaultMethod$0(QueryOperationConverter.java:135) at java.util.Collections$SingletonList.forEach(Collections.java:4822) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:135) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:116) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:46) at org.apache.flink.table.operations.ProjectQueryOperation.accept(ProjectQueryOperation.java:75) at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.scala:151) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$1.apply(BatchPlanner.scala:81) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$1.apply(BatchPlanner.scala:79) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:79) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnvironmentImpl.java:296) at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyExplain(TableTestBase.scala:404) at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExplain(TableTestBase.scala:304) at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExplain(TableTestBase.scala:301) at org.apache.flink.table.planner.plan.batch.table.validation.JoinValidationTest.testOuterJoinWithPythonFunctionInCondition(JoinValidationTest.scala:130) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)"	FLINK	Closed	3	1	5059	pull-request-available
13301143	Add the sign and shasum logic for PyFlink wheel packages	Add the sign and sha logic for PyFlink wheel packages	FLINK	Closed	3	7	5059	pull-request-available
13296167	Python UDF doesn't work with protobuf 3.6.1	PyFlink UDF execution module is not compatible with protobuf 3.6.1 because it uses a newer interface to access the enum value defined in proto model. We need to fix this.	FLINK	Closed	3	1	5059	pull-request-available
13298014	Add Cython support for primitive data types	Support Primitive DataTypes in Cython	FLINK	Closed	3	7	5059	pull-request-available
13258762	Optimize Python UDFs with parameters of constant values	We need support Python UDFs with parameters of constant values. It should be noticed that the constant parameters are not needed to be transferred between the Java operator and the Python worker.	FLINK	Closed	3	7	5059	pull-request-available
13454551	test_es_sink_dynamic failed in jdk11	"
{code:java}
2022-06-21T03:01:35.4707985Z Jun 21 03:01:35 _________________ FlinkElasticsearch7Test.test_es_sink_dynamic _________________
2022-06-21T03:01:35.4709206Z Jun 21 03:01:35 
2022-06-21T03:01:35.4710708Z Jun 21 03:01:35 self = <pyflink.datastream.tests.test_connectors.FlinkElasticsearch7Test testMethod=test_es_sink_dynamic>
2022-06-21T03:01:35.4711754Z Jun 21 03:01:35 
2022-06-21T03:01:35.4712481Z Jun 21 03:01:35     def test_es_sink_dynamic(self):
2022-06-21T03:01:35.4715653Z Jun 21 03:01:35         ds = self.env.from_collection(
2022-06-21T03:01:35.4718082Z Jun 21 03:01:35             [{'name': 'ada', 'id': '1'}, {'name': 'luna', 'id': '2'}],
2022-06-21T03:01:35.4719972Z Jun 21 03:01:35             type_info=Types.MAP(Types.STRING(), Types.STRING()))
2022-06-21T03:01:35.4721209Z Jun 21 03:01:35     
2022-06-21T03:01:35.4722120Z Jun 21 03:01:35 >       es_dynamic_index_sink = Elasticsearch7SinkBuilder() \
2022-06-21T03:01:35.4723876Z Jun 21 03:01:35             .set_emitter(ElasticsearchEmitter.dynamic_index('name', 'id')) \
2022-06-21T03:01:35.4725448Z Jun 21 03:01:35             .set_hosts(['localhost:9200']) \
2022-06-21T03:01:35.4726419Z Jun 21 03:01:35             .build()
2022-06-21T03:01:35.4727430Z Jun 21 03:01:35 
2022-06-21T03:01:35.4877335Z Jun 21 03:01:35 pyflink/datastream/tests/test_connectors.py:132: 
2022-06-21T03:01:35.4882723Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4884972Z Jun 21 03:01:35 pyflink/datastream/connectors/elasticsearch.py:130: in set_hosts
2022-06-21T03:01:35.4886124Z Jun 21 03:01:35     j_http_hosts_array = to_jarray(JHttpHost, j_http_hosts_list)
2022-06-21T03:01:35.4887527Z Jun 21 03:01:35 pyflink/util/java_utils.py:37: in to_jarray
2022-06-21T03:01:35.4888600Z Jun 21 03:01:35     j_arr[i] = arr[i]
2022-06-21T03:01:35.4890812Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:238: in __setitem__
2022-06-21T03:01:35.4892201Z Jun 21 03:01:35     return self.__set_item(key, value)
2022-06-21T03:01:35.4893842Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:221: in __set_item
2022-06-21T03:01:35.4895153Z Jun 21 03:01:35     return get_return_value(answer, self._gateway_client)
2022-06-21T03:01:35.4896282Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4897191Z Jun 21 03:01:35 
2022-06-21T03:01:35.4900656Z Jun 21 03:01:35 answer = 'zsorg.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.ap...haded.py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n'
2022-06-21T03:01:35.4903369Z Jun 21 03:01:35 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f7dd5b8b580>
2022-06-21T03:01:35.4904543Z Jun 21 03:01:35 target_id = None, name = None
2022-06-21T03:01:35.4905404Z Jun 21 03:01:35 
2022-06-21T03:01:35.4906381Z Jun 21 03:01:35     def get_return_value(answer, gateway_client, target_id=None, name=None):
2022-06-21T03:01:35.4908583Z Jun 21 03:01:35         """"""Converts an answer received from the Java gateway into a Python object.
2022-06-21T03:01:35.4909687Z Jun 21 03:01:35     
2022-06-21T03:01:35.4910838Z Jun 21 03:01:35         For example, string representation of integers are converted to Python
2022-06-21T03:01:35.4912061Z Jun 21 03:01:35         integer, string representation of objects are converted to JavaObject
2022-06-21T03:01:35.4913137Z Jun 21 03:01:35         instances, etc.
2022-06-21T03:01:35.4913921Z Jun 21 03:01:35     
2022-06-21T03:01:35.4914859Z Jun 21 03:01:35         :param answer: the string returned by the Java gateway
2022-06-21T03:01:35.4916648Z Jun 21 03:01:35         :param gateway_client: the gateway client used to communicate with the Java
2022-06-21T03:01:35.4918294Z Jun 21 03:01:35             Gateway. Only necessary if the answer is a reference (e.g., object,
2022-06-21T03:01:35.4919591Z Jun 21 03:01:35             list, map)
2022-06-21T03:01:35.4920758Z Jun 21 03:01:35         :param target_id: the name of the object from which the answer comes from
2022-06-21T03:01:35.4921963Z Jun 21 03:01:35             (e.g., *object1* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4923122Z Jun 21 03:01:35         :param name: the name of the member from which the answer comes from
2022-06-21T03:01:35.4924246Z Jun 21 03:01:35             (e.g., *hello* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4925140Z Jun 21 03:01:35         """"""
2022-06-21T03:01:35.4925981Z Jun 21 03:01:35         if is_error(answer)[0]:
2022-06-21T03:01:35.4926846Z Jun 21 03:01:35             if len(answer) > 1:
2022-06-21T03:01:35.4927828Z Jun 21 03:01:35                 type = answer[1]
2022-06-21T03:01:35.4928784Z Jun 21 03:01:35                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2022-06-21T03:01:35.4929792Z Jun 21 03:01:35                 if answer[1] == REFERENCE_TYPE:
2022-06-21T03:01:35.4930858Z Jun 21 03:01:35                     raise Py4JJavaError(
2022-06-21T03:01:35.4931806Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}.\n"".
2022-06-21T03:01:35.4932792Z Jun 21 03:01:35                         format(target_id, ""."", name), value)
2022-06-21T03:01:35.4933346Z Jun 21 03:01:35                 else:
2022-06-21T03:01:35.4933841Z Jun 21 03:01:35 >                   raise Py4JError(
2022-06-21T03:01:35.4934829Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
2022-06-21T03:01:35.4935466Z Jun 21 03:01:35                         format(target_id, ""."", name, value))
2022-06-21T03:01:35.4936110Z Jun 21 03:01:35 E                   py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:
2022-06-21T03:01:35.4937114Z Jun 21 03:01:35 E                   org.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost to org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost
2022-06-21T03:01:35.4938983Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166)
2022-06-21T03:01:35.4940139Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144)
2022-06-21T03:01:35.4941251Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.execute(ArrayCommand.java:97)
2022-06-21T03:01:35.4942313Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2022-06-21T03:01:35.4943334Z Jun 21 03:01:35 E                   	at java.base/java.lang.Thread.run(Thread.java:829)
2022-06-21T03:01:35.4943905Z Jun 21 03:01:35 
2022-06-21T03:01:35.4945225Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/protocol.py:330: Py4JError
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2

"	FLINK	Closed	2	1	5059	pull-request-available, test-stability
13284178	Introduce Python Physical Correlate RelNodes  which are containers for Python TableFunction	Dedicated Python Physical Correlate RelNodes should be introduced for Python TableFunction execution. These nodes exists as containers for Python TableFunctions which could be executed in a batch and then we can employ PythonTableFunctionOperator for Python TableFunction execution.	FLINK	Closed	3	7	5059	pull-request-available
13348664	Fix some aggregate and flat_aggregate tests failed in py35	"We need to fix some aggregate/flat_aggregate tests due to FLINK-20769 , which cased the difference of result oder.

These tests include:

pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_aggregate
pyflink/table/tests/test_row_based_operation.py::StreamRowBasedOperationITTests::test_flat_aggregate_list_view
pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_list_view
pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_map_view_iterate"	FLINK	Closed	2	1	5059	pull-request-available, test-stability
13321230	Python UDTF doesn't work well when the return type isn't generator	"For the following Python UDTF which return type is not a generator:
{code}
# test specify the input_types
@udtf(input_types=[DataTypes.BIGINT()],
           result_types=[DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()])
def split(x):
    return Row(10, 10, 10)
{code}

When used in a job, the operator containing the UDTF will not emit data to the downstream operator and there is also no exception thrown. The job just finished without any result.

We should properly handle this case: either support this use case or throw a proper exception if we don't want to support this case."	FLINK	Closed	3	1	5059	pull-request-available
13315069	Extract the implementation logic of Beam in coders	Extract the implementation logic of Beam in coders, so that the implementation of general coders and Beam coders can be decoupled	FLINK	Closed	3	4	5059	pull-request-available
13419297	Build wheels failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=bf344275-d244-5694-d05a-7ad127794669
"	FLINK	Resolved	1	1	5059	pull-request-available, test-stability
13326648	Add Pandas Batch Group Aggregation Function Operator	Add Pandas Batch Group Aggregation Function Operator	FLINK	Closed	3	7	5059	pull-request-available
13284696	Add rules to push down the Java Calls contained in Python Correlate node	The Java Calls contained in Python Correlate node should be extracted to make sure the TableFunction works well.	FLINK	Closed	3	7	5059	pull-request-available
13326810	Add Python building blocks to make sure the basic functionality of Pandas Batch Group Aggregation could work	"We need to add a few Python building blocks such as PandasAggregateFunctionOperation, PythonAggregateFunction, etc for Pandas Aggregation execution. 

This PR makes sure that a basic end to end Pandas Batch Group Aggregation could be executed."	FLINK	Closed	3	7	5059	pull-request-available
13265258	Support time data types in Python user-defined functions	This jira is a sub-task of FLINK-14388. In this jira, only time types are dedicated to be supported for python UDF.	FLINK	Closed	3	2	5059	pull-request-available
13500812	Replace and redesign the Python api documentation base	The doc of the existing python api is difficult to read and use. I have a demo site for redesigning the Python api documentation base. See https://pyflink-api-docs-test.readthedocs.io/en/latest/ as an example.	FLINK	Closed	3	4	5059	pull-request-available
13319183	The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct	The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct. I will fix it as soon as possible.	FLINK	Closed	3	1	5059	pull-request-available
13414660	Add many kinds of checks in ML Python API	Add many kinds of checks in ML Python API. These checks include pytest, flask8 and mypy.	FLINK	Resolved	3	7	5059	pull-request-available
13328664	Introduce BatchArrowPythonOverWindowAggregateFunctionOperator	Introduce BatchArrowPythonOverWindowAggregateFunctionOperator to support Pandas Batch Over Window Aggregation	FLINK	Closed	3	7	5059	pull-request-available
13382721	Exception on JobClient.get_job_status().result()	"Following code finish with exception
{code:java}
table_env.execute_sql(""""""
    CREATE TABLE IF NOT EXISTS datagen (
        id INT,
        data STRING
    ) WITH (
        'connector' = 'datagen'
    )
table_env.execute_sql(""""""
    CREATE TABLE IF NOT EXISTS print (
        id INT,
        data STRING
    ) WITH (
        'connector' = 'print'
    )
""""""){code}
{code:java}
table_result = table_env.execute_sql(""INSERT INTO print SELECT * FROM datagen"")
table_result.get_job_client().get_job_status().result()

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
ValueError: JavaObject id=o125 is not a valid JobStatus

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-16-ee785b26d044> in <module>
----> 1 result.get_job_client().get_job_status().result()

/usr/local/lib/python3.8/dist-packages/pyflink/common/completable_future.py in result(self)
     76             return self._j_completable_future.get()
     77         else:
---> 78             return self._py_class(self._j_completable_future.get())
     79 
     80     def exception(self):

/usr/lib/python3.8/enum.py in __call__(cls, value, names, module, qualname, type, start)
    307         """"""
    308         if names is None:  # simple value lookup
--> 309             return cls.__new__(cls, value)
    310         # otherwise, functional API: we're creating a new Enum type
    311         return cls._create_(value, names, module=module, qualname=qualname, type=type, start=start)

/usr/lib/python3.8/enum.py in __new__(cls, value)
    598                         )
    599             exc.__context__ = ve_exc
--> 600             raise exc
    601 
    602     def _generate_next_value_(name, start, count, last_values):

/usr/lib/python3.8/enum.py in __new__(cls, value)
    582         try:
    583             exc = None
--> 584             result = cls._missing_(value)
    585         except Exception as e:
    586             exc = e

/usr/lib/python3.8/enum.py in _missing_(cls, value)
    611     @classmethod
    612     def _missing_(cls, value):
--> 613         raise ValueError(""%r is not a valid %s"" % (value, cls.__name__))
    614 
    615     def __repr__(self):

ValueError: JavaObject id=o125 is not a valid JobStatus
{code}"	FLINK	Resolved	3	1	5059	pull-request-available
13290824	Optimize the execution of Python UDF to use generator to eliminate unnecessary function calls	Optimize the result of FlattenRowCoder and ArrowCoder to generator to eliminate unnecessary function calls.	FLINK	Closed	3	7	5059	pull-request-available
13237397	Add the Python Table API Sphinx docs	"As the Python Table API is added, we should add the Python Table API Sphinx docs. This includes the following work:
1) Add scripts to build the Sphinx docs
2) Add a link in the main page to the generated doc"	FLINK	Closed	3	7	5059	pull-request-available
13470341	Reduce pyflink tests time	Currently, it costs about 1 hour 30mins in pyflink tests. We need to optimize it.	FLINK	Resolved	2	4	5059	pull-request-available
13298800	Failed to download conda when running python tests	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7549&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9
This pipeline failed to download conda

If this issue starts appearing more often we should come up with some solution for those kinds of problems.

{code}
CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/anaconda/noarch/babel-2.8.0-py_0.tar.bz2>
Elapsed: -

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.


conda install sphinx failed         please try to exec the script again.        if failed many times, you can try to exec in the form of sudo ./lint-python.sh -f
PYTHON exited with EXIT CODE: 1.
{code}
"	FLINK	Closed	1	1	5059	pull-request-available, test-stability
13305133	"""Failed to find the file"" in ""build_wheels"" stage"	"CI https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1343&view=logs&j=fe7ebddc-3e2f-5c50-79ee-226c8653f218&t=b2830442-93c7-50ff-36f4-5b3e2dca8c83

{code}
Successfully built dill crcmod httplib2 hdfs oauth2client future avro-python3
Installing collected packages: six, pbr, mock, dill, typing, crcmod, numpy, pyarrow, python-dateutil, typing-extensions, fastavro, httplib2, protobuf, pymongo, docopt, idna, chardet, urllib3, requests, hdfs, pyparsing, pydot, pyasn1, pyasn1-modules, rsa, oauth2client, grpcio, future, avro-python3, pytz, apache-beam, cython
Successfully installed apache-beam-2.19.0 avro-python3-1.9.2.1 chardet-3.0.4 crcmod-1.7 cython-0.29.16 dill-0.3.1.1 docopt-0.6.2 fastavro-0.21.24 future-0.18.2 grpcio-1.29.0 hdfs-2.5.8 httplib2-0.12.0 idna-2.9 mock-2.0.0 numpy-1.18.4 oauth2client-3.0.0 pbr-5.4.5 protobuf-3.11.3 pyarrow-0.15.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.1 pymongo-3.10.1 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 requests-2.23.0 rsa-4.0 six-1.14.0 typing-3.7.4.1 typing-extensions-3.7.4.2 urllib3-1.25.9
+ (( i++ ))
+ (( i<3 ))
+ (( i=0 ))
+ (( i<3 ))
+ /home/vsts/work/1/s/flink-python/dev/.conda/envs/3.5/bin/python setup.py bdist_wheel
Compiling pyflink/fn_execution/fast_coder_impl.pyx because it changed.
Compiling pyflink/fn_execution/fast_operations.pyx because it changed.
[1/2] Cythonizing pyflink/fn_execution/fast_coder_impl.pyx
[2/2] Cythonizing pyflink/fn_execution/fast_operations.pyx
Failed to find the file /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/opt/flink-sql-client_*.jar.
{code}"	FLINK	Closed	3	1	5059	pull-request-available, test-stability
13431610	StreamExecutionEnvironmentTests::test_add_python_file failed with ModuleNotFoundError	"
{code:java}
2022-03-02T16:33:43.6649755Z Mar 02 16:33:43 Traceback (most recent call last):
2022-03-02T16:33:43.6650839Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
2022-03-02T16:33:43.6651405Z Mar 02 16:33:43     response = task()
2022-03-02T16:33:43.6652257Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
2022-03-02T16:33:43.6652846Z Mar 02 16:33:43     lambda: self.create_worker().do_instruction(request), request)
2022-03-02T16:33:43.6653655Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 606, in do_instruction
2022-03-02T16:33:43.6654200Z Mar 02 16:33:43     return getattr(self, request_type)(
2022-03-02T16:33:43.6654969Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
2022-03-02T16:33:43.6655549Z Mar 02 16:33:43     bundle_processor.process_bundle(instruction_id))
2022-03-02T16:33:43.6656337Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 999, in process_bundle
2022-03-02T16:33:43.6656936Z Mar 02 16:33:43     input_op_by_transform_id[element.transform_id].process_encoded(
2022-03-02T16:33:43.6657773Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
2022-03-02T16:33:43.6658326Z Mar 02 16:33:43     self.output(decoded_value)
2022-03-02T16:33:43.6658829Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
2022-03-02T16:33:43.6659460Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
2022-03-02T16:33:43.6660261Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2022-03-02T16:33:43.6661064Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
2022-03-02T16:33:43.6661902Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
2022-03-02T16:33:43.6662440Z Mar 02 16:33:43     self._output_processor.process_outputs(o, self.process_element(value))
2022-03-02T16:33:43.6663200Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 63, in process_outputs
2022-03-02T16:33:43.6663750Z Mar 02 16:33:43     self._consumer.process(windowed_value.with_value(results))
2022-03-02T16:33:43.6664475Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 131, in process
2022-03-02T16:33:43.6664969Z Mar 02 16:33:43     for value in o.value:
2022-03-02T16:33:43.6665754Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/datastream/operations.py"", line 179, in wrapped_func
2022-03-02T16:33:43.6666280Z Mar 02 16:33:43     yield from _emit_results(timestamp, watermark, results)
2022-03-02T16:33:43.6667010Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/datastream/input_handler.py"", line 101, in _emit_results
2022-03-02T16:33:43.6667492Z Mar 02 16:33:43     for result in results:
2022-03-02T16:33:43.6668154Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/datastream/data_stream.py"", line 271, in process_element
2022-03-02T16:33:43.6668626Z Mar 02 16:33:43     yield self._map_func(value)
2022-03-02T16:33:43.6669335Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/datastream/tests/test_stream_execution_environment.py"", line 350, in plus_two_map
2022-03-02T16:33:43.6669848Z Mar 02 16:33:43     from test_dep1 import add_two
2022-03-02T16:33:43.6670561Z Mar 02 16:33:43 ModuleNotFoundError: No module named 'test_dep1'
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32448&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901
"	FLINK	Closed	2	1	5059	stale-assigned, test-stability
13476513	Optimize the Python Execution Mode Documentation	https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/python_execution_mode/	FLINK	Closed	3	4	5059	pull-request-available
13355390	Introduce TimeWindow and CountWindow in PyFlink	Introduce TimeWindow, CountWindow and corresponding coders.	FLINK	Resolved	3	7	5059	pull-request-available
13284697	Add rules to transpose the  join condition of Python Correlate node	Because the conditions can’t be executed in Python correlate node, this rule will transpose the conditions after the Python correlate node if the join type is inner join.	FLINK	Closed	3	7	5059	pull-request-available
13309319	Tests Python UDTF support	"Check items as follows:
 * test Python UDTF (inner join/left join) in Blink Planner in batch mode
 * test Python UDTF (inner join/left join) in Blink Planner in streaming mode
 * test Python UDTF (inner join/left join) in Flink Planner in batch mode
 * test Python UDTF (inner join/left join) in Flink Planner in streaming mode"	FLINK	Closed	1	4	5059	release-testing
13437142	Python EmbeddedThreadDependencyTests.test_add_python_file failed on azure	"
{code:java}
Mar 31 10:49:17 =================================== FAILURES ===================================
Mar 31 10:49:17 ______________ EmbeddedThreadDependencyTests.test_add_python_file ______________
Mar 31 10:49:17 
Mar 31 10:49:17 self = <pyflink.table.tests.test_dependency.EmbeddedThreadDependencyTests testMethod=test_add_python_file>
Mar 31 10:49:17 
Mar 31 10:49:17     def test_add_python_file(self):
Mar 31 10:49:17         python_file_dir = os.path.join(self.tempdir, ""python_file_dir_"" + str(uuid.uuid4()))
Mar 31 10:49:17         os.mkdir(python_file_dir)
Mar 31 10:49:17         python_file_path = os.path.join(python_file_dir, ""test_dependency_manage_lib.py"")
Mar 31 10:49:17         with open(python_file_path, 'w') as f:
Mar 31 10:49:17             f.write(""def add_two(a):\n    raise Exception('This function should not be called!')"")
Mar 31 10:49:17         self.t_env.add_python_file(python_file_path)
Mar 31 10:49:17     
Mar 31 10:49:17         python_file_dir_with_higher_priority = os.path.join(
Mar 31 10:49:17             self.tempdir, ""python_file_dir_"" + str(uuid.uuid4()))
Mar 31 10:49:17         os.mkdir(python_file_dir_with_higher_priority)
Mar 31 10:49:17         python_file_path_higher_priority = os.path.join(python_file_dir_with_higher_priority,
Mar 31 10:49:17                                                         ""test_dependency_manage_lib.py"")
Mar 31 10:49:17         with open(python_file_path_higher_priority, 'w') as f:
Mar 31 10:49:17             f.write(""def add_two(a):\n    return a + 2"")
Mar 31 10:49:17         self.t_env.add_python_file(python_file_path_higher_priority)
Mar 31 10:49:17     
Mar 31 10:49:17         def plus_two(i):
Mar 31 10:49:17             from test_dependency_manage_lib import add_two
Mar 31 10:49:17             return add_two(i)
Mar 31 10:49:17     
Mar 31 10:49:17         self.t_env.create_temporary_system_function(
Mar 31 10:49:17             ""add_two"", udf(plus_two, DataTypes.BIGINT(), DataTypes.BIGINT()))
Mar 31 10:49:17         table_sink = source_sink_utils.TestAppendSink(
Mar 31 10:49:17             ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])
Mar 31 10:49:17         self.t_env.register_table_sink(""Results"", table_sink)
Mar 31 10:49:17         t = self.t_env.from_elements([(1, 2), (2, 5), (3, 1)], ['a', 'b'])
Mar 31 10:49:17 >       t.select(expr.call(""add_two"", t.a), t.a).execute_insert(""Results"").wait()
Mar 31 10:49:17 
Mar 31 10:49:17 pyflink/table/tests/test_dependency.py:63: 
Mar 31 10:49:17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 31 10:49:17 pyflink/table/table_result.py:76: in wait
Mar 31 10:49:17     get_method(self._j_table_result, ""await"")()
Mar 31 10:49:17 .tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py:1321: in __call__

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=27239"	FLINK	Open	2	1	5059	auto-deprioritized-major, stale-assigned, test-stability
13347850	Support minibatch to optimize Python UDAF	Support minibatch to optimize Python UDAF	FLINK	Closed	3	4	5059	pull-request-available
13346264	Support row-based operation to accept user-defined function directly	"Usage
{code:java}
@udf(result_type=DataTypes.ROW([DataTypes.FIELD(""a"", DataTypes.INT()),
                               DataTypes.FIELD(""b"", DataTypes.INT())])
def map_func(*args):
    return Row(*args)

t = ...  # type: Table, table schema: [a: String, b: Int]
t.map(map_func){code}"	FLINK	Closed	3	7	5059	pull-request-available
13476330	Optimize the Python DataStream Window Documentation	"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/datastream/operators/windows/
"	FLINK	Resolved	3	4	5059	pull-request-available
13474822	Cannot run PyFlink 1.16 on MacOS with M1 chip	"I have tested it with 2 m1 machines. i will reproduce the bug 100%.

1.m1 machine
macos bigsur 11.5.1 & jdk8 * & jdk11 & python 3.8 & python 3.9
1.m1 machine
macos monterey 12.1 & jdk8 * & jdk11 & python 3.8 & python 3.9

reproduce step:
1.python -m pip install -r flink-python/dev/dev-requirements.txt
2.cd flink-python; python setup.py sdist bdist_wheel; cd apache-flink-libraries; python setup.py sdist; cd ..;
3.python -m pip install apache-flink-libraries/dist/*.tar.gz
4.python -m pip install dist/*.whl

when run [word_count.py|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/table_api_tutorial/] it will cause


{code:java}
<frozen importlib._bootstrap>:219: RuntimeWarning: apache_beam.coders.coder_impl.StreamCoderImpl size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
Traceback (most recent call last):
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 129, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 49, in word_count
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1637, in _open
    startup_loopback_server()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1628, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 44, in <module>
    from pyflink.fn_execution.beam import beam_sdk_worker_main  # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_sdk_worker_main.py"", line 21, in <module>
    import pyflink.fn_execution.beam.beam_operations # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_operations.py"", line 27, in <module>
    from pyflink.fn_execution.state_impl import RemoteKeyedStateBackend, RemoteOperatorStateBackend
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/state_impl.py"", line 33, in <module>
    from pyflink.fn_execution.beam.beam_coders import FlinkCoder
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_coders.py"", line 27, in <module>
    from pyflink.fn_execution.beam import beam_coder_impl_fast as beam_coder_impl
  File ""pyflink/fn_execution/beam/beam_coder_impl_fast.pyx"", line 1, in init pyflink.fn_execution.beam.beam_coder_impl_fast
KeyError: '__pyx_vtable__'
{code}


"	FLINK	Closed	3	1	5059	pull-request-available
13348210	Fix NamesTest due to code style refactor	"Due to the [FLINK-20651|https://issues.apache.org/jira/browse/FLINK-20651], the NameTest failed

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11403&view=results]

I will fix it asap"	FLINK	Closed	3	1	5059	pull-request-available, test-stability
13532614	Python py37-cython: commands failed, error at setup of ProcessWindowTests.test_count_sliding_window	"{noformat}
Apr 06 05:55:13 ___________________________________ summary ____________________________________
Apr 06 05:55:13 ERROR:   py37-cython: commands failed
Apr 06 05:55:13   py38-cython: commands succeeded
Apr 06 05:55:13   py39-cython: commands succeeded
Apr 06 05:55:13   py310-cython: commands succeeded
{noformat}
in logs there is such error for 37 not sure if it is related
{noformat}
Apr 06 04:26:30 ________ ERROR at setup of ProcessWindowTests.test_count_sliding_window ________
Apr 06 04:26:30 
Apr 06 04:26:30 cls = <class 'pyflink.datastream.tests.test_window.ProcessWindowTests'>
Apr 06 04:26:30 
Apr 06 04:26:30     @classmethod
Apr 06 04:26:30     def setUpClass(cls):
Apr 06 04:26:30         super(PyFlinkStreamingTestCase, cls).setUpClass()
Apr 06 04:26:30         cls.env.set_parallelism(2)
Apr 06 04:26:30 >       cls.env.set_runtime_mode(RuntimeExecutionMode.STREAMING)
Apr 06 04:26:30 
Apr 06 04:26:30 pyflink/testing/test_case_utils.py:193: 
Apr 06 04:26:30 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 {noformat}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47960&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24593]"	FLINK	Open	3	1	5059	stale-assigned, test-stability
13397554	Lookback mode doesn't work when mixing use of Python Table API and Python DataStream API	"For the following program:
{code}
import logging
import time

from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment, CoMapFunction
from pyflink.table import StreamTableEnvironment, DataTypes, Schema


def test_chaining():
    env = StreamExecutionEnvironment.get_execution_environment()
    t_env = StreamTableEnvironment.create(stream_execution_environment=env)
    t_env.get_config().get_configuration().set_boolean(""python.operator-chaining.enabled"", False)

    # 1. create source Table
    t_env.execute_sql(""""""
        CREATE TABLE datagen (
            id INT,
            data STRING
        ) WITH (
            'connector' = 'datagen',
            'rows-per-second' = '1000000',
            'fields.id.kind' = 'sequence',
            'fields.id.start' = '1',
            'fields.id.end' = '1000'
        )
    """""")

    # 2. create sink Table
    t_env.execute_sql(""""""
        CREATE TABLE print (
            id BIGINT,
            data STRING,
            flag STRING
        ) WITH (
            'connector' = 'blackhole'
        )
    """""")

    t_env.execute_sql(""""""
        CREATE TABLE print_2 (
            id BIGINT,
            data STRING,
            flag STRING
        ) WITH (
            'connector' = 'blackhole'
        )
    """""")

    # 3. query from source table and perform calculations
    # create a Table from a Table API query:
    source_table = t_env.from_path(""datagen"")

    ds = t_env.to_append_stream(
        source_table,
        Types.ROW([Types.INT(), Types.STRING()]))

    ds1 = ds.map(lambda i: (i[0] * i[0], i[1]))
    ds2 = ds.map(lambda i: (i[0], i[1][2:]))

    class MyCoMapFunction(CoMapFunction):

        def map1(self, value):
            print('hahah')
            return value

        def map2(self, value):
            print('hahah')
            return value

    ds3 = ds1.connect(ds2).map(MyCoMapFunction(), output_type=Types.TUPLE([Types.LONG(), Types.STRING()]))

    ds4 = ds3.map(lambda i: (i[0], i[1], ""left""),
                  output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING()]))

    ds5 = ds3.map(lambda i: (i[0], i[1], ""right""))\
             .map(lambda i: i,
                  output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING()]))

    schema = Schema.new_builder() \
        .column(""f0"", DataTypes.BIGINT()) \
        .column(""f1"", DataTypes.STRING()) \
        .column(""f2"", DataTypes.STRING()) \
        .build()

    result_table_3 = t_env.from_data_stream(ds4, schema)
    statement_set = t_env.create_statement_set()
    statement_set.add_insert(""print"", result_table_3)

    result_table_4 = t_env.from_data_stream(ds5, schema)
    statement_set.add_insert(""print_2"", result_table_4)

    statement_set.execute().wait()


if __name__ == ""__main__"":

    start_ts = time.time()
    test_chaining()
    end_ts = time.time()
    print(""--- %s seconds ---"" % (end_ts - start_ts))
{code}

Lookback mode doesn't work."	FLINK	Resolved	1	1	5059	pull-request-available
13401492	BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23011

{code}
Sep 15 20:40:43 cls = <class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'>
Sep 15 20:40:43 actual = JavaObject id=o8666
Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']
Sep 15 20:40:43 
Sep 15 20:40:43     @classmethod
Sep 15 20:40:43     def assert_equals(cls, actual, expected):
Sep 15 20:40:43         if isinstance(actual, JavaObject):
Sep 15 20:40:43             actual_py_list = cls.to_py_list(actual)
Sep 15 20:40:43         else:
Sep 15 20:40:43             actual_py_list = actual
Sep 15 20:40:43         actual_py_list.sort()
Sep 15 20:40:43         expected.sort()
Sep 15 20:40:43         assert len(actual_py_list) == len(expected)
Sep 15 20:40:43 >       assert all(x == y for x, y in zip(actual_py_list, expected))
Sep 15 20:40:43 E       AssertionError: assert False
Sep 15 20:40:43 E        +  where False = all(<generator object PyFlinkTestCase.assert_equals.<locals>.<genexpr> at 0x7f792d98b900>)
{code}"	FLINK	Resolved	1	1	5059	pull-request-available, test-stability
13412195	Support ML Python API to implement FLIP-173 and FLIP-174	Support ML Python API to implement FLIP-173 and FLIP-174	FLINK	Resolved	3	7	5059	pull-request-available
13290730	Use type hints to declare the signature of the methods	[Type Hints|https://www.python.org/dev/peps/pep-0484/] was introduced in Python 3.5 and it would be great if we can declare the signature of the methods using type hints and introduce [type check|https://realpython.com/python-type-checking/] in the python APIs	FLINK	Closed	3	4	5059	pull-request-available
13368986	"Python Test failed with ""OSError: [Errno 12] Cannot allocate memory"""	"https://dev.azure.com/sewen0794/Flink/_build/results?buildId=249&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=443dc6bf-b240-56df-6acf-c882d4b238da&l=21533

Python Test failed with ""OSError: [Errno 12] Cannot allocate memory"" in Azure Pipeline. I am not sure if it is caused by insufficient machine memory on Azure.
"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13426365	pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state failed on AZP	"The test {{pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state}} fails on AZP:

{code}
2022-02-02T17:44:12.1898582Z Feb 02 17:44:12 =================================== FAILURES ===================================
2022-02-02T17:44:12.1899860Z Feb 02 17:44:12 _____ StreamingModeDataStreamTests.test_keyed_process_function_with_state ______
2022-02-02T17:44:12.1900493Z Feb 02 17:44:12 
2022-02-02T17:44:12.1901218Z Feb 02 17:44:12 self = <pyflink.datastream.tests.test_data_stream.StreamingModeDataStreamTests testMethod=test_keyed_process_function_with_state>
2022-02-02T17:44:12.1901948Z Feb 02 17:44:12 
2022-02-02T17:44:12.1902745Z Feb 02 17:44:12     def test_keyed_process_function_with_state(self):
2022-02-02T17:44:12.1903722Z Feb 02 17:44:12         self.env.get_config().set_auto_watermark_interval(2000)
2022-02-02T17:44:12.1904473Z Feb 02 17:44:12         self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
2022-02-02T17:44:12.1906780Z Feb 02 17:44:12         data_stream = self.env.from_collection([(1, 'hi', '1603708211000'),
2022-02-02T17:44:12.1908034Z Feb 02 17:44:12                                                 (2, 'hello', '1603708224000'),
2022-02-02T17:44:12.1909166Z Feb 02 17:44:12                                                 (3, 'hi', '1603708226000'),
2022-02-02T17:44:12.1910122Z Feb 02 17:44:12                                                 (4, 'hello', '1603708289000'),
2022-02-02T17:44:12.1911099Z Feb 02 17:44:12                                                 (5, 'hi', '1603708291000'),
2022-02-02T17:44:12.1912451Z Feb 02 17:44:12                                                 (6, 'hello', '1603708293000')],
2022-02-02T17:44:12.1913456Z Feb 02 17:44:12                                                type_info=Types.ROW([Types.INT(), Types.STRING(),
2022-02-02T17:44:12.1914338Z Feb 02 17:44:12                                                                     Types.STRING()]))
2022-02-02T17:44:12.1914811Z Feb 02 17:44:12     
2022-02-02T17:44:12.1915317Z Feb 02 17:44:12         class MyTimestampAssigner(TimestampAssigner):
2022-02-02T17:44:12.1915724Z Feb 02 17:44:12     
2022-02-02T17:44:12.1916782Z Feb 02 17:44:12             def extract_timestamp(self, value, record_timestamp) -> int:
2022-02-02T17:44:12.1917621Z Feb 02 17:44:12                 return int(value[2])
2022-02-02T17:44:12.1918262Z Feb 02 17:44:12     
2022-02-02T17:44:12.1918855Z Feb 02 17:44:12         class MyProcessFunction(KeyedProcessFunction):
2022-02-02T17:44:12.1919363Z Feb 02 17:44:12     
2022-02-02T17:44:12.1919744Z Feb 02 17:44:12             def __init__(self):
2022-02-02T17:44:12.1920143Z Feb 02 17:44:12                 self.value_state = None
2022-02-02T17:44:12.1920648Z Feb 02 17:44:12                 self.list_state = None
2022-02-02T17:44:12.1921298Z Feb 02 17:44:12                 self.map_state = None
2022-02-02T17:44:12.1921864Z Feb 02 17:44:12     
2022-02-02T17:44:12.1922479Z Feb 02 17:44:12             def open(self, runtime_context: RuntimeContext):
2022-02-02T17:44:12.1923907Z Feb 02 17:44:12                 value_state_descriptor = ValueStateDescriptor('value_state', Types.INT())
2022-02-02T17:44:12.1924922Z Feb 02 17:44:12                 self.value_state = runtime_context.get_state(value_state_descriptor)
2022-02-02T17:44:12.1925741Z Feb 02 17:44:12                 list_state_descriptor = ListStateDescriptor('list_state', Types.INT())
2022-02-02T17:44:12.1926482Z Feb 02 17:44:12                 self.list_state = runtime_context.get_list_state(list_state_descriptor)
2022-02-02T17:44:12.1927465Z Feb 02 17:44:12                 map_state_descriptor = MapStateDescriptor('map_state', Types.INT(), Types.STRING())
2022-02-02T17:44:12.1927998Z Feb 02 17:44:12                 state_ttl_config = StateTtlConfig \
2022-02-02T17:44:12.1928444Z Feb 02 17:44:12                     .new_builder(Time.seconds(1)) \
2022-02-02T17:44:12.1928943Z Feb 02 17:44:12                     .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \
2022-02-02T17:44:12.1929462Z Feb 02 17:44:12                     .set_state_visibility(
2022-02-02T17:44:12.1929939Z Feb 02 17:44:12                         StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp) \
2022-02-02T17:44:12.1930601Z Feb 02 17:44:12                     .disable_cleanup_in_background() \
2022-02-02T17:44:12.1931032Z Feb 02 17:44:12                     .build()
2022-02-02T17:44:12.1931480Z Feb 02 17:44:12                 map_state_descriptor.enable_time_to_live(state_ttl_config)
2022-02-02T17:44:12.1932018Z Feb 02 17:44:12                 self.map_state = runtime_context.get_map_state(map_state_descriptor)
2022-02-02T17:44:12.1932610Z Feb 02 17:44:12     
2022-02-02T17:44:12.1933172Z Feb 02 17:44:12             def process_element(self, value, ctx):
2022-02-02T17:44:12.1933623Z Feb 02 17:44:12                 import time
2022-02-02T17:44:12.1934007Z Feb 02 17:44:12                 time.sleep(1)
2022-02-02T17:44:12.1934419Z Feb 02 17:44:12                 current_value = self.value_state.value()
2022-02-02T17:44:12.1934977Z Feb 02 17:44:12                 self.value_state.update(value[0])
2022-02-02T17:44:12.1935451Z Feb 02 17:44:12                 current_list = [_ for _ in self.list_state.get()]
2022-02-02T17:44:12.1935921Z Feb 02 17:44:12                 self.list_state.add(value[0])
2022-02-02T17:44:12.1936401Z Feb 02 17:44:12                 map_entries = {k: v for k, v in self.map_state.items()}
2022-02-02T17:44:12.1936862Z Feb 02 17:44:12                 keys = sorted(map_entries.keys())
2022-02-02T17:44:12.1937649Z Feb 02 17:44:12                 map_entries_string = [str(k) + ': ' + str(map_entries[k]) for k in keys]
2022-02-02T17:44:12.1938404Z Feb 02 17:44:12                 map_entries_string = '{' + ', '.join(map_entries_string) + '}'
2022-02-02T17:44:12.1938906Z Feb 02 17:44:12                 self.map_state.put(value[0], value[1])
2022-02-02T17:44:12.1939350Z Feb 02 17:44:12                 current_key = ctx.get_current_key()
2022-02-02T17:44:12.1939889Z Feb 02 17:44:12                 yield ""current key: {}, current value state: {}, current list state: {}, "" \
2022-02-02T17:44:12.1940521Z Feb 02 17:44:12                       ""current map state: {}, current value: {}"".format(str(current_key),
2022-02-02T17:44:12.1941111Z Feb 02 17:44:12                                                                         str(current_value),
2022-02-02T17:44:12.1941645Z Feb 02 17:44:12                                                                         str(current_list),
2022-02-02T17:44:12.1942254Z Feb 02 17:44:12                                                                         map_entries_string,
2022-02-02T17:44:12.1942796Z Feb 02 17:44:12                                                                         str(value))
2022-02-02T17:44:12.1943369Z Feb 02 17:44:12     
2022-02-02T17:44:12.1943761Z Feb 02 17:44:12             def on_timer(self, timestamp, ctx):
2022-02-02T17:44:12.1944178Z Feb 02 17:44:12                 pass
2022-02-02T17:44:12.1944503Z Feb 02 17:44:12     
2022-02-02T17:44:12.1944898Z Feb 02 17:44:12         watermark_strategy = WatermarkStrategy.for_monotonous_timestamps() \
2022-02-02T17:44:12.1945537Z Feb 02 17:44:12             .with_timestamp_assigner(MyTimestampAssigner())
2022-02-02T17:44:12.1946018Z Feb 02 17:44:12         data_stream.assign_timestamps_and_watermarks(watermark_strategy) \
2022-02-02T17:44:12.1946525Z Feb 02 17:44:12             .key_by(lambda x: x[1], key_type=Types.STRING()) \
2022-02-02T17:44:12.1947019Z Feb 02 17:44:12             .process(MyProcessFunction(), output_type=Types.STRING()) \
2022-02-02T17:44:12.1947465Z Feb 02 17:44:12             .add_sink(self.test_sink)
2022-02-02T17:44:12.1948146Z Feb 02 17:44:12         self.env.execute('test time stamp assigner with keyed process function')
2022-02-02T17:44:12.1948637Z Feb 02 17:44:12         results = self.test_sink.get_results()
2022-02-02T17:44:12.1949166Z Feb 02 17:44:12         expected = [""current key: hi, current value state: None, current list state: [], ""
2022-02-02T17:44:12.1949957Z Feb 02 17:44:12                     ""current map state: {}, current value: Row(f0=1, f1='hi', ""
2022-02-02T17:44:12.1950624Z Feb 02 17:44:12                     ""f2='1603708211000')"",
2022-02-02T17:44:12.1951234Z Feb 02 17:44:12                     ""current key: hello, current value state: None, ""
2022-02-02T17:44:12.1951822Z Feb 02 17:44:12                     ""current list state: [], current map state: {}, current value: Row(f0=2,""
2022-02-02T17:44:12.1952596Z Feb 02 17:44:12                     "" f1='hello', f2='1603708224000')"",
2022-02-02T17:44:12.1953292Z Feb 02 17:44:12                     ""current key: hi, current value state: 1, current list state: [1], ""
2022-02-02T17:44:12.1954134Z Feb 02 17:44:12                     ""current map state: {1: hi}, current value: Row(f0=3, f1='hi', ""
2022-02-02T17:44:12.1954799Z Feb 02 17:44:12                     ""f2='1603708226000')"",
2022-02-02T17:44:12.1955331Z Feb 02 17:44:12                     ""current key: hello, current value state: 2, current list state: [2], ""
2022-02-02T17:44:12.1956145Z Feb 02 17:44:12                     ""current map state: {2: hello}, current value: Row(f0=4, f1='hello', ""
2022-02-02T17:44:12.1956826Z Feb 02 17:44:12                     ""f2='1603708289000')"",
2022-02-02T17:44:12.1957362Z Feb 02 17:44:12                     ""current key: hi, current value state: 3, current list state: [1, 3], ""
2022-02-02T17:44:12.1958156Z Feb 02 17:44:12                     ""current map state: {1: hi, 3: hi}, current value: Row(f0=5, f1='hi', ""
2022-02-02T17:44:12.1958845Z Feb 02 17:44:12                     ""f2='1603708291000')"",
2022-02-02T17:44:12.1959382Z Feb 02 17:44:12                     ""current key: hello, current value state: 4, current list state: [2, 4],""
2022-02-02T17:44:12.1960011Z Feb 02 17:44:12                     "" current map state: {2: hello, 4: hello}, current value: Row(f0=6, ""
2022-02-02T17:44:12.1960715Z Feb 02 17:44:12                     ""f1='hello', f2='1603708293000')""]
2022-02-02T17:44:12.1961159Z Feb 02 17:44:12 >       self.assert_equals_sorted(expected, results)
2022-02-02T17:44:12.1961533Z Feb 02 17:44:12 
2022-02-02T17:44:12.1961906Z Feb 02 17:44:12 pyflink/datastream/tests/test_data_stream.py:683: 
2022-02-02T17:44:12.1962464Z Feb 02 17:44:12 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-02T17:44:12.1963186Z Feb 02 17:44:12 pyflink/datastream/tests/test_data_stream.py:62: in assert_equals_sorted
2022-02-02T17:44:12.1963670Z Feb 02 17:44:12     self.assertEqual(expected, actual)
2022-02-02T17:44:12.1964685Z Feb 02 17:44:12 E   AssertionError: Lists differ: [""cur[719 chars]te: {1: hi, 3: hi}, current value: Row(f0=5, f[172 chars]0')""] != [""cur[719 chars]te: {3: hi}, current value: Row(f0=5, f1='hi',[165 chars]0')""]
2022-02-02T17:44:12.1965369Z Feb 02 17:44:12 E   
2022-02-02T17:44:12.1965731Z Feb 02 17:44:12 E   First differing element 4:
2022-02-02T17:44:12.1966428Z Feb 02 17:44:12 E   ""curr[80 chars]te: {1: hi, 3: hi}, current value: Row(f0=5, f[23 chars]00')""
2022-02-02T17:44:12.1967192Z Feb 02 17:44:12 E   ""curr[80 chars]te: {3: hi}, current value: Row(f0=5, f1='hi',[16 chars]00')""
2022-02-02T17:44:12.1967860Z Feb 02 17:44:12 E   
2022-02-02T17:44:12.1968268Z Feb 02 17:44:12 E   Diff is 1211 characters long. Set self.maxDiff to None to see it.
2022-02-02T17:44:12.1968783Z Feb 02 17:44:12 =============================== warnings summary ===============================
2022-02-02T17:44:12.1969374Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_classpaths
2022-02-02T17:44:12.1970541Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/future/standard_library/__init__.py:65: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
2022-02-02T17:44:12.1971219Z Feb 02 17:44:12     import imp
2022-02-02T17:44:12.1971530Z Feb 02 17:44:12 
2022-02-02T17:44:12.1972027Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_classpaths
2022-02-02T17:44:12.1973535Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/typehints/typehints.py:693: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
2022-02-02T17:44:12.1974526Z Feb 02 17:44:12     if not isinstance(type_params, collections.Iterable):
2022-02-02T17:44:12.1974930Z Feb 02 17:44:12 
2022-02-02T17:44:12.1975407Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_classpaths
2022-02-02T17:44:12.1976680Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/typehints/typehints.py:532: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
2022-02-02T17:44:12.1977509Z Feb 02 17:44:12     if not isinstance(type_params, (collections.Sequence, set)):
2022-02-02T17:44:12.1977939Z Feb 02 17:44:12 
2022-02-02T17:44:12.1978432Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_python_archive
2022-02-02T17:44:12.1979475Z Feb 02 17:44:12   /__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/_pytest/threadexception.py:75: PytestUnhandledThreadExceptionWarning: Exception in thread read_grpc_client_inputs
2022-02-02T17:44:12.1980064Z Feb 02 17:44:12   
2022-02-02T17:44:12.1980434Z Feb 02 17:44:12   Traceback (most recent call last):
2022-02-02T17:44:12.1981152Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/dev/.conda/envs/3.8/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
2022-02-02T17:44:12.1981642Z Feb 02 17:44:12       self.run()
2022-02-02T17:44:12.1982368Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/dev/.conda/envs/3.8/lib/python3.8/threading.py"", line 870, in run
2022-02-02T17:44:12.1982884Z Feb 02 17:44:12       self._target(*self._args, **self._kwargs)
2022-02-02T17:44:12.1983865Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/data_plane.py"", line 598, in <lambda>
2022-02-02T17:44:12.1984471Z Feb 02 17:44:12       target=lambda: self._read_inputs(elements_iterator),
2022-02-02T17:44:12.1985299Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/data_plane.py"", line 581, in _read_inputs
2022-02-02T17:44:12.1985881Z Feb 02 17:44:12       for elements in elements_iterator:
2022-02-02T17:44:12.1986760Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/grpc/_channel.py"", line 426, in __next__
2022-02-02T17:44:12.1987262Z Feb 02 17:44:12       return self._next()
2022-02-02T17:44:12.1987946Z Feb 02 17:44:12     File ""/__w/1/s/flink-python/.tox/py38/lib/python3.8/site-packages/grpc/_channel.py"", line 826, in _next
2022-02-02T17:44:12.1988425Z Feb 02 17:44:12       raise self
2022-02-02T17:44:12.1989023Z Feb 02 17:44:12   grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
2022-02-02T17:44:12.1990067Z Feb 02 17:44:12   	status = StatusCode.CANCELLED
2022-02-02T17:44:12.1990653Z Feb 02 17:44:12   	details = ""Multiplexer hanging up""
2022-02-02T17:44:12.1991849Z Feb 02 17:44:12   	debug_error_string = ""{""created"":""@1643823819.576493566"",""description"":""Error received from peer ipv4:127.0.0.1:33091"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1074,""grpc_message"":""Multiplexer hanging up"",""grpc_status"":1}""
2022-02-02T17:44:12.1993432Z Feb 02 17:44:12   >
2022-02-02T17:44:12.1993889Z Feb 02 17:44:12   
2022-02-02T17:44:12.1994521Z Feb 02 17:44:12     warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))
2022-02-02T17:44:12.1995279Z Feb 02 17:44:12 
2022-02-02T17:44:12.1996037Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_python_file
2022-02-02T17:44:12.1997435Z Feb 02 17:44:12   /__w/1/s/flink-python/pyflink/table/table_environment.py:1997: DeprecationWarning: Deprecated in 1.12. Use from_data_stream(DataStream, *Expression) instead.
2022-02-02T17:44:12.1998269Z Feb 02 17:44:12     warnings.warn(
2022-02-02T17:44:12.1998594Z Feb 02 17:44:12 
2022-02-02T17:44:12.1999075Z Feb 02 17:44:12 pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_execute
2022-02-02T17:44:12.2000008Z Feb 02 17:44:12   /__w/1/s/flink-python/pyflink/table/table_environment.py:538: DeprecationWarning: Deprecated in 1.10. Use create_table instead.
2022-02-02T17:44:12.2000823Z Feb 02 17:44:12     warnings.warn(""Deprecated in 1.10. Use create_table instead."", DeprecationWarning)
2022-02-02T17:44:12.2001522Z Feb 02 17:44:12 
2022-02-02T17:44:12.2002614Z Feb 02 17:44:12 -- Docs: https://docs.pytest.org/en/stable/warnings.html
2022-02-02T17:44:12.2003603Z Feb 02 17:44:12 ============================= slowest 20 durations =============================
2022-02-02T17:44:12.2004618Z Feb 02 17:44:12 10.16s call     pyflink/datastream/tests/test_connectors.py::ConnectorTests::test_stream_file_sink
2022-02-02T17:44:12.2005726Z Feb 02 17:44:12 9.83s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_process_function_with_state
2022-02-02T17:44:12.2006511Z Feb 02 17:44:12 8.79s call     pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state
2022-02-02T17:44:12.2007232Z Feb 02 17:44:12 6.78s call     pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_add_python_file
2022-02-02T17:44:12.2007961Z Feb 02 17:44:12 5.52s call     pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_execute_and_collect
2022-02-02T17:44:12.2009001Z Feb 02 17:44:12 5.44s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_execute_and_collect
2022-02-02T17:44:12.2010033Z Feb 02 17:44:12 5.26s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_basic_co_operations_with_output_type
2022-02-02T17:44:12.2011152Z Feb 02 17:44:12 5.25s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_basic_co_operations
2022-02-02T17:44:12.2012377Z Feb 02 17:44:12 4.53s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_co_process
2022-02-02T17:44:12.2013701Z Feb 02 17:44:12 4.35s call     pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_set_requirements_with_cached_directory
2022-02-02T17:44:12.2014884Z Feb 02 17:44:12 4.32s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_reduce_with_state
2022-02-02T17:44:12.2015900Z Feb 02 17:44:12 4.26s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_flat_map
2022-02-02T17:44:12.2016970Z Feb 02 17:44:12 4.21s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_co_map
2022-02-02T17:44:12.2018270Z Feb 02 17:44:12 4.06s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_filter
2022-02-02T17:44:12.2019463Z Feb 02 17:44:12 3.90s call     pyflink/datastream/tests/test_stream_execution_environment.py::StreamExecutionEnvironmentTests::test_set_requirements_without_cached_directory
2022-02-02T17:44:12.2020715Z Feb 02 17:44:12 3.90s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_aggregating_state
2022-02-02T17:44:12.2021749Z Feb 02 17:44:12 3.87s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_keyed_co_flat_map
2022-02-02T17:44:12.2022862Z Feb 02 17:44:12 3.84s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_multi_key_by
2022-02-02T17:44:12.2024078Z Feb 02 17:44:12 3.83s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_time_window
2022-02-02T17:44:12.2024912Z Feb 02 17:44:12 3.83s call     pyflink/datastream/tests/test_data_stream.py::BatchModeDataStreamTests::test_count_window
2022-02-02T17:44:12.2025750Z Feb 02 17:44:12 =========================== short test summary info ============================
2022-02-02T17:44:12.2026370Z Feb 02 17:44:12 FAILED pyflink/datastream/tests/test_data_stream.py::StreamingModeDataStreamTests::test_keyed_process_function_with_state
2022-02-02T17:44:12.2027008Z Feb 02 17:44:12 ======= 1 failed, 154 passed, 1 skipped, 6 warnings in 235.76s (0:03:55) =======
2022-02-02T17:44:12.5428501Z Feb 02 17:44:12 test module /__w/1/s/flink-python/pyflink/datastream failed
2022-02-02T17:44:12.5431151Z Feb 02 17:44:12 ERROR: InvocationError for command /bin/bash ./dev/integration_test.sh (exited with code 1)
2022-02-02T17:44:12.5432097Z Feb 02 17:44:12 py38 finish: run-test  after 999.77 seconds
2022-02-02T17:44:12.5436171Z Feb 02 17:44:12 py38 start: run-test-post 
2022-02-02T17:44:12.5437071Z Feb 02 17:44:12 py38 finish: run-test-post  after 0.00 seconds
2022-02-02T17:44:12.5438162Z Feb 02 17:44:12 ___________________________________ summary ____________________________________
2022-02-02T17:44:12.5453873Z Feb 02 17:44:12 ERROR:   py38: commands failed
2022-02-02T17:44:12.5455066Z Feb 02 17:44:12 cleanup /__w/1/s/flink-python/.tox/.tmp/package/1/apache-flink-1.15.dev0.zip
2022-02-02T17:44:12.6013749Z Feb 02 17:44:12 ============tox checks... [FAILED]============
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30642&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24759"	FLINK	Resolved	2	1	5059	pull-request-available, test-stability
13237387	Add an interactive shell for Python Table API	We should add an interactive shell for the Python Table API. It will have the similar functionality like the Scala Shell.	FLINK	Closed	3	7	5059	pull-request-available
13346477	test_configuration.test_add_all test failed in py35	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10946&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490]
{code:java}
2020-12-17T01:07:04.9062839Z _______________________ ConfigurationTests.test_add_all ________________________
2020-12-17T01:07:04.9063107Z 
2020-12-17T01:07:04.9063436Z self = <pyflink.common.tests.test_configuration.ConfigurationTests testMethod=test_add_all>
2020-12-17T01:07:04.9063719Z 
2020-12-17T01:07:04.9063951Z     def test_add_all(self):
2020-12-17T01:07:04.9064224Z >       conf = Configuration()
2020-12-17T01:07:04.9064411Z 
2020-12-17T01:07:04.9064665Z pyflink/common/tests/test_configuration.py:85: 
2020-12-17T01:07:04.9065074Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-12-17T01:07:04.9065474Z pyflink/common/configuration.py:43: in __init__
2020-12-17T01:07:04.9065765Z     gateway = get_gateway()
2020-12-17T01:07:04.9066065Z pyflink/java_gateway.py:62: in get_gateway
2020-12-17T01:07:04.9066352Z     _gateway = launch_gateway()
2020-12-17T01:07:04.9066671Z pyflink/java_gateway.py:104: in launch_gateway
2020-12-17T01:07:04.9076442Z     p = launch_gateway_server_process(env, args)
2020-12-17T01:07:04.9076987Z pyflink/pyflink_gateway_server.py:197: in launch_gateway_server_process
2020-12-17T01:07:04.9079207Z     download_apache_avro()
2020-12-17T01:07:04.9079558Z pyflink/pyflink_gateway_server.py:129: in download_apache_avro
2020-12-17T01:07:04.9163662Z     cwd=flink_source_root).decode(""utf-8"")
2020-12-17T01:07:04.9164205Z dev/.conda/envs/3.5/lib/python3.5/subprocess.py:316: in check_output
2020-12-17T01:07:04.9164558Z     **kwargs).stdout
2020-12-17T01:07:04.9164887Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-12-17T01:07:04.9165168Z 
2020-12-17T01:07:04.9165396Z input = None, timeout = None, check = True
2020-12-17T01:07:04.9166036Z popenargs = (['mvn', 'help:evaluate', '-Dexpression=avro.version'],)
2020-12-17T01:07:04.9166615Z kwargs = {'cwd': '/__w/1/s', 'stdout': -1}
2020-12-17T01:07:04.9166959Z process = <subprocess.Popen object at 0x7f0ff8a7a320>
2020-12-17T01:07:04.9168169Z stdout = b""[INFO] Scanning for projects...\nDownloading: https://repo.maven.apache.org/maven2/org/apache/apache/20/apache-20.po...R] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :flink-parent\n""
2020-12-17T01:07:04.9169079Z stderr = None, retcode = 1
2020-12-17T01:07:04.9169259Z 
2020-12-17T01:07:04.9169600Z     def run(*popenargs, input=None, timeout=None, check=False, **kwargs):
2020-12-17T01:07:04.9170061Z         """"""Run command with arguments and return a CompletedProcess instance.
2020-12-17T01:07:04.9170373Z     
2020-12-17T01:07:04.9170683Z         The returned instance will have attributes args, returncode, stdout and
2020-12-17T01:07:04.9171117Z         stderr. By default, stdout and stderr are not captured, and those attributes
2020-12-17T01:07:04.9171577Z         will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them.
2020-12-17T01:07:04.9171895Z     
2020-12-17T01:07:04.9172409Z         If check is True and the exit code was non-zero, it raises a
2020-12-17T01:07:04.9172852Z         CalledProcessError. The CalledProcessError object will have the return code
2020-12-17T01:07:04.9173305Z         in the returncode attribute, and output & stderr attributes if those streams
2020-12-17T01:07:04.9173662Z         were captured.
2020-12-17T01:07:04.9173879Z     
2020-12-17T01:07:04.9174175Z         If timeout is given, and the process takes too long, a TimeoutExpired
2020-12-17T01:07:04.9174537Z         exception will be raised.
2020-12-17T01:07:04.9174773Z     
2020-12-17T01:07:04.9175040Z         There is an optional argument ""input"", allowing you to
2020-12-17T01:07:04.9175663Z         pass a string to the subprocess's stdin.  If you use this argument
2020-12-17T01:07:04.9176301Z         you may not also use the Popen constructor's ""stdin"" argument, as
2020-12-17T01:07:04.9176645Z         it will be used internally.
2020-12-17T01:07:04.9176884Z     
2020-12-17T01:07:04.9177163Z         The other arguments are the same as for the Popen constructor.
2020-12-17T01:07:04.9177469Z     
2020-12-17T01:07:04.9177777Z         If universal_newlines=True is passed, the ""input"" argument must be a
2020-12-17T01:07:04.9178337Z         string and stdout/stderr in the returned object will be strings rather than
2020-12-17T01:07:04.9178779Z         bytes.
2020-12-17T01:07:04.9178981Z         """"""
2020-12-17T01:07:04.9179206Z         if input is not None:
2020-12-17T01:07:04.9179677Z             if 'stdin' in kwargs:
2020-12-17T01:07:04.9180236Z                 raise ValueError('stdin and input arguments may not both be used.')
2020-12-17T01:07:04.9180719Z             kwargs['stdin'] = PIPE
2020-12-17T01:07:04.9180942Z     
2020-12-17T01:07:04.9181203Z         with Popen(*popenargs, **kwargs) as process:
2020-12-17T01:07:04.9181505Z             try:
2020-12-17T01:07:04.9181820Z                 stdout, stderr = process.communicate(input, timeout=timeout)
2020-12-17T01:07:04.9182159Z             except TimeoutExpired:
2020-12-17T01:07:04.9182636Z                 process.kill()
2020-12-17T01:07:04.9182936Z                 stdout, stderr = process.communicate()
2020-12-17T01:07:04.9183280Z                 raise TimeoutExpired(process.args, timeout, output=stdout,
2020-12-17T01:07:04.9183718Z                                      stderr=stderr)
2020-12-17T01:07:04.9183997Z             except:
2020-12-17T01:07:04.9184232Z                 process.kill()
2020-12-17T01:07:04.9184491Z                 process.wait()
2020-12-17T01:07:04.9184716Z                 raise
2020-12-17T01:07:04.9184967Z             retcode = process.poll()
2020-12-17T01:07:04.9185256Z             if check and retcode:
2020-12-17T01:07:04.9185573Z                 raise CalledProcessError(retcode, process.args,
2020-12-17T01:07:04.9185943Z >                                        output=stdout, stderr=stderr)
2020-12-17T01:07:04.9186749Z E               subprocess.CalledProcessError: Command '['mvn', 'help:evaluate', '-Dexpression=avro.version']' returned non-zero exit status 1
{code}
 "	FLINK	Resolved	2	1	5059	pull-request-available, stale-assigned, test-stability
13284969	Travis failed due to python setup	"[https://api.travis-ci.com/v3/job/286671652/log.txt]

[https://api.travis-ci.org/v3/job/649754603/log.txt]

[https://api.travis-ci.com/v3/job/286409130/log.txt]

Collecting avro-python3<2.0.0,>=1.8.1; python_version >= ""3.0"" (from apache-beam==2.19.0->apache-flink==1.11.dev0) Using cached https://files.pythonhosted.org/packages/31/21/d98e2515e5ca0337d7e747e8065227ee77faf5c817bbb74391899613178a/avro-python3-1.9.2.tar.gz Complete output from command python setup.py egg_info: Traceback (most recent call last): File ""<string>"", line 1, in <module> File ""/tmp/pip-install-d6uvsl_b/avro-python3/setup.py"", line 41, in <module> import pycodestyle ModuleNotFoundError: No module named 'pycodestyle' ---------------------------------------- Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-d6uvsl_b/avro-python3/ You are using pip version 10.0.1, however version 20.0.2 is available. You should consider upgrading via the 'pip install --upgrade pip' command."	FLINK	Closed	2	1	5059	pull-request-available
13355071	Add internal state hierarchy in PyFlink	!InternalKvState.png!	FLINK	Resolved	3	7	5059	pull-request-available
13267334	python tox checks fails on travis	"ImportError: cannot import name 'ensure_is_path' from 'importlib_metadata._compat' (/home/travis/build/apache/flink/flink-python/dev/.conda/lib/python3.7/site-packages/importlib_metadata/_compat.py)

============tox checks... [FAILED]============

see: [https://api.travis-ci.org/v3/job/609614353/log.txt]"	FLINK	Closed	3	4	5059	pull-request-available
13335551	Improve the execution time of PyFlink end-to-end tests	"Thanks for the sharing from [~rmetzger], currently the test duration for PyFlink end-to-end test is as following:

||test case||average execution-time||maximum execution-time||
|PyFlink Table end-to-end test|1340s|1877s|
|PyFlink DataStream end-to-end test|387s|575s|
|Kubernetes PyFlink application test|606s|694s|

We need to investigate how to improve them to reduce the execution time.
"	FLINK	Closed	4	1	5059	auto-deprioritized-major, auto-unassigned, pull-request-available
13346697	Fix the deserialized Row losing the field_name information in PyFlink	"Now, the deserialized Row loses the field_name information.
{code:java}
@udf(result_type=DataTypes.STRING())
def get_string_element(my_list):
    my_string = 'xxx'
    for element in my_list:
        if element.integer_element == 2:  # element lost the field_name information
            my_string = element.string_element
    return my_string

t = t_env.from_elements(
    [(""1"", [Row(3, ""flink"")]), (""3"", [Row(2, ""pyflink"")]), (""2"", [Row(2, ""python"")])],
    DataTypes.ROW(
        [DataTypes.FIELD(""Key"", DataTypes.STRING()),
         DataTypes.FIELD(""List_element"",
                         DataTypes.ARRAY(DataTypes.ROW(
                             [DataTypes.FIELD(""integer_element"", DataTypes.INT()),
                              DataTypes.FIELD(""string_element"", DataTypes.STRING())])))]))
print(t.select(get_string_element(t.List_element)).to_pandas())
{code}
element lost the field_name information"	FLINK	Closed	3	1	5059	pull-request-available
13446790	Throw exception when UDAF used in sliding window does not implement merge method in PyFlink	"We use the pane state to optimize the result of calculating the window state, which requires udaf to implement the merge method. However, due to the lack of detection of whether the merge method of udaf is implemented, the user's output result did not meet his expectations and there is no exception. Below is an example of a UDAF that implements the merge method:

{code:python}
class SumAggregateFunction(AggregateFunction):

    def get_value(self, accumulator):
        return accumulator[0]

    def create_accumulator(self):
        return [0]

    def accumulate(self, accumulator, *args):
        accumulator[0] = accumulator[0] + args[0]

    def retract(self, accumulator, *args):
        accumulator[0] = accumulator[0] - args[0]

    def merge(self, accumulator, accumulators):
        for other_acc in accumulators:
            accumulator[0] = accumulator[0] + other_acc[0]

    def get_accumulator_type(self):
        return DataTypes.ARRAY(DataTypes.BIGINT())

    def get_result_type(self):
        return DataTypes.BIGINT()
{code}
"	FLINK	Closed	3	4	5059	pull-request-available
13338024	Use iloc for positional slicing instead of direct slicing in from_pandas	"When you use floats are index of pandas, it produces a wrong results:

 
{code:java}
>>> import pandas as pd
>>> t_env.from_pandas(pd.DataFrame({'a': [1, 2, 3]}, index=[2., 3., 4.])).to_pandas()
   a
0  1
1  2
{code}
 

This is because direct slicing uses the value as index when the index contains floats:

 
{code:java}
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:]
     a
2.0  1
3.0  2
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:]
     a
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:]
   a
4  3{code}
 "	FLINK	Closed	3	1	5059	pull-request-available
13417062	StreamingModeDataStreamTests::test_set_stream_env failed on azure	"{code:java}
2021-12-13T02:25:17.0905034Z Dec 13 02:25:17 =================================== FAILURES ===================================
2021-12-13T02:25:17.0905626Z Dec 13 02:25:17 _____________ StreamExecutionEnvironmentTests.test_set_stream_env ______________
2021-12-13T02:25:17.0906084Z Dec 13 02:25:17 
2021-12-13T02:25:17.0906569Z Dec 13 02:25:17 self = <pyflink.datastream.tests.test_stream_execution_environment.StreamExecutionEnvironmentTests testMethod=test_set_stream_env>
2021-12-13T02:25:17.0907047Z Dec 13 02:25:17 
2021-12-13T02:25:17.0907483Z Dec 13 02:25:17     @unittest.skipIf(on_windows(), ""Symbolic link is not supported on Windows, skipping."")
2021-12-13T02:25:17.0909942Z Dec 13 02:25:17     def test_set_stream_env(self):
2021-12-13T02:25:17.0910424Z Dec 13 02:25:17         import sys
2021-12-13T02:25:17.0910816Z Dec 13 02:25:17         python_exec = sys.executable
2021-12-13T02:25:17.0911369Z Dec 13 02:25:17         tmp_dir = self.tempdir
2021-12-13T02:25:17.0911736Z Dec 13 02:25:17         env = self.env
2021-12-13T02:25:17.0912153Z Dec 13 02:25:17         python_exec_link_path = os.path.join(tmp_dir, ""py_exec"")
2021-12-13T02:25:17.0912876Z Dec 13 02:25:17         os.symlink(python_exec, python_exec_link_path)
2021-12-13T02:25:17.0913342Z Dec 13 02:25:17         env.set_python_executable(python_exec_link_path)
2021-12-13T02:25:17.0913799Z Dec 13 02:25:17     
2021-12-13T02:25:17.0914365Z Dec 13 02:25:17         def check_python_exec(i):
2021-12-13T02:25:17.0914944Z Dec 13 02:25:17             import os
2021-12-13T02:25:17.0915541Z Dec 13 02:25:17             assert os.environ[""python""] == python_exec_link_path
2021-12-13T02:25:17.0917263Z Dec 13 02:25:17             return i
2021-12-13T02:25:17.0917659Z Dec 13 02:25:17     
2021-12-13T02:25:17.0918488Z Dec 13 02:25:17         ds = env.from_collection([1, 2, 3, 4, 5])
2021-12-13T02:25:17.0919021Z Dec 13 02:25:17         ds.map(check_python_exec).add_sink(self.test_sink)
2021-12-13T02:25:17.0919717Z Dec 13 02:25:17 >       env.execute(""test set python executable"")
2021-12-13T02:25:17.0920265Z Dec 13 02:25:17 
2021-12-13T02:25:17.0920707Z Dec 13 02:25:17 pyflink/datastream/tests/test_stream_execution_environment.py:546: 
2021-12-13T02:25:17.0921533Z Dec 13 02:25:17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-12-13T02:25:17.0922325Z Dec 13 02:25:17 pyflink/datastream/stream_execution_environment.py:691: in execute
2021-12-13T02:25:17.0922897Z Dec 13 02:25:17     return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
2021-12-13T02:25:17.0924157Z Dec 13 02:25:17 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1286: in __call__
2021-12-13T02:25:17.0924680Z Dec 13 02:25:17     answer, self.gateway_client, self.target_id, self.name)
2021-12-13T02:25:17.0925131Z Dec 13 02:25:17 pyflink/util/exceptions.py:146: in deco
2021-12-13T02:25:17.0925615Z Dec 13 02:25:17     return f(*a, **kw)
2021-12-13T02:25:17.0926311Z Dec 13 02:25:17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-12-13T02:25:17.0926793Z Dec 13 02:25:17 
2021-12-13T02:25:17.0927550Z Dec 13 02:25:17 answer = 'xro13244'
2021-12-13T02:25:17.0928239Z Dec 13 02:25:17 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f6f553685f8>
2021-12-13T02:25:17.0929456Z Dec 13 02:25:17 target_id = 'o13195', name = 'execute'
2021-12-13T02:25:17.0929975Z Dec 13 02:25:17 
2021-12-13T02:25:17.0930616Z Dec 13 02:25:17     def get_return_value(answer, gateway_client, target_id=None, name=None):
2021-12-13T02:25:17.0931506Z Dec 13 02:25:17         """"""Converts an answer received from the Java gateway into a Python object.
2021-12-13T02:25:17.0931993Z Dec 13 02:25:17     
2021-12-13T02:25:17.0932493Z Dec 13 02:25:17         For example, string representation of integers are converted to Python
2021-12-13T02:25:17.0933249Z Dec 13 02:25:17         integer, string representation of objects are converted to JavaObject
2021-12-13T02:25:17.0933779Z Dec 13 02:25:17         instances, etc.
2021-12-13T02:25:17.0934191Z Dec 13 02:25:17     
2021-12-13T02:25:17.0934809Z Dec 13 02:25:17         :param answer: the string returned by the Java gateway
2021-12-13T02:25:17.0935350Z Dec 13 02:25:17         :param gateway_client: the gateway client used to communicate with the Java
2021-12-13T02:25:17.0935983Z Dec 13 02:25:17             Gateway. Only necessary if the answer is a reference (e.g., object,
2021-12-13T02:25:17.0936593Z Dec 13 02:25:17             list, map)
2021-12-13T02:25:17.0937254Z Dec 13 02:25:17         :param target_id: the name of the object from which the answer comes from
2021-12-13T02:25:17.0937783Z Dec 13 02:25:17             (e.g., *object1* in `object1.hello()`). Optional.
2021-12-13T02:25:17.0938649Z Dec 13 02:25:17         :param name: the name of the member from which the answer comes from
2021-12-13T02:25:17.0939345Z Dec 13 02:25:17             (e.g., *hello* in `object1.hello()`). Optional.
2021-12-13T02:25:17.0939964Z Dec 13 02:25:17         """"""
2021-12-13T02:25:17.0940478Z Dec 13 02:25:17         if is_error(answer)[0]:
2021-12-13T02:25:17.0940892Z Dec 13 02:25:17             if len(answer) > 1:
2021-12-13T02:25:17.0941376Z Dec 13 02:25:17                 type = answer[1]
2021-12-13T02:25:17.0942007Z Dec 13 02:25:17                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2021-12-13T02:25:17.0942632Z Dec 13 02:25:17                 if answer[1] == REFERENCE_TYPE:
2021-12-13T02:25:17.0943271Z Dec 13 02:25:17                     raise Py4JJavaError(
2021-12-13T02:25:17.0943777Z Dec 13 02:25:17                         ""An error occurred while calling {0}{1}{2}.\n"".
2021-12-13T02:25:17.0944418Z Dec 13 02:25:17 >                       format(target_id, ""."", name), value)
2021-12-13T02:25:17.0945358Z Dec 13 02:25:17 E                   py4j.protocol.Py4JJavaError: An error occurred while calling o13195.execute.
2021-12-13T02:25:17.0946044Z Dec 13 02:25:17 E                   : org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-12-13T02:25:17.0946828Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-12-13T02:25:17.0947676Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-12-13T02:25:17.0948832Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-12-13T02:25:17.0949879Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-12-13T02:25:17.0950937Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-12-13T02:25:17.0951856Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-12-13T02:25:17.0952574Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2021-12-13T02:25:17.0953291Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-12-13T02:25:17.0954025Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-12-13T02:25:17.0954868Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-12-13T02:25:17.0955515Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-12-13T02:25:17.0956286Z Dec 13 02:25:17 E                   	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2021-12-13T02:25:17.0956987Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2021-12-13T02:25:17.0957747Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2021-12-13T02:25:17.0958744Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2021-12-13T02:25:17.0959510Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-12-13T02:25:17.0960192Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-12-13T02:25:17.0961333Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-12-13T02:25:17.0962057Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-12-13T02:25:17.0962745Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2021-12-13T02:25:17.0963385Z Dec 13 02:25:17 E                   	at akka.dispatch.OnComplete.internal(Future.scala:300)
2021-12-13T02:25:17.0963953Z Dec 13 02:25:17 E                   	at akka.dispatch.OnComplete.internal(Future.scala:297)
2021-12-13T02:25:17.0964529Z Dec 13 02:25:17 E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2021-12-13T02:25:17.0965092Z Dec 13 02:25:17 E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2021-12-13T02:25:17.0965783Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2021-12-13T02:25:17.0966981Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2021-12-13T02:25:17.0967839Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2021-12-13T02:25:17.0968667Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2021-12-13T02:25:17.0969372Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2021-12-13T02:25:17.0970092Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2021-12-13T02:25:17.0970725Z Dec 13 02:25:17 E                   	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2021-12-13T02:25:17.0971571Z Dec 13 02:25:17 E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2021-12-13T02:25:17.0972299Z Dec 13 02:25:17 E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2021-12-13T02:25:17.0972960Z Dec 13 02:25:17 E                   	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2021-12-13T02:25:17.0973539Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2021-12-13T02:25:17.0974153Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2021-12-13T02:25:17.0974894Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2021-12-13T02:25:17.0975538Z Dec 13 02:25:17 E                   	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2021-12-13T02:25:17.0976226Z Dec 13 02:25:17 E                   	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2021-12-13T02:25:17.0976890Z Dec 13 02:25:17 E                   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2021-12-13T02:25:17.0977517Z Dec 13 02:25:17 E                   	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2021-12-13T02:25:17.0978328Z Dec 13 02:25:17 E                   	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2021-12-13T02:25:17.0978966Z Dec 13 02:25:17 E                   	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2021-12-13T02:25:17.0979629Z Dec 13 02:25:17 E                   	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2021-12-13T02:25:17.0980303Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-12-13T02:25:17.0980930Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-12-13T02:25:17.0981634Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-12-13T02:25:17.0982267Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-12-13T02:25:17.0982938Z Dec 13 02:25:17 E                   Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-12-13T02:25:17.0983705Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-12-13T02:25:17.0984560Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-12-13T02:25:17.0985362Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2021-12-13T02:25:17.0986176Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2021-12-13T02:25:17.0986940Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2021-12-13T02:25:17.0987690Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2021-12-13T02:25:17.0988544Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-12-13T02:25:17.0989257Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2021-12-13T02:25:17.0989869Z Dec 13 02:25:17 E                   	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2021-12-13T02:25:17.0990476Z Dec 13 02:25:17 E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-13T02:25:17.0991160Z Dec 13 02:25:17 E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-13T02:25:17.0991810Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-12-13T02:25:17.0992548Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-12-13T02:25:17.0993369Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-12-13T02:25:17.0994063Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-12-13T02:25:17.0994782Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-12-13T02:25:17.0995498Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-12-13T02:25:17.0996134Z Dec 13 02:25:17 E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-12-13T02:25:17.0996728Z Dec 13 02:25:17 E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-12-13T02:25:17.0997323Z Dec 13 02:25:17 E                   	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-12-13T02:25:17.0997922Z Dec 13 02:25:17 E                   	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-12-13T02:25:17.0998648Z Dec 13 02:25:17 E                   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-12-13T02:25:17.0999267Z Dec 13 02:25:17 E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-12-13T02:25:17.0999884Z Dec 13 02:25:17 E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-12-13T02:25:17.1000502Z Dec 13 02:25:17 E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-12-13T02:25:17.1001186Z Dec 13 02:25:17 E                   	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-12-13T02:25:17.1001737Z Dec 13 02:25:17 E                   	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-12-13T02:25:17.1002318Z Dec 13 02:25:17 E                   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-12-13T02:25:17.1002914Z Dec 13 02:25:17 E                   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-12-13T02:25:17.1003468Z Dec 13 02:25:17 E                   	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-12-13T02:25:17.1004032Z Dec 13 02:25:17 E                   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-12-13T02:25:17.1004770Z Dec 13 02:25:17 E                   	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-12-13T02:25:17.1005426Z Dec 13 02:25:17 E                   	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-12-13T02:25:17.1005900Z Dec 13 02:25:17 E                   	... 4 more
2021-12-13T02:25:17.1006435Z Dec 13 02:25:17 E                   Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush
2021-12-13T02:25:17.1007254Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:361)
2021-12-13T02:25:17.1008300Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.processWatermark(AbstractPythonFunctionOperator.java:222)
2021-12-13T02:25:17.1009150Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.PythonProcessOperator.processWatermark(PythonProcessOperator.java:104)
2021-12-13T02:25:17.1010004Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:239)
2021-12-13T02:25:17.1010906Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:200)
2021-12-13T02:25:17.1011881Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
2021-12-13T02:25:17.1012809Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:136)
2021-12-13T02:25:17.1013645Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
2021-12-13T02:25:17.1014454Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-12-13T02:25:17.1015210Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
2021-12-13T02:25:17.1015953Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2021-12-13T02:25:17.1016812Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
2021-12-13T02:25:17.1017496Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
2021-12-13T02:25:17.1018310Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2021-12-13T02:25:17.1019030Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2021-12-13T02:25:17.1019655Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2021-12-13T02:25:17.1020246Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2021-12-13T02:25:17.1020809Z Dec 13 02:25:17 E                   	at java.lang.Thread.run(Thread.java:748)
2021-12-13T02:25:17.1021413Z Dec 13 02:25:17 E                   Caused by: java.lang.RuntimeException: Failed to close remote bundle
2021-12-13T02:25:17.1022119Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:377)
2021-12-13T02:25:17.1022913Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:361)
2021-12-13T02:25:17.1023754Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$invokeFinishBundle$2(AbstractPythonFunctionOperator.java:340)
2021-12-13T02:25:17.1024611Z Dec 13 02:25:17 E                   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-12-13T02:25:17.1025210Z Dec 13 02:25:17 E                   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-12-13T02:25:17.1025827Z Dec 13 02:25:17 E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-12-13T02:25:17.1026462Z Dec 13 02:25:17 E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-12-13T02:25:17.1026982Z Dec 13 02:25:17 E                   	... 1 more
2021-12-13T02:25:17.1027656Z Dec 13 02:25:17 E                   Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
2021-12-13T02:25:17.1029249Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
2021-12-13T02:25:17.1029844Z Dec 13 02:25:17 E                       response = task()
2021-12-13T02:25:17.1030674Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
2021-12-13T02:25:17.1031384Z Dec 13 02:25:17 E                       lambda: self.create_worker().do_instruction(request), request)
2021-12-13T02:25:17.1032404Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
2021-12-13T02:25:17.1033046Z Dec 13 02:25:17 E                       getattr(request, request_type), request.instruction_id)
2021-12-13T02:25:17.1033923Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
2021-12-13T02:25:17.1034563Z Dec 13 02:25:17 E                       bundle_processor.process_bundle(instruction_id))
2021-12-13T02:25:17.1035445Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
2021-12-13T02:25:17.1036033Z Dec 13 02:25:17 E                       element.data)
2021-12-13T02:25:17.1036863Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
2021-12-13T02:25:17.1037451Z Dec 13 02:25:17 E                       self.output(decoded_value)
2021-12-13T02:25:17.1038003Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1038848Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1039542Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2021-12-13T02:25:17.1040229Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
2021-12-13T02:25:17.1041187Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
2021-12-13T02:25:17.1041793Z Dec 13 02:25:17 E                       self._output_processor.process_outputs(o, self.process_element(value))
2021-12-13T02:25:17.1042611Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 63, in process_outputs
2021-12-13T02:25:17.1043330Z Dec 13 02:25:17 E                       self._consumer.process(windowed_value.with_value(results))
2021-12-13T02:25:17.1044122Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 131, in process
2021-12-13T02:25:17.1044657Z Dec 13 02:25:17 E                       for value in o.value:
2021-12-13T02:25:17.1045416Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/operations.py"", line 179, in wrapped_func
2021-12-13T02:25:17.1046002Z Dec 13 02:25:17 E                       yield from _emit_results(timestamp, watermark, results)
2021-12-13T02:25:17.1046792Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/input_handler.py"", line 101, in _emit_results
2021-12-13T02:25:17.1047351Z Dec 13 02:25:17 E                       for result in results:
2021-12-13T02:25:17.1048215Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/data_stream.py"", line 271, in process_element
2021-12-13T02:25:17.1048808Z Dec 13 02:25:17 E                       yield self._map_func(value)
2021-12-13T02:25:17.1049619Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/tests/test_stream_execution_environment.py"", line 541, in check_python_exec
2021-12-13T02:25:17.1050206Z Dec 13 02:25:17 E                       assert os.environ[""python""] == python_exec_link_path
2021-12-13T02:25:17.1050911Z Dec 13 02:25:17 E                   AssertionError: assert 'python' == '/tmp/tmp1mas4ii5/py_exec'
2021-12-13T02:25:17.1051732Z Dec 13 02:25:17 E                     - /tmp/tmp1mas4ii5/py_exec
2021-12-13T02:25:17.1052146Z Dec 13 02:25:17 E                     + python
2021-12-13T02:25:17.1052510Z Dec 13 02:25:17 E                   
2021-12-13T02:25:17.1053005Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-12-13T02:25:17.1053659Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-12-13T02:25:17.1054257Z Dec 13 02:25:17 E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:60)
2021-12-13T02:25:17.1054970Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
2021-12-13T02:25:17.1055806Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
2021-12-13T02:25:17.1056648Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:375)
2021-12-13T02:25:17.1057241Z Dec 13 02:25:17 E                   	... 7 more
2021-12-13T02:25:17.1057839Z Dec 13 02:25:17 E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
2021-12-13T02:25:17.1059051Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
2021-12-13T02:25:17.1059629Z Dec 13 02:25:17 E                       response = task()
2021-12-13T02:25:17.1060435Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
2021-12-13T02:25:17.1061175Z Dec 13 02:25:17 E                       lambda: self.create_worker().do_instruction(request), request)
2021-12-13T02:25:17.1062217Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
2021-12-13T02:25:17.1062851Z Dec 13 02:25:17 E                       getattr(request, request_type), request.instruction_id)
2021-12-13T02:25:17.1063842Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
2021-12-13T02:25:17.1064466Z Dec 13 02:25:17 E                       bundle_processor.process_bundle(instruction_id))
2021-12-13T02:25:17.1065345Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
2021-12-13T02:25:17.1065925Z Dec 13 02:25:17 E                       element.data)
2021-12-13T02:25:17.1066750Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
2021-12-13T02:25:17.1067358Z Dec 13 02:25:17 E                       self.output(decoded_value)
2021-12-13T02:25:17.1067912Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1068748Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1069441Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2021-12-13T02:25:17.1070226Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
2021-12-13T02:25:17.1071169Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
2021-12-13T02:25:17.1071779Z Dec 13 02:25:17 E                       self._output_processor.process_outputs(o, self.process_element(value))
2021-12-13T02:25:17.1072601Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 63, in process_outputs
2021-12-13T02:25:17.1073175Z Dec 13 02:25:17 E                       self._consumer.process(windowed_value.with_value(results))
2021-12-13T02:25:17.1073958Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 131, in process
2021-12-13T02:25:17.1074508Z Dec 13 02:25:17 E                       for value in o.value:
2021-12-13T02:25:17.1075271Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/operations.py"", line 179, in wrapped_func
2021-12-13T02:25:17.1075850Z Dec 13 02:25:17 E                       yield from _emit_results(timestamp, watermark, results)
2021-12-13T02:25:17.1076635Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/input_handler.py"", line 101, in _emit_results
2021-12-13T02:25:17.1077195Z Dec 13 02:25:17 E                       for result in results:
2021-12-13T02:25:17.1077928Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/data_stream.py"", line 271, in process_element
2021-12-13T02:25:17.1078658Z Dec 13 02:25:17 E                       yield self._map_func(value)
2021-12-13T02:25:17.1079464Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/tests/test_stream_execution_environment.py"", line 541, in check_python_exec
2021-12-13T02:25:17.1080069Z Dec 13 02:25:17 E                       assert os.environ[""python""] == python_exec_link_path
2021-12-13T02:25:17.1080770Z Dec 13 02:25:17 E                   AssertionError: assert 'python' == '/tmp/tmp1mas4ii5/py_exec'
2021-12-13T02:25:17.1081471Z Dec 13 02:25:17 E                     - /tmp/tmp1mas4ii5/py_exec
2021-12-13T02:25:17.1081878Z Dec 13 02:25:17 E                     + python
2021-12-13T02:25:17.1082234Z Dec 13 02:25:17 E                   
2021-12-13T02:25:17.1082893Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
2021-12-13T02:25:17.1083710Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
2021-12-13T02:25:17.1084561Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
2021-12-13T02:25:17.1085416Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
2021-12-13T02:25:17.1086210Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
2021-12-13T02:25:17.1087049Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
2021-12-13T02:25:17.1087941Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
2021-12-13T02:25:17.1089000Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
2021-12-13T02:25:17.1089933Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2021-12-13T02:25:17.1090672Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2021-12-13T02:25:17.1091321Z Dec 13 02:25:17 E                   	... 3 more
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28010&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=24236"	FLINK	Open	3	1	5059	stale-assigned, test-stability
13362527	Support Python UDAF in Session Window	Support Python UDAF in Session Window	FLINK	Resolved	3	7	5059	pull-request-available
13230004	Add simplicity support for submitting Python Table API job in CliFrontend, i.e. `flink run -py wordcount.py` can be work(with simple test).	"Add simplicity support for submitting Python Table API job in CliFrontend, i.e. `flink run -py wordcount.py` can be work(with simple test).   

Support for submitting Python Table API job in CliFrontend，And using `flink run` submit Python Table API job. The current `flink` command command line syntax is as follows:

flink <ACTION> [OPTIONS] [ARGUMENTS]

On the basis of the current `run` ACTION, we add to Python Table API support, specific OPTIONS are as follows:

-py --python  <python-file-name>
Python script with the program entry point. We can configure dependent resources with the `--py-files` option.

* -pyfs --py-files <python-files>   
Attach custom python files for job. Comma can be used as the separator to specify multiple files. The standard python resource file suffixes such as .py/.egg/.zip all also supported.

* -pym --py-module <python-module>  Python module with the program entry point. This option must be used in conjunction with ` --py-files`.

For more details, please refer to [FLIP-38|https://cwiki.apache.org/confluence/display/FLINK/FLIP-38%3A+Python+Table+API]

NOTE: In this JIRA we only need to implement the basic options, without fully implementing the parameters related to UDFs in FLIP-38."	FLINK	Closed	3	7	5059	pull-request-available
13446366	CheckPubSubEmulatorTest.testPull failed with AssertionError	"
{code:java}
2022-05-23T10:08:12.4382583Z May 23 10:08:12 [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull  Time elapsed: 24.092 s  <<< FAILURE!
2022-05-23T10:08:12.4383262Z May 23 10:08:12 java.lang.AssertionError: expected:<1> but was:<0>
2022-05-23T10:08:12.4383939Z May 23 10:08:12 	at org.junit.Assert.fail(Assert.java:89)
2022-05-23T10:08:12.4384555Z May 23 10:08:12 	at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-23T10:08:12.4385199Z May 23 10:08:12 	at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-23T10:08:12.4385829Z May 23 10:08:12 	at org.junit.Assert.assertEquals(Assert.java:633)
2022-05-23T10:08:12.4386614Z May 23 10:08:12 	at org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull(CheckPubSubEmulatorTest.java:78)
2022-05-23T10:08:12.4389064Z May 23 10:08:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-23T10:08:12.4389735Z May 23 10:08:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-23T10:08:12.4390436Z May 23 10:08:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-23T10:08:12.4391073Z May 23 10:08:12 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-23T10:08:12.4391708Z May 23 10:08:12 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-05-23T10:08:12.4392415Z May 23 10:08:12 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-05-23T10:08:12.4393116Z May 23 10:08:12 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-05-23T10:08:12.4393811Z May 23 10:08:12 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-05-23T10:08:12.4394491Z May 23 10:08:12 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-05-23T10:08:12.4395111Z May 23 10:08:12 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-05-23T10:08:12.4395718Z May 23 10:08:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:08:12.4396384Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-05-23T10:08:12.4397042Z May 23 10:08:12 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-05-23T10:08:12.4397697Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-05-23T10:08:12.4398392Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-05-23T10:08:12.4399020Z May 23 10:08:12 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-05-23T10:08:12.4399622Z May 23 10:08:12 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-05-23T10:08:12.4400236Z May 23 10:08:12 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-05-23T10:08:12.4400851Z May 23 10:08:12 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-05-23T10:08:12.4401466Z May 23 10:08:12 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-05-23T10:08:12.4402292Z May 23 10:08:12 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-05-23T10:08:12.4402950Z May 23 10:08:12 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-05-23T10:08:12.4403713Z May 23 10:08:12 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-05-23T10:08:12.4404416Z May 23 10:08:12 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-05-23T10:08:12.4405003Z May 23 10:08:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:08:12.4405602Z May 23 10:08:12 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-05-23T10:08:12.4406166Z May 23 10:08:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-05-23T10:08:12.4406792Z May 23 10:08:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-05-23T10:08:12.4407415Z May 23 10:08:12 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-05-23T10:08:12.4408134Z May 23 10:08:12 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-05-23T10:08:12.4408839Z May 23 10:08:12 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-05-23T10:08:12.4409588Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-05-23T10:08:12.4410406Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-05-23T10:08:12.4411234Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-05-23T10:08:12.4412084Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-05-23T10:08:12.4412930Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-05-23T10:08:12.4413733Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-05-23T10:08:12.4414435Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-05-23T10:08:12.4415214Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-05-23T10:08:12.4416039Z May 23 10:08:12 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-05-23T10:08:12.4416826Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-05-23T10:08:12.4417633Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-05-23T10:08:12.4418426Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2022-05-23T10:08:12.4419181Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-05-23T10:08:12.4419878Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-05-23T10:08:12.4420538Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-05-23T10:08:12.4421196Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-05-23T10:08:12.4421667Z May 23 10:08:12 
2022-05-23T10:08:12.8012357Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8014745Z May 23 10:08:12 [INFO] Results:
2022-05-23T10:08:12.8015399Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8015938Z May 23 10:08:12 [ERROR] Failures: 
2022-05-23T10:08:12.8016703Z May 23 10:08:12 [ERROR]   CheckPubSubEmulatorTest.testPull:78 expected:<1> but was:<0>
2022-05-23T10:08:12.8017540Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8018140Z May 23 10:08:12 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35954&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
"	FLINK	Closed	3	1	5059	stale-assigned, test-stability
13405842	Python installdeps hangs	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24922&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23587

{code}
Oct 10 02:30:01 py38-cython create: /__w/1/s/flink-python/.tox/py38-cython
Oct 10 02:30:04 py38-cython installdeps: pytest, apache-beam==2.27.0, cython==0.29.16, grpcio>=1.29.0,<2, grpcio-tools>=1.3.5,<=1.14.2, apache-flink-libraries
Oct 10 02:45:22 ==============================================================================
Oct 10 02:45:22 Process produced no output for 900 seconds.
Oct 10 02:45:22 ==============================================================================
{code}"	FLINK	Resolved	2	1	5059	pull-request-available, test-stability
13371090	PyFlinkStreamUserDefinedFunctionTests.test_udf_in_join_condition_2 fail due to NPE	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16326&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb&l=21130

"	FLINK	Resolved	1	1	5059	pull-request-available, test-stability
13347673	PythonCalcSplitConditionRule is not working as expected	"Currently if users write such a SQL:

`SELECT pyFunc5(f0, f1) FROM (SELECT e.f0, e.f1 FROM (SELECT pyFunc5(a) as e FROM MyTable) where e.f0 is NULL)`

It will be optimized to:

`FlinkLogicalCalc(select=[pyFunc5(pyFunc5(a)) AS f0])
+- FlinkLogicalCalc(select=[a], where=[IS NULL(pyFunc5(a).f0)])
 +- FlinkLogicalLegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, d)]]], fields=[a, b, c, d])`

The optimized plan is not runnable, we need to fix this."	FLINK	Closed	3	1	5059	pull-request-available
13448986	PyFlink installation failure on Windows OS	Because pemja doesn't support windows os, it makes installation failed in windows os in release-1.15. We need to fix it asap.	FLINK	Resolved	2	1	5059	pull-request-available
13410223	py37-cython failed on AZP	"{{py37-cython}} failed on AZP with the following exception:

{code}
Nov 05 02:43:55 Exception in thread ""flink-shutdown-hook-1"" java.lang.NoClassDefFoundError: akka/actor/CoordinatedShutdown$JvmExitReason$
Nov 05 02:43:55 	at akka.actor.CoordinatedShutdown$.$anonfun$initJvmHook$1(CoordinatedShutdown.scala:272)
Nov 05 02:43:55 	at akka.actor.CoordinatedShutdown$$anon$3.run(CoordinatedShutdown.scala:814)
Nov 05 02:43:55 Caused by: java.lang.ClassNotFoundException: akka.actor.CoordinatedShutdown$JvmExitReason$
Nov 05 02:43:55 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
Nov 05 02:43:55 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
Nov 05 02:43:55 	at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:125)
Nov 05 02:43:55 	at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:104)
Nov 05 02:43:55 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
Nov 05 02:43:55 	... 2 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25983&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22560"	FLINK	Closed	3	1	5059	stale-assigned, test-stability
13382038	Support to execute PyFlink jobs in YARN application mode	"for now pyflink is not support hadoop yarn application mode, cause of yarn nodemanager may not have suitable python version

after test for use venv(python virtual environment) that uploaded by 'python.files' properties, then change 'env.pythonExec' path, it also works

so,is there any possiable to support this in a suitable way

 

 "	FLINK	Resolved	3	2	5059	auto-unassigned, pull-request-available
13341196	Add check of unsupported result type in Pandas UDAF	Currently the return type of Pandas UDAF should be a primitive data type, and the returned scalar can be either a python primitive type, e.g., {{int}} or {{float}} or a numpy data type, e.g., {{numpy.int64}} or {{numpy.float64}}. {{Any}} should ideally be a specific scalar type accordingly. We will add related DataType check and throw a more readable exception for unsupported DataTypes. What's more, we will add related notes in docs.	FLINK	Closed	3	4	5059	pull-request-available
13419660	Python tests hangs on install dependencies	"{code:java}
Dec 25 04:35:54 py38-cython create: /__w/1/s/flink-python/.tox/py38-cython
Dec 25 04:35:58 py38-cython installdeps: -rdev/dev-requirements.txt, pytest, apache-flink-libraries
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 Process produced no output for 900 seconds.
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 The following Java processes are running (JPS)
Dec 25 04:51:00 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Dec 25 04:51:00 137834 Jps
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 Printing stack trace of Java process 137834
Dec 25 04:51:00 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
137834: No such process
Dec 25 04:51:00 Killing process with pid=725 and all descendants
./flink-python/dev/lint-python.sh: line 580:  2770 Terminated              $TOX_PATH -c $FLINK_PYTHON_DIR/tox.ini --recreate 2>&1
      2771                       | tee -a $LOG_FILE
/__w/1/s/tools/ci/watchdog.sh: line 113:   725 Terminated              $cmd
Dec 25 04:51:00 Process exited with EXIT CODE: 143.
Dec 25 04:51:00 Trying to KILL watchdog (720).
Dec 25 04:51:00 Searching for .dump, .dumpstream and related files in '/__w/1/s'
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '143'.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23407"	FLINK	Resolved	2	1	5059	pull-request-available, test-stability
13355494	pyarrow exception when using window with pandas udaf	"I write a pyflink demo and execute in local environment, the logic is simple:generate records and aggerate in 100s tumle window, using a pandas udaf.
But the job failed after several minutes, I don't think it's a resource problem because the amount of data is small, here is the error trace.

{code:java}
Caused by: org.apache.flink.streaming.runtime.tasks.AsynchronousException: Caught exception while processing timer.
	at org.apache.flink.streaming.runtime.tasks.StreamTask$StreamTaskAsyncExceptionHandler.handleAsyncException(StreamTask.java:1108)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.handleAsyncException(StreamTask.java:1082)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1213)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$17(StreamTask.java:1202)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:302)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:184)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
	at java.lang.Thread.run(Thread.java:748)
Caused by: TimerException{java.lang.RuntimeException: Failed to close remote bundle}
	... 11 more
Caused by: java.lang.RuntimeException: Failed to close remote bundle
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:371)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:325)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:291)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByTime(AbstractPythonFunctionOperator.java:285)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$open$0(AbstractPythonFunctionOperator.java:134)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1211)
	... 10 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 3: Traceback (most recent call last):
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 253, in _execute
    response = task()
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 310, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 480, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 515, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 978, in process_bundle
    element.data)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 218, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 330, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 332, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""apache_beam/runners/worker/operations.py"", line 292, in apache_beam.runners.worker.operations.Operation.process
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py"", line 73, in process
    for value in o.value:
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 625, in decode_from_stream
    yield self._decode_one_batch_from_stream(in_stream, in_stream.read_var_int64())
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 636, in _decode_one_batch_from_stream
    return arrow_to_pandas(self._timezone, self._field_types, [next(self._batch_reader)])
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 629, in _load_from_stream
    reader = pa.ipc.open_stream(stream)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 146, in open_stream
    return RecordBatchStreamReader(source)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 62, in __init__
    self._open(source)
  File ""pyarrow/ipc.pxi"", line 360, in pyarrow.lib._RecordBatchStreamReader._open
  File ""pyarrow/error.pxi"", line 123, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
OSError: Expected IPC message of type schema but got record batch

	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:57)
	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:458)
	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:547)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:369)
	... 15 more
Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 3: Traceback (most recent call last):
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 253, in _execute
    response = task()
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 310, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 480, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 515, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 978, in process_bundle
    element.data)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 218, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 330, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 332, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""apache_beam/runners/worker/operations.py"", line 292, in apache_beam.runners.worker.operations.Operation.process
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py"", line 73, in process
    for value in o.value:
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 625, in decode_from_stream
    yield self._decode_one_batch_from_stream(in_stream, in_stream.read_var_int64())
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 636, in _decode_one_batch_from_stream
    return arrow_to_pandas(self._timezone, self._field_types, [next(self._batch_reader)])
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 629, in _load_from_stream
    reader = pa.ipc.open_stream(stream)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 146, in open_stream
    return RecordBatchStreamReader(source)
  File ""/Users/lyf/Library/Python/3.7/lib/python/site-packages/pyarrow/ipc.py"", line 62, in __init__
    self._open(source)
  File ""pyarrow/ipc.pxi"", line 360, in pyarrow.lib._RecordBatchStreamReader._open
  File ""pyarrow/error.pxi"", line 123, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
OSError: Expected IPC message of type schema but got record batch

	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:177)
	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:157)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
{code}

And my test code:

{code:python}
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import *
from pyflink.table.udf import udaf, AggregateFunction
from pyflink.table.window import Tumble


class MyTestAggregateFunction(AggregateFunction):

    def get_value(self, accumulator):
        return accumulator[0]

    def create_accumulator(self):
        return Row(0)

    def accumulate(self, accumulator, *args):
        accumulator[0] = len(args[0])

    def get_result_type(self):
        return DataTypes.BIGINT()


if __name__ == '__main__':
    env = StreamExecutionEnvironment.get_execution_environment()
    f_s_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()
    t_env = StreamTableEnvironment.create(env, None, f_s_settings)

    my_udaf = udaf(MyTestAggregateFunction(), func_type=""pandas"")
    t_env.register_function('my_udaf', my_udaf)
    t_env.sql_update(""""""
    CREATE TABLE `source_table` (
        `header` STRING,
        ts AS PROCTIME()
    ) WITH (
          'connector' = 'datagen',
          'rows-per-second' = '100'
    )
    """""")
    t_env.sql_update(""""""
    CREATE TABLE `sink_table` (
        `content` BIGINT,
        `wstart` TIMESTAMP(3)
    ) WITH (
        'connector' = 'print'
    )
    """""")
    t_env.scan(""source_table"") \
        .window(Tumble.over(""100.second"").on(""ts"").alias(""w"")) \
        .group_by('w') \
        .select(""my_udaf(header), w.start"")\
        .insert_into(""sink_table"")
    t_env.execute(""test_job"")

{code}

"	FLINK	Resolved	3	1	5059	pull-request-available
13330261	"PyFlink Table end-to-end test failed with ""FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs'"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7130&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-09-30T17:13:14.7489481Z Collecting package metadata (current_repodata.json): ...working... failed
2020-09-30T17:13:14.7699351Z 
2020-09-30T17:13:14.7699995Z # >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<
2020-09-30T17:13:14.7700398Z 
2020-09-30T17:13:14.7700782Z     Traceback (most recent call last):
2020-09-30T17:13:14.7702095Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py"", line 107, in touch
2020-09-30T17:13:14.7702736Z         mkdir_p_sudo_safe(dirpath)
2020-09-30T17:13:14.7703608Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py"", line 84, in mkdir_p_sudo_safe
2020-09-30T17:13:14.7704221Z         os.mkdir(path)
2020-09-30T17:13:14.7704992Z     FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs'
2020-09-30T17:13:14.7705512Z     
2020-09-30T17:13:14.7705956Z     During handling of the above exception, another exception occurred:
2020-09-30T17:13:14.7706402Z     
2020-09-30T17:13:14.7706789Z     Traceback (most recent call last):
2020-09-30T17:13:14.7707615Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 185, in _load
2020-09-30T17:13:14.7708341Z         mtime = getmtime(self.cache_path_json)
2020-09-30T17:13:14.7709527Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 153, in cache_path_json
2020-09-30T17:13:14.7710340Z         return self.cache_path_base + '.json'
2020-09-30T17:13:14.7711227Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 144, in cache_path_base
2020-09-30T17:13:14.7711832Z         create_cache_dir(),
2020-09-30T17:13:14.7712821Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 645, in create_cache_dir
2020-09-30T17:13:14.7715308Z         cache_dir = join(PackageCacheData.first_writable(context.pkgs_dirs).pkgs_dir, 'cache')
2020-09-30T17:13:14.7715986Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/package_cache_data.py"", line 162, in first_writable
2020-09-30T17:13:14.7716407Z         created = create_package_cache_directory(package_cache.pkgs_dir)
2020-09-30T17:13:14.7717084Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/create.py"", line 435, in create_package_cache_directory
2020-09-30T17:13:14.7717522Z         touch(join(pkgs_dir, PACKAGE_CACHE_MAGIC_FILE), mkdir=True, sudo_safe=sudo_safe)
2020-09-30T17:13:14.7718150Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/update.py"", line 125, in touch
2020-09-30T17:13:14.7718694Z         raise NotWritableError(path, e.errno, caused_by=e)
2020-09-30T17:13:14.7719040Z     conda.exceptions.NotWritableError: The current user does not have write permissions to a required path.
2020-09-30T17:13:14.7719797Z       path: /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt
2020-09-30T17:13:14.7720054Z       uid: 1001
2020-09-30T17:13:14.7720217Z       gid: 118
2020-09-30T17:13:14.7720375Z     
2020-09-30T17:13:14.7720625Z     If you feel that permissions on this path are set incorrectly, you can manually
2020-09-30T17:13:14.7720898Z     change them by executing
2020-09-30T17:13:14.7721072Z     
2020-09-30T17:13:14.7721513Z       $ sudo chown 1001:118 /home/vsts/work/1/s/flink-python/dev/.conda/pkgs/urls.txt
2020-09-30T17:13:14.7721778Z     
2020-09-30T17:13:14.7722334Z     In general, it's not advisable to use 'sudo conda'.
2020-09-30T17:13:14.7722539Z     
2020-09-30T17:13:14.7722680Z     
2020-09-30T17:13:14.7722946Z     During handling of the above exception, another exception occurred:
2020-09-30T17:13:14.7723200Z     
2020-09-30T17:13:14.7723407Z     Traceback (most recent call last):
2020-09-30T17:13:14.7724000Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/exceptions.py"", line 1062, in __call__
2020-09-30T17:13:14.7724516Z         return func(*args, **kwargs)
2020-09-30T17:13:14.7725075Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main.py"", line 84, in _main
2020-09-30T17:13:14.7725408Z         exit_code = do_call(args, p)
2020-09-30T17:13:14.7725983Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/conda_argparse.py"", line 82, in do_call
2020-09-30T17:13:14.7726556Z         exit_code = getattr(module, func_name)(args, parser)
2020-09-30T17:13:14.7727157Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/main_install.py"", line 20, in execute
2020-09-30T17:13:14.7727688Z         install(args, parser, 'install')
2020-09-30T17:13:14.7728274Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/cli/install.py"", line 256, in install
2020-09-30T17:13:14.7728663Z         force_reinstall=context.force_reinstall or context.force,
2020-09-30T17:13:14.7729601Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 112, in solve_for_transaction
2020-09-30T17:13:14.7729964Z         force_remove, force_reinstall)
2020-09-30T17:13:14.7730791Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 150, in solve_for_diff
2020-09-30T17:13:14.7731132Z         force_remove)
2020-09-30T17:13:14.7731878Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 249, in solve_final_state
2020-09-30T17:13:14.7732267Z         ssc = self._collect_all_metadata(ssc)
2020-09-30T17:13:14.7732844Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
2020-09-30T17:13:14.7733199Z         return f(*args, **kwds)
2020-09-30T17:13:14.7733780Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 389, in _collect_all_metadata
2020-09-30T17:13:14.7734177Z         index, r = self._prepare(prepared_specs)
2020-09-30T17:13:14.7734922Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/solve.py"", line 974, in _prepare
2020-09-30T17:13:14.7735307Z         self.subdirs, prepared_specs, self._repodata_fn)
2020-09-30T17:13:14.7736067Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/index.py"", line 214, in get_reduced_index
2020-09-30T17:13:14.7736435Z         repodata_fn=repodata_fn)
2020-09-30T17:13:14.7737001Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 91, in query_all
2020-09-30T17:13:14.7737494Z         result = tuple(concat(executor.map(subdir_query, channel_urls)))
2020-09-30T17:13:14.7738148Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py"", line 586, in result_iterator
2020-09-30T17:13:14.7738507Z         yield fs.pop().result()
2020-09-30T17:13:14.7739046Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py"", line 425, in result
2020-09-30T17:13:14.7739578Z         return self.__get_result()
2020-09-30T17:13:14.7740341Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/_base.py"", line 384, in __get_result
2020-09-30T17:13:14.7740685Z         raise self._exception
2020-09-30T17:13:14.7741217Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/concurrent/futures/thread.py"", line 57, in run
2020-09-30T17:13:14.7741767Z         result = self.fn(*self.args, **self.kwargs)
2020-09-30T17:13:14.7742581Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 87, in <lambda>
2020-09-30T17:13:14.7743099Z         package_ref_or_match_spec))
2020-09-30T17:13:14.7743680Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 96, in query
2020-09-30T17:13:14.7744019Z         self.load()
2020-09-30T17:13:14.7744563Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 160, in load
2020-09-30T17:13:14.7744932Z         _internal_state = self._load()
2020-09-30T17:13:14.7745837Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 188, in _load
2020-09-30T17:13:14.7746162Z         self.cache_path_json)
2020-09-30T17:13:14.7746724Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 153, in cache_path_json
2020-09-30T17:13:14.7747244Z         return self.cache_path_base + '.json'
2020-09-30T17:13:14.7748013Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 144, in cache_path_base
2020-09-30T17:13:14.7748381Z         create_cache_dir(),
2020-09-30T17:13:14.7749159Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/core/subdir_data.py"", line 646, in create_cache_dir
2020-09-30T17:13:14.7749537Z         mkdir_p_sudo_safe(cache_dir)
2020-09-30T17:13:14.7750896Z       File ""/home/vsts/work/1/s/flink-python/dev/.conda/lib/python3.7/site-packages/conda/gateways/disk/__init__.py"", line 84, in mkdir_p_sudo_safe
2020-09-30T17:13:14.7751274Z         os.mkdir(path)
2020-09-30T17:13:14.7751795Z     FileExistsError: [Errno 17] File exists: '/home/vsts/work/1/s/flink-python/dev/.conda/pkgs/cache'
{code}"	FLINK	Closed	2	1	5059	pull-request-available, test-stability
13336243	Make fast_operation and slow_operation produce functions consistent	The function generated by slow_operation uses the characteristics of python syntax. In order to better reconstruct the python operation, we need to keep the functions generated by fast_operation and slow_operation consistent.	FLINK	Closed	3	4	5059	pull-request-available
13328023	Introduce BatchArrowPythonGroupWindowAggregateFunctionOperator	Introduce BatchArrowPythonGroupWindowAggregateFunctionOperator to support Pandas Batch Group Window Aggregation	FLINK	Closed	3	7	5059	pull-request-available
13315070	Extract the implementation logic of Beam in Operations	Extract the implementation logic of Beam in Operations, so that the implementation of general operations and Beam operations can be decoupled	FLINK	Closed	3	4	5059	pull-request-available
13262093	Support all the data types in Python user-defined functions	Currently, only BigInt type is supported and we should support the other types as well. We will support all these types in some seperate child-jiras.	FLINK	Closed	3	7	5059	pull-request-available
13326012	Python UDF supports directly specifying input_types as DataTypes.ROW	Python UDF supports input_types=DataTypes.ROW 	FLINK	Closed	3	1	5059	pull-request-available
13469672	Python Bash e2e tests don't clean-up after they've ran, causing disk space issues	"The Bash based E2E tests that are used in Python aren't cleaned-up after they've ran. These cause disk space issues further downstream.

See the CI run from https://github.com/apache/flink/pull/20114 for results, for example:

-- When starting with the Bash e2e tests
{code:java}
08:47:10 ##[group]Top 15 biggest directories in terms of used disk space
Jul 01 08:47:12 3983560	.
Jul 01 08:47:12 1266692	./flink-end-to-end-tests
Jul 01 08:47:12 624568	./flink-dist
Jul 01 08:47:12 624180	./flink-dist/target
Jul 01 08:47:12 500076	./flink-dist/target/flink-1.16-SNAPSHOT-bin
Jul 01 08:47:12 500072	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
Jul 01 08:47:12 460812	./flink-connectors
Jul 01 08:47:12 392588	./.git
Jul 01 08:47:12 366396	./.git/objects
Jul 01 08:47:12 366388	./.git/objects/pack
Jul 01 08:47:12 349272	./flink-table
Jul 01 08:47:12 335592	./.git/objects/pack/pack-38d46915823ebec2bc660fd160e5cfca5bc3e567.pack
Jul 01 08:47:12 293044	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/opt
Jul 01 08:47:12 251272	./flink-filesystems
Jul 01 08:47:12 246596	./flink-end-to-end-tests/flink-streaming-kinesis-test
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d&l=664

-- After completing all Bash bashed e2e tests:
{code:java}
2022-07-01T10:20:17.3594718Z Jul 01 10:20:17 ##[group]Top 15 biggest directories in terms of used disk space
2022-07-01T10:20:18.7520631Z Jul 01 10:20:18 5425892	.
2022-07-01T10:20:18.7521823Z Jul 01 10:20:18 1521472	./flink-end-to-end-tests
2022-07-01T10:20:18.7522566Z Jul 01 10:20:18 1242528	./flink-python
2022-07-01T10:20:18.7523244Z Jul 01 10:20:18 952336	./flink-python/dev
2022-07-01T10:20:18.7524159Z Jul 01 10:20:18 878764	./flink-python/dev/.conda
2022-07-01T10:20:18.7524870Z Jul 01 10:20:18 834200	./flink-python/dev/.conda/lib
2022-07-01T10:20:18.7525619Z Jul 01 10:20:18 726528	./flink-python/dev/.conda/lib/python3.7
2022-07-01T10:20:18.7526397Z Jul 01 10:20:18 683256	./flink-python/dev/.conda/lib/python3.7/site-packages
2022-07-01T10:20:18.7527101Z Jul 01 10:20:18 624568	./flink-dist
2022-07-01T10:20:18.7527768Z Jul 01 10:20:18 624180	./flink-dist/target
2022-07-01T10:20:18.7528494Z Jul 01 10:20:18 500076	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-07-01T10:20:18.7529298Z Jul 01 10:20:18 500072	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-07-01T10:20:18.7530046Z Jul 01 10:20:18 460812	./flink-connectors
2022-07-01T10:20:18.7530546Z Jul 01 10:20:18 392588	./.git
2022-07-01T10:20:18.7531014Z Jul 01 10:20:18 366396	./.git/objects
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d&l=9631"	FLINK	Resolved	2	1	5059	pull-request-available
13297305	Update pyarrow version bounds less than 0.14.0	"We need to update pyarrow version bounds less than 0.14.0 in PyFlink 1.10. The bug[1] comes from the dependency of beam 2.15 which has been resolved in beam 2.17.

 [1] https://issues.apache.org/jira/browse/BEAM-8368"	FLINK	Closed	1	1	5059	pull-request-available
13346291	Port BatchExecPythonCalc and StreamExecPythonCalc to Java	"https://issues.apache.org/jira/browse/FLINK-20610 will separate the implementation of BatchExecCalc and StreamExecCalc, and port BatchExecCalc and StreamExecCalc to Java. 
{{StreamExecPythonCalc}} extends from {{CommonExecPythonCalc}} and {{CommonExecPythonCalc}} extends from {{CommonPythonBase}}, they are all Scala classes, and involves a lot of code. Java class can't extend Scala interface with default implementation. So I create an issue separately to port them to Java."	FLINK	Closed	3	7	5059	pull-request-available
13483752	ProcessDataStreamStreamingTests.test_process_function unstable	"
{code:java}
2022-09-29T02:10:45.3571648Z Sep 29 02:10:45 self = <pyflink.datastream.tests.test_data_stream.ProcessDataStreamStreamingTests testMethod=test_process_function>
2022-09-29T02:10:45.3572279Z Sep 29 02:10:45 
2022-09-29T02:10:45.3572810Z Sep 29 02:10:45     def test_process_function(self):
2022-09-29T02:10:45.3573495Z Sep 29 02:10:45         self.env.set_parallelism(1)
2022-09-29T02:10:45.3574148Z Sep 29 02:10:45         self.env.get_config().set_auto_watermark_interval(2000)
2022-09-29T02:10:45.3580634Z Sep 29 02:10:45         self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
2022-09-29T02:10:45.3583194Z Sep 29 02:10:45         data_stream = self.env.from_collection([(1, '1603708211000'),
2022-09-29T02:10:45.3584515Z Sep 29 02:10:45                                                 (2, '1603708224000'),
2022-09-29T02:10:45.3585957Z Sep 29 02:10:45                                                 (3, '1603708226000'),
2022-09-29T02:10:45.3587132Z Sep 29 02:10:45                                                 (4, '1603708289000')],
2022-09-29T02:10:45.3588094Z Sep 29 02:10:45                                                type_info=Types.ROW([Types.INT(), Types.STRING()]))
2022-09-29T02:10:45.3589090Z Sep 29 02:10:45     
2022-09-29T02:10:45.3589949Z Sep 29 02:10:45         class MyProcessFunction(ProcessFunction):
2022-09-29T02:10:45.3590710Z Sep 29 02:10:45     
2022-09-29T02:10:45.3591856Z Sep 29 02:10:45             def process_element(self, value, ctx):
2022-09-29T02:10:45.3592873Z Sep 29 02:10:45                 current_timestamp = ctx.timestamp()
2022-09-29T02:10:45.3593862Z Sep 29 02:10:45                 current_watermark = ctx.timer_service().current_watermark()
2022-09-29T02:10:45.3594915Z Sep 29 02:10:45                 yield ""current timestamp: {}, current watermark: {}, current_value: {}""\
2022-09-29T02:10:45.3596201Z Sep 29 02:10:45                     .format(str(current_timestamp), str(current_watermark), str(value))
2022-09-29T02:10:45.3597089Z Sep 29 02:10:45     
2022-09-29T02:10:45.3597942Z Sep 29 02:10:45         watermark_strategy = WatermarkStrategy.for_monotonous_timestamps()\
2022-09-29T02:10:45.3599260Z Sep 29 02:10:45             .with_timestamp_assigner(SecondColumnTimestampAssigner())
2022-09-29T02:10:45.3600611Z Sep 29 02:10:45         data_stream.assign_timestamps_and_watermarks(watermark_strategy)\
2022-09-29T02:10:45.3601877Z Sep 29 02:10:45             .process(MyProcessFunction(), output_type=Types.STRING()).add_sink(self.test_sink)
2022-09-29T02:10:45.3603527Z Sep 29 02:10:45         self.env.execute('test process function')
2022-09-29T02:10:45.3604445Z Sep 29 02:10:45         results = self.test_sink.get_results()
2022-09-29T02:10:45.3605684Z Sep 29 02:10:45         expected = [""current timestamp: 1603708211000, current watermark: ""
2022-09-29T02:10:45.3607157Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=1, f1='1603708211000')"",
2022-09-29T02:10:45.3608256Z Sep 29 02:10:45                     ""current timestamp: 1603708224000, current watermark: ""
2022-09-29T02:10:45.3609650Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=2, f1='1603708224000')"",
2022-09-29T02:10:45.3610854Z Sep 29 02:10:45                     ""current timestamp: 1603708226000, current watermark: ""
2022-09-29T02:10:45.3612279Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=3, f1='1603708226000')"",
2022-09-29T02:10:45.3613382Z Sep 29 02:10:45                     ""current timestamp: 1603708289000, current watermark: ""
2022-09-29T02:10:45.3615683Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=4, f1='1603708289000')""]
2022-09-29T02:10:45.3617687Z Sep 29 02:10:45 >       self.assert_equals_sorted(expected, results)
2022-09-29T02:10:45.3618620Z Sep 29 02:10:45 
2022-09-29T02:10:45.3619425Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:986: 
2022-09-29T02:10:45.3620424Z Sep 29 02:10:45 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-09-29T02:10:45.3621886Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:66: in assert_equals_sorted
2022-09-29T02:10:45.3622847Z Sep 29 02:10:45     self.assertEqual(expected, actual)
2022-09-29T02:10:45.3624658Z Sep 29 02:10:45 E   AssertionError: Lists differ: [""cur[414 chars]ark: -9223372036854775808, current_value: Row([22 chars]0')""] != [""cur[414 chars]ark: 1603708225999, current_value: Row(f0=4, f[15 chars]0')""]
2022-09-29T02:10:45.3625881Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3626591Z Sep 29 02:10:45 E   First differing element 3:
2022-09-29T02:10:45.3627726Z Sep 29 02:10:45 E   ""curr[44 chars]ark: -9223372036854775808, current_value: Row([21 chars]00')""
2022-09-29T02:10:45.3628758Z Sep 29 02:10:45 E   ""curr[44 chars]ark: 1603708225999, current_value: Row(f0=4, f[14 chars]00')""
2022-09-29T02:10:45.3629276Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3629842Z Sep 29 02:10:45 E   Diff is 753 characters long. Set self.maxDiff to None to see it.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41436&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901"	FLINK	Closed	2	1	5059	pull-request-available, test-stability
13213879	Support multiple languages for the framework of flink docs	"A more detailed description can be found in the proposed doc: https://docs.google.com/document/d/1R1-uDq-KawLB8afQYrczfcoQHjjIhq6tvUksxrfhBl0/edit#

This step aims to integrate the mulitple-language-plugin for flink docs to support Chinese. All the $pagename.zh.md should be created first in this JIRA but keep the original English contents. A link between English version and Chinese version should also be considered.

"	FLINK	Closed	3	7	5351	pull-request-available
13218671	Support Code Generation for RexNode	Introduce {{ExprCodeGenerator}} to support generate codes from RexNode.	FLINK	Closed	3	2	5351	pull-request-available
13380454	KafkaChangelogTableITCase.testKafkaDebeziumChangelogSource fail due to ConcurrentModificationException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18330&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6608

"	FLINK	Closed	3	1	5351	test-stability
13329522	Fix ArrayIndexOutOfBoundsException when executing DELETE statement in JDBC upsert sink	"We found  that the primary key position can cause  ArrayIndexOutOfBoundsException

the sink like that( the primary key select the position of 1, 3):
{code:java}
CREATE TABLE `test`(
  col1 STRING, 
  col2 STRING, 
  col3 STRING, 
  PRIMARY KEY (col1, col3) NOT ENFORCED ) WITH (
  'connector' = 'jdbc',
  ...
){code}
when the DELETE (cdc message) come , it will raise ArrayIndexOutOfBoundsException:
{code:java}
Caused by: java.lang.RuntimeException: Writing records to JDBC failed.    ... 10 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: 2    at org.apache.flink.table.data.GenericRowData.getString(GenericRowData.java:169)    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$245ca7d1$1(RowData.java:310)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.getPrimaryKey(JdbcDynamicOutputFormatBuilder.java:216)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.lambda$createRowKeyExtractor$7(JdbcDynamicOutputFormatBuilder.java:193)    at org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormatBuilder.lambda$createKeyedRowExecutor$3fd497bb$1(JdbcDynamicOutputFormatBuilder.java:128)    at org.apache.flink.connector.jdbc.internal.executor.KeyedBatchStatementExecutor.executeBatch(KeyedBatchStatementExecutor.java:71)    at org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor.executeBatch(BufferReduceStatementExecutor.java:99)    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:200)    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:171)    ... 8 more
{code}
 "	FLINK	Closed	2	1	5351	pull-request-available
13229349	Bump Calcite dependency to 1.19.0 in blink planner	Bump Calcite dependency to 1.19.0 in {{flink-table-planner-blink}} module.	FLINK	Closed	3	2	5351	pull-request-available
13273411	Introduce TypeTransformation interface and basic transformations	"Currently, the default DataType derived from properties is the default conversion now. However, some connectors/formats are still using sql Timestamp. But bridging DataType into sql Timestamp  is not simple, because DataType is nested. So we propose to introduce a TypeTransformation which transform one data type to another, this is also a useful tool for FLIP-65. 

The proposal including:
- Remove {{CallContext}} from the exiting {{TypeTransformation}}.
- add a package {{o.a.f.table.types.inference.transforms}}
- add commonly used transform classes there, e.g. {{timeToSqlTypes}}
- add a class `o.a.f.table.types.inference.TypeTransforms` for listing all available transforms"	FLINK	Resolved	3	4	5351	pull-request-available
13293298	Fix cast exception when having time point literal as parameters	"I defined as ScalarFunction as follow:

 
{code:java}
public class DateFunc extends ScalarFunction {


    public String eval(Date date) {
      return date.toString();
    }

    @Override
    public TypeInformation<?> getResultType(Class<?>[] signature) {
        return Types.STRING;
    }

   @Override
   public TypeInformation<?>[] getParameterTypes(Class<?>[] signature) {
      return new TypeInformation[]{Types.INT};
   }
}
{code}
I ues it in sql: `select func(DATE '2020-11-12') as a from source` , Flink throws 'cannot cast 2020-11-12 as class java.time.LocalDate '

 

The full code is in the [^Flinktest.zip] Main class is com.lorinda.template.TestDateFunction"	FLINK	Resolved	3	1	5351	pull-request-available
13279010	Add documentation for INSERT statements for Flink SQL	"We missed to add documentation for INSERT statements which should be added under ""SQL"" page. "	FLINK	Resolved	3	4	5351	pull-request-available
13490592	CatalogPropertiesUtil supports de/serializing column comment	We should consider adding {{schema.${i}.comment}} to {{CatalogPropertiesUtil, in order to support comment's de/serialization.}}	FLINK	Closed	3	7	5351	pull-request-available
13288307	Improve exception message when reading an unbounded source in batch mode	"We can just ignore watermark in batch mode. 

cc [~jark]"	FLINK	Resolved	3	4	5351	pull-request-available
13232568	Fix broken links in documentation to make CRON travis job work	"The CRON travis job is failing because of documentation link checks. 

https://travis-ci.org/apache/flink/jobs/530213609

Following are the broken links:


{code:java}
[2019-05-09 14:05:44] ERROR `/zh/dev/stream/side_output.html' not found.
[2019-05-09 14:05:45] ERROR `/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.8.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.7.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.6.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/release-notes/flink-1.5.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/levels_of_abstraction.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/table_api.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/program_dataflow.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/parallel_dataflow.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/windows.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/event_ingestion_processing_time.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/state_partitioning.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/tasks_chains.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/processes.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/tasks_slots.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/slot_sharing.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/fig/checkpoints.svg' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/linking_with_flink.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/linking.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/apis/streaming/event_timestamps_watermarks.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/apis/streaming/event_timestamp_extractors.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/apis/streaming/event_time.html' not found.
[2019-05-09 14:05:48] ERROR `/zh/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-09 14:05:49] ERROR `/zh/fig/checkpoint_tuning.svg' not found.
[2019-05-09 14:05:49] ERROR `/zh/fig/local_recovery.png' not found.
{code}
"	FLINK	Closed	3	1	5351	pull-request-available
13306250	PostgresCatalogITCase . testGroupByInsert() fails on CI	"{{org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase . testGroupByInsert}}


Error:
{code}
2020-05-20T16:36:33.9647037Z org.apache.flink.table.api.ValidationException: 
2020-05-20T16:36:33.9647354Z Field types of query result and registered TableSink mypg.postgres.primitive_table2 do not match.

2020-05-20T16:36:33.9648233Z Query schema: [int: INT NOT NULL, EXPR$1: VARBINARY(2147483647) NOT NULL, short: SMALLINT NOT NULL, EXPR$3: BIGINT, EXPR$4: FLOAT, EXPR$5: DOUBLE, EXPR$6: DECIMAL(10, 5), EXPR$7: BOOLEAN, EXPR$8: VARCHAR(2147483647), EXPR$9: CHAR(1) NOT NULL, EXPR$10: CHAR(1) NOT NULL, EXPR$11: VARCHAR(20), EXPR$12: TIMESTAMP(5), EXPR$13: DATE, EXPR$14: TIME(0), EXPR$15: DECIMAL(38, 18)]

2020-05-20T16:36:33.9650272Z Sink schema: [int: INT, bytea: VARBINARY(2147483647), short: SMALLINT, long: BIGINT, real: FLOAT, double_precision: DOUBLE, numeric: DECIMAL(10, 5), decimal: DECIMAL(10, 1), boolean: BOOLEAN, text: VARCHAR(2147483647), char: CHAR(1), character: CHAR(3), character_varying: VARCHAR(20), timestamp: TIMESTAMP(5), date: DATE, time: TIME(0), default_numeric: DECIMAL(38, 18)]

2020-05-20T16:36:33.9651218Z 	at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyImplicitCast(TableSinkUtils.scala:100)
2020-05-20T16:36:33.9651689Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:218)
2020-05-20T16:36:33.9652136Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:193)
2020-05-20T16:36:33.9652936Z 	at scala.Option.map(Option.scala:146)
2020-05-20T16:36:33.9653593Z 	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:193)
2020-05-20T16:36:33.9653993Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:152)
2020-05-20T16:36:33.9654428Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:152)
2020-05-20T16:36:33.9654841Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-05-20T16:36:33.9655221Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-05-20T16:36:33.9655759Z 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-05-20T16:36:33.9656072Z 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-05-20T16:36:33.9656413Z 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-05-20T16:36:33.9656890Z 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-05-20T16:36:33.9657211Z 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
2020-05-20T16:36:33.9657525Z 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2020-05-20T16:36:33.9657878Z 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:152)
2020-05-20T16:36:33.9658350Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1217)
2020-05-20T16:36:33.9658784Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:663)
2020-05-20T16:36:33.9659391Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:750)
2020-05-20T16:36:33.9659856Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:653)
2020-05-20T16:36:33.9660507Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil$.execInsertSqlAndWaitResult(TableEnvUtil.scala:27)
2020-05-20T16:36:33.9661115Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil.execInsertSqlAndWaitResult(TableEnvUtil.scala)
2020-05-20T16:36:33.9661583Z 	at org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.testGroupByInsert(PostgresCatalogITCase.java:88)
{code}

Full log: https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/25/logs/93"	FLINK	Closed	1	1	5351	pull-request-available, test-stability
13301765	Support CSV serialization and deseriazation schema for RowData type	Add support {{CsvRowDataDeserializationSchema}} and {{CsvRowDataSerializationSchema}} for the new data structure {{RowData}}.	FLINK	Closed	3	7	5351	pull-request-available
13370387	Add documentation for the new window TVF based operations	"In this 1.13 version, we have supported window TVF based aggregation and TopN of FLIP-145. We should add documentation for them. We may also need to restructure the ""Queries"" page.
"	FLINK	Closed	3	7	5351	pull-request-available
13379431	UpsertKafkaTableITCase.testAggregate fail due to ConcurrentModificationException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18151&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6613


{code:java}
2021-05-19T21:28:02.8689083Z May 19 21:28:02 [ERROR] testAggregate[format = avro](org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase)  Time elapsed: 2.067 s  <<< ERROR!
2021-05-19T21:28:02.8708337Z May 19 21:28:02 java.util.ConcurrentModificationException
2021-05-19T21:28:02.8710333Z May 19 21:28:02 	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
2021-05-19T21:28:02.8712083Z May 19 21:28:02 	at java.util.HashMap$ValueIterator.next(HashMap.java:1474)
2021-05-19T21:28:02.8712680Z May 19 21:28:02 	at java.util.AbstractCollection.toArray(AbstractCollection.java:141)
2021-05-19T21:28:02.8713142Z May 19 21:28:02 	at java.util.ArrayList.addAll(ArrayList.java:583)
2021-05-19T21:28:02.8716029Z May 19 21:28:02 	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.lambda$getResults$0(TestValuesRuntimeFunctions.java:114)
2021-05-19T21:28:02.8717007Z May 19 21:28:02 	at java.util.HashMap$Values.forEach(HashMap.java:981)
2021-05-19T21:28:02.8718041Z May 19 21:28:02 	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.getResults(TestValuesRuntimeFunctions.java:114)
2021-05-19T21:28:02.8719339Z May 19 21:28:02 	at org.apache.flink.table.planner.factories.TestValuesTableFactory.getResults(TestValuesTableFactory.java:184)
2021-05-19T21:28:02.8720309Z May 19 21:28:02 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.waitingExpectedResults(KafkaTableTestUtils.java:82)
2021-05-19T21:28:02.8721311Z May 19 21:28:02 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.wordFreqToUpsertKafka(UpsertKafkaTableITCase.java:440)
2021-05-19T21:28:02.8730402Z May 19 21:28:02 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.testAggregate(UpsertKafkaTableITCase.java:73)
2021-05-19T21:28:02.8731390Z May 19 21:28:02 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-19T21:28:02.8732095Z May 19 21:28:02 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-19T21:28:02.8732935Z May 19 21:28:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-19T21:28:02.8733726Z May 19 21:28:02 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-19T21:28:02.8734598Z May 19 21:28:02 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-19T21:28:02.8735450Z May 19 21:28:02 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-19T21:28:02.8736313Z May 19 21:28:02 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-19T21:28:02.8737329Z May 19 21:28:02 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-19T21:28:02.8738165Z May 19 21:28:02 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-19T21:28:02.8738989Z May 19 21:28:02 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-19T21:28:02.8739741Z May 19 21:28:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-19T21:28:02.8740563Z May 19 21:28:02 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-19T21:28:02.8741340Z May 19 21:28:02 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-19T21:28:02.8742077Z May 19 21:28:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-19T21:28:02.8742802Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-19T21:28:02.8743594Z May 19 21:28:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-19T21:28:02.8744811Z May 19 21:28:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-19T21:28:02.8745580Z May 19 21:28:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-19T21:28:02.8746330Z May 19 21:28:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-19T21:28:02.8747222Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-19T21:28:02.8748007Z May 19 21:28:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-19T21:28:02.8748791Z May 19 21:28:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-19T21:28:02.8749564Z May 19 21:28:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-19T21:28:02.8750251Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-05-19T21:28:02.8785419Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-05-19T21:28:02.8786427Z May 19 21:28:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-19T21:28:02.8787364Z May 19 21:28:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-19T21:28:02.8788096Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-19T21:28:02.8788855Z May 19 21:28:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-19T21:28:02.8789859Z May 19 21:28:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-19T21:28:02.8790792Z May 19 21:28:02 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2021-05-19T21:28:02.8791626Z May 19 21:28:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-19T21:28:02.8792399Z May 19 21:28:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-19T21:28:02.8793090Z May 19 21:28:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-19T21:28:02.8794167Z May 19 21:28:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-19T21:28:02.8794864Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-05-19T21:28:02.8795467Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-05-19T21:28:02.8796145Z May 19 21:28:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-19T21:28:02.8796795Z May 19 21:28:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-19T21:28:02.8797701Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-19T21:28:02.8798488Z May 19 21:28:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-19T21:28:02.8799258Z May 19 21:28:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-19T21:28:02.8799967Z May 19 21:28:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-19T21:28:02.8800706Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-05-19T21:28:02.8801594Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-05-19T21:28:02.8802426Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-05-19T21:28:02.8803344Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-05-19T21:28:02.8804358Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-05-19T21:28:02.8805238Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-05-19T21:28:02.8806153Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-19T21:28:02.8807081Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-19T21:28:02.8808226Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-19T21:28:02.8809039Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-19T21:28:02.8809548Z May 19 21:28:02 
{code}
"	FLINK	Closed	3	1	5351	test-stability
13339708	Fix can't generate plan when joining on changelog source without updates	INSTANCE: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9379&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=e25d5e7e-2a9c-5589-4940-0b638d75a414	FLINK	Closed	3	1	5351	pull-request-available, test-stability
13007621	Add possibility to get column names	"For debugging and maybe for visualization in future (e.g. in a shell) it would be good to have the possibilty to get the names of {{Table}} columns. At the moment the user has no idea how the table columns are named; if they need to be matched with POJO fields for example.

My suggestion:

{code}
Schema s = table.schema();
TypeInformation<?> type = s.getType(1);
TypeInformation<?> type = s.getType(""col"");
String s = s.getColumnName(1);
String[] s = s.getColumnNames();
{code}"	FLINK	Resolved	3	2	5351	starter
13298583	Refactor BaseRow to use RowKind instead of byte header	"This is a pre-step of FLINK-16996. We will refactor BaseRow#get/setHeader to get/setRowKind first. And update all the existing code to send ""insert/delete/update_before/update_after"" messages instead of ""+/-"" messages. This is possible now since FLINK-16887 is supported. "	FLINK	Closed	3	7	5351	pull-request-available
13296436	Refactor planner and connectors to use new data structures	"Refactors existing code to use the new data structures interfaces.

This issue might be split into smaller subtasks if necessary."	FLINK	Closed	3	7	5351	pull-request-available
13272974	types with precision can't be executed in sql client with blink planner	"I created a table in sql client with blink planner:  
{noformat}
create table t (
    a int,
    b varchar,
    c decimal(10, 5))
with (
    'connector.type' = 'filesystem',
    'format.type' = 'csv',
    'format.derive-schema' = 'true',
    'connector.path' = 'xxxxxxx'
);
{noformat}
The table description looks good:
{noformat}
Flink SQL> describe t; 
root 
  |-- a: INT 
  |-- b: STRING 
  |-- c: DECIMAL(10, 5){noformat}
But the select query failed:
{noformat}
Flink SQL> select * from t;
[ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(field$3,isNull$3,,DECIMAL(38, 18),None)] type is [DECIMAL(38, 18)], result type is [DECIMAL(10, 5)]
{noformat}
 "	FLINK	Resolved	3	1	5351	pull-request-available
13225682	Introduce unbounded streaming inner/left/right/full join operator	"This operator is responsible for unbounded streaming inner/left/right/full join, and will be optimized in following cases:
# If the join keys (with equality condition) are also primary key, we will have a more efficient state layout
# If the inputs have primary keys, but join keys are not primary key, we can also come up with an efficient state layout
# Inputs don't have primary keys, this will go to default implementation
"	FLINK	Closed	3	4	5351	pull-request-available
13262640	ValueLiteralExpression#equals should take array value into account	{{ValueLiteralExpression#equals}} uses {{Objects.equals}} to check the equality between two value object. Howeveer, the value object might be array object, using {{Object.equals}} will lead to wrong result. 	FLINK	Resolved	3	1	5351	pull-request-available
13243703	SplitAggregateITCase.testMinMaxWithRetraction failed on Travis	"{{SplitAggregateITCase.testMinMaxWithRetraction}} failed on Travis with

{code}
Failures: 
10:50:43.355 [ERROR]   SplitAggregateITCase.testMinMaxWithRetraction:195 expected:<List(2,2,2,1, 5,1,4,2, 6,2,2,1)> but was:<List(2,1,2,1, 5,1,4,2, 6,2,2,1)>
{code}

https://api.travis-ci.org/v3/job/554991853/log.txt"	FLINK	Resolved	1	1	5351	test-stability
13260687	Support to parse watermark statement in SQL DDL	"Support to parse watermark syntax in SQL DDL. This can implemented in {{flink-sql-parser}} module. 

The watermark syntax is as following:

{{WATERMARK FOR columnName AS <watermark_strategy_expression>}}

We should also do some validation during parsing, for example, whether the referenced rowtime field exist. We should also support to reference a nested field as the rowtime field. "	FLINK	Resolved	3	7	5351	pull-request-available
13528993	Move execution logic of ShowOperation out from TableEnvironmentImpl	This should implement {{ExecutableOperation}} for all the {{ShowOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.	FLINK	Closed	3	7	5351	pull-request-available
13250410	Fix TableFactory doesn't work with DDL when containing TIMESTAMP/DATE/TIME types	"Currently, in blink planner, we will convert DDL to {{TableSchema}} with new type system, i.e. DataTypes.TIMESTAMP()/DATE()/TIME() whose underlying TypeInformation are  Types.LOCAL_DATETIME/LOCAL_DATE/LOCAL_TIME. 

However, this makes the existing connector implementations (Kafka, ES, CSV, etc..) don't work because they only accept the old TypeInformations (Types.SQL_TIMESTAMP/SQL_DATE/SQL_TIME).

A simple solution is encode DataTypes.TIMESTAMP() as ""TIMESTAMP"" when translating to properties. And will be converted back to the old TypeInformation: Types.SQL_TIMESTAMP. This would fix all factories at once.
"	FLINK	Resolved	2	1	5351	pull-request-available
13310359	Init lookup join failed when use udf on lookup table	"Throw exception 

{code}
Caused by: scala.MatchError: (CONCAT(_UTF-16LE'Hello', $2),_UTF-16LE'Hello,Jark':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") (of class scala.Tuple2)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.org$apache$flink$table$planner$plan$nodes$common$CommonLookupJoin$$extractConstantField(CommonLookupJoin.scala:617)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.extractConstantFieldsFromEquiCondition(CommonLookupJoin.scala:607)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.analyzeLookupKeys(CommonLookupJoin.scala:567)
	at org.apache.flink.table.planner.plan.nodes.common.CommonLookupJoin.<init>(CommonLookupJoin.scala:129)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLookupJoin.<init>(StreamExecLookupJoin.scala:49)
{code}

SQL:

{code:sql}
SELECT
  T.id, T.len, T.content, D.name 
FROM 
  T JOIN userTable for system_time as of T.proctime AS D 
ON T.id = D.id 
WHERE 
  add(T.id, D.id) > 3 AND add(T.id, 2) > 3 AND CONCAT('Hello', D.name) = 'Hello,Jark'
{code}

When use function a RexCall can't match RexInputRef and cause this error, myabe shoud add condition""{{case _ => return}}"" to skip this.
"	FLINK	Closed	3	1	5351	pull-request-available
13341006	FactoryUtil#createTableSource will be confused by a table source and a table sink factory with same identifier	"When creating a table source I'm faced with the following exception:
{code:java}
 Caused by: org.apache.flink.table.api.ValidationException: Multiple factories for identifier 'odps' that implement 'org.apache.flink.table.factories.DynamicTableFactory' found in the classpath.

Ambiguous factory classes are:
{code}
However there is only one table source factory with this identifier, and another table sink factory with this identifier. {{FactoryUtil#createTableSource}} shouldn't be confused.

This is caused by {{FactoryUtil.java}} line 370, where {{factory = discoverFactory(context.getClassLoader(), DynamicTableFactory.class, connectorOption);}} should be {{factory = discoverFactory(context.getClassLoader(), factoryClass, connectorOption);}}.

I understand that this change aims to display a better exception message. We might need to change the logic of selecting a proper exception message a little bit."	FLINK	Closed	1	1	5351	pull-request-available
13248367	DDL do not support CSV tableFactory because CSV require format.fields	"(Now DDL do not support key of properties contains number or ""-"".)

And old csv validator require ""format.fields.#.type"". So there is an validation exception now."	FLINK	Resolved	2	1	5351	pull-request-available
13290971	"Add documentation about ""GROUPING SETS"" and ""CUBE"" support in streaming mode"	"""GROUPING SETS"" and ""CUBE"" are already supported in streaming mode in blink planner, but we missed to add that feature in the documentation. And that confused some users [1].

Note that it is only supported in blink planner, not old planner.

[1]: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Question-on-the-SQL-quot-GROUPING-SETS-quot-and-quot-CUBE-quot-syntax-usability-td33504.html"	FLINK	Resolved	3	4	5351	pull-request-available
13277861	Set default planner for SQL Client to Blink planner	"As discussed in the mailing list [1], we will change the default planner to Blink planner for SQL CLI. 

[1]: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Set-default-planner-for-SQL-Client-to-Blink-planner-in-1-10-release-td36379.html
"	FLINK	Resolved	3	4	5351	pull-request-available
13313012	Deprecate TableEnvironment#connect API 	"In the past releases, the community focused on the new SQL DDL features and the {{TableEnvironment#connect}} API is not maintained for a long time. We have seen many (bug) issues around this APIs. 

The {{TableEnvironment#connect()}} API might receive some refactoring in the next release. So we should deprecate the connect API in 1.11 and recommend users to use {{executeSql(ddl)}} instead. Thus, we can remove/refactor the existing {{connect()}} API in the next release even though it is API breaking. "	FLINK	Closed	3	4	5351	pull-request-available
13231910	Synchronize the latest documentation changes into Chinese documents	There are several commits to documentations have not been synchronized to Chinese documents, i.e. `xx.zh.md`. This pull request will synchronize the latest changes into Chinese documents.	FLINK	Closed	4	7	5351	pull-request-available
13280358	The same sql run in a streaming environment producing a Exception, but a batch env can run normally.	"*summary:*
The same sql can run in a batch environment normally,  but in a streaming environment there will be a exception like this:
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [f1]





*The sql is:*

CREATE TABLE `tenk1` (
	unique1 int,
	unique2 int,
	two int,
	four int,
	ten int,
	twenty int,
	hundred int,
	thousand int,
	twothousand int,
	fivethous int,
	tenthous int,
	odd int,
	even int,
	stringu1 varchar,
	stringu2 varchar,
	string4 varchar
) WITH (
	'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/tenk1.csv',
	'format.empty-column-as-null'='true',
	'format.field-delimiter'='|',
	'connector.type'='filesystem',
	'format.derive-schema'='true',
	'format.type'='csv'
);

CREATE TABLE `int4_tbl` (
	f1 INT
) WITH (
	'connector.path'='/daily_regression_test_stream_postgres_1.10/test_join/sources/int4_tbl.csv',
	'format.empty-column-as-null'='true',
	'format.field-delimiter'='|',
	'connector.type'='filesystem',
	'format.derive-schema'='true',
	'format.type'='csv'
);

select a.f1, b.f1, t.thousand, t.tenthous from
  tenk1 t,
  (select sum(f1)+1 as f1 from int4_tbl i4a) a,
  (select sum(f1) as f1 from int4_tbl i4b) b
where b.f1 = t.thousand and a.f1 = b.f1 and (a.f1+b.f1+999) = t.tenthous;





"	FLINK	Resolved	3	1	5351	pull-request-available
13296434	Add new data structure interfaces in table-common	"This add the new data structure interfaces to {{table-common}}.

The planner and connector refactoring happens in a separate issue."	FLINK	Closed	3	7	5351	pull-request-available
13248892	Fix some transformation names are not set in blink planner	"Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion."	FLINK	Resolved	3	1	5351	pull-request-available
13312940	JdbcFullTest failed to compile on JDK11	"master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3928&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=946871de-358d-5815-3994-8175615bc253
release-1.11: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=946871de-358d-5815-3994-8175615bc253
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3929&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6

{code}
2020-06-22T20:19:50.2157534Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2158031Z [ERROR] COMPILATION ERROR : 
2020-06-22T20:19:50.2158826Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2159987Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[137,51] cannot find symbol
2020-06-22T20:19:50.2160676Z   symbol:   variable f1
2020-06-22T20:19:50.2161236Z   location: variable tuple2 of type java.lang.Object
2020-06-22T20:19:50.2163372Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[136,33] incompatible types: cannot infer functional interface descriptor for org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.StatementExecutorFactory<org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor>
2020-06-22T20:19:50.2164788Z [INFO] 2 errors 
2020-06-22T20:19:50.2165569Z [INFO] -------------------------------------------------------------
2020-06-22T20:19:50.2166430Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2167374Z [INFO] Reactor Summary:
2020-06-22T20:19:50.2167713Z [INFO] 
2020-06-22T20:19:50.2168486Z [INFO] force-shading ...................................... SUCCESS [  5.905 s]
2020-06-22T20:19:50.2169067Z [INFO] flink .............................................. SUCCESS [ 10.173 s]
2020-06-22T20:19:50.2169978Z [INFO] flink-annotations .................................. SUCCESS [  1.637 s]
2020-06-22T20:19:50.2170980Z [INFO] flink-test-utils-parent ............................ SUCCESS [  0.117 s]
2020-06-22T20:19:50.2171877Z [INFO] flink-test-utils-junit ............................. SUCCESS [  1.224 s]
2020-06-22T20:19:50.2172896Z [INFO] flink-metrics ...................................... SUCCESS [  0.101 s]
2020-06-22T20:19:50.2173788Z [INFO] flink-metrics-core ................................. SUCCESS [  1.726 s]
2020-06-22T20:19:50.2175058Z [INFO] flink-core ......................................... SUCCESS [ 29.372 s]
2020-06-22T20:19:50.2175982Z [INFO] flink-java ......................................... SUCCESS [  5.577 s]
2020-06-22T20:19:50.2176868Z [INFO] flink-queryable-state .............................. SUCCESS [  0.085 s]
2020-06-22T20:19:50.2177760Z [INFO] flink-queryable-state-client-java .................. SUCCESS [  1.619 s]
2020-06-22T20:19:50.2178600Z [INFO] flink-filesystems .................................. SUCCESS [  0.105 s]
2020-06-22T20:19:50.2179500Z [INFO] flink-hadoop-fs .................................... SUCCESS [ 20.792 s]
2020-06-22T20:19:50.2180402Z [INFO] flink-runtime ...................................... SUCCESS [01:51 min]
2020-06-22T20:19:50.2181462Z [INFO] flink-scala ........................................ SUCCESS [ 36.797 s]
2020-06-22T20:19:50.2182326Z [INFO] flink-mapr-fs ...................................... SUCCESS [  0.848 s]
2020-06-22T20:19:50.2183372Z [INFO] flink-filesystems :: flink-fs-hadoop-shaded ........ SUCCESS [  4.422 s]
2020-06-22T20:19:50.2184407Z [INFO] flink-s3-fs-base ................................... SUCCESS [  2.085 s]
2020-06-22T20:19:50.2185259Z [INFO] flink-s3-fs-hadoop ................................. SUCCESS [  6.051 s]
2020-06-22T20:19:50.2186131Z [INFO] flink-s3-fs-presto ................................. SUCCESS [ 10.325 s]
2020-06-22T20:19:50.2186990Z [INFO] flink-swift-fs-hadoop .............................. SUCCESS [ 22.021 s]
2020-06-22T20:19:50.2187820Z [INFO] flink-oss-fs-hadoop ................................ SUCCESS [  6.407 s]
2020-06-22T20:19:50.2188686Z [INFO] flink-azure-fs-hadoop .............................. SUCCESS [  8.868 s]
2020-06-22T20:19:50.2189526Z [INFO] flink-optimizer .................................... SUCCESS [ 10.922 s]
2020-06-22T20:19:50.2190385Z [INFO] flink-streaming-java ............................... SUCCESS [ 14.119 s]
2020-06-22T20:19:50.2191563Z [INFO] flink-clients ...................................... SUCCESS [  2.558 s]
2020-06-22T20:19:50.2192425Z [INFO] flink-test-utils ................................... SUCCESS [  1.837 s]
2020-06-22T20:19:50.2193609Z [INFO] flink-runtime-web .................................. SUCCESS [02:01 min]
2020-06-22T20:19:50.2194615Z [INFO] flink-examples ..................................... SUCCESS [  0.174 s]
2020-06-22T20:19:50.2195284Z [INFO] flink-examples-batch ............................... SUCCESS [ 12.889 s]
2020-06-22T20:19:50.2195902Z [INFO] flink-connectors ................................... SUCCESS [  0.109 s]
2020-06-22T20:19:50.2196513Z [INFO] flink-hadoop-compatibility ......................... SUCCESS [  6.164 s]
2020-06-22T20:19:50.2197043Z [INFO] flink-state-backends ............................... SUCCESS [  0.075 s]
2020-06-22T20:19:50.2197591Z [INFO] flink-statebackend-rocksdb ......................... SUCCESS [  4.125 s]
2020-06-22T20:19:50.2198116Z [INFO] flink-tests ........................................ SUCCESS [ 36.488 s]
2020-06-22T20:19:50.2198658Z [INFO] flink-streaming-scala .............................. SUCCESS [ 33.694 s]
2020-06-22T20:19:50.2199392Z [INFO] flink-hcatalog ..................................... SUCCESS [  7.401 s]
2020-06-22T20:19:50.2199957Z [INFO] flink-table ........................................ SUCCESS [  0.073 s]
2020-06-22T20:19:50.2200496Z [INFO] flink-table-common ................................. SUCCESS [  5.219 s]
2020-06-22T20:19:50.2201146Z [INFO] flink-table-api-java ............................... SUCCESS [  3.072 s]
2020-06-22T20:19:50.2201699Z [INFO] flink-table-api-java-bridge ........................ SUCCESS [  1.363 s]
2020-06-22T20:19:50.2202221Z [INFO] flink-table-api-scala .............................. SUCCESS [ 11.035 s]
2020-06-22T20:19:50.2202978Z [INFO] flink-table-api-scala-bridge ....................... SUCCESS [  9.510 s]
2020-06-22T20:19:50.2203548Z [INFO] flink-sql-parser ................................... SUCCESS [  6.408 s]
2020-06-22T20:19:50.2204152Z [INFO] flink-libraries .................................... SUCCESS [  0.080 s]
2020-06-22T20:19:50.2204832Z [INFO] flink-cep .......................................... SUCCESS [  3.582 s]
2020-06-22T20:19:50.2205395Z [INFO] flink-table-planner ................................ SUCCESS [02:02 min]
2020-06-22T20:19:50.2205980Z [INFO] flink-sql-parser-hive .............................. SUCCESS [  2.555 s]
2020-06-22T20:19:50.2206691Z [INFO] flink-table-runtime-blink .......................... SUCCESS [  6.373 s]
2020-06-22T20:19:50.2207523Z [INFO] flink-table-planner-blink .......................... SUCCESS [02:42 min]
2020-06-22T20:19:50.2208335Z [INFO] flink-metrics-jmx .................................. SUCCESS [  0.559 s]
2020-06-22T20:19:50.2209071Z [INFO] flink-formats ...................................... SUCCESS [  0.069 s]
2020-06-22T20:19:50.2209852Z [INFO] flink-json ......................................... SUCCESS [  1.230 s]
2020-06-22T20:19:50.2210584Z [INFO] flink-connector-kafka-base ......................... SUCCESS [  4.715 s]
2020-06-22T20:19:50.2211540Z [INFO] flink-avro ......................................... SUCCESS [  4.357 s]
2020-06-22T20:19:50.2212347Z [INFO] flink-csv .......................................... SUCCESS [  1.086 s]
2020-06-22T20:19:50.2213216Z [INFO] flink-connector-kafka-0.10 ......................... SUCCESS [  4.828 s]
2020-06-22T20:19:50.2214121Z [INFO] flink-connector-kafka-0.11 ......................... SUCCESS [  5.160 s]
2020-06-22T20:19:50.2214850Z [INFO] flink-connector-elasticsearch-base ................. SUCCESS [ 10.191 s]
2020-06-22T20:19:50.2215579Z [INFO] flink-connector-elasticsearch5 ..................... SUCCESS [ 16.121 s]
2020-06-22T20:19:50.2216348Z [INFO] flink-connector-elasticsearch6 ..................... SUCCESS [ 13.925 s]
2020-06-22T20:19:50.2217097Z [INFO] flink-connector-elasticsearch7 ..................... SUCCESS [ 14.855 s]
2020-06-22T20:19:50.2217870Z [INFO] flink-connector-hbase .............................. SUCCESS [ 26.486 s]
2020-06-22T20:19:50.2218627Z [INFO] flink-hadoop-bulk .................................. SUCCESS [  0.804 s]
2020-06-22T20:19:50.2219401Z [INFO] flink-orc .......................................... SUCCESS [  1.600 s]
2020-06-22T20:19:50.2220154Z [INFO] flink-orc-nohive ................................... SUCCESS [  1.258 s]
2020-06-22T20:19:50.2221047Z [INFO] flink-parquet ...................................... SUCCESS [  5.085 s]
2020-06-22T20:19:50.2221830Z [INFO] flink-connector-hive ............................... SUCCESS [ 29.718 s]
2020-06-22T20:19:50.2222595Z [INFO] flink-connector-jdbc ............................... FAILURE [ 42.673 s]
2020-06-22T20:19:50.2223455Z [INFO] flink-connector-rabbitmq ........................... SKIPPED
2020-06-22T20:19:50.2224297Z [INFO] flink-connector-twitter ............................ SKIPPED
2020-06-22T20:19:50.2225011Z [INFO] flink-connector-nifi ............................... SKIPPED
2020-06-22T20:19:50.2225705Z [INFO] flink-connector-cassandra .......................... SKIPPED
2020-06-22T20:19:50.2226427Z [INFO] flink-connector-filesystem ......................... SKIPPED
2020-06-22T20:19:50.2227146Z [INFO] flink-connector-kafka .............................. SKIPPED
2020-06-22T20:19:50.2228045Z [INFO] flink-connector-gcp-pubsub ......................... SKIPPED
2020-06-22T20:19:50.2228760Z [INFO] flink-connector-kinesis ............................ SKIPPED
2020-06-22T20:19:50.2229459Z [INFO] flink-sql-connector-elasticsearch7 ................. SKIPPED
2020-06-22T20:19:50.2230163Z [INFO] flink-connector-base ............................... SKIPPED
2020-06-22T20:19:50.2230956Z [INFO] flink-sql-connector-elasticsearch6 ................. SKIPPED
2020-06-22T20:19:50.2231671Z [INFO] flink-sql-connector-kafka-0.10 ..................... SKIPPED
2020-06-22T20:19:50.2232353Z [INFO] flink-sql-connector-kafka-0.11 ..................... SKIPPED
2020-06-22T20:19:50.2233168Z [INFO] flink-sql-connector-kafka .......................... SKIPPED
2020-06-22T20:19:50.2234041Z [INFO] flink-sql-connector-hive-1.2.2 ..................... SKIPPED
2020-06-22T20:19:50.2234926Z [INFO] flink-sql-connector-hive-2.2.0 ..................... SKIPPED
2020-06-22T20:19:50.2235647Z [INFO] flink-sql-connector-hive-2.3.6 ..................... SKIPPED
2020-06-22T20:19:50.2236328Z [INFO] flink-sql-connector-hive-3.1.2 ..................... SKIPPED
2020-06-22T20:19:50.2236998Z [INFO] flink-avro-confluent-registry ...................... SKIPPED
2020-06-22T20:19:50.2237717Z [INFO] flink-sequence-file ................................ SKIPPED
2020-06-22T20:19:50.2238397Z [INFO] flink-compress ..................................... SKIPPED
2020-06-22T20:19:50.2239109Z [INFO] flink-sql-orc ...................................... SKIPPED
2020-06-22T20:19:50.2239793Z [INFO] flink-sql-parquet .................................. SKIPPED
2020-06-22T20:19:50.2240474Z [INFO] flink-examples-streaming ........................... SKIPPED
2020-06-22T20:19:50.2241300Z [INFO] flink-examples-table ............................... SKIPPED
2020-06-22T20:19:50.2241996Z [INFO] flink-examples-build-helper ........................ SKIPPED
2020-06-22T20:19:50.2242836Z [INFO] flink-examples-streaming-twitter ................... SKIPPED
2020-06-22T20:19:50.2243548Z [INFO] flink-examples-streaming-state-machine ............. SKIPPED
2020-06-22T20:19:50.2244395Z [INFO] flink-examples-streaming-gcp-pubsub ................ SKIPPED
2020-06-22T20:19:50.2245069Z [INFO] flink-container .................................... SKIPPED
2020-06-22T20:19:50.2245787Z [INFO] flink-queryable-state-runtime ...................... SKIPPED
2020-06-22T20:19:50.2246509Z [INFO] flink-mesos ........................................ SKIPPED
2020-06-22T20:19:50.2247208Z [INFO] flink-kubernetes ................................... SKIPPED
2020-06-22T20:19:50.2247921Z [INFO] flink-yarn ......................................... SKIPPED
2020-06-22T20:19:50.2248641Z [INFO] flink-gelly ........................................ SKIPPED
2020-06-22T20:19:50.2249329Z [INFO] flink-gelly-scala .................................. SKIPPED
2020-06-22T20:19:50.2250039Z [INFO] flink-gelly-examples ............................... SKIPPED
2020-06-22T20:19:50.2250869Z [INFO] flink-external-resources ........................... SKIPPED
2020-06-22T20:19:50.2251554Z [INFO] flink-external-resource-gpu ........................ SKIPPED
2020-06-22T20:19:50.2252277Z [INFO] flink-metrics-dropwizard ........................... SKIPPED
2020-06-22T20:19:50.2253101Z [INFO] flink-metrics-graphite ............................. SKIPPED
2020-06-22T20:19:50.2253833Z [INFO] flink-metrics-influxdb ............................. SKIPPED
2020-06-22T20:19:50.2254742Z [INFO] flink-metrics-prometheus ........................... SKIPPED
2020-06-22T20:19:50.2255401Z [INFO] flink-metrics-statsd ............................... SKIPPED
2020-06-22T20:19:50.2256077Z [INFO] flink-metrics-datadog .............................. SKIPPED
2020-06-22T20:19:50.2256755Z [INFO] flink-metrics-slf4j ................................ SKIPPED
2020-06-22T20:19:50.2257446Z [INFO] flink-cep-scala .................................... SKIPPED
2020-06-22T20:19:50.2258126Z [INFO] flink-table-uber ................................... SKIPPED
2020-06-22T20:19:50.2259021Z [INFO] flink-table-uber-blink ............................. SKIPPED
2020-06-22T20:19:50.2259740Z [INFO] flink-python ....................................... SKIPPED
2020-06-22T20:19:50.2260417Z [INFO] flink-sql-client ................................... SKIPPED
2020-06-22T20:19:50.2261262Z [INFO] flink-state-processor-api .......................... SKIPPED
2020-06-22T20:19:50.2261940Z [INFO] flink-ml-parent .................................... SKIPPED
2020-06-22T20:19:50.2262773Z [INFO] flink-ml-api ....................................... SKIPPED
2020-06-22T20:19:50.2263475Z [INFO] flink-ml-lib ....................................... SKIPPED
2020-06-22T20:19:50.2264295Z [INFO] flink-ml-uber ...................................... SKIPPED
2020-06-22T20:19:50.2264976Z [INFO] flink-scala-shell .................................. SKIPPED
2020-06-22T20:19:50.2265828Z [INFO] flink-dist ......................................... SKIPPED
2020-06-22T20:19:50.2266560Z [INFO] flink-yarn-tests ................................... SKIPPED
2020-06-22T20:19:50.2267234Z [INFO] flink-end-to-end-tests ............................. SKIPPED
2020-06-22T20:19:50.2267933Z [INFO] flink-cli-test ..................................... SKIPPED
2020-06-22T20:19:50.2268619Z [INFO] flink-parent-child-classloading-test-program ....... SKIPPED
2020-06-22T20:19:50.2269313Z [INFO] flink-parent-child-classloading-test-lib-package ... SKIPPED
2020-06-22T20:19:50.2269984Z [INFO] flink-dataset-allround-test ........................ SKIPPED
2020-06-22T20:19:50.2270699Z [INFO] flink-dataset-fine-grained-recovery-test ........... SKIPPED
2020-06-22T20:19:50.2271519Z [INFO] flink-datastream-allround-test ..................... SKIPPED
2020-06-22T20:19:50.2272199Z [INFO] flink-batch-sql-test ............................... SKIPPED
2020-06-22T20:19:50.2273062Z [INFO] flink-stream-sql-test .............................. SKIPPED
2020-06-22T20:19:50.2273762Z [INFO] flink-bucketing-sink-test .......................... SKIPPED
2020-06-22T20:19:50.2274588Z [INFO] flink-distributed-cache-via-blob ................... SKIPPED
2020-06-22T20:19:50.2275278Z [INFO] flink-high-parallelism-iterations-test ............. SKIPPED
2020-06-22T20:19:50.2275976Z [INFO] flink-stream-stateful-job-upgrade-test ............. SKIPPED
2020-06-22T20:19:50.2276675Z [INFO] flink-queryable-state-test ......................... SKIPPED
2020-06-22T20:19:50.2277352Z [INFO] flink-local-recovery-and-allocation-test ........... SKIPPED
2020-06-22T20:19:50.2278076Z [INFO] flink-elasticsearch5-test .......................... SKIPPED
2020-06-22T20:19:50.2278778Z [INFO] flink-elasticsearch6-test .......................... SKIPPED
2020-06-22T20:19:50.2279483Z [INFO] flink-quickstart ................................... SKIPPED
2020-06-22T20:19:50.2280173Z [INFO] flink-quickstart-java .............................. SKIPPED
2020-06-22T20:19:50.2280982Z [INFO] flink-quickstart-scala ............................. SKIPPED
2020-06-22T20:19:50.2281719Z [INFO] flink-quickstart-test .............................. SKIPPED
2020-06-22T20:19:50.2282416Z [INFO] flink-confluent-schema-registry .................... SKIPPED
2020-06-22T20:19:50.2283272Z [INFO] flink-stream-state-ttl-test ........................ SKIPPED
2020-06-22T20:19:50.2284102Z [INFO] flink-sql-client-test .............................. SKIPPED
2020-06-22T20:19:50.2284793Z [INFO] flink-streaming-file-sink-test ..................... SKIPPED
2020-06-22T20:19:50.2285536Z [INFO] flink-state-evolution-test ......................... SKIPPED
2020-06-22T20:19:50.2286243Z [INFO] flink-rocksdb-state-memory-control-test ............ SKIPPED
2020-06-22T20:19:50.2287011Z [INFO] flink-end-to-end-tests-common ...................... SKIPPED
2020-06-22T20:19:50.2287749Z [INFO] flink-metrics-availability-test .................... SKIPPED
2020-06-22T20:19:50.2288440Z [INFO] flink-metrics-reporter-prometheus-test ............. SKIPPED
2020-06-22T20:19:50.2289231Z [INFO] flink-heavy-deployment-stress-test ................. SKIPPED
2020-06-22T20:19:50.2290158Z [INFO] flink-connector-gcp-pubsub-emulator-tests .......... SKIPPED
2020-06-22T20:19:50.2291011Z [INFO] flink-streaming-kafka-test-base .................... SKIPPED
2020-06-22T20:19:50.2291740Z [INFO] flink-streaming-kafka-test ......................... SKIPPED
2020-06-22T20:19:50.2292476Z [INFO] flink-streaming-kafka011-test ...................... SKIPPED
2020-06-22T20:19:50.2293323Z [INFO] flink-streaming-kafka010-test ...................... SKIPPED
2020-06-22T20:19:50.2294177Z [INFO] flink-plugins-test ................................. SKIPPED
2020-06-22T20:19:50.2294858Z [INFO] dummy-fs ........................................... SKIPPED
2020-06-22T20:19:50.2295612Z [INFO] another-dummy-fs ................................... SKIPPED
2020-06-22T20:19:50.2296338Z [INFO] flink-tpch-test .................................... SKIPPED
2020-06-22T20:19:50.2297112Z [INFO] flink-streaming-kinesis-test ....................... SKIPPED
2020-06-22T20:19:50.2298047Z [INFO] flink-elasticsearch7-test .......................... SKIPPED
2020-06-22T20:19:50.2298793Z [INFO] flink-end-to-end-tests-common-kafka ................ SKIPPED
2020-06-22T20:19:50.2299551Z [INFO] flink-tpcds-test ................................... SKIPPED
2020-06-22T20:19:50.2300251Z [INFO] flink-netty-shuffle-memory-control-test ............ SKIPPED
2020-06-22T20:19:50.2301104Z [INFO] flink-python-test .................................. SKIPPED
2020-06-22T20:19:50.2301876Z [INFO] flink-statebackend-heap-spillable .................. SKIPPED
2020-06-22T20:19:50.2302551Z [INFO] flink-contrib ...................................... SKIPPED
2020-06-22T20:19:50.2303405Z [INFO] flink-connector-wikiedits .......................... SKIPPED
2020-06-22T20:19:50.2304247Z [INFO] flink-fs-tests ..................................... SKIPPED
2020-06-22T20:19:50.2305061Z [INFO] flink-docs ......................................... SKIPPED
2020-06-22T20:19:50.2305813Z [INFO] flink-walkthroughs ................................. SKIPPED
2020-06-22T20:19:50.2306505Z [INFO] flink-walkthrough-common ........................... SKIPPED
2020-06-22T20:19:50.2307228Z [INFO] flink-walkthrough-table-java ....................... SKIPPED
2020-06-22T20:19:50.2307920Z [INFO] flink-walkthrough-table-scala ...................... SKIPPED
2020-06-22T20:19:50.2308641Z [INFO] flink-walkthrough-datastream-java .................. SKIPPED
2020-06-22T20:19:50.2309341Z [INFO] flink-walkthrough-datastream-scala ................. SKIPPED
2020-06-22T20:19:50.2310060Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2310483Z [INFO] BUILD FAILURE
2020-06-22T20:19:50.2311235Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.2311680Z [INFO] Total time: 17:47 min
2020-06-22T20:19:50.2312305Z [INFO] Finished at: 2020-06-22T20:19:50+00:00
2020-06-22T20:19:50.3937346Z [INFO] Final Memory: 326M/1212M
2020-06-22T20:19:50.3938874Z [INFO] ------------------------------------------------------------------------
2020-06-22T20:19:50.3953129Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-jdbc_2.11: Compilation failure: Compilation failure:
2020-06-22T20:19:50.3954979Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[137,51] cannot find symbol
2020-06-22T20:19:50.3955665Z [ERROR] symbol:   variable f1
2020-06-22T20:19:50.3956090Z [ERROR] location: variable tuple2 of type java.lang.Object
2020-06-22T20:19:50.3958074Z [ERROR] /__w/2/s/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java:[136,33] incompatible types: cannot infer functional interface descriptor for org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.StatementExecutorFactory<org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor>
2020-06-22T20:19:50.3959865Z [ERROR] -> [Help 1]
2020-06-22T20:19:50.3960153Z [ERROR] 
2020-06-22T20:19:50.3961449Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-22T20:19:50.3962383Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-22T20:19:50.3962995Z [ERROR] 
2020-06-22T20:19:50.3963580Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-22T20:19:50.3964490Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
2020-06-22T20:19:50.3964969Z [ERROR] 
2020-06-22T20:19:50.3965374Z [ERROR] After correcting the problems, you can resume the build with the command
2020-06-22T20:19:50.3966310Z [ERROR]   mvn <goals> -rf :flink-connector-jdbc_2.11
{code}"	FLINK	Closed	1	1	5351	test-stability
13295288	Refactor retraction rules to support inferring ChangelogMode	"Current retraction machanism only support 2 message kinds (+ and -). However, since FLIP-95, we will introduce more message kinds to users (insert/delete/update_before/update_after). 

In order to support that, we should first refactor current retraction rules to support ChangelogMode inference. In previous, every node will be attached with a AccMode trait after retraction rule. In the proposed design, we will infer ChangelogMode trait for every node. 

Design documentation: https://docs.google.com/document/d/1n_iXIQsKT3uiBqENR8j8RdjRhZfzMhhB66QZvx2rFjE/edit?ts=5e8419c1#"	FLINK	Closed	3	7	5351	pull-request-available
13285715	ExpressionReducer shouldn't escape the reduced string value	"ExpressionReducer shouldn't escape the reduced string value, the escaping should only happen in code generation, otherwise the output result is inccorect. 

The problem is this line I guess: https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala#L142

Here is a simple example to reproduce the problem:

{code:java}
  val smallTupleData3: Seq[(Int, Long, String)] = {
    val data = new mutable.MutableList[(Int, Long, String)]
    data.+=((1, 1L, ""你好""))
    data.+=((2, 2L, ""你好""))
    data.+=((3, 2L, ""你好世界""))
    data
  }

  @Test
  def test(): Unit = {
    val t = env.fromCollection(smallTupleData3)
      .toTable(tEnv, 'a, 'b, 'c)
    tEnv.createTemporaryView(""MyTable"", t)
    val sqlQuery = s""select * from MyTable where c = '你好'""

    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
    val sink = new TestingAppendSink
    result.addSink(sink)
    env.execute()
    println(sink.getAppendResults.mkString(""\n""))
  }
{code}

The output:

{code:java}
1,1,\u4F60\u597D
2,2,\u4F60\u597D
{code}

This is also mentioned in user mailing list: http://apache-flink.147419.n8.nabble.com/ParquetTableSource-blink-table-planner-tp1696p1720.html
"	FLINK	Resolved	2	1	5351	pull-request-available
13217573	Add a language switch to the sidebar for Flink documents	"Add a language switch similar to the project webpage. 

We didn't add the switch in the first version of supporting Chinese language, because we want to expose the switch when we satisfied the translation coverage."	FLINK	Closed	3	7	5351	pull-request-available
13312938	CollectionExecutorTest failed to compiled in release-1.11	 !image-2020-06-23-10-00-45-519.png! 	FLINK	Closed	1	4	5351	pull-request-available
13571580	"Add ""Special Thanks"" Page on the Flink Website"	"This issue aims to add a ""Special Thanks"" page on the Flink website (https://flink.apache.org/) to honor and appreciate the companies and organizations that have sponsored machines or services for our project.

Discussion thread: https://lists.apache.org/thread/y5g0nd5t8h2ql4gq7d0kb9tkwv1wkm1j"	FLINK	Open	3	2	5351	pull-request-available
13223246	Introduce TableImpl and remove Table in flink-table-planner-blink	"After FLINK-11068 is merged, the {{Table}} interfaced is added into {{flink-table-api-java}}. The classpath is conflicted with {{Table}} in {{flink-table-planner-blink}} which result in IDE errors and some tests fail (only in my local, looks good in mvn verify). 

This issue make {{Table}} in {{flink-table-planner-blink}} to extends {{Table}} in {{flink-table-api-java}} and rename to {{TableImpl}}. We still left the methods implementation to be empty until the {{LogicalNode}} is refactored."	FLINK	Closed	3	4	5351	pull-request-available
13287576	NullPointerException in GroupAggProcessFunction.close()	"CI run: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5586&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=48867695-c47f-5af3-2f21-7845611247b9

{code}
java.lang.NullPointerException: null
	at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.close(GroupAggProcessFunction.scala:182) ~[flink-table_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:627) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:565) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:483) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]

{code}"	FLINK	Resolved	3	1	5351	pull-request-available, test-stability
13273224	Improve schema derivation for Avro format	"For JSON, CSV and OldCsv, we already supported {{derive.schema=true}} to get the schema from table schema. But for Avro format, a user has to pass an Avro schema file or define the format schema explicitly via {{avro.schema}}.

We can think of if we can drop {{avro.schema}} and make {{derive.schema=true}} as the default behavior."	FLINK	Closed	3	7	5351	pull-request-available
13335154	Implement cumulative windowing for window aggregate operator	Support cumulative windows for existing window aggregate operator, i.e. {{WindowOperator}}.	FLINK	Closed	3	7	5351	pull-request-available
13248473	Set parallelism of table sink operator to input transformation parallelism	Currently, there are a lot of {{TableSink}} connectors uses {{dataStream.addSink()}} without {{setParallelism}} explicitly. This will use default parallelism of the environment. However, the parallelism of input transformation might not be env.parallelism, for example, global aggregation has 1 parallelism. In this case, it will lead to data reorder, and result in incorrect result.	FLINK	Resolved	2	1	5351	pull-request-available
12987796	Add a basic streaming Table API example	Although the Table API does not offer much streaming features yet, there should be a runnable example showing how to convert, union, filter and project streams with the Table API.	FLINK	Resolved	3	2	5351	starter
13335940	Writing Table with RowTime Column of type TIMESTAMP(3) to Kafka fails with ClassCastException	"When I try to write a table to Kafka (JSON format) that has a rowtime attribute of type TIMESTAMP(3) the job fails with 

{noformat}
2020-10-18 18:02:08
java.lang.ClassCastException: org.apache.flink.table.data.TimestampData cannot be cast to java.lang.Long
	at org.apache.flink.table.data.GenericRowData.getLong(GenericRowData.java:154)
	at org.apache.flink.table.runtime.operators.sink.SinkOperator$SimpleContext.timestamp(SinkOperator.java:144)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:866)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)
	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:86)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.processElement(WatermarkAssignerOperator.java:123)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collectWithTimestamp(StreamSourceContexts.java:111)
	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:352)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:185)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:141)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
{noformat}

From looking at the relevant code in SinkOperator$SimpleContext#timestamp it seems that we can only deal with long type timestamps in SinkOperator?!
"	FLINK	Closed	2	1	5351	pull-request-available
13238749	Introduce PartitionableTableSource for partition pruning	Many data sources are partitionable storage, e.g. HIVE, HDFS, Druid. And many queries just need to read a small subset of the total data. We can use partition information to prune or skip over files irrelevant to the user's queries. Both query optimization time and execution time can be reduced obviously, especially for a large partitioned table.	FLINK	Closed	3	2	5351	pull-request-available
13285095	Blink planner produces wrong aggregate results with state clean up	"It seems that FLINK-10674 has not been ported to the Blink planner.

Because state clean up happens in processing time, it might be the case that retractions are arriving after the state has been cleaned up. Before these changes, a new accumulator was created and invalid retraction messages were emitted. This change drops retraction messages for which no accumulator exists.

These lines are missing in {{org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction}}:
{code}
if (null == accumulators) {
      // Don't create a new accumulator for a retraction message. This
      // might happen if the retraction message is the first message for the
      // key or after a state clean up.
      if (!inputC.change) {
        return
      }
      // first accumulate message
      firstRow = true
      accumulators = function.createAccumulators()
    } else {
      firstRow = false
    }
{code}

The bug has not been verified. I spotted it only by looking at the code."	FLINK	Resolved	2	1	5351	pull-request-available
13339476	Outdated SQL docs on aggregate functions' merge	"In the java docs as well as the user docs, the {{merge}} method of an aggregation UDF is described as optional, e.g.
{quote}Merges a group of accumulator instances into one accumulator instance. This function must be implemented for data stream session window grouping aggregates and data set grouping aggregates.{quote}

However, it seems that nowadays this method is required in more cases (I stumbled on this for a HOP window in streaming):
{code}
StreamExecGlobalGroupAggregate.scala
      .needMerge(mergedAccOffset, mergedAccOnHeap, mergedAccExternalTypes)
StreamExecGroupWindowAggregateBase.scala
      generator.needMerge(mergedAccOffset = 0, mergedAccOnHeap = false)
StreamExecIncrementalGroupAggregate.scala
      .needMerge(mergedAccOffset, mergedAccOnHeap = true, mergedAccExternalTypes)
StreamExecLocalGroupAggregate.scala
      .needMerge(mergedAccOffset = 0, mergedAccOnHeap = true)
{code}"	FLINK	Closed	3	1	5351	pull-request-available
13268598	Improve schema derivation for CSV and JSON formats 	Currently, one also needs to define a schema for formats in DDL or decriptor API. In this ticket, we will make derive schema as the default behavior, and keep compatibility with previous version to still support explicitly declare a format schema. 	FLINK	Resolved	3	7	5351	pull-request-available
13353502	Implement mini-batch optimized slicing window aggregate operator	"We have supported cumulative windows in FLINK-19605. However, the current cumulative window is not efficient, because the slices are not shared. 

We leverages the slicing ideas proposed in FLINK-7001 and this design doc [1]. The slicing is an optimized implementation for hopping, cumulative, tumbling windows. Besides of that, we introduced ManagedMemory based mini-batch optimization for the slicing window aggregate operator, this can tremendously reduce the accessing of state and get the higher throughtput without latency loss.  

[1]: https://docs.google.com/document/d/1ziVsuW_HQnvJr_4a9yKwx_LEnhVkdlde2Z5l6sx5HlY/edit#"	FLINK	Closed	3	7	5351	pull-request-available
13296762	Introduce a new HBase connector with new property keys	"This new hbase connector should use new interfaces proposed by FLIP-95, e.g. DynamicTableSource, DynamicTableSink, and Factory.

The new proposed keys :
||Old key||New key||Note||
|connector.type|connector| |
|connector.version|N/A|merged into 'connector' key|
|connector.table-name|table-name| |
|connector.zookeeper.quorum|zookeeper.quorum| |
|connector.zookeeper.znode.parent|zookeeper.znode-parent| |
|connector.write.buffer-flush.max-size|sink.buffer-flush.max-size| |
|connector.write.buffer-flush.max-rows|sink.buffer-flush.max-rows| |
|connector.write.buffer-flush.interval|sink.buffer-flush.interval| |
 
 "	FLINK	Closed	3	7	5351	pull-request-available
13230519	AsyncWaitOperator should deep copy StreamElement when object reuse is enabled	"Currently, AsyncWaitOperator directly put the input StreamElement into {{StreamElementQueue}}. But when object reuse is enabled, the StreamElement is reused, which means the element in {{StreamElementQueue}} will be modified. As a result, the output of AsyncWaitOperator might be wrong.

An easy way to fix this might be deep copy the input StreamElement when object reuse is enabled, like this: https://github.com/apache/flink/blob/blink/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/async/AsyncWaitOperator.java#L209"	FLINK	Closed	3	1	5351	pull-request-available, stale-assigned
13237194	Introduce new Interfaces for source and sink to make Blink runner work	"In order to support Blink batch and temporal table join, we need some new source&sink interfaces.

1. Introduce {{InputFormatTableSource}}
 - add {{isBounded}} interface to {{StreamTableSource}}
 - {{InputFormatTableSource}} extends {{StreamTableSource}} and expose {{getInputFormat}}
 - removes {{BatchTableSource}} and {{StreamTableSource}} in blink planner
 - support it in blink and flink planner

2. Introduce {{OutputFormatTableSink}}
 - {{OutputFormatTableSink}} extends {{StreamTableSink}} expose {{getOutputFormat}}
 - removes {{BatchTableSink}} in blink planner
 - support it in blink and flink planner

3. Introduce {{LookupableTableSource}}
 - removes {{LookupableTableSource}} and {{LookupConfig}} in blink planner
 - support it only in blink planner

4. Expose {{getTableStats}} in {{TableSource}}
 - support it in blink and flink planner"	FLINK	Closed	3	2	5351	pull-request-available
13260688	Add watermark information in TableSchema	"As discussed in FLIP-66, the watermark information should be part of TableSchema, and expose to connectors vis CatalogTable#getTableSchema. 

We may need to introduce a {{WatermarkSpec}} class to describe watermark information. "	FLINK	Resolved	3	7	5351	pull-request-available
13283636	Unable to use watermark when depends both on flink planner and blink planner	"Run the following code in module `flink-examples-table` (must be under this module)
{code:java}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.flink.table.examples.java;


import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.java.StreamTableEnvironment;

/**
 * javadoc.
 */
public class TableApiExample {

   /**
    *
    * @param args
    * @throws Exception
    */
   public static void main(String[] args) throws Exception {

      StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
      bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
      EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
      StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);
      bsTableEnv.sqlUpdate( ""CREATE TABLE sink_kafka (\n"" +
         ""    status  STRING,\n"" +
         ""    direction STRING,\n"" +
         ""    event_ts TIMESTAMP(3),\n"" +
         ""    WATERMARK FOR event_ts AS event_ts - INTERVAL '5' SECOND\n"" +
         "") WITH (\n"" +
         ""  'connector.type' = 'kafka',       \n"" +
         ""  'connector.version' = 'universal',    \n"" +
         ""  'connector.topic' = 'generated.events2',\n"" +
         ""  'connector.properties.zookeeper.connect' = 'localhost:2181',\n"" +
         ""  'connector.properties.bootstrap.servers' = 'localhost:9092',\n"" +
         ""  'connector.properties.group.id' = 'testGroup',\n"" +
         ""  'format.type'='json',\n"" +
         ""  'update-mode' = 'append'\n"" +
         "")\n"");

   }
}
 {code}
 

And hit the following error:
{code:java}
Exception in thread ""main"" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts'Exception in thread ""main"" org.apache.calcite.runtime.CalciteContextException: From line 5, column 31 to line 5, column 38: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:834) at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:819) at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4841) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5667) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.SqlOperator.deriveType(SqlOperator.java:501) at org.apache.calcite.sql.SqlBinaryOperator.deriveType(SqlBinaryOperator.java:144) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5600) at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5587) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1691) at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1676) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:947) at org.apache.calcite.sql.validate.SqlValidatorImpl.validateParameterizedExpression(SqlValidatorImpl.java:930) at org.apache.flink.table.planner.operations.SqlToOperationConverter.lambda$createTableSchema$8(SqlToOperationConverter.java:509) at java.util.Optional.ifPresent(Optional.java:159) at org.apache.flink.table.planner.operations.SqlToOperationConverter.createTableSchema(SqlToOperationConverter.java:505) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:177) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:130) at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:484) at org.apache.flink.table.examples.java.TableApiExample.main(TableApiExample.java:43)Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Unknown identifier 'event_ts' at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463) at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572) ... 25 more {code}"	FLINK	Resolved	1	1	5351	pull-request-available
13320159	MiniBatch doesn't work with FLIP-95 source	"The following Table API streaming job is stuck when enabling mini batching

{code}
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    EnvironmentSettings settings =
        EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);

    // disable mini-batching completely to get a result
    Configuration tableConf = tableEnv.getConfig()
        .getConfiguration();
    tableConf.setString(""table.exec.mini-batch.enabled"", ""true"");
    tableConf.setString(""table.exec.mini-batch.allow-latency"", ""5 s"");
    tableConf.setString(""table.exec.mini-batch.size"", ""5000"");
    tableConf.setString(""table.optimizer.agg-phase-strategy"", ""TWO_PHASE"");

    tableEnv.executeSql(
        ""CREATE TABLE input_table (""
            + ""location STRING, ""
            + ""population INT""
            + "") WITH (""
            + ""'connector' = 'kafka', ""
            + ""'topic' = 'kafka_batching_input', ""
            + ""'properties.bootstrap.servers' = 'localhost:9092', ""
            + ""'format' = 'csv', ""
            + ""'scan.startup.mode' = 'earliest-offset'""
            + "")"");

    tableEnv.executeSql(
        ""CREATE TABLE result_table WITH ('connector' = 'print') LIKE input_table (EXCLUDING OPTIONS)"");

    tableEnv
        .from(""input_table"")
        .groupBy($(""location""))
        .select($(""location"").cast(DataTypes.CHAR(2)).as(""location""), $(""population"").sum().as(""population""))
        .executeInsert(""result_table"");
{code}

I am using a pre-populated Kafka topic called {{kafka_batching_input}} with these elements:
{code}
""Berlin"",1
""Berlin"",2
{code}"	FLINK	Closed	1	1	5351	pull-request-available
13221348	Support code generation for all Blink built-in functions and operators	"Support code generation for built-in functions and operators. 

FLINK-11788 has supported some of the operators. This issue is aiming to complement the functions and operators supported in Flink SQL.

This should inlclude: CONCAT, LIKE, SUBSTRING, UPPER, LOWER, and so on."	FLINK	Closed	3	2	5351	pull-request-available
13290444	Improve default flush strategy for Elasticsearch sink to make it work out-of-box	"Currently, Elasticsearch sink provides 3 flush options: 

{code:java}
'connector.bulk-flush.max-actions' = '42'
'connector.bulk-flush.max-size' = '42 mb'
'connector.bulk-flush.interval' = '60000'
{code}

All of them are optional and have no default value in Flink side [1]. But flush actions and flush size have a default value {{1000}} and {{5mb}} in Elasticsearch client [2]. This results in some surprising behavior that no results are outputed by default, see user report [3]. Because it has to wait for 1000 records however there is no so many records in the testing. 

This will also be a potential ""problem"" in production. Because if it's a low throughout job, soem data may take a very long time to be visible in the elasticsearch. 

So we propose to have a default flush '1s' interval  and '1000' rows and '2mb' size for ES sink flush. This only applies to new ES sink options:

{code}
'sink.bulk-flush.max-actions' = '1000'
'sink.bulk-flush.max-size' = '2mb'
'sink.bulk-flush.interval' = '1s'
{code}


[1]: https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java#L357-L356
[2]: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk-processor.html
[3]: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Should-I-use-a-Sink-or-Connector-Or-Both-td33352.html"	FLINK	Closed	2	4	5351	usability
13314196	Fix StateMigrationException because RetractableTopNFunction#ComparatorWrapper might be incompatible	"We found that in SQL jobs using ""Top-N"" functionality provided by the blink planner, the job state cannot be retrieved because of ""incompatible"" state serializers (in fact they are compatible).

The error log is displayed like below
{panel:title=taskmanager.log}
2020-06-30 09:19:32.089 [Rank(strategy=[RetractStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[appkey, serverid], orderBy=[quantity DESC], select=[appkey, serverid,  quantity]) (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task  - Rank(strategy=[RetractStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[appkey, serverid], orderBy=[quantity DESC], select=[appkey, serverid, oid, quantity]) (1/1) (bd4d2e4327efac57dc70e220b8de460b) switched from RUNNING to FAILED.
java.lang.RuntimeException: Error while getting state
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:62)
        at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:144)
        at org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.open(RetractableTopNFunction.java:115)
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
        at org.apache.flink.streaming.api.operators.KeyedProcessOperator.open(KeyedProcessOperator.java:57)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:990)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.StateMigrationException: The new state serializer cannot be incompatible.
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.updateRestoredStateMetaInfo(RocksDBKeyedStateBackend.java:543)
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:491)
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:652)
        at org.apache.flink.runtime.state.KeyedStateFactory.createInternalState(KeyedStateFactory.java:47)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory.createStateAndWrapWithTtlIfEnabled(TtlStateFactory.java:72)
        at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getOrCreateKeyedState(AbstractKeyedStateBackend.java:279)
        at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getPartitionedState(AbstractKeyedStateBackend.java:328)
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getPartitionedState(DefaultKeyedStateStore.java:124)
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:60)
        ... 13 more{panel}
 
After careful debugging, it is found to be an issue with the compatibility check of type serializers.
 
In short, during checkpointing, Flink serializes _SortedMapSerializer_ by creating a _SortedMapSerializerSnapshot_ object, and the original comparator is encapsulated within the object (here we call it _StreamExecSortComparator$579_).
 
At restoration, the object is read and restored as normal. However, during the construction of RetractableTopNFunction instance, another Comparator is provided by Flink as an argument (we call it _StreamExecSortComparator$626_), and it is later used in the _ValueStateDescriptor_ which acts like a key to the state store.
 
Here comes the problem: when the newly-restored Flink program tries to access state (_getState_) through the previously mentioned _ValueStateDescriptor_, the State Backend firstly detects whether the provided comparator in state descriptor is compatible with the one in snapshot, eventually the logic goes to the _equals_ method at _RetractableTopNFunction.ComparatorWrapper_ class.
 
In the equals method, here is a code snippet:
{code:java}
return generatedRecordComparator.getClassName().equals(oGeneratedComparator.getClassName()) &&
      generatedRecordComparator.getCode().equals(oGeneratedComparator.getCode()) &&
      Arrays.equals(generatedRecordComparator.getReferences(), oGeneratedComparator.getReferences());
{code}
After debugging, we found that the class name of comparator within snapshot is _StreamExecSortComparator$579_, and the class name of comparator provided in the new job is _StreamExecSortComparator$626_, hence this method always returns false, even though actually they are indeed compatible (acts the same). Also, because the code in each generator is generated independently, the corresponding varaibles within the two comparators are highly likely to be different (_isNullA$581_ vs _isNullA$682_).
 
Hence we believe that the implementation of equals method has serious flaws, and should be addressed in later releases."	FLINK	Closed	3	1	5351	pull-request-available
13335158	Support window TVF based window aggreagte in planner	"Support window TVF based window aggreagte in planner.  We will introduce new physical nodes for {{StreamExecWindowPropertyAggregate}} and {{StreamExecWindowAssigner}}, and optimize them into {{StreamExecGroupWindowAggregate}} if possible. 
"	FLINK	Closed	3	7	5351	pull-request-available
13301828	toRetractStream doesn't work correctly with Pojo conversion class	"The toRetractStream(table, Pojo.class) does not map the query columns properly to the pojo fields.

This either leads to exceptions due to type incompatibility or simply incorrect results.

It can be simple reproduced by the following test code:
{code:java}
@Test
public void testRetract() throws Exception {
 EnvironmentSettings settings = EnvironmentSettings
 .newInstance()
 .useBlinkPlanner()
 .inStreamingMode()
 .build();

 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 StreamTableEnvironment tableEnv = StreamTableEnvironment
 .create(StreamExecutionEnvironment.getExecutionEnvironment(), settings);

 tableEnv.createTemporaryView(""person"", env.fromElements(new Person()));
 tableEnv.toRetractStream(tableEnv.sqlQuery(""select name, age from person""), Person.class).print();
 tableEnv.execute(""Test"");

}

public static class Person {
 public String name = ""bob"";
 public int age = 1;
}{code}
Runtime Error:
{noformat}
java.lang.ClassCastException: org.apache.flink.table.dataformat.BinaryString cannot be cast to java.lang.Integer{noformat}
Changing the query to ""select age,name from person"" in this case would resolve the problem but it also highlights the possible underlying issue."	FLINK	Closed	2	1	5351	pull-request-available
13226867	Support unbounded aggregate in streaming table runtime	"This ticket is aiming to support unbounded aggregate in streaming runtime. This should includes:

1. GroupAggFunction: function that support unbounded aggregate without optimizations
2. MiniBatchGroupAggFunction: function that support unbounded aggregate with minibatch optimization
3. MiniBatchLocalGroupAggFunction & MiniBatchGlobalGroupAggFunction:  function that support unbounded aggregate with local combine optimization"	FLINK	Closed	3	2	5351	pull-request-available
13342191	Accessing a versioned table as of time fails with a cryptic message	"I tried running a query on a versioned table:

{code}
CREATE TABLE RatesHistory (
    currency_time TIMESTAMP(3) METADATA FROM 'timestamp',
    currency STRING,
    rate DECIMAL(38, 10),
    WATERMARK FOR currency_time AS currency_time   -- defines the event time
) WITH (
  'connector' = 'kafka',
  'topic' = 'rates',
  'scan.startup.mode' = 'earliest-offset',
  'properties.bootstrap.servers' = 'kafka:9092',
  'format' = 'json'                                -- this is an append only source
);

SELECT * from RatesHistory FOR SYSTEM_TIME AS OF TIMESTAMP '2020-11-11 13:12:13';
{code}

I understand that might not be supported now, but the exception I got is not very helpful:

{code}
org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=STREAM_PHYSICAL, FlinkRelDistributionTraitDef=any, MiniBatchIntervalTraitDef=None: 0, ModifyKindSetTraitDef=[NONE], UpdateKindTraitDef=[NONE].
Missing conversion is FlinkLogicalSnapshot[convention: LOGICAL -> STREAM_PHYSICAL]
There is 1 empty subset: rel#987:RelSubset#44.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE], the relevant part of the original plan is as follows
977:FlinkLogicalSnapshot(period=[2020-11-11 13:12:13])
  975:FlinkLogicalCalc(subset=[rel#976:RelSubset#43.LOGICAL.any.None: 0.[NONE].[NONE]], select=[CAST(Reinterpret(CAST(timestamp))) AS currency_time, currency, CAST(rate) AS rate])
    962:FlinkLogicalTableSourceScan(subset=[rel#974:RelSubset#42.LOGICAL.any.None: 0.[NONE].[NONE]], table=[[default_catalog, default_database, RatesHistory, watermark=[CAST($2):TIMESTAMP(3)]]], fields=[currency, rate, timestamp])

Root: rel#981:RelSubset#45.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE]
Original rel:
FlinkLogicalLegacySink(subset=[rel#117:RelSubset#4.LOGICAL.any.None: 0.[NONE].[NONE]], name=[`default_catalog`.`default_database`.`_tmp_table_1885690557`], fields=[currency_time, currency, rate]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 127
  FlinkLogicalCalc(subset=[rel#126:RelSubset#3.LOGICAL.any.None: 0.[NONE].[NONE]], select=[CAST(currency_time) AS currency_time, currency, CAST(rate) AS rate]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 129
    FlinkLogicalWatermarkAssigner(subset=[rel#124:RelSubset#2.LOGICAL.any.None: 0.[NONE].[NONE]], rowtime=[currency_time], watermark=[$0]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 123
      FlinkLogicalCalc(subset=[rel#122:RelSubset#1.LOGICAL.any.None: 0.[NONE].[NONE]], select=[CAST(timestamp) AS currency_time, currency, rate]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 128
        FlinkLogicalTableSourceScan(subset=[rel#120:RelSubset#0.LOGICAL.any.None: 0.[NONE].[NONE]], table=[[default_catalog, default_database, RatesHistory]], fields=[currency, rate, timestamp]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}, id = 119

Sets:
Set#42, type: RecordType(VARCHAR(2147483647) currency, DECIMAL(38, 10) rate, TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) timestamp)
	rel#974:RelSubset#42.LOGICAL.any.None: 0.[NONE].[NONE], best=rel#962
		rel#962:FlinkLogicalTableSourceScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, RatesHistory, watermark=[CAST($2):TIMESTAMP(3)]],fields=currency, rate, timestamp), rowcount=1.0E8, cumulative cost={1.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}
	rel#984:RelSubset#42.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE], best=rel#983
		rel#983:StreamExecTableSourceScan.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, RatesHistory, watermark=[CAST($2):TIMESTAMP(3)]],fields=currency, rate, timestamp), rowcount=1.0E8, cumulative cost={1.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}
Set#43, type: RecordType(TIMESTAMP(3) currency_time, VARCHAR(2147483647) currency, DECIMAL(38, 18) rate)
	rel#976:RelSubset#43.LOGICAL.any.None: 0.[NONE].[NONE], best=rel#975
		rel#975:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#974,select=CAST(Reinterpret(CAST(timestamp))) AS currency_time, currency, CAST(rate) AS rate), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}
	rel#986:RelSubset#43.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE], best=rel#985
		rel#985:StreamExecCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#984,select=CAST(Reinterpret(CAST(timestamp))) AS currency_time, currency, CAST(rate) AS rate), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}
Set#44, type: RecordType(TIMESTAMP(3) currency_time, VARCHAR(2147483647) currency, DECIMAL(38, 18) rate)
	rel#978:RelSubset#44.LOGICAL.any.None: 0.[NONE].[NONE], best=rel#977
		rel#977:FlinkLogicalSnapshot.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#976,period=2020-11-11 13:12:13), rowcount=1.0E8, cumulative cost={3.0E8 rows, 2.0E8 cpu, 7.2E9 io, 0.0 network, 0.0 memory}
	rel#987:RelSubset#44.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE], best=null
Set#45, type: RecordType:peek_no_expand(BOOLEAN f0, RecordType:peek_no_expand(TIMESTAMP(3) currency_time, VARCHAR(2147483647) currency, DECIMAL(38, 18) rate) f1)
	rel#980:RelSubset#45.LOGICAL.any.None: 0.[NONE].[NONE], best=rel#979
		rel#979:FlinkLogicalLegacySink.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#978,name=`default_catalog`.`default_database`.`_tmp_table_934371579`,fields=currency_time, currency, rate), rowcount=1.0E8, cumulative cost={4.0E8 rows, 3.0E8 cpu, 7.2E9 io, 0.0 network, 0.0 memory}
	rel#981:RelSubset#45.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE], best=null
		rel#982:AbstractConverter.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#980,convention=STREAM_PHYSICAL,FlinkRelDistributionTraitDef=any,MiniBatchIntervalTraitDef=None: 0,ModifyKindSetTraitDef=[NONE],UpdateKindTraitDef=[NONE]), rowcount=1.0E8, cumulative cost={inf}
		rel#988:StreamExecLegacySink.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#987,name=`default_catalog`.`default_database`.`_tmp_table_934371579`,fields=currency_time, currency, rate), rowcount=1.0E8, cumulative cost={inf}

Graphviz:
digraph G {
	root [style=filled,label=""Root""];
	subgraph cluster42{
		label=""Set 42 RecordType(VARCHAR(2147483647) currency, DECIMAL(38, 10) rate, TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) timestamp)"";
		rel962 [label=""rel#962:FlinkLogicalTableSourceScan\ntable=[default_catalog, default_database, RatesHistory, watermark=[CAST($2):TIMESTAMP(3)]],fields=currency, rate, timestamp\nrows=1.0E8, cost={1.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel983 [label=""rel#983:StreamExecTableSourceScan\ntable=[default_catalog, default_database, RatesHistory, watermark=[CAST($2):TIMESTAMP(3)]],fields=currency, rate, timestamp\nrows=1.0E8, cost={1.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		subset974 [label=""rel#974:RelSubset#42.LOGICAL.any.None: 0.[NONE].[NONE]""]
		subset984 [label=""rel#984:RelSubset#42.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE]""]
	}
	subgraph cluster43{
		label=""Set 43 RecordType(TIMESTAMP(3) currency_time, VARCHAR(2147483647) currency, DECIMAL(38, 18) rate)"";
		rel975 [label=""rel#975:FlinkLogicalCalc\ninput=RelSubset#974,select=CAST(Reinterpret(CAST(timestamp))) AS currency_time, currency, CAST(rate) AS rate\nrows=1.0E8, cost={2.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel985 [label=""rel#985:StreamExecCalc\ninput=RelSubset#984,select=CAST(Reinterpret(CAST(timestamp))) AS currency_time, currency, CAST(rate) AS rate\nrows=1.0E8, cost={2.0E8 rows, 1.0E8 cpu, 3.6E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		subset976 [label=""rel#976:RelSubset#43.LOGICAL.any.None: 0.[NONE].[NONE]""]
		subset986 [label=""rel#986:RelSubset#43.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE]""]
	}
	subgraph cluster44{
		label=""Set 44 RecordType(TIMESTAMP(3) currency_time, VARCHAR(2147483647) currency, DECIMAL(38, 18) rate)"";
		rel977 [label=""rel#977:FlinkLogicalSnapshot\ninput=RelSubset#976,period=2020-11-11 13:12:13\nrows=1.0E8, cost={3.0E8 rows, 2.0E8 cpu, 7.2E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		subset978 [label=""rel#978:RelSubset#44.LOGICAL.any.None: 0.[NONE].[NONE]""]
		subset987 [label=""rel#987:RelSubset#44.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE]"",color=red]
	}
	subgraph cluster45{
		label=""Set 45 RecordType:peek_no_expand(BOOLEAN f0, RecordType:peek_no_expand(TIMESTAMP(3) currency_time, VARCHAR(2147483647) currency, DECIMAL(38, 18) rate) f1)"";
		rel979 [label=""rel#979:FlinkLogicalLegacySink\ninput=RelSubset#978,name=`default_catalog`.`default_database`.`_tmp_table_934371579`,fields=currency_time, currency, rate\nrows=1.0E8, cost={4.0E8 rows, 3.0E8 cpu, 7.2E9 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
		rel982 [label=""rel#982:AbstractConverter\ninput=RelSubset#980,convention=STREAM_PHYSICAL,FlinkRelDistributionTraitDef=any,MiniBatchIntervalTraitDef=None: 0,ModifyKindSetTraitDef=[NONE],UpdateKindTraitDef=[NONE]\nrows=1.0E8, cost={inf}"",shape=box]
		rel988 [label=""rel#988:StreamExecLegacySink\ninput=RelSubset#987,name=`default_catalog`.`default_database`.`_tmp_table_934371579`,fields=currency_time, currency, rate\nrows=1.0E8, cost={inf}"",shape=box]
		subset980 [label=""rel#980:RelSubset#45.LOGICAL.any.None: 0.[NONE].[NONE]""]
		subset981 [label=""rel#981:RelSubset#45.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE]""]
	}
	root -> subset981;
	subset974 -> rel962[color=blue];
	subset984 -> rel983[color=blue];
	subset976 -> rel975[color=blue]; rel975 -> subset974[color=blue];
	subset986 -> rel985[color=blue]; rel985 -> subset984[color=blue];
	subset978 -> rel977[color=blue]; rel977 -> subset976[color=blue];
	subset980 -> rel979[color=blue]; rel979 -> subset978[color=blue];
	subset981 -> rel982; rel982 -> subset980;
	subset981 -> rel988; rel988 -> subset987;
}
{code}"	FLINK	Closed	3	4	5351	pull-request-available
13276527	ScalarOperatorsTest failed in travis	"The travis of release-1.9 failed with the following error:
{code:java}
14:50:19.796 [ERROR] ScalarOperatorsTest>ExpressionTestBase.evaluateExprs:161 Wrong result for: [CASE WHEN (CASE WHEN f2 = 1 THEN CAST('' as INT) ELSE 0 END) is null THEN 'null' ELSE 'not null' END] optimized to: [_UTF-16LE'not null':VARCHAR(8) CHARACTER SET ""UTF-16LE""] expected:<n[]ull> but was:<n[ot n]ull>
{code}
instance: [https://api.travis-ci.org/v3/job/629636107/log.txt]"	FLINK	Resolved	3	1	5351	pull-request-available
13213874	Support multiple languages for the framework of flink-web	"A more detailed description can be found in the proposed doc: https://docs.google.com/document/d/1R1-uDq-KawLB8afQYrczfcoQHjjIhq6tvUksxrfhBl0/edit# 

This step aims to integrate the mulitple-language-plugin for flink-web to support Chinese. All the $pagename.zh.md should be created first in this JIRA but keep the original English contents. A link between  English version and Chinese version should also be considered. 
"	FLINK	Closed	3	7	5351	pull-request-available
13337366	Refactor and merge SupportsComputedColumnPushDown and SupportsWatermarkPushDown interfaces	"As discussed in mailing list [1], the existing SupportsComputedColumnPushDown and SupportsWatermarkPushDown are confusing and hard to implement for connectors. The 
{{SupportsComputedColumnPushDown}} only used for watermark push down. Therefore, 
 combining them into a single interface {{SupportsWatermarkPushDown}} would be better and also work. The proposed interface looks like this:


{code:java}
public interface SupportsWatermarkPushDown {
    
    void applyWatermark(org.apache.flink.table.sources.wmstrategies.WatermarkStrategy<RowData> watermarkStrategy);
    
}
{code}



[1]: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Merge-SupportsComputedColumnPushDown-and-SupportsWatermarkPushDown-td44387.html"	FLINK	Closed	3	7	5351	pull-request-available
13316393	Remove deprecated classes in flink-connector-jdbc	We have refactored the class structure of flink-connector-jdbc module and kept the old API classes in {{org.apache.flink.api.java.io.jdbc}} compatible purpose. Now, it's safe to remove these classes. 	FLINK	Closed	3	4	5351	pull-request-available
13348850	Deliver bootstrap resouces ourselves for website and documentation	"The typesetting of the official website is abnormal. The typesetting seen by users in China is abnormal.Because user can't load https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js CDN.

 !screenshot-1.png! "	FLINK	Closed	3	1	5351	pull-request-available
13371172	KafkaChangelogTableITCase.testKafkaCanalChangelogSource fail due to ConcurrentModificationException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16332&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518&l=7425

{code:java}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
	at java.util.HashMap$ValueIterator.next(HashMap.java:1474)
	at java.util.AbstractCollection.toArray(AbstractCollection.java:141)
	at java.util.ArrayList.addAll(ArrayList.java:583)
	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.lambda$getResults$0(TestValuesRuntimeFunctions.java:108)
	at java.util.HashMap$Values.forEach(HashMap.java:981)
	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.getResults(TestValuesRuntimeFunctions.java:108)
	at org.apache.flink.table.planner.factories.TestValuesTableFactory.getResults(TestValuesTableFactory.java:164)
	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.waitingExpectedResults(KafkaTableTestUtils.java:82)
	at org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.testKafkaCanalChangelogSource(KafkaChangelogTableITCase.java:348)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)

{code}
"	FLINK	Closed	2	1	5351	pull-request-available, test-stability
13337074	Fix Flink SQL throws exception when changelog source contains duplicate change events	We are using Canal to synchornize MySQL data into Kafka, the synchornization delivery is not exactly-once, so there might be dupcliate INSERT/UPDATE/DELETE messages for the same primary key. We are using {{'connecotr' = 'kafka', 'format' = 'canal-json'}} to consume such topic. However, when appling TopN query on this created source table, the TopN operator will thrown exception: {{Caused by: java.lang.RuntimeException: Can not retract a non-existent record. This should never happen.}}	FLINK	Closed	3	7	5351	pull-request-available
13239460	Carry primary key and unique key information in TableSchema	"The primary key and unique key is a standard meta information in SQL. And they are important information for optimization, for example, AggregateRemove, AggregateReduceGrouping and state layout optimization for TopN and Join.

So in this issue, we want to extend {{TableSchema}} to carry more information about primary key and unique keys. So that the TableSource can declare this meta information."	FLINK	Closed	3	2	5351	pull-request-available
13314588	Changelog source can't be insert into upsert sink	"{code:sql}
CREATE TABLE t_pick_order (
      order_no VARCHAR,
      status INT
) WITH (
      'connector' = 'kafka',
      'topic' = 'example',
      'scan.startup.mode' = 'latest-offset',
      'properties.bootstrap.servers' = '172.19.78.32:9092',
      'format' = 'canal-json'
);

CREATE TABLE order_status (
          order_no VARCHAR,
          status INT,
		  PRIMARY KEY (order_no) NOT ENFORCED
) WITH (
          'connector' = 'jdbc',
          'url' = 'jdbc:mysql://xxx:3306/flink_test',
          'table-name' = 'order_status',
          'username' = 'dev',
          'password' = 'xxxx'
);

INSERT INTO order_status SELECT order_no, status FROM t_pick_order ;
{code}

The above queries throw the following exception:

{code}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Provided trait [BEFORE_AND_AFTER] can't satisfy required trait [ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue. 
Current node is TableSourceScan(table=[[default_catalog, default_database, t_pick_order]], fields=[order_no, status])
{code}

It is a bug in planner that we didn't fallback to {{BEFORE_AND_AFTER}} trait when {{ONLY_UPDATE_AFTER}} can't be satisfied. This results in Changelog source can't be used to written into upsert sink. "	FLINK	Closed	1	1	5351	pull-request-available
13305641	Align the behavior between the new and legacy HBase table source	The legacy HBase table source, i.e. {{HBaseTableSource}}, supports projection push down. In order to make the user experience consistent. We should align the behavior and add tests. 	FLINK	Closed	3	7	5351	pull-request-available
13342076	Optimize the exception message of FileSystemTableSink when missing format dependencies	"Current when the format factory failed to load, the following exception would be thrown:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.sink'.

Table options are:

'auto-compaction'='true'
'connector'='filesystem'
'format'='csv'
'path'='file:///tmp/compaction'
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:166)
  at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:362)
  at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:220)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.Iterator$class.foreach(Iterator.scala:891)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:164)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:674)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:757)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664)
  at FileCompactionTest.main(FileCompactionTest.java:147)
Caused by: org.apache.flink.table.api.ValidationException: Please implement at least one of the following formats: BulkWriter.Factory, SerializationSchema, FileSystemFormatFactory.
  at org.apache.flink.table.filesystem.FileSystemTableSink.<init>(FileSystemTableSink.java:124)
  at org.apache.flink.table.filesystem.FileSystemTableFactory.createDynamicTableSink(FileSystemTableFactory.java:83)
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:163)
  ... 18 more
{code}
 

We might directly advice users to check if the format dependency is added or if there are package conflicts so that users would fix this issue faster.

 "	FLINK	Closed	2	1	5351	pull-request-available
13298447	Introduce Debezium format to support reading debezium changelogs	"Introduce {{DebeziumFormatFactory}} and {{DebeziumRowDeserializationSchema}} to read debezium changelogs.


{code:sql}
CREATE TABLE my_table (
  ...
) WITH (
  'connector'='...',  -- e.g. 'kafka'
  'format'='debezium-json',
  'debezium-json.schema-include'='true' -- default false, Debeizum can be configured to include or exclude the message schema
  'debezium-json.ignore-parse-errors'='true' -- default false
);
{code}

"	FLINK	Closed	3	7	5351	pull-request-available
13260693	Support to apply watermark assigner according to the WatermarkSpec in TableSourceTable	"Apply watermark assigner according to the {{WatermarkSpec}} in {{TableSourceTable}} when {{StreamExecTableSourceScan#translateToPlan}}. Ignore TableSource’s {{DefinedRowtimeAttributes}} if {{WatermarkSpec}} exists. 
"	FLINK	Resolved	3	7	5351	pull-request-available
13475083	Introduce write.skip-compaction to skip compaction on write	We want to separate the compaction from the writing, first is support writing without compaction.	FLINK	Closed	3	7	6732	pull-request-available
13437128	UniversalCompaction should pick by size ratio after picking by file num	This way we can avoid the problem of oversized first files	FLINK	Closed	3	7	6732	pull-request-available
13222241	Introduce DeclarativeAggregateFunction and AggsHandlerCodeGenerator to blink planner	"Introduce DeclarativeAggregateFunction: Use java Expressions to write a AggregateFunction, just like Table Api. Then the Table generates the corresponding CodeGenerator code according to the user's Expression logic. This avoids the Java object overhead in AggregateFunction before.

Introduce AggsHandlerCodeGenerator: According to multiple AggregateFunctions, generate a complete aggregation processing Class."	FLINK	Closed	3	2	6732	pull-request-available
13293470	[Umbrella] Introduce datagen, print, blackhole connectors	"Discussion: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Introduce-TableFactory-for-StatefulSequenceSource-td39116.html]

Introduce:
 * DataGeneratorSource
 * DataGenTableSourceFactory
 * PrintTableSinkFactory
 * BlackHoleTableSinkFactory"	FLINK	Closed	3	2	6732	pull-request-available
13279554	SQL client requires both legacy and blink planner to be on the classpath	"Sql client uses directly some of the internal classes of the legacy planner, thus it does not work with only the blink planner on the classpath.

The internal class that's being used is {{org.apache.flink.table.functions.FunctionService}}

This dependency was introduced in FLINK-13195"	FLINK	Closed	3	1	6732	pull-request-available
13277313	Avoid failing when required memory calculation not accurate in BinaryHashTable	"In BinaryHashBucketArea.insertToBucket.

When BinaryHashTable.buildTableFromSpilledPartition.""Build in memory hash table"", it requires memory can put all records, if not, will fail.

Because the linked hash conflict solution, the required memory calculation are not accurate, in this case, we should apply for insufficient memory from heap.

And must be careful, the steal memory should not return to table."	FLINK	Resolved	3	1	6732	pull-request-available
13473527	Check warehouse path in CatalogFactory	"* Not exist, automatic creating the directory.
* Exist but it is not directory, throw exception.
* Exist and it is a directory, pass..."	FLINK	Closed	1	1	6732	pull-request-available
13239258	Support CharType and BinaryType in blink runner	"1.Now we use LogicalType VarcharType to support calcite char type.

2.Subsequent TableApi also generates LogicalType's CharType.

We need real support CharType in internal code gen and computation."	FLINK	Closed	3	2	6732	pull-request-available
13253199	Hive functions can not work in blink planner stream mode	"In flink, specifying the StreamTableEnvironment through the EnvironmentSetting using the blink planner, when using the UDAF in hive in the table API, the error is reported.

The hive function should been make by correct constants and argTypes. Otherwise it will throw an exception. (See HiveAggSqlFunction)
In this isTableAggregate, it just want to check the aggregate function class type, so the better way is get the function instead of make a function.


{code:java}
Caused by: java.lang.NullPointerException
	at java.util.Arrays.stream(Arrays.java:5004)
	at java.util.stream.Stream.of(Stream.java:1000)
	at org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType(TypeConversions.java:67)
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeSetArgs(HiveFunctionUtils.java:59)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:68)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:47)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.isTableAggregate(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.RelExplainUtil$.streamGroupAggregationToString(RelExplainUtil.scala:346)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.explainTerms(StreamExecGroupAggregate.scala:109)
	at org.apache.calcite.rel.AbstractRelNode.explain(AbstractRelNode.java:307)
	at org.apache.calcite.rel.AbstractRelNode.computeDigest(AbstractRelNode.java:388)
	at org.apache.calcite.rel.AbstractRelNode.recomputeDigest(AbstractRelNode.java:351)
	at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:345)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1668)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:846)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:868)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1939)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:129)
	... 60 more
{code}"	FLINK	Resolved	3	1	6732	pull-request-available
13225145	Support appropriate precision and scale processing of Decimal in Blink SQL	"1.Make Decimal's output more precise than random.

2.Let Decimal process closer to mainstream databases such as Hive/SqlServer."	FLINK	Closed	3	4	6732	pull-request-available
13235299	Make time indicator nullable in blink	"SQL: select max(rowtime), count(a) from T

There will be a AssertionError: type mismatch:
aggCall type:
TIMESTAMP(3) NOT NULL
inferred type:
TIMESTAMP(3)

Agg type checking is done before TimeIndicator materializes. So there is a exception.

And before introducing nullable of LogicalType, we should modify this to avoid more potential TypeCheck problems."	FLINK	Closed	3	4	6732	pull-request-available
13347529	Clean useless codes: Never push calcProgram to correlate	projectProgram in StreamPhysicalCorrelateBase never be used.	FLINK	Closed	3	4	6732	pull-request-available
13510712	Supports lookup a partial-update table	"The lookup uses streaming read for reading table. (In TableStreamingReader)
- We should support lookup a partial-update table with full compaction.
- But partial-update table without full compaction, we should throw exception.

"	FLINK	Closed	3	4	6732	pull-request-available
13316614	Hive bundle jar URLs are broken	we should use [https://repo.maven.apache.org/maven2/] instead	FLINK	Closed	3	4	6732	pull-request-available
13438823	Add version field to manifest entries	Metadata related format should be added to the version field for better forward compatibility in the future.	FLINK	Closed	3	7	6732	pull-request-available
13244318	Introduce type inference for hive functions in blink	See some conversation in [https://github.com/apache/flink/pull/8920]	FLINK	Closed	3	4	6732	pull-request-available
13342092	File Source lost data when reading from directories created by FileSystemTableSink with JSON format	"When testing the compaction functionality of the FileSystemTableSink, I found that when using json format, the produced directories could not be read correctly by the file source, namely only a part of records are read.


By checking the produced directories, the number of the records in it is the same as expected, thus it seems to be the issue of the source side.

 

The issue only exists for JSON format.

The data is produced by [FileCompactionTest|https://github.com/gaoyunhaii/flink1.12test/blob/main/src/main/java/FileCompactionTest.java] and read by  [FileCompactionCheckTest|https://github.com/gaoyunhaii/flink1.12test/blob/main/src/main/java/FileCompactionCheckTest.java] . An example directories tar file of 8000 records are also attached.

 "	FLINK	Closed	1	1	6732	pull-request-available
13289467	Hide hive version to avoid user confuse	Version in Yaml/HiveCatalog needs to be consistent with the dependencies version. There are three places: version in metastore, version in dependencies, version in Yaml/HiveCatalog, users are easy to make mistakes.	FLINK	Resolved	3	1	6732	pull-request-available
13288616	 connector on hive 2.0.1 don't  support type conversion from STRING to VARCHAR	" it threw  exception  when we query hive 2.0.1 by flink 1.10.0

 Exception stack：

org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=50, backoffTimeMS=10000)
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
 at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180)
 at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:484)
 at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
 at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
 at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
 at akka.actor.ActorCell.invoke(ActorCell.scala:561)
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
 at akka.dispatch.Mailbox.run(Mailbox.scala:225)
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:76)
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:123)
 at org.apache.flink.orc.OrcSplitReader.<init>(OrcSplitReader.java:73)
 at org.apache.flink.orc.OrcColumnarRowSplitReader.<init>(OrcColumnarRowSplitReader.java:55)
 at org.apache.flink.orc.OrcSplitReaderUtil.genPartColumnarRowReader(OrcSplitReaderUtil.java:96)
 at org.apache.flink.connectors.hive.read.HiveVectorizedOrcSplitReader.<init>(HiveVectorizedOrcSplitReader.java:65)
 at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:117)
 at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:56)
 at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:85)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)
Caused by: java.lang.reflect.InvocationTargetException
 at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.commons.lang3.reflect.MethodUtils.invokeExactMethod(MethodUtils.java:204)
 at org.apache.commons.lang3.reflect.MethodUtils.invokeExactMethod(MethodUtils.java:165)
 at org.apache.flink.orc.shim.OrcShimV200.createRecordReader(OrcShimV200.java:74)
 ... 11 more
Caused by: java.io.IOException: ORC does not support type conversion from STRING to VARCHAR
 at org.apache.hadoop.hive.ql.io.orc.SchemaEvolution.validateAndCreate(SchemaEvolution.java:96)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:255)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:79)
 at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$Builder.build(RecordReaderImpl.java:236)
 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:680)
 ... 17 more"	FLINK	Resolved	3	1	6732	pull-request-available
13424863	Introduce Table Store Flink Source	Introduce FLIP-27 source implementation for table file store.	FLINK	Closed	3	7	6732	pull-request-available
13441353	Record schema on filesystem path	"We can store the schema on the path of the table store, which includes type, options, etc.

This schema should be in a format that supports evolution, which means that the fields contain id information."	FLINK	Closed	3	7	6732	pull-request-available
13241425	UserDefinedFunctionUtils should distinguish overload any parameters methods in blink	"Some function like:
{code:java}
class TableFunc7 extends TableFunction[MyAnyObject3] {

  def eval(row: MyAnyObject1): Unit = {
  }

  def eval(row: jMyAnyObject2): Unit = {
  }
}{code}
UserDefinedFunctionUtils can't distinguish these two methods, and throw exception."	FLINK	Closed	3	1	6732	pull-request-available
13310026	Bundle flink-csv and flink-json jars in lib	"The biggest problem for distributions I see is the variety of problems caused by users' lack of format dependency.
 These three formats are very small and no third party dependence, and they are widely used by table users.
 Actually, we don't have any other built-in table formats now..
 * flink-csv-1.10.0.jar
 * flink-json-1.10.0.jar

 We can just bundle them in ""flink/lib/"".
 It not solve all problems and it is independent of ""fat"" and ""slim"". But also improve usability."	FLINK	Closed	1	1	6732	pull-request-available
13264979	Enable partition statistics in blink planner	We need update statistics after partition pruning in PushPartitionIntoTableSourceScanRule.	FLINK	Closed	3	7	6732	pull-request-available
13286180	Statistics zero should be unknown in HiveCatalog	"In hive, treat statistics zero as unknown, but in Flink HiveCatalog, treat zero as real value.

This lead wrong inputs to CBO.

Previous discussed in [https://github.com/apache/flink/pull/10380]"	FLINK	Resolved	1	1	6732	pull-request-available
13295166	SingleDirectoryWriter should not produce file when no input record	"SingleDirectoryWriter should not produce file when no input record.

Now it will produce a empty file, we should avoid this."	FLINK	Resolved	3	7	6732	pull-request-available
13311654	Lack LICENSE.protobuf in flink-sql-orc	flink-sql-orc bundle protobuf but not include LICENSE.protobuf.	FLINK	Closed	1	1	6732	pull-request-available
13298802	Table with processing time attribute can not be read from Hive catalog	"DDL:
{code}
CREATE TABLE PROD_LINEITEM (
  L_ORDERKEY       INTEGER,
  L_PARTKEY        INTEGER,
  L_SUPPKEY        INTEGER,
  L_LINENUMBER     INTEGER,
  L_QUANTITY       DOUBLE,
  L_EXTENDEDPRICE  DOUBLE,
  L_DISCOUNT       DOUBLE,
  L_TAX            DOUBLE,
  L_CURRENCY       STRING,
  L_RETURNFLAG     STRING,
  L_LINESTATUS     STRING,
  L_ORDERTIME      TIMESTAMP(3),
  L_SHIPINSTRUCT   STRING,
  L_SHIPMODE       STRING,
  L_COMMENT        STRING,
  WATERMARK FOR L_ORDERTIME AS L_ORDERTIME - INTERVAL '5' MINUTE,
  L_PROCTIME       AS PROCTIME()
) WITH (
  'connector.type' = 'kafka',
  'connector.version' = 'universal',
  'connector.topic' = 'Lineitem',
  'connector.properties.zookeeper.connect' = 'not-needed',
  'connector.properties.bootstrap.servers' = 'kafka:9092',
  'connector.startup-mode' = 'earliest-offset',
  'format.type' = 'csv',
  'format.field-delimiter' = '|'
);
{code}

Query:
{code}
SELECT * FROM prod_lineitem;
{code}

Result:
{code}
[ERROR] Could not execute SQL statement. Reason:
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType(INTEGER L_ORDERKEY, INTEGER L_PARTKEY, INTEGER L_SUPPKEY, INTEGER L_LINENUMBER, DOUBLE L_QUANTITY, DOUBLE L_EXTENDEDPRICE, DOUBLE L_DISCOUNT, DOUBLE L_TAX, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_CURRENCY, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_RETURNFLAG, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_LINESTATUS, TIME ATTRIBUTE(ROWTIME) L_ORDERTIME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPINSTRUCT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPMODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_COMMENT, TIMESTAMP(3) NOT NULL L_PROCTIME) NOT NULL
converted type:
RecordType(INTEGER L_ORDERKEY, INTEGER L_PARTKEY, INTEGER L_SUPPKEY, INTEGER L_LINENUMBER, DOUBLE L_QUANTITY, DOUBLE L_EXTENDEDPRICE, DOUBLE L_DISCOUNT, DOUBLE L_TAX, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_CURRENCY, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_RETURNFLAG, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_LINESTATUS, TIME ATTRIBUTE(ROWTIME) L_ORDERTIME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPINSTRUCT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPMODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_COMMENT, TIME ATTRIBUTE(PROCTIME) NOT NULL L_PROCTIME) NOT NULL
rel:
LogicalProject(L_ORDERKEY=[$0], L_PARTKEY=[$1], L_SUPPKEY=[$2], L_LINENUMBER=[$3], L_QUANTITY=[$4], L_EXTENDEDPRICE=[$5], L_DISCOUNT=[$6], L_TAX=[$7], L_CURRENCY=[$8], L_RETURNFLAG=[$9], L_LINESTATUS=[$10], L_ORDERTIME=[$11], L_SHIPINSTRUCT=[$12], L_SHIPMODE=[$13], L_COMMENT=[$14], L_PROCTIME=[$15])
  LogicalWatermarkAssigner(rowtime=[L_ORDERTIME], watermark=[-($11, 300000:INTERVAL MINUTE)])
    LogicalProject(L_ORDERKEY=[$0], L_PARTKEY=[$1], L_SUPPKEY=[$2], L_LINENUMBER=[$3], L_QUANTITY=[$4], L_EXTENDEDPRICE=[$5], L_DISCOUNT=[$6], L_TAX=[$7], L_CURRENCY=[$8], L_RETURNFLAG=[$9], L_LINESTATUS=[$10], L_ORDERTIME=[$11], L_SHIPINSTRUCT=[$12], L_SHIPMODE=[$13], L_COMMENT=[$14], L_PROCTIME=[PROCTIME()])
      LogicalTableScan(table=[[hcat, default, prod_lineitem, source: [KafkaTableSource(L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_LINENUMBER, L_QUANTITY, L_EXTENDEDPRICE, L_DISCOUNT, L_TAX, L_CURRENCY, L_RETURNFLAG, L_LINESTATUS, L_ORDERTIME, L_SHIPINSTRUCT, L_SHIPMODE, L_COMMENT)]]])
{code}"	FLINK	Closed	1	1	6732	pull-request-available
13280094	Cannot use generic types as the result of an AggregateFunction in Blink planner	"It is not possible to use a GenericTypeInfo for a result type of an {{AggregateFunction}} in a retract mode with state cleaning disabled.

{code}

  @Test
  def testGenericTypes(): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val setting = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()
    val tEnv = StreamTableEnvironment.create(env, setting)
    val t = env.fromElements(1, 2, 3).toTable(tEnv, 'a)

    val results = t
      .select(new GenericAggregateFunction()('a))
      .toRetractStream[Row]

    val sink = new TestingRetractSink
    results.addSink(sink).setParallelism(1)
    env.execute()
  }

class RandomClass(var i: Int)

class GenericAggregateFunction extends AggregateFunction[java.lang.Integer, RandomClass] {
  override def getValue(accumulator: RandomClass): java.lang.Integer = accumulator.i

  override def createAccumulator(): RandomClass = new RandomClass(0)

  override def getResultType: TypeInformation[java.lang.Integer] = new GenericTypeInfo[Integer](classOf[Integer])

  override def getAccumulatorType: TypeInformation[RandomClass] = new GenericTypeInfo[RandomClass](
    classOf[RandomClass])

  def accumulate(acc: RandomClass, value: Int): Unit = {
    acc.i = value
  }

  def retract(acc: RandomClass, value: Int): Unit = {
    acc.i = value
  }

  def resetAccumulator(acc: RandomClass): Unit = {
    acc.i = 0
  }
}
{code}

The code above fails with:

{code}
Caused by: java.lang.UnsupportedOperationException: BinaryGeneric cannot be compared
	at org.apache.flink.table.dataformat.BinaryGeneric.equals(BinaryGeneric.java:77)
	at GroupAggValueEqualiser$17.equalsWithoutHeader(Unknown Source)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:177)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:170)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
{code}

This is related to FLINK-13702"	FLINK	Resolved	3	1	6732	pull-request-available
13438825	Adjust default value of compaction and compress	"* By default, numLevels = numSortedRunMax + 1, this ensures that the compaction does not fall to level 0, but at least to level 1
 * orc default compress is lz4, zlib is too slow"	FLINK	Closed	3	7	6732	pull-request-available
13227639	Support e2e limit, sortLimit, rank, union in blink batch	"Support limit and add limit it cases to blink batch.

Support sortLimit and add sortLimit it cases to blink batch.

Support rank and add rank it cases to blink batch.

Support union and add union it cases to blink batch."	FLINK	Closed	3	2	6732	pull-request-available
13335135	Introduce BulkFormatFactory to integrate new FileSource to table	"Introduce BulkFormatFactory: BulkFormat<T> createBulkFormat(context).

Filesystem connector use this factory to create BulkFormat, and use new FileSource to read files."	FLINK	Closed	3	7	6732	pull-request-available
13244486	TemporalTypesTest randomly failed on travis	"TemporalTypesTest>ExpressionTestBase.evaluateExprs:154 Wrong result for: [CURRENT_DATE] optimized to: [CURRENT_DATE] expected:<2019-07-1[2]> but was:<2019-07-1[1]>

 

more details in: [https://api.travis-ci.org/v3/job/557582157/log.txt]"	FLINK	Closed	2	1	6732	test-stability
13310086	StreamingFileCommitter should not use fs modification time for proc committer	"The filesystem path modification time is the last modification time of all files in directory. 

If there is new data in partition all the time, it will never end.

We should use earlier time for committer."	FLINK	Closed	2	1	6732	pull-request-available
13432569	KafkaWriterITCase.testMetadataPublisher  failed on azure	"{code:java}
022-03-07T13:43:34.3882626Z Mar 07 13:43:34 [ERROR] org.apache.flink.connector.kafka.sink.KafkaWriterITCase.testMetadataPublisher  Time elapsed: 0.205 s  <<< FAILURE!
2022-03-07T13:43:34.3883743Z Mar 07 13:43:34 java.lang.AssertionError: 
2022-03-07T13:43:34.3884867Z Mar 07 13:43:34 
2022-03-07T13:43:34.3885412Z Mar 07 13:43:34 Expecting actual:
2022-03-07T13:43:34.3886464Z Mar 07 13:43:34   [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.3887361Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.3888222Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.3888833Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.3892032Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.3893140Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.3893849Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.3895077Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.3895779Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.3896423Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.3897164Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.3897792Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.3949208Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.3950956Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.3952287Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.3954341Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.3955834Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.3957048Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.3958287Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.3959519Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.3960798Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.3961973Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.3963302Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.3964563Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.3966941Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.3968246Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.3969452Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.3970656Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.3971853Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.3974163Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.3975441Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.3976380Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.3977278Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.3978197Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.3979120Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.3980051Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.3981017Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.3981952Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.3982975Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.3983882Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.3984940Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.3985838Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.3986702Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.3987661Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.3988564Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.3989444Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.3990347Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.3991206Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.3992100Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.3993091Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.3994383Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.3995399Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.3996287Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.3997270Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.3998095Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.3998915Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.3999752Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4000838Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4001693Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4002979Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4003852Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4004693Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4005626Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4006473Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4007269Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4008109Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4008948Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4009771Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4010608Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4011436Z Mar 07 13:43:34     ""testMetadataPublisher-0@69""]
2022-03-07T13:43:34.4011993Z Mar 07 13:43:34 to be equal to:
2022-03-07T13:43:34.4012951Z Mar 07 13:43:34   [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4013825Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4014670Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4015693Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4016554Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4017408Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4018257Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4019101Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4019961Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4020826Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4021702Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4022572Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4023663Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4024553Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4025531Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4026421Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4027296Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4028132Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4029000Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4029870Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4030733Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4031578Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4032456Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4033487Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4034372Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4035335Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4036217Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4037098Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4037963Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4038783Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4039621Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4040479Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4041774Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4042684Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4043595Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4044311Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4045166Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4045886Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4046625Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4047419Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4048409Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4049167Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4049936Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4050682Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4051494Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4052280Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4053217Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4053988Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4054891Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4055671Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4056416Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4057149Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4058037Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4059007Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4059996Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4060761Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4061524Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4062243Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4063142Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4063867Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4064594Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4065485Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4066207Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4066926Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4067666Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4068423Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4069208Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4069987Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4070747Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4071503Z Mar 07 13:43:34     ""testMetadataPublisher-0@69"",
2022-03-07T13:43:34.4072260Z Mar 07 13:43:34     ""testMetadataPublisher-0@70"",
2022-03-07T13:43:34.4073205Z Mar 07 13:43:34     ""testMetadataPublisher-0@71"",
2022-03-07T13:43:34.4073954Z Mar 07 13:43:34     ""testMetadataPublisher-0@72"",
2022-03-07T13:43:34.4074687Z Mar 07 13:43:34     ""testMetadataPublisher-0@73"",
2022-03-07T13:43:34.4075576Z Mar 07 13:43:34     ""testMetadataPublisher-0@74"",
2022-03-07T13:43:34.4076438Z Mar 07 13:43:34     ""testMetadataPublisher-0@75"",
2022-03-07T13:43:34.4077280Z Mar 07 13:43:34     ""testMetadataPublisher-0@76"",
2022-03-07T13:43:34.4078103Z Mar 07 13:43:34     ""testMetadataPublisher-0@77"",
2022-03-07T13:43:34.4078906Z Mar 07 13:43:34     ""testMetadataPublisher-0@78"",
2022-03-07T13:43:34.4079740Z Mar 07 13:43:34     ""testMetadataPublisher-0@79"",
2022-03-07T13:43:34.4080568Z Mar 07 13:43:34     ""testMetadataPublisher-0@80"",
2022-03-07T13:43:34.4081382Z Mar 07 13:43:34     ""testMetadataPublisher-0@81"",
2022-03-07T13:43:34.4082183Z Mar 07 13:43:34     ""testMetadataPublisher-0@82"",
2022-03-07T13:43:34.4083434Z Mar 07 13:43:34     ""testMetadataPublisher-0@83"",
2022-03-07T13:43:34.4084219Z Mar 07 13:43:34     ""testMetadataPublisher-0@84"",
2022-03-07T13:43:34.4085164Z Mar 07 13:43:34     ""testMetadataPublisher-0@85"",
2022-03-07T13:43:34.4085981Z Mar 07 13:43:34     ""testMetadataPublisher-0@86"",
2022-03-07T13:43:34.4086783Z Mar 07 13:43:34     ""testMetadataPublisher-0@87"",
2022-03-07T13:43:34.4087594Z Mar 07 13:43:34     ""testMetadataPublisher-0@88"",
2022-03-07T13:43:34.4088399Z Mar 07 13:43:34     ""testMetadataPublisher-0@89"",
2022-03-07T13:43:34.4089197Z Mar 07 13:43:34     ""testMetadataPublisher-0@90"",
2022-03-07T13:43:34.4090016Z Mar 07 13:43:34     ""testMetadataPublisher-0@91"",
2022-03-07T13:43:34.4091073Z Mar 07 13:43:34     ""testMetadataPublisher-0@92"",
2022-03-07T13:43:34.4091883Z Mar 07 13:43:34     ""testMetadataPublisher-0@93"",
2022-03-07T13:43:34.4092685Z Mar 07 13:43:34     ""testMetadataPublisher-0@94"",
2022-03-07T13:43:34.4093638Z Mar 07 13:43:34     ""testMetadataPublisher-0@95"",
2022-03-07T13:43:34.4094429Z Mar 07 13:43:34     ""testMetadataPublisher-0@96"",
2022-03-07T13:43:34.4095340Z Mar 07 13:43:34     ""testMetadataPublisher-0@97"",
2022-03-07T13:43:34.4096112Z Mar 07 13:43:34     ""testMetadataPublisher-0@98"",
2022-03-07T13:43:34.4096909Z Mar 07 13:43:34     ""testMetadataPublisher-0@99""]
2022-03-07T13:43:34.4097650Z Mar 07 13:43:34 when recursively comparing field by field, but found the following difference:
2022-03-07T13:43:34.4098330Z Mar 07 13:43:34 
2022-03-07T13:43:34.4098903Z Mar 07 13:43:34 Top level actual and expected objects differ:
2022-03-07T13:43:34.4099837Z Mar 07 13:43:34 - actual value  : [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4100698Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4101474Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4102387Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4103366Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4104138Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4105064Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4105984Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4106818Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4107589Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4108447Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4109302Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4110170Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4111034Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4111918Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4113454Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4114503Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4115478Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4116351Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4117223Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4118093Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4118957Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4119823Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4120664Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4121523Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4122384Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4123353Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4124229Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4125165Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4126006Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4126874Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4127731Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4129264Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4130106Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4130969Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4131840Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4132709Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4133674Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4134532Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4135442Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4136523Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4137388Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4138251Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4139110Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4139970Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4140817Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4141681Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4142544Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4143507Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4144371Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4145318Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4146161Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4147023Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4147874Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4148716Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4149571Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4150426Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4151259Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4152128Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4153090Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4153968Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4154903Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4155764Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4156608Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4157477Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4158334Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4159195Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4160059Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4160917Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4161745Z Mar 07 13:43:34     ""testMetadataPublisher-0@69""]
2022-03-07T13:43:34.4162674Z Mar 07 13:43:34 - expected value: [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4163702Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4164568Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4165546Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4166403Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4167243Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4168103Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4168961Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4169823Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4170689Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4171559Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4172403Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4173359Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4174233Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4175394Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4176266Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4177121Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4177964Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4178833Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4179694Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4180556Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4181413Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4182411Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4183347Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4184211Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4185151Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4186023Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4186901Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4187764Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4188609Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4189479Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4190338Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4191198Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4192060Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4193009Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4193872Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4194809Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4195680Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4196549Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4197410Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4198275Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4199110Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4199970Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4200826Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4201685Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4202544Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4203506Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4204356Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4205303Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4206169Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4207030Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4207895Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4208754Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4209606Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4210468Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4211327Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4212182Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4213139Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4214002Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4214920Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4215784Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4216653Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4217523Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4218387Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4219251Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4220089Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4222481Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4223512Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4224379Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4225326Z Mar 07 13:43:34     ""testMetadataPublisher-0@69"",
2022-03-07T13:43:34.4226187Z Mar 07 13:43:34     ""testMetadataPublisher-0@70"",
2022-03-07T13:43:34.4227046Z Mar 07 13:43:34     ""testMetadataPublisher-0@71"",
2022-03-07T13:43:34.4227888Z Mar 07 13:43:34     ""testMetadataPublisher-0@72"",
2022-03-07T13:43:34.4228748Z Mar 07 13:43:34     ""testMetadataPublisher-0@73"",
2022-03-07T13:43:34.4229755Z Mar 07 13:43:34     ""testMetadataPublisher-0@74"",
2022-03-07T13:43:34.4230618Z Mar 07 13:43:34     ""testMetadataPublisher-0@75"",
2022-03-07T13:43:34.4231483Z Mar 07 13:43:34     ""testMetadataPublisher-0@76"",
2022-03-07T13:43:34.4232344Z Mar 07 13:43:34     ""testMetadataPublisher-0@77"",
2022-03-07T13:43:34.4233292Z Mar 07 13:43:34     ""testMetadataPublisher-0@78"",
2022-03-07T13:43:34.4234161Z Mar 07 13:43:34     ""testMetadataPublisher-0@79"",
2022-03-07T13:43:34.4235139Z Mar 07 13:43:34     ""testMetadataPublisher-0@80"",
2022-03-07T13:43:34.4236004Z Mar 07 13:43:34     ""testMetadataPublisher-0@81"",
2022-03-07T13:43:34.4236856Z Mar 07 13:43:34     ""testMetadataPublisher-0@82"",
2022-03-07T13:43:34.4237713Z Mar 07 13:43:34     ""testMetadataPublisher-0@83"",
2022-03-07T13:43:34.4238554Z Mar 07 13:43:34     ""testMetadataPublisher-0@84"",
2022-03-07T13:43:34.4239420Z Mar 07 13:43:34     ""testMetadataPublisher-0@85"",
2022-03-07T13:43:34.4240278Z Mar 07 13:43:34     ""testMetadataPublisher-0@86"",
2022-03-07T13:43:34.4241143Z Mar 07 13:43:34     ""testMetadataPublisher-0@87"",
2022-03-07T13:43:34.4242004Z Mar 07 13:43:34     ""testMetadataPublisher-0@88"",
2022-03-07T13:43:34.4242931Z Mar 07 13:43:34     ""testMetadataPublisher-0@89"",
2022-03-07T13:43:34.4243787Z Mar 07 13:43:34     ""testMetadataPublisher-0@90"",
2022-03-07T13:43:34.4244657Z Mar 07 13:43:34     ""testMetadataPublisher-0@91"",
2022-03-07T13:43:34.4245634Z Mar 07 13:43:34     ""testMetadataPublisher-0@92"",
2022-03-07T13:43:34.4246508Z Mar 07 13:43:34     ""testMetadataPublisher-0@93"",
2022-03-07T13:43:34.4247365Z Mar 07 13:43:34     ""testMetadataPublisher-0@94"",
2022-03-07T13:43:34.4248219Z Mar 07 13:43:34     ""testMetadataPublisher-0@95"",
2022-03-07T13:43:34.4249064Z Mar 07 13:43:34     ""testMetadataPublisher-0@96"",
2022-03-07T13:43:34.4249919Z Mar 07 13:43:34     ""testMetadataPublisher-0@97"",
2022-03-07T13:43:34.4250774Z Mar 07 13:43:34     ""testMetadataPublisher-0@98"",
2022-03-07T13:43:34.4251627Z Mar 07 13:43:34     ""testMetadataPublisher-0@99""]
2022-03-07T13:43:34.4252463Z Mar 07 13:43:34 actual and expected values are collections of different size, actual size=70 when expected size=100
2022-03-07T13:43:34.4253310Z Mar 07 13:43:34 
2022-03-07T13:43:34.4254001Z Mar 07 13:43:34 The recursive comparison was performed with this configuration:
2022-03-07T13:43:34.4255270Z Mar 07 13:43:34 - no overridden equals methods were used in the comparison (except for java types)
2022-03-07T13:43:34.4256419Z Mar 07 13:43:34 - these types were compared with the following comparators:
2022-03-07T13:43:34.4257510Z Mar 07 13:43:34   - java.lang.Double -> DoubleComparator[precision=1.0E-15]
2022-03-07T13:43:34.4258541Z Mar 07 13:43:34   - java.lang.Float -> FloatComparator[precision=1.0E-6]
2022-03-07T13:43:34.4259606Z Mar 07 13:43:34   - java.nio.file.Path -> lexicographic comparator (Path natural order)
2022-03-07T13:43:34.4261366Z Mar 07 13:43:34 - actual and expected objects and their fields were compared field by field recursively even if they were not of the same type, this allows for example to compare a Person to a PersonDto (call strictTypeChecking(true) to change that behavior).
2022-03-07T13:43:34.4262493Z Mar 07 13:43:34 
2022-03-07T13:43:34.4263436Z Mar 07 13:43:34 	at org.apache.flink.connector.kafka.sink.KafkaWriterITCase.testMetadataPublisher(KafkaWriterITCase.java:236)
2022-03-07T13:43:34.4264640Z Mar 07 13:43:34 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-07T13:43:34.4265699Z Mar 07 13:43:34 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-07T13:43:34.4266770Z Mar 07 13:43:34 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-07T13:43:34.4267730Z Mar 07 13:43:34 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-07T13:43:34.4268684Z Mar 07 13:43:34 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-03-07T13:43:34.4269757Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-03-07T13:43:34.4271099Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-03-07T13:43:34.4272304Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-03-07T13:43:34.4273543Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-03-07T13:43:34.4274806Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-03-07T13:43:34.4276066Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-03-07T13:43:34.4277323Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-03-07T13:43:34.4278583Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-03-07T13:43:34.4279873Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-03-07T13:43:34.4281114Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-03-07T13:43:34.4282354Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-03-07T13:43:34.4283570Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-03-07T13:43:34.4284671Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-03-07T13:43:34.4285944Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-03-07T13:43:34.4287198Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4288409Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-03-07T13:43:34.4289624Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-03-07T13:43:34.4290798Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-03-07T13:43:34.4291999Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-03-07T13:43:34.4293287Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4294482Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4295692Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4296804Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4298169Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4299357Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4300494Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4301851Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4303669Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-03-07T13:43:34.4305318Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-03-07T13:43:34.4306715Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-03-07T13:43:34.4307899Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4309109Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4310224Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4311342Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4312547Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4313807Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4315052Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4316417Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4317981Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-03-07T13:43:34.4319355Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-03-07T13:43:34.4320552Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4321766Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4322967Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4324075Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4325327Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4326373Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4327462Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4328690Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4330020Z Mar 07 13:43:34 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-03-07T13:43:34.4330850Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-03-07T13:43:34.4331809Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-03-07T13:43:34.4332901Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-03-07T13:43:34.4333858Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32628&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=36036"	FLINK	Closed	2	1	6732	pull-request-available, test-stability
13522459	Introduce FileIO for table store	FileIO aims to make table-store and the FileSystem of Flink independent, In this way, we can provide different FileSystem support in the Flink cluster, such as other S3 buckets. In addition, different engines can provide the same FileIO experience (such as configuration and usage)	FLINK	Closed	3	7	6732	pull-request-available
13474979	Publish flink-table-store snapshot artifacts	"It is better to publish the Maven artifacts, so that downstream Java projects can use this.
See FLINK-26639"	FLINK	Closed	3	4	6732	pull-request-available
13267666	Optimize mapred.HadoopInputSplit to not serialize conf when split is not configurable	"JobConf may very big, contains hundreds of configurations, if it is serialized by every split, that will significantly reduce performance.

Consider thousands of splits, the akka thread of JobMaster will all on the serialization of conf. That may will lead to various akka timeouts too."	FLINK	Closed	3	7	6732	pull-request-available
13422227	Introduce CompactStrategy and CompactManager for table store	"- Introduce IntervalPartition: Algorithm to partition several sst files into the minimum number of `SortedRun`.
- Introduce UniversalCompaction CompactStrategy: Universal Compaction Style is a compaction style, targeting the use cases requiring lower write amplification, trading off read amplification and space amplification.
- Introduce CompactManager: Manager to submit compaction task."	FLINK	Closed	3	7	6732	pull-request-available
13272674	JoinITCase.testFullJoinWithNonEquiJoinPred failed in travis	"04:45:22.404 [ERROR] Tests run: 21, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.909 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.table.JoinITCase 04:45:22.406 [ERROR] testFullJoinWithNonEquiJoinPred(org.apache.flink.table.planner.runtime.batch.table.JoinITCase) Time elapsed: 0.168 s <<< ERROR! org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.table.planner.runtime.batch.table.JoinITCase.testFullJoinWithNonEquiJoinPred(JoinITCase.scala:344) Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 32 pages. Only 0 pages are remaining.

 

details: [https://api.travis-ci.org/v3/job/621407747/log.txt]"	FLINK	Closed	3	1	6732	pull-request-available
13226881	Support e2e sort merge join operator in batch mode	"Complete BatchExecSortMergeJoin and support join it cases.

Support queries like ""select a, b, c, e from T1, T2 where T1.a = T2.d"" run in batch mode"	FLINK	Closed	3	2	6732	pull-request-available
13431399	Introduce sequence.field to custom the order of the records	"In general, the order of the record can be generated directly by the store itself.
But for some cases, its order may be broken, and then a field can be specified to control the sequence number information. (For example, which one should be returned when querying multiple records of data under one primary key)"	FLINK	Closed	3	4	6732	pull-request-available
13382460	Introduce getChangeLogUpsertKeys in FlinkRelMetadataQuery	"For fix FLINK-20374, we need to resolve streaming computation disorder. we need to introduce a change log upsert keys, this is not unique keys.

 
{code:java}
/**
 * Determines the set of change log upsert minimal keys for this expression. A key is
 * represented as an {@link org.apache.calcite.util.ImmutableBitSet}, where each bit position
 * represents a 0-based output column ordinal.
 *
 * <p>Different from the unique keys: In distributed streaming computing, one record may be
 * divided into RowKind.UPDATE_BEFORE and RowKind.UPDATE_AFTER. If a key changing join is
 * connected downstream, the two records will be divided into different tasks, resulting in
 * disorder. In this case, the downstream cannot rely on the order of the original key. So in
 * this case, it has unique keys in the traditional sense, but it doesn't have change log upsert
 * keys.
 *
 * @return set of keys, or null if this information cannot be determined (whereas empty set
 *     indicates definitely no keys at all)
 */
public Set<ImmutableBitSet> getChangeLogUpsertKeys(RelNode rel);

{code}"	FLINK	Closed	3	7	6732	pull-request-available
13337361	Integrate Filesystem and Hive connector with changelog format (e.g. debezium-json)	Once Filesystem and Hive have been migrated to FLIP-95 interfaces, it should be possible to integrate with changelog format (e.g. debezium-json and canal-json) in source and sink. We should add tests for that. 	FLINK	Closed	3	7	6732	pull-request-available
13410771	CsvFilesystemStreamSinkITCase.testPart times out on AZP	"The test {{CsvFilesystemStreamSinkITCase.testPart}} times out on AZP.

{code}
2021-11-08T16:36:28.6542078Z Nov 08 16:36:28 org.junit.runners.model.TestTimedOutException: test timed out after 20 seconds
2021-11-08T16:36:28.6561998Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.putFields(ObjectOutputStream.java:463)
2021-11-08T16:36:28.6581789Z Nov 08 16:36:28 	at java.util.Locale.writeObject(Locale.java:2156)
2021-11-08T16:36:28.6601916Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6621871Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6632222Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6633082Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6633845Z Nov 08 16:36:28 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)
2021-11-08T16:36:28.6634442Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
2021-11-08T16:36:28.6634968Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6637691Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6640766Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6641958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6642763Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6643563Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6644365Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6645138Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6647747Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6648657Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6649439Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6650189Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6650958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6651975Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6652632Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6653314Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6664918Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6665679Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6666409Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6667211Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6667907Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6668585Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6669301Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6669991Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6670706Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6671353Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6672227Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6672878Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6673381Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6673864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6674366Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6674864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6675348Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6675851Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6676340Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6676827Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6677321Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6677797Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6678290Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6678781Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6679262Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6679764Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6680236Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
2021-11-08T16:36:28.6680733Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:632)
2021-11-08T16:36:28.6681669Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:548)
2021-11-08T16:36:28.6682620Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamConfig.setStreamOperatorFactory(StreamConfig.java:308)
2021-11-08T16:36:28.6683633Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setVertexConfig(StreamingJobGraphGenerator.java:713)
2021-11-08T16:36:28.6684729Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:461)
2021-11-08T16:36:28.6685788Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6686964Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6688033Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setChaining(StreamingJobGraphGenerator.java:377)
2021-11-08T16:36:28.6689024Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:178)
2021-11-08T16:36:28.6689973Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:116)
2021-11-08T16:36:28.6690944Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:960)
2021-11-08T16:36:28.6692027Z Nov 08 16:36:28 	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50)
2021-11-08T16:36:28.6692998Z Nov 08 16:36:28 	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39)
2021-11-08T16:36:28.6694130Z Nov 08 16:36:28 	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56)
2021-11-08T16:36:28.6695274Z Nov 08 16:36:28 	at org.apache.flink.test.util.MiniClusterPipelineExecutorServiceLoader$MiniClusterExecutor.execute(MiniClusterPipelineExecutorServiceLoader.java:137)
2021-11-08T16:36:28.6696466Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2095)
2021-11-08T16:36:28.6697500Z Nov 08 16:36:28 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
2021-11-08T16:36:28.6698470Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:772)
2021-11-08T16:36:28.6699494Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:753)
2021-11-08T16:36:28.6700427Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:574)
2021-11-08T16:36:28.6701363Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:556)
2021-11-08T16:36:28.6702418Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:118)
2021-11-08T16:36:28.6703498Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPart(FsStreamingSinkITCaseBase.scala:84)
2021-11-08T16:36:28.6704348Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6705109Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6705993Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6706775Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6707560Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-08T16:36:28.6708439Z Nov 08 16:36:28 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-08T16:36:28.6709327Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-08T16:36:28.6710196Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-08T16:36:28.6711113Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-11-08T16:36:28.6712067Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-11-08T16:36:28.6712971Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2021-11-08T16:36:28.6714031Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2021-11-08T16:36:28.6714883Z Nov 08 16:36:28 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-11-08T16:36:28.6715538Z Nov 08 16:36:28 	at java.lang.Thread.run(Thread.java:748)
2021-11-08T16:36:28.6715983Z Nov 08 16:36:28 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26165&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13306"	FLINK	Closed	2	1	6732	pull-request-available, test-stability
13241176	Add a upsert table sink for JDBC	"Now we just support append JDBC sink, this is very limited. Consider a simple wordCount, we need insert/update the sink with primary keys.

So we need to support upsert JDBC sink.

1.Introduce a AbstractJdbcOutputFormat to extract some common method from JDBCOutputFormat. Introduce a UpsertJDBCOutputFormat which extends AbstractJdbcOutputFormat.

2.Introduce UpsertJdbcTableSink."	FLINK	Closed	3	2	6732	pull-request-available
13474643	Unclean shade in flink-table-store-dist	"java.lang.NoSuchFieldError: callback
  at org.apache.flink.table.store.kafka.KafkaSinkFunction.open (KafkaSinkFunction.java:71)

Error when using table store with kafka sql-jar.

We should shade more for flink-connector-kafka."	FLINK	Closed	1	1	6732	pull-request-available
13441392	Record schema id in ManifestFileMeta and DataFileMeta	For future schema evolution, the schema id should be recorded inside the data, and when the schema changes, the previous schema can still be used to restore the existing data.	FLINK	Closed	3	7	6732	pull-request-available
13527915	Refactor classes code of full-compaction	Refactor classes code of full-compaction, this is to prepare some shared codes for lookup changelog producer.	FLINK	Closed	3	7	6732	pull-request-available
13273797	Move DDL to first tab in table connector page	Since we have a good support for DDL in tableEnv.sqlUpdate and SQL-CLI, I think it is time to highlight DDL in the document.	FLINK	Resolved	3	7	6732	pull-request-available
13469018	Get rid of BulkReaderFormatFactory and BulkWriterFormatFactory	These APIs were changed significantly in 1.14 and 1.15 and are not compatible	FLINK	Closed	3	7	6732	pull-request-available
13435125	Managed table breaks legacy connector without 'connector.type'	"{code:java}
CREATE TABLE T (a INT) WITH ('type'='legacy');
INSERT INTO T VALUES (1); {code}
This case can be misinterpreted as a managed table, which the user might expect to be resolved by the legacy table factory."	FLINK	Closed	3	1	6732	pull-request-available
13436369	Introduce parallelism setter for table store	"Support:
 * scan.parallelism
 * sink.parallelism"	FLINK	Closed	3	7	6732	pull-request-available
13310959	flink-orc and flink-parquet have invalid NOTICE file	"flink-orc provides a {{-jar-with-dependencies.jar}} variant which ships binaries.
However, these binaries are not documented in {{META-INF/NOTICE}}.
There are two similar files in that directory (NOTICE from force-shading and NOTICE.txt from Commons Lang). 

There is a NOTICE file that looks valid, but it is in {{META-INF/services}}.


I assume this has been introduced in FLINK-17460."	FLINK	Closed	1	1	6732	pull-request-available
13274910	ack flink-hadoop-compatibility and flink-orc into flink-hive	flink-connector-hive should contain flink-hadoop-compatibility to reduce users' efforts in figuring out dependency jars, and flink-orc for adding orc dependency which is missing in hive 2.2.x.	FLINK	Closed	3	1	6732	pull-request-available
13295928	Integrate parquet to file system connector	Implement ParquetFileSystemFormatFactory	FLINK	Resolved	3	7	6732	pull-request-available
13267622	BytesHashMap should not warn invoking stack when it want to spill	"Now BytesHashMap will warn invoking stack when it want to spill, this lead to a lot of messages in logs.

We need remove the stack and just print warns."	FLINK	Closed	3	4	6732	pull-request-available
13218523	Introduce MemorySegmentWritable to let Segments direct copy to internal bytes	"Blink new binary format is based on MemorySegment.

Introduce MemorySegmentWritable to let DataOutputView direct copy to internal bytes
{code:java}
/**
 * Provides the interface for write(Segment).
 */
public interface MemorySegmentWritable {

 /**
 * Writes {@code len} bytes from memory segment {@code segment} starting at offset {@code off}, in order,
 * to the output.
 *
 * @param segment memory segment to copy the bytes from.
 * @param off the start offset in the memory segment.
 * @param len The number of bytes to copy.
 * @throws IOException if an I/O error occurs.
 */
 void write(MemorySegment segment, int off, int len) throws IOException;
}{code}
 

If we want to write a Memory Segment to DataOutputView, we need to copy bytes to byte[] and then write it in, which is less effective.

If we let AbstractPagedOutputView have a write(MemorySegment) interface, we can copy it directly.

We need to ensure this in network serialization, batch operator calculation serialization, Streaming State serialization to avoid new byte[] and copy."	FLINK	Closed	3	2	6732	pull-request-available
13217071	Introduce an abstract set of data formats	"Blink uses an abstract set of data formats to make internal calculations use the binary format as much as possible. This minimizes the serialization overhead and java object overhead.

It includes:

BaseRow <=> Row

BaseMap <=> Java Map

BaseArray <=> Java array

BaseString  <=> Java String

Decimal <=> BigDecimal  //Scale of this object is specified by the user, not automatically determined(like BigDecimal).

int <=> Date //Flink used to use int in the calculation, but the remaining in Row is still Date, we will change it completely.

int <=> Time

long <=> Timestamp

byte[] <=> byte[]

BaseGeneric <=> T (GenericRelDataType, we don't know it, let user define serializer)

primitive type keep same, but use less boxed type."	FLINK	Closed	3	2	6732	pull-request-available
13226620	Support e2e SortAggregate and HashAggregate operator run in batch mode	"1.Finish BatchExecSortAggregate

2.Finish BatchExecHashAggregate

3.Add AggITCase"	FLINK	Closed	3	2	6732	pull-request-available
13292452	BytesColumnVector should init buffer in Hive 3.x	The failed test is {{TableEnvHiveConnectorTest#testDifferentFormats}} when hive 3.x.	FLINK	Resolved	3	1	6732	pull-request-available
13525159	Split flink connector to each module of each version	This will make compilation and testing much easier.	FLINK	Closed	3	4	6732	pull-request-available
13243326	Support LocalZonedTimestampType in blink	"Now we just support TimestampType, it is without time zone.

We need support LocalZonedTimestampType, and define time zone in config."	FLINK	Closed	3	2	6732	pull-request-available
13505201	Projection pushdown is not work for partial update	We did not properly process the project in MergeFunction, which resulted in subsequent reading position errors.	FLINK	Closed	3	1	6732	pull-request-available
13473511	Optimize Spark documentation to Catalog and Dataset	"* Introduce Dataset API.
* Unify table_store and tablestore."	FLINK	Closed	1	1	6732	pull-request-available
13276978	Use JDBC connector write FLOAT value occur ClassCastException	"I defined a float type field in mysql table, when I use jdbc connector write float value into db, there are ClassCastException occurs.
{code:java}
//代码占位符
Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1.Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1. Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1.Caused by: java.lang.ClassCastException: java.lang.Float cannot be cast to java.lang.Double, field index: 6, field value: 0.1.  at org.apache.flink.api.java.io.jdbc.JDBCUtils.setField(JDBCUtils.java:106)  at org.apache.flink.api.java.io.jdbc.JDBCUtils.setRecordToStatement(JDBCUtils.java:63) at org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter.addRecord(AppendOnlyWriter.java:56) at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.writeRecord(JDBCUpsertOutputFormat.java:144)

{code}
 "	FLINK	Resolved	3	1	6732	pull-request-available
13241007	Support intersect all and minus all to blink planner	"Now, we just support intersect and minus, See ReplaceIntersectWithSemiJoinRule and ReplaceMinusWithAntiJoinRule, replace intersect with null aware semi-join and distinct aggregate.

We need support intersect all and minus all too.

Presto and Spark already support them:

[https://github.com/prestodb/presto/issues/4918]

https://issues.apache.org/jira/browse/SPARK-21274

I think them have a good rewrite design and we can follow them:

1.For intersect all

Input Query
{code:java}
SELECT c1 FROM ut1 INTERSECT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
  SELECT c1
    FROM (
         SELECT replicate_row(min_count, c1)
         FROM (
              SELECT c1,
                     IF (vcol1_cnt > vcol2_cnt, vcol2_cnt, vcol1_cnt) AS min_count
              FROM (
                   SELECT   c1, count(vcol1) as vcol1_cnt, count(vcol2) as vcol2_cnt
                   FROM (
                        SELECT c1, true as vcol1, null as vcol2 FROM ut1
                        UNION ALL
                        SELECT c1, null as vcol1, true as vcol2 FROM ut2
                        ) AS union_all
                   GROUP BY c1
                   HAVING vcol1_cnt >= 1 AND vcol2_cnt >= 1
                  )
              )
          )
{code}
2.For minus all:

Input Query
{code:java}
SELECT c1 FROM ut1 EXCEPT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
 SELECT c1
    FROM (
     SELECT replicate_rows(sum_val, c1)
       FROM (
         SELECT c1, sum_val
           FROM (
             SELECT c1, sum(vcol) AS sum_val
               FROM (
                 SELECT 1L as vcol, c1 FROM ut1
                 UNION ALL
                 SELECT -1L as vcol, c1 FROM ut2
              ) AS union_all
            GROUP BY union_all.c1
          )
        WHERE sum_val > 0
       )
   )
{code}"	FLINK	Closed	3	2	6732	pull-request-available
13525881	Provides option to sort partition for full stage in streaming read	"The overall order may be out of order due to the writing of the old partition. We can provide an option to sort the full reading stage by partition fields to avoid the disorder.
(Actually, Currently, it is out of order for partitions. Because HashMap is used, we may be able to sort according to the creation time of the first file?)"	FLINK	Closed	1	4	6732	pull-request-available
13370533	 Empty values with sort willl fail	SELECT * FROM (VALUES 1, 2, 3) AS T (a) WHERE a = 1 and a = 2 ORDER BY a	FLINK	Closed	3	1	6732	pull-request-available
13293693	HiveModuleTest failed to compile on release-1.10	"The cron task of release-1.10 failed to compile with the following exception:
{code}
23:36:45.190 [ERROR] /home/travis/build/apache/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java:[158,45] constructor HiveModule in class org.apache.flink.table.module.hive.HiveModule cannot be applied to given types;
 required: java.lang.String
 found: no arguments
 reason: actual and formal argument lists differ in length
{code}

instance: [https://api.travis-ci.org/v3/job/666450476/log.txt]"	FLINK	Resolved	3	1	6732	pull-request-available
13295240	SQL hive-connector wilcard excludes don't work on maven 3.1.X	"The sql-connector-hive modules added in FLINK-16455 use wildcards imports to exclude all transitive dependencies from hive.

This is a maven 3.2.1+ feature. This may imply that Flink cannot be properly built anymore with maven 3.1 ."	FLINK	Resolved	1	1	6732	pull-request-available
13246709	Blink-planner not support generic TableSource	"Now there is a exception when user use table source like:
{code:java}
class MyTableSource[T] extend StreamTableSource[T]
{code}
The reason is that blink-planner use TypeExtractor to extract class from TableSource, and use this class to DataFormatConverter.

Now, table source has DataType return type, so we don't need extract class from TableSource, we can just use conversionClass of DataType."	FLINK	Resolved	3	7	6732	pull-request-available
13470498	SplitGenerator should use Ordered Packing	"SplitGenerator should use Ordered Packing
- The full part data in the stream is consumed in an orderly way
- Ordered batch reads can make subsequent calculations more efficient, such as aggregation by partial primary key groupBy"	FLINK	Closed	3	4	6732	pull-request-available
13461398	Refactor TableStoreCatalog: Introduce a dedicated Catalog for table store	"We currently have developed a Flink's Catalog.
If we expose this Catalog directly to other connector developers, it is not good and there will be many unsupported interfaces and capabilities.

So here we create a tablestore dedicated catalog."	FLINK	Closed	3	4	6732	pull-request-available
13415503	Introduce CatalogLock	"Currently, only HiveCatalog can provide this catalog lock.
{code:java}
/**
 * An interface that allows source and sink to use global lock to some transaction-related things.
 */
@Internal
public interface CatalogLock extends Closeable {
 
    /** Run with catalog lock. The caller should tell catalog the database and table name. */
    <T> T runWithLock(String database, String table, Callable<T> callable) throws Exception;
 
    /** Factory to create {@link CatalogLock}. */
    interface Factory extends Serializable {
        CatalogLock create();
    }
} {code}
And we need a interface to set lock to source&sink by catalog:
{code:java}
/**
 * Source and sink implement this interface if they require {@link CatalogLock}. This is marked as
 * internal. If we need lock to be more general, we can put lock factory into {@link
 * DynamicTableFactory.Context}.
 */
@Internal
public interface RequireCatalogLock {
 
    void setLockFactory(CatalogLock.Factory lockFactory);
} {code}
{{}}"	FLINK	Closed	3	7	6732	pull-request-available
13286366	Use configuration from TableFactory in hive connector	"Now {{HiveOptions}} is used for {{GlobalConfiguration.loadConfiguration()}} .

It is not natural for table, we should use configuration from TableFactory to enable table config."	FLINK	Resolved	3	4	6732	pull-request-available
13304288	Fs connector should use FLIP-122 format options style	"format.parquet.compression -> parquet.compression

format.field-delimiter -> csv.field-delimiter"	FLINK	Closed	2	1	6732	pull-request-available
13302027	Test and correct case insensitive for parquet and orc in hive	"Orc and parquet should be field names case insensitive to compatible with hive.

Both hive mapred reader and vectorization reader."	FLINK	Closed	2	1	6732	pull-request-available
13384793	Correct upsert optimization by upsert keys	"After FLINK-22901.

We can use upsert keys to fix upsert join, upsert rank, and upsert sink.
 * For join and rank: if input has no upsert keys, do not use upsert optimization.
 * For upsert sink: if input has unique keys but no upsert keys, we need add a materialize operator to produce upsert records."	FLINK	Closed	3	7	6732	pull-request-available
13337825	Integrate file compaction to filesystem connector	Integrate file compaction to Filesystem connector.	FLINK	Closed	3	7	6732	pull-request-available
13359971	AdaptiveSchedulerSlotSharingITCase.testSchedulingOfJobRequiringSlotSharing fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13510&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865
{code:java}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 23.313 s <<< FAILURE! - in org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase [ERROR] testSchedulingOfJobRequiringSlotSharing(org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase) Time elapsed: 20.683 s <<< ERROR! org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) at org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase.runJob(DeclarativeSchedulerSlotSharingITCase.java:83) at org.apache.flink.runtime.scheduler.declarative.DeclarativeSchedulerSlotSharingITCase.testSchedulingOfJobRequiringSlotSharing(DeclarativeSchedulerSlotSharingITCase.java:71) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
{code}"	FLINK	Closed	2	1	6847	pull-request-available, test-stability
13521506	Refactor redundant code in AbstractHaServices 	"{{AbstractHaServices.createLeaderElectionService}} returns {{{}LeaderElectionService{}}}. All implementations return {{{}DefaultLeaderElectionService{}}}. The actual implementation-specific code creates the {{{}LeaderElectionDriverFactory{}}}.

We can remove the redundant code here."	FLINK	Resolved	3	7	6847	pull-request-available
13346283	Remove InputDependencyConstraint and InputDependencyConstraintChecker	InputDependencyConstraint was used by legacy scheduler and lazy-from-sources scheduling strategy. It is no longer needed since both legacy scheduler and lazy-from-sources are removed. We can remove it as well as InputDependencyConstraintChecker.	FLINK	Closed	3	7	6847	pull-request-available
13435068	JobMaster.testJobFailureWhenTaskExecutorHeartbeatTimeout failed due to missing Execution	"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=897&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=8399] failed due to an {{ExecutionGraphException}} indicating that an expected {{Execution}} wasn't around:
{code}
[...]
Caused by: org.apache.flink.util.FlinkException: Execution 48dbc880c8225256b8bc112ea36e9082 is unexpectedly no longer running on task executor bbad15fcb93d4b2b4f80fe2c35e03e6d.
        at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:250) ~[classes/:?]
        ... 35 more
{code}"	FLINK	Resolved	3	1	6847	pull-request-available, test-stability
13541194	EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown is not properly implemented	"The purpose of {{EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown}} is to check that there is no {{NullPointerException}} happening if the event processing happens after the shutdown of the {{EmbeddedExecutorService}} (see FLINK-11855).

But the concurrent execution is not handled properly. The test also succeeds if the close call happened before the shutdown (due to the multi-threaded nature of the test) which leaves us without the actual test scenario being tested."	FLINK	Resolved	3	1	6847	pull-request-available
13537532	Move error handling into MultipleComponentLeaderElectionDriverFactory	{{LeaderElectionDriverFactory}} allows passing the error handling which can then be used to pass in an error handler that  forwards any error to the contender.	FLINK	Resolved	3	7	6847	pull-request-available
13481052	DefaultJobmanagerRunnerRegistry#localCleanupAsync calls close instead of closeAsync	"{{DefaultJobmanagerRunnerRegistry#localCleanupAsync}} is meant to be called from the main thread. The current implementation calls {{close}} on the {{JobManagerRunner}} instead of {{closeAsync}}. This results in a blocking call on the {{Dispatcher}}'s main thread which we want to avoid.

Thanks for identifying this issue, [~chesnay]"	FLINK	Resolved	3	1	6847	pull-request-available
13553730	IllegalArgumentException in NoticeFileChecker	"{code}
2023-10-11T15:55:54.7616189Z 15:55:54.760 [INFO] --- exec-maven-plugin:3.1.0:java (default-cli) @ flink-ci-tools ---
2023-10-11T15:55:55.2153082Z 15:55:55,212 WARN  org.apache.flink.tools.ci.licensecheck.LicenseChecker        [] - THIS UTILITY IS ONLY CHECKING FOR COMMON LICENSING MISTAKES. A MANUAL CHECK OF THE NOTICE FILES, DEPLOYED ARTIFACTS, ETC. IS STILL NEEDED!
2023-10-11T15:55:55.2217611Z 15:55:55,221 DEBUG org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Loaded 3 items from resource modules-defining-excess-dependencies.modulelist
2023-10-11T15:55:58.1540870Z 15:55:58,153 INFO  org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Extracted 128 modules that were deployed and 174 modules which bundle dependencies with a total of 174 dependencies
2023-10-11T15:55:58.1700608Z 15:55:58.162 [WARNING] 
2023-10-11T15:55:58.1701401Z java.lang.IllegalArgumentException
2023-10-11T15:55:58.1702263Z     at sun.nio.fs.UnixPath.getName (UnixPath.java:334)
2023-10-11T15:55:58.1702685Z     at sun.nio.fs.UnixPath.getName (UnixPath.java:43)
2023-10-11T15:55:58.1703278Z     at org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.lambda$findNoticeFiles$10 (NoticeFileChecker.java:362)
2023-10-11T15:55:58.1703907Z     at java.util.stream.ReferencePipeline$2$1.accept (ReferencePipeline.java:176)
2023-10-11T15:55:58.1704412Z     at java.util.stream.ReferencePipeline$3$1.accept (ReferencePipeline.java:195)
2023-10-11T15:55:58.1704877Z     at java.util.Iterator.forEachRemaining (Iterator.java:133)
2023-10-11T15:55:58.1705356Z     at java.util.Spliterators$IteratorSpliterator.forEachRemaining (Spliterators.java:1801)
2023-10-11T15:55:58.1705902Z     at java.util.stream.AbstractPipeline.copyInto (AbstractPipeline.java:484)
2023-10-11T15:55:58.1706457Z     at java.util.stream.AbstractPipeline.wrapAndCopyInto (AbstractPipeline.java:474)
2023-10-11T15:55:58.1706975Z     at java.util.stream.ReduceOps$ReduceOp.evaluateSequential (ReduceOps.java:913)
2023-10-11T15:55:58.1707485Z     at java.util.stream.AbstractPipeline.evaluate (AbstractPipeline.java:234)
2023-10-11T15:55:58.1708005Z     at java.util.stream.ReferencePipeline.collect (ReferencePipeline.java:578)
2023-10-11T15:55:58.1708667Z     at org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.findNoticeFiles (NoticeFileChecker.java:366)
2023-10-11T15:55:58.1709374Z     at org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.run (NoticeFileChecker.java:91)
2023-10-11T15:55:58.1710011Z     at org.apache.flink.tools.ci.licensecheck.LicenseChecker.main (LicenseChecker.java:42)
2023-10-11T15:55:58.1710548Z     at org.codehaus.mojo.exec.ExecJavaMojo$1.run (ExecJavaMojo.java:279)
2023-10-11T15:55:58.1710929Z     at java.lang.Thread.run (Thread.java:829)
{code}
https://github.com/XComp/flink/actions/runs/6483465871/job/17607225139#step:8:41046"	FLINK	Resolved	3	7	6847	github-actions, test-stability
13302625	Make memory configuration logging more user-friendly	"The newly introduced memory configuration logs some output when using the Mini Cluster (or local environment):

{code}
2020-05-04 11:50:05,984 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.cpu.cores' , default: null (fallback keys: []) required for local execution is not set, setting it to its default value 1.7976931348623157E308
2020-05-04 11:50:05,989 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.task.heap.size' , default: null (fallback keys: []) required for local execution is not set, setting it to its default value 9223372036854775807 bytes
2020-05-04 11:50:05,989 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.task.off-heap.size' , default: 0 bytes (fallback keys: []) required for local execution is not set, setting it to its default value 9223372036854775807 bytes
2020-05-04 11:50:05,990 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.network.min' , default: 64 mb (fallback keys: [{key=taskmanager.network.memory.min, isDeprecated=true}]) required for local execution is not set, setting it to its default value 64 mb
2020-05-04 11:50:05,990 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.network.max' , default: 1 gb (fallback keys: [{key=taskmanager.network.memory.max, isDeprecated=true}]) required for local execution is not set, setting it to its default value 64 mb
2020-05-04 11:50:05,991 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option Key: 'taskmanager.memory.managed.size' , default: null (fallback keys: [{key=taskmanager.memory.size, isDeprecated=true}]) required for local execution is not set, setting it to its default value 128 mb
{code}

This logging output could be made more user-friendly the following way:

* Print only the key string of a {{ConfigOption}}, not the config option object with all the deprecated keys
* Skipping the lines for {{taskmanager.memory.task.heap.size}} and {{taskmanager.memory.task.off-heap.size}} - we don't really set them (they are JVM paramaters) and the printing of long max looks strange (user would have to know these are place holders without effect).
* Maybe similarly skipping the CPU cores value, this looks the strangest (double max)."	FLINK	Closed	3	4	6847	pull-request-available, usability
13335243	Add Metaspace metric	We want to expose the currently used Metaspace memory as well that should be provided through the metrics system.	FLINK	Closed	3	7	6847	pull-request-available
13553837	LicenseChecker fails in GHA but succeeds in Azure	"Both builds are based on [master@011b6b44|https://github.com/apache/flink/commit/011b6b44]:
 * [GitHub Actions|https://github.com/XComp/flink/actions/runs/6487689661/job/17620650207#step:8:41307]
 * [Azure CI|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53624&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=43193]

The GitHub Actions run reports 12 severe issues where it's unclear where they are coming from:
{code:java}
23:30:45,534 WARN  org.apache.flink.tools.ci.licensecheck.LicenseChecker        [] - Found a total of 12 severe license issues {code}"	FLINK	Resolved	3	7	6847	github-actions, test-stability
13345822	Remove DefaultExecutionSlotAllocator	Remove the {{DefaultExecutionSlotAllocator}} which is only used by the legacy {{SchedulingStrategies}}. Probably {{AbstractExecutionSlotAllocator}} can also be removed in the same step.	FLINK	Closed	3	7	6847	pull-request-available
13544960	Renaming AkkaOptions into RpcOptions	"FLINK-32468 introduced Apache Pekko as an replacement for Akka. This involved renaming classes (besides updating comments). {{AkkaOptions}} was the only occurrence that wasn't renamed as it's annotated as {{@PublicEvolving}}.

This issue is about renaming {{AkkaOptions}} into {{PekkoOptions}} (or a more general term considering FLINK-29281)"	FLINK	Resolved	3	11500	6847	2.0-related, pull-request-available
13555205	JDK 17 CI run doesn't set java17-target profile	"In contrast to the jdk11 CI run which has the java11-target profile set (see [tools/azure-pipelines/build-apache-repo.yml:138|https://github.com/apache/flink/blob/9b63099964b36ad9d78649bb6f5b39473e0031bd/tools/azure-pipelines/build-apache-repo.yml#L138]), it's missing for the jdk17 CI run (see [tools/azure-pipelines/build-apache-repo.yml:149|https://github.com/apache/flink/blob/9b63099964b36ad9d78649bb6f5b39473e0031bd/tools/azure-pipelines/build-apache-repo.yml#L149]).

The profile for the source version (i.e. {{java11}} and {{java17}}) are automatically activated through the JDK version of the run."	FLINK	Resolved	3	1	6847	pull-request-available
13262831	Add TaskManageResourceInfo which match the memory compositions of taskmanager	"* information from TaskExecutorResourceSpec in flip-49, add it to TaskExecutorRegistration.

{code:json}
public class TaskManagerResourceInfo {
    private final double cpuCores;
    private final long frameworkHeap;
    private final long frameworkOffHeap;
    private final long taskHeap;
    private final long taskOffHeap;
    private final long shuffleMemory;
    private final long managedMemory;
    private final long jvmMetaSpace;
    private final long jvmOverhead;
    private final long totalProcessMemory;
}{code}
 * url: /taskmanagers/:taskmanagerid
 * response: add

{code:java}
resource: {
  cpuCores: 4,
  frameworkHeap: 134217728,
  frameworkOffHeap: 134217728,
  taskHeap: 181193928,
  taskOffHeap: 0,
  shuffleMemory: 33554432,
  managedMemory: 322122552,
  jvmMetaSpace: 134217728,
  jvmOverhead: 134217728,
  totalProcessMemory: 1073741824
}
{code}"	FLINK	Closed	3	7	6847	pull-request-available
13355146	Expose extended exception history through REST handler and adapt UI accordingly	"A first idea of a UI change was proposed by [~vthinkxie] in the parent task:

!https://issues.apache.org/jira/secure/attachment/13019553/13019553_%E6%88%AA%E5%B1%8F2021-01-28+%E4%B8%8B%E5%8D%884.47.46.png|width=476,height=245!

The {{JobExceptionsHandler}} response should be extended introducing a new (deprecated) field {{otherFailures}}. This way we avoid breaking the API."	FLINK	Closed	3	7	6847	pull-request-available
13548177	Silence curls in test code	"We use {{curl}} in several locations to download artifacts. Usually, the a progress bar is printed which spams the console output of the test execution. This issue is about cleaning this up.

Parameters to consider (depending on the usecase):
 * {{\-L}}/{{\-\-location}} redirects the curl command and retries if the server reported that the artifact was moved
 * {{\-O}}/{{\-\-remote-name}} writes output to file matching the remote name (which was extracted from the URL) instead of stdout; alternative: {{\-o}}/{{\-\-output}} writes output to a file with the given name instead of stdout
* {{\-f}}/{{\-\-fail}} makes curl command fail with non-0 exit code for HTTP error codes
* {{\-s \-S}}/{{\-\-silent \-\-show-error}} doesn't print progress bar but shows error
* {{\-r}}/{{\-\-retry}} Retries certain errors

{{curl}} uses a default config file {{${user.home}/.curlrc}}. But one could make it more explicit using {{\-K}}/{{\-\-config}}"	FLINK	In Progress	3	4	6847	pull-request-available, stale-assigned, starter
13563759	Flink Job stuck in suspend state after losing leadership in HA Mode	"The observation is that Job manager goes to suspend state with a failed container not able to register itself to resource manager after timeout.

JM Log, see attached

 "	FLINK	Resolved	1	1	6847	pull-request-available
13428282	ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification failed on azure	"
{code:java}
2022-02-11T21:43:35.4936452Z Feb 11 21:43:35 java.lang.AssertionError: The TestingFatalErrorHandler caught an exception.
2022-02-11T21:43:35.4940444Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.after(TestingFatalErrorHandlerResource.java:81)
2022-02-11T21:43:35.4941937Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.access$300(TestingFatalErrorHandlerResource.java:36)
2022-02-11T21:43:35.4943249Z Feb 11 21:43:35 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:60)
2022-02-11T21:43:35.4944745Z Feb 11 21:43:35 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-02-11T21:43:35.4945682Z Feb 11 21:43:35 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-02-11T21:43:35.4946655Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-11T21:43:35.4947847Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-02-11T21:43:35.4948876Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-02-11T21:43:35.4949842Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-02-11T21:43:35.4951142Z Feb 11 21:43:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-02-11T21:43:35.4952153Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-02-11T21:43:35.4953115Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-02-11T21:43:35.4954068Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-02-11T21:43:35.4955003Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-02-11T21:43:35.4955981Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-02-11T21:43:35.4956930Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-02-11T21:43:35.4958008Z Feb 11 21:43:35 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-02-11T21:43:35.4958899Z Feb 11 21:43:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-02-11T21:43:35.4959774Z Feb 11 21:43:35 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-02-11T21:43:35.4960911Z Feb 11 21:43:35 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-02-11T21:43:35.4962095Z Feb 11 21:43:35 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-02-11T21:43:35.4963136Z Feb 11 21:43:35 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-02-11T21:43:35.4964275Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-02-11T21:43:35.4965527Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-02-11T21:43:35.4966787Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-02-11T21:43:35.4968228Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-02-11T21:43:35.4969485Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-02-11T21:43:35.4970753Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-02-11T21:43:35.4971842Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-02-11T21:43:35.4973291Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-02-11T21:43:35.4974538Z Feb 11 21:43:35 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-02-11T21:43:35.4975737Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-02-11T21:43:35.4976936Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-02-11T21:43:35.4978291Z Feb 11 21:43:35 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-02-11T21:43:35.4979588Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-02-11T21:43:35.4980728Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-02-11T21:43:35.4981753Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-02-11T21:43:35.4982863Z Feb 11 21:43:35 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-02-11T21:43:35.4983958Z Feb 11 21:43:35 Caused by: org.apache.flink.runtime.leaderretrieval.LeaderRetrievalException: Could not handle node changed event.
2022-02-11T21:43:35.4985193Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper(ZooKeeperLeaderRetrievalDriver.java:143)
2022-02-11T21:43:35.4986451Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.onReconnectedConnectionState(ZooKeeperLeaderRetrievalDriver.java:181)
2022-02-11T21:43:35.4987902Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.handleStateChange(ZooKeeperLeaderRetrievalDriver.java:164)
2022-02-11T21:43:35.4989235Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.lambda$new$0(ZooKeeperLeaderRetrievalDriver.java:61)
2022-02-11T21:43:35.4990632Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.lambda$processEvents$0(ConnectionStateManager.java:279)
2022-02-11T21:43:35.4991926Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)
2022-02-11T21:43:35.4993137Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)
2022-02-11T21:43:35.4994438Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2022-02-11T21:43:35.4995807Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:279)
2022-02-11T21:43:35.4997130Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
2022-02-11T21:43:35.4999203Z Feb 11 21:43:35 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:132)
2022-02-11T21:43:35.4999927Z Feb 11 21:43:35 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-02-11T21:43:35.5000679Z Feb 11 21:43:35 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-02-11T21:43:35.5001306Z Feb 11 21:43:35 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-02-11T21:43:35.5001866Z Feb 11 21:43:35 	at java.lang.Thread.run(Thread.java:748)
2022-02-11T21:43:35.5002381Z Feb 11 21:43:35 Caused by: java.lang.IllegalStateException: java.lang.InterruptedException
2022-02-11T21:43:35.5003374Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest$QueueLeaderElectionListener.notifyLeaderAddress(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:375)
2022-02-11T21:43:35.5004370Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper(ZooKeeperLeaderRetrievalDriver.java:136)
2022-02-11T21:43:35.5005005Z Feb 11 21:43:35 	... 14 more
2022-02-11T21:43:35.5005464Z Feb 11 21:43:35 Caused by: java.lang.InterruptedException
2022-02-11T21:43:35.5006079Z Feb 11 21:43:35 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
2022-02-11T21:43:35.5006973Z Feb 11 21:43:35 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
2022-02-11T21:43:35.5007877Z Feb 11 21:43:35 	at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:353)
2022-02-11T21:43:35.5008787Z Feb 11 21:43:35 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest$QueueLeaderElectionListener.notifyLeaderAddress(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:373)
2022-02-11T21:43:35.5009547Z Feb 11 21:43:35 	... 15 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31294&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7581
"	FLINK	Resolved	3	1	6847	pull-request-available, test-stability
13562450	Trial Period: GitHub Actions	This issue is (in contrast to FLINK-27075 which is used for issues that were collected while preparing [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]) collecting all the subtasks that are necessary to initiate the trial phase for GitHub Actions (as discussed in [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]).	FLINK	Open	3	2	6847	github-actions
13429737	The ZooKeeperStateHandleStore cleans the metadata before cleaning the StateHandle	Cleanup of job state does not work properly in an HA setup. {{releaseAndTryRemove}} deletes the meta data stored in the store before cleaning up the {{StateHandle}}. If the {{StateHandle}} cleanup fails after the reference is already deleted in the {{StateHandleStore}}, a cleanup retry will constantly fail because it cannot deserialize the {{StateHandle}} anymore.	FLINK	Resolved	1	1	6847	pull-request-available
13571898	HA deadlock between JobMasterServiceLeadershipRunner and DefaultLeaderElectionService	"We recently observed a deadlock in the JM within the HA system.
(see below for the thread dump)

[~mapohl] and I looked a bit into it and there appears to be a race condition when leadership is revoked while a JobMaster is being started.
It appears to be caused by {{JobMasterServiceLeadershipRunner#createNewJobMasterServiceProcess}} forwarding futures while holding a lock; depending on whether the forwarded future is already complete the next stage may or may not run while holding that same lock.
We haven't determined yet whether we should be holding that lock or not.

{code}
""DefaultLeaderElectionService-leadershipOperationExecutor-thread-1"" #131 daemon prio=5 os_prio=0 cpu=157.44ms elapsed=78749.65s tid=0x00007f531f43d000 nid=0x19d waiting for monitor entry  [0x00007f53084fd000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.runIfStateRunning(JobMasterServiceLeadershipRunner.java:462)
        - waiting to lock <0x00000000f1c0e088> (a java.lang.Object)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.revokeLeadership(JobMasterServiceLeadershipRunner.java:397)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.notifyLeaderContenderOfLeadershipLoss(DefaultLeaderElectionService.java:484)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1252/0x0000000840ddec40.accept(Unknown Source)
        at java.util.HashMap.forEach(java.base@11.0.22/HashMap.java:1337)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onRevokeLeadershipInternal(DefaultLeaderElectionService.java:452)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1251/0x0000000840dcf840.run(Unknown Source)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.lambda$runInLeaderEventThread$3(DefaultLeaderElectionService.java:549)
        - locked <0x00000000f0e3f4d8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1075/0x0000000840c23040.run(Unknown Source)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(java.base@11.0.22/CompletableFuture.java:1736)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.22/ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.22/ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(java.base@11.0.22/Thread.java:829)
{code}

{code}
""jobmanager-io-thread-1"" #636 daemon prio=5 os_prio=0 cpu=125.56ms elapsed=78699.01s tid=0x00007f5321c6e800 nid=0x396 waiting for monitor entry  [0x00007f530567d000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.hasLeadership(DefaultLeaderElectionService.java:366)
        - waiting to lock <0x00000000f0e3f4d8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElection.hasLeadership(DefaultLeaderElection.java:52)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.isValidLeader(JobMasterServiceLeadershipRunner.java:509)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$forwardIfValidLeader$15(JobMasterServiceLeadershipRunner.java:520)
        - locked <0x00000000f1c0e088> (a java.lang.Object)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner$$Lambda$1320/0x0000000840e1a840.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(java.base@11.0.22/CompletableFuture.java:859)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(java.base@11.0.22/CompletableFuture.java:837)
        at java.util.concurrent.CompletableFuture.postComplete(java.base@11.0.22/CompletableFuture.java:506)
        at java.util.concurrent.CompletableFuture.complete(java.base@11.0.22/CompletableFuture.java:2079)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.registerJobMasterServiceFutures(DefaultJobMasterServiceProcess.java:124)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:114)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess$$Lambda$1319/0x0000000840e1a440.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(java.base@11.0.22/CompletableFuture.java:859)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(java.base@11.0.22/CompletableFuture.java:837)
        at java.util.concurrent.CompletableFuture.postComplete(java.base@11.0.22/CompletableFuture.java:506)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(java.base@11.0.22/CompletableFuture.java:1705)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.22/ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.22/ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(java.base@11.0.22/Thread.java:829)
{code}"	FLINK	In Progress	3	1	6847	pull-request-available
13541013	Add fallback error handler to DefaultLeaderElectionService	"The FLIP-285 work separated the driver lifecycle from the contender lifecycle. Now, a contender can be removed but the driver could still be running. Error could be produced on the driver's side. The {{DefaultLeaderElectionService}} would try to forward the error to the contender. With not contender being registered, the error would be swallowed.

We should add a fallback error handler for this specific case."	FLINK	Resolved	3	7	6847	pull-request-available
13555290	DispatcherResourceCleanupTest.testFatalErrorIfJobCannotBeMarkedDirtyInJobResultStore fails on AZP	"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53905&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=6800

failed with 
{noformat}
Oct 22 00:59:32 Caused by: java.io.IOException: Expected IOException.
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.lambda$testFatalErrorIfJobCannotBeMarkedDirtyInJobResultStore$6(DispatcherResourceCleanupTest.java:558)
Oct 22 00:59:32 	at org.apache.flink.runtime.testutils.TestingJobResultStore.createDirtyResultAsync(TestingJobResultStore.java:81)
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.Dispatcher.createDirtyJobResultEntryAsync(Dispatcher.java:1441)
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createDirtyJobResultEntryIfMissingAsync$45(Dispatcher.java:1422)
Oct 22 00:59:32 	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
Oct 22 00:59:32 	... 39 more

{noformat}"	FLINK	Resolved	2	11500	6847	pull-request-available, test-stability
13436863	FileSystemJobResultStore calls flush on an already closed OutputStream 	"We experienced problems with some FileSystems when creating the dirty JRS entries (see initial discussion in FLINK-26555). The {{writeValue}} method closes the {{OutputStream}} by default which causes the subsequent {{flush}} call to fail.

It didn't appear in the unit tests because {{LocalDataOutputStream.flush}} is a no-op operation. We still have to investigate why it didn't appear when doing the tests with the presto and hadoop S3 filesystems."	FLINK	Resolved	1	1	6847	pull-request-available
13322761	Added Copyright information to coding style guide	"Add Copyright as a requirement to [https://flink.apache.org/contributing/code-style-and-quality-common.html]

Add Copyright profile instructions to ide_setup.md (including the Chinese version)."	FLINK	Closed	4	4	6847	pull-request-available
13338096	Update documentation to address difference between -Xmx and the metric for maximum heap	"We observed a difference between the configured maximum heap and the maximum heap returned by the metric system. This is caused by the used garbage collection (see [this blogpost|https://plumbr.io/blog/memory-leaks/less-memory-than-xmx] for further details).

We should make the user aware of this in the documentation mentioning it in the memory configuration and the metrics page."	FLINK	Closed	4	4	6847	pull-request-available
13570336	Align retry mechanisms of FutureUtils	The retry mechanisms of FutureUtils include quite a bit of redundant code which makes it hard to understand and to extend. The logic should be aligned properly.	FLINK	Open	3	11500	6847	pull-request-available
13375384	Avoid discarding checkpoints in case of failure	"Both {{StateHandleStore}} implementations (i.e. [KubernetesStateHandleStore:157|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesStateHandleStore.java#L157] and [ZooKeeperStateHandleStore:170|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L170]) discard checkpoints if the checkpoint metadata wasn't written to the backend. 

This does not cover the cases where the data was actually written to the backend but the call failed anyway (e.g. due to network issues). In such a case, we might end up having a pointer in the backend pointing to a checkpoint that was discarded.

Instead of discarding the checkpoint data in this case, we might want to keep it for this specific use case. Otherwise, we might run into Exceptions when recovering from the Checkpoint later on. We might want to add a warning to the user pointing to the possibly orphaned checkpoint data."	FLINK	Closed	2	1	6847	pull-request-available
13362910	YARNSessionCapacitySchedulerITCase.testStartYarnSessionClusterInQaTeamQueue fail because of NullPointerException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14265&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code:java}
2021-03-07T23:00:44.6390668Z [ERROR] testStartYarnSessionClusterInQaTeamQueue(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase)  Time elapsed: 7.338 s  <<< ERROR!
2021-03-07T23:00:44.6391415Z java.lang.NullPointerException: 
2021-03-07T23:00:44.6403594Z java.lang.NullPointerException
2021-03-07T23:00:44.6404575Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2021-03-07T23:00:44.6405710Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2021-03-07T23:00:44.6406830Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2021-03-07T23:00:44.6407970Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2021-03-07T23:00:44.6409075Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2021-03-07T23:00:44.6412848Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2021-03-07T23:00:44.6417313Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2021-03-07T23:00:44.6421872Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2021-03-07T23:00:44.6423676Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2021-03-07T23:00:44.6424387Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2021-03-07T23:00:44.6424997Z 	at java.security.AccessController.doPrivileged(Native Method)
2021-03-07T23:00:44.6425608Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-03-07T23:00:44.6426513Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2021-03-07T23:00:44.6427351Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2021-03-07T23:00:44.6427767Z 
2021-03-07T23:00:44.6428196Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-03-07T23:00:44.6428975Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-03-07T23:00:44.6429888Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-03-07T23:00:44.6442419Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2021-03-07T23:00:44.6445364Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
2021-03-07T23:00:44.6644429Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateRuntimeException(RPCUtil.java:85)
2021-03-07T23:00:44.6658468Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:122)
2021-03-07T23:00:44.6669171Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:291)
2021-03-07T23:00:44.6680027Z 	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
2021-03-07T23:00:44.6690713Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-07T23:00:44.6701085Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-07T23:00:44.6708626Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2021-03-07T23:00:44.6709488Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2021-03-07T23:00:44.6710261Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2021-03-07T23:00:44.6711051Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2021-03-07T23:00:44.6711864Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2021-03-07T23:00:44.6729939Z 	at com.sun.proxy.$Proxy111.getApplications(Unknown Source)
2021-03-07T23:00:44.6746044Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:528)
2021-03-07T23:00:44.6747093Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:505)
2021-03-07T23:00:44.6748256Z 	at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:293)
2021-03-07T23:00:44.6749122Z 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:274)
2021-03-07T23:00:44.6750975Z 	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.testStartYarnSessionClusterInQaTeamQueue(YARNSessionCapacitySchedulerITCase.java:164)
2021-03-07T23:00:44.6751907Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-07T23:00:44.6753574Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-07T23:00:44.6754504Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-07T23:00:44.6755270Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-07T23:00:44.6757569Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-07T23:00:44.6758434Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-07T23:00:44.6759463Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-07T23:00:44.6760279Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-07T23:00:44.6761082Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-03-07T23:00:44.6762689Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-03-07T23:00:44.6763589Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6764601Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6765295Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-03-07T23:00:44.6765968Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-07T23:00:44.6766601Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-07T23:00:44.6767442Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-07T23:00:44.6768248Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-07T23:00:44.6768968Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-07T23:00:44.6769645Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-07T23:00:44.6770337Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-07T23:00:44.6771022Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-07T23:00:44.6771746Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-07T23:00:44.6772483Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-03-07T23:00:44.6773298Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-03-07T23:00:44.6774046Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6774773Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6775435Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-07T23:00:44.6776067Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-07T23:00:44.6776794Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-03-07T23:00:44.6777694Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-03-07T23:00:44.6778543Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-03-07T23:00:44.6779362Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-03-07T23:00:44.6780201Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-07T23:00:44.6781085Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-07T23:00:44.6781878Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-07T23:00:44.6782634Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-03-07T23:00:44.6783545Z Caused by: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
2021-03-07T23:00:44.6784609Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2021-03-07T23:00:44.6785745Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2021-03-07T23:00:44.6786821Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2021-03-07T23:00:44.6787886Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2021-03-07T23:00:44.6794073Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2021-03-07T23:00:44.6796335Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2021-03-07T23:00:44.6798877Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2021-03-07T23:00:44.6801068Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2021-03-07T23:00:44.6803385Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2021-03-07T23:00:44.6805566Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2021-03-07T23:00:44.6807902Z 	at java.security.AccessController.doPrivileged(Native Method)
2021-03-07T23:00:44.6810554Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-03-07T23:00:44.6813030Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2021-03-07T23:00:44.6815324Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2021-03-07T23:00:44.6817630Z 
2021-03-07T23:00:44.6819989Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2021-03-07T23:00:44.6820646Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2021-03-07T23:00:44.6821270Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2021-03-07T23:00:44.6822571Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2021-03-07T23:00:44.6823429Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2021-03-07T23:00:44.6825060Z 	at com.sun.proxy.$Proxy110.getApplications(Unknown Source)
2021-03-07T23:00:44.6827746Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:288)
2021-03-07T23:00:44.6828489Z 	... 50 more
2021-03-07T23:00:44.6828728Z 
2021-03-07T23:01:15.2299964Z [INFO] Running org.apache.flink.yarn.YARNITCase
2021-03-07T23:01:20.5439420Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5442231Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
2021-03-07T23:01:20.5442949Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5443688Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
2021-03-07T23:01:20.5444176Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5444733Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2021-03-07T23:01:20.5471230Z Mar 07, 2021 11:01:20 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2021-03-07T23:01:20.5473577Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2021-03-07T23:01:20.6368864Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getC
{code}
"	FLINK	Closed	3	1	6847	pull-request-available, test-stability
13562463	Makes copying test jars being done later	"We experienced an issue in GHA which is due to the fact how test resources are pre-computed in GHA:
{code:java}
This fixes the following error when compiling flink-clients:
Error: 2.054 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.2.0:copy-dependencies (copy-dependencies) on project flink-clients: Artifact has not been packaged yet. When used on reactor artifact, copy should be executed after packaging: see MDEP-187. -> [Help 1] {code}
We need to move this goal to a later phase.

The reason why this popped up is (as far as I remember) that we do only do test-compile in GitHub Actions."	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
13441434	Make Job mode wait with cluster shutdown until the cleanup is done	The shutdown is triggered as soon as the job terminates globally without waiting for any cleanup. This behavior was ok'ish in 1.14- because we didn't bother so much about the cleanup. In 1.15+ we might want to wait for the cleanup to finish.	FLINK	Resolved	3	7	6847	pull-request-available
13521721	Migrate LeaderElection-related unit tests to JUnit5	To prepare the merge of the {{MultipleComponentLeaderElectionService}}-related tests with the legacy test, we want to align the JUnit versin they are using.	FLINK	Resolved	3	7	6847	pull-request-available
13522764	Crictl/Minikube version mismatch causes errors in k8s setup	"We observed constant failures in the e2e k8s tests with permission issues. This was initially accidentally reported through FLINK-29671. But FLINK-29671 actually covers a different instability.

Here are the build failures initially reported in FLINK-29671:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45548&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4972]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4900]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=5597]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4818]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=5852]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45591&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4915]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45598&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4921]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45603&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4991]

{code:java}
Feb 01 11:00:45 Starting minikube ...
Feb 01 11:00:45 * minikube v1.29.0 on Ubuntu 20.04
Feb 01 11:00:45 * Using the none driver based on existing profile
Feb 01 11:00:45 * Starting control plane node minikube in cluster minikube
Feb 01 11:00:45 * Restarting existing none bare metal machine for ""minikube"" ...
Feb 01 11:00:45 * OS release is Ubuntu 20.04.5 LTS
Feb 01 11:01:22 
X Exiting due to RUNTIME_ENABLE: Temporary Error: sudo /usr/local/bin/crictl version: exit status 1
stdout:
[...]  
Feb 01 11:01:22 
E0201 11:01:22.809164  241870 root.go:80] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
Feb 01 11:01:22 
X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
* Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
* Related issue: https://github.com/kubernetes/minikube/issues/9165
[...]{code}"	FLINK	Resolved	1	1	6847	pull-request-available, test-stability
13541200	EmbeddedLeaderService doesn't handle the leader events properly in edge cases	"The leadership is granted when registering the first contender. This sets the leadership flag within the EmbeddedLeaderService (see [EmbeddedLeaderService:312ff|https://github.com/apache/flink/blob/033aca7566a0a561410b3c0e1ae8dca856cd26ce/flink-runtime/src/main/java/org/apache/flink/runtime/highavailability/nonha/embedded/EmbeddedLeaderService.java#L312]: the grantLeadershipCall is triggered afterwards informing the contender about its leadership). In the meantime, close can be called on the contender which deregisters the contender again calling revoke on the contender without having been able to gain the leadership.

This issue was introduced by FLINK-30765."	FLINK	In Progress	4	7	6847	pull-request-available, stale-assigned
13553700	misc module: YARN tests are flaky	"https://github.com/XComp/flink/actions/runs/6473584177/job/17581942919

{code}
2023-10-10T23:16:09.3548634Z Oct 10 23:16:09 23:16:09.354 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.664 s <<< FAILURE! - in org.apache.flink.yarn.YarnPrioritySchedulingITCase
2023-10-10T23:16:09.3564980Z Oct 10 23:16:09 23:16:09.354 [ERROR] org.apache.flink.yarn.YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority  Time elapsed: 1.226 s  <<< ERROR!
2023-10-10T23:16:09.3565608Z Oct 10 23:16:09 java.lang.RuntimeException: Runner failed with exception.
2023-10-10T23:16:09.3566290Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase.startWithArgs(YarnTestBase.java:949)
2023-10-10T23:16:09.3566954Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnPrioritySchedulingITCase.lambda$yarnApplication_submissionWithPriority_shouldRespectPriority$0(YarnPrioritySchedulingITCase.java:45)
2023-10-10T23:16:09.3567646Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:09.3568447Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority(YarnPrioritySchedulingITCase.java:41)
2023-10-10T23:16:09.3569187Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:09.3569805Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:09.3570485Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:09.3571052Z Oct 10 23:16:09 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:09.3571527Z Oct 10 23:16:09 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:09.3572075Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:09.3572716Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:09.3573350Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:09.3573954Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:09.3574665Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:09.3575378Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:09.3576139Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:09.3576852Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:09.3577539Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:09.3578225Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:09.3578898Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:09.3579568Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:09.3580243Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:09.3580917Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:09.3581584Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3582276Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:09.3582952Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:09.3583651Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:09.3584282Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:09.3584914Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3585589Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3586368Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3587030Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3587749Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3588550Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3589237Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3590109Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3591187Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:09.3592081Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:09.3592840Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3593551Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3594182Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3594898Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3595600Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3596324Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3596994Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3597858Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3598950Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:09.3599832Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:09.3600565Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3601268Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3601893Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3602519Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3603281Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3604234Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3604926Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3605850Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3606648Z Oct 10 23:16:09 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:09.3607160Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:09.3607702Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:09.3608332Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:09.3608855Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:09.3609416Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:09.3610332Z Oct 10 23:16:09 Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster
2023-10-10T23:16:09.3611024Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:479)
2023-10-10T23:16:09.3611663Z Oct 10 23:16:09 	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:604)
2023-10-10T23:16:09.3612282Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase$Runner.run(YarnTestBase.java:1154)
2023-10-10T23:16:09.3613022Z Oct 10 23:16:09 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:09.3613849Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:09.3614501Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:09.3615153Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:472)
2023-10-10T23:16:09.3615632Z Oct 10 23:16:09 	... 2 more
2023-10-10T23:16:09.3615862Z Oct 10 23:16:09 
2023-10-10T23:16:10.1130633Z Oct 10 23:16:10 23:16:10.112 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-10-10T23:16:11.1935140Z Oct 10 23:16:11 23:16:11.192 [INFO] Running org.apache.flink.yarn.YARNApplicationITCase
2023-10-10T23:16:17.2611617Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2612300Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2612901Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2613468Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
2023-10-10T23:16:17.2614031Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2614540Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2615047Z Oct 10, 2023 11:16:13 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2615720Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2616208Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2617123Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2617787Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2618489Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2619296Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2619952Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2620582Z Oct 10, 2023 11:16:14 PM com.google.inject.servlet.GuiceFilter setPipeline
2023-10-10T23:16:17.2621470Z WARNING: Multiple Servlet injectors detected. This is a warning indicating that you have more than one GuiceFilter running in your web application. If this is deliberate, you may safely ignore this message. If this is NOT deliberate however, your application may not work as expected.
2023-10-10T23:16:17.2622217Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2622759Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
2023-10-10T23:16:17.2623380Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2623919Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2624481Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2625294Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2625850Z Oct 10, 2023 11:16:14 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2626484Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2627039Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2627774Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2628487Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2629143Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2629800Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2630486Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2631081Z Oct 10, 2023 11:16:14 PM com.google.inject.servlet.GuiceFilter setPipeline
2023-10-10T23:16:17.2631822Z WARNING: Multiple Servlet injectors detected. This is a warning indicating that you have more than one GuiceFilter running in your web application. If this is deliberate, you may safely ignore this message. If this is NOT deliberate however, your application may not work as expected.
2023-10-10T23:16:17.2632613Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2633238Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
2023-10-10T23:16:17.2633853Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2634431Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2638068Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2638660Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2639242Z Oct 10, 2023 11:16:14 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2639820Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2640381Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2641137Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2641897Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2642619Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2643219Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2644145Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2646774Z Oct 10 23:16:17 Formatting using clusterid: testClusterID
2023-10-10T23:16:18.7266553Z Oct 10 23:16:18 23:16:18.726 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 7.525 s <<< FAILURE! - in org.apache.flink.yarn.YARNApplicationITCase
2023-10-10T23:16:18.7276855Z Oct 10 23:16:18 23:16:18.726 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion  Time elapsed: 0.242 s  <<< ERROR!
2023-10-10T23:16:18.7278287Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7279023Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7279745Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7280549Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion$1(YARNApplicationITCase.java:72)
2023-10-10T23:16:18.7281269Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7282068Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion(YARNApplicationITCase.java:70)
2023-10-10T23:16:18.7282892Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7283646Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7284329Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7284895Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7285448Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7286066Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7286781Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7287482Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7288165Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7288983Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7289779Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7290658Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7291445Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7292193Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7292954Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7293781Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7294515Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7295309Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7296068Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7296820Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7298389Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7299074Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7299710Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7300336Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7300981Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7301613Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7302172Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7302727Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7303360Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7304014Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7304630Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7305425Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7306495Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7307582Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7308509Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7309149Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7309816Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7310378Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7310936Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7311567Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7312301Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7312921Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7313708Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7314698Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7315498Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7316174Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7316814Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7317364Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7317914Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7318538Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7319185Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7319804Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7320583Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7321309Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7321766Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7322238Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7335962Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7336477Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7336970Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7337680Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7338554Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7339149Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7339757Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7340170Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7340437Z Oct 10 23:16:18 
2023-10-10T23:16:18.7344548Z Oct 10 23:16:18 23:16:18.732 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithRemoteUserJar  Time elapsed: 0.554 s  <<< ERROR!
2023-10-10T23:16:18.7345418Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7346043Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7346765Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7347401Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithRemoteUserJar$2(YARNApplicationITCase.java:86)
2023-10-10T23:16:18.7347951Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7348549Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithRemoteUserJar(YARNApplicationITCase.java:84)
2023-10-10T23:16:18.7349174Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7349713Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7350446Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7350937Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7351416Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7351965Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7352606Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7353236Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7353840Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7354480Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7355203Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7355951Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7356667Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7357354Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7358040Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7358719Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7359434Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7360111Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7360797Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7361504Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7362189Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7362863Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7363762Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7364405Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7365043Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7365673Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7366233Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7366788Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7367690Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7368345Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7368964Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7369752Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7370829Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7371914Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7372743Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7373385Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7374007Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7374561Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7375120Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7375746Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7376395Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7377016Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7377868Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7378859Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7379707Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7380337Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7380964Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7381523Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7382136Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7382756Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7383402Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7384019Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7384791Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7385545Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7386000Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7386477Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7386946Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7399865Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7400385Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7401100Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7401860Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7402463Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7403060Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7404050Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7404263Z Oct 10 23:16:18 
2023-10-10T23:16:18.7404867Z Oct 10 23:16:18 23:16:18.737 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion  Time elapsed: 0.006 s  <<< ERROR!
2023-10-10T23:16:18.7405830Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7406446Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7407060Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7407923Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion$0(YARNApplicationITCase.java:62)
2023-10-10T23:16:18.7408561Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7409274Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion(YARNApplicationITCase.java:60)
2023-10-10T23:16:18.7410056Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7410601Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7411193Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7411685Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7412290Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7412842Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7413472Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7414100Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7414702Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7415340Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7416112Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7416861Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7417556Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7418232Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7418920Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7419588Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7420253Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7420937Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7421621Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7422293Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7422964Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7423638Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7424278Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7424956Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7425602Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7426235Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7426860Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7427420Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7428035Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7428687Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7429351Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7430139Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7431226Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7432309Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7433117Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7433795Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7434496Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7435049Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7435604Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7436234Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7436874Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7437499Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7438284Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7439272Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7440071Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7440681Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7441310Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7441865Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7442412Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7443068Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7448970Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7449605Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7450536Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7451260Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7451723Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7452269Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7452735Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7453187Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7453670Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7454381Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7455128Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7455775Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7456391Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7456805Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7457010Z Oct 10 23:16:18 
2023-10-10T23:16:19.0886760Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0887052Z Oct 10 23:16:19 23:16:19.087 [INFO] Results:
2023-10-10T23:16:19.0887292Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0887538Z Oct 10 23:16:19 23:16:19.087 [ERROR] Errors: 
2023-10-10T23:16:19.0890525Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion:70->YarnTestBase.runTest:303->lambda$testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion$1:72->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0892075Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion:60->YarnTestBase.runTest:303->lambda$testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion$0:62->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0893304Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithRemoteUserJar:84->YarnTestBase.runTest:303->lambda$testApplicationClusterWithRemoteUserJar$2:86->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0894114Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNITCase.testPerJobWithProvidedLibDirs » Remote File /flink-provided-lib/fli...
2023-10-10T23:16:19.0895062Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority:41->YarnTestBase.runTest:303->lambda$yarnApplication_submissionWithPriority_shouldRespectPriority$0:45->YarnTestBase.startWithArgs:949 » Runtime
2023-10-10T23:16:19.0895643Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0895944Z Oct 10 23:16:19 23:16:19.087 [ERROR] Tests run: 27, Failures: 0, Errors: 5, Skipped: 0
{code}"	FLINK	Closed	3	7	6847	github-actions, test-stability
13553705	.scalafmt.conf cannot be found in Test packaging/licensing job	https://github.com/XComp/flink/actions/runs/6473584177/job/17581941684#step:8:4327	FLINK	Resolved	3	7	6847	github-actions, test-stability
13433283	ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification failed on azure	"
{code:java}
Mar 10 09:16:30 [ERROR] org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification  Time elapsed: 20.752 s  <<< ERROR!
Mar 10 09:16:30 java.lang.NullPointerException
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.lambda$null$9(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:292)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:161)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:137)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.lambda$testNewLeaderAfterReconnectTriggersListenerNotification$10(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:288)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testWithQueueLeaderElectionListener(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:313)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:250)
Mar 10 09:16:30 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 10 09:16:30 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 10 09:16:30 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 10 09:16:30 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 10 09:16:30 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32815&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7544"	FLINK	Resolved	3	1	6847	pull-request-available, test-stability
13427234	FileSystemJobResultStore fails to access Minio	"We're experiencing issues with accessing Minio-backed filesystems (probably s3 object stores in general). The base directory appears to be not created.
{code:java}
2022-02-08 13:13:31,682 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkRuntimeException: Could not retrieve JobResults of globally-terminated jobs from JobResultStore
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_322]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) [?:1.8.0_322]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606) [?:1.8.0_322]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_322]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_322]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_322]
Caused by: org.apache.flink.util.FlinkRuntimeException: Could not retrieve JobResults of globally-terminated jobs from JobResultStore
	at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResults(SessionDispatcherLeaderProcess.java:186) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.dispatcher.runner.AbstractDispatcherLeaderProcess.supplyUnsynchronizedIfRunning(AbstractDispatcherLeaderProcess.java:198) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResultsIfRunning(SessionDispatcherLeaderProcess.java:178) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_322]
	... 3 more
Caused by: java.io.FileNotFoundException: No such file or directory: s3://store/myorg/myscope/3aa35e65-df86-4b16-8cc7-7c75af879317-test-job-name-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa/ha/job-result-store/default
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2344) ~[?:?]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2226) ~[?:?]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2160) ~[?:?]
	at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.getFileStatus(HadoopFileSystem.java:85) ~[?:?]
	at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.getFileStatus(PluginFileSystemFactory.java:105) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.highavailability.FileSystemJobResultStore.getDirtyResultsInternal(FileSystemJobResultStore.java:158) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.highavailability.AbstractThreadsafeJobResultStore.withReadLock(AbstractThreadsafeJobResultStore.java:118) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.highavailability.AbstractThreadsafeJobResultStore.getDirtyResults(AbstractThreadsafeJobResultStore.java:100) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResults(SessionDispatcherLeaderProcess.java:184) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.dispatcher.runner.AbstractDispatcherLeaderProcess.supplyUnsynchronizedIfRunning(AbstractDispatcherLeaderProcess.java:198) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResultsIfRunning(SessionDispatcherLeaderProcess.java:178) ~[flink-dist-flink-nightly.jar:flink-nightly]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_322]
	... 3 more {code}"	FLINK	Resolved	2	7	6847	pull-request-available
13419212	Introduce JobResultStore	"This issue includes introducing the interface and coming up with a in-memory implementation of it that should be integrated into the {{Dispatcher}}. 
* We’ll introduce the new interface {{JobResultStore}}
* We’ll remove the {{RunningJobsRegistry}} a replace its functionality with {{(Standalone|Embedded)JobResultStore}} (This is basically only about {{RunningJobsRegistry#setJobFinished}} method
* The {{JobResultStore}} should be initialized along the {{JobGraphWriter}} since both components are closely related."	FLINK	Resolved	3	7	6847	pull-request-available
13426572	Reorganizes tests around Dispatcher cleanup	FLINK-25432 introduced new interfaces for the local and global cleanup of job-related data. This enables us reorganize tests (more specifically {{DispatcherCleanupResourcesTest}}).	FLINK	Resolved	3	7	6847	pull-request-available
13574442	PullRequest template doesn't use the correct format to refer to the testing code convention	The PR template refers to https://flink.apache.org/contributing/code-style-and-quality-common.html#testing rather than https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#7-testing	FLINK	Resolved	4	1	6847	pull-request-available
13590912	Disabling japicmp plugin for deprecated APIs	The Apache Flink 2.0 release allows for the removal of public API. The japicmp plugin usually checks for these kind of changes. To avoid adding explicit excludes for each change, this Jira issue suggest to disable the API check for APIs that are marked as deprecated.	FLINK	Resolved	3	4	6847	pull-request-available
13434170	DefaultJobGraphStore.globalCleanup does not trigger the cleanup when retrying the cleanup after a failover	The {{DefaultJobGraphStore}} does not get the JobGraph of a dirty globally-terminated job after failover anymore because the job is not going to be recovered. The globalCleanup checks the addedJobs still, which is not necessary but prevents the cleanup in that case.	FLINK	Resolved	1	1	6847	pull-request-available
13530212	docker-build.sh build fails on Linux machines	"Building the Flink website on Linux fails due to how Docker is used as a Daemon running under root in Linux (see [this blog|https://jtreminio.com/blog/running-docker-containers-as-current-host-user/#ok-so-what-actually-works] for more details).

Building the website will fail when copying the artifacts because they are owned by the root user. 

{code}
./docker-build.sh build                                                                                     
latest: Pulling from jakejarvis/hugo-extended                                                                                                
Digest: sha256:7d7eb41d7949b5ed338c27926098b84e152e7e1d8ad8f1955c29b383a2336548                                                              
Status: Image is up to date for jakejarvis/hugo-extended:latest                                                                              
docker.io/jakejarvis/hugo-extended:latest                                                                                                    
Start building sites …                                                                                                                       
hugo v0.111.3-5d4eb5154e1fed125ca8e9b5a0315c4180dab192+extended linux/amd64 BuildDate=2023-03-12T11:40:50Z VendorInfo=docker                 
Error: Error building site: open /src/target/news/2014/08/26/release-0.6.html: permission denied                                             
Total in 153 ms                                                                                                                              
mv: cannot move 'docs/target/2014' to 'content/2014': Permission denied                                                                      
mv: cannot move 'docs/target/2015' to 'content/2015': Permission denied                                                                      
mv: cannot move 'docs/target/2016' to 'content/2016': Permission denied                                                                      
mv: cannot move 'docs/target/2017' to 'content/2017': Permission denied                                                                      
mv: cannot move 'docs/target/2018' to 'content/2018': Permission denied                                                                      
mv: cannot move 'docs/target/2019' to 'content/2019': Permission denied                                                                      
mv: cannot move 'docs/target/2020' to 'content/2020': Permission denied                                                                      
mv: cannot move 'docs/target/2021' to 'content/2021': Permission denied                                                                      
mv: cannot move 'docs/target/2022' to 'content/2022': Permission denied                                                                                                                                                                                                                   
mv: cannot move 'docs/target/2023' to 'content/2023': Permission denied                                                                      
mv: cannot move 'docs/target/categories' to 'content/categories': Permission denied
[...]
{code}"	FLINK	Resolved	3	1	6847	pull-request-available
13345832	Remove SlotSharingManager	Once the {{Scheduler}} is removed, then we can also remove the {{SlotSharingManager}}. This will allow us to remove the dynamic slot sharing functionality which was hard to maintain and to understand.	FLINK	Closed	3	7	6847	pull-request-available
13581918	Move CheckpointStatsTracker out of ExecutionGraph into Scheduler	The scheduler needs to know about the CheckpointStatsTracker to allow listening to checkpoint failures and completion.	FLINK	Resolved	3	7	6847	pull-request-available
13523205	"Too many CI failed due to ""Could not connect to azure.archive.ubuntu.com"""	"!image-2023-02-06-17-59-20-019.png!

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45762&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13556226	HAJobRunOnHadoopS3FileSystemITCase failed due to NoSuchMethodError	"{code:java}
Error: 23:16:52 23:16:52.433 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.271 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HAJobRunOnHadoopS3FileSystemITCase
37033Error: 23:16:52 23:16:52.433 [ERROR] org.apache.flink.fs.s3hadoop.HAJobRunOnHadoopS3FileSystemITCase  Time elapsed: 6.271 s  <<< ERROR!
37034Oct 10 23:16:52 java.lang.NoSuchMethodError: 'void org.apache.hadoop.security.HadoopKerberosName.setRuleMechanism(java.lang.String)'
37035Oct 10 23:16:52 	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:84)
37036Oct 10 23:16:52 	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315)
37037Oct 10 23:16:52 	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
37038Oct 10 23:16:52 	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
37039Oct 10 23:16:52 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:474)
37040Oct 10 23:16:52 	at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:129)
37041Oct 10 23:16:52 	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:508)
37042Oct 10 23:16:52 	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
37043Oct 10 23:16:52 	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:279)
37044Oct 10 23:16:52 	at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:99)
37045Oct 10 23:16:52 	at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:86)
37046Oct 10 23:16:52 	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:89)
37047Oct 10 23:16:52 	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createAvailableOrEmbeddedServices(HighAvailabilityServicesUtils.java:69)
37048Oct 10 23:16:52 	at org.apache.flink.runtime.minicluster.MiniCluster$RegularHighAvailabilityServicesFactory.createHAServices(MiniCluster.java:1530)
37049Oct 10 23:16:52 	at org.apache.flink.runtime.minicluster.MiniCluster.createHighAvailabilityServices(MiniCluster.java:617)
37050Oct 10 23:16:52 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:438)
37051Oct 10 23:16:52 	at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:246)
37052Oct 10 23:16:52 	at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:110)
37053Oct 10 23:16:52 	at org.apache.flink.runtime.testutils.InternalMiniClusterExtension.beforeAll(InternalMiniClusterExtension.java:72)
37054Oct 10 23:16:52 	at org.apache.flink.test.junit5.MiniClusterExtension.beforeAll(MiniClusterExtension.java:231)
[...] {code}
[Run #14|https://github.com/XComp/flink/actions/runs/6472816505/job/17575963787#step:11:37035] in the {{finegrained_resourcemanagement}} stage (see FLINK-33245)

[Run #11|https://github.com/XComp/flink/actions/runs/6471147857/job/17571310183#step:11:41740] in the {{finegrained_resourcemanagement}} stage (see FLINK-33245)"	FLINK	Resolved	3	7	6847	github-actions, test-stability
13562469	Template: Add CI template for running Flink's test suite	We want to have a template that runs the entire Flink test suite.	FLINK	Closed	3	7	6847	github-actions
13557347	Rely on Maven wrapper instead of having custom Maven installation logic	I noticed that we could use the Maven wrapper instead of having a custom setup logic for Maven in CI.	FLINK	Resolved	3	4	6847	pull-request-available
13431872	FileSystem.delete is not implemented consistently	"The BlobServer cleanup does not work for the Presto S3 filesystem in case of failure due to some bug in the recursive delete implementation. The {{false}} return value is not processed which leads to an error case being ""swallowed"", i.e. recursive cleanups do not work in this case (see [PrestoS3FileSystem:496|https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystem.java#L496])"	FLINK	Resolved	2	1	6847	pull-request-available
13339733	DispatcherTest.testOnRemovedJobGraphDoesNotCleanUpHAFiles does not test the desired functionality	{{DispatcherTest.testOnRemovedJobGraphDoesNotCleanUpHAFiles}} succeeds but due to different reasons: The used {{TestingJobGraphStore}} is not started. An {{IllegalStateException}} prevents the code from reaching the set `removeJobGraphFuture` to get triggered. Hence, the test succeeds but not for the reason the test was implemented for.	FLINK	Closed	4	1	6847	pull-request-available
13429750	ZooKeeperStateHandleStore does not handle not existing nodes properly in getAllAndLock	[c3a6b514595ea3c1bf52126f6f1715b26c871ae9|https://github.com/apache/flink/commit/c3a6b514595ea3c1bf52126f6f1715b26c871ae9] introduces new exceptions that are not properly handled in [ZooKeeperStateHandleStore:378|https://github.com/apache/flink/blob/0cf7c3dedd3575cdfed57727e9712c28c013d7ca/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L378]	FLINK	Resolved	4	1	6847	pull-request-available
13349168	Analyze whether CoLocationConstraints and CoLocationGroup can be removed	"It appears that {{CoLocationGroup}} and {{CoLocationContraint}} are not used by {{LocalInputPreferredSlotSharingStrategy}} anymore. Instead, {{CoLocationGroupDesc}} and {{CoLocationContraintDesc}} serve as light-weight versions of these concepts.

Does this mean that we can remove the former ones?"	FLINK	Closed	3	7	6847	pull-request-available
13256308	Use Java's Duration instead of Flink's Time	"As discussion in mailing list [here|https://lists.apache.org/x/thread.html/90ad2f1d7856cfe5bdc8f7dd678c626be96eeaeeb736e98f31660039@%3Cdev.flink.apache.org%3E] the community reaches a consensus that we will use Java's Duration for representing ""time interval"" instead of use Flink's Time for it.

Specifically, Flink has two {{Time}} classes, which are

{{org.apache.flink.api.common.time.Time}}
{{org.apache.flink.streaming.api.windowing.time.Time}}

the latter has been already deprecated and superseded by the former. Now we want to also deprecated the former and drop it in 2.0.0(we don't drop it just now because it is part of {{@Public}} interfaces)."	FLINK	Closed	3	7	6847	2.0-related, pull-request-available
13544715	JobMasterTest.testRetrievingCheckpointStats fails with NPE on AZP	"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51452&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8654
fails with NPE as
{noformat}
Jul 20 01:01:33 01:01:33.491 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats  Time elapsed: 0.036 s  <<< ERROR!
Jul 20 01:01:33 java.lang.NullPointerException
Jul 20 01:01:33 	at org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats(JobMasterTest.java:2132)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 20 01:01:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 20 01:01:33 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 20 01:01:33 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
...
{noformat}"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13315521	"ZKCheckpointIDCounterMultiServersTest.testRecoveredAfterConnectionLoss failed with ""Address already in use"""	"[https://travis-ci.org/github/apache/flink/jobs/705770513]

{code}
15:09:34.674 [ERROR] testRecoveredAfterConnectionLoss(org.apache.flink.runtime.checkpoint.ZKCheckpointIDCounterMultiServersTest)  Time elapsed: 5.74 s  <<< ERROR!
java.net.BindException: Address already in use
{code}"	FLINK	Closed	3	4	6847	test-stability
13362235	"testMapAfterRepartitionHasCorrectParallelism2 Fail because of ""NoResourceAvailableException"" "	"
{code:java}
2021-03-04T00:17:41.2017402Z [ERROR] testMapAfterRepartitionHasCorrectParallelism2[Execution mode = CLUSTER](org.apache.flink.api.scala.operators.PartitionITCase)  Time elapsed: 300.117 s  <<< ERROR!
2021-03-04T00:17:41.2018058Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-03-04T00:17:41.2018525Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-03-04T00:17:41.2019563Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-03-04T00:17:41.2020129Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-03-04T00:17:41.2021974Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-03-04T00:17:41.2022634Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-03-04T00:17:41.2023118Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-03-04T00:17:41.2023682Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
2021-03-04T00:17:41.2024244Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-03-04T00:17:41.2024749Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-03-04T00:17:41.2025261Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-03-04T00:17:41.2026070Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-03-04T00:17:41.2026814Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1066)
2021-03-04T00:17:41.2027633Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2021-03-04T00:17:41.2028245Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-03-04T00:17:41.2028796Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-03-04T00:17:41.2029327Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-03-04T00:17:41.2030017Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-03-04T00:17:41.2030795Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-03-04T00:17:41.2031885Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2021-03-04T00:17:41.2032678Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2021-03-04T00:17:41.2033428Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2021-03-04T00:17:41.2034197Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2021-03-04T00:17:41.2035094Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-03-04T00:17:41.2035915Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2021-03-04T00:17:41.2036617Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2021-03-04T00:17:41.2037537Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2021-03-04T00:17:41.2038019Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-03-04T00:17:41.2038554Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2021-03-04T00:17:41.2039117Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-03-04T00:17:41.2039671Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2021-03-04T00:17:41.2040159Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2021-03-04T00:17:41.2040632Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2021-03-04T00:17:41.2041086Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-03-04T00:17:41.2041810Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-03-04T00:17:41.2042514Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-03-04T00:17:41.2042977Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-03-04T00:17:41.2043425Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-03-04T00:17:41.2043887Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-03-04T00:17:41.2044399Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-03-04T00:17:41.2044991Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:130)
2021-03-04T00:17:41.2045695Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:81)
2021-03-04T00:17:41.2046343Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:221)
2021-03-04T00:17:41.2047000Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:212)
2021-03-04T00:17:41.2047579Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:203)
2021-03-04T00:17:41.2048171Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:696)
2021-03-04T00:17:41.2049092Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2021-03-04T00:17:41.2049893Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1470)
2021-03-04T00:17:41.2050492Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1111)
2021-03-04T00:17:41.2050989Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1051)
2021-03-04T00:17:41.2051474Z 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:885)
2021-03-04T00:17:41.2052211Z 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:661)
2021-03-04T00:17:41.2052877Z 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)
2021-03-04T00:17:41.2053654Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:505)
2021-03-04T00:17:41.2054285Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:490)
2021-03-04T00:17:41.2054838Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2021-03-04T00:17:41.2055323Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2021-03-04T00:17:41.2055805Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-03-04T00:17:41.2056318Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-03-04T00:17:41.2056943Z 	at org.apache.flink.runtime.scheduler.SharedSlot.cancelLogicalSlotRequest(SharedSlot.java:222)
2021-03-04T00:17:41.2057554Z 	at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.cancelLogicalSlotRequest(SlotSharingExecutionSlotAllocator.java:164)
2021-03-04T00:17:41.2058220Z 	at org.apache.flink.runtime.scheduler.SharingPhysicalSlotRequestBulk.cancel(SharingPhysicalSlotRequestBulk.java:86)
2021-03-04T00:17:41.2058875Z 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkWithTimestamp.cancel(PhysicalSlotRequestBulkWithTimestamp.java:66)
2021-03-04T00:17:41.2059642Z 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:91)
2021-03-04T00:17:41.2060319Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-03-04T00:17:41.2060938Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-03-04T00:17:41.2061472Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-03-04T00:17:41.2062265Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-03-04T00:17:41.2062824Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-03-04T00:17:41.2063375Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-03-04T00:17:41.2063821Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-03-04T00:17:41.2064246Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-03-04T00:17:41.2064669Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-03-04T00:17:41.2065093Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-03-04T00:17:41.2065537Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-03-04T00:17:41.2065975Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-03-04T00:17:41.2066390Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-03-04T00:17:41.2066798Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-03-04T00:17:41.2067249Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-03-04T00:17:41.2067916Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-03-04T00:17:41.2068415Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-03-04T00:17:41.2068785Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-03-04T00:17:41.2069166Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-03-04T00:17:41.2069523Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-03-04T00:17:41.2069784Z 	... 4 more
2021-03-04T00:17:41.2070383Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
2021-03-04T00:17:41.2071162Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2021-03-04T00:17:41.2071905Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2021-03-04T00:17:41.2072420Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2021-03-04T00:17:41.2073089Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-03-04T00:17:41.2073462Z 	... 31 more
2021-03-04T00:17:41.2073977Z Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
2021-03-04T00:17:41.2074838Z 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)
2021-03-04T00:17:41.2075386Z 	... 24 more
2021-03-04T00:17:41.2075706Z Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 ms
2021-03-04T00:17:41.2076048Z 	... 25 more
{code}
"	FLINK	Closed	3	1	6847	test-stability
13515060	DefaultMultipleComponentLeaderElectionService triggers HA backend change even if it's not the leader	{{DefaultMultipleComponentLeaderElectionService}} calls {{LeaderElectionEventHandler#onLeaderInformationChange}} in any case even though the contracts of that method states that it should be only called by the leader to update the HA backend information (see [JavaDoc|https://github.com/apache/flink/blob/5a2f220e31c50306a60aae8281f0ab4073fb85e1/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/LeaderElectionEventHandler.java#L46-L50]).	FLINK	Closed	2	7	6847	pull-request-available
13522716	KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock	"We're seeing a test failure in {{KubernetesHighAvailabilityRecoverFromSavepointITCase}} due to a deadlock:
{code:java}
2023-02-01T18:53:35.5540322Z ""ForkJoinPool-1-worker-1"" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]
2023-02-01T18:53:35.5540900Z    java.lang.Thread.State: TIMED_WAITING (parking)
2023-02-01T18:53:35.5541272Z 	at sun.misc.Unsafe.park(Native Method)
2023-02-01T18:53:35.5541932Z 	- parking to wait for  <0x00000000d14d7b60> (a java.util.concurrent.CompletableFuture$Signaller)
2023-02-01T18:53:35.5542496Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2023-02-01T18:53:35.5543088Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)
2023-02-01T18:53:35.5543672Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2023-02-01T18:53:35.5544240Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)
2023-02-01T18:53:35.5544801Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-01T18:53:35.5545632Z 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)
2023-02-01T18:53:35.5546409Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=61916]

The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474"	FLINK	Resolved	1	1	6847	pull-request-available, test-stability
13495972	Stacktrace printing in DefaultExecutionGraphCacheTest is confusing maven test log output	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42849&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8700]

 

 
{code:java}
java.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (408c1ab89f41c2d4f99c870e8abde94d)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest.testImmediateCacheInvalidationAfterFailure(DefaultExecutionGraphCacheTest.java:147)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
{code}
 

 "	FLINK	Resolved	4	1	6847	pull-request-available
13562460	tools/azure-pipelines/debug_files_utils.sh should support GHA output as well	{{tools/azure-pipelines/debug_files_utils.sh}} sets variables to reference the debug output. This is backend-specific and only supports Azure CI right now. We should add support for GHA.	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
13564080	Add docs generation to workflow	"The goal of this subtask is to mimick the optional docs generation from the Azure Pipelines workflow. This includes:
* Running the docs check in each PR CI if some artifacts in the ./docs folder were touched
* Do not run the docs build if it's not a PR
* Skip e2e tests in the Flink CI template if the PR is an docs-only change"	FLINK	In Progress	3	7	6847	github-actions, pull-request-available
13434297	FileSystemJobResultStore#constructDirtyPath might lost the scheme	" 
{code:java}
/**
     * Given a job ID, construct the path for a dirty entry corresponding to it in the job result
     * store.
     *
     * @param jobId The job ID to construct a dirty entry path from.
     * @return A path for a dirty entry for the given the Job ID.
     */
    private Path constructDirtyPath(JobID jobId) {
        return new Path(this.basePath.getPath(), jobId.toString() + DIRTY_FILE_EXTENSION);
    } {code}
 

Just like above piece of code, we are using {{{}this.basePath.getPath(){}}}, not directly use {{this.basePath}} when create a new Path. I am afraid this will cause scheme lost and cause issue when some filesystem implementation tries to stat the path."	FLINK	Resolved	1	1	6847	pull-request-available
13562457	Add zip  as a package to GitHub Actions runners	"FLINK-33253 shows that {{test_pyflink.sh}} fails in GHA because it doesn't find {{{}zip{}}}. We should add this as a dependency in the e2e test.
{code:java}
/root/flink/flink-end-to-end-tests/test-scripts/test_pyflink.sh: line 107: zip: command not found {code}"	FLINK	Closed	3	7	6847	github-actions, pull-request-available
13219310	Standby per job mode Dispatchers don't know job's JobSchedulingStatus	"At the moment, it can happen that standby {{Dispatchers}} in per job mode will restart a terminated job after they gained leadership. The problem is that we currently clear the {{RunningJobsRegistry}} once a job has reached a globally terminal state. After the leading {{Dispatcher}} terminates, a standby {{Dispatcher}} will gain leadership. Without having the information from the {{RunningJobsRegistry}} it cannot tell whether the job has been executed or whether the {{Dispatcher}} needs to re-execute the job. At the moment, the {{Dispatcher}} will assume that there was a fault and hence re-execute the job. This can lead to duplicate results.

I think we need some way to tell standby {{Dispatchers}} that a certain job has been successfully executed. One trivial solution could be to not clean up the {{RunningJobsRegistry}} but then we will clutter ZooKeeper."	FLINK	Closed	3	1	6847	pull-request-available
13335637	Create or extend REST endpoint to expose memory configuration	We need to expose the memory configuration finally used within the JobManager. The effective configuration is calculated differently depending on the type of cluster (legacy vs containerized). The goal is to have the configuration being calculated only in one place. Therefore, we want to forward the information through environment variables similar to the approach already done for the TaskManager.	FLINK	Closed	3	7	6847	pull-request-available
13430177	Make max retries configurable	"Right now, the retry mechanism is hard-coded to {{Integer.MAX_VALUE}}. We want to make that configurable as well and keep the default value at {{MAX_VALUE}}. This enables the user to disable the retry mechanism if necessary.

We verify how retries strategies are configured in other places and align with that to have a consistent user experience."	FLINK	Resolved	3	7	6847	pull-request-available
13540393	KubernetesTestFixture doesn't implement the checkAndUpdateConfigMapFunction properly	"[FlinkKubeClient.checkAndUpdateConfigMap|https://github.com/apache/flink/blob/ab3eb40d920fa609f49164a0bbb5fcbb3004a808/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/FlinkKubeClient.java#L163] expects an error to be forwarded through the {{CompletableFuture}} instead of throwing a {{RuntimeException}}.

The actual implementation implements it accordingly in [Fabric8FlinkKubeClient:313|https://github.com/apache/flink/blob/025a95b627faf8ec8b725a7784d1279b41e10ba7/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/Fabric8FlinkKubeClient.java#L313] where a {{CompletionException}} is thrown within the {{CompletableFuture}}'s {{supplyAsync}} call resulting the future to fail.

{{KubernetesTestFixture}} doesn't make the returned future complete exceptionally but throws a {{CompletionException}} (see [KubernetesTestFixture:172|https://github.com/apache/flink/blob/6fc5f789869433688eb5f62494f1a4404e0dd11b/flink-kubernetes/src/test/java/org/apache/flink/kubernetes/highavailability/KubernetesTestFixture.java#L172]).

This results in inconsistent test behavior."	FLINK	Resolved	3	1	6847	pull-request-available
13581917	Introduces RescaleManager#onTrigger endpoint	The new endpoint would allow use from separating observing change events from actually triggering the rescale operation.	FLINK	Resolved	3	7	6847	pull-request-available
13354616	YARNSessionFIFOSecuredITCase cannot connect to BlobServer	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12483&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9

{code}
java.io.IOException: Could not connect to BlobServer at address 29c91476178c/172.21.0.2:44412
java.io.IOException: Could not connect to BlobServer at address 29c91476178c/172.21.0.2:44412
	at org.apache.flink.runtime.blob.BlobClient.<init>(BlobClient.java:102) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.blob.BlobClient.downloadFromBlobServer(BlobClient.java:137) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:538)
	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


{code}"	FLINK	Closed	3	1	6847	pull-request-available, test-stability
13565966	Job doesn't disconnect from ResourceManager	"https://github.com/XComp/flink/actions/runs/7634987973/job/20800205972#step:10:14557

{code}
[...]
""main"" #1 prio=5 os_prio=0 tid=0x00007fcccc4b7000 nid=0x24ec0 waiting on condition [0x00007fccce1eb000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000bdd52618> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2099)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2077)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:876)
	at org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.testHopWindow_Cube(WindowDistinctAggregateITCase.scala:550)
[...]
{code}"	FLINK	In Progress	2	1	6847	github-actions, pull-request-available, test-stability
13510803	Migrate ZooKeeperLeaderElectionTest	"||ZooKeeperLeaderElectionTest||ZooKeeperMultipleComponentLeaderElectionDriverTest||
|testZooKeeperLeaderElectionRetrieval|testPublishLeaderInformation|
|testZooKeeperReelection|testLeaderElectionWithMultipleDrivers|
|testZooKeeperReelectionWithReplacement|<missing>|
|testLeaderShouldBeCorrectedWhenOverwritten|testNonLeaderCannotPublishLeaderInformation (slightly different)|
|testExceptionForwarding|<missing>|
|testEphemeralZooKeeperNodes|<missing>|
|testNotLeaderShouldNotCleanUpTheLeaderInformation|<missing> (but similar to testNonLeaderCannotPublishLeaderInformation)|
|testUnExpectedErrorForwarding|<missing>|"	FLINK	Resolved	3	7	6847	pull-request-available
13426890	Make cancellation of jobs depend on the JobResultStore	"{{JobManagerRunner}} instances were cancellable as long as the instance was still registered in the {{Dispatcher.jobManagerRunnerRegistry}}. With the cleanup being done concurrently (i.e. not relying on the {{JobManagerRunnerRegistry}} to be cleaned up anymore), the cancellation of a job should only be possible as long as the job is not globally finished and before cleanup is triggered.

We should verify whether a job is listed in the {{JobResultStore}} and only enable the user to cancel the job if that's not the case."	FLINK	Resolved	3	7	6847	pull-request-available
13426918	ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers failed	"We experienced a [build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30783&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=15997] in {{ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers}}. The test halted when waiting for the next leader in [ZooKeeperMultipleComponentLeaderElectionDriverTest:256|https://github.com/apache/flink/blob/e8742f7f5cac34852d0e621036e1614bbdfe8ec3/flink-runtime/src/test/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriverTest.java#L256]
{code}
Feb 04 18:02:54 ""main"" #1 prio=5 os_prio=0 tid=0x00007fab0800b800 nid=0xe07 waiting on condition [0x00007fab12574000]
Feb 04 18:02:54    java.lang.Thread.State: WAITING (parking)
Feb 04 18:02:54 	at sun.misc.Unsafe.park(Native Method)
Feb 04 18:02:54 	- parking to wait for  <0x000000008065c5c8> (a java.util.concurrent.CompletableFuture$Signaller)
Feb 04 18:02:54 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Feb 04 18:02:54 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Feb 04 18:02:54 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Feb 04 18:02:54 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Feb 04 18:02:54 	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
Feb 04 18:02:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers(ZooKeeperMultipleComponentLeaderElectionDriverTest.java:256)
[...]
{code}

The extended Maven logs indicate that the timeout happened while waiting for the second leader to be selected.
{code}
Test org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers is running.
--------------------------------------------------------------------------------
17:15:10,437 [           Thread-16] INFO  org.apache.curator.test.TestingZooKeeperMain                 [] - Starting server
17:15:10,450 [                main] INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Enforcing default ACL for ZK connections
17:15:10,451 [                main] INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Using '/flink/default' as Zookeeper namespace.
17:15:10,452 [                main] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Starting
17:15:10,455 [                main] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Default schema
17:15:10,462 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: CONNECTED
17:15:10,467 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
17:15:10,482 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
17:15:10,483 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
17:15:10,483 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
17:15:10,484 [    main-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
17:15:10,562 [    main-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
17:15:10,600 [                main] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
{code}"	FLINK	Resolved	3	1	6847	test-stability
13395798	Test Kafka table connector with new runtime provider	The runtime provider of Kafka table connector has been replaced with new KafkaSource and KafkaSink. The table connector requires to be tested to make sure nothing is surprised to Table/SQL API users. 	FLINK	Closed	1	7	6847	release-testing
13545853	Add .mvn/maven.config to .gitignore	"FLINK-28016 allowed Flink to switch to newer (i.e. 3.2.5+) Maven versions. Maven 3.3.1 introduced certain local configuration files that are automatically injected (e.g. {{.mvn/jvm.options}} and {{.mvn/maven.config}}).

The former one is actively used (and checked into the repository) to define certain Java-related requirements. {{.mvn/maven.config}} instead, is not utilized right now. It can be used, though, to customize the Maven execution to once needs (e.g. always skipping {{flink-runtime-web}} through {{-Pskip-webui-build}}).

To be fair, that would be also possible through the {{${user.home}/.m2/settings.xml}} through profiles. There are certain configuration parameters, though, that are not configurable easily through profiles.

Configuring a checkout-specific local repository isn't possible. Checkout-specific local repositories are handy, though when working in multiple Flink checkouts (e.g. one checkout for reviews and one for contributions). Using a single local repository would cause problems when working on a branch locally while reviewing other PRs (and checking them out) that touch the same files.

A workaround for such a scenario would be to have a checkout-specific settings.xml that specifies a checkout-specific local repository.

To support this, I'm suggesting to add {{.mvn/maven.config}} to {{.gitignore}} for now. That would allow the developer to enable specific Maven parameters by default that match his/her needs. We can revert this change as soon as we identify the need to have this file included in the repository."	FLINK	Resolved	3	4	6847	pull-request-available
13556281	SqlGatewayE2ECase failed due to ConnectException	"The container couldn't be started in [this build|https://github.com/XComp/flink/actions/runs/6696839844/job/18195926497#step:15:11765]:
{code}
Error: 20:18:40 20:18:40.111 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 110.789 s <<< FAILURE! - in org.apache.flink.table.gateway.SqlGatewayE2ECase
Error: 20:18:40 20:18:40.111 [ERROR] org.apache.flink.table.gateway.SqlGatewayE2ECase  Time elapsed: 110.789 s  <<< ERROR!
Oct 30 20:18:40 org.testcontainers.containers.ContainerLaunchException: Container startup failed for image prestodb/hdp2.6-hive:10
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:349)
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.doStart(HiveContainer.java:69)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:322)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1131)
Oct 30 20:18:40 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:28)
Oct 30 20:18:40 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Oct 30 20:18:40 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Oct 30 20:18:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Oct 30 20:18:40 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Oct 30 20:18:40 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Oct 30 20:18:40 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Oct 30 20:18:40 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Oct 30 20:18:40 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Oct 30 20:18:40 Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
Oct 30 20:18:40 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:334)
Oct 30 20:18:40 	... 29 more
Oct 30 20:18:40 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:553)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:344)
Oct 30 20:18:40 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
Oct 30 20:18:40 	... 30 more
Oct 30 20:18:40 Caused by: java.lang.RuntimeException: java.net.ConnectException: Failed to connect to /127.0.0.1:32779
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:93)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.containerIsStarted(GenericContainer.java:712)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:532)
Oct 30 20:18:40 	... 32 more
Oct 30 20:18:40 Caused by: java.net.ConnectException: Failed to connect to /127.0.0.1:32779
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:265)
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:183)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.java:224)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.java:108)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.java:88)
Oct 30 20:18:40 	at okhttp3.internal.connection.Transmitter.newExchange(Transmitter.java:169)
Oct 30 20:18:40 	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:41)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:94)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:88)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:229)
Oct 30 20:18:40 	at okhttp3.RealCall.execute(RealCall.java:81)
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:86)
Oct 30 20:18:40 	... 34 more
Oct 30 20:18:40 Caused by: java.net.ConnectException: Connection refused (Connection refused)
Oct 30 20:18:40 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
Oct 30 20:18:40 	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
Oct 30 20:18:40 	at java.base/java.net.Socket.connect(Socket.java:609)
Oct 30 20:18:40 	at okhttp3.internal.platform.Platform.connectSocket(Platform.java:130)
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:263)
Oct 30 20:18:40 	... 53 more
{code}"	FLINK	Resolved	3	7	6847	github-actions, pull-request-available, test-stability
13562471	Workflow: Add nightly workflow for master and release-1.18	"The nightly builds run on master and the two most-recently released versions of Flink as those are the supported versions. This logic is currently captured in [flink-ci/git-repo-sync:sync_repo.sh|https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh#L28].

In [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure] we decided to go ahead and provide nightly builds for {{master}} and {{{}release-1.18{}}}. Keep in mind that 1.18 has no support for JDK 21"	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
12730082	Projects wiki page link in contribution page is broken	In http://flink.incubator.apache.org/how-to-contribute.html, there is a link to projects wiki page  (http://the/wiki/url). It is broken.	FLINK	Closed	4	1	8669	documentation
12945410	Kafka09ITCase.testBigRecordJob fails on Travis	"The test case {{Kafka09ITCase.testBigRecordJob}} failed on Travis.

https://s3.amazonaws.com/archive.travis-ci.org/jobs/112049279/log.txt"	FLINK	Resolved	3	1	8669	test-stability
13342842	Incorrect use of yarn-session.sh -id vs -yid in log statements.	"The Yarn per-job modes log about the recommended shutdown of yarn, which doesn't work.


See: https://github.com/apache/flink/pull/10964#issuecomment-734166399"	FLINK	Closed	2	1	8669	pull-request-available
13328729	DispatcherResourceCleanupTest#testJobSubmissionUnderSameJobId is unstable on Azure Pipeline	Here is the log and stack: https://dev.azure.com/kevin-flink/3f520f11-5170-4153-99d0-2ade0d99b911/_apis/build/builds/88/logs/102	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13309424	"Kubernetes test fails with ""error: timed out waiting for the condition on jobs/flink-job-cluster"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2697&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-06-04T09:25:40.7205843Z service/flink-job-cluster created
2020-06-04T09:25:40.9661515Z job.batch/flink-job-cluster created
2020-06-04T09:25:41.2189123Z deployment.apps/flink-task-manager created
2020-06-04T10:32:32.6402983Z error: timed out waiting for the condition on jobs/flink-job-cluster
2020-06-04T10:32:33.8057757Z error: unable to upgrade connection: container not found (""flink-task-manager"")
2020-06-04T10:32:33.8111302Z sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-56335570120/out/kubernetes_wc_out*': No such file or directory
2020-06-04T10:32:33.8124455Z FAIL WordCount: Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected e682ec6622b5e83f2eb614617d5ab2cf.
2020-06-04T10:32:33.8125379Z head hexdump of actual:
2020-06-04T10:32:33.8136133Z head: cannot open '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-56335570120/out/kubernetes_wc_out*' for reading: No such file or directory
2020-06-04T10:32:33.8344715Z Debugging failed Kubernetes test:
2020-06-04T10:32:33.8345469Z Currently existing Kubernetes resources
2020-06-04T10:32:36.4977853Z I0604 10:32:36.497383   13191 request.go:621] Throttling request took 1.198606989s, request: GET:https://10.1.0.4:8443/apis/rbac.authorization.k8s.io/v1?timeout=32s
2020-06-04T10:32:46.6975735Z I0604 10:32:46.697234   13191 request.go:621] Throttling request took 4.398107353s, request: GET:https://10.1.0.4:8443/apis/authorization.k8s.io/v1?timeout=32s
2020-06-04T10:32:57.4978637Z I0604 10:32:57.497209   13191 request.go:621] Throttling request took 1.198449167s, request: GET:https://10.1.0.4:8443/apis/apps/v1?timeout=32s
2020-06-04T10:33:07.4980104Z I0604 10:33:07.497320   13191 request.go:621] Throttling request took 4.198274438s, request: GET:https://10.1.0.4:8443/apis/apiextensions.k8s.io/v1?timeout=32s
2020-06-04T10:33:18.4976060Z I0604 10:33:18.497258   13191 request.go:621] Throttling request took 1.19871495s, request: GET:https://10.1.0.4:8443/apis/apps/v1?timeout=32s
2020-06-04T10:33:28.4979129Z I0604 10:33:28.497276   13191 request.go:621] Throttling request took 4.198369672s, request: GET:https://10.1.0.4:8443/apis/rbac.authorization.k8s.io/v1?timeout=32s
2020-06-04T10:33:30.9182069Z NAME                                     READY   STATUS              RESTARTS   AGE
2020-06-04T10:33:30.9184099Z pod/flink-job-cluster-dtb67              0/1     ErrImageNeverPull   0          67m
2020-06-04T10:33:30.9184869Z pod/flink-task-manager-74ccc9bd9-psqwm   0/1     ErrImageNeverPull   0          67m
2020-06-04T10:33:30.9185226Z 
2020-06-04T10:33:30.9185926Z NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                       AGE
2020-06-04T10:33:30.9186832Z service/flink-job-cluster   NodePort    10.111.92.199   <none>        6123:32501/TCP,6124:31360/TCP,6125:30025/TCP,8081:30081/TCP   67m
2020-06-04T10:33:30.9187545Z service/kubernetes          ClusterIP   10.96.0.1       <none>        443/TCP                                                       68m
2020-06-04T10:33:30.9187976Z 
2020-06-04T10:33:30.9188472Z NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
2020-06-04T10:33:30.9189179Z deployment.apps/flink-task-manager   0/1     1            0           67m
2020-06-04T10:33:30.9189508Z 
2020-06-04T10:33:30.9189815Z NAME                                           DESIRED   CURRENT   READY   AGE
2020-06-04T10:33:30.9190418Z replicaset.apps/flink-task-manager-74ccc9bd9   1         1         0       67m
2020-06-04T10:33:30.9190662Z 
2020-06-04T10:33:30.9190891Z NAME                          COMPLETIONS   DURATION   AGE
2020-06-04T10:33:30.9191423Z job.batch/flink-job-cluster   0/1           67m        67m
2020-06-04T10:33:33.7840921Z I0604 10:33:33.783675   13482 request.go:621] Throttling request took 1.198522435s, request: GET:https://10.1.0.4:8443/apis/batch/v1beta1?timeout=32s
2020-06-04T10:33:43.7842354Z I0604 10:33:43.783896   13482 request.go:621] Throttling request took 4.198512626s, request: GET:https://10.1.0.4:8443/apis/rbac.authorization.k8s.io/v1?timeout=32s
2020-06-04T10:33:54.7840871Z I0604 10:33:54.783716   13482 request.go:621] Throttling request took 1.198557318s, request: GET:https://10.1.0.4:8443/apis/batch/v1?timeout=32s
2020-06-04T10:34:04.7844128Z I0604 10:34:04.783887   13482 request.go:621] Throttling request took 4.199015469s, request: GET:https://10.1.0.4:8443/apis/authorization.k8s.io/v1?timeout=32s
2020-06-04T10:34:15.7841963Z I0604 10:34:15.783767   13482 request.go:621] Throttling request took 1.198438099s, request: GET:https://10.1.0.4:8443/apis/apiregistration.k8s.io/v1?timeout=32s
2020-06-04T10:34:25.9842118Z I0604 10:34:25.983800   13482 request.go:621] Throttling request took 4.39847004s, request: GET:https://10.1.0.4:8443/apis/apiextensions.k8s.io/v1beta1?timeout=32s
2020-06-04T10:34:34.1008762Z Name:         flink-job-cluster-dtb67
2020-06-04T10:34:34.1009742Z Namespace:    default
2020-06-04T10:34:34.1010171Z Priority:     0
2020-06-04T10:34:34.1010837Z Node:         fv-az555/10.1.0.4
2020-06-04T10:34:34.1011354Z Start Time:   Thu, 04 Jun 2020 09:25:40 +0000
2020-06-04T10:34:34.1011820Z Labels:       app=flink
2020-06-04T10:34:34.1012442Z               component=job-cluster
2020-06-04T10:34:34.1013225Z               controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1013955Z               job-name=flink-job-cluster
2020-06-04T10:34:34.1014406Z Annotations:  <none>
2020-06-04T10:34:34.1014802Z Status:       Pending
2020-06-04T10:34:34.1015188Z IP:           172.17.0.4
2020-06-04T10:34:34.1015553Z IPs:
2020-06-04T10:34:34.1015933Z   IP:           172.17.0.4
2020-06-04T10:34:34.1016936Z Controlled By:  Job/flink-job-cluster
2020-06-04T10:34:34.1017389Z Containers:
2020-06-04T10:34:34.1017951Z   flink-job-cluster:
2020-06-04T10:34:34.1018377Z     Container ID:  
2020-06-04T10:34:34.1018814Z     Image:         test_kubernetes_embedded_job
2020-06-04T10:34:34.1019257Z     Image ID:      
2020-06-04T10:34:34.1019726Z     Ports:         6123/TCP, 6124/TCP, 6125/TCP, 8081/TCP
2020-06-04T10:34:34.1020231Z     Host Ports:    0/TCP, 0/TCP, 0/TCP, 0/TCP
2020-06-04T10:34:34.1020652Z     Args:
2020-06-04T10:34:34.1021192Z       standalone-job
2020-06-04T10:34:34.1021782Z       --job-classname
2020-06-04T10:34:34.1022241Z       org.apache.flink.examples.java.wordcount.WordCount
2020-06-04T10:34:34.1022995Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1023666Z       -Dparallelism.default=1
2020-06-04T10:34:34.1024289Z       -Dblob.server.port=6124
2020-06-04T10:34:34.1025391Z       -Dqueryable-state.server.ports=6125
2020-06-04T10:34:34.1027276Z       --output
2020-06-04T10:34:34.1027537Z       /cache/kubernetes_wc_out
2020-06-04T10:34:34.1027841Z     State:          Waiting
2020-06-04T10:34:34.1028137Z       Reason:       ErrImageNeverPull
2020-06-04T10:34:34.1028442Z     Ready:          False
2020-06-04T10:34:34.1028702Z     Restart Count:  0
2020-06-04T10:34:34.1028985Z     Environment:    <none>
2020-06-04T10:34:34.1029230Z     Mounts:
2020-06-04T10:34:34.1029760Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1030426Z       /var/run/secrets/kubernetes.io/serviceaccount from default-token-lgfgh (ro)
2020-06-04T10:34:34.1030791Z Conditions:
2020-06-04T10:34:34.1031038Z   Type              Status
2020-06-04T10:34:34.1031293Z   Initialized       True 
2020-06-04T10:34:34.1031561Z   Ready             False 
2020-06-04T10:34:34.1031814Z   ContainersReady   False 
2020-06-04T10:34:34.1032554Z   PodScheduled      True 
2020-06-04T10:34:34.1032784Z Volumes:
2020-06-04T10:34:34.1033242Z   job-artifacts-volume:
2020-06-04T10:34:34.1033582Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1034672Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1035133Z     HostPathType:  
2020-06-04T10:34:34.1035585Z   default-token-lgfgh:
2020-06-04T10:34:34.1035928Z     Type:        Secret (a volume populated by a Secret)
2020-06-04T10:34:34.1036470Z     SecretName:  default-token-lgfgh
2020-06-04T10:34:34.1036778Z     Optional:    false
2020-06-04T10:34:34.1037042Z QoS Class:       BestEffort
2020-06-04T10:34:34.1037504Z Node-Selectors:  <none>
2020-06-04T10:34:34.1038098Z Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
2020-06-04T10:34:34.1038529Z                  node.kubernetes.io/unreachable:NoExecute for 300s
2020-06-04T10:34:34.1038850Z Events:
2020-06-04T10:34:34.1039173Z   Type     Reason             Age                    From               Message
2020-06-04T10:34:34.1040006Z   ----     ------             ----                   ----               -------
2020-06-04T10:34:34.1041609Z   Warning  ErrImageNeverPull  8m48s (x278 over 68m)  kubelet, fv-az555  Container image ""test_kubernetes_embedded_job"" is not present with pull policy of Never
2020-06-04T10:34:34.1042600Z   Warning  Failed             3m50s (x301 over 68m)  kubelet, fv-az555  Error: ErrImageNeverPull
2020-06-04T10:34:34.1071792Z 
2020-06-04T10:34:34.1072076Z 
2020-06-04T10:34:34.1073266Z Name:         flink-task-manager-74ccc9bd9-psqwm
2020-06-04T10:34:34.1073595Z Namespace:    default
2020-06-04T10:34:34.1073998Z Priority:     0
2020-06-04T10:34:34.1074519Z Node:         fv-az555/10.1.0.4
2020-06-04T10:34:34.1075030Z Start Time:   Thu, 04 Jun 2020 09:25:41 +0000
2020-06-04T10:34:34.1075349Z Labels:       app=flink
2020-06-04T10:34:34.1076024Z               component=task-manager
2020-06-04T10:34:34.1076729Z               pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1077193Z Annotations:  <none>
2020-06-04T10:34:34.1077470Z Status:       Pending
2020-06-04T10:34:34.1077878Z IP:           172.17.0.5
2020-06-04T10:34:34.1078133Z IPs:
2020-06-04T10:34:34.1078354Z   IP:           172.17.0.5
2020-06-04T10:34:34.1079106Z Controlled By:  ReplicaSet/flink-task-manager-74ccc9bd9
2020-06-04T10:34:34.1079575Z Containers:
2020-06-04T10:34:34.1080041Z   flink-task-manager:
2020-06-04T10:34:34.1080461Z     Container ID:  
2020-06-04T10:34:34.1080783Z     Image:         test_kubernetes_embedded_job
2020-06-04T10:34:34.1081216Z     Image ID:      
2020-06-04T10:34:34.1081488Z     Port:          <none>
2020-06-04T10:34:34.1081902Z     Host Port:     <none>
2020-06-04T10:34:34.1082159Z     Args:
2020-06-04T10:34:34.1082382Z       taskmanager
2020-06-04T10:34:34.1083054Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1083554Z     State:          Waiting
2020-06-04T10:34:34.1083851Z       Reason:       ErrImageNeverPull
2020-06-04T10:34:34.1084469Z     Ready:          False
2020-06-04T10:34:34.1084736Z     Restart Count:  0
2020-06-04T10:34:34.1085027Z     Environment:    <none>
2020-06-04T10:34:34.1085431Z     Mounts:
2020-06-04T10:34:34.1086215Z       /cache from cache-volume (rw)
2020-06-04T10:34:34.1086969Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1087858Z       /var/run/secrets/kubernetes.io/serviceaccount from default-token-lgfgh (ro)
2020-06-04T10:34:34.1088236Z Conditions:
2020-06-04T10:34:34.1088620Z   Type              Status
2020-06-04T10:34:34.1088896Z   Initialized       True 
2020-06-04T10:34:34.1089459Z   Ready             False 
2020-06-04T10:34:34.1089732Z   ContainersReady   False 
2020-06-04T10:34:34.1090529Z   PodScheduled      True 
2020-06-04T10:34:34.1090783Z Volumes:
2020-06-04T10:34:34.1091497Z   cache-volume:
2020-06-04T10:34:34.1092464Z     Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
2020-06-04T10:34:34.1092940Z     Medium:     
2020-06-04T10:34:34.1093155Z     SizeLimit:  <unset>
2020-06-04T10:34:34.1097800Z   job-artifacts-volume:
2020-06-04T10:34:34.1098334Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1099487Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1099959Z     HostPathType:  
2020-06-04T10:34:34.1100476Z   default-token-lgfgh:
2020-06-04T10:34:34.1100744Z     Type:        Secret (a volume populated by a Secret)
2020-06-04T10:34:34.1101341Z     SecretName:  default-token-lgfgh
2020-06-04T10:34:34.1101574Z     Optional:    false
2020-06-04T10:34:34.1101995Z QoS Class:       BestEffort
2020-06-04T10:34:34.1102342Z Node-Selectors:  <none>
2020-06-04T10:34:34.1102849Z Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
2020-06-04T10:34:34.1103161Z                  node.kubernetes.io/unreachable:NoExecute for 300s
2020-06-04T10:34:34.1103370Z Events:
2020-06-04T10:34:34.1103707Z   Type     Reason             Age                    From               Message
2020-06-04T10:34:34.1104581Z   ----     ------             ----                   ----               -------
2020-06-04T10:34:34.1106176Z   Warning  ErrImageNeverPull  18m (x233 over 68m)    kubelet, fv-az555  Container image ""test_kubernetes_embedded_job"" is not present with pull policy of Never
2020-06-04T10:34:34.1107412Z   Warning  Failed             3m44s (x301 over 68m)  kubelet, fv-az555  Error: ErrImageNeverPull
2020-06-04T10:34:34.1171266Z 
2020-06-04T10:34:34.1171483Z 
2020-06-04T10:34:34.1172145Z Name:                     flink-job-cluster
2020-06-04T10:34:34.1172399Z Namespace:                default
2020-06-04T10:34:34.1172649Z Labels:                   app=flink
2020-06-04T10:34:34.1173085Z                           component=job-cluster
2020-06-04T10:34:34.1173333Z Annotations:              <none>
2020-06-04T10:34:34.1173795Z Selector:                 app=flink,component=job-cluster
2020-06-04T10:34:34.1174063Z Type:                     NodePort
2020-06-04T10:34:34.1174311Z IP:                       10.111.92.199
2020-06-04T10:34:34.1174567Z Port:                     rpc  6123/TCP
2020-06-04T10:34:34.1174817Z TargetPort:               6123/TCP
2020-06-04T10:34:34.1175062Z NodePort:                 rpc  32501/TCP
2020-06-04T10:34:34.1175304Z Endpoints:                
2020-06-04T10:34:34.1175542Z Port:                     blob  6124/TCP
2020-06-04T10:34:34.1175781Z TargetPort:               6124/TCP
2020-06-04T10:34:34.1176034Z NodePort:                 blob  31360/TCP
2020-06-04T10:34:34.1176460Z Endpoints:                
2020-06-04T10:34:34.1176706Z Port:                     query  6125/TCP
2020-06-04T10:34:34.1176941Z TargetPort:               6125/TCP
2020-06-04T10:34:34.1177196Z NodePort:                 query  30025/TCP
2020-06-04T10:34:34.1177513Z Endpoints:                
2020-06-04T10:34:34.1177722Z Port:                     ui  8081/TCP
2020-06-04T10:34:34.1177939Z TargetPort:               8081/TCP
2020-06-04T10:34:34.1178144Z NodePort:                 ui  30081/TCP
2020-06-04T10:34:34.1178351Z Endpoints:                
2020-06-04T10:34:34.1178544Z Session Affinity:         None
2020-06-04T10:34:34.1178764Z External Traffic Policy:  Cluster
2020-06-04T10:34:34.1178965Z Events:                   <none>
2020-06-04T10:34:34.1226465Z 
2020-06-04T10:34:34.1226805Z 
2020-06-04T10:34:34.1227023Z Name:              kubernetes
2020-06-04T10:34:34.1227223Z Namespace:         default
2020-06-04T10:34:34.1227453Z Labels:            component=apiserver
2020-06-04T10:34:34.1227811Z                    provider=kubernetes
2020-06-04T10:34:34.1228028Z Annotations:       <none>
2020-06-04T10:34:34.1228306Z Selector:          <none>
2020-06-04T10:34:34.1228506Z Type:              ClusterIP
2020-06-04T10:34:34.1228703Z IP:                10.96.0.1
2020-06-04T10:34:34.1228892Z Port:              https  443/TCP
2020-06-04T10:34:34.1229100Z TargetPort:        8443/TCP
2020-06-04T10:34:34.1229301Z Endpoints:         10.1.0.4:8443
2020-06-04T10:34:34.1229513Z Session Affinity:  None
2020-06-04T10:34:34.1229693Z Events:            <none>
2020-06-04T10:34:34.1335732Z 
2020-06-04T10:34:34.1336572Z 
2020-06-04T10:34:34.1337318Z Name:                   flink-task-manager
2020-06-04T10:34:34.1337768Z Namespace:              default
2020-06-04T10:34:34.1338052Z CreationTimestamp:      Thu, 04 Jun 2020 09:25:41 +0000
2020-06-04T10:34:34.1338347Z Labels:                 <none>
2020-06-04T10:34:34.1338634Z Annotations:            deployment.kubernetes.io/revision: 1
2020-06-04T10:34:34.1339150Z Selector:               app=flink,component=task-manager
2020-06-04T10:34:34.1339510Z Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
2020-06-04T10:34:34.1339825Z StrategyType:           RollingUpdate
2020-06-04T10:34:34.1340066Z MinReadySeconds:        0
2020-06-04T10:34:34.1340333Z RollingUpdateStrategy:  25% max unavailable, 25% max surge
2020-06-04T10:34:34.1340588Z Pod Template:
2020-06-04T10:34:34.1340772Z   Labels:  app=flink
2020-06-04T10:34:34.1341151Z            component=task-manager
2020-06-04T10:34:34.1341367Z   Containers:
2020-06-04T10:34:34.1341778Z    flink-task-manager:
2020-06-04T10:34:34.1342030Z     Image:      test_kubernetes_embedded_job
2020-06-04T10:34:34.1342267Z     Port:       <none>
2020-06-04T10:34:34.1342478Z     Host Port:  <none>
2020-06-04T10:34:34.1342657Z     Args:
2020-06-04T10:34:34.1342835Z       taskmanager
2020-06-04T10:34:34.1343217Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1343470Z     Environment:  <none>
2020-06-04T10:34:34.1343659Z     Mounts:
2020-06-04T10:34:34.1344016Z       /cache from cache-volume (rw)
2020-06-04T10:34:34.1344451Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1344679Z   Volumes:
2020-06-04T10:34:34.1344997Z    cache-volume:
2020-06-04T10:34:34.1345459Z     Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
2020-06-04T10:34:34.1345752Z     Medium:     
2020-06-04T10:34:34.1345945Z     SizeLimit:  <unset>
2020-06-04T10:34:34.1346310Z    job-artifacts-volume:
2020-06-04T10:34:34.1346571Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1347294Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1347659Z     HostPathType:  
2020-06-04T10:34:34.1347841Z Conditions:
2020-06-04T10:34:34.1348044Z   Type           Status  Reason
2020-06-04T10:34:34.1348412Z   ----           ------  ------
2020-06-04T10:34:34.1348675Z   Available      False   MinimumReplicasUnavailable
2020-06-04T10:34:34.1348948Z   Progressing    False   ProgressDeadlineExceeded
2020-06-04T10:34:34.1349301Z OldReplicaSets:  <none>
2020-06-04T10:34:34.1349693Z NewReplicaSet:   flink-task-manager-74ccc9bd9 (1/1 replicas created)
2020-06-04T10:34:34.1349944Z Events:          <none>
2020-06-04T10:34:34.1493429Z 
2020-06-04T10:34:34.1493887Z 
2020-06-04T10:34:34.1494891Z Name:           flink-task-manager-74ccc9bd9
2020-06-04T10:34:34.1495330Z Namespace:      default
2020-06-04T10:34:34.1496049Z Selector:       app=flink,component=task-manager,pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1496748Z Labels:         app=flink
2020-06-04T10:34:34.1497158Z                 component=task-manager
2020-06-04T10:34:34.1497717Z                 pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1498300Z Annotations:    deployment.kubernetes.io/desired-replicas: 1
2020-06-04T10:34:34.1498904Z                 deployment.kubernetes.io/max-replicas: 2
2020-06-04T10:34:34.1499173Z                 deployment.kubernetes.io/revision: 1
2020-06-04T10:34:34.1499742Z Controlled By:  Deployment/flink-task-manager
2020-06-04T10:34:34.1499971Z Replicas:       1 current / 1 desired
2020-06-04T10:34:34.1500580Z Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
2020-06-04T10:34:34.1500850Z Pod Template:
2020-06-04T10:34:34.1501033Z   Labels:  app=flink
2020-06-04T10:34:34.1501607Z            component=task-manager
2020-06-04T10:34:34.1502188Z            pod-template-hash=74ccc9bd9
2020-06-04T10:34:34.1502416Z   Containers:
2020-06-04T10:34:34.1502957Z    flink-task-manager:
2020-06-04T10:34:34.1503226Z     Image:      test_kubernetes_embedded_job
2020-06-04T10:34:34.1503792Z     Port:       <none>
2020-06-04T10:34:34.1504011Z     Host Port:  <none>
2020-06-04T10:34:34.1504190Z     Args:
2020-06-04T10:34:34.1504536Z       taskmanager
2020-06-04T10:34:34.1505344Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1505611Z     Environment:  <none>
2020-06-04T10:34:34.1505831Z     Mounts:
2020-06-04T10:34:34.1506423Z       /cache from cache-volume (rw)
2020-06-04T10:34:34.1507112Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1507363Z   Volumes:
2020-06-04T10:34:34.1507951Z    cache-volume:
2020-06-04T10:34:34.1508616Z     Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
2020-06-04T10:34:34.1508939Z     Medium:     
2020-06-04T10:34:34.1509279Z     SizeLimit:  <unset>
2020-06-04T10:34:34.1509710Z    job-artifacts-volume:
2020-06-04T10:34:34.1510145Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1511303Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1511852Z     HostPathType:  
2020-06-04T10:34:34.1512076Z Events:            <none>
2020-06-04T10:34:34.1566048Z 
2020-06-04T10:34:34.1566248Z 
2020-06-04T10:34:34.1566883Z Name:           flink-job-cluster
2020-06-04T10:34:34.1568025Z Namespace:      default
2020-06-04T10:34:34.1568988Z Selector:       controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1569250Z Labels:         app=flink
2020-06-04T10:34:34.1569760Z                 component=job-cluster
2020-06-04T10:34:34.1570182Z                 controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1570589Z                 job-name=flink-job-cluster
2020-06-04T10:34:34.1570788Z Annotations:    <none>
2020-06-04T10:34:34.1570972Z Parallelism:    1
2020-06-04T10:34:34.1571149Z Completions:    1
2020-06-04T10:34:34.1571397Z Start Time:     Thu, 04 Jun 2020 09:25:40 +0000
2020-06-04T10:34:34.1571680Z Pods Statuses:  1 Running / 0 Succeeded / 0 Failed
2020-06-04T10:34:34.1571883Z Pod Template:
2020-06-04T10:34:34.1572056Z   Labels:  app=flink
2020-06-04T10:34:34.1572369Z            component=job-cluster
2020-06-04T10:34:34.1572776Z            controller-uid=376b1bca-7698-4395-b623-962eb4587851
2020-06-04T10:34:34.1573155Z            job-name=flink-job-cluster
2020-06-04T10:34:34.1573337Z   Containers:
2020-06-04T10:34:34.1573634Z    flink-job-cluster:
2020-06-04T10:34:34.1573843Z     Image:       test_kubernetes_embedded_job
2020-06-04T10:34:34.1574101Z     Ports:       6123/TCP, 6124/TCP, 6125/TCP, 8081/TCP
2020-06-04T10:34:34.1574345Z     Host Ports:  0/TCP, 0/TCP, 0/TCP, 0/TCP
2020-06-04T10:34:34.1574542Z     Args:
2020-06-04T10:34:34.1574806Z       standalone-job
2020-06-04T10:34:34.1575098Z       --job-classname
2020-06-04T10:34:34.1575306Z       org.apache.flink.examples.java.wordcount.WordCount
2020-06-04T10:34:34.1575704Z       -Djobmanager.rpc.address=flink-job-cluster
2020-06-04T10:34:34.1576055Z       -Dparallelism.default=1
2020-06-04T10:34:34.1576805Z       -Dblob.server.port=6124
2020-06-04T10:34:34.1577160Z       -Dqueryable-state.server.ports=6125
2020-06-04T10:34:34.1577456Z       --output
2020-06-04T10:34:34.1577637Z       /cache/kubernetes_wc_out
2020-06-04T10:34:34.1577820Z     Environment:  <none>
2020-06-04T10:34:34.1578002Z     Mounts:
2020-06-04T10:34:34.1578343Z       /opt/flink/usrlib from job-artifacts-volume (rw)
2020-06-04T10:34:34.1578555Z   Volumes:
2020-06-04T10:34:34.1578840Z    job-artifacts-volume:
2020-06-04T10:34:34.1579081Z     Type:          HostPath (bare host directory volume)
2020-06-04T10:34:34.1579623Z     Path:          /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/examples/batch
2020-06-04T10:34:34.1579918Z     HostPathType:  
2020-06-04T10:34:34.1580108Z Events:            <none>
2020-06-04T10:34:34.1598723Z Flink logs:
2020-06-04T10:34:34.3990017Z Error from server (BadRequest): container ""flink-job-cluster"" in pod ""flink-job-cluster-dtb67"" is waiting to start: ErrImageNeverPull
2020-06-04T10:34:34.4881295Z Error from server (BadRequest): container ""flink-task-manager"" in pod ""flink-task-manager-74ccc9bd9-psqwm"" is waiting to start: ErrImageNeverPull
2020-06-04T10:34:34.6105005Z job.batch ""flink-job-cluster"" deleted
2020-06-04T10:34:34.7547554Z service ""flink-job-cluster"" deleted
2020-06-04T10:34:34.9134745Z deployment.apps ""flink-task-manager"" deleted
2020-06-04T10:34:37.7996959Z pod/flink-job-cluster-dtb67 condition met
2020-06-04T10:34:37.9451396Z pod/flink-task-manager-74ccc9bd9-psqwm condition met
2020-06-04T10:34:37.9468470Z Stopping minikube ...
2020-06-04T10:34:38.0178327Z * Stopping ""minikube"" in none ...
2020-06-04T10:34:48.4722227Z * Node """" stopped.
2020-06-04T10:34:48.4767883Z [FAIL] Test script contains errors.
2020-06-04T10:34:48.4775183Z Checking for errors...
2020-06-04T10:34:48.4963953Z No errors in log files.
2020-06-04T10:34:48.4964817Z Checking for exceptions...
2020-06-04T10:34:48.5139288Z No exceptions in log files.
2020-06-04T10:34:48.5150249Z Checking for non-empty .out files...
2020-06-04T10:34:48.5157683Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
2020-06-04T10:34:48.5158245Z No non-empty .out files.
2020-06-04T10:34:48.5158815Z 
2020-06-04T10:34:48.5159286Z [FAIL] 'Run Kubernetes test' failed after 70 minutes and 52 seconds! Test exited with exit code 1
{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13311390	Streaming File Sink s3 end-to-end test stalls	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3444&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2020-06-12T21:55:57.6277963Z Number of produced values 10870/60000
2020-06-12T21:57:10.5467073Z Number of produced values 22960/60000
2020-06-12T21:58:01.0025226Z Number of produced values 59650/60000
2020-06-12T21:58:52.5624619Z Number of produced values 60000/60000
2020-06-12T21:58:53.2407133Z Cancelling job 9412dcb358631ab461a3a1e851417b9e.
2020-06-12T21:58:54.0819168Z Cancelled job 9412dcb358631ab461a3a1e851417b9e.
2020-06-12T21:58:54.1097745Z Waiting for job (9412dcb358631ab461a3a1e851417b9e) to reach terminal state CANCELED ...
2020-06-13T00:00:35.0502923Z ##[error]The operation was canceled.
2020-06-13T00:00:35.0522780Z ##[section]Finishing: Run e2e tests
{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
12731999	Materials section Project Website	"I suggest to add a materials section to the new project website (http://flink.incubator.apache.org/index.html) with an updated logo etc. to allow people to get materials for their projects around flink.

In addition to that I think that the menu entry project -> source code is unnecessary because there is a link to github on the main page... . If you want to keep it it should be renamed to github as well."	FLINK	Resolved	4	4	8669	documentation, easyfix, starter
13304801	Resolve CVE-2019-11358 from jquery	https://nvd.nist.gov/vuln/detail/CVE-2019-11358	FLINK	Closed	3	4	8669	pull-request-available
13381697	Move our Azure pipelines away from Ubuntu 16.04 by September	"Azure won't support Ubuntu 16.04 starting from October, hence we need to migrate to a newer ubuntu version.

We should do this at a time when the builds are relatively stable to be able to clearly identify issues relating to the version upgrade. Also, we shouldn't do this before a feature freeze ;) "	FLINK	Closed	2	1	8669	pull-request-available
13353801	Document how to use the reactive mode	"We need to document how our users can use the reactive mode. I propose to create a dedicated ""Execution mode"" page under ""Deployment"" describing the active and reactive mode and how to activate the reactive mode."	FLINK	Closed	3	7	8669	pull-request-available
13384766	cron_snapshot_deployment_maven unstable on maven	"{{cron_snapshot_deployment_maven}}, the cron build on azure that deploys snapshot artifacts to maven central repository, has become unstable recently.

The failures fall into two categories.
- Maven failed to upload/download an artifact
- The stage overall takes too long time.

As far as I can see, the instability starts being observed since June 18th.

Observed instances:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19152&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19154&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19167&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19169&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19176&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19175&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19177&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7"	FLINK	Closed	2	4	8669	pull-request-available, test-stability
13037641	Default Flink configuration contains whitespace characters, causing parser WARNings	"{code}
2017-01-25 09:45:30,670 WARN  org.apache.flink.configuration.GlobalConfiguration            - Error while trying to split key and value in configuration file /yarn/nm/usercache/robert/appcache/application_1485249546281_0018/container_1485249546281_0018_01_000001/flink-conf.yaml:  
{code}

The whitespace is currently in line 67:
{code}
#==============================================================================
 
# The address under which the web-based runtime monitor listens.
{code}

I think we should add a test to the {{GlobalConfigurationTest}} that ensures the configuration file we are shipping doesn't produce any WARNings by default."	FLINK	Resolved	3	1	8669	starter
13327801	Use classloader release hooks with Kinesis producer to avoid metaspace leak	"FLINK-17554 introduced hooks for clearing references before unloading a classloader.

The Kinesis Producer library is currently preventing the usercode classloader from being unloaded because it keeps references around.

This ticket is to use the hooks with the Kinesis producer."	FLINK	Closed	3	4	8669	pull-request-available
13294106	End to end tests timeout on Azure	"Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6650&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179 or https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6637&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}##[error]The job running on agent Azure Pipelines 6 ran longer than the maximum time of 200 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134
{code}
and {code}##[error]The operation was canceled.{code}
"	FLINK	Closed	1	1	8669	pull-request-available
13354473	Reactive Mode: Change Adaptive Scheduler to set infinite parallelism in JobGraph	"For Reactive Mode, the scheduler needs to change the parallelism and maxParalllelism of the submitted job graph to it's max value (2^15).

+ check if an unsupported flag is enabled in the submitted jobgraph or configuration (unaligned checkpoints)"	FLINK	Closed	3	7	8669	pull-request-available
13301908	Flink task executor process permanently hangs on `flink-daemon.sh stop`, deletes PID file	"Hi Flink team!

We've attempted to upgrade our flink 1.9 cluster to 1.10, but are experiencing reproducible instability on shutdown. Speciically, it appears that the `kill` issued in the `stop` case of flink-daemon.sh is causing the task executor process to hang permanently. Specifically, the process seems to be hanging in the `org.apache.flink.runtime.util.JvmShutdownSafeguard$DelayedTerminator.run` in a `Thread.sleep()` call. I think this is a bizarre behavior. Also note that every thread in the process is BLOCKED. on a `pthread_cond_wait` call. Is this an OS level issue? Banging my head on a wall here. See attached stack traces for details."	FLINK	Closed	1	1	8669	pull-request-available
13312283	Test Flink on Azure-hosted VMs nightly	"There are some tests which happen a lot more frequently on the VMs provided by Azure (instead of the CI infrastructure hosted in Alibaba Cloud).

Since we have enough CI resources available (at night), we can add a run on the Azure machines to get more visibility into those failures, and to increase the stability of personal account CI runs."	FLINK	Closed	3	4	8669	pull-request-available
13294984	Make job submission non-blocking	"Currently, Flink waits to acknowledge a job submission until the corresponding {{JobManager}} has been created. Since its creation also involves the creation of the {{ExecutionGraph}} and potential FS operations, it can take a bit of time. If the user has configured a too low {{web.timeout}}, the submission can time out only reporting a {{TimeoutException}} to the user.

I propose to change the notion of job submission slightly. Instead of waiting until the {{JobManager}} has been created, a job submission is complete once all job relevant files have been uploaded to the {{Dispatcher}} and the {{Dispatcher}} has been told about it. Creating the {{JobManager}} will then belong to the actual job execution. Consequently, if problems occur while creating the {{JobManager}} it will result into a job failure."	FLINK	Closed	2	4	8669	pull-request-available
13326005	"MetricsAvailabilityITCase.testReporter failed with ""Could not satisfy the predicate within the allowed time"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6179&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a]

{code}
2020-09-03T23:33:18.3687261Z [ERROR] testReporter(org.apache.flink.metrics.tests.MetricsAvailabilityITCase)  Time elapsed: 15.217 s  <<< ERROR!
2020-09-03T23:33:18.3698260Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not satisfy the predicate within the allowed time.
2020-09-03T23:33:18.3698749Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-09-03T23:33:18.3699163Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-09-03T23:33:18.3699754Z 	at org.apache.flink.metrics.tests.MetricsAvailabilityITCase.fetchMetric(MetricsAvailabilityITCase.java:162)
2020-09-03T23:33:18.3700234Z 	at org.apache.flink.metrics.tests.MetricsAvailabilityITCase.checkJobManagerMetricAvailability(MetricsAvailabilityITCase.java:116)
2020-09-03T23:33:18.3700726Z 	at org.apache.flink.metrics.tests.MetricsAvailabilityITCase.testReporter(MetricsAvailabilityITCase.java:101)
2020-09-03T23:33:18.3701097Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-03T23:33:18.3701425Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-03T23:33:18.3701798Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-03T23:33:18.3702146Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-03T23:33:18.3702471Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-03T23:33:18.3702866Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-03T23:33:18.3703253Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-03T23:33:18.3703621Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-03T23:33:18.3703997Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-03T23:33:18.3704339Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-03T23:33:18.3704629Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-03T23:33:18.3704940Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-03T23:33:18.3705354Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-03T23:33:18.3705725Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-03T23:33:18.3706072Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-03T23:33:18.3706397Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-03T23:33:18.3706714Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-03T23:33:18.3707044Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-03T23:33:18.3707373Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-03T23:33:18.3707708Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-03T23:33:18.3708073Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-03T23:33:18.3708410Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-03T23:33:18.3708691Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-09-03T23:33:18.3708976Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-09-03T23:33:18.3709273Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-03T23:33:18.3709579Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-03T23:33:18.3709910Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-03T23:33:18.3710242Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-03T23:33:18.3710554Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-03T23:33:18.3710875Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-03T23:33:18.3711203Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-09-03T23:33:18.3711585Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-09-03T23:33:18.3712015Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-09-03T23:33:18.3712428Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-09-03T23:33:18.3712814Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-09-03T23:33:18.3713251Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-09-03T23:33:18.3713675Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-03T23:33:18.3714085Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-03T23:33:18.3714469Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-03T23:33:18.3714833Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-03T23:33:18.3715243Z Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not satisfy the predicate within the allowed time.
2020-09-03T23:33:18.3715729Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retrySuccessfulOperationWithDelay$12(FutureUtils.java:382)
2020-09-03T23:33:18.3716161Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-09-03T23:33:18.3716546Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-09-03T23:33:18.3716940Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-03T23:33:18.3717311Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2020-09-03T23:33:18.3717674Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2020-09-03T23:33:18.3718062Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-09-03T23:33:18.3718431Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-09-03T23:33:18.3718749Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-09-03T23:33:18.3719191Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2020-09-03T23:33:18.3719679Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2020-09-03T23:33:18.3720093Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-03T23:33:18.3720472Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-03T23:33:18.3720782Z 	at java.lang.Thread.run(Thread.java:748)
{code}"	FLINK	Closed	3	1	8669	test-stability
13304855	Remove flink-shaded-hadoop-2-parent and submodules	Since Flink does not use flink-shaded-hadoop-2 anymore, we want to remove it from flink-shaded.	FLINK	Closed	3	7	8669	pull-request-available
13300810	Clean up CI system related scripts	"Once we have only one CI system in place for Flink (again), it makes sense to clean up the available scripts:
- Separate ""Azure-specific"" from ""CI-generic"" files (names of files, methods, build profiles)
- separate ""log handling"" from ""build timeout"" in ""travis_watchdog""
- remove workarounds needed because of Travis limitations"	FLINK	Closed	3	7	8669	pull-request-available
13392555	stop-cluster.sh produces warning on macOS 11.4	"Since FLINK-17470, we are stopping daemons with a timeout, to SIGKILL them if they are not gracefully stopping.

I noticed that this mechanism causes warnings on macOS:

{code}
❰robert❙/tmp/flink-1.14-SNAPSHOT❱✔≻ ./bin/start-cluster.sh
Starting cluster.
Starting standalonesession daemon on host MacBook-Pro-2.localdomain.
Starting taskexecutor daemon on host MacBook-Pro-2.localdomain.
❰robert❙/tmp/flink-1.14-SNAPSHOT❱✔≻ ./bin/stop-cluster.sh
Stopping taskexecutor daemon (pid: 50044) on host MacBook-Pro-2.localdomain.
tail: illegal option -- -
usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]
Stopping standalonesession daemon (pid: 49812) on host MacBook-Pro-2.localdomain.
tail: illegal option -- -
usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]
{code}
"	FLINK	Closed	4	1	8669	pull-request-available
12981817	CliFrontendYarnAddressConfigurationTest fails	"The {{CliFrontendYarnAddressConfigurationTest}} failed here:

https://s3.amazonaws.com/archive.travis-ci.org/jobs/139195089/log.txt

https://travis-ci.org/apache/flink/builds/139195083

Another failed run with logs is here: https://travis-ci.org/uce/flink/builds/139262754

"	FLINK	Resolved	3	1	8669	test-stability
13392731	Update CI docker image to latest java version (1.8.0_292)	"The java version we are using on our CI is outdated (1.8.0_282 vs 1.8.0_292). 
The latest java version has TLSv1 disabled, which makes the KubernetesClusterDescriptorTest fail.

This will be fixed by FLINK-22802."	FLINK	Closed	2	11500	8669	pull-request-available
12719683	Add support for Tachyon File System to Flink	"Implementation of the Tachyon file system.

The code was tested using junit and the wordcount example on a tachyon file system running in local mode.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/512
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Wed Feb 26 13:58:24 CET 2014
State: closed
"	FLINK	Resolved	4	2	8669	github-import
13303342	Add release hooks for user code class loader	"Release hooks for the user code class loader which are run just before the user code class loader is released would allow clean up static references to classes of the user code class loader. This is important because these static references could prevent the user code classes from being garbage collected and eventually causing metaspace OOMs.

Hence I suggest to extend the {{RuntimeContext}} with an additional method {{registerUserCodeClassLoaderReleaseHook(Runnable releaseHook)}} which allows the user code to register a release hook for the user code class loader."	FLINK	Closed	3	2	8669	pull-request-available
13336881	Upgrade mesos to 1.7 or newer	"A user reported a dependency vulnerability which affects {{mesos}} [1]. We should upgrade {{mesos}} to {{1.7.0}} or newer.

[1] https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E"	FLINK	Closed	2	4	8669	pull-request-available
13353987	Set up cron job to run CI with declarative scheduler	Once the declarative scheduler has been merged, we should create a Cron job to run all CI profiles with this scheduler in order to find all remaining test failures.	FLINK	Closed	3	7	8669	pull-request-available
13357921	Extract interface out of ExecutionGraph for better testability	"This is a follow up to this comment: https://github.com/apache/flink/pull/14879#discussion_r573613450

The ExecutionGraph class is currently not very handy for tests, as it has a lot of dependencies. Extracting an interface for the ExecutionGraph will make testing and future changes easier."	FLINK	Closed	3	4	8669	pull-request-available
13371859	Clarify Reactive Mode documentation wrt to timeouts	In the release testing of Reactive Mode (FLINK-22134) we found that the documentation of the timeouts needs some clarification.	FLINK	Closed	3	4	8669	pull-request-available
13375399	Document how to use the reactive mode on K8s	We should extend the existing standalone K8s documentation https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes to contain an example for the reactive mode (including the resource definitions).	FLINK	Closed	2	4	8669	pull-request-available
13335977	PrometheusReporterEndToEndITCase crashes with exit code 143	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7814&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729]

{code}
2020-10-18T23:46:04.9667443Z [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-18T23:46:04.9669237Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp
2020-10-18T23:46:04.9670440Z [ERROR] Error occurred in starting fork, check output in log
2020-10-18T23:46:04.9671283Z [ERROR] Process Exit Code: 143
2020-10-18T23:46:04.9671614Z [ERROR] Crashed tests:
2020-10-18T23:46:04.9672025Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
2020-10-18T23:46:04.9672649Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-18T23:46:04.9674834Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp
2020-10-18T23:46:04.9676153Z [ERROR] Error occurred in starting fork, check output in log
2020-10-18T23:46:04.9676556Z [ERROR] Process Exit Code: 143
2020-10-18T23:46:04.9676882Z [ERROR] Crashed tests:
2020-10-18T23:46:04.9677288Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
2020-10-18T23:46:04.9677827Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-10-18T23:46:04.9678408Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:282)
2020-10-18T23:46:04.9678965Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)
2020-10-18T23:46:04.9679575Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-10-18T23:46:04.9680983Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-10-18T23:46:04.9681749Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-10-18T23:46:04.9682246Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-10-18T23:46:04.9682728Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-10-18T23:46:04.9683179Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-10-18T23:46:04.9683609Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-10-18T23:46:04.9684102Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-10-18T23:46:04.9684639Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-10-18T23:46:04.9685180Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-10-18T23:46:04.9685711Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-10-18T23:46:04.9686145Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-10-18T23:46:04.9686516Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-10-18T23:46:04.9689517Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-10-18T23:46:04.9689917Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-10-18T23:46:04.9690262Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-10-18T23:46:04.9690606Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-18T23:46:04.9690994Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-18T23:46:04.9691435Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-18T23:46:04.9691856Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-18T23:46:04.9692450Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-10-18T23:46:04.9693419Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-10-18T23:46:04.9693885Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-10-18T23:46:04.9694334Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-10-18T23:46:04.9694939Z [ERROR] -> [Help 1]
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13581648	Remove deprecated stedolan/jq Docker image from Flink e2e tests	"Our CI logs contain this warning: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60060&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3828

{code}
latest: Pulling from stedolan/jq
[DEPRECATION NOTICE] Docker Image Format v1, and Docker Image manifest version 2, schema 1 support will be removed in an upcoming release. Suggest the author of docker.io/stedolan/jq:latest to upgrade the image to the OCI Format, or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/
{code}"	FLINK	Resolved	4	1	8669	pull-request-available
13285802	build system: transfer.sh uploads are unstable (February 2020)	"This issue has been brought up on the dev@ list: https://lists.apache.org/thread.html/rb6661e419b869f040e66a4dd46022fd11961e8e5aebe646b2260f6f8%40%3Cdev.flink.apache.org%3E

Issues:
- timeouts
- logs not available

"	FLINK	Resolved	3	4	8669	pull-request-available
13358139	Remove State#onEnter	Since we no longer need to construct the new state before calling {{State#onLeave}} we could remove {{State#onEnter}} and move all contained logic into the constructor.	FLINK	Closed	3	7	8669	pull-request-available
12719899	"Forbid catching exceptions only with ""e.printStackTrace()"" using Checkstyle"	"Once https://github.com/stratosphere/stratosphere/issues/596 is merged, I would like to forbid statements like:
```java
} catch (InterruptedException e) {
	e.printStackTrace();
}
```

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/720
Created by: [rmetzger|https://github.com/rmetzger]
Labels: enhancement, 
Created at: Fri Apr 25 11:28:22 CEST 2014
State: open
"	FLINK	Closed	3	4	8669	github-import
13295827	ArtifactResolutionException: Could not transfer artifact.  Entry [...] has not been leased from this pool	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6982&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5
Build of flink-metrics-availability-test failed with:
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-metrics-availability-test: Unable to generate classpath: org.apache.maven.artifact.resolver.ArtifactResolutionException: Could not transfer artifact org.apache.maven.surefire:surefire-grouper:jar:2.22.1 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): Entry [id:13][route:{s}->https://maven-central-eu.storage-download.googleapis.com:443][state:null] has not been leased from this pool
[ERROR] org.apache.maven.surefire:surefire-grouper:jar:2.22.1
[ERROR] 
[ERROR] from the specified remote repositories:
[ERROR] google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/, releases=true, snapshots=false),
[ERROR] apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true)
[ERROR] Path to dependency:
[ERROR] 1) dummy:dummy:jar:1.0
[ERROR] 2) org.apache.maven.surefire:surefire-junit47:jar:2.22.1
[ERROR] 3) org.apache.maven.surefire:common-junit48:jar:2.22.1
[ERROR] 4) org.apache.maven.surefire:surefire-grouper:jar:2.22.1
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-metrics-availability-test

{noformat}
"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13375357	AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond found unexpected files	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17285&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=9340

{code}
Apr 27 11:10:07 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Apr 27 11:10:24 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 17.177 s <<< FAILURE! - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Apr 27 11:10:24 [ERROR] testStopWithSavepointFailOnFirstSavepointSucceedOnSecond(org.apache.flink.test.scheduling.AdaptiveSchedulerITCase)  Time elapsed: 0.305 s  <<< FAILURE!
Apr 27 11:10:24 java.lang.AssertionError: Found unexpected files: /tmp/junit3745203124457058148/savepoint/savepoint-8596b1-b3046c9bcf40
Apr 27 11:10:24 	at org.junit.Assert.fail(Assert.java:88)
Apr 27 11:10:24 	at org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond(AdaptiveSchedulerITCase.java:226)
Apr 27 11:10:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 27 11:10:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 27 11:10:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 27 11:10:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 27 11:10:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 27 11:10:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 27 11:10:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 27 11:10:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 11:10:24 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 27 11:10:24 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 27 11:10:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 27 11:10:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 27 11:10:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 27 11:10:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 11:10:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Apr 27 11:10:24 
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13294350	"Run Kubernetes test failed with invalid named ""minikube"""	"This is the test run [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6702&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

Log output
{code:java}
2020-03-27T00:07:38.9666021Z Running 'Run Kubernetes test'
2020-03-27T00:07:38.9666656Z ==============================================================================
2020-03-27T00:07:38.9677101Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-38967103614
2020-03-27T00:07:41.7529865Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-27T00:07:41.7721475Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-27T00:07:41.8208394Z Docker version 19.03.8, build afacb8b7f0
2020-03-27T00:07:42.4793914Z docker-compose version 1.25.4, build 8d51620a
2020-03-27T00:07:42.5359301Z Installing minikube ...
2020-03-27T00:07:42.5494076Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-03-27T00:07:42.5494729Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-03-27T00:07:42.5498136Z 
2020-03-27T00:07:42.6214887Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-03-27T00:07:43.3467750Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-03-27T00:07:43.3469636Z 100 52.0M  100 52.0M    0     0  65.2M      0 --:--:-- --:--:-- --:--:-- 65.2M
2020-03-27T00:07:43.4262625Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.4264438Z   - To fix this, run: minikube start
2020-03-27T00:07:43.4282404Z Starting minikube ...
2020-03-27T00:07:43.7749694Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:43.7761742Z * Using the none driver based on user configuration
2020-03-27T00:07:43.7762229Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:43.8202161Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.8203353Z   - To fix this, run: minikube start
2020-03-27T00:07:43.8568899Z * There is no local cluster named ""minikube""
2020-03-27T00:07:43.8570685Z   - To fix this, run: minikube start
2020-03-27T00:07:43.8583793Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:48.9017252Z * There is no local cluster named ""minikube""
2020-03-27T00:07:48.9019347Z   - To fix this, run: minikube start
2020-03-27T00:07:48.9031515Z Starting minikube ...
2020-03-27T00:07:49.0612601Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:49.0616688Z * Using the none driver based on user configuration
2020-03-27T00:07:49.0620173Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:49.1040676Z * There is no local cluster named ""minikube""
2020-03-27T00:07:49.1042353Z   - To fix this, run: minikube start
2020-03-27T00:07:49.1453522Z * There is no local cluster named ""minikube""
2020-03-27T00:07:49.1454594Z   - To fix this, run: minikube start
2020-03-27T00:07:49.1468436Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:54.1907713Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.1909876Z   - To fix this, run: minikube start
2020-03-27T00:07:54.1921479Z Starting minikube ...
2020-03-27T00:07:54.3388738Z * minikube v1.9.0 on Ubuntu 16.04
2020-03-27T00:07:54.3395499Z * Using the none driver based on user configuration
2020-03-27T00:07:54.3396443Z X The none driver requires conntrack to be installed for kubernetes version 1.18.0
2020-03-27T00:07:54.3824399Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.3837652Z   - To fix this, run: minikube start
2020-03-27T00:07:54.4203902Z * There is no local cluster named ""minikube""
2020-03-27T00:07:54.4204895Z   - To fix this, run: minikube start
2020-03-27T00:07:54.4217866Z Command: start_kubernetes_if_not_running failed. Retrying...
2020-03-27T00:07:59.4235917Z Command: start_kubernetes_if_not_running failed 3 times.
2020-03-27T00:07:59.4236459Z Could not start minikube. Aborting...
2020-03-27T00:07:59.8439850Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.8939088Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.9515679Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2020-03-27T00:07:59.9528463Z Stopping minikube ...
2020-03-27T00:07:59.9921558Z * There is no local cluster named ""minikube""
2020-03-27T00:07:59.9922957Z   - To fix this, run: minikube start
2020-03-27T00:07:59.9943342Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:05.0475257Z * There is no local cluster named ""minikube""
2020-03-27T00:08:05.0476544Z   - To fix this, run: minikube start
2020-03-27T00:08:05.0498749Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:10.1843339Z * There is no local cluster named ""minikube""
2020-03-27T00:08:10.1846448Z   - To fix this, run: minikube start
2020-03-27T00:08:10.1890972Z Command: sudo minikube stop failed. Retrying...
2020-03-27T00:08:15.1900926Z Command: sudo minikube stop failed 3 times.
2020-03-27T00:08:15.1906577Z Could not stop minikube. Aborting...
2020-03-27T00:08:15.1907434Z [FAIL] Test script contains errors.
2020-03-27T00:08:15.1915373Z Checking for errors...
2020-03-27T00:08:15.2133082Z No errors in log files.
2020-03-27T00:08:15.2133514Z Checking for exceptions...
2020-03-27T00:08:15.2390795Z No exceptions in log files.
2020-03-27T00:08:15.2392029Z Checking for non-empty .out files...
2020-03-27T00:08:15.2412061Z grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*.out: No such file or directory
2020-03-27T00:08:15.2415311Z No non-empty .out files.
2020-03-27T00:08:15.2415821Z 
2020-03-27T00:08:15.2416806Z [FAIL] 'Run Kubernetes test' failed after 0 minutes and 34 seconds! Test exited with exit code 1
2020-03-27T00:08:15.2417355Z 
2020-03-27T00:08:15.2454057Z cp: cannot stat '/home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*': No such file or directory
2020-03-27T00:08:15.2459692Z Published e2e logs into debug logs artifact:
2020-03-27T00:08:15.2489410Z COMPRESSING build artifacts.
2020-03-27T00:08:15.2519389Z tar: Removing leading `/' from member names
2020-03-27T00:08:15.2528098Z /home/vsts/work/1/s/flink-end-to-end-tests/artifacts/
2020-03-27T00:08:15.2529353Z /home/vsts/work/1/s/flink-end-to-end-tests/artifacts/e2e-flink-logs/
2020-03-27T00:08:15.5819526Z No taskexecutor daemon to stop on host fv-az678.
2020-03-27T00:08:15.8011570Z No standalonesession daemon to stop on host fv-az678.
2020-03-27T00:08:16.2280113Z 
2020-03-27T00:08:16.2409524Z ##[error]Bash exited with code '1'.
2020-03-27T00:08:16.2456126Z ##[section]Finishing: Run e2e tests{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13423033	Build arm64 Linux images for Apache Flink	"Building Flink images for arm64 Linux should be trivial to support, since upstream docker images support arm64, as well as frocksdb.

Building the images locally is also easily possible using Docker's buildx features, and the build system of the official docker images most likely supports ARM arch.

This improvement would allow us supporting development / testing on Apple M1-based systems, as well as ARM architecture at various cloud providers (AWS Graviton)"	FLINK	Closed	3	4	8669	pull-request-available
13356470	Add DeclarativeScheduler / WaitingForResources state	"This subtask of adding the declarative scheduler is about adding the WaitingForResources state to Flink, including tests.

Waiting for resources: The required resources are declared. The scheduler waits until either the requirements are fulfilled or the set of resources has stabilised.
"	FLINK	Closed	3	7	8669	pull-request-available
13503396	Bump maven-shade-plugin to 3.4.1	"FLINK-24273 proposes to relocate the io.fabric8 dependencies of flink-kubernetes.
This is not possible because of a problem with the maven shade plugin (""mvn install"" doesn't work, it needs to be ""mvn clean install"").
MSHADE-425 solves this issue, and has been released with maven-shade-plugin 3.4.0.

Upgrading the shade plugin will solve the problem, unblocking the K8s relocation."	FLINK	Closed	3	11500	8669	pull-request-available
13243397	Add note to PubSub connector documentation about beta status	As part of the review of FLINK-9311, we decided to add a note to the documentation page that the connector is considered beta by the community.	FLINK	Resolved	3	4	8669	pull-request-available
13337848	Local recovery and sticky scheduling end-to-end test fails to report error properly	"INSTANCE: [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/8563/logs/141]
{code:java}
2020-10-29T09:43:24.0088180Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (end-to-end-tests) on project flink-end-to-end-tests-hbase: There are test failures.
2020-10-29T09:43:24.0088792Z [ERROR] 
2020-10-29T09:43:24.0089518Z [ERROR] Please refer to /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire-reports for the individual test results.
2020-10-29T09:43:24.0090427Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-10-29T09:43:24.0090914Z [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-29T09:43:24.0093105Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire/surefirebooter6795869883612750001.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire 2020-10-29T09-34-47_778-jvmRun2 surefire2269050977160717631tmp surefire_67897497331523564186tmp
2020-10-29T09:43:24.0094488Z [ERROR] Error occurred in starting fork, check output in log
2020-10-29T09:43:24.0094797Z [ERROR] Process Exit Code: 143
2020-10-29T09:43:24.0095033Z [ERROR] Crashed tests:
2020-10-29T09:43:24.0095321Z [ERROR] org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-10-29T09:43:24.0095828Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-29T09:43:24.0097838Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire/surefirebooter6795869883612750001.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-end-to-end-tests-hbase/target/surefire 2020-10-29T09-34-47_778-jvmRun2 surefire2269050977160717631tmp surefire_67897497331523564186tmp
2020-10-29T09:43:24.0098966Z [ERROR] Error occurred in starting fork, check output in log
2020-10-29T09:43:24.0099266Z [ERROR] Process Exit Code: 143
2020-10-29T09:43:24.0099502Z [ERROR] Crashed tests:
2020-10-29T09:43:24.0099789Z [ERROR] org.apache.flink.tests.util.hbase.SQLClientHBaseITCase
2020-10-29T09:43:24.0100331Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-10-29T09:43:24.0100883Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:282)
2020-10-29T09:43:24.0101774Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)
2020-10-29T09:43:24.0102360Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-10-29T09:43:24.0103004Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-10-29T09:43:24.0103737Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-10-29T09:43:24.0104301Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-10-29T09:43:24.0104828Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-10-29T09:43:24.0105334Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-10-29T09:43:24.0105826Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-10-29T09:43:24.0106384Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-10-29T09:43:24.0106969Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-10-29T09:43:24.0107603Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-10-29T09:43:24.0108201Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-10-29T09:43:24.0108673Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-10-29T09:43:24.0109110Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-10-29T09:43:24.0109517Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-10-29T09:43:24.0110063Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-10-29T09:43:24.0110601Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-10-29T09:43:24.0110998Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-29T09:43:24.0111426Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-29T09:43:24.0112032Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-29T09:43:24.0112487Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-29T09:43:24.0112955Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-10-29T09:43:24.0113563Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-10-29T09:43:24.0114072Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-10-29T09:43:24.0114578Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-10-29T09:43:24.0115188Z [ERROR] -> [Help 1]
{code}"	FLINK	Closed	1	4	8669	pull-request-available, test-stability
12719785	Replace Avro serialization by Kryo 	"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/610
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Milestone: Release 0.6 (unplanned)
Created at: Tue Mar 18 17:29:28 CET 2014
State: open
"	FLINK	Resolved	2	1	8669	github-import
13290270	Improve e2e test failure error reporting	"The purpose of this change is to improve the error reporting for e2e tests:
- The log upload for e2e tests fails if the bash e2e tests fail
- coredumps, dumpstreams etc. are not included into the log upload
- Logs are not scanned for exceptions when exception checking is turned off"	FLINK	Resolved	3	4	8669	pull-request-available
13333949	ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange times out	"Full logs:
https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/148/logs/115

Exception:
{code}
[ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase)  Time elapsed: 301.093 s  <<< ERROR!
java.util.concurrent.TimeoutException: Condition was not met in given timeout.
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:132)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.getNextLeadingDispatcherGateway(ZooKeeperLeaderElectionITCase.java:140)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:122)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)

{code}"	FLINK	Closed	2	1	8669	test-stability
13296031	"Various builds failing with ""Corrupted STDOUT by directly writing"""	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7028&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=f66652e3-384e-5b25-be29-abfea69ea8da (kafka/gelly)
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7021&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=40015e30-d9f1-555e-929f-497bfa903ca8 (libraries)

{noformat}
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /__w/3/s/flink-connectors/flink-connector-kafka/target/surefire-reports/2020-04-03T11-40-23_195-jvmRun1.dumpstream
{noformat}

followed by:
{noformat}

[ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)
[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
[ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
[ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
[ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
[ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
[ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
[ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
[ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
[ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{noformat}
"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13328411	YARNSessionFIFOITCase.checkForProhibitedLogContents found a log with prohibited string	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6661&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354]

{code}
2020-09-19T22:08:13.5364974Z [ERROR]   YARNSessionFIFOITCase.checkForProhibitedLogContents:83->YarnTestBase.ensureNoProhibitedStringInLogFiles:476 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-1_0/application_1600553154281_0001/container_1600553154281_0001_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13357936	Add tests for StateWithExecutionGraph	"This ticket is about adding dedicated tests for the StateWithExecutionGraph class.

This is a follow up from https://github.com/apache/flink/pull/14879#discussion_r573707768

"	FLINK	Closed	3	7	8669	pull-request-available
13289338	Maven central connection timeouts on Azure Pipelines	"Some test stages invoke maven again, where additional dependencies are downloaded, sometimes failing the build.

This ticket is about using the Google mirror wherever possible.

Examples of failing tests:
- https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5882&view=logs&j=636f54dd-dda5-5b4b-f495-2d92ec493b6c&t=6c30efdf-a92a-5da3-9a6a-004c8552b2df

A failure looks like this:
{code}
[ERROR] Failed to execute goal on project flink-hadoop-fs: Could not resolve dependencies for project org.apache.flink:flink-hadoop-fs:jar:1.11-SNAPSHOT: Could not transfer artifact org.apache.flink:flink-shaded-hadoop-2:jar:2.8.3-10.0 from/to central (https://repo.maven.apache.org/maven2): GET request of: org/apache/flink/flink-shaded-hadoop-2/2.8.3-10.0/flink-shaded-hadoop-2-2.8.3-10.0.jar from central failed: Connection reset -> [Help 1]
{code}"	FLINK	Closed	2	1	8669	pull-request-available
12720081	Webclient does not show System.out messages	"When submitting the KMeans Job using the web client, I'm getting the following exception:
```
An error occurred while invoking the program:

The program plan could not be fetched. The program silently swallowed the control flow exceptions.


eu.stratosphere.client.program.ProgramInvocationException: The program plan could not be fetched. The program silently swallowed the control flow exceptions.
	at eu.stratosphere.client.program.Client.getOptimizedPlan(Client.java:154)
	at eu.stratosphere.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:161)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)
	at org.eclipse.jetty.server.Server.handle(Server.java:352)
	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)
	at org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211)
	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:436)
	at java.lang.Thread.run(Thread.java:744)
```

The problem is that the main() method just returns (printing a message to `System.out`).
This is not very intuitive, since you have to look into the webclient.out log

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/900
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Milestone: Release 0.5.1
Created at: Tue Jun 03 12:25:04 CEST 2014
State: open
"	FLINK	Resolved	3	1	8669	github-import
13376787	Adaptive Scheduler: Can not cancel restarting job	"I have a job in state RESTARTING. When I now issue a cancel RPC call, I get the following exception:

Relevant trace:
{code}
Caused by: java.lang.IllegalStateException: Assuming running execution graph 
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
 at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.(StateWithExecutionGraph.java:94) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling.(Canceling.java:41) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:98) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:72) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1128) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCanceling(AdaptiveScheduler.java:802) 
 at org.apache.flink.runtime.scheduler.adaptive.Restarting.cancel(Restarting.java:74) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.cancel(AdaptiveScheduler.java:453) 
 at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:417) 
{code}

Full trace as reported in the UI:
{code}
org.apache.flink.runtime.rest.handler.RestHandlerException: Job cancellation failed: Cancellation failed. 
 at org.apache.flink.runtime.rest.handler.job.JobCancellationHandler.lambda$handleRequest$0(JobCancellationHandler.java:127) 
 at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) 
 at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) 
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) 
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079) 
 at akka.dispatch.OnComplete.internal(Future.scala:263) 
 at akka.dispatch.OnComplete.internal(Future.scala:261) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) 
 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) 
 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:573) 
 at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) 
 at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) 
 at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532) 
 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29) 
 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) 
 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91) 
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) 
 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) 
 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91) 
 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) 
 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) 
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) 
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: org.apache.flink.util.FlinkException: Cancellation failed. 
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$cancel$3(JobMasterServiceLeadershipRunner.java:197) 
 at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) 
 at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) 
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) 
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079) 
 at akka.dispatch.OnComplete.internal(Future.scala:263) 
 at akka.dispatch.OnComplete.internal(Future.scala:261) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) 
 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) 
 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:573) 
 at akka.actor.ActorRef.tell(ActorRef.scala:126) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:311) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) 
 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) 
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) 
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) 
 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) 
 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) 
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
 at akka.actor.Actor.aroundReceive(Actor.scala:517) 
 at akka.actor.Actor.aroundReceive$(Actor.scala:515) 
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) 
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) 
 at akka.actor.ActorCell.invoke(ActorCell.scala:561) 
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) 
 at akka.dispatch.Mailbox.run(Mailbox.scala:225) 
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ... 4 more Caused by: java.lang.IllegalStateException: Assuming running execution graph 
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
 at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.(StateWithExecutionGraph.java:94) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling.(Canceling.java:41) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:98) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:72) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1128) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCanceling(AdaptiveScheduler.java:802) 
 at org.apache.flink.runtime.scheduler.adaptive.Restarting.cancel(Restarting.java:74) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.cancel(AdaptiveScheduler.java:453) 
 at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:417) 
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
 at java.lang.reflect.Method.invoke(Method.java:498) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) ... 23 more
{code}
(Sorry for the poor stack trace formatting, I coped the stack trace from the UI)

It seems that the Canceling state assumes we only transition into it from Executing (ExecutionGraph = RUNNING).

In my opinion a job should be cancellable at all times, for example when a job is stuck in a restart loop, cancelling is the only way out (unless retries are exhausted)."	FLINK	Closed	2	1	8669	pull-request-available
13336879	Upgrade commons_codec to 1.13 or newer	"A user reported a dependency vulnerability which affects {{commons_codec}} [1]. We should try to upgrade this version to 1.13 or newer.

[1] https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E"	FLINK	Closed	2	4	8669	pull-request-available
13318752	"Update version of aws to support use of default constructor of ""WebIdentityTokenCredentialsProvider"""	"*Background:*

I am using Flink 1.11.0 on kubernetes platform. To give access of aws services to taskmanager/jobmanager, we are using ""IAM Roles for Service Accounts"" . I have configured below property in flink-conf.yaml to use credential provider.

fs.s3a.aws.credentials.provider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider

 

*Issue:*

When taskmanager/jobmanager is starting up, during this it complains that ""WebIdentityTokenCredentialsProvider"" doesn't have ""public constructor"" and container doesn't come up.

 

*Solution:*

Currently the above credential's class is being used from ""*flink-s3-fs-hadoop""* which gets ""aws-java-sdk-core"" dependency from ""*flink-s3-fs-base*"". In *""flink-s3-fs-base"",*  version of aws is 1.11.754 . The support of default constructor for ""WebIdentityTokenCredentialsProvider"" is provided from aws version 1.11.788 and onward."	FLINK	Closed	4	4	8669	pull-request-available
13329990	flink-dist won't build locally with newer (3.3+) maven versions	"flink-dist will fail on non Maven 3.2.5 versions because of banned dependencies.

These are the messages you'll see:
{code}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) @ flink-dist_2.11 ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: org.yaml:snakeyaml:jar:1.24
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Flink : 1.12-SNAPSHOT:

...

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (ban-unsafe-snakeyaml) on project flink-dist_2.11: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]
{code}"	FLINK	Closed	1	1	8669	pull-request-available
13240859	YARNSessionCapacitySchedulerITCase failed due to non prohibited exception	"YARNSessionCapacitySchedulerITCase fails due to non prohibited exception.

[https://api.travis-ci.org/v3/job/548491542/log.txt]
{code:java}
2019-06-21 08:22:27,313 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (SUM(1), at main(WordCount.java:79) (2/2) (a1708bb0544633b4e57e8bb84a1a48f3) switched from RUNNING to FAILED.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeNoLongerUsedSlots(TaskExecutor.java:1475)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.syncSlotsWithSnapshotFromJobMaster(TaskExecutor.java:1436)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$3200(TaskExecutor.java:141)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobManagerHeartbeatListener.lambda$reportPayload$1(TaskExecutor.java:1691)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2019-06-21 08:22:27,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Flink Java Job at Fri Jun 21 08:22:16 UTC 2019 (8f8dced4fb89f8e5cb05d9286683ecaf) switched from state RUNNING to FAILING.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeNoLongerUsedSlots(TaskExecutor.java:1475)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.syncSlotsWithSnapshotFromJobMaster(TaskExecutor.java:1436)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$3200(TaskExecutor.java:141)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobManagerHeartbeatListener.lambda$reportPayload$1(TaskExecutor.java:1691)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21){code}"	FLINK	Closed	2	1	8669	test-stability
13335303	Test failed in Azure For EmulatedPubSubSourceTest	" 

The link is [https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/7545/logs/133|https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/7545/logs/133]
{code:java}

 [ERROR] Tests run: 3, Failures: 1, Errors: 2, Skipped: 0, Time elapsed: 1.705 s <<< FAILURE! - in org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest
2020-10-13T18:12:53.5967780Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest  Time elapsed: 1.703 s  <<< FAILURE!
2020-10-13T18:12:53.5973768Z java.lang.AssertionError: We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.5979530Z 	at org.junit.Assert.fail(Assert.java:88)
2020-10-13T18:12:53.5980372Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-10-13T18:12:53.5980722Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-10-13T18:12:53.5981575Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager.launchDocker(GCloudEmulatorManager.java:141)
2020-10-13T18:12:53.5982596Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudUnitTestBase.launchGCloudEmulator(GCloudUnitTestBase.java:45)
2020-10-13T18:12:53.5983234Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-13T18:12:53.5983626Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-13T18:12:53.5984410Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-13T18:12:53.5985246Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-13T18:12:53.5985825Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-13T18:12:53.5986306Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-13T18:12:53.5986988Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-13T18:12:53.5987740Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2020-10-13T18:12:53.5988167Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-13T18:12:53.5988550Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-13T18:12:53.5988954Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-13T18:12:53.5989404Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-13T18:12:53.5989888Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-13T18:12:53.5990332Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-13T18:12:53.5990819Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-13T18:12:53.5991302Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-13T18:12:53.5991752Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-13T18:12:53.5992161Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-13T18:12:53.5992420Z 
2020-10-13T18:12:53.5992746Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest  Time elapsed: 1.704 s  <<< ERROR!
2020-10-13T18:12:53.5993127Z java.lang.NullPointerException
2020-10-13T18:12:53.5993502Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest.tearDown(EmulatedPubSubSinkTest.java:62)
2020-10-13T18:12:53.5993944Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-13T18:12:53.5994307Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-13T18:12:53.5994757Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-13T18:12:53.5995151Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-13T18:12:53.5995532Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-13T18:12:53.5995990Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-13T18:12:53.5996534Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-13T18:12:53.5996974Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2020-10-13T18:12:53.5997352Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-13T18:12:53.5997760Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-13T18:12:53.5998211Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-13T18:12:53.5998687Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-13T18:12:53.5999127Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-13T18:12:53.5999613Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-13T18:12:53.6000096Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-13T18:12:53.6000551Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-13T18:12:53.6001131Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-13T18:12:53.6001361Z 
2020-10-13T18:12:53.6001722Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest  Time elapsed: 1.705 s  <<< ERROR!
2020-10-13T18:12:53.6002100Z java.lang.NullPointerException
2020-10-13T18:12:53.6002531Z 	at org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudUnitTestBase.terminateGCloudEmulator(GCloudUnitTestBase.java:50)
2020-10-13T18:12:53.6002970Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-13T18:12:53.6003334Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-13T18:12:53.6003932Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-13T18:12:53.6004505Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-13T18:12:53.6005045Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-13T18:12:53.6005741Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-13T18:12:53.6006340Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-13T18:12:53.6006990Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2020-10-13T18:12:53.6007557Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-13T18:12:53.6007987Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-13T18:12:53.6008614Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-13T18:12:53.6009099Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-13T18:12:53.6009906Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-13T18:12:53.6010569Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-13T18:12:53.6011069Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-13T18:12:53.6011754Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-13T18:12:53.6012190Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-13T18:12:53.6012431Z 
2020-10-13T18:12:53.9374060Z [INFO] 
2020-10-13T18:12:53.9374787Z [INFO] Results:
2020-10-13T18:12:53.9375080Z [INFO] 
2020-10-13T18:12:53.9375355Z [ERROR] Errors: 
2020-10-13T18:12:53.9375824Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest
2020-10-13T18:12:53.9376542Z [ERROR]   Run 1: CheckPubSubEmulatorTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9377425Z [ERROR]   Run 2: CheckPubSubEmulatorTest.tearDown:63 NullPointer
2020-10-13T18:12:53.9378871Z [ERROR]   Run 3: CheckPubSubEmulatorTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9379354Z [INFO] 
2020-10-13T18:12:53.9379804Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedFullTopologyTest.org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedFullTopologyTest
2020-10-13T18:12:53.9380731Z [ERROR]   Run 1: EmulatedFullTopologyTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9381341Z [ERROR]   Run 2: EmulatedFullTopologyTest.tearDown:80 Missing pubsubHelper.
2020-10-13T18:12:53.9382337Z [ERROR]   Run 3: EmulatedFullTopologyTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9383133Z [INFO] 
2020-10-13T18:12:53.9383578Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest.org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSinkTest
2020-10-13T18:12:53.9384435Z [ERROR]   Run 1: EmulatedPubSubSinkTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9384995Z [ERROR]   Run 2: EmulatedPubSubSinkTest.tearDown:62 NullPointer
2020-10-13T18:12:53.9385871Z [ERROR]   Run 3: EmulatedPubSubSinkTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9386348Z [INFO] 
2020-10-13T18:12:53.9386781Z [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSourceTest.org.apache.flink.streaming.connectors.gcp.pubsub.EmulatedPubSubSourceTest
2020-10-13T18:12:53.9387466Z [ERROR]   Run 1: EmulatedPubSubSourceTest>GCloudUnitTestBase.launchGCloudEmulator:45 We expect 1 port to be mapped expected:<1> but was:<0>
2020-10-13T18:12:53.9388032Z [ERROR]   Run 2: EmulatedPubSubSourceTest.tearDown:66 NullPointer
2020-10-13T18:12:53.9388799Z [ERROR]   Run 3: EmulatedPubSubSourceTest>GCloudUnitTestBase.terminateGCloudEmulator:50 Â» NullPointer
2020-10-13T18:12:53.9389237Z [INFO] 
2020-10-13T18:12:53.9389484Z [INFO]
{code}
 "	FLINK	Closed	1	1	8669	pull-request-available, test-stability
13356475	Add DeclarativeScheduler / Failing state	"This subtask of adding the declarative scheduler is about adding the Failing state to Flink, including tests.

Failing: An unrecoverable fault has occurred. The scheduler stops the ExecutionGraph by canceling it.
"	FLINK	Closed	3	7	8669	pull-request-available
12719627	Integrate runtime metrics / statistics	"The engine should collect job execution statistics (e.g., via accumulators) such as:
- total number of input / output records per operator
- histogram of input/output ratio of UDF calls
- histogram of number of input records per reduce / cogroup UDF call
- histogram of number of output records per UDF call
- histogram of time spend in UDF calls
- number of local and remote bytes read (not via accumulators)
- ...

These stats should be made available to the user after execution (via webfrontend). The purpose of this feature is to ease performance debugging of parallel jobs (e.g., to detect data skew).
It should be possible to deactivate (or activate) the gathering of these statistics.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/456
Created by: [fhueske|https://github.com/fhueske]
Labels: enhancement, runtime, user satisfaction, 
Created at: Tue Feb 04 20:32:49 CET 2014
State: open
"	FLINK	Closed	3	2	8669	github-import
13468525	flink-parquet doesn't compile on M1 mac without rosetta	"Compiling Flink 1.16-SNAPSHOT fails on an M1 Mac (apple silicon) without the rosetta translation layer, because the automatically downloaded ""protoc-3.17.3-osx-aarch_64.exe"" file is actually just a copy of ""protoc-3.17.3-osx-x86_64.exe"". (as you can read here: https://github.com/os72/protoc-jar/issues/93)

This is the error:
{code}
[ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:test-compile (default) on project flink-parquet: An error occurred while invoking protoc. Error while executing process. Cannot run program ""/Users/rmetzger/Projects/flink/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.17.3-osx-aarch_64.exe"": error=86, Bad CPU type in executable -> [Help 1]
{code}

"	FLINK	Closed	3	7	8669	pull-request-available
13337178	Automatically run a basic NOTICE file check on CI 	"For every release, we are manually validating the NOTICE files, according to this wiki page: https://cwiki.apache.org/confluence/display/FLINK/Licensing

The most time-consuming task is ensuring that all modules that deploy a shaded dependency to maven central are properly documenting this dependency in their NOTICE file.

I would like to add a tool to Flink that checks if all shaded dependencies are at least mentioned in the NOTICE file.
We will still need to perform a manual checks, but the tool should catch the most common, severe and difficult to find problems."	FLINK	Closed	3	4	8669	pull-request-available
13309727	YARN session logs about HADOOP_CONF_DIR even though HADOOP_CLASSPATH containing a config is set	"Flink prints 
{code}
Setting HADOOP_CONF_DIR=/etc/hadoop/conf because no HADOOP_CONF_DIR was set.
{code}

When running Flink on YARN with the HADOOP_CLASSPATH set. ""hadoop classpath"" also returns the configuration directory, so HADOOP_CONF_DIR is not needed anymore.

I suggest to revisit this log message / behavior as it is misleading"	FLINK	Closed	3	4	8669	pull-request-available, usability
13076769	Elasticsearch 5 release artifacts not published to Maven central	Release artifacts for the Elasticsearch 5 connector is not published to the Maven Central. Elasticsearch 5 requires Java 8 at minimum, so for the release we need to build with Java 8 for this.	FLINK	Resolved	1	1	8669	flink-rel-1.3.1-blockers
13344781	generate-stackbrew-library.sh in flink-docker doesn't properly prune the java11 tag	The output of {generate-stackbrew-library.sh} contains two {java11} tags.	FLINK	Closed	2	1	8669	pull-request-available
13310147	CheckPubSubEmulatorTest failed on azure	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2930&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2930&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

 

 
{code:java}
2020-06-08T12:45:15.9874996Z 82609 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest [] - Waiting a while to receive the m
 essage...
*2020-06-08T12:45:16.1955546Z 82816 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest [] - Timeout during shutdown
*2020-06-08T12:45:16.1956405Z java.util.concurrent.TimeoutException: Timed out waiting for InnerService [STOPPING] to reach a terminal state. Current state: ST*OPPING
...
2020-06-08T12:46:08.5914230Z 135213 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager [] -
 2020-06-08T12:46:08.6054783Z [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 54.754 s <<< FAILURE! - in org.apache.flink.streaming.con nectors.gcp.pubsub.CheckPubSubEmulatorTest
 2020-06-08T12:46:08.6062906Z [ERROR] testPull(org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest) Time elapsed: 52.123 s <<< FAILURE!
 2020-06-08T12:46:08.6063659Z java.lang.AssertionError: expected:<1> but was:<0>
{code}
 "	FLINK	Closed	2	1	8669	pull-request-available
13346419	Missing docker images for 1.12 release	"Images for Flink 1.12 are missing in Docker hub https://hub.docker.com/_/flink. As a result Kubernetes deployment as in the documentation example is not working.

https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/native_kubernetes.html"	FLINK	Closed	1	1	8669	pull-request-available
13343324	Broken links to hadoop.md	"In FLINK-20347 we removed the {{hadoop.md}} page, but there are still links in other pages:
* dev/project-configuration.md
* dev/batch/hadoop-compatibility.md
* connectors/hive/index.md"	FLINK	Closed	2	1	8669	pull-request-available
13307739	Test native K8s integration	"Test Flink's native K8s integration:

* session mode
* application mode
* custom Flink image
* custom configuration and log properties"	FLINK	Closed	2	7	8669	pull-request-available, release-testing
13310438	"""Avro Confluent Schema Registry nightly end-to-end test"" unstable with ""Kafka cluster did not start after 120 seconds"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3045&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-06-09T15:16:48.1427795Z ==============================================================================
2020-06-09T15:16:48.1428609Z Running 'Avro Confluent Schema Registry nightly end-to-end test'
2020-06-09T15:16:48.1429204Z ==============================================================================
2020-06-09T15:16:48.1438117Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-48143298170
2020-06-09T15:16:48.2985167Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-06-09T15:16:48.3157575Z Downloading Kafka from https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz
2020-06-09T15:16:48.3214487Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-06-09T15:16:48.3215154Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-06-09T15:16:48.3215597Z 
2020-06-09T15:16:48.3528820Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:49.3421526Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:50.3415678Z   8 35.8M    8 2960k    0     0  2896k      0  0:00:12  0:00:01  0:00:11 2896k
2020-06-09T15:16:51.3406836Z  23 35.8M   23 8544k    0     0  4226k      0  0:00:08  0:00:02  0:00:06 4225k
2020-06-09T15:16:51.6553485Z  70 35.8M   70 25.2M    0     0  8550k      0  0:00:04  0:00:03  0:00:01 8548k
2020-06-09T15:16:51.6555606Z 100 35.8M  100 35.8M    0     0  10.7M      0  0:00:03  0:00:03 --:--:-- 10.7M
2020-06-09T15:16:51.9818041Z Downloading confluent from http://packages.confluent.io/archive/3.2/confluent-oss-3.2.0-2.11.tar.gz
2020-06-09T15:16:51.9880242Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2020-06-09T15:16:51.9880983Z                                  Dload  Upload   Total   Spent    Left  Speed
2020-06-09T15:16:51.9914252Z 
2020-06-09T15:16:52.3398614Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2020-06-09T15:16:53.3399552Z   9  398M    9 39.5M    0     0   111M      0  0:00:03 --:--:--  0:00:03  111M
2020-06-09T15:16:53.9149276Z  47  398M   47  188M    0     0   139M      0  0:00:02  0:00:01  0:00:01  138M
2020-06-09T15:16:53.9150980Z 100  398M  100  398M    0     0   206M      0  0:00:01  0:00:01 --:--:--  206M
2020-06-09T15:17:04.3565942Z Waiting for broker...
2020-06-09T15:17:12.4215170Z Waiting for broker...
2020-06-09T15:17:14.3012835Z Waiting for broker...
2020-06-09T15:17:16.1965074Z Waiting for broker...
2020-06-09T15:17:18.1102274Z Waiting for broker...
2020-06-09T15:17:19.9929632Z Waiting for broker...
2020-06-09T15:17:21.8607172Z Waiting for broker...
2020-06-09T15:17:23.7802949Z Waiting for broker...
2020-06-09T15:17:25.6695260Z Waiting for broker...
2020-06-09T15:17:27.5536417Z Waiting for broker...
2020-06-09T15:17:29.4327778Z Waiting for broker...
2020-06-09T15:17:31.3203091Z Waiting for broker...
2020-06-09T15:17:33.1987150Z Waiting for broker...
2020-06-09T15:17:35.0694860Z Waiting for broker...
2020-06-09T15:17:36.9595576Z Waiting for broker...
2020-06-09T15:17:38.9243558Z Waiting for broker...
2020-06-09T15:17:40.7984064Z Waiting for broker...
2020-06-09T15:17:42.6676095Z Waiting for broker...
2020-06-09T15:17:44.5628797Z Waiting for broker...
2020-06-09T15:17:46.4374532Z Waiting for broker...
2020-06-09T15:17:48.3086761Z Waiting for broker...
2020-06-09T15:17:50.1574336Z Waiting for broker...
2020-06-09T15:17:52.0432952Z Waiting for broker...
2020-06-09T15:17:53.9406541Z Waiting for broker...
2020-06-09T15:17:55.8162052Z Waiting for broker...
2020-06-09T15:17:57.7090015Z Waiting for broker...
2020-06-09T15:17:59.5747770Z Waiting for broker...
2020-06-09T15:18:01.4601854Z Waiting for broker...
2020-06-09T15:18:03.3332039Z Waiting for broker...
2020-06-09T15:18:05.2210453Z Waiting for broker...
2020-06-09T15:18:07.1133675Z Waiting for broker...
2020-06-09T15:18:09.0132417Z Waiting for broker...
2020-06-09T15:18:10.8769511Z Waiting for broker...
2020-06-09T15:18:12.7601639Z Waiting for broker...
2020-06-09T15:18:14.6389770Z Waiting for broker...
2020-06-09T15:18:16.5210725Z Waiting for broker...
2020-06-09T15:18:18.4088216Z Waiting for broker...
2020-06-09T15:18:20.2732225Z Waiting for broker...
2020-06-09T15:18:22.1558390Z Waiting for broker...
2020-06-09T15:18:24.0400570Z Waiting for broker...
2020-06-09T15:18:25.9134038Z Waiting for broker...
2020-06-09T15:18:27.7922350Z Waiting for broker...
2020-06-09T15:18:29.6748679Z Waiting for broker...
2020-06-09T15:18:31.5340996Z Waiting for broker...
2020-06-09T15:18:33.3998472Z Waiting for broker...
2020-06-09T15:18:35.2718135Z Waiting for broker...
2020-06-09T15:18:37.1426082Z Waiting for broker...
2020-06-09T15:18:39.1282264Z Waiting for broker...
2020-06-09T15:18:41.0029183Z Waiting for broker...
2020-06-09T15:18:42.8700037Z Waiting for broker...
2020-06-09T15:18:44.7531621Z Waiting for broker...
2020-06-09T15:18:46.6465173Z Waiting for broker...
2020-06-09T15:18:48.9504192Z Waiting for broker...
2020-06-09T15:18:50.4165383Z Waiting for broker...
2020-06-09T15:18:52.2931688Z Waiting for broker...
2020-06-09T15:18:54.1669857Z Waiting for broker...
2020-06-09T15:18:56.0238505Z Waiting for broker...
2020-06-09T15:18:57.8931143Z Waiting for broker...
2020-06-09T15:18:59.7607751Z Kafka cluster did not start after 120 seconds. Printing Kafka logs:
{code}
There's a lot of log output I didn't analyze yet."	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13533001	Azure Warning: no space left on device	"In this CI run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=841082b6-1a93-5908-4d37-a071f4387a5f&l=21

There was this warning:
{code}
Loaded image: confluentinc/cp-kafka:6.2.2
Loaded image: testcontainers/ryuk:0.3.3
ApplyLayer exit status 1 stdout:  stderr: write /opt/jdk-15.0.1+9/lib/modules: no space left on device
##[error]Bash exited with code '1'.
Finishing: Restore docker images
{code}"	FLINK	Closed	3	1	8669	build-stability, pull-request-available
13293813	the java e2e profile isn't setting the hadoop switch on Azure	"Context: https://lists.apache.org/thread.html/r06e597b3dadfee00593989b1cfae0f2b83548f412c8fdca6d4bc3dbe%40%3Cdev.flink.apache.org%3E

{quote}- the azure setup doesn't appear to be equivalent yet since the java e2e profile isn't setting the hadoop switch (-Pe2e-hadoop), as a result of which SQLClientKafkaITCase isn't run
{quote}"	FLINK	Resolved	3	4	8669	pull-request-available
12986476	CliFrontendYarnAddressConfigurationTest picks wrong IP address on Travis	"{code}
 CliFrontendYarnAddressConfigurationTest.testManualOptionsOverridesYarn:274->checkJobManagerAddress:424 expected:<[ip-10-221-130-22.ec2.internal]> but was:<[10.221.130.22]>
{code}

https://s3.amazonaws.com/archive.travis-ci.org/jobs/142007244/log.txt
https://s3.amazonaws.com/archive.travis-ci.org/jobs/142007245/log.txt
https://s3.amazonaws.com/archive.travis-ci.org/jobs/142007246/log.txt

This is a build from a personal branch, but should also happen on master."	FLINK	Resolved	3	1	8669	test-stability
13287779	Execute all end to end tests on AZP	"Ensure that we execute all end to end tests on AZP:
- Make sure that all the e2e tests referenced in the splits are also referenced in the ""run nightly tests"" script
- make sure the java e2e tests are executed"	FLINK	Resolved	3	7	8669	pull-request-available
13300458	"MVN exited with EXIT CODE: 143. in ""libraries"" test job"	"CI:https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=89&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=32b25b6b-f46f-5bca-b5eb-2c6936ee77a4

maven reports ""build success"", but the exit code is 143?

{code}
[INFO] flink-state-processor-api .......................... SUCCESS [  0.273 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 19:24 min
[INFO] Finished at: 2020-04-23T00:46:43+00:00
[INFO] Final Memory: 246M/4214M
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile ""e2e-hadoop"" could not be activated because it does not exist.
MVN exited with EXIT CODE: 143.
Trying to KILL watchdog (265).
==============================================================================

{code}"	FLINK	Resolved	2	1	8669	test-stability
13292119	Streaming bucketing end-to-end test output hash mismatch	"https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/6298/logs/722

Some of the output mismatch failures were reported in another ticket: https://issues.apache.org/jira/browse/FLINK-16227

{code}
2020-03-17T02:04:19.9176915Z Number of produced values 30618/60000
2020-03-17T02:04:19.9202731Z Truncating buckets
2020-03-17T02:04:25.0504959Z Truncating buckets
2020-03-17T02:04:30.1731295Z Truncating buckets
2020-03-17T02:04:35.3190114Z Truncating buckets
2020-03-17T02:04:40.4723887Z Truncating buckets
2020-03-17T02:04:45.5984655Z Truncating buckets
2020-03-17T02:04:50.7185356Z Truncating buckets
2020-03-17T02:04:55.8627129Z Truncating buckets
2020-03-17T02:05:01.0715985Z Number of produced values 74008/60000
2020-03-17T02:05:02.3976850Z Cancelling job dba2fdb79579158295db27d0214fc2ff.
2020-03-17T02:05:03.4633541Z Cancelled job dba2fdb79579158295db27d0214fc2ff.
2020-03-17T02:05:03.4738270Z Waiting for job (dba2fdb79579158295db27d0214fc2ff) to reach terminal state CANCELED ...
2020-03-17T02:05:03.5149228Z Job (dba2fdb79579158295db27d0214fc2ff) reached terminal state CANCELED
2020-03-17T02:05:03.5150587Z Job dba2fdb79579158295db27d0214fc2ff was cancelled, time to verify
2020-03-17T02:05:03.5590118Z FAIL Bucketing Sink: Output hash mismatch.  Got c3787e7a52d913675e620837a7531742, expected 01aba5ff77a0ef5e5cf6a727c248bdc3.
2020-03-17T02:05:03.5591888Z head hexdump of actual:
2020-03-17T02:05:03.5989908Z 0000000   (   7   ,   1   0   ,   0   ,   S   o   m   e       p   a   y
2020-03-17T02:05:03.5991252Z 0000010   l   o   a   d   .   .   .   )  \n   (   7   ,   1   0   ,   1
2020-03-17T02:05:03.5991923Z 0000020   ,   S   o   m   e       p   a   y   l   o   a   d   .   .   .
2020-03-17T02:05:03.5993055Z 0000030   )  \n   (   7   ,   1   0   ,   2   ,   S   o   m   e       p
2020-03-17T02:05:03.5993690Z 0000040   a   y   l   o   a   d   .   .   .   )  \n   (   7   ,   1   0
2020-03-17T02:05:03.5994332Z 0000050   ,   3   ,   S   o   m   e       p   a   y   l   o   a   d   .
2020-03-17T02:05:03.5994967Z 0000060   .   .   )  \n   (   7   ,   1   0   ,   4   ,   S   o   m   e
2020-03-17T02:05:03.5995744Z 0000070       p   a   y   l   o   a   d   .   .   .   )  \n   (   7   ,
2020-03-17T02:05:03.5996359Z 0000080   1   0   ,   5   ,   S   o   m   e       p   a   y   l   o   a
2020-03-17T02:05:03.5997133Z 0000090   d   .   .   .   )  \n   (   7   ,   1   0   ,   6   ,   S   o
2020-03-17T02:05:03.5997704Z 00000a0   m   e       p   a   y   l   o   a   d   .   .   .   )  \n   (
2020-03-17T02:05:03.5998295Z 00000b0   7   ,   1   0   ,   7   ,   S   o   m   e       p   a   y   l
2020-03-17T02:05:03.5999087Z 00000c0   o   a   d   .   .   .   )  \n   (   7   ,   1   0   ,   8   ,
2020-03-17T02:05:03.6000243Z 00000d0   S   o   m   e       p   a   y   l   o   a   d   .   .   .   )
2020-03-17T02:05:03.6000880Z 00000e0  \n   (   7   ,   1   0   ,   9   ,   S   o   m   e       p   a
2020-03-17T02:05:03.6001494Z 00000f0   y   l   o   a   d   .   .   .   )  \n                        
2020-03-17T02:05:03.6001999Z 00000fa
2020-03-17T02:05:03.9875220Z Stopping taskexecutor daemon (pid: 49278) on host fv-az668.
2020-03-17T02:05:04.2569285Z Stopping standalonesession daemon (pid: 46323) on host fv-az668.
2020-03-17T02:05:04.7664418Z Stopping taskexecutor daemon (pid: 46615) on host fv-az668.
2020-03-17T02:05:04.7674722Z Skipping taskexecutor daemon (pid: 47009), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7687383Z Skipping taskexecutor daemon (pid: 47299), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7689091Z Skipping taskexecutor daemon (pid: 47619), because it is not running anymore on fv-az668.
2020-03-17T02:05:04.7690289Z Stopping taskexecutor daemon (pid: 48538) on host fv-az668.
2020-03-17T02:05:04.7691796Z Stopping taskexecutor daemon (pid: 48988) on host fv-az668.
2020-03-17T02:05:04.7692365Z [FAIL] Test script contains errors.
2020-03-17T02:05:04.7713750Z Checking of logs skipped.
2020-03-17T02:05:04.7714249Z 
2020-03-17T02:05:04.7715316Z [FAIL] 'Streaming bucketing end-to-end test' failed after 2 minutes and 43 seconds! Test exited with exit code 1
{code}"	FLINK	Resolved	1	1	8669	pull-request-available, test-stability
13255281	Maven instructions for 3.3+ do not cover all shading special cases	"When building Flink on Maven 3.3+ extra care must be taken to ensure that the shading works as expected. Since 3.3 the dependency graph is immutable, as a result of which downstream modules (like flink-dist) see the unaltered set of dependencies of bundled modules; regardless of these were bundled or not. As a result dependencies may be bundled multiple times (original and relocated versions).

The [instructions for building Flink with Maven 3.3+|https://ci.apache.org/projects/flink/flink-docs-master/flinkDev/building.html#dependency-shading] correctly point out that flink-dist must be built separately, however (at the very least) all filesystems relying on {{flink-fs-hadoop-shaded}} are also affected."	FLINK	Closed	3	4	8669	pull-request-available
13293736	HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart hangs	"Logs: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6584&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=d26b3528-38b0-53d2-05f7-37557c2405e4]
{code:java}
2020-03-24T15:52:18.9196862Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd36c00b800 nid=0xc21 runnable [0x00007fd3743ce000]
2020-03-24T15:52:18.9197235Z    java.lang.Thread.State: RUNNABLE
2020-03-24T15:52:18.9197536Z 	at java.net.SocketInputStream.socketRead0(Native Method)
2020-03-24T15:52:18.9197931Z 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
2020-03-24T15:52:18.9198340Z 	at java.net.SocketInputStream.read(SocketInputStream.java:171)
2020-03-24T15:52:18.9198749Z 	at java.net.SocketInputStream.read(SocketInputStream.java:141)
2020-03-24T15:52:18.9199171Z 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)
2020-03-24T15:52:18.9199840Z 	at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593)
2020-03-24T15:52:18.9200265Z 	at sun.security.ssl.InputRecord.read(InputRecord.java:532)
2020-03-24T15:52:18.9200663Z 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)
2020-03-24T15:52:18.9201213Z 	- locked <0x00000000927583d8> (a java.lang.Object)
2020-03-24T15:52:18.9201589Z 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)
2020-03-24T15:52:18.9202026Z 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)
2020-03-24T15:52:18.9202583Z 	- locked <0x0000000092758c00> (a sun.security.ssl.AppInputStream)
2020-03-24T15:52:18.9203029Z 	at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)
2020-03-24T15:52:18.9203558Z 	at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)
2020-03-24T15:52:18.9204121Z 	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)
2020-03-24T15:52:18.9204626Z 	at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
2020-03-24T15:52:18.9205121Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9205679Z 	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
2020-03-24T15:52:18.9206164Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9206786Z 	at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
2020-03-24T15:52:18.9207361Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9207839Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9208327Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9208809Z 	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
2020-03-24T15:52:18.9209273Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9210003Z 	at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
2020-03-24T15:52:18.9210658Z 	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
2020-03-24T15:52:18.9211154Z 	at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$read$3(S3AInputStream.java:445)
2020-03-24T15:52:18.9211631Z 	at org.apache.hadoop.fs.s3a.S3AInputStream$$Lambda$42/1936375962.execute(Unknown Source)
2020-03-24T15:52:18.9212044Z 	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
2020-03-24T15:52:18.9212553Z 	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)
2020-03-24T15:52:18.9212972Z 	at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/1457226878.execute(Unknown Source)
2020-03-24T15:52:18.9213408Z 	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
2020-03-24T15:52:18.9213866Z 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)
2020-03-24T15:52:18.9214273Z 	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:231)
2020-03-24T15:52:18.9214701Z 	at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:441)
2020-03-24T15:52:18.9215443Z 	- locked <0x00000000926e88b0> (a org.apache.hadoop.fs.s3a.S3AInputStream)
2020-03-24T15:52:18.9215852Z 	at java.io.DataInputStream.read(DataInputStream.java:149)
2020-03-24T15:52:18.9216305Z 	at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:94)
2020-03-24T15:52:18.9216781Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-03-24T15:52:18.9217187Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-03-24T15:52:18.9217571Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-03-24T15:52:18.9218108Z 	- locked <0x00000000926ea000> (a java.io.InputStreamReader)
2020-03-24T15:52:18.9218475Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-03-24T15:52:18.9218876Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-03-24T15:52:18.9219261Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-03-24T15:52:18.9219890Z 	- locked <0x00000000926ea000> (a java.io.InputStreamReader)
2020-03-24T15:52:18.9220256Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-03-24T15:52:18.9220914Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.getContentsOfFile(HadoopS3RecoverableWriterITCase.java:423)
2020-03-24T15:52:18.9221704Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersist(HadoopS3RecoverableWriterITCase.java:411)
2020-03-24T15:52:18.9222457Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(HadoopS3RecoverableWriterITCase.java:364)
2020-03-24T15:52:18.9223222Z 	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterITCase.testRecoverWithStateWithMultiPart(HadoopS3RecoverableWriterITCase.java:330)
2020-03-24T15:52:18.9223817Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-24T15:52:18.9224232Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-24T15:52:18.9224729Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-24T15:52:18.9225160Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-24T15:52:18.9225675Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-03-24T15:52:18.9226171Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-03-24T15:52:18.9226682Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-03-24T15:52:18.9227187Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-03-24T15:52:18.9227661Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-03-24T15:52:18.9228145Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-03-24T15:52:18.9228718Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-03-24T15:52:18.9229112Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-03-24T15:52:18.9229582Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-03-24T15:52:18.9230029Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-03-24T15:52:18.9230525Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-03-24T15:52:18.9230963Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-03-24T15:52:18.9231546Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-03-24T15:52:18.9231999Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-03-24T15:52:18.9232432Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-03-24T15:52:18.9232862Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-03-24T15:52:18.9233307Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-03-24T15:52:18.9233833Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-03-24T15:52:18.9234284Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-03-24T15:52:18.9234700Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-03-24T15:52:18.9235076Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-03-24T15:52:18.9235599Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-24T15:52:18.9236124Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-24T15:52:18.9236648Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-24T15:52:18.9237167Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-24T15:52:18.9237688Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-24T15:52:18.9238244Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-24T15:52:18.9238745Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-24T15:52:18.9239202Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-24T15:52:18.9239549Z 
2020-03-24T15:52:18.9239794Z ""VM Thread"" os_prio=0 tid=0x00007fd36c260800 nid=0xc58 runnable {code}
 "	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13288852	Disable JDK 11 Docker tests on AZP	"Build log: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5779&view=logs&j=eec879f1-c5a2-5810-2b49-ba5c6bfecb27&t=484f04d6-55db-5161-9f93-391b1677737d

{code}Error: A JNI error has occurred, please check your installation and try again
Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/flink/client/cli/CliFrontend has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)
Running the job failed.
{code}"	FLINK	Resolved	3	1	8669	pull-request-available
13387573	Cannot compile Flink on MacOS with M1 chip	"Flink doesn't currently compile on MacOS with M1 silicon.

This is true for all recent versions (1.13.X) as well as master.

Some of the problems have potentially easy fixes, such as installing node separately or updating the relevant pom.xml to use a newer version of node. I am getting some errors about deprecated features being used which are not supported by newer node, but on the surface they seem easy to resolve. 

I've had less success with complex dependencies such as protobuf.

My long term objective is to use and contribute to Flink. If I can get some help with the above issues, I am willing to make the modifications, submit the changes as a pull request, and shepherd them to release. If compilation on MacOS/M1 is not a priority, I can look for a virtual machine solution instead. Feedback appreciated. 

 

Thanks

 

Osama"	FLINK	Closed	4	1	8669	pull-request-available
13394871	Hide any configuration, API or docs	"As the feature will not make it to the upcoming 1.14 release,

hide the related config options like CheckpointingOptions.ENABLE_STATE_CHANGE_LOG, API  (e.g. StreamExecutionEnvironment.enableChangelogStateBackend).

Also check Python and Scala APIs and the documentation."	FLINK	Closed	2	7	8742	pull-request-available
13413442	Add ChangelogBackend documentation	"Currently, changelog backend is hidden from users documentation-wise.

Once the feature is ready, the following needs to be documented:
 * General description (page [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/] )
 * Configuration (page [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/] - StateChangelogOptions, FsStateChangelogOptions)
 * Uploader metrics (page [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/] , see FLINK-23486)"	FLINK	Resolved	3	7	8742	pull-request-available
13399887	BoundedSourceITCase hangs on azure	https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23718&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5389	FLINK	Closed	3	1	8742	test-stability
13436935	Notify CheckpointStrategy about checkpoint completion/abortion	"Notifications could be used by incremental snapshot strategy
to replace state handles with placeholders."	FLINK	In Progress	3	7	8742	pull-request-available, stale-assigned
13427582	[Changelog] Non-deterministic recovery of PriorityQueue states	"Currently, InternalPriorityQueue.poll() is logged as a separate operation, without specifying the element that has been polled. On recovery, this recorded poll() is replayed.

However, this is not deterministic because the order of PQ elements with equal priorityis not specified. For example, TimerHeapInternalTimer only compares timestamps, which are often equal. This results in polling timers from queue in wrong order => dropping timers => and not firing timers.

 

ProcessingTimeWindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow fails with materialization enabled and using heap state backend (both in-memory and fs-based implementations).

 

Proposed solution is to replace poll with remove operation (which is based on equality).
 
cc: [~masteryhx], [~ym], [~yunta]"	FLINK	Resolved	3	1	8742	pull-request-available
13479477	Fix logging in DefaultCompletedCheckpointStore	See [https://github.com/apache/flink/pull/16582#discussion_r949214456]	FLINK	Resolved	4	4	8742	pull-request-available
13525410	PartiallyFinishedSourcesITCase hangs if a checkpoint fails	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b

This build ran into a timeout. Based on the stacktraces reported, it was either caused by [SnapshotMigrationTestBase.restoreAndExecute|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=13475]:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f23d800b800 nid=0x60cdd waiting on condition [0x00007f23e1c0d000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.test.checkpointing.utils.SnapshotMigrationTestBase.restoreAndExecute(SnapshotMigrationTestBase.java:382)
	at org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.testSnapshot(TypeSerializerSnapshotMigrationITCase.java:172)
	at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
[...]
{code}

or [PartiallyFinishedSourcesITCase.test|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10401]:
{code}
2023-02-20T07:13:05.6084711Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd35c00b800 nid=0x8c8a waiting on condition [0x00007fd363d0f000]
2023-02-20T07:13:05.6085149Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2023-02-20T07:13:05.6085487Z 	at java.lang.Thread.sleep(Native Method)
2023-02-20T07:13:05.6085925Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2023-02-20T07:13:05.6086512Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
2023-02-20T07:13:05.6087103Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
2023-02-20T07:13:05.6087730Z 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
2023-02-20T07:13:05.6088410Z 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
2023-02-20T07:13:05.6088957Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

Still, it sounds odd: Based on a code analysis it's quite unlikely that those two caused the issue. The former one has a 5 min timeout (see related code in [SnapshotMigrationTestBase:382|https://github.com/apache/flink/blob/release-1.15/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L382]). For the other one, we found it being not responsible in the past when some other concurrent test caused the issue (see FLINK-30261).

An investigation on where we lose the time for the timeout revealed that {{AdaptiveSchedulerITCase}} took 2980s to finish (see [build logs|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5265]).
{code}
2023-02-20T03:43:55.4546050Z Feb 20 03:43:55 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-02-20T03:43:58.0448506Z Feb 20 03:43:58 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
2023-02-20T04:33:38.6824634Z Feb 20 04:33:38 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,980.445 s - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13348970	Update CompletedCheckpointStore.shutdown() signature	"# remove unused postCleanup argument
 # add javadoc for checkpointsCleaner"	FLINK	Resolved	4	4	8742	pull-request-available
13306013	Memory threshold is ignored for channel state	"Config parameter state.backend.fs.memory-threshold is ignored for channel state. Causing each subtask to have a file per checkpoint. Regardless of the size of channel state (of this subtask).

This also causes slow cleanup and delays the next checkpoint.

 

The problem is that {{ChannelStateCheckpointWriter.finishWriteAndResult}} calls flush(); which actually flushes the data on disk.

 

From FSDataOutputStream.flush Javadoc:

A completed flush does not mean that the data is necessarily persistent. Data persistence can is only assumed after calls to close() or sync().

 

Possible solutions:

1. not to flush in {{ChannelStateCheckpointWriter.finishWriteAndResult (which can lead to data loss in a wrapping stream).}}

{{2. change }}{{FsCheckpointStateOutputStream.flush behavior}}

{{3. wrap }}{{FsCheckpointStateOutputStream to prevent flush}}{{}}{{}}"	FLINK	Closed	2	1	8742	pull-request-available
13426956	[Changelog] IllegalArgumentException thrown from FsStateChangelogWriter.truncate	"{code}
java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.flink.changelog.fs.FsStateChangelogWriter.truncate(FsStateChangelogWriter.java:278)
	at org.apache.flink.state.changelog.ChangelogKeyedStateBackend.updateChangelogSnapshotState(ChangelogKeyedStateBackend.java:702)
	at org.apache.flink.state.changelog.PeriodicMaterializationManager.lambda$null$2(PeriodicMaterializationManager.java:163)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:750){code}
"	FLINK	Resolved	3	1	8742	pull-request-available
13360271	Add local recovery support to adaptive scheduler	"local recovery means that, on a failure, we are able to re-use the state in a taskmanager, instead of loading it again from distributed storage (which means the scheduler needs to know where which state is located, and schedule tasks accordingly).

Adaptive Scheduler is currently not respecting the location of state, so failures require the re-loading of state from the distributed storage.

Adding this feature will allow us to enable the {{Local recovery and sticky scheduling end-to-end test}} for adaptive scheduler again."	FLINK	Resolved	3	4	8742	auto-deprioritized-major, auto-deprioritized-minor, auto-unassigned, pull-request-available
13305181	Can't subsume checkpoint with no channel state in UC mode	"When there are no channel state handles, the underlying FS stream is still created.

On discard it is not deleted because it's not referenced by any state handles."	FLINK	Closed	2	1	8742	pull-request-available
13311386	Tests are crashing with exit code 239	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3467&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8]
Kafka011ProducerExactlyOnceITCase
 
{code:java}
2020-06-15T03:24:28.4677649Z [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
2020-06-15T03:24:28.4692049Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-connector-kafka-0.11_2.11: There are test failures.
2020-06-15T03:24:28.4692585Z [ERROR] 
2020-06-15T03:24:28.4693170Z [ERROR] Please refer to /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire-reports for the individual test results.
2020-06-15T03:24:28.4693928Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-06-15T03:24:28.4694423Z [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4696762Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4698486Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4699066Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4699458Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4699960Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4700849Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4703760Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4705501Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4706297Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4706592Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4706895Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4707386Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2020-06-15T03:24:28.4708053Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2020-06-15T03:24:28.4708908Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2020-06-15T03:24:28.4709720Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-06-15T03:24:28.4710497Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-06-15T03:24:28.4711448Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-06-15T03:24:28.4712395Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-06-15T03:24:28.4712997Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-06-15T03:24:28.4713524Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-06-15T03:24:28.4714079Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-06-15T03:24:28.4714560Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-06-15T03:24:28.4715096Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-06-15T03:24:28.4715672Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-06-15T03:24:28.4716445Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-06-15T03:24:28.4717024Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-06-15T03:24:28.4717478Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-06-15T03:24:28.4717939Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-06-15T03:24:28.4718378Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-06-15T03:24:28.4718852Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-06-15T03:24:28.4719230Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-06-15T03:24:28.4719676Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-15T03:24:28.4720309Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-15T03:24:28.4720882Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-15T03:24:28.4721339Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-15T03:24:28.4721888Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-06-15T03:24:28.4722658Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-06-15T03:24:28.4723430Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-06-15T03:24:28.4724062Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-06-15T03:24:28.4724657Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4726770Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4728582Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4729202Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4729612Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4730247Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4730781Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-06-15T03:24:28.4731292Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-06-15T03:24:28.4731829Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2020-06-15T03:24:28.4732353Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2020-06-15T03:24:28.4732792Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-15T03:24:28.4733235Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-15T03:24:28.4733718Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-15T03:24:28.4734170Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2020-06-15T03:24:28.4734682Z [ERROR] -> [Help 1]
2020-06-15T03:24:28.4734859Z [ERROR] 
2020-06-15T03:24:28.4735312Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-15T03:24:28.4735927Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-15T03:24:28.4736439Z [ERROR] 
2020-06-15T03:24:28.4736952Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-15T03:24:28.4737706Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2020-06-15T03:24:28.4738167Z [ERROR] 
2020-06-15T03:24:28.4738553Z [ERROR] After correcting the problems, you can resume the build with the command
2020-06-15T03:24:28.4739663Z [ERROR]   mvn <goals> -rf :flink-connector-kafka-0.11_2.11
2020-06-15T03:24:29.0980029Z MVN exited with EXIT CODE: 1.
{code}

This could be a CI environment issue...
When did it start?"	FLINK	Closed	1	1	8742	pull-request-available, test-stability
13373051	Channel state iterator is accessed concurrently without proper synchronization	"ChannelStateWriter adds input/output data that is written by a dedicated thread.

The data is passed as CloseableIterator.

In some cases, iterator.close can be called from the task thread which can lead to double release of buffers."	FLINK	Closed	2	1	8742	pull-request-available
13362862	FLIP-151: Incremental snapshots for heap-based state backend	"Umbrella ticket for [https://cwiki.apache.org/confluence/display/FLINK/FLIP-151%3A+Incremental+snapshots+for+heap-based+state+backend]

 "	FLINK	Open	3	2	8742	auto-deprioritized-major, auto-unassigned, stale-assigned
13336925	PartitionRequestClientFactoryTest.testInterruptsNotCached fails with NullPointerException	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8517&view=logs&j=6e58d712-c5cc-52fb-0895-6ff7bd56c46b&t=f30a8e80-b2cf-535c-9952-7f521a4ae374

{code}
2020-10-23T13:25:12.0774554Z [ERROR] testInterruptsNotCached(org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest)  Time elapsed: 0.762 s  <<< ERROR!
2020-10-23T13:25:12.0775695Z java.io.IOException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0776455Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:95)
2020-10-23T13:25:12.0777038Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactoryTest.testInterruptsNotCached(PartitionRequestClientFactoryTest.java:72)
2020-10-23T13:25:12.0777465Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-23T13:25:12.0777815Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-23T13:25:12.0778221Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-23T13:25:12.0778581Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-23T13:25:12.0778921Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-23T13:25:12.0779331Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-23T13:25:12.0779733Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-23T13:25:12.0780117Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-23T13:25:12.0780484Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-23T13:25:12.0780851Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-23T13:25:12.0781236Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-23T13:25:12.0781600Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-23T13:25:12.0781937Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-23T13:25:12.0782431Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-23T13:25:12.0782877Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-23T13:25:12.0783223Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-23T13:25:12.0783541Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-23T13:25:12.0783905Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-23T13:25:12.0784315Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-23T13:25:12.0784718Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-23T13:25:12.0785125Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-23T13:25:12.0785552Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-23T13:25:12.0785980Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-23T13:25:12.0786379Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-23T13:25:12.0786763Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-23T13:25:12.0787922Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0788575Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-10-23T13:25:12.0788954Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-10-23T13:25:12.0789431Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:88)
2020-10-23T13:25:12.0789808Z 	... 26 more
2020-10-23T13:25:12.0790546Z Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '934dfa03c743/172.18.0.2:8080' has failed. This might indicate that the remote task manager has been lost.
2020-10-23T13:25:12.0791396Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:134)
2020-10-23T13:25:12.0791959Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:111)
2020-10-23T13:25:12.0792732Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:77)
2020-10-23T13:25:12.0793118Z 	... 26 more
2020-10-23T13:25:12.0793342Z Caused by: java.lang.NullPointerException
2020-10-23T13:25:12.0793681Z 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:61)
2020-10-23T13:25:12.0794319Z 	at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.<init>(NettyPartitionRequestClient.java:73)
2020-10-23T13:25:12.0794854Z 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:126)
{code}
"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13381494	ChangelogStateBackend tests use nested backend on recovery	"For example, ChangelogDelegateMemoryStateBackendTest overrides createKeyedBackend() but does NOT getStateBackend(). The latter is being used on recovery.

cc: [~ym]]"	FLINK	Closed	3	7	8742	pull-request-available
13324992	Performance regression 2020-08-27 in globalWindow benchmark	"[http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&env=2]

[http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=tumblingWindow&env=2&revs=200&equid=off&quarts=on&extr=on] 

The results started to decrease 2 days before decomissioning of an old jenkins node.

The other tests, however, were stable.

 

cc: [~pnowojski]"	FLINK	Closed	3	1	8742	pull-request-available
13395473	DeleteExecutor NPE	"Encountered a situation where I get an NPE from JDBCUpsertOutputFormat.

This occurs when jdbc disconnected and try to reconnect.

I need to write data to mysql in upsert way in sql, So it must group by unique key and the JdbcBatchingOutputFormat of Jdbc sink would use TableJdbcUpsertOutputFormat.

 

Jdbc would disconnected when The data interval exceeds the set connection time.I see that when jdbc reconnect , only JdbcBatchingOutputFormat#jdbcStatementExecutor(insert) would prepareStatements but TableJdbcUpsertOutputFormat#deleteExecutor would not prepareStatements so that come up NPE.

if in JdbcBatchingOutputFormat have a protected function to reset PrepareStatement and TableJdbcUpsertOutputFormat override this function to reset deleteExecutor, it would work well.

prepareStatements "	FLINK	Resolved	2	1	8742	pull-request-available
13320514	FlinkKinesisProducer.backpressureLatch should be volatile	"(confirm first)

 

cc: [~rmetzger]"	FLINK	Closed	4	1	8742	pull-request-available
13585024	NPE in BlobServer / shutdownHook	"In constructor, BlobServer registers a shutdown hook to close the socket.

Later in constructor, BlobServer creates this socket (and makes sure it's not null).

 

But if the shutdown hook gets invoked before opening the socket, NPE will be thrown:
{code:java}
  12:02:49,983 [PermanentBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
  12:02:49,985 [BlobServer shutdown hook] ERROR org.apache.flink.runtime.blob.BlobServer                     [] - Error during shutdown of BlobServer via JVM shutdown hook.
  java.lang.NullPointerException: null
          at org.apache.flink.runtime.blob.BlobServer.close(BlobServer.java:358) ~[classes/:?]
          at org.apache.flink.util.ShutdownHookUtil.lambda$addShutdownHook$0(ShutdownHookUtil.java:39) ~[flink-core-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
          at java.lang.Thread.run(Thread.java:829) [?:?]
 {code}"	FLINK	Closed	3	1	8742	pull-request-available
13382347	JdbcExactlyOnceSinkE2eTest.testInsert hangs on azure	https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=16658	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13387888	Changelog backend not always updates delegating functions	"(currently disabled in tests, so doesn't affect build stability)

E.g. org.apache.flink.table.planner.runtime.stream.table.GroupWindowITCase, or other tests from the same package.

{code}
 2021-07-03T20:30:28.0384912Z Jul 03 20:30:28 Caused by: java.lang.NullPointerException
*2021-07-03T20:30:28.0385566Z Jul 03 20:30:28    at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:59)
*2021-07-03T20:30:28.0386330Z Jul 03 20:30:28    at org.apache.flink.state.changelog.restore.FunctionDelegationHelper$DelegatingReduceFunction.redu*ce(FunctionDelegationHelper.java:138)
*2021-07-03T20:30:28.0387147Z Jul 03 20:30:28    at org.apache.flink.contrib.streaming.state.RocksDBReducingState.add(RocksDBReducingState.java:95)
*2021-07-03T20:30:28.0387892Z Jul 03 20:30:28    at org.apache.flink.state.changelog.ChangelogReducingState.add(ChangelogReducingState.java:82)
*2021-07-03T20:30:28.0388677Z Jul 03 20:30:28    at org.apache.flink.table.runtime.operators.window.triggers.ElementTriggers$CountElement.onElement*(ElementTriggers.java:124)
*2021-07-03T20:30:28.0389503Z Jul 03 20:30:28    at org.apache.flink.table.runtime.operators.window.WindowOperator$TriggerContext.onElement(WindowO*perator.java:572)
*2021-07-03T20:30:28.0390296Z Jul 03 20:30:28    at org.apache.flink.table.runtime.operators.window.WindowOperator.processElement(WindowOperator.ja*va:379)
*2021-07-03T20:30:28.0391107Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(*OneInputStreamTask.java:228)
*2021-07-03T20:30:28.0391936Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStr*eamTaskNetworkInput.java:134)
*2021-07-03T20:30:28.0392776Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTas*kNetworkInput.java:105)
*2021-07-03T20:30:28.0393929Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProces*sor.java:66)
*2021-07-03T20:30:28.0394611Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:428)
*2021-07-03T20:30:28.0395288Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcess*or.java:204)
*2021-07-03T20:30:28.0395966Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:691)
*2021-07-03T20:30:28.0396892Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:646)
*2021-07-03T20:30:28.0397601Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:657)
*2021-07-03T20:30:28.0398398Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:630)
*2021-07-03T20:30:28.0399051Z Jul 03 20:30:28    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
*2021-07-03T20:30:28.0399669Z Jul 03 20:30:28    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
 2021-07-03T20:30:28.0400238Z Jul 03 20:30:28    at java.lang.Thread.run(Thread.java:748)

{code}

The reason is missing functionDelegationHelper.addOrUpdate() call in  
ChangelogKeyedStateBackend.getPartitionedState()."	FLINK	Resolved	3	1	8742	pull-request-available
13428022	SavepointFormatITCase fails with ChangelogStateBackend	"The test vaildates corrects types of state handles created by savepoint. For NATIVE savepoints, it expects IncrementalRemoteKeyedStateHandle and KeyGroupsStateHandle.

However, with changelog those will be wrapped into ChangelogStateBackendHandle and the test fails.

It can be refactored to account for changelog."	FLINK	Resolved	3	1	8742	pull-request-available
13407762	Prevent JM from discarding state on checkpoint abortion	"================================================================================
MOTIVATION
When a checkpoint is aborted, JM discards any state that was sent to it and wasn't used in other checkpoints. This forces incremental state backends to wait for confirmation from JM before re-using this state. For changelog backend this is even more critical.

One approach proposed was to make backends/TMs responsible for their state, until it's not shared with other TMs, i.e. until rescaling (private/shared state ownership track: FLINK-23342 and more).
However, that approach is quite invasive.

An alternative solution would be:
1. SharedStateRegistry remembers the latest checkpoint for each shared state (instead of usage count currently)
2. CompletedCheckpointStore notifies it about the lowest valid checkpoint (on subsumption)
3. SharedStateRegistry then discards any state associated with the lower (subsumed/aborted) checkpoints
So the aborted checkpoint can only be discarded after some subsequent successful checkpoint (which can mark state as used).

Mostly JM code is changed. IncrementalRemoteKeyedStateHandle.discard needs to be adjusted.
Backends don't re-upload state.

================================================================================
IMPLEMENTATION CONSIDERATIONS

On subsumption, JM needs to find all the unused state and discard it.
This can either be done by
1) simply traversing all entries; or by 
2) maintaining a set of entries per checkpoint (e.g. SortedMap<Long, Set<K>>). This allows to skip unnecessary traversal at the cost of higher memory usage

In both cases:
- each entry stores last checkpoint ID it was used in (long)
- key is hashed (even with plain traversal, map.entrySet.iterator.remove() computes hash internally)

Because of the need to maintain secondary sets, (2) isn't asymptotically better than (1), and is likely worse in practice and requires more memory (see discussion in the comments). So approach (1) seems reasonable.

================================================================================
CORNER CASES

The following cases shouldn't pose any difficulties:
1. Recovery, re-scaling, and state used by not all or by no tasks - we still register all states on recovery even after FLINK-22483/FLINK-24086
2. Cross-task state sharing - not an issue as long as everything is managed by JM
3. Dependencies between SharedStateRegistry and CompletedCheckpointStore - simple after FLINK-24086
4. Multiple concurrent checkpoints (below)

Consider the following case:
(nr. concurrent checkpoints > 1)
1. checkpoint 1 starts, TM reports that it uses file f1; checkpoint 1 gets aborted - f1 is now tracked
2. savepoint 2 starts, it *will* use f1
3. checkpoint 3 starts and finishes; it does NOT use file f1

When a checkpoint finishes, all pending checkpoints before it are aborted - but not savepoints.
Savepoints currently are NOT incremental. And in the future, incremental savepoints shouldn't share any artifacts with checkpionts.

The following should be kept in mind:
1. On job cancellation, state of aborted checkpoints should be cleaned up explicitly
2. Savepoints should be ignored and not change CheckpointStore.lowestCheckpointID
3. In case of JM failover, there might be more state left undeleted (see follow-up FLINK-24852 and/or comments)
4. To handle JM leadership change,  backends should NOT send PlaceholderStreamStateHandles unless the checkpoint was confirmed (see comments)
5. To handle TM failover after an incomplete checkpoint, JM should replace old state handle with a new one in case of collision (test that old state isn't included into a completed checkpoint) (see comments)

================================================================================
USER IMPACT

For the end users, this change might render as a delay in discarding state of aborted checkpoints and in slight increase of undeleted state in case of failures; which seems acceptable.

After updating backends to not re-upload state, checkpointing time should be reduced and less IO will be used (in cases when notifications are delayed or new JM is elected)."	FLINK	Resolved	3	7	8742	pull-request-available
13419872	Changlog materialization with incremental checkpoint cannot work well in local tests	"Currently, changelog materialization would call RocksDB state backend's snapshot method to generate {{IncrementalRemoteKeyedStateHandle}} as ChangelogStateBackendHandleImpl's materialized artifacts. And before next materialization, it will always report the same {{IncrementalRemoteKeyedStateHandle}} as before.

For local tests, TM would report the {{IncrementalRemoteKeyedStateHandle}} to JM via local {{LocalRpcInvocation}}. However, as {{LocalRpcInvocation}} would not de/serialize message, which leads once we register the {{IncrementalRemoteKeyedStateHandle}} on JM side, it will also add a {{sharedStateRegistry}} to the one located on TM side. For the 2nd checkpoint, TM would reported same {{IncrementalRemoteKeyedStateHandle}} with  {{sharedStateRegistry}} to JM. And it will then throw exception as it already contains a {{sharedStateRegistry}}:

IncrementalRemoteKeyedStateHandle
{code:java}
public void registerSharedStates(SharedStateRegistry stateRegistry, long checkpointID) {
       Preconditions.checkState(
                sharedStateRegistry != stateRegistry,
                ""The state handle has already registered its shared states to the given registry."");

}
{code}

This bug would go in distribution environment as {{IncrementalRemoteKeyedStateHandle}} would be serialized and {{sharedStateRegistry}} is tagged as {{transient}}."	FLINK	Resolved	3	1	8742	pull-request-available
13423852	Include changelog jars into distribution	"Add changelog jars to dist/opt folder:
- flink-dstl-dfs - so users can add it to plugins/ easily (plugin, cluster level)
- flink-statebackend-changelog - so that it can be added to lib/ if needed (not plugin, cluster or job-level)

Update docs if done after FLINK-25024.

cc: [~chesnay]"	FLINK	Resolved	3	7	8742	pull-request-available
13510463	TaskManagerWideRocksDbMemorySharingITCase.testBlockCache failed	"{{TaskManagerWideRocksDbMemorySharingITCase.testBlockCache}} failed in this build: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43763&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9836]
{code:java}
Dec 06 16:33:59 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.926 s <<< FAILURE! - in org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase
Dec 06 16:33:59 [ERROR] org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase.testBlockCache  Time elapsed: 12.907 s  <<< FAILURE!
Dec 06 16:33:59 java.lang.AssertionError: 
Dec 06 16:33:59 Block cache usage reported by different tasks varies too much: DoubleSummaryStatistics{count=20, sum=3783523840.000000, min=189045056.000000, average=189176192.000000, max=189569600.000000}
Dec 06 16:33:59 That likely mean that they use different cache objects expected:<1.895696E8> but was:<1.89045056E8>
Dec 06 16:33:59 	at org.junit.Assert.fail(Assert.java:89)
Dec 06 16:33:59 	at org.junit.Assert.failNotEquals(Assert.java:835)
Dec 06 16:33:59 	at org.junit.Assert.assertEquals(Assert.java:555)
Dec 06 16:33:59 	at org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase.testBlockCache(TaskManagerWideRocksDbMemorySharingITCase.java:133)
[...] {code}"	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13574402	"JobIDLoggingITCase fails because of ""checkpoint confirmation for unknown task"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58640&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8735]
{code:java}
Mar 30 03:46:07 03:46:07.807 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.147 s <<< FAILURE! -- in org.apache.flink.test.misc.JobIDLoggingITCase
Mar 30 03:46:07 03:46:07.807 [ERROR] org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(ClusterClient) -- Time elapsed: 2.301 s <<< FAILURE!
Mar 30 03:46:07 java.lang.AssertionError: 
Mar 30 03:46:07 [too many events without Job ID logged by org.apache.flink.runtime.taskexecutor.TaskExecutor] 
Mar 30 03:46:07 Expecting empty but was: [Logger=org.apache.flink.runtime.taskexecutor.TaskExecutor Level=DEBUG Message=TaskManager received a checkpoint confirmation for unknown task b45d406844d494592784a88e47d201e2_cbc357ccb763df2852fee8c4fc7d55f2_0_0.]
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:264)
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:149)
Mar 30 03:46:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 30 03:46:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
[https://github.com/apache/flink/actions/runs/8502821551/job/23287730632#step:10:8131]

[https://github.com/apache/flink/actions/runs/8507870399/job/23300810619#step:10:8086]

 

 "	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13472158	Empty checkpoint folders not deleted on job cancellation if their shared state is still in use	"After FLINK-25872, SharedStateRegistry registers all state handles, including private ones.
Once the state isn't use AND the checkpoint is subsumed, it will actually be discarded.

This is done to prevent premature deletion when recovering in CLAIM mode:
1. RocksDB native savepoint folder (shared state is stored in chk-xx folder so it might fail the deletion)
2. Initial non-changelog checkpoint when switching to changelog-based checkpoints (private state of the initial checkpoint might be included into later checkpoints and its deletion would invalidate them)

Additionally, checkpoint folders are not deleted for a longer time which might be confusing.
In case of a crash, more folders will remain.

cc: [~Yanfei Lei], [~ym]"	FLINK	Closed	3	1	8742	pull-request-available
13362864	On heap state restore, skip key groups from other state backends	"In new incremental mode, heap backend wraps KeyGroupsStateHandle
into IncrementalRemoteKeyedStateHandle. The latter don't compute
intersection because it is not directly aware of the offsets. So it just
returns a full keyrange if there is SOME intersection.
On recovery, unused keyGroups are filtered out by RocksDB state backend.
With this change, Heap state backend does the same."	FLINK	Resolved	3	7	8742	pull-request-available
13337645	Incompatible semantics of channelIndex in UnionInputGate.resumeConsumption and its clients	"Given channelIndex only, UnionInputGate has to guess which wrapped input
gate this channel belongs to. For that, UnionInputGate expects channel
index with an inputGate offset. This contradicts with the contract of
other resumeConsumption() implementations.
UnionInputGate.resumeConsumption isn't used currently but is planned to
be used in FLINK-19856."	FLINK	Resolved	4	1	8742	pull-request-available
13420752	If enabled changelog, RocksDB incremental checkpoint would always be full	"Once changelog is enabled, RocksDB incremental checkpoint would only be executed during materialization. During this phase, it will leverage the {{materization id}} as the checkpoint id for RocksDB state backend's snapshot method.

However, current incremental checkpoint mechanism heavily depends on the checkpoint id. And {{SortedMap<Long, Set<StateHandleID>> uploadedStateIDs}} with checkpoint id as the key within {{RocksIncrementalSnapshotStrategy}} is the kernel for incremental checkpoint. Once we notify checkpoint complete of previous checkpoint, it will then remove the uploaded stateIds of that checkpoint, leading to we cannot get proper checkpoint information on the next RocksDBKeyedStateBackend#snapshot. That is to say, we will always upload all RocksDB artifacts."	FLINK	Resolved	3	1	8742	pull-request-available
13355112	Buffer pool is destroyed error when outputting data over a timer after cancellation.	"A [user reported|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/What-causes-a-buffer-pool-exception-How-can-I-mitigate-it-td40959.html] the issue and provided some taskmanager log with the following relevant lines:
{noformat}
2021-01-26 04:37:43,280 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Attempting to cancel task forward fill -> (Sink: tag db sink, Sink: back fill db sink, Sink: min max step db sink) (2/2) (8c1f256176fb89f112c27883350a02bc).
2021-01-26 04:37:43,280 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - forward fill -> (Sink: tag db sink, Sink: back fill db sink, Sink: min max step db sink) (2/2) (8c1f256176fb89f112c27883350a02bc) switched from RUNNING to CANCELING.
2021-01-26 04:37:43,280 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Triggering cancellation of task code forward fill -> (Sink: tag db sink, Sink: back fill db sink, Sink: min max step db sink) (2/2) (8c1f256176fb89f112c27883350a02bc).
2021-01-26 04:37:43,282 ERROR xxxxxx.pipeline.stream.functions.process.ForwardFillKeyedProcessFunction [] - Error in timer.
java.lang.RuntimeException: Buffer pool is destroyed.
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:110) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:89) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:45) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$BroadcastingOutputCollector.collect(OperatorChain.java:787) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$BroadcastingOutputCollector.collect(OperatorChain.java:740) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at xxxxxx.pipeline.stream.functions.process.ForwardFillKeyedProcessFunction.collect(ForwardFillKeyedProcessFunction.java:452) ~[develop-17e9fd0e.jar:?]
	at xxxxxx.pipeline.stream.functions.process.ForwardFillKeyedProcessFunction.onTimer(ForwardFillKeyedProcessFunction.java:277) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.invokeUserFunction(KeyedProcessOperator.java:94) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.onProcessingTime(KeyedProcessOperator.java:78) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:260) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1181) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$13(StreamTask.java:1172) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:270) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxStep(MailboxProcessor.java:190) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:181) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:558) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:530) [flink-dist_2.12-1.11.0.jar:1.11.0]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721) [develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546) [develop-17e9fd0e.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_252]
Caused by: java.lang.IllegalStateException: Buffer pool is destroyed.
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentFromGlobal(LocalBufferPool.java:339) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegment(LocalBufferPool.java:309) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:290) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:266) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.partition.ResultPartition.getBufferBuilder(ResultPartition.java:213) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.requestNewBufferBuilder(RecordWriter.java:294) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.requestNewBufferBuilder(ChannelSelectorRecordWriter.java:103) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.copyFromSerializerToTargetChannel(RecordWriter.java:149) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:120) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:60) ~[develop-17e9fd0e.jar:?]
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:107) ~[flink-dist_2.12-1.11.0.jar:1.11.0]
	... 24 more
{noformat}
 "	FLINK	Resolved	3	1	8742	pull-request-available
13428604	SavepointFormatITCase fails on azure	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31474&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=13116]

{code}
[ERROR] org.apache.flink.test.checkpointing.SavepointFormatITCase.testTriggerSavepointAndResumeWithFileBasedCheckpointsAndRelocateBasePath(SavepointFormatType, StateBackendConfig)[2]  Time elapsed: 14.209 s  <<< ERROR!
java.util.concurrent.ExecutionException: java.io.IOException: Unknown implementation of StreamStateHa ndle: class org.apache.flink.runtime.state.PlaceholderStreamStateHandle
   at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
   at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
   at org.apache.flink.test.checkpointing.SavepointFormatITCase.submitJobAndTakeSavepoint(SavepointFormatITCase.java:328)
   at org.apache.flink.test.checkpointing.SavepointFormatITCase.testTriggerSavepointAndResumeWithFileBasedCheckpointsAndRelocateBasePath(SavepointFormatITCase.java:248)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
   at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(Invo cationInterceptorChain.java:131)
   at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
   at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.ja va:140)
   at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtensio n.java:92)
   at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMet hod$0(ExecutableInvoker.java:115)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105 )
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(Inv ocationInterceptorChain.java:106)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChai n.java:64)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationIntercep torChain.java:45)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain .java:37)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
   at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMeth odTestDescriptor.java:214)
   at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.ja
{code}"	FLINK	Resolved	1	1	8742	pull-request-available
13357979	Implement incremental checkpointing and recovery using state changelog	"* Including state handles serialization (MetadataV2V3SerializerBase)
* Not including  materialization"	FLINK	Closed	3	7	8742	auto-unassigned, pull-request-available
13436934	Allow HeapBackend creation customization	"Incremental Heap State Backend needs to customize
SnapshotStrategy and RestoreOperation and their components.

"	FLINK	In Progress	3	7	8742	pull-request-available, stale-assigned
13337646	Add EndOfChannelRecovery rescaling epoch	"This event would allow to tear down ""virtual channels""     This event would allow to tear down ""virtual channels""     used to read channel state on recovery with unaligned checkpoints and     rescaling."	FLINK	Resolved	3	2	8742	pull-request-available
13587360	Skip distributing maxAllowedWatermark if there are no subtasks	On JM, `SourceCoordinator.announceCombinedWatermark` executes unnecessary if there are no subtasks to distribute maxAllowedWatermark. This involves Heap and ConcurrentHashMap accesses and lots of logging.	FLINK	Closed	3	4	8742	pull-request-available
13395884	Race condition while cancelling task during initialization	"While debugging the recent failures in FLINK-22889, I see that sometimes the operator chain is not closed if the task is cancelled while it's being initialized.

 

The reason is that on restore(), cleanUpInvoke() is only closed if there was an exception, including CancelTaskException.

The latter is only thrown if StreamTask.canceled is set, i.e. TaskCanceler has called StreamTask.cancel().

 

So if StreamTask is cancelled in between restore and normal invoke then it may not close the operator chain and not do other cleanup.

 

One solution is to make StreamTask.cleanup visible to and called from Task.

 

cc: [~akalashnikov], [~pnowojski]"	FLINK	Closed	1	1	8742	pull-request-available
13352479	Checkpoint cleanup can kill JobMaster	"A user reported that cancelling a job can lead to an uncaught exception which kills the {{JobMaster}}. The problem seems to be that the {{CheckpointsCleaner}} might trigger {{CheckpointCoordinator}} actions after the job has reached a terminal state and, thus, is shut down. Apparently, we do not properly manage the lifecycles of {{CheckpointCoordinator}} and checkpoint post clean up actions.

The uncaught exception is 

{code}
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@41554407 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5d0ec6f7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 25977] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063
 at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830
 at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326
 at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533
 at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622
 at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668
 at org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter.execute(ScheduledExecutorServiceAdapter.java:62
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.scheduleTriggerRequest(CheckpointCoordinator.java:1152
 at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.lambda$cleanCheckpoint$0(CheckpointsCleaner.java:58
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624
 at java.lang.Thread.run(Thread.java:748 undefined)
{code}

cc [~roman_khachatryan].

https://lists.apache.org/thread.html/r75901008d88163560aabb8ab6fc458cd9d4f19076e03ae85e00f9a3a%40%3Cuser.flink.apache.org%3E"	FLINK	Closed	2	1	8742	pull-request-available
13340030	Race conditions in InputChannel.ChannelStatePersister	"In InputChannel.ChannelStatePersister, stopPersisting() and checkForBarrier() always update pendingCheckpointBarrierId, potentially overwriting newer id (or BARRIER_RECEIVED value) with an old one.


For stopPersisting(), consider a case:
 # Two consecutive UC barriers arrive at the same channel (1st being stale at some point)
 # In RemoteInputChannel.onBuffer, netty thread updates pendingCheckpointBarrierId to BARRIER_RECEIVED
 # Task thread processes the 1st barrier and triggers a checkpoint
Task thread processes the 2nd barrier and aborts 1st checkpoint, calling stopPersisting() from UC controller and setting pendingCheckpointBarrierId to CHECKPOINT_COMPLETED
 # Task thread starts 2nd checkpoint and calls startPersisting() setting pendingCheckpointBarrierId to 2
 # now new buffers have a chance to be included in the 2nd checkpoint (though they belong to the next one)

 

For pendingCheckpointBarrierId(), consider an input gate with two channels A and B and two barriers 1 and 2:
 # Channel A receives both barriers, channel B receives nothing yet
 # Task thread processes both barriers on A, eventually triggering 2nd checkpoint
 # Channel A state is now BARRIER_RECEIVED, channel B - pending (with id=2)
 # Channel B receives the 1st barrier and becomes BARRIER_RECEIVED
 # No buffers in B between barriers 1 and 2 will be included in the checkpoint 
 # Channel B receives the 2nd barrier which will eventually conclude the checkpoint

 

I see a solution in doing an action only if passed checkpointId >= pendingCheckpointId. For that, a separate field will be needed to hold the status (RECEIVED/COMPLETED/PENDING). The class isn't thread-safe so it shouldn't be a problem.
 "	FLINK	Resolved	2	1	8742	pull-request-available
13353899	Add E2E/ITCase test for checkpoints after tasks finished	"We should write an E2e test for the feature. The corner cases should include:
* must check for committing side effects (notifyCheckpointComplete() was called)
* BroadcastState
* both non keyed followed by keyed exchanges? 
* aligned/unaligned checkpoints? (with unaligned checkpoint keyed operators/subtasks can be partially finished, which is very very rare in aligned checkpoints) - maybe it’s good enough to test only unaligned checkpoints."	FLINK	Closed	3	7	8742	pull-request-available
13387895	"Changelog backend creates ""raw materialized"" savepoint but expects ""normal"" snapshot on recovery"	"Savepoint consist of ""raw"" keyed handles from the underlying backend:
{code}
    public SavepointResources<K> savepoint() throws Exception {
        return keyedStateBackend.savepoint();
    }
{code}

On recovery, ChangelogStateBackendHandles are expected.

This fails e.g. SavepointWriterITCase if enabled."	FLINK	Resolved	3	1	8742	pull-request-available
13393606	BatchingStateChangeUploaderTest.testDelay fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21518&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22202

{code}
Aug 04 16:31:14 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.616 s <<< FAILURE! - in org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest
Aug 04 16:31:14 [ERROR] testDelay(org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest)  Time elapsed: 0.198 s  <<< FAILURE!
Aug 04 16:31:14 java.lang.AssertionError: expected:<[logId=96102f85-c8a5-454a-9651-14bc1c6b0858, sequenceNumber=0, changes=[keyGroup=0, dataSize=4]]> but was:<[]>
Aug 04 16:31:14 	at org.junit.Assert.fail(Assert.java:89)
Aug 04 16:31:14 	at org.junit.Assert.failNotEquals(Assert.java:835)
Aug 04 16:31:14 	at org.junit.Assert.assertEquals(Assert.java:120)
Aug 04 16:31:14 	at org.junit.Assert.assertEquals(Assert.java:146)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.lambda$testDelay$4(BatchingStateChangeUploaderTest.java:105)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.withStore(BatchingStateChangeUploaderTest.java:210)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.testDelay(BatchingStateChangeUploaderTest.java:97)
Aug 04 16:31:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 04 16:31:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 04 16:31:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 04 16:31:14 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 04 16:31:14 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 04 16:31:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 04 16:31:14 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 04 16:31:14 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13521245	RocksDB Memory Management end-to-end test failed due to unexpected exception	"We see a test instability with {{RocksDB Memory Management end-to-end test}}. The test failed because an exception was detected in the logs:
{code}
2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...
2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.
2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...
2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:
[...]
2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...
2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.
2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 
2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

The only exception being reported in the Flink logs is due to a warning:
{code}
2023-01-25 02:47:38,242 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5117"	FLINK	Closed	2	1	8742	pull-request-available, test-stability
13398480	Do not re-register SharedStateRegistry to reduce the recovery time of the job	"At present, we only recover the {{CompletedCheckpointStore}} when the {{JobManager}} starts, so it seems that we do not need to re-register the {{SharedStateRegistry}} when the task restarts.

The reason for this issue is that in our production environment, we discard part of the data and state to only restart the failed task, but found that it may take several seconds to register the {{SharedStateRegistry}} (thousands of tasks and dozens of TB states). When there are a large number of task failures at the same time, this may take several minutes (number of tasks * several seconds).

Therefore, if the {{SharedStateRegistry}} can be reused, the time for task recovery can be reduced."	FLINK	Resolved	3	4	8742	pull-request-available
13325470	Split Reader eats chained periodic watermarks	"Attempting to generate watermarks chained to the Split Reader / ContinuousFileReaderOperator, as in
{code:java}
SingleOutputStreamOperator<Event> results = env
  .readTextFile(...)
  .map(...)
  .assignTimestampsAndWatermarks(bounded)
  .keyBy(...)
  .process(...);{code}
leads to the Watermarks failing to be produced. Breaking the chain, via {{disableOperatorChaining()}} or a {{rebalance}}, works around the bug. Using punctuated watermarks also avoids the issue.

Looking at this in the debugger reveals that timer service is being prematurely quiesced.

In many respects this is FLINK-7666 brought back to life.

The problem is not present in 1.9.3.

There's a minimal reproducible example in [https://github.com/alpinegizmo/flink-question-001/tree/bug]."	FLINK	Resolved	1	1	8742	pull-request-available
13333678	Add StateChangelog interface and its in-memory implementation	"StateChangelog is a component proposed in FLIP-158 to store state changes for incremental checkpoints.

 "	FLINK	Closed	3	7	8742	pull-request-available
13392694	Randomize periodic materialisation interval in tests	"FLINK-21448 adds the capability of test randomization.

It's already used for the changelog backend (FLINK-23279).

 

FLINK-21357 adds periodic materialization; the default interval is 10m which is likely too high for tests (so materialization isn't triggered).

This interval should be randomized/reduced;

 

Depends on FLINK-23170.

 "	FLINK	Resolved	3	7	8742	pull-request-available
13433368	BatchingStateChangeUploadSchedulerTest.testRetryOnTimeout fails on azure	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32896&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24724]

{code}
[ERROR] Tests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s <<< FAILURE! - in org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest
[ERROR] org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.testRetryOnTimeout  Time elapsed: 0.042 s  <<< FAILURE!
java.lang.AssertionError: expected:<[0]> but was:<[]>
   at org.junit.Assert.fail(Assert.java:89)
   at org.junit.Assert.failNotEquals(Assert.java:835)
   at org.junit.Assert.assertEquals(Assert.java:120)
   at org.junit.Assert.assertEquals(Assert.java:146)
   at org.apache.flink.changelog.fs.BatchingStateChangeUploadSchedulerTest.testRetryOnTimeout(BatchingStateChangeUploadSchedulerTest.java:240)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
   at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
   at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
   at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
   at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
   at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
   at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
   at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
   at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
   at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
   at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
   at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
   at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
   at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
   at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrato r.java:107)
   at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrato r.java:88)
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13427313	[Changelog] PriorityQueue elements recovered out-of-order	"StateChangeFormat is the class responsible for writing out changelog data.
Each chunk of data is sorted by: logId -> sequenceNumber -> keyGroup.
Sorting by sequenceNumber preserves temporal order.
Sorting by keyGroup a) puts metadata (group -1) at the beginning and b) allows to write KG only once.

However, the assumption that the order of changes across groups currently doesn't hold: poll operation of InternalPriorityQueue may affect any group (the smaller item across groups so far will be polled).

This results in wrong processing time timers being removed on recovery in ProcessingTimeWindowCheckpointingITCase#testAggregatingSlidingProcessingTimeWindow

One way to solve this probelm is to simply disable KG-sorting and grouping (only output metadata at the beginning). 
The other one is to associate polled element with the correct key group while logging changes.

Both ways should work with re-scaling.

cc: [~masteryhx], [~ym], [~yunta]"	FLINK	Closed	3	1	8742	pull-request-available
13277326	Should wait for the end of the source thread during the Task cancellation	"     In the new mailBox model, SourceStreamTask starts a source thread to run user methods, and the current execution thread will block on mailbox.takeMail (). When a task cancels, the TaskCanceler thread will cancel the task and interrupt the execution thread. Therefore, the execution thread of SourceStreamTask will throw InterruptedException, then cancel the task again, and throw an exception.
{code:java}
//代码占位符
@Override
protected void performDefaultAction(ActionContext context) throws Exception {
   // Against the usual contract of this method, this implementation is not step-wise but blocking instead for
   // compatibility reasons with the current source interface (source functions run as a loop, not in steps).
   sourceThread.start();

   // We run an alternative mailbox loop that does not involve default actions and synchronizes around actions.
   try {
      runAlternativeMailboxLoop();
   } catch (Exception mailboxEx) {
      // We cancel the source function if some runtime exception escaped the mailbox.
      if (!isCanceled()) {
         cancelTask();
      }
      throw mailboxEx;
   }

   sourceThread.join();
   if (!isFinished) {
      sourceThread.checkThrowSourceExecutionException();
   }

   context.allActionsCompleted();
}
{code}
   When all tasks of this TaskExecutor are canceled, the blob file will be cleaned up. But the real source thread is not finished at this time, which will cause a ClassNotFoundException when loading a new class. In this case, the source thread may not be able to properly clean up and release resources (such as closing child threads, cleaning up local files, etc.). Therefore, I think we should mark this task canceled or finished after the execution of the source thread is completed."	FLINK	Resolved	3	1	8742	pull-request-available
13355694	Resuming Savepoint (rocks, scale up, rocks timers) end-to-end test	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12664&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529]

 

Jan 29 15:20:55 [FAIL] 'Resuming Savepoint (rocks, scale up, rocks timers) end-to-end test' failed after 0 minutes and 37 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files"	FLINK	Closed	3	1	8742	test-stability
13285302	Performance regression in ContinuousFileReaderOperator	"After switching CFRO to a single-threaded execution model performance regression was expected to be about 15-20% (benchmarked in November).

But after merging to master it turned out to be about 50%.

  

One reason is that the chaining strategy isn't set by default in CFRO factory.

Without that even reading and outputting all records of a split in a single mail action doesn't reverse the regression (only about half).

However,  with strategy set AND batching enabled fixes the regression (starting from batch size 6).

Though batching can't be used in practice because it can significantly delay checkpointing.

 

Another approach would be to process one record and the repeat until defaultMailboxActionAvailable OR haveNewMail.

This reverses regression and even improves the performance by about 50% compared to the old version.

 

The final solution could also be FLIP-27.

 

Other things tried (didn't help):
 * CFRO rework without subsequent commits (removing checkpoint lock)
 * different batch sizes, including the whole split, without chaining strategy fixed - partial improvement only
 * disabling close
 * disabling checkpointing
 * disabling output (serialization)
 * using LinkedList instead of PriorityQueue

 "	FLINK	Closed	1	1	8742	pull-request-available
13408646	Channel state writer would fail the task directly if meeting exception previously	"Currently, if channel state writer come across exception when closing a file, such as meet exception during {{SubtaskCheckpointCoordinatorImpl#cancelAsyncCheckpointRunnable}}, it will exit the loop. However, in the following {{channelStateWriter#abort}} it will throw exception directly：


{code:java}
switched from RUNNING to FAILED with failure cause: java.io.IOException: java.lang.RuntimeException: unable to send request to worker
	at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.checkError(InputChannel.java:228)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkPartitionRequestQueueInitialized(RemoteInputChannel.java:735)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:204)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:651)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:626)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:612)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:109)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:149)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:98)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:424)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:798)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:585)
{code}

This is not expected as checkpoint failure should not lead to task failover each time."	FLINK	Resolved	3	1	8742	pull-request-available
13300212	ChannelStateReader rejects requests about unkown channels (Unaligned checkpoints)	"ChannelStateReader expects requests only for channels or subpartitions that have state.

In case of upscaling or starting from scratch this behavior is incorrect. It should return NO_MORE_DATA."	FLINK	Closed	3	1	8742	pull-request-available
13375535	DefaultCompletedCheckpointStore drops unrecoverable checkpoints silently	"The {{DefaultCompletedCheckpointStore.recover()}} tries to be resilient if it cannot recover a checkpoint (e.g. due to a transient storage outage or a checkpoint being corrupted). This behaviour was introduced with FLINK-7783.

The problem is that this behaviour might cause us to ignore the latest valid checkpoint if there is a transient problem when restoring it. This might be ok for at least once processing guarantees, but it clearly violates exactly once processing guarantees. On top of it, it is very hard to spot.

I propose to change this behaviour so that {{DefaultCompletedCheckpointStore.recover()}} fails if it cannot read the checkpoints it is supposed to read. If the {{recover}} method fails during a recovery, it will kill the process. This will usually restart the process which will retry the checkpoint recover operation. If the problem is of transient nature, then it should eventually succeed. In case that this problem occurs during an initial job submission, then the job will directly transition to a {{FAILED}} state.

The proposed behaviour entails that if there is a permanent problem with the checkpoint (e.g. corrupted checkpoint), then Flink won't be able to recover without the intervention of the user. I believe that this is the right decision because Flink can no longer give exactly once guarantees in this situation and a user needs to explicitly resolve this situation."	FLINK	Resolved	2	1	8742	pull-request-available
13357954	Incremental checkpoint data would be lost once a non-stop savepoint completed	"FLINK-10354 counted savepoint as retained checkpoint so that job could failover from latest position. I think this operation is reasonable, however, current implementation would let incremental checkpoint data lost immediately once a non-stop savepoint completed.

Current general phase of incremental checkpoints: once a newer checkpoint completed, it would be added to checkpoint store. And if the size of completed checkpoints larger than max retained limit, it would subsume the oldest one. This lead to the reference of incremental data decrease one and data would be deleted once reference reached to zero. As we always ensure to register newer checkpoint and then unregister older checkpoint, current phase works fine as expected.

However, if a non-stop savepoint (a median manual trigger savepoint) is completed, it would be also added into checkpoint store and just subsume previous added checkpoint (in default retain one checkpoint case), which would unregister older checkpoint without newer checkpoint registered, leading to data lost.

Thanks for [~banmoy] reporting this problem first."	FLINK	Resolved	1	1	8742	pull-request-available
13338219	Channel state (upstream) can be restored after emission of new elements (watermarks)	"In StreamTask.beforeInvoke:

1. operatorChain.initializeStateAndOpenOperators(createStreamTaskStateInitializer());

2. readRecoveredChannelState();

 But operatorChain.initializeStateAndOpenOperators can emit watermarks (or potentially some other stream elements).

 I've encountered this issue while adding an EndOfRecovery marker - in some runs of in OverWindowITCase.testRowTimeBoundedPartitionedRangeOver the marker was emitted after the watermark.

 

cc: [~zjwang], [~pnowojski]"	FLINK	Resolved	3	1	8742	pull-request-available
13423016	TaskExecutorStateChangelogStoragesManager.shutdown is not thread-safe	"[https://github.com/apache/flink/pull/18169#discussion_r785741977]

The method is called from the shutdown hook and therefore should be thread-safe.

cc: [~Zakelly] , [~dmvk] "	FLINK	Resolved	3	1	8742	pull-request-available
13392457	Banned dependencies in flink-statebackend-changelog	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21129&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=34482

{code}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-statebackend-changelog ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.14-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.14-SNAPSHOT
Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6
Found Banned Dependency: org.apache.flink:flink-scala_2.11:jar:1.14-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-dstl-dfs_2.11:jar:1.14-SNAPSHOT
Use 'mvn dependency:tree' to locate the source of the banned dependencies.

{code}"	FLINK	Resolved	1	1	8742	pull-request-available
13339836	Modified UnalignedCheckpointITCase...MassivelyParallel fails	"In FLINK-19681, UnalignedCheckpointITCase was updated to put more backpressure.

This revealed a bug introduced in FLINK-19907: resultPartitions can be requested before the operator chain is initialized.

A proper fix would be to initialize the chain after the outputs but before the inputs."	FLINK	Resolved	3	1	8742	pull-request-available
13307804	Erroneous check in FsCheckpointStateOutputStream#write(int)	When fixing FLINK-17820 a {{flush}} call was accidentally introduced on every single byte/int write to {{FsCheckpointStateOutputStream}}, which could significantly affect performance.	FLINK	Closed	1	1	8742	pull-request-available
13570453	TVF Window Aggregations might get stuck	"RecordsWindowBuffer flushes buffered records in the following cases:
 * watermark
 * checkpoint barrier
 * buffer overflow

 

In two-phase aggregations, this creates the following problems:

1) Local aggregation: enters hard-backpressure because for flush, it outputs the data downstream and doesn't check network buffer availability

This already disrupts normal checkpointing and watermarks progression

 

2) Global aggregation: 

When the window is large enough and/or the watermark is lagging, lots of data is flushed to state backend (and the state is updated) in checkpoint SYNC phase.

 

All this eventually causes checkpoint timeouts (10 minutes in our env).

 

Example query
{code:java}
INSERT INTO `target_table` 

SELECT window_start, window_end, some, attributes, SUM(view_time) AS total_view_time, COUNT(*) AS num, LISTAGG(DISTINCT page_url) AS pages 

FROM TABLE(TUMBLE(TABLE source_table, DESCRIPTOR($rowtime), INTERVAL '1' HOUR)) 

GROUP BY window_start, window_end, some, attributes;{code}
In our setup, the issue can be reproduced deterministically.

 

As a quick fix, we might want to:
 # limit the amount of data buffered in Global Aggregation nodes
 # disable two-phase aggregations, i.e. Local Aggregations (we can try to limit buffing there two, but network buffer availability can not be easily checked from the operator)"	FLINK	In Progress	3	1	8742	pull-request-available
13429354	SplitAggregateITCase.testAggWithJoin failed on azure	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31850&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10497]

Acknowledge of a checkpoint failed, then the checkpoint expired, then checkpoint failure threshold was reached and job failed.

{code}
Randomly selected true for execution.checkpointing.unaligned
Randomly selected PT2S for execution.checkpointing.alignment-timeout
Randomly selected true for state.backend.changelog.enabled
Randomly selected PT0.1S for state.backend.changelog.periodic-materialize.interval
{code}

{code}
[ERROR] Tests run: 64, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 700.545 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase
[ERROR] SplitAggregateITCase.testAggWithJoin  Time elapsed: 601.77 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
   at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
   at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniCl usterJobClient.java:141)
   at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
   at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
   at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandle r.java:259)
   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
   at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java :93)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadi ngUtils.java:68)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextCla ssLoader$2(ClassLoadingUtils.java:92)
   at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
   at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
   at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
   at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
   at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
   at akka.dispatch.OnComplete.internal(Future.scala:300)
   at akka.dispatch.OnComplete.internal(Future.scala:297)
   at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
   at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
   at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
   at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFut ureUtils.java:65)
   at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
   at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
   at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
   at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
...
Caused by: org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
   at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(Checkpo intFailureManager.java:160)
   at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleJobLevelCheckpointException( CheckpointFailureManager.java:123)
   at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(Checkpoi ntFailureManager.java:90)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoor dinator.java:2046)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoor dinator.java:2025)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$600(CheckpointCoordinator.java :98)
   at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoo rdinator.java:2104)
   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
   at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThread PoolExecutor.java:180)
   at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExe cutor.java:293)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
   at java.lang.Thread.run(Thread.java:748)
{code}

{code}
12:18:11,760 [jobmanager-io-thread-5] WARN  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Error while processing AcknowledgeCheckpoint message
java.lang.IllegalStateException: Attempt to reference unknown state: 4a798990-1428-424c-813a-2ec1c4fcee8f-KeyGroupRange{startKeyGroup=0, endKeyGroup=31}-000019.sst
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.SharedStateRegistryImpl.registerReference(SharedStateRegistryImpl.java:82) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.registerSharedStates(IncrementalRemoteKeyedStateHandle.java:317) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.SharedStateRegistryImpl.registerAll(SharedStateRegistryImpl.java:172) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.ChangelogStateBackendHandle$ChangelogStateBackendHandleImpl.registerSharedStates(ChangelogStateBackendHandle.java:124) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.registerSharedState(OperatorSubtaskState.java:229) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.registerSharedStates(OperatorSubtaskState.java:219) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.TaskStateSnapshot.registerSharedStates(TaskStateSnapshot.java:189) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1114) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}
"	FLINK	Closed	1	1	8742	test-stability
13374533	testScheduleRunAsync fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=6833


{code:java}
Apr 22 22:56:40 [ERROR] testScheduleRunAsync(org.apache.flink.runtime.rpc.RpcEndpointTest)  Time elapsed: 0.404 s  <<< FAILURE!
Apr 22 22:56:40 java.lang.AssertionError
Apr 22 22:56:40 	at org.junit.Assert.fail(Assert.java:86)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:41)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:52)
Apr 22 22:56:40 	at org.apache.flink.runtime.rpc.RpcEndpointTest.testScheduleRunAsync(RpcEndpointTest.java:318)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 22 22:56:40 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 22 22:56:40 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 22 22:56:40 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 22 22:56:40 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 22 22:56:40 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 22 22:56:40 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)

{code}
"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13556591	UnsupportedOperationException thrown from RocksDBIncrementalRestoreOperation	"When using the new rescaling API, it's possible to get
{code:java}
2023-10-31 18:25:05,179 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.
java.lang.UnsupportedOperationException: null
	at java.util.Collections$1.remove(Collections.java:4714) ~[?:?]
	at java.util.AbstractCollection.remove(AbstractCollection.java:299) ~[?:?]
	at org.apache.flink.runtime.checkpoint.StateObjectCollection.remove(StateObjectCollection.java:105) ~[flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithRescaling(RocksDBIncrementalRestoreOperation.java:294) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:167) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:327) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:512) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:338) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:735) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:710) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:676) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-10-31 18:25:05,182 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for KeyedProcessOperator_353a6b34b8b7f1c1d0fb4616d911049c_(1/2) from alternative (1/2), will retry while more alternatives are available.
org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:407) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:512) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:338) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:735) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:710) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:676) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.UnsupportedOperationException
	at java.util.Collections$1.remove(Collections.java:4714) ~[?:?]
	at java.util.AbstractCollection.remove(AbstractCollection.java:299) ~[?:?]
	at org.apache.flink.runtime.checkpoint.StateObjectCollection.remove(StateObjectCollection.java:105) ~[flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithRescaling(RocksDBIncrementalRestoreOperation.java:294) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:167) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:327) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	... 18 more

{code}
presumably on upscaling.
The job continues to recover (using the remote state).

The issue occurs on 1.17 and should be fixed in 1.18 and master."	FLINK	Resolved	4	1	8742	pull-request-available
13357975	Add FS-based StateChangelog implementation	Detailed design: [https://docs.google.com/document/d/1vifa8cqZqirVr6Ke1A0FclLa2aNltOQb25GZon79btM/edit?usp=sharing] 	FLINK	Closed	3	7	8742	auto-unassigned, pull-request-available
13399496	PartiallyFinishedSourcesITCase fails due to assertion error in DrainingValidator.validateOperatorLifecycle	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23526&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4639

{code}
Sep 03 23:17:11 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 19.233 s <<< FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase
Sep 03 23:17:11 [ERROR] test[simple graph SINGLE_SUBTASK, failover: true]  Time elapsed: 2.27 s  <<< FAILURE!
Sep 03 23:17:11 java.lang.AssertionError
Sep 03 23:17:11 	at org.junit.Assert.fail(Assert.java:87)
Sep 03 23:17:11 	at org.junit.Assert.assertTrue(Assert.java:42)
Sep 03 23:17:11 	at org.junit.Assert.assertFalse(Assert.java:65)
Sep 03 23:17:11 	at org.junit.Assert.assertFalse(Assert.java:75)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.DrainingValidator.validateOperatorLifecycle(DrainingValidator.java:56)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.TestOperatorLifecycleValidator.lambda$checkOperatorsLifecycle$1(TestOperatorLifecycleValidator.java:52)
Sep 03 23:17:11 	at java.util.HashMap.forEach(HashMap.java:1289)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.TestOperatorLifecycleValidator.checkOperatorsLifecycle(TestOperatorLifecycleValidator.java:47)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:94)
{code}"	FLINK	Resolved	1	1	8742	pull-request-available, test-stability
13361179	SourceStreamTaskTest.testStopWithSavepointShouldNotInterruptTheSource is failing	"We experience a test instability with {{SourceStreamTaskTest.testStopWithSavepointShouldNotInterruptTheSource}}. The test is occassionally timing out.
See [this build|https://dev.azure.com/mapohl/flink/_build/results?buildId=290&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=7846] being related to FLINK-21030.

{noformat}
""main"" #1 prio=5 os_prio=0 tid=0x00007f72fc00b800 nid=0x2133 runnable [0x00007f73046ed000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.flink.streaming.runtime.tasks.StreamTaskMailboxTestHarness.waitForTaskCompletion(StreamTaskMailboxTestHarness.java:147)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.testStopWithSavepointShouldNotInterruptTheSource(SourceStreamTaskTest.java:604)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}

This failure was reproducible on {{master}}."	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13476389	BlockingPartitionBenchmark doesn't compile	"{code}
10:15:12  [ERROR] /home/jenkins/workspace/flink-master-benchmarks-java8/flink-benchmarks/src/main/java/org/apache/flink/benchmark/BlockingPartitionBenchmark.java:117:50:  error: cannot find symbol
{code}

Caused by
https://github.com/apache/flink/commit/9f5d0c48f198ff69a175f630832687ba02cf4c3e#diff-f72e79ebd747b6fde91988d65de9121a5907c97e4630cb1e30ab65601b4d9753R79"	FLINK	Closed	3	11500	8742	pull-request-available
13362519	Consider shaping newly introduced RuntimeContext.getJobId to return JobID with no Optional wrapper	"Currently, this newly introduced {{RuntimeContext.getJobId()}} returns {{Optional<JobID>}}. The only path where it returns no job id is {{RuntimeUDFContext}}(through {{CollectionExecutor}} through {{CollectionEnvironment}}).

But after {{DataSet}} dropped, there will be no paths to return no job id. Both FLINK-21581 and [my comment|https://github.com/apache/flink/pull/15053#issuecomment-789410967] raised this concern. But different with FLINK-21581, I think we could return an environment/executor/plan level unique job id in {{RuntimeUDFContext}} for this new api. This way there will be no breaking change after {{DataSet}} dropped. And more importantly, a careful chosen job id does not hurt callers of {{RuntimeUDFContext}} in my opinion.

cc  [~chesnay] [~roman_khachatryan] [~aljoscha] [~sewen] "	FLINK	Closed	1	4	8742	pull-request-available
13427453	Too many small sst files in rocksdb state backend when using time window created in ascending order	"When using processing or event time windows, in some workloads, there will be a lot of small sst files(serveral KB) in rocksdb local directory and may cause ""Too many files error"".

Use rocksdb tool ldb to find out content in sst files:
 * column family of these small sst files is ""processing_window-timers"".
 * most sst files are in level-1.
 * records in sst files are almost kTypeDeletion.
 * creation time of sst file correspond to checkpoint interval.

These small sst files seem to be generated when flink checkpoint is triggered. Although all content in sst are delete tags, they are not compacted and deleted in rocksdb compaction because of not intersecting with each other(rocksdb [compaction trivial move|https://github.com/facebook/rocksdb/wiki/Compaction-Trivial-Move]). And there seems to be no chance to delete them because of small size and not intersect with other sst files.

 

I will attach a simple program to reproduce the problem.

 

Since timer in processing time window is generated in strictly ascending order(both put and delete). So If workload of job happen to generate level-0 sst files not intersect with each other.(for example: processing window size much smaller than checkpoint interval, and no window content cross checkpoint interval or no new data in window crossing checkpoint interval). There will be many small sst files generated until job restored from savepoint, or incremental checkpoint is disabled. 

 

May be similar problem exists when user use timer in operators with same workload.

 

Code to reproduce the problem:
{code:java}
package org.apache.flink.jira;

import lombok.extern.slf4j.Slf4j;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.configuration.RestOptions;
import org.apache.flink.configuration.TaskManagerOptions;
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.checkpoint.ListCheckpointed;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.source.SourceFunction;
import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;

import java.util.Collections;
import java.util.List;
import java.util.Random;

@Slf4j
public class StreamApp  {
  public static void main(String[] args) throws Exception {
    Configuration config = new Configuration();
    config.set(RestOptions.ADDRESS, ""127.0.0.1"");
    config.set(RestOptions.PORT, 10086);
    config.set(TaskManagerOptions.NUM_TASK_SLOTS, 6);
    new StreamApp().configureApp(StreamExecutionEnvironment.createLocalEnvironment(1, config));
  }

  public void configureApp(StreamExecutionEnvironment env) throws Exception {

    env.enableCheckpointing(20000); // 20sec

    RocksDBStateBackend rocksDBStateBackend =
        new RocksDBStateBackend(""file:///Users/shenjiaqi/Workspace/jira/flink-51/checkpoints/"", true); // need to be reconfigured
    rocksDBStateBackend.setDbStoragePath(""/Users/shenjiaqi/Workspace/jira/flink-51/flink/rocksdb_local_db""); // need to be reconfigured

    env.setStateBackend(rocksDBStateBackend);
    env.getCheckpointConfig().setCheckpointTimeout(100000);
    env.getCheckpointConfig().setTolerableCheckpointFailureNumber(5);
    env.setParallelism(1);
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

    env.getConfig().setTaskCancellationInterval(10000);

    for (int i = 0; i < 1; ++i) {
      createOnePipeline(env);
    }

    env.execute(""StreamApp"");
  }


  private void createOnePipeline(StreamExecutionEnvironment env) {
    // data source is configured so that little window cross checkpoint interval
    DataStreamSource<String> stream = env.addSource(new Generator(1, 3000, 3600));

    stream.keyBy(x -> x)
        // make sure window size less than checkpoint interval. though 100ms is too small, I think increase this value can still reproduce the problem with longer time.
        .window(TumblingProcessingTimeWindows.of(Time.milliseconds(100)))
        .process(new ProcessWindowFunction<String, String, String, TimeWindow>() {
          @Override
          public void process(String s, ProcessWindowFunction<String, String, String, TimeWindow>.Context context,
              Iterable<String> elements, Collector<String> out) {
            for (String ele: elements) {
              out.collect(ele);
            }
          }
        }).print();
  }

  public static final class Generator implements SourceFunction<String>, ListCheckpointed<Integer> {

    private static final long serialVersionUID = -2819385275681175792L;

    private final int numKeys;
    private final int idlenessMs;
    private final int recordsToEmit;

    private volatile int numRecordsEmitted = 0;
    private volatile boolean canceled = false;

    Generator(final int numKeys, final int idlenessMs, final int durationSeconds) {
      this.numKeys = numKeys;
      this.idlenessMs = idlenessMs;

      this.recordsToEmit = ((durationSeconds * 1000) / idlenessMs) * numKeys;
    }

    @Override
    public void run(final SourceContext<String> ctx) throws Exception {
      Random rnd = new Random();
      while (numRecordsEmitted < recordsToEmit) {
        synchronized (ctx.getCheckpointLock()) {
          for (int i = 0; i < numKeys; i++) {
            ctx.collect("""" + rnd.nextInt(1));
            numRecordsEmitted++;
          }
        }
        Thread.sleep(idlenessMs);
      }

      while (!canceled) {
        Thread.sleep(50);
      }

    }

    @Override
    public void cancel() {
      canceled = true;
    }

    @Override
    public List<Integer> snapshotState(final long checkpointId, final long timestamp) {
      return Collections.singletonList(numRecordsEmitted);
    }

    @Override
    public void restoreState(final List<Integer> states) {
      for (final Integer state : states) {
        numRecordsEmitted += state;
      }
    }
  }
}
 {code}
 

 Code to simulate flink checkpointing and timer creation and deletion and reproduce the problem:
{code:cpp}
//
//  main.cpp
//  reproduce
//
//  Created by shenjiaqi on 2022/2/8.
//

#include <iostream>
#include <filesystem>
#include <cstdio>
#include <cstdlib>
#include <string>

#include ""rocksdb/utilities/checkpoint.h""
#include ""rocksdb/db.h""
#include ""rocksdb/slice.h""
#include ""rocksdb/options.h""

using namespace ROCKSDB_NAMESPACE;
using ROCKSDB_NAMESPACE::DB;
using ROCKSDB_NAMESPACE::Options;
using ROCKSDB_NAMESPACE::PinnableSlice;
using ROCKSDB_NAMESPACE::ReadOptions;
using ROCKSDB_NAMESPACE::Status;
using ROCKSDB_NAMESPACE::WriteBatch;
using ROCKSDB_NAMESPACE::WriteOptions;

std::string kDBPath = ""/Users/shenjiaqi/Workspace/flink/jira/data-test""; // need to be reconfigured

static void createCheckpoint(rocksdb::DB *db, rocksdb::Status &s) {
    std::cout << ""create checkpoint"" << std::endl;
    std::string chkPath = kDBPath + ""-chp"";
    assert(chkPath.find(""/Users/shenjiaqi/Workspace/flink/jira/"") >= 0); // just in case
    system((""rm -rf "" + chkPath).data()); // use with care.
    
    Checkpoint* checkpoint_ptr;
    s = Checkpoint::Create(db, &checkpoint_ptr);
    assert(s.ok());
    
    s = checkpoint_ptr->CreateCheckpoint(chkPath);
    assert(s.ok());
}


int main() {
    DB* db;
    Options options;
    // Optimize RocksDB. This is the easiest way to get RocksDB to perform well
    options.IncreaseParallelism();
    options.OptimizeLevelStyleCompaction();
    // create the DB if it's not already present
    options.create_if_missing = true;
    options.info_log_level = DEBUG_LEVEL;
//    options.level_compaction_dynamic_level_bytes = true;

    // open DB
    Status s = DB::Open(options, kDBPath, &db);
    assert(s.ok());

    for (int i = 0; i < 1000; ++i) {

        std::string key = ""key"" + /* std::to_string((int)rand()); // */std::to_string(i);
        std::string value = ""value"" + std::to_string(i);

        // Put key-value
        s = db->Put(WriteOptions(), key, value);
        assert(s.ok());

        // delete after put
        s = db->Delete(WriteOptions(), key);
        assert(s.ok());

        if (i > 0 && (i % 5) == 0) {
            createCheckpoint(db, s);
        }
    }
    
    createCheckpoint(db, s);
    return 0;
}

{code}
Many log such as: ""Moving #407 to level-1 1047 bytes"" can be found in LOG of rocksdb (not enabled in flink by default)."	FLINK	Closed	3	1	8742	pull-request-available
13323402	Optimize reading of channel state on recovery	"Curently, channel state is read not sequentially.

Inverting control would make it more efficient.

Current call chain: 
{code:java}
StreamTask.readRecoveredChannelState  
    ResultPartition.readRecoveredState - loop through subpartitions
        PipelinedSubpartition.readRecoveredState - loop while have data; bufferBuilder = parent.getBufferPool().requestBufferBuilderBlocking(subpartitionInfo.getSubPartitionIdx());
            ChannelStateReader.readOutputData   {code}
Proposed call chain:
{code:java}
StreamTask.readRecoveredChannelState
    ChannelStateReader.readOutputData loop through state handles ordererd by handle, offset
        request buffer in the same way: BufferBuilder bufferBuilder = resPart.getBufferPool().requestBufferBuilderBlocking(subpartitionInfo.getSubPartitionIdx());
        pass to resPart.getSubpartition(idx).add(BufferConsumer, boolean, boolean)
{code}
 "	FLINK	Resolved	3	7	8742	pull-request-available
13567930	Add JobID to logging MDC	"Adding JobID to logging MDC allows to apply Structural Logging 
and analyze Flink logs more efficiently."	FLINK	Closed	3	4	8742	pull-request-available
13443935	Performance regression in checkpointSingleInput.UNALIGNED on 29.04.2022	http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200	FLINK	Closed	3	1	8742	pull-request-available
13330010	Checkpoint statistics for unfinished task snapshots	"If a checkpoint times out, there are currently no stats on the not-yet-finished tasks in the Web UI, so you have to crawl into (debug?) logs.

It would be nice to have these incomplete stats in there instead so that you know quickly what was going on. I could think of these ways to accomplish this:
 * the checkpoint coordinator could ask the TMs for it after failing the checkpoint or
 * the TMs could send the stats when they notice that the checkpoint is aborted

Maybe there are more options, but I think, this improvement in general would benefit debugging checkpoints."	FLINK	Closed	3	4	8742	pull-request-available, usability
13306407	Channel state handles, when inlined, duplicate underlying data	"If Unaligned checkpoints are enabled, channel state is written as state handles. Each channel has a handle and each such handle references the same underlying {{streamStateHandle}} (this is done to have a single file per subtask).
But, if the state is less then {{state.backend.fs.memory-threshold}}, the data is sent directly to JM as a byteStreamHandle. This causes each channel state handle to hold the whole subtask state.

This PR solves this by extracting relevant potions of the underlying handles if they are {{byteStreamHandle}}s."	FLINK	Closed	2	1	8742	pull-request-available
13429142	[Changelog] Incorrect MaterializationID passed to ChangelogStateBackendHandleImpl	"In ChangelogStateBackendHandleImpl constructor, materializationID and persistedSizeOfThisCheckpoint are mixed up.

 

cc: [~yunta]"	FLINK	Resolved	1	1	8742	pull-request-available
13375100	Built-in functions for collections	"There is a number of built-in functions to work with collections are supported by other vendors. After looking at Postgresql, BigQuery, Spark there was selected a list of more or less generic functions for collections (for more details see [1]).
Feedback for the doc is  welcome

[1] [https://docs.google.com/document/d/1nS0Faur9CCop4sJoQ2kMQ2XU1hjg1FaiTSQp2RsZKEE/edit?usp=sharing]

MAP_KEYS
MAP_VALUES
MAP_FROM_ARRAYS"	FLINK	Reopened	3	7	9144	pull-request-available, stale-assigned
13560045	Upgrade flink-shaded to 18.0	"Currently flink-shaded is in a process of releasing

Once it is released it would make sense to upgrade the dependency in Flink"	FLINK	Resolved	3	11500	9144	pull-request-available
13554658	FlinkSecurityManagerITCase fails on java 21	"There are 2 tests {{testForcedJVMExit}} and {{testIgnoredJVMExit}} failing like 
{noformat}

expected: 222
 but was: 1
	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at org.apache.flink.runtime.util.FlinkSecurityManagerITCase.testForcedJVMExit(FlinkSecurityManagerITCase.java:51)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)

{noformat}

and
{noformat}

expected: 0
 but was: 1
	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at org.apache.flink.runtime.util.FlinkSecurityManagerITCase.testIgnoredJVMExit(FlinkSecurityManagerITCase.java:65)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)

{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13596838	TimestampITCase generates test artifacts outside of tmp folder	"steps to reproduce
{noformat}
cd flink-tests && ../mvnw -Dtest=TimestampITCase test && git status
{noformat}"	FLINK	Resolved	3	11500	9144	pull-request-available
13555007	Bump zookeeper  to address CVE-2023-44981	There is a [CVE-2023-44981|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-44981] which is fixed in 3.7.2, 3.8.3	FLINK	Closed	3	11500	9144	pull-request-available
13594971	Bump checkstyle to 10+	Since java was bumped to 11 checkstyle could be bumped to 10+	FLINK	Resolved	3	11500	9144	pull-request-available
13415312	Update dependency for japicmp-maven-plugin 	"compiliation with jdk 17 fails like below

update of jaxb-impl to 2.3.1 helps
{noformat}
java.security.PrivilegedActionException: java.lang.NoSuchMethodException: sun.misc.Unsafe.defineClass(java.lang.String,[B,int,int,java.lang.ClassLoader,java.security.ProtectionDomain)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:573)
	at com.sun.xml.bind.v2.runtime.reflect.opt.Injector.<clinit>(Injector.java:197)
	at com.sun.xml.bind.v2.runtime.reflect.opt.AccessorInjector.prepare(AccessorInjector.java:81)
	at com.sun.xml.bind.v2.runtime.reflect.opt.OptimizedAccessorFactory.get(OptimizedAccessorFactory.java:125)
	at com.sun.xml.bind.v2.runtime.reflect.Accessor$GetterSetterReflection.optimize(Accessor.java:402)
	at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.<init>(TransducedAccessor.java:235)
	at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor.get(TransducedAccessor.java:175)
	at com.sun.xml.bind.v2.runtime.property.AttributeProperty.<init>(AttributeProperty.java:91)
	at com.sun.xml.bind.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:108)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.<init>(ClassBeanInfoImpl.java:181)
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:514)
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:331)
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:139)
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1156)
	at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:165)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297)
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286)
	at javax.xml.bind.ContextFinder.find(ContextFinder.java:409)
	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721)
	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:662)
	at japicmp.output.xml.XmlOutputGenerator.createXmlDocumentAndSchema(XmlOutputGenerator.java:119)
	at japicmp.output.xml.XmlOutputGenerator.generate(XmlOutputGenerator.java:70)
	at japicmp.maven.JApiCmpMojo.generateXmlOutput(JApiCmpMojo.java:866)
	at japicmp.maven.JApiCmpMojo.executeWithParameters(JApiCmpMojo.java:149)
	at japicmp.maven.JApiCmpMojo.execute(JApiCmpMojo.java:125)


{noformat}
"	FLINK	Closed	3	7	9144	pull-request-available
13503034	Make polling for metadata no more than specified timeout by using new Consumer#poll(Duration) 	"New {{Consumer#poll}}
* poll for metadata responses (counts against timeout)
* if no response within timeout, return an empty collection immediately

Also more details are at https://cwiki.apache.org/confluence/display/KAFKA/KIP-266%3A+Fix+consumer+indefinite+blocking+behavior#KIP266:Fixconsumerindefiniteblockingbehavior-Consumer#poll

The behavior was changed at https://issues.apache.org/jira/browse/KAFKA-5697"	FLINK	Closed	3	4	9144	pull-request-available
13475175	Update postgres driver because of CVE-2022-31197	More details about CVE at pgjdbc repo page https://github.com/pgjdbc/pgjdbc/security/advisories/GHSA-r38f-c4h4-hqq2	FLINK	Closed	3	11500	9144	pull-request-available
13445345	[JUnit5 Migration] Migrate TypeInformationTestBase to Junit5	"The task is a follow up for the feedback comment
https://github.com/apache/flink/pull/19716#discussion_r873685730"	FLINK	Closed	3	7	9144	pull-request-available
13554831	VertexFlameGraphFactoryTest#verifyRecursively fails on Java 21	"With jdk 21 it fails as
{noformat}
java.lang.AssertionError: No lambdas encountered in the test, cleanup functionality was not tested

	at org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.testLambdaClassNamesCleanUp(VertexFlameGraphFactoryTest.java:54)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13560042	Use maven 3.8.6 for releasing of flink-shaded	"Currently there is maven-enforcer-plugin configuration (for release only)
{noformat}
<requireMavenVersion>
                                            <!-- maven version must be lower than 3.3. See FLINK-3158 -->
                                            <version>(,3.3)</version>
                                        </requireMavenVersion>
{noformat}
which seems to be outdated and for ci 3.8.6 is used

We should keep them in sync and use 3.8.6 for both"	FLINK	Open	3	11500	9144	pull-request-available
13417000	Use ResolvedSchema in DataGen instead of TableSchema	"{{TableSchema}} is deprecated 
It is recommended to use {{ResolvedSchema}} and {{Schema}} in {{TableSchema}} javadoc"	FLINK	Closed	4	1	9144	pull-request-available
13300194	Support for casting collection types.	"Casts of collection types are not supported yet.
E.g. query: {{""SELECT cast (a as ARRAY<double>) FROM (VALUES (array[3, 2, 1])) AS T(a)""}}

fails with:
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Unsupported cast from 'ARRAY<INT NOT NULL> NOT NULL' to 'ARRAY<DOUBLE> NOT NULL'.

	at org.apache.flink.table.planner.codegen.calls.ScalarOperatorGens$.generateCast(ScalarOperatorGens.scala:1284)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:691)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:486)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:52)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:288)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:132)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$5.apply(CalcCodeGenerator.scala:152)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$5.apply(CalcCodeGenerator.scala:152)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:152)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:179)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:49)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.translateToPlanInternal(BatchExecCalc.scala:62)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalc.translateToPlanInternal(BatchExecCalc.scala:38)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecCalcBase.translateToPlan(BatchExecCalcBase.scala:42)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToTransformation(BatchExecSink.scala:131)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.scala:97)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.scala:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)
	at org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecSink.translateToPlan(BatchExecSink.scala:49)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:72)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:71)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:71)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)
...
{code}

Similar behaviour can be observed for MULTISET, MAP, ROW"	FLINK	Closed	2	7	9144	pull-request-available
13527535	Upgrade to Calcite version to 1.33.0	"In Calcite 1.33.0, C-style escape strings have been supported. We could leverage it to enhance our string literals usage.

issue: https://issues.apache.org/jira/browse/CALCITE-5305
"	FLINK	Open	3	4	9144	pull-request-available
13555504	Add Java 17 compatibility to Flink Kafka connector	"When currently trying to {{mvn clean install -Dflink.version=1.18.0 -Dscala-2.12 -Prun-end-to-end-tests -DdistDir=/Users/mvisser/Developer/flink-1.18.0 -Dflink.convergence.phase=install -Dlog4j.configurationFile=tools/ci/log4j.properties}} this fails with errors like:

{code:java}
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR] FlinkKafkaConsumerBaseMigrationTest.testRestore
[ERROR]   Run 1: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 2: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 3: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 4: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 5: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 6: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 7: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 8: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 9: Exception while creating StreamOperatorStateContext.
[INFO] 
[ERROR]   FlinkKafkaConsumerBaseTest.testExplicitStateSerializerCompatibility:721 » Runtime
[ERROR]   FlinkKafkaConsumerBaseTest.testScaleDown:742->testRescaling:817 » Checkpoint C...
[ERROR]   FlinkKafkaConsumerBaseTest.testScaleUp:737->testRescaling:817 » Checkpoint Cou...
[ERROR]   UpsertKafkaDynamicTableFactoryTest.testBufferedTableSink:243 » UncheckedIO jav...
{code}

Example stacktrace:

{code:java}
Test testBufferedTableSink(org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest) failed with:
java.io.UncheckedIOException: java.io.IOException: Serializing the source elements failed: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.setOutputType(FromElementsFunction.java:162)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySetOutputType(StreamingFunctionUtils.java:84)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.setOutputType(StreamingFunctionUtils.java:60)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.setOutputType(AbstractUdfStreamOperator.java:146)
	at org.apache.flink.streaming.api.operators.SimpleOperatorFactory.setOutputType(SimpleOperatorFactory.java:118)
	at org.apache.flink.streaming.api.graph.StreamGraph.addOperator(StreamGraph.java:434)
	at org.apache.flink.streaming.api.graph.StreamGraph.addOperator(StreamGraph.java:402)
	at org.apache.flink.streaming.api.graph.StreamGraph.addLegacySource(StreamGraph.java:356)
	at org.apache.flink.streaming.runtime.translators.LegacySourceTransformationTranslator.translateInternal(LegacySourceTransformationTranslator.java:66)
	at org.apache.flink.streaming.runtime.translators.LegacySourceTransformationTranslator.translateForStreamingInternal(LegacySourceTransformationTranslator.java:53)
	at org.apache.flink.streaming.runtime.translators.LegacySourceTransformationTranslator.translateForStreamingInternal(LegacySourceTransformationTranslator.java:40)
	at org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.translateForStreaming(SimpleTransformationTranslator.java:62)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:860)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:590)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.getParentInputIds(StreamGraphGenerator.java:881)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:839)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:590)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.generate(StreamGraphGenerator.java:328)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2289)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2280)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2266)
	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.testBufferedTableSink(UpsertKafkaDynamicTableFactoryTest.java:243)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.io.IOException: Serializing the source elements failed: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.serializeElements(FromElementsFunction.java:139)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.setOutputType(FromElementsFunction.java:160)
	... 68 more
Caused by: java.lang.RuntimeException: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:69)
	at org.apache.flink.api.java.typeutils.runtime.kryo.FlinkChillPackageRegistrar.registerSerializers(FlinkChillPackageRegistrar.java:67)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:513)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:522)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:348)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.serializeElements(FromElementsFunction.java:136)
	... 69 more
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:67)
	... 74 more
================================================================================
{code}"	FLINK	Closed	3	4	9144	pull-request-available
13561371	Add ability to customize java version for python ci in connectors	"In FLINK-33556 there was added ci for python however currently it is sticked to jdk8

 

It would be great to have ability to customize java version"	FLINK	Closed	3	4	9144	pull-request-available
13548196	Update reference data for Migration Tests	"Update migration tests in master to cover migration from new version. Since 1.18, this step could be done automatically with the following steps. For more information please refer to [this page.|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/README.md]
 # {*}On the published release tag (e.g., release-1.16.0){*}, run 
{panel}
{panel}
|{{$ mvn clean }}{{package}} {{{}-Pgenerate-migration-test-data -Dgenerate.version={}}}{{{}1.16{}}} {{-nsu -Dfast -DskipTests}}|

The version (1.16 in the command above) should be replaced with the target one.

 # Modify the content of the file [apache/flink:flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version] to the latest version (it would be ""v1_16"" if sticking to the example where 1.16.0 was released). 
 # Commit the modification in step a and b with ""{_}[release] Generate reference data for state migration tests based on release-1.xx.0{_}"" to the corresponding release branch (e.g. {{release-1.16}} in our example), replace ""xx"" with the actual version (in this example ""16""). You should use the Jira issue ID in case of [release]  as the commit message's prefix if you have a dedicated Jira issue for this task.

 # Cherry-pick the commit to the master branch. "	FLINK	Closed	3	7	9144	pull-request-available
13449408	Upgrade Calcite version to 1.30	"The latest available version of Calcite is currently 1.30. We already need to execute the rework that was planned when upgrading to Calcite 1.27 FLINK-20873 and when upgrading to Calcite 1.28 FLINK-21239

When doing this upgrade, we should do this to the last available version. This is needed to resolve multiple bugs. 

Additional note: Flink currently uses Calcite 1.26.0 which has this note on the release page:

{{Warning: Calcite 1.26.0 has severe issues with RexNode simplification caused by SEARCH operator ( wrong data from query optimization like in CALCITE-4325, CALCITE-4352, NullPointerException), so use 1.26.0 for development only, and beware that Calcite 1.26.0 might corrupt your data.}}
https://calcite.apache.org/news/2020/10/06/release-1.26.0/

The following files should be removed from the Flink code base when upgrading calcite to 1.30.0

in `org.apahce.calcite.rel.core`:
 * Correlate
 * Filter
 * Intersect
 * Minus
 * SetOp
 * Sort
 * Union
 * Values
 * Window

in `org.apahce.calcite.rel.hint`:
 * HintPredicates
 * NodeTypeHintPredicate

in `org.apahce.calcite.rel.logical`:
 * LogicalCorrelate
 * LogicalFilter
 * LogicalIntersect
 * LogicalMinus
 * LogicalSort
 * LogicalUnion
 * LogicalValues
 * LogicalWindow"	FLINK	Closed	3	11500	9144	pull-request-available
13561454	Allow passing parameters to database via jdbc url	"Currently it does not allow to pass extra properties e.g.
an attempt to connect to 
{{jdbc:postgresql://...?sslmode=require}}
fails with 
{noformat}
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:229)
	at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:83)
	... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation b70b5cf7-7068-4eb6-83a4-78aed36dbd35.
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)


{noformat}
because of of a logic at {{org.apache.flink.connector.jdbc.catalog.JdbcCatalogUtils#validateJdbcUrl}}"	FLINK	Open	3	4	9144	pull-request-available
13556767	FlinkImageBuilder checks for Java 21	"Currently for java 21 it fails like
{noformat}
Nov 04 03:03:08 Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image ""flink-configured-jobmanager""
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)
Nov 04 03:03:08 	... 61 more
Nov 04 03:03:08 Caused by: java.lang.IllegalStateException: Unexpected Java version: 21
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.getJavaVersionSuffix(FlinkImageBuilder.java:284)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.lambda$buildBaseImage$3(FlinkImageBuilder.java:250)
Nov 04 03:03:08 	at org.testcontainers.images.builder.traits.DockerfileTrait.withDockerfileFromBuilder(DockerfileTrait.java:19)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:246)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)
Nov 04 03:03:08 	... 62 more
Nov 04 03:03:08 

{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13588651	SHOW CREATE VIEW returns invalid query	"especially for views with comments
for instance
1. create view
{code:sql}
CREATE VIEW v1 COMMENT 'test view' AS SELECT 1, 'a';
{code}
2. show create
{code:sql}
SHOW CREATE VIEW v1;
{code}
 it returns query
{code:sql}
CREATE VIEW `default_catalog`.`default_database`.`v1`(`EXPR$0`, `EXPR$1`) as
SELECT 1, 'a' COMMENT 'test view'
{code}

now if we try to execute it fails as
{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Incorrect syntax near the keyword 'COMMENT' at line 2, column 15.
Was expecting one of:
    <EOF>
    ""AS"" ...
    ""EXCEPT"" ...
{noformat}

the reason is that {{COMMENT}} should be before the query according to syntax mentioned at https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#create-view"	FLINK	Resolved	3	1	9144	pull-request-available
13576182	Release flink-connector-opensearch v1.2.0 and v.2.0.0 for Flink 1.19	"[https://github.com/apache/flink-connector-opensearch]

 "	FLINK	In Progress	3	7	9144	pull-request-available
13429898	'HELP ;', 'QUIT ;' and other sql-client commands fail with CalciteException: Non-query expression encountered in illegal context	"It seems the reason is https://github.com/apache/flink/pull/18363/files
where added condition like 
{code:java}
super(Pattern.compile(""HELP;?"", DEFAULT_PATTERN_FLAGS))
{code}
that means {{HELP;}} will work however if there is any space between {{HELP}} and {{;}} then not
and it fails like below (before that it worked without failing)

Having a space between command and a semicolon is pretty common since autocompletion inserts it after tab
{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.apache.calcite.runtime.CalciteException: Non-query expression encountered in illegal context
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.checkNonQueryExpression(FlinkSqlParserImpl.java:395)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression3(FlinkSqlParserImpl.java:21147)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2b(FlinkSqlParserImpl.java:20816)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression2(FlinkSqlParserImpl.java:20857)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.Expression(FlinkSqlParserImpl.java:20788)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.LeafQueryOrExpr(FlinkSqlParserImpl.java:20765)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.QueryOrExpr(FlinkSqlParserImpl.java:20213)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.OrderedQueryOrExpr(FlinkSqlParserImpl.java:588)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3986)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:2915)
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:287)
	at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:193)
	at org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:77)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$parseStatement$1(LocalExecutor.java:172)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172)
	at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45)
	at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71)
	at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2731)
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:585)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:296)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:281)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:229)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)

{noformat}"	FLINK	Closed	3	1	9144	pull-request-available
13470681	Update assertj to 3.23.1	"Among others there is a performance improvement for {{containsExactly}} at https://github.com/assertj/assertj-core/issues/2548

the full release notes are available at https://assertj.github.io/doc/#assertj-core-3-23-1-release-notes"	FLINK	Closed	4	11500	9144	pull-request-available
13476726	Header in janino generated java files can merge with line numbers	"Since Line numbers are generated only for debug output it should not be a big issue.
From the other side currently this behavior leads to not compiled code.
The suggestion is usage of one-line comments for header to prevent this"	FLINK	Closed	4	1	9144	pull-request-available
13161739	Support extract epoch, decade, millisecond, microsecond	"The task is to separate activity from depending on https://issues.apache.org/jira/browse/CALCITE-2303 from all others that could be done without upgrade avatica/calcite in  https://issues.apache.org/jira/browse/FLINK-8518

Now the implementations of next functions are blocked
{code:sql}
extract(decade from ...)
extract(epoch from ...)
extract(millisecond from ...)
extract(microsecond from ...)
extract(isodow from ...)
extract(isoyear from ...)
{code}
"	FLINK	Closed	4	2	9144	auto-deprioritized-major, auto-unassigned, pull-request-available
13593144	Do not use StringBuilder for EquivalentExprShuttle	"Currently there is {{EquivalentExprShuttle}} where there is a map of {{toString}} node to {{RelNode}}. In fact there is no need for toString which is calculated each time with usage of {{StringBuilder}} under the hood. And even more we faced one weird case where it consumes memory.
"	FLINK	Resolved	3	1	9144	pull-request-available
13350780	Upgrade Calcite version to 1.27	"The following files should be removed from the Flink code base during an upgrade:
 - org.apache.calcite.rex.RexSimplify
 - org.apache.calcite.sql.SqlMatchRecognize
 - org.apache.calcite.sql.SqlTableRef
 - org.apache.calcite.sql2rel.RelDecorrelator
 - org.apache.flink.table.planner.functions.sql.SqlJsonObjectFunction (added in FLINK-16203)
 - Adopt calcite's behaviour and add SQL tests once [https://github.com/apache/calcite/pull/2555] is merged, (check FLINK-24576 )"	FLINK	Closed	3	4	9144	pull-request-available
13591063	Remove powermock usage	"Most of the tests are either moved to a different repo like connectors or rewritten in powermock free way.
Powermock itself became unmaintained (latest release was in 2020 https://github.com/powermock/powermock/releases/tag/powermock-2.0.9)
and latest commit 2 years ago https://github.com/powermock/powermock

also there is no support for junit5 (the request to support it and even PR from junit5 maintainers is ready for review since Feb 2023 https://github.com/powermock/powermock/pull/1146, however still no feedback from maintainers...)"	FLINK	Resolved	3	11500	9144	pull-request-available
13528991	Move execution logic of DropOperation out from TableEnvironmentImpl	This should implement {{ExecutableOperation}} for all the {{DropOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.	FLINK	Closed	3	7	9144	pull-request-available
13159667	Improve TableException message with TypeName usage	"Currently in TableException simple name is in use. It is not clear what is the issue while having error message like {noformat}
Exception in thread ""main"" org.apache.flink.table.api.TableException: Result field does not match requested type. Requested: Date; Actual: Date
	at org.apache.flink.table.api.TableEnvironment$$anonfun$generateRowConverterFunction$1.apply(TableEnvironment.scala:953)
{noformat}
or
{noformat}Caused by: org.apache.flink.table.api.TableException: Type is not supported: Date
	at org.apache.flink.table.api.TableException$.apply(exceptions.scala:53){noformat}
also for more detailed have a look at FLINK-9341"	FLINK	Closed	4	4	9144	pull-request-available
13561594	Update junit to 5.10.1	"There is logging improved for 
>Exceptions thrown for files that cannot be deleted when cleaning up a temporary directory created via {{@TempDir}} now include the root cause.

[https://junit.org/junit5/docs/current/release-notes/index.html#bug-fixes-2]

which could help with debugging of FLINK-33641"	FLINK	Resolved	3	11500	9144	pull-request-available
13526876	TRY_CAST fails for constructed types	"In case of problems with cast it is expected to return {{null}}

however for arrays, maps it fails

example of failing queries
{code:sql}
select try_cast(array['a'] as array<int>);
select try_cast(map['a', '1'] as map<int, int>);
{code}

 {noformat}
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: 'a'. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at StreamExecCalc$15.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{noformat}"	FLINK	Resolved	3	1	9144	pull-request-available
13408414	Ceil, floor for some timeunit return wrong results or fail with CompileException	"There are issues
1. for {{TIMESTAMP WITHOUT TIMEZONE}} and {{DATE}} it returns wrong result for queries
{code:sql}
select ceil(timstamp'2020-26-10 12:12:12' to decade);
select ceil(timstamp'2020-26-10 12:12:12' to century);
select ceil(timstamp'2020-26-10 12:12:12' to millennium);
{code}

same for {{FLOOR}} and {{DATE}}
2. for {{TIMESTAMP WITH TIMEZONE}} it throws exception below.

Expected for the query
{code:sql}
select floor(date '2021-10-07' to decade) as floor_decade,      
         ceil(date '2021-10-07' to decade) as ceil_decade,
         floor(date '2021-10-07' to century) as floor_century,
         ceil(date '2021-10-07' to century) as ceil_century,
         floor(date '2021-10-07' to millennium) as floor_millennium,
         ceil(date '2021-10-07' to millennium) as ceil_millennium;

{code}
is
{noformat}
 floor_decade ceil_decade floor_century ceil_century floor_millennium ceil_millennium
   2020-01-01  2030-01-01    2001-01-01   2101-01-01       2001-01-01      3001-01-01
{noformat}
based on  PostgreSQL[1] and Vertica[2] defibitions 
And for both the definition is the same
{{DECADE}} - The year field divided by 10
{{CENTURY}} - The first century starts at 0001-01-01 00:00:00 AD, although they did not know it at the time. This definition applies to all Gregorian calendar countries. There is no century number 0, you go from -1 century to 1 century.
{{MILLENNIUM}} - The millennium number, where the first millennium is 1 and each millenium starts on 01-01-y001. For example, millennium 2 starts on 01-01-1001.
from
[1] https://www.postgresql.org/docs/14/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT
[2] https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Date-Time/DATE_PART.htm

{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 57, Column 0: No applicable constructor/method found for actual parameters ""org.apache.flink.table.data.TimestampData, org.apache.flink.table.data.TimestampData""; candidates are: ""public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, java.math.BigDecimal)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, int)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, java.math.BigDecimal)"", ""public static short org.apache.calcite.runtime.SqlFunctions.ceil(short, short)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal)"", ""public static double org.apache.calcite.runtime.SqlFunctions.ceil(double)"", ""public static float org.apache.calcite.runtime.SqlFunctions.ceil(float)"", ""public static byte org.apache.calcite.runtime.SqlFunctions.ceil(byte, byte)"", ""public static long org.apache.calcite.runtime.SqlFunctions.ceil(long, long)"", ""public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, int)""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9263)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:712)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:686)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:187)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:63)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:675)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:661)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:960)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:929)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:753)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:574)
	at java.base/java.lang.Thread.run(Thread.java:834)

{noformat}"	FLINK	Closed	3	1	9144	pull-request-available
13562043	Support OpenSearch v2	"The main issue is that in OpenSearch v2 there were several breaking changes like 
[https://github.com/opensearch-project/OpenSearch/pull/9082]
[https://github.com/opensearch-project/OpenSearch/pull/5902]

which made current connector version failing while communicating with v2

 

Also it would make sense to add integration and e2e tests to test against v2"	FLINK	Resolved	3	4	9144	pull-request-available
13481847	Upgrade Calcite version to 1.32	"{code}
This release fixes CVE-2022-39135, an XML External Entity (XEE) vulnerability that allows a SQL query to read the contents of files via the SQL functions EXISTS_NODE, EXTRACT_XML, XML_TRANSFORM or EXTRACT_VALUE.

Coming 1 month after 1.31.0 with 19 issues fixed by 17 contributors, this release also replaces the ESRI spatial engine with JTS and proj4j, adds 65 spatial SQL functions including ST_Centroid, ST_Covers and ST_GeomFromGeoJSON, adds the CHAR SQL function, and improves the return type of the ARRAY and MULTISET functions.{code}"	FLINK	Closed	3	4	9144	pull-request-available
13549257	NPE when using GREATEST() in Flink SQL	"Hi,

I see NPEs in flink 1.14 and flink 1.16 when running queries with GREATEST() and timestamps. Below is an example to help in reproducing the issue.
{code:java}
CREATE TEMPORARY VIEW Positions AS
SELECT
SecurityId,
ccy1,
CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp
FROM (VALUES
(1, 'USD', '2022-01-01'),
(2, 'GBP', '2022-02-02'),
(3, 'GBX', '2022-03-03'),
(4, 'GBX', '2022-04-4'))
AS ccy(SecurityId, ccy1, publishTimestamp);

CREATE TEMPORARY VIEW Benchmarks AS
SELECT
SecurityId,
ccy1,
CAST(publishTimestamp AS TIMESTAMP(3)) as publishTimestamp
FROM (VALUES
(3, 'USD', '2023-01-01'),
(4, 'GBP', '2023-02-02'),
(5, 'GBX', '2023-03-03'),
(6, 'GBX', '2023-04-4'))
AS ccy(SecurityId, ccy1, publishTimestamp);

SELECT *,
GREATEST(
IFNULL(Positions.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3))),
IFNULL(Benchmarks.publishTimestamp,CAST('1970-1-1' AS TIMESTAMP(3)))
)
FROM Positions
FULL JOIN Benchmarks ON Positions.SecurityId = Benchmarks.SecurityId {code}
 

Using ""IF"" is a workaround at the moment instead of using ""GREATEST""

  "	FLINK	Resolved	4	1	9144	pull-request-available
13446270	Update testcontainers dependency to v1.17.2	"testcontainers 1.17.2 is released

Among others there is a fix for connection leak in jdbc, performance
Main benefits (based on [https://github.com/testcontainers/testcontainers-java/releases/tag/1.17.2)]"	FLINK	Closed	4	11500	9144	pull-request-available
13555023	A number of json plan tests fail with comparisonfailure	"for instance
{noformat}
[ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.SortJsonPlanTest.testSort  Time elapsed: 0.037 s  <<< FAILURE!
org.junit.ComparisonFailure: 
expected:<...alse"",
            ""[table-sink-class"" : ""DEFAULT"",
            ""connector"" : ""values]""
          }
      ...> but was:<...alse"",
            ""[connector"" : ""values"",
            ""table-sink-class"" : ""DEFAULT]""
          }
      ...>
	at org.junit.Assert.assertEquals(Assert.java:117)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyJsonPlan(TableTestBase.scala:846)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyJsonPlan(TableTestBase.scala:813)
	at org.apache.flink.table.planner.plan.nodes.exec.stream.SortJsonPlanTest.testSort(SortJsonPlanTest.java:64)

{noformat}"	FLINK	Closed	3	7	9144	pull-request-available
13410580	Examples in documentation for value1 IS DISTINCT FROM value2 are wrong	"Currently it is stated in docs for {{value1 IS DISTINCT FROM value2}}
{quote}
E.g., 1 IS NOT DISTINCT FROM NULL returns TRUE; NULL IS NOT DISTINCT FROM NULL returns FALSE.
{quote}
In fact they return opposite values."	FLINK	Closed	4	1	9144	auto-deprioritized-major, pull-request-available
13540035	SQL array_union could return wrong result	"This is was mentioned at [https://github.com/apache/flink/pull/22717#issuecomment-1587333488]

 how to reproduce
{code:sql}
SELECT array_union(ARRAY[CAST(NULL AS INT)], ARRAY[1]); -- returns [NULL, 1], this is OK
SELECT array_union(ARRAY[1], ARRAY[CAST(NULL AS INT)]); -- returns [1, 0], this is NOT OK
{code}"	FLINK	Closed	3	1	9144	pull-request-available
13474166	Upgrade Calcite version to 1.31	We should upgrade to Calcite 1.31 so we can benefit from https://issues.apache.org/jira/browse/CALCITE-4865	FLINK	Closed	3	11500	9144	pull-request-available
13411537	Flink SQL Client should print corrently multisets	"Probably the easiest way to reproduce is 
{code:sql}
CREATE TABLE flink_multiset_example (
     m multiset<BIGINT>
 ) WITH (
   'connector' = 'datagen'
 );
select * from flink_multiset_example;
{code}

I think it relates to https://issues.apache.org/jira/browse/FLINK-21456"	FLINK	Closed	3	1	9144	pull-request-available
13554583	Allow to customize arg line for connector's surefire plugin	"Currently in case there is a need to add {{add opens}} {{add exports}} it is impossible to do without complete rewriting of surefire configuration...

 

The idea is to apply same approach as for flink and it's modules to allow customization via {{surefire.module.config}} property"	FLINK	Closed	3	11500	9144	pull-request-available
13409746	SQL client add info about key strokes to docs	"SQL client supports key strokes from jline.
Unfortunately there is no docs about that in jline however there is source from which it could be found [1]
here it is a list of most useful key strokes which are already supported by all existing Flink SQL client

|| Key-Stroke || Description ||
| `alt-b` | Backward word |
| `alt-f` | Forward word |
| `alt-c` | Capitalize word |
| `alt-l` | Lowercase word |
| `alt-u` | Uppercase word |
| `alt-d` | Kill word |
| `alt-n` | History search forward |
| `alt-p` | History search backward |
| `alt-t` | Transpose words |
| `ctrl-a` | To the beginning of line |
| `ctrl-e` | To the end of line |
| `ctrl-b` | Backward char |
| `ctrl-f` | Forward char |
| `ctrl-d` | Delete char |
| `ctrl-h` | Backward delete char |
| `ctrl-t` | Transpose chars |
| `ctrl-i` | Invoke completion |
| `ctrl-j` | Submit a query |
| `ctrl-m` | Submit a query |
| `ctrl-k` | Kill the line to the right from the cursor |
| `ctrl-w` | Kill the line to the left from the cursor |
| `ctrl-u` | Kill the whole line |
| `ctrl-l` | Clear screen |
| `ctrl-n` | Down line from history |
| `ctrl-p` | Up line from history |
| `ctrl-r` | History incremental search backward |
| `ctrl-s` | History incremental search forward |


[1] https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907"	FLINK	Resolved	4	1	9144	auto-deprioritized-major, pull-request-available
13409859	Single quotes converted to left/right single quotes during docs generation	"The problem is that there are lots of system functions at [1] with examples. Before 1.14.0 (probably before generation of docs with Hugo) it was possible to copy paste this examples to Flink SQL Client, submit a query and to see the result.
Now it does not work as right and left single quote violate SQL syntax

some examples(not all) of impacted functions
{{EXTRACT}}, {{PARSE_URL}}, {{YEAR}}, {{MONTH}}

probably the issue is not only with this page.

There is no such problem for version specific pages for versions 1.13 and lower.
The problem is for 1.14 and master

[1] https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/"	FLINK	Open	10200	1	9144	auto-deprioritized-major, auto-deprioritized-minor, pull-request-available, stale-assigned
13538322	libssl not found when running CI	"{noformat}
--2023-05-31 19:10:13--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.12_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 185.125.190.39, 91.189.91.38, 91.189.91.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|185.125.190.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-05-31 19:10:13 ERROR 404: Not Found.

{noformat}
e.g.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49523&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=d6e79740-7cf7-5407-2e69-ca34c9be0efb&l=265"	FLINK	Resolved	1	11500	9144	pull-request-available, test-stability
13554991	Bump commons-compress to address CVEs	current version has CVE-2023-42503	FLINK	Closed	3	11500	9144	pull-request-available
13540594	Skip archunit tests in java1X-target profiles	"When compiling to Java 11/17 byte code archunit fails; not sure why. Maybe it finds more/less stuff or signatures are represented differently.

In any case let's use the Java 8 bytecode version as the ""canonical"" version and skip archunit otherwise."	FLINK	Closed	2	11500	9144	pull-request-available, test-stability
13417217	Support nulls in DataGen	"Currently it is impossible to specify that some values should be null sometimes.
It would be nice to have some property something like {{null-rate}} telling how often there should be {{null}} value generated
something like that
{code:sql}
CREATE TABLE Orders (
    order_number STRING,
    price        DECIMAL(32,2),
    buyer        ROW<id INT, last_name STRING>,
    order_time   TIMESTAMP(3),
    my_map       MAP<INT,STRING>,
    my_arrray    ARRAY<STRING>
) WITH (
   'connector' = 'datagen',
   'fields.order_number.null-rate' = '0.7',
   'fields.price.null-rate' = '1.0',
   'fields.order_time.null-rate' = '0.5',
   'fields.buyer.id.null-rate' = '0.5',
   'fields.buyer.null-rate' = '0.5',
   'fields.my_map.key.null-rate' = '0.5',
   'fields.my_map.null-rate' = '0.5',
   'fields.my_array.element.null-rate' = '0.1',
   'fields.my_array.null-rate' = '0.5'
);
{code}"	FLINK	Resolved	4	1	9144	pull-request-available, stale-assigned
13573430	Cannot convert org.apache.flink.streaming.api.CheckpointingMode  to org.apache.flink.core.execution.CheckpointingMode	"After this change FLINK-34516 elasticsearch connector for 1.20-SNAPSHOT starts failing with
{noformat}
 Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-e2e-tests/flink-connector-elasticsearch-e2e-tests-common/src/main/java/org/apache/flink/streaming/tests/ElasticsearchSinkE2ECaseBase.java:[75,5] method does not override or implement a method from a supertype
Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-e2e-tests/flink-connector-elasticsearch-e2e-tests-common/src/main/java/org/apache/flink/streaming/tests/ElasticsearchSinkE2ECaseBase.java:[85,84] incompatible types: org.apache.flink.streaming.api.CheckpointingMode cannot be converted to org.apache.flink.core.execution.CheckpointingMode
{noformat}
https://github.com/apache/flink-connector-elasticsearch/actions/runs/8436631571/job/23104522666#step:15:12668

set blocker since now every build of elasticsearch connector against  1.20-SNAPSHOT  is failing
probably same issue is for opensearch connector"	FLINK	Closed	1	1	9144	pull-request-available
13407293	FlinkSQL multiline parser improvements	"Currently existing multiline parser has limitations e.g.
line could not end with semicolon e.g. as a part of field value, comment or column name.
Also if a query contains '--' e.g. as a part of varchar field value then it fails.

In case there is no objections I would put some efforts to improve this behavior;

here it is a list of sample problem queries
{code:sql}
select 123; -- comment

select 1 as `1--`;

select '--';

-- This query works if a user copy-pastes it to FlinkSQL, however it fails if a user types it in FlinkSQL
select '1;
';
{code}
"	FLINK	Closed	3	7	9144	pull-request-available, stale-assigned
13480351	Remove usages of deprecated agg.indicator	"In https://issues.apache.org/jira/browse/CALCITE-2944 {{Aggregate#indicator}} was made deprecated and always {{false}}.
So it could be removed"	FLINK	Closed	4	11500	9144	pull-request-available
13548191	Update japicmp configuration	"Update the japicmp reference version and wipe exclusions / enable API compatibility checks for {{@PublicEvolving}} APIs on the corresponding SNAPSHOT branch with the {{update_japicmp_configuration.sh}} script (see below).

For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.

Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.
{code:bash}
tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.sh
tools $ cd ..$ git add *$ git commit -m ""Update japicmp configuration for $RELEASE_VERSION"" {code}"	FLINK	Resolved	3	7	9144	pull-request-available
13555456	Kubernetes operator supports compiling with Java 17	"In the voting mailing list for flink-kubernetes-operator version 1.6.1, Thomas mentioned Kubernetes operator cannot compile with java 17.

Offline discussion with [~gyfora] , we hope Kubernetes operator supports compiling with Java 17 as a critical ticket in 1.7.0."	FLINK	Closed	2	4	9144	pull-request-available
13169852	"Duplicate lines for ""Weekday name (Sunday .. Saturday)"""	"could be seen at https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql.html#date-format-specifier
+ attach"	FLINK	Closed	4	4	9144	pull-request-available
13556961	Kubernetes operator supports compiling with Java 21	Since there is a new Java LTS version available (21) it would make sense to support it	FLINK	Closed	3	4	9144	pull-request-available
13590651	FsMergingCheckpointStorageLocationTest generates test data not in tmp folder	"to reproduce this command could be used (assuming that something like {{./mvnw clean verify -DskipTests -Dfast}} was run before)
{code:bash}
cd flink-runtime && ./../mvnw -Dtest=FsMergingCheckpointStorageLocationTest test && git status
{code}
the output will contain something like
{noformat}
Untracked files:
  (use ""git add <file>..."" to include in what will be committed)
	org.junit.rules.TemporaryFolder@3cc1435c/
	org.junit.rules.TemporaryFolder@625732/
	org.junit.rules.TemporaryFolder@bef2d72/
{noformat}

the reason is that in {{@Before}} there *not* absolute path is in use"	FLINK	Resolved	3	1	9144	pull-request-available
13544529	Reuse Calcite's SqlWindowTableFunction	There is Calcite's {{SqlWindowTableFunction}} which could extended with reuse of some existing logic	FLINK	Resolved	3	7	9144	pull-request-available
13556752	Upgrade maven-shade-plugin to 3.5.1	"Current plugin version does not support jdk20+
The support was added within MSHADE-454 and released with 3.5.1"	FLINK	Closed	3	7	9144	pull-request-available
13557390	Make AWS connectors compilable with jdk17	Since 1.18 Flink with jdk 17 support is released it would make sense to add such support for connectors	FLINK	Resolved	3	4	9144	pull-request-available
13165930	Redundant spaces for Collect at sql.md	"could be seen at https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql.html
+ attach"	FLINK	Closed	4	1	9144	pull-request-available
13445608	[JUnit5 Migration] Migrate ComparatorTestBase to Junit5	Several modules depend on it	FLINK	Closed	3	7	9144	pull-request-available
13168808	ATAN2 Sql Function support	"simple query fails {code}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());

DataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);
tableEnv.registerDataSet(""t1"", ds, ""x, y, z"");

String sqlQuery = ""SELECT atan2(1,2)"";
Table result = tableEnv.sqlQuery(sqlQuery);
{code}
while at the same time Calcite supports it and in Calcite's sqlline it works like {noformat}
0: jdbc:calcite:model=target/test-classes/mod> select atan2(1,2);
+-----------------+
|     EXPR$0      |
+-----------------+
| 0.4636476090008061 |
+-----------------+
1 row selected (0.173 seconds)
{noformat}"	FLINK	Resolved	4	2	9144	pull-request-available
13572681	Bump Checkstyle to 9+	"The issue with current checkstyle is that there is checkstyle IntellijIdea plugin

And recently it dropped checkstyle 8 support [1]

At the same time we can not move to Checkstyle 10 since 10.x requires java 11+

[1] https://github.com/jshiell/checkstyle-idea/blob/main/CHANGELOG.md"	FLINK	Closed	3	1	9144	pull-request-available
13246759	Remove Hive and Hadoop dependencies from SQL Client	"340/550 lines in the SQL Client {{pom.xml}} are just around Hive and Hadoop dependencies.  Hive has nothing to do with the SQL Client and it will be hard to maintain the long list of  exclusion there. Some dependencies are even in a {{provided}} scope and not {{test}} scope.

We should remove all dependencies on Hive/Hadoop and replace catalog-related tests by a testing catalog. Similar to how we tests source/sinks."	FLINK	Closed	4	1	9144	auto-deprioritized-critical, auto-deprioritized-major, pull-request-available, stale-assigned
13273004	"Drop ""RequiredParameters"" and ""Options"""	"As per [mailing list discussion|https://lists.apache.org/thread/22cf21xwzytldp235yw1yhzdtnrny23v], we want to drop those because they are unused redundant code.
There are many options for command line parsing, including one in Flink (Parameter Tool)."	FLINK	Resolved	4	11500	9144	pull-request-available
13533267	Update maven cyclonedx plugin to 2.7.7	"there are at least 2 related improvements

1. current version depends on jackson-databind 2.14.0 and has a memory issue described at [https://github.com/FasterXML/jackson-databind/issues/3665] which is fixed in later versions

2. current version leads to lots of traces in logs (e.g. {{mvn clean verify}} for {{flink-core}}) which is fixed in later versions
{noformat}
[ERROR] An error occurred attempting to read POM
org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) 
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)
    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)
    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)
    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)
    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)
    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)
    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)
    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:196)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)

{noformat}"	FLINK	Closed	3	11500	9144	pull-request-available
13486274	Update jackson bom because of CVE-2022-42003	"There is a CVE-2022-42003 fixed in 2.13.4.1 and 2.14.0-rc1
https://nvd.nist.gov/vuln/detail/CVE-2022-42003


P.S. It seems there will not be 2.14.0 release until end of October according to https://github.com/FasterXML/jackson-databind/issues/3590#issuecomment-1270363915"	FLINK	Resolved	3	11500	9144	pull-request-available
13474555	Update JUnit5 to v5.9.1	"Junit 5.9.0 is released

with release notes https://junit.org/junit5/docs/current/release-notes/#release-notes-5.9.0"	FLINK	Closed	4	11500	9144	pull-request-available, stale-assigned
13554525	Update testcontainers dependency to v1.19.1	"One of the main features is life simplification for SELinux users (I'm one of those)
https://github.com/testcontainers/testcontainers-java/pull/6294
https://github.com/testcontainers/testcontainers-java/pull/7187

Also more release notes are at
https://github.com/testcontainers/testcontainers-java/releases/tag/1.19.1
https://github.com/testcontainers/testcontainers-java/releases/tag/1.19.0
"	FLINK	Resolved	3	11500	9144	pull-request-available
13561053	Allow to specify optional profile for connectors	"The issue is that sometimes the connector should be tested against several versions of sinks/sources

e.g. hive connector should be tested against hive 2 and hive3, opensearch should be tested against 1 and 2
one of the way is using profiles for that"	FLINK	Closed	3	4	9144	pull-request-available
13538926	Update testcontainers dependency to v1.18.3	"Among others there are
* Fixes the issue of missing root cause in container launch TimeoutException (e.g. SSLHandshakeException) 
* Make sure we don't hide exceptions from waitUntilContainerStarted

also full list is at 
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.0
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.1
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.2
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.3

 "	FLINK	Closed	3	11500	9144	pull-request-available
13554655	Scala before 2.12.18 doesn't compile on Java 21	"It fails with

{noformat}
[ERROR] error:
[INFO]   bad constant pool index: 0 at pos: 48445
[INFO]      while compiling: <no file>
[INFO]         during phase: globalPhase=<no phase>, enteringPhase=<some phase>
[INFO]      library version: version 2.12.15
[INFO]     compiler version: version 2.12.15
...
[INFO]   last tree to typer: EmptyTree
[INFO]        tree position: <unknown>
[INFO]             tree tpe: <notype>
[INFO]               symbol: null
[INFO]            call site: <none> in <none>
[INFO] 
[INFO] == Source file context for tree position ==

{noformat}

based on release notes 2.12.18 - the first 2.12.x supporting jdk21
https://github.com/scala/scala/releases/tag/v2.12.18"	FLINK	Closed	3	7	9144	pull-request-available
13544724	TableSourceJsonPlanTest.testReuseSourceWithoutProjectionPushDown is failing	"Blocker since it's failing on every build and reproduced locally
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51661&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11529"	FLINK	Closed	1	1	9144	pull-request-available, test-stability
13528615	"Backport ""FilterJoinRule misses opportunity to push filter to semijoin input"" to FlinkFilterJoinRule"	"In https://issues.apache.org/jira/browse/CALCITE-4499 there has been done an optimization.
Since Flink has it's own copy of slightly changed {{FilterJoiRule}} this optimization does not come with 1.28 update.

The idea is to apply this change to {{FlinkFilterJoinRule}}"	FLINK	Closed	3	4	9144	pull-request-available
13554559	Allow to customize jdk version for connectors	This will allow to enable jdk 17 per connector once it's ready to compile and run with jdk17 for instance	FLINK	Closed	3	11500	9144	pull-request-available
13514022	Archunit can't find any tests in Pulsar repository	"CI is failing on {{main}} with:

{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-pulsar: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: 'java.lang.Object com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(java.lang.Class)' -> [Help 1]
{code}

It does work for {{v3.0}} but there's no obvious difference between the two branches that might explain this issue. "	FLINK	Closed	1	1	9144	pull-request-available
13537819	Remove dependency on flink-shaded from flink-connector-aws	The AWS connectors depend on flink-shaded. With the externalization of connector, connectors shouldn't rely on Flink-Shaded but instead shade dependencies such as this one themselves	FLINK	Resolved	3	11500	9144	pull-request-available
12831042	Add pipelining mechanism for chainable transformers and estimators	The key concept of an easy to use ML library is the quick and simple construction of data analysis pipelines. Scikit-learn's approach to define transformers and estimators seems to be a really good solution to this problem. I propose to follow a similar path, because it makes FlinkML flexible in terms of code reuse as well as easy for people coming from Scikit-learn to use the FlinkML.	FLINK	Closed	3	4	10066	ML
13108329	SlotManager releases idle TaskManager in standalone mode	"The {{SlotManager}} releases idle {{TaskManagers}} and removes all their slots. This also happens in standalone mode where we cannot release task managers. 

I suggest to let the {{ResourceManager}} decide whether a resource can be released or not. Only in the former case, we will remove the associated slots from the {{SlotManager}}."	FLINK	Closed	3	1	10066	flip-6
13229903	Remove legacy ActorTaskManagerGateway	Remove the legacy {{ActorTaskManagerGateway}} component from Flink.	FLINK	Closed	4	7	10066	pull-request-available
13388414	AkkaRpcActorTest#testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20163&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6023

{code}
Jul 08 11:03:13 java.lang.AssertionError: 
Jul 08 11:03:13 
Jul 08 11:03:13 Expected: is <false>
Jul 08 11:03:13      but: was <true>
Jul 08 11:03:13 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:964)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:930)
Jul 08 11:03:13 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor(AkkaRpcActorTest.java:375)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 08 11:03:13 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 08 11:03:13 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 08 11:03:13 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 08 11:03:13 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 08 11:03:13 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 08 11:03:13 
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.107 s - in org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:13 [INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.823 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest
Jul 08 11:03:14 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [INFO] Results:
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [ERROR] Failures: 
Jul 08 11:03:14 [ERROR]   AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor:375 
Jul 08 11:03:14 Expected: is <false>
Jul 08 11:03:14      but: was <true>

{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13277212	Drop vendor specific repositories from pom.xml	"Since Flink no longer bundles Hadoop dependencies we also don't need the vendor specific Hadoop repositories in Flink's {{pom.xml}}. Consequently, I suggest to remove them. 

This idea has been discussed on Flink's dev ML: https://lists.apache.org/thread.html/be402a11bc986219eabd9dd8af507f36f49784d5400d0873e9ec0c2e%40%3Cdev.flink.apache.org%3E."	FLINK	Closed	4	4	10066	pull-request-available
13258744	Distinguish duplicate job submissions from other job submission errors	In order to better handle duplicate job submissions, I propose to add a new exception type {{DuplicateJobSubmissionException}} which inherits from {{JobSubmissionException}} and which is returned if the submitted job is a duplicate.	FLINK	Closed	4	4	10066	pull-request-available
13139005	Make MetricRegistryImpl#shutdown non blocking 	In order to better shut down multiple components concurrently, we should make all shutdown operation non-blocking if possible. This also includes the {{MetricRegistryImpl}}.	FLINK	Closed	3	4	10066	flip-6
13359043	Rename DeclarativeScheduler to AdaptiveScheduler	DeclarativeScheduler does not seem to be an appropriate name for what it does: In particular looking at the difference to the DefaultScheduler, calling it AdaptiveScheduler (as in, adapts the parallelism of the ExecutionGraph during the job lifetime) seems more appropriate.	FLINK	Closed	2	7	10066	pull-request-available
13253500	Set default restart-strategy delay to 1s	"As agreed in [FLIP-62|https://cwiki.apache.org/confluence/display/FLINK/FLIP-62%3A+Set+default+restart+delay+for+FixedDelay-+and+FailureRateRestartStrategy+to+1s], we should set the default delay of all restart strategies to {{""1 s""}}."	FLINK	Closed	3	4	10066	pull-request-available
13186907	Port AbstractTaskManagerProcessFailureRecoveryTest to new code base	Port {{AbstractTaskManagerProcessFailureRecoveryTest}} to new code base.	FLINK	Closed	3	7	10066	pull-request-available
13196124	Resume externalized checkpoint end-to-end test fails	The {{test_resume_externalized_checkpoints.sh}} fails because the log verification does not ignore the artificial failure reason and the {{ExceptionThrowingFailureMapper}} name of a mapper.	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13119890	Cannot submit jobs to YARN Session in FLIP-6 mode	"Cannot submit jobs to YARN Session in FLIP-6 mode because {{FlinkYarnSessionCli}} becomes the _active_ CLI (should be {{Flip6DefaultCLI}}).

*Steps to reproduce*
# Build Flink 1.5 {{101fef7397128b0aba23221481ab86f62b18118f}}
# {{bin/yarn-session.sh -flip6 -d -n 1 -s 1 -jm 1024 -tm 1024}}
# {{bin/flink run -flip6 ./examples/streaming/WordCount.jar}}
# Verify that the job will not run.
"	FLINK	Resolved	1	7	10066	flip-6
13141578	Add MiniClusterClient to allow fast MiniCluster operations	We should offer a {{ClusterClient}} implementation for the {{MiniCluster}}. That way we would be able to submit and wait for result without polling how it would be the case by using the {{RestClusterClient}}.	FLINK	Closed	2	4	10066	flip-6
13186822	Remove legacy entrypoints from startup scripts	Remove the legacy entrypoints from the startup scripts.	FLINK	Resolved	3	7	10066	pull-request-available
13197847	Harden resume from externalized checkpoint E2E test	The resume from externalized checkpoints E2E test can fail due to FLINK-10855. We should harden the test script to not expect a single checkpoint directory being present but to take the checkpoint with the highest checkpoint counter.	FLINK	Closed	2	1	10066	pull-request-available
13351108	Remove SchedulerNG.initialize method	"The {{SchedulerNG}} no longer needs the {{initialize}} method because the {{JobMaster}} is now started with a valid {{MainThreadExecutor}}. Hence, we can simplify the {{SchedulerNG}} by passing the {{ComponentMainThreadExecutor}} to the constructor of the implementations.

Similarly, we can remove the {{SchedulerNG.registerJobStatusListener}} method."	FLINK	Closed	3	4	10066	pull-request-available
13097764	TaskExecutor should filter out duplicate JobManager gained leadership messages	Currently, the {{TaskExecutor}} does not filter out duplicate JobManager gained leadership messages. This causes that multiple {{JobManagerConnections}} are opened which are not properly closed. In order to solve the problem, we should filter out duplicate messages wrt the leader session id.	FLINK	Closed	4	1	10066	flip-6
13116971	Let ClusterOverviewHandler directly extend from AbstractRestHandler	In order to get rid of the {{LegacyRestHandler}} we should add a proper implementation of {{ClusterOverviewHandler}} which extends from {{AbstractRestHandler}}.	FLINK	Closed	4	7	10066	flip-6
13140718	Make shut down of ResourceManagerRunner non blocking	Make the shut down of the {{ResourceManagerRunner}} non blocking. This will allow to shut down the {{MiniCluster}} in a non blocking fashion.	FLINK	Closed	3	4	10066	flip-6
13116977	Generalize existing rest handlers to work with arbitrary RestfulGateway	In order to reuse the existing {{AbstractRestHandler}} we should refactor them such that they work with arbitrary {{RestfulGateway}}.	FLINK	Closed	3	7	10066	flip-6
13196125	connection leak when partition discovery is disabled and open throws exception	"Here is the scenario to reproduce the issue
 * partition discovery is disabled
 * open method throws an exception (e.g. when broker SSL authorization denies request)

In this scenario, run method won't be executed. As a result, _partitionDiscoverer.close()_ won't be called. that caused the connection leak, because KafkaConsumer is initialized but not closed. That has caused outage that brought down our Kafka cluster, when a high-parallelism job got into a restart/failure loop."	FLINK	Resolved	3	1	10066	pull-request-available
13259895	Extract JobGraphWriter from JobGraphStore	In order to follow the ISP, I suggest to extract a {{JobGraphWriter}} interface from the {{JobGraphStore}}. The background is that in the future, the {{Dispatcher}} does not need to recover jobs anymore and, hence, it should not know the recovery related methods of the {{JobGraphStore}}. Instead, the only methods it needs to know are {{putJobGraph}}, {{removeJobGraph}} and {{releaseJobGraph}}.	FLINK	Closed	4	4	10066	pull-request-available
13122889	Failing JobManagerLeaderSessionIDITCase on Travis	"The {{JobManagerLeaderSessionIDITCase}} fails on Travis.

https://travis-ci.org/apache/flink/jobs/311502350"	FLINK	Closed	2	1	10066	test-stability
13133041	Integrate queryable state with Flip-6	In order to make Flip-6 support queryable state, we have to start and register the {{KvStateServer}} and {{KvStateProxyClient}} in the {{TaskExecutor}}.	FLINK	Closed	3	4	10066	flip-6
13115325	Add support for scheduling with slot sharing	In order to reach feature equivalence with the old code base, we should add support for scheduling with slot sharing to the {{SlotPool}}. This will also allow us to run all the IT cases based on the {{AbstractTestBase}} on the Flip-6 {{MiniCluster}}.	FLINK	Closed	3	7	10066	flip-6
13339202	Job fails when stopping JobMaster	"When a {{JobMaster}} is stopped, we first disconnect all {{TaskExecutors}}. This disconnection causes potentially running {{Executions}} to fail. This in turn can cause a restart of the job or in the worst case a transition into {{FAILED}} state if the restarts are depleted. This again can cause the clean up of HA data.

Instead of failing the job, the job should be suspended if the {{JobMaster}} gets stopped because this happens if the {{Dispatcher}} loses its leadership. The problem has been fixed unintentionally by FLINK-19237 in the master branch."	FLINK	Closed	2	1	10066	pull-request-available
13374537	UnalignedCheckpointITCase failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17052&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=9442


{code:java}
Apr 22 14:28:21 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
Apr 22 14:28:21 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
Apr 22 14:28:21 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Apr 22 14:28:21 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Apr 22 14:28:21 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
Apr 22 14:28:21 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
Apr 22 14:28:21 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
Apr 22 14:28:21 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
Apr 22 14:28:21 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
Apr 22 14:28:21 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
Apr 22 14:28:21 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
Apr 22 14:28:21 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Apr 22 14:28:21 Caused by: org.apache.flink.util.FlinkException: An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: '[NoMoreSplitEvent]', targetTask: Source: source (1/1) - execution #5
Apr 22 14:28:21 	... 26 more
Apr 22 14:28:21 

{code}


As described in the comment https://issues.apache.org/jira/browse/FLINK-21996?focusedCommentId=17326449&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17326449 we might need to adjust the tests  to allow failover."	FLINK	Closed	4	1	10066	auto-deprioritized-major, pull-request-available, test-stability
13097922	Akka hostnames are not normalised consistently	"In {{NetUtils.unresolvedHostToNormalizedString()}} we lowercase hostnames, Akka seems to preserve the uppercase/lowercase distinctions when starting the Actor. This leads to problems because other parts (for example {{JobManagerRetriever}}) cannot find the actor leading to a nonfunctional cluster.

h1. Original Issue Text

Hostnames in my  hadoop cluster are like these: “DSJ-RTB-4T-177”,” DSJ-signal-900G-71”
When using the following command:
./bin/flink run -m yarn-cluster -yn 1 -yqu xl_trip -yjm 1024 ~/flink-1.3.1/examples/batch/WordCount.jar --input /user/all_trip_dev/test/testcount.txt --output /user/all_trip_dev/test/result  
Or
./bin/yarn-session.sh -d -jm 6144  -tm 12288 -qu xl_trip -s 24 -n 5 -nm ""flink-YarnSession-jm6144-tm12288-s24-n5-xl_trip""
There will be some exceptions at Command line interface:

java.lang.RuntimeException: Unable to get ClusterClient status from Application Client
at org.apache.flink.yarn.YarnClusterClient.getClusterStatus(YarnClusterClient.java:243)
…
Caused by: org.apache.flink.util.FlinkException: Could not connect to the leading JobManager. Please check that the JobManager is running.

h4. Then the job fails , starting the yarn-session is the same.

The exceptions of the application log:
2017-08-10 17:36:10,334 WARN  org.apache.flink.runtime.webmonitor.JobManagerRetriever       - Failed to retrieve leader gateway and port.
akka.actor.ActorNotFound: Actor not found for: ActorSelection[Anchor(akka.tcp://flink@dsj-signal-4t-248:65082/), Path(/user/jobmanager)]
…
2017-08-10 17:36:10,837 ERROR org.apache.flink.yarn.YarnFlinkResourceManager                - Resource manager could not register at JobManager
akka.pattern.AskTimeoutException: Ask timed out on [ActorSelection[Anchor(akka.tcp://flink@dsj-signal-4t-248:65082/), Path(/user/jobmanager)]] after [10000 ms]


And I found some differences in actor System:
2017-08-10 17:35:56,791 INFO  org.apache.flink.yarn.YarnJobManager                          - Starting JobManager at akka.tcp://flink@DSJ-signal-4T-248:65082/user/jobmanager.
2017-08-10 17:35:56,880 INFO  org.apache.flink.yarn.YarnJobManager                          - JobManager akka.tcp://flink@DSJ-signal-4T-248:65082/user/jobmanager was granted leadership with leader session ID Some(00000000-0000-0000-0000-000000000000).
2017-08-10 17:36:00,312 INFO  org.apache.flink.runtime.webmonitor.WebRuntimeMonitor         - Web frontend listening at 0:0:0:0:0:0:0:0:54921
2017-08-10 17:36:00,312 INFO  org.apache.flink.runtime.webmonitor.WebRuntimeMonitor         - Starting with JobManager akka.tcp://flink@DSJ-signal-4T-248:65082/user/jobmanager on port 54921
2017-08-10 17:36:00,313 INFO  org.apache.flink.runtime.webmonitor.JobManagerRetriever       - New leader reachable under akka.tcp://flink@dsj-signal-4t-248:65082/user/jobmanager:00000000-0000-0000-0000-000000000000.


The JobManager is  “akka.tcp://flink@DSJ-signal-4T-248:65082” and the JobManagerRetriever is “akka.tcp://flink@dsj-signal-4t-248:65082”
The hostname of JobManagerRetriever’s actor is lowercase.


And I read source code,
Class NetUtils the unresolvedHostToNormalizedString(String host) method of line 127:
	public static String unresolvedHostToNormalizedString(String host) { 		
// Return loopback interface address if host is null 		
// This represents the behavior of {@code InetAddress.getByName } and RFC 3330 		if (host == null) { 			
   host = InetAddress.getLoopbackAddress().getHostAddress(); 		
} else { 			host = host.trim().toLowerCase(); 		}
...
}


It turns the host name into lowercase.
Therefore, JobManagerRetriever certainly can not find Jobmanager's actorSYstem.
Then I removed the call to the toLowerCase() method in the source code.

Finally ,I can submit a job in yarn-cluster mode and start a yarn-session.


"	FLINK	Resolved	1	1	10066	patch
13284798	Improve error reporting when submitting batch job (instead of AskTimeoutException)	"While debugging the {{Shaded Hadoop S3A end-to-end test (minio)}} pre-commit test, I noticed that the JobSubmission is not producing very helpful error messages.

Environment:
- A simple batch wordcount job 
- a unavailable minio s3 filesystem service

What happens from a user's perspective:
- The job submission fails after 10 seconds with a AskTimeoutException:
{code}
2020-02-07T11:38:27.1189393Z akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-939201095]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-02-07T11:38:27.1189538Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-02-07T11:38:27.1189616Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-02-07T11:38:27.1189713Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-02-07T11:38:27.1189789Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-02-07T11:38:27.1189883Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
2020-02-07T11:38:27.1189973Z 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
2020-02-07T11:38:27.1190067Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
2020-02-07T11:38:27.1190159Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2020-02-07T11:38:27.1190267Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2020-02-07T11:38:27.1190358Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
2020-02-07T11:38:27.1190465Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-02-07T11:38:27.1190540Z 	at java.lang.Thread.run(Thread.java:748)
{code}

What a user would expect:
- An error message indicating why the job submission failed.
"	FLINK	Resolved	1	4	10066	pull-request-available
13118476	Decouple Execution from actual slot implementation	In order to plug in a different slot implementation, we should introduce a slot interface which abstracts away the implementation details of {{SimpleSlot}} wrt {{Execution}}. The reason this is necessary is to provide a simpler slot implementation for Flip-6 since all allocation/release logic will go through the {{SlotPool}}. Thus, we no longer need the concurrent structure of {{Slot}}, {{SharedSlot}}, {{SimpleSlot}} and {{SlotSharingGroupAssignment}}.	FLINK	Closed	3	4	10066	flip-6
13130513	Timeout exceptions are not properly recognized by RetryingRegistration	The {{RetryingRegistration}} does not correctly respond to {{TimeoutExceptions}} and instead treats them like errors. This causes that it waits for the delay on error instead of backing exponentially off.	FLINK	Closed	3	1	10066	flip-6
13307368	Cannot run mvn clean verify flink-yarn-tests	As part of FLINK-11086, we introduced the setting of the yarn class path in a static initializer of {{YarnTestBase.java:199}}. The yarn class path file will be generated by the {{maven-dependency-plugin}} in the {{package}} phase. Due to this, the {{yarn.classpath}} file won't be accessible to all users of the {{YarnTestBase}} class which are run in a previous phase (e.g. {{UtilsTest.testUberjarLocator}}).	FLINK	Closed	2	1	10066	pull-request-available
13294511	Losing leadership does not clear rpc connection in JobManagerLeaderListener	"When losing the leadership the {{JobManagerLeaderListener}} closes the current {{rpcConnection}} but does not clear the field. This can lead to a failure of {{JobManagerLeaderListener#reconnect}} if this method is called after the {{JobMaster}} has lost its leadership.

I propose to clear the field so that {{RegisteredRpcConnection#tryReconnect}} won't be called on a closed rpc connection."	FLINK	Closed	3	1	10066	pull-request-available
13259527	Introduce DispatcherRunner#getShutDownFuture	I suggest to extend the {{DispatcherRunner}} interface to return a shut down future. The idea of the shut down future is that it gets completed once the runner intends to be shut down by its owner.	FLINK	Closed	4	4	10066	pull-request-available
13425810	Add README.md to flink-statefun/statefun-sdk-js	"We should add a {{README.md}} to {{flink-statefun/statefun-sdk-js}}. This would then also be displayed on [npmjs.com|https://www.npmjs.com/package/apache-flink-statefun].

Unfortunately, in order to publish the {{README.md}} we have to release a new version of the npm package."	FLINK	Closed	3	4	10066	pull-request-available
13111427	Add Flip6 build profile	In order to separate Flip-6 related from non Flip-6 related test cases we should introduce a flip-6 build profile which runs only the flip-6 related tests. I suggest to use JUnit's {{Category}} for that.	FLINK	Closed	3	4	10066	flip-6
13217243	Remove JobMaster#start(JobMasterId) and #suspend	Currently, the {{JobMaster}} contains a lot of mutable state which is necessary because it is used across different leadership sessions by the {{JobManagerRunner}}. For this purpose, we have the methods {{JobMaster#start(JobMasterId)}} and {{#suspend}}. The mutable state management makes things on the {{JobMaster}} side more complicated than they need to be. In order to improve the {{JobMaster's}} maintainability I suggest to remove this logic and instead terminate the {{JobMaster}} if the {{JobManagerRunner}} loses leadership. This entails that for every leadership we will create a new {{JobMaster}} instance.	FLINK	Closed	3	4	10066	pull-request-available
13198336	Submitting a jobs without enough slots times out due to a unspecified timeout	"When submitting a job without enough slots being available the job will stay in a SCHEDULED/CREATED state. After some time (a few minutes) the job execution will fail with the following timeout exception:
{code}
2018-11-14 13:38:26,614 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPool          - Pending slot request [SlotRequestId{d9c0c94b6b81eae406f3d6cb6150fee4}] timed out.
2018-11-14 13:38:26,615 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN DataSource (at getDefaultTextLineDataSet(WordCountData.java:70) (org.apache.flink.api.java.io.CollectionInputFormat)) -> FlatMap (FlatMap at main(WordCount.java:76)) -> Combine (SUM(1), at main(WordCount.java:79) (1/$java.util.concurrent.TimeoutException
        at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:795)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}

That the job submission may time out is not documented, neither is which timeout is responsible in the first place nor how/whether this can be disabled."	FLINK	Resolved	3	4	10066	pull-request-available
13186823	Remove legacy mode testing profiles from Travis config	Remove the legacy mode testing profiles from Travis config.	FLINK	Closed	3	7	10066	pull-request-available
13200570	Define flink-sql-client uber-jar dependencies via artifactSet	"The module {{flink-sql-client}} defines the content of its uber jar via filtering files from the set of all dependencies. I think this is not ideal because it misses for example the {{NOTICE}} files from down stream dependencies. 

A solution could be to define an {{<artifactSet><includes><include></include></includes></artifactSet>}} and exclude files via the filter."	FLINK	Closed	3	4	10066	pull-request-available
13137752	Pass JobManagerSharedServices to JobMaster	In order to pass in the {{JobMaster}} services we should use a {{JobManagerServices}} instance. That way we don't have to pass all services to the constructor individually.	FLINK	Closed	4	4	10066	flip-6
13379751	MesosWorkerStore is started with an illegal namespace	The MesosWorkerStore is started with an illegal namespace because of FLINK-22636.	FLINK	Closed	2	1	10066	pull-request-available
13224005	Let ResultConjunctFuture return future results in same order as futures	"The {{ResultConjunctFuture}} should return future results in the same order as the futures were specified. The advantage would be that we maintain the same output order as the input order if the input has been specified with a special ordering.

This should also fix a current performance regression where we don't maintain the topological ordering when deploying tasks. Due to this, we need to make an additional result partition lookup which adds additional latency. The problem has occurred due to FLINK-10431 which changes the interleaving how the slot futures are completed."	FLINK	Closed	3	4	10066	pull-request-available
13138205	Introduce JobMasterConfiguration	The {{JobMaster}} already contains some configuration settings which we pass as constructor arguments. In order to make it better maintainable, I suggest to add a {{JobMasterConfiguration}} object similar to the {{TaskManagerConfiguration}}. This object will contain all {{JobMaster}} specific configuration settings.	FLINK	Closed	3	4	10066	flip-6
13146902	SlotPool can fail to release slots	The {{SlotPool}} releases idling slots. If the release operation fails (e.g. timeout), then it simply continues using the slot. This is problematic if the owning {{TaskExecutor}} failed before and was unregistered in the meantime from the {{SlotPool}}. As a result, the {{SlotPool}} will reuse the slot and whenever it tries to return because it is idling it will fail again. This, effectively, renders the scheduling of a job impossible.	FLINK	Closed	1	1	10066	flip-6
13097293	Set JobMaster leader session id in main thread	Currently, the {{JobMaster}} leader id is set via an {{AtomicReferenceUpdater}}. In order to make it work with the new {{FencedRpcEndpoint}} it should be set in the main thread, because the former only allows to modify the leader session id from the main thread.	FLINK	Closed	4	7	10066	flip-6
13426270	Modernize statefun playground examples	"It is about time to touch up abit the examples in playground.

Most of the docker-compose/docker files are pretty old and there are a lot of room for improvement.
 # use redpanda instead of kafka+zk - from local experiments it seems to cut the start time and the memory requirements significantly. In addition it also comes with a REST proxy, which can improve the interactivity with the examples quite a lot.
 # For the Java examples, there is no reason to use java8 for the remote functions. We can use at least 11, if not higher.
 # Replace the pair of a JobManager+TaskManager by a simple Minicluster "	FLINK	Closed	3	4	10066	pull-request-available
13388528	HAQueryableStateRocksDBBackendITCase failed due to heap OOM	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=14431

{code}
Jul 08 21:43:22 [ERROR] Tests run: 12, Failures: 0, Errors: 9, Skipped: 1, Time elapsed: 246.345 s <<< FAILURE! - in org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase
Jul 08 21:43:22 [ERROR] testReducingState(org.apache.flink.queryablestate.itcases.HAQueryableStateRocksDBBackendITCase)  Time elapsed: 241.454 s  <<< ERROR!
Jul 08 21:43:22 java.lang.OutOfMemoryError: Java heap space
{code}"	FLINK	Closed	3	1	10066	auto-deprioritized-critical, test-stability
13337309	Flink restored from a wrong checkpoint (a very old one and not the last completed one)	"h2. Summary

Upon failure, it seems that Flink didn't restore from the last completed checkpoint. Instead, it restored from a very old checkpoint. As a result, Kafka offsets are invalid and caused the job to replay from the beginning as Kafka consumer ""auto.offset.reset"" was set to ""EARLIEST"".

This is an embarrassingly parallel stateless job. Parallelism is over 1,000. I have the full log file from jobmanager at INFO level available upon request.

h2. Sequence of events from the logs

Just before the failure, checkpoint *210768* completed.

{code}
2020-10-25 02:35:05,970 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [jobmanager-future-thread-5] - Completed checkpoint 210768 for job 233b4938179c06974e4535ac8a868675 (4623776 bytes in 120402 ms).
{code}

During restart, somehow it decided to restore from a very old checkpoint *203531*.
{code:java}
2020-10-25 02:36:03,301 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-3]  - Start SessionDispatcherLeaderProcess.
2020-10-25 02:36:03,302 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-5]  - Recover all persisted job graphs.
2020-10-25 02:36:03,304 INFO  com.netflix.bdp.s3fs.BdpS3FileSystem                         [cluster-io-thread-25]  - Deleting path: s3://<bucket>/checkpoints/XM3B/clapp_avro-clapp_avro_nontvui/1593/233b4938179c06974e4535ac8a868675/chk-210758/c31aec1e-07a7-4193-aa00-3fbe83f9e2e6
2020-10-25 02:36:03,307 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [cluster-io-thread-5]  - Trying to recover job with job id 233b4938179c06974e4535ac8a868675.

2020-10-25 02:36:03,381 INFO  com.netflix.bdp.s3fs.BdpS3FileSystem                         [cluster-io-thread-25]  - Deleting path: s3://<bucket>/checkpoints/Hh86/clapp_avro-clapp_avro_nontvui/1593/233b4938179c06974e4535ac8a868675/chk-210758/4ab92f70-dfcd-4212-9b7f-bdbecb9257fd
...
2020-10-25 02:36:03,427 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Recovering checkpoints from ZooKeeper.
2020-10-25 02:36:03,432 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Found 0 checkpoints in ZooKeeper.
2020-10-25 02:36:03,432 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore [flink-akka.actor.default-dispatcher-82003]  - Trying to fetch 0 checkpoints from storage.
2020-10-25 02:36:03,432 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [flink-akka.actor.default-dispatcher-82003]  - Starting job 233b4938179c06974e4535ac8a868675 from savepoint s3://<bucket>/checkpoints/metadata/clapp_avro-clapp_avro_nontvui/1113/47e2a25a8d0b696c7d0d423722bb6f54/chk-203531/_metadata ()
{code}

"	FLINK	Closed	2	1	10066	pull-request-available
13108554	Move CurrentJobsOverviewHandler to jobs/overview	The {{CurrentJobsOverviewHandler}} is currently registered under {{/joboverview}}. I think it would be more idiomatic to register it under {{/jobs/overview}}.	FLINK	Closed	4	7	10066	flip-6
13140242	Release slots when scheduling operation is canceled in ExecutionGraph	In order to quickly release slots, we should explicitly return them to the {{SlotProvider}} if the scheduling operation is cancelled in the {{ExecutionGraph}}.	FLINK	Closed	3	4	10066	flip-6
13361552	Elasticsearch6DynamicSinkITCase.testWritingDocuments fails when submitting job	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13868&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361

{code}
2021-02-27T00:16:06.9493539Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-02-27T00:16:06.9494494Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-02-27T00:16:06.9495733Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:117)
2021-02-27T00:16:06.9496596Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-02-27T00:16:06.9497354Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-02-27T00:16:06.9525795Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-27T00:16:06.9526744Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-02-27T00:16:06.9527784Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
2021-02-27T00:16:06.9528552Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-02-27T00:16:06.9529271Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-02-27T00:16:06.9530013Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-27T00:16:06.9530482Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-02-27T00:16:06.9531068Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1046)
2021-02-27T00:16:06.9531544Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2021-02-27T00:16:06.9531908Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-02-27T00:16:06.9532449Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-02-27T00:16:06.9532860Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-02-27T00:16:06.9533245Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2021-02-27T00:16:06.9533721Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-02-27T00:16:06.9534225Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2021-02-27T00:16:06.9534697Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2021-02-27T00:16:06.9535217Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2021-02-27T00:16:06.9535718Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2021-02-27T00:16:06.9536127Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:573)
2021-02-27T00:16:06.9536861Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2021-02-27T00:16:06.9537394Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2021-02-27T00:16:06.9537916Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2021-02-27T00:16:06.9605804Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2021-02-27T00:16:06.9606794Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2021-02-27T00:16:06.9607642Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2021-02-27T00:16:06.9608419Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2021-02-27T00:16:06.9609252Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)
2021-02-27T00:16:06.9610024Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2021-02-27T00:16:06.9613676Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2021-02-27T00:16:06.9615526Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)
2021-02-27T00:16:06.9616727Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2021-02-27T00:16:06.9617826Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2021-02-27T00:16:06.9618940Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-02-27T00:16:06.9620109Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-02-27T00:16:06.9621415Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-02-27T00:16:06.9622598Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-02-27T00:16:06.9623716Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-02-27T00:16:06.9625006Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118)
2021-02-27T00:16:06.9626398Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80)
2021-02-27T00:16:06.9628020Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:233)
2021-02-27T00:16:06.9629257Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:224)
2021-02-27T00:16:06.9630622Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:215)
2021-02-27T00:16:06.9631835Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:669)
2021-02-27T00:16:06.9633415Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:56)
2021-02-27T00:16:06.9634940Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1869)
2021-02-27T00:16:06.9636193Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1437)
2021-02-27T00:16:06.9637220Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1377)
2021-02-27T00:16:06.9638462Z 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1205)
2021-02-27T00:16:06.9639683Z 	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$11(Execution.java:856)
2021-02-27T00:16:06.9640771Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-02-27T00:16:06.9641839Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-02-27T00:16:06.9643554Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-02-27T00:16:06.9644658Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-02-27T00:16:06.9645998Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-02-27T00:16:06.9647143Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-02-27T00:16:06.9648506Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-02-27T00:16:06.9649340Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-02-27T00:16:06.9650021Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-02-27T00:16:06.9650741Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-02-27T00:16:06.9651406Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-02-27T00:16:06.9652093Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-02-27T00:16:06.9652972Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-27T00:16:06.9653685Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-02-27T00:16:06.9654385Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-02-27T00:16:06.9655010Z 	at akka.actor.Actor.aroundReceive(Actor.scala:517)
2021-02-27T00:16:06.9655606Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
2021-02-27T00:16:06.9656223Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-02-27T00:16:06.9656910Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-02-27T00:16:06.9739719Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-02-27T00:16:06.9740802Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-02-27T00:16:06.9741521Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-02-27T00:16:06.9742086Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-02-27T00:16:06.9742776Z 	... 4 more
2021-02-27T00:16:06.9743982Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.
2021-02-27T00:16:06.9745460Z 	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
2021-02-27T00:16:06.9746212Z 	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
2021-02-27T00:16:06.9746961Z 	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)
2021-02-27T00:16:06.9747806Z 	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)
2021-02-27T00:16:06.9748553Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-27T00:16:06.9749330Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-02-27T00:16:06.9750210Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234)
2021-02-27T00:16:06.9751031Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-02-27T00:16:06.9751954Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-02-27T00:16:06.9752836Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-02-27T00:16:06.9753594Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2021-02-27T00:16:06.9754400Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1044)
2021-02-27T00:16:06.9755076Z 	at akka.dispatch.OnComplete.internal(Future.scala:263)
2021-02-27T00:16:06.9755623Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2021-02-27T00:16:06.9756221Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2021-02-27T00:16:06.9756841Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2021-02-27T00:16:06.9757772Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2021-02-27T00:16:06.9758524Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
2021-02-27T00:16:06.9759315Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2021-02-27T00:16:06.9760053Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2021-02-27T00:16:06.9760865Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2021-02-27T00:16:06.9761785Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2021-02-27T00:16:06.9762565Z 	at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:650)
2021-02-27T00:16:06.9763213Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2021-02-27T00:16:06.9763902Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
2021-02-27T00:16:06.9764625Z 	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109)
2021-02-27T00:16:06.9765323Z 	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
2021-02-27T00:16:06.9766035Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
2021-02-27T00:16:06.9766812Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2021-02-27T00:16:06.9767768Z 	at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2021-02-27T00:16:06.9768801Z 	at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:283)
2021-02-27T00:16:06.9769614Z 	at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:235)
2021-02-27T00:16:06.9770261Z 	at java.lang.Thread.run(Thread.java:748)
2021-02-27T00:16:06.9771578Z Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.taskexecutor.TaskExecutorGateway.submitTask(org.apache.flink.runtime.deployment.TaskDeploymentDescriptor,org.apache.flink.runtime.jobmaster.JobMasterId,org.apache.flink.api.common.time.Time) timed out.
2021-02-27T00:16:06.9773056Z 	at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:68)
2021-02-27T00:16:06.9773869Z 	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$10(Execution.java:832)
2021-02-27T00:16:06.9774669Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2021-02-27T00:16:06.9775423Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-02-27T00:16:06.9776076Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-02-27T00:16:06.9776887Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2021-02-27T00:16:06.9777916Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2021-02-27T00:16:06.9778756Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-02-27T00:16:06.9779504Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-02-27T00:16:06.9780025Z 	... 1 more
2021-02-27T00:16:06.9782426Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_12#1583248815]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2021-02-27T00:16:06.9783716Z 	at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:635)
2021-02-27T00:16:06.9784445Z 	at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:650)
2021-02-27T00:16:06.9785102Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2021-02-27T00:16:06.9785957Z 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
2021-02-27T00:16:06.9786701Z 	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109)
2021-02-27T00:16:06.9787410Z 	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
2021-02-27T00:16:06.9788182Z 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
2021-02-27T00:16:06.9788982Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2021-02-27T00:16:06.9789866Z 	at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2021-02-27T00:16:06.9790699Z 	at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:283)
2021-02-27T00:16:06.9791644Z 	at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:235)
2021-02-27T00:16:06.9792194Z 	... 1 more
{code}"	FLINK	Closed	4	11500	10066	auto-deprioritized-major, auto-unassigned, pull-request-available, test-stability
13397715	Dispatcher does not log JobMaster initialization error on info level	The {{Dispatcher}} does not log JobMaster initialization errors. This can make it very hard to understand why a job has failed if the client does not receive the {{JobResult}}. Therefore, I propose to log the failure cause for a job when it finishes on the {{Dispatcher}}.	FLINK	Closed	3	1	10066	pull-request-available
13173686	Non-queued scheduling failure sometimes does not return the slot	Similar to FLINK-9908, it can happen that in case of a non-queued scheduling failure a slot is not properly returned to the {{SlotPool}}. The reason for the failure seems to be the exceptional completion of the {{allocationFuture}} in {{Execution#scheduleForExecution}}.	FLINK	Closed	1	1	10066	pull-request-available
12781666	Add polynomial base feature mapper to ML library	Add feature mapper which maps a vector into the polynomial feature space. This can be used as a preprocessing step prior to applying a {{Learner}} of Flink's ML library.	FLINK	Closed	3	4	10066	ML
13137191	Add result future to JobManagerRunner	Adding a {{CompletableFuture<ArchivedExecutionGraph>}} result future to the {{JobManagerRunner}} will allow to return a {{JobResult}} future for an still running job. This is helpful for the implementation of a non-detached job mode.	FLINK	Closed	4	4	10066	flip-6
12782794	Add support to read libSVM and SVMLight input files	"In order to train SVMs, the machine learning library should be able to read standard SVM input file formats. A widespread format is used by libSVM and SMVLight which has the following format:

<line> .=. <target> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>
<target> .=. +1 | -1 | 0 | <float> 
<feature> .=. <integer> | ""qid""
<value> .=. <float>
<info> .=. <string>

Details can be found [here|http://svmlight.joachims.org/] and [here|http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#/Q03:_Data_preparation]"	FLINK	Closed	3	2	10066	ML
13243849	Complete requested slots in request order	When executing batch jobs with fewer slots than requested we should make sure that the slot requests are being completed in the order in which they were enqueued into the {{SlotPool}}. Otherwise we might risk that a consumer task gets deployed before a producer causing a resource deadlock situation.	FLINK	Closed	3	7	10066	pull-request-available
13139933	Cancel scheduling operation when cancelling the ExecutionGraph	"With the Flip-6 changes and the support for queued scheduling, the {{ExecutionGraph}} must be able to handle cancellation calls when it is not yet fully scheduled. This is for example the case when waiting for new containers.

A cancellation will cancel all {{Executions}}. As a result, available slots can get assigned to other {{Executions}} (already canceled). Since the slot cannot be assigned to this slot because it's already canceled, this can fail the overall eager scheduling operation. The scheduling result callback will then trigger a global fail operation. This can happen before all {{Executions}} have been released and, thus, when the {{ExecutionGraph}} is still in the state {{CANCELLING}}. The result is that the {{ExecutionGraph}} goes into the state {{FAILING}} and then {{FAILED}}.

In order to solve this problem, I propose to keep track of the scheduling operation and cancelling the result future when a concurrent {{suspend}}, {{cancel}} or {{fail}} call happens."	FLINK	Closed	3	1	10066	flip-6
13121680	Remove work arounds in Flip6LocalStreamEnvironment	After adding FLINK-7956, it is no longer necessary that the {{Flip6LocalStreamEnvironment}} waits for the registration of TaskManagers before submitting a job. Moreover, it is also possible to use slot sharing when submitting jobs.	FLINK	Closed	3	4	10066	flip-6
13097584	Add termination future to ClusterEntrypoint	In order to wait for the termination of {{ClusterEntrypoint}}, e.g. when testing it, a termination future would be helpful.	FLINK	Closed	4	4	10066	flip-6
13305397	Verbose client error messages	"Some client operations if they fail produce very verbose error messages which are hard to decipher for the user. For example, if the job submission fails because the savepoint path does not exist, then the user sees the following:

{code}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:148)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:689)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:227)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:906)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:982)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:982)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:290)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1766)
        at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:104)
        at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:71)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1645)
        at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)
        ... 8 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1761)
        ... 17 more
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:366)
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
        at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:290)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:343)
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
        ... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:149)
        at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:386)
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
        ... 7 more
Caused by: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'file:///does/not/exist' on file system 'file'.
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpointPointer(AbstractFsCheckpointStorage.java:243)
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorage.resolveCheckpoint(AbstractFsCheckpointStorage.java:110)
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1300)
        at org.apache.flink.runtime.scheduler.SchedulerBase.tryRestoreExecutionGraphFromSavepoint(SchedulerBase.java:299)
        at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:252)
        at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:228)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:119)
        at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:103)
        at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:284)
        at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:272)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
        at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:143)
        ... 10 more
End of exception on server side>]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:390)
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:374)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
        ... 4 more
{code}

We might be able to remove a lot of the clutter by introducing a special exception which represents an exception which should be reported back to the user and which is then not rewrapped as it bubbles up the call stack."	FLINK	Closed	2	1	10066	pull-request-available
13129645	Reconnect to last known JobMaster when connection is lost	In case of a connection loss to the {{JobMaster}}, e.g. due to a heartbeat timeout or a disconnect call, then the {{TaskExecutor}} should try to reconnect to the last known {{JobMaster}} location in case that the timeout was a false positive.	FLINK	Closed	3	1	10066	flip-6
13302805	Send flink-shaded issues and pull request notifications to issues@flink.apache.org	Similar to FLINK-17512, we should send issues and pull request notifications to issues@flink.apache.org instead of dev@flink.apache.org.	FLINK	Closed	3	4	10066	pull-request-available
13390367	MiniClusterITCase.testHandleBatchJobsWhenNotEnoughSlot fails on Azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20588&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=8065

{code}
[ERROR] Tests run: 17, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 14.001 s <<< FAILURE! - in org.apache.flink.runtime.minicluster.MiniClusterITCase
[ERROR] testHandleBatchJobsWhenNotEnoughSlot(org.apache.flink.runtime.minicluster.MiniClusterITCase)  Time elapsed: 0.524 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.runtime.minicluster.MiniClusterITCase.testHandleBatchJobsWhenNotEnoughSlot(MiniClusterITCase.java:140)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13370079	Remove console logging for Kafka connector for AZP runs	For the Kafka connector we do log to the console. These logging statements clutter the AZP output considerably. I propose to remove this logic. Moreover, we still have some DEBUG logging for FLINK-16383 which has been fixed.	FLINK	Closed	4	4	10066	pull-request-available
13103978	Allow AbstractRestHandler to handle bad requests	The {{AbstractRestHandler}} parses the request and tries to generate a {{HandlerRequest}}. If this fails, then the server answers with an internal server error. Instead we should allow the {{AbstractRestHandler}} to be able to return a BAD_REQUEST status code. In order to do that, I would like to introduce a {{HandlerRequestException}} which can be thrown while creating the {{HandlerRequest}}. If this exception is thrown, then we return an error message with {{BAD_REQUEST}} {{HttpResponseStatus}}.	FLINK	Closed	3	1	10066	flip-6
13216667	Remove control flow break point from Execution#releaseAssignedResource	"In {{Execution#releaseAssignedResource}} we release the assigned resource by calling {{LogicalSlot#releaseSlot}} and use {{FutureUtils.whenCompleteAsyncIfNotDone}} to merge the future back into the main thread in order to complete the {{Execution#releaseFuture}}. This is no longer necessary since the returned future is always completed from within the main thread (with the changes from FLINK-10431).

In fact this control flow break point makes it hard to properly suspend the {{ExecutionGraph}} atomically as required for FLINK-11537."	FLINK	Closed	3	4	10066	pull-request-available
13127933	Let CustomCommandLine return a ClusterDescriptor	The {{CustomCommandLine}} currently is able to retrieve a {{ClusterClient}} and deploy a cluster. In order to better separate concerns it would be good if the {{CustomCommandLine}} would simply return a {{ClusterDescriptor}} which could then be used to retrieve a {{ClusterClient}} or to deploy a Flink cluster. 	FLINK	Closed	3	7	10066	flip-6
13107496	Test instability in UtilsTest#testYarnFlinkResourceManagerJobManagerLostLeadership	"{{UtilsTest#testYarnFlinkResourceManagerJobManagerLostLeadership}} may result in the following exception (repeated run in IntelliJ until failure, but also on Travis here: https://travis-ci.org/NicoK/flink/jobs/283696974 )

{code}
org.apache.flink.yarn.UtilsTest ""Until Failure""

org.mockito.exceptions.misusing.UnfinishedStubbingException: 
Unfinished stubbing detected here:
-> at org.apache.flink.yarn.UtilsTest$1.<init>(UtilsTest.java:171)

E.g. thenReturn() may be missing.
Examples of correct stubbing:
    when(mock.isOk()).thenReturn(true);
    when(mock.isOk()).thenThrow(exception);
    doThrow(exception).when(mock).someVoidMethod();
Hints:
 1. missing thenReturn()
 2. you are trying to stub a final method, you naughty developer!
 3: you are stubbing the behaviour of another mock inside before 'thenReturn' instruction if completed


	at org.apache.flink.yarn.UtilsTest$1.<init>(UtilsTest.java:179)
	at org.apache.flink.yarn.UtilsTest.testYarnFlinkResourceManagerJobManagerLostLeadership(UtilsTest.java:95)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:67)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}

The incriminating code is this:
{code}
doAnswer(new Answer() {
	@Override
	public Object answer(InvocationOnMock invocation) throws Throwable {
		Container container = (Container) invocation.getArguments()[0];
		resourceManagerGateway.tell(new NotifyResourceStarted(YarnFlinkResourceManager.extractResourceID(container)),
			leader1Gateway);
		return null;
	}
}).when(nodeManagerClient).startContainer(Matchers.any(Container.class), Matchers.any(ContainerLaunchContext.class));
{code}"	FLINK	Resolved	2	1	10066	test-stability
12996092	Implement new TaskManager	"This is the parent issue for the efforts to implement the {{TaskManager}} changes based on FLIP-6 (https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077)

Because of the breadth of changes, we should implement a new version of the {{TaskManager}} (let's call it {{TaskExecutor}}) rather than updating the current {{TaskManager}}. That will allow us to keep a working master branch.

At the point when the new cluster management is on par with the current implementation, we will drop the old {{TaskManager}} and rename the {{TaskExecutor}} to {{TaskManager}}."	FLINK	Resolved	3	2	10066	flip-6
13253709	Migrate restart strategy config constants to RestartStrategyOptions	In order to improve detectability of restart strategy related config constants I propose to migrate them to {{ConfigOptions}}. For that to happen we should create a {{RestartStrategyOptions}} class which contains {{ConfigOptions}} for all restart strategy related config constants.	FLINK	Closed	4	4	10066	pull-request-available
13423956	YARNSessionFIFOSecuredITCase.testDetachedMode fails on AZP	"The test {{YARNSessionFIFOSecuredITCase.testDetachedMode}} fails on AZP:

{code}
2022-01-21T03:28:18.3712993Z Jan 21 03:28:18 java.lang.AssertionError: 
2022-01-21T03:28:18.3715115Z Jan 21 03:28:18 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-0_0/application_1642735639007_0002/container_1642735639007_0002_01_000001/jobmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2022-01-21T03:28:18.3716389Z Jan 21 03:28:18 [
2022-01-21T03:28:18.3717531Z Jan 21 03:28:18 2022-01-21 03:27:56,921 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2022-01-21T03:28:18.3720496Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@11c5f741db81:37697/user/rpc/dispatcher_0.
2022-01-21T03:28:18.3722401Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Interrupted while waiting for queue
2022-01-21T03:28:18.3723661Z Jan 21 03:28:18 java.lang.InterruptedException: null
2022-01-21T03:28:18.3724529Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014) ~[?:1.8.0_292]
2022-01-21T03:28:18.3725450Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048) ~[?:1.8.0_292]
2022-01-21T03:28:18.3726239Z Jan 21 03:28:18 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) ~[?:1.8.0_292]
2022-01-21T03:28:18.3727618Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:323) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3729147Z Jan 21 03:28:18 2022-01-21 03:27:56,927 WARN  org.apache.hadoop.ipc.Client                                 [] - Failed to connect to server: 11c5f741db81/172.25.0.2:39121: retries get failed due to exceeded maximum allowed retries number: 0
2022-01-21T03:28:18.3730293Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3730834Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3731499Z Jan 21 03:28:18 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[?:1.8.0_292]
2022-01-21T03:28:18.3732203Z Jan 21 03:28:18 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658) ~[?:1.8.0_292]
2022-01-21T03:28:18.3733478Z Jan 21 03:28:18 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3734470Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3735432Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3736414Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3737734Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3738853Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3739752Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1381) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3740638Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3741589Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3742621Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3743549Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3744684Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3745594Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3746221Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
2022-01-21T03:28:18.3746937Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
2022-01-21T03:28:18.3747615Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
2022-01-21T03:28:18.3748595Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3749706Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3750820Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3751915Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3753193Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3753988Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy52.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3754973Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainerInternal(NMClientImpl.java:316) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3756059Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainer(NMClientImpl.java:271) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3757457Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:541) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3758840Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:532) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3760149Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3761466Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3762578Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3763987Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3765348Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer.handle(NMClientAsyncImpl.java:617) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3766590Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor.run(NMClientAsyncImpl.java:676) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3767443Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
2022-01-21T03:28:18.3768153Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
2022-01-21T03:28:18.3768778Z Jan 21 03:28:18 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
2022-01-21T03:28:18.3769835Z Jan 21 03:28:18 2022-01-21 03:27:56,930 WARN  org.apache.flink.yarn.YarnResourceManagerDriver              [] - Error while calling YARN Node Manager to stop container container_1642735639007_0002_01_000002.
2022-01-21T03:28:18.3770910Z Jan 21 03:28:18 java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""11c5f741db81/172.25.0.2""; destination host is: ""11c5f741db81"":39121; 
2022-01-21T03:28:18.3772073Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3773188Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3774186Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3775072Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3776041Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3777094Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3777769Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) ~[?:?]
2022-01-21T03:28:18.3778869Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) ~[hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3779720Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3780182Z Jan 21 03:28:18 ]
2022-01-21T03:28:18.3780584Z Jan 21 03:28:18 	at org.junit.Assert.fail(Assert.java:89)
2022-01-21T03:28:18.3781169Z Jan 21 03:28:18 	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:591)
2022-01-21T03:28:18.3782025Z Jan 21 03:28:18 	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:82)
2022-01-21T03:28:18.3782677Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-21T03:28:18.3783557Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-21T03:28:18.3784250Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-21T03:28:18.3784878Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-21T03:28:18.3785487Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-21T03:28:18.3786168Z Jan 21 03:28:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-21T03:28:18.3786840Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-21T03:28:18.3787505Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-01-21T03:28:18.3788173Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-01-21T03:28:18.3788909Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3789557Z Jan 21 03:28:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-21T03:28:18.3790179Z Jan 21 03:28:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-21T03:28:18.3790755Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3791399Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-21T03:28:18.3792038Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-21T03:28:18.3792667Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-21T03:28:18.3793646Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-21T03:28:18.3794282Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T03:28:18.3794889Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T03:28:18.3795483Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T03:28:18.3796096Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T03:28:18.3796697Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T03:28:18.3797324Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-21T03:28:18.3797967Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-21T03:28:18.3798600Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3799223Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3799806Z Jan 21 03:28:18 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-21T03:28:18.3800367Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3800963Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T03:28:18.3801528Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-21T03:28:18.3802074Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-21T03:28:18.3802689Z Jan 21 03:28:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-21T03:28:18.3803675Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-21T03:28:18.3804487Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-21T03:28:18.3805205Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-21T03:28:18.3805990Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-21T03:28:18.3806794Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-21T03:28:18.3807616Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-21T03:28:18.3808420Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-21T03:28:18.3809106Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-21T03:28:18.3809759Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-21T03:28:18.3810463Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-21T03:28:18.3835763Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-21T03:28:18.3836503Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-21T03:28:18.3837248Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-21T03:28:18.3838212Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-01-21T03:28:18.3838953Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-21T03:28:18.3839649Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-21T03:28:18.3840279Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-21T03:28:18.3840921Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

The test {{YARNSessionFIFOSecuredITCase.testQueryCluster}} failed with:

{code}
2022-01-21T03:28:18.3842564Z Jan 21 03:28:18 java.lang.AssertionError: 
2022-01-21T03:28:18.3844597Z Jan 21 03:28:18 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo-secured/flink-yarn-tests-fifo-secured-logDir-nm-0_0/application_1642735639007_0002/container_1642735639007_0002_01_000001/jobmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
2022-01-21T03:28:18.3845483Z Jan 21 03:28:18 [
2022-01-21T03:28:18.3846666Z Jan 21 03:28:18 2022-01-21 03:27:56,921 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2022-01-21T03:28:18.3848108Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@11c5f741db81:37697/user/rpc/dispatcher_0.
2022-01-21T03:28:18.3849225Z Jan 21 03:28:18 2022-01-21 03:27:56,922 INFO  org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Interrupted while waiting for queue
2022-01-21T03:28:18.3849824Z Jan 21 03:28:18 java.lang.InterruptedException: null
2022-01-21T03:28:18.3850480Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014) ~[?:1.8.0_292]
2022-01-21T03:28:18.3851311Z Jan 21 03:28:18 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048) ~[?:1.8.0_292]
2022-01-21T03:28:18.3852206Z Jan 21 03:28:18 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) ~[?:1.8.0_292]
2022-01-21T03:28:18.3853638Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:323) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3855047Z Jan 21 03:28:18 2022-01-21 03:27:56,927 WARN  org.apache.hadoop.ipc.Client                                 [] - Failed to connect to server: 11c5f741db81/172.25.0.2:39121: retries get failed due to exceeded maximum allowed retries number: 0
2022-01-21T03:28:18.3855925Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3856444Z Jan 21 03:28:18 java.nio.channels.ClosedByInterruptException: null
2022-01-21T03:28:18.3857075Z Jan 21 03:28:18 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[?:1.8.0_292]
2022-01-21T03:28:18.3857778Z Jan 21 03:28:18 	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658) ~[?:1.8.0_292]
2022-01-21T03:28:18.3858858Z Jan 21 03:28:18 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3859778Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3860702Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3861692Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3862939Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3863944Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3864820Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1381) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3865660Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3866604Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3867586Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3868233Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3869278Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3870212Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3870850Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
2022-01-21T03:28:18.3871580Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
2022-01-21T03:28:18.3872230Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
2022-01-21T03:28:18.3873375Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3874477Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3875544Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3876609Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3877809Z Jan 21 03:28:18 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) [hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3878470Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy52.stopContainers(Unknown Source) [?:?]
2022-01-21T03:28:18.3879381Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainerInternal(NMClientImpl.java:316) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3880442Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.stopContainer(NMClientImpl.java:271) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3881623Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:541) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3883111Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition.transition(NMClientAsyncImpl.java:532) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3884553Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3885610Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3886619Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3887692Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448) [hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3888827Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer.handle(NMClientAsyncImpl.java:617) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3890014Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor.run(NMClientAsyncImpl.java:676) [hadoop-yarn-client-2.8.5.jar:?]
2022-01-21T03:28:18.3890841Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
2022-01-21T03:28:18.3891528Z Jan 21 03:28:18 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
2022-01-21T03:28:18.3892129Z Jan 21 03:28:18 	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
2022-01-21T03:28:18.3893275Z Jan 21 03:28:18 2022-01-21 03:27:56,930 WARN  org.apache.flink.yarn.YarnResourceManagerDriver              [] - Error while calling YARN Node Manager to stop container container_1642735639007_0002_01_000002.
2022-01-21T03:28:18.3894308Z Jan 21 03:28:18 java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: ""11c5f741db81/172.25.0.2""; destination host is: ""11c5f741db81"":39121; 
2022-01-21T03:28:18.3895436Z Jan 21 03:28:18 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3896356Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3897214Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3898060Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3898994Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3899987Z Jan 21 03:28:18 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5.jar:?]
2022-01-21T03:28:18.3900639Z Jan 21 03:28:18 	at com.sun.proxy.$Proxy51.stopContainers(Unknown Source) ~[?:?]
2022-01-21T03:28:18.3901798Z Jan 21 03:28:18 	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:120) ~[hadoop-yarn-common-2.8.5.jar:?]
2022-01-21T03:28:18.3902618Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
2022-01-21T03:28:18.3903137Z Jan 21 03:28:18 ]
2022-01-21T03:28:18.3903592Z Jan 21 03:28:18 	at org.junit.Assert.fail(Assert.java:89)
2022-01-21T03:28:18.3904183Z Jan 21 03:28:18 	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:591)
2022-01-21T03:28:18.3904901Z Jan 21 03:28:18 	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:82)
2022-01-21T03:28:18.3905526Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-21T03:28:18.3906105Z Jan 21 03:28:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-21T03:28:18.3906769Z Jan 21 03:28:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-21T03:28:18.3907469Z Jan 21 03:28:18 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-21T03:28:18.3908059Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-21T03:28:18.3908700Z Jan 21 03:28:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-21T03:28:18.3909350Z Jan 21 03:28:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-21T03:28:18.3909993Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-01-21T03:28:18.3910632Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-01-21T03:28:18.3911243Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3911869Z Jan 21 03:28:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-21T03:28:18.3912465Z Jan 21 03:28:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-21T03:28:18.3913092Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3913775Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-21T03:28:18.3914387Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-21T03:28:18.3914995Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-21T03:28:18.3915645Z Jan 21 03:28:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-21T03:28:18.3916252Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-21T03:28:18.3916823Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-21T03:28:18.3917405Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-21T03:28:18.3917976Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-21T03:28:18.3918554Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-21T03:28:18.3919152Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-21T03:28:18.3919778Z Jan 21 03:28:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-21T03:28:18.3920386Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3920980Z Jan 21 03:28:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-21T03:28:18.3921545Z Jan 21 03:28:18 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-21T03:28:18.3922166Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-21T03:28:18.3922727Z Jan 21 03:28:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-21T03:28:18.3923398Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-21T03:28:18.3923921Z Jan 21 03:28:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-21T03:28:18.3924505Z Jan 21 03:28:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-21T03:28:18.3925163Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-21T03:28:18.3926090Z Jan 21 03:28:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-21T03:28:18.3926927Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-21T03:28:18.3927700Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-21T03:28:18.3928487Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-21T03:28:18.3929388Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-21T03:28:18.3930162Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-21T03:28:18.3930858Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-21T03:28:18.3931520Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-21T03:28:18.3932245Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-21T03:28:18.3933105Z Jan 21 03:28:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-21T03:28:18.3933889Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-01-21T03:28:18.3934645Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-21T03:28:18.3935380Z Jan 21 03:28:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-01-21T03:28:18.3936073Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-21T03:28:18.3936725Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-21T03:28:18.3937348Z Jan 21 03:28:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29841&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461"	FLINK	Closed	2	1	10066	test-stability
12875385	ZookeeperOffsetHandlerTest.runOffsetManipulationinZooKeeperTest failed on Travis	"The {{ZookeeperOffsetHandlerTest.runOffsetManipulationinZooKeeperTest}} failed on Travis with 

{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.flink.streaming.connectors.kafka.KafkaConsumerPartitionAssignmentTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.18 sec - in org.apache.flink.streaming.connectors.kafka.KafkaConsumerPartitionAssignmentTest
Running org.apache.flink.streaming.connectors.kafka.internals.ZookeeperOffsetHandlerTest
org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 20000
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:880)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createZookeeperClient(KafkaTestBase.java:278)
	at org.apache.flink.streaming.connectors.kafka.internals.ZookeeperOffsetHandlerTest.runOffsetManipulationinZooKeeperTest(ZookeeperOffsetHandlerTest.java:44)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 43.695 sec <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.internals.ZookeeperOffsetHandlerTest
runOffsetManipulationinZooKeeperTest(org.apache.flink.streaming.connectors.kafka.internals.ZookeeperOffsetHandlerTest)  Time elapsed: 21.258 sec  <<< FAILURE!
java.lang.AssertionError: Unable to connect to zookeeper server within timeout: 20000
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.streaming.connectors.kafka.internals.ZookeeperOffsetHandlerTest.runOffsetManipulationinZooKeeperTest(ZookeeperOffsetHandlerTest.java:57)

Running org.apache.flink.streaming.connectors.kafka.KafkaConsumerTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.385 sec - in org.apache.flink.streaming.connectors.kafka.KafkaConsumerTest
Running org.apache.flink.streaming.connectors.kafka.TestFixedPartitioner
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 sec - in org.apache.flink.streaming.connectors.kafka.TestFixedPartitioner

Results :

Failed tests: 
  ZookeeperOffsetHandlerTest.runOffsetManipulationinZooKeeperTest:57 Unable to connect to zookeeper server within timeout: 20000
{code}

https://s3.amazonaws.com/archive.travis-ci.org/jobs/80770637/log.txt"	FLINK	Closed	2	1	10066	test-stability
13302804	Send flink-web issues and pull request notifications to issues@flink.apache.org	"With the introduction of https://cwiki.apache.org/confluence/display/INFRA/.asf.yaml+features+for+git+repositories the {{flink-web}} repository started sending issues and pull request notifications to dev@f.a.o.

I propose to change it to issues@f.a.o."	FLINK	Closed	3	4	10066	pull-request-available
13127930	Make CustomCommandLines non static in CliFrontend	For better testability and maintainability we should make the {{CustomCommandLine}} registration non-static in {{CliFrontend}}.	FLINK	Closed	3	7	10066	flip-6
13359408	Consolidate Scheduler/SlotPool factories	"We have run into the unfortunate situation where different schedulers effectively work against different slot pool implementations and interfaces.
This makes it quite difficult to understand which slot pool factories can work with which scheduler.

We may be able to alleviate these issues by wrapping the factories for both into a single factory."	FLINK	Closed	3	7	10066	pull-request-available
13362269	Creation of ExecutionGraph happens in main thread	Currently, the {{AdaptiveScheduler}} creates the {{ExecutionGraph}} in the main thread. This also means that we are recovering state in the {{JobMasters's}} main thread. This can lead to instabilities because we are blocking the main thread for too long. I think the creation of the {{ExecutionGraph}} should happen in an {{ioExecutor}}.	FLINK	Closed	2	7	10066	pull-request-available
13173679	Remove cancellation of input futures from ConjunctFutures	With FLINK-8749, we introduced that a {{ConjunctFutures}} cancels all of its input futures if it is cancelled. This has, however, some unpleasant side effects since a all of the cancelled future's completing callbacks won't be called. Since this can lead to subtle bugs like in FLINK-9908, I would propose to remove this feature and require the user to do the cancellation of input futures explicitly.	FLINK	Closed	3	4	10066	pull-request-available
13105468	Port CheckpointStatsHandler to new REST endpoint	Port existing {{CheckpointStatsHandler}} to new REST endpoint.	FLINK	Resolved	3	7	10066	flip-6
13500815	Update streaming query document for Table Store to include full compaction changelog producer	As we've now implemented the full compaction changelog producer, we need to update the document so that user can understand when and how to use it.	FLINK	Closed	3	7	10239	pull-request-available
13374849	can not be execute an extreme long sql under batch mode	"1. execute an extreme long sql under batch mode

 
{code:java}
select
'CD' product_name,
r.code business_platform,
5 statisticperiod,
cast('2021-03-24 00:00:00' as timestamp) coltime,
cast(r1.indicatorvalue as double) as YWPT_ZHQI_CD_038_GZ_00002,
cast(r2.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00007,
cast(r3.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00005,
cast(r4.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00006,
cast(r5.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00029,
cast(r6.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00028,
cast(r7.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00015,
cast(r8.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00014,
cast(r9.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00011,
cast(r10.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00010,
cast(r11.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00013,
cast(r12.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00012,
cast(r13.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00027,
cast(r14.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00026,
cast(r15.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00046,
cast(r16.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00047,
cast(r17.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00049,
cast(r18.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00048,
cast(r19.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00024,
cast(r20.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00025,
cast(r21.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00022,
cast(r22.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00023,
cast(r23.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00054,
cast(r24.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00055,
cast(r25.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00033,
cast(r26.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00032,
cast(r27.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00053,
cast(r28.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00052,
cast(r29.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00051,
cast(r30.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00050,
cast(r31.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00043,
cast(r32.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00042,
cast(r33.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00017,
cast(r34.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00016,
cast(r35.indicatorvalue as double) as YWPT_ZHQI_CD_038_GZ_00003,
cast(r36.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00045,
cast(r37.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00044,
cast(r38.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00038,
cast(r39.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00039,
cast(r40.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00037,
cast(r41.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00036,
cast(r42.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00040,
cast(r43.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00041,
cast(r44.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00034,
cast(r45.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00035,
cast(r46.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00030,
cast(r47.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00031,
cast(r48.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00020,
cast(r49.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00021,
cast(r50.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00018,
cast(r51.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00019,
cast(r52.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00004,
cast(r53.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00008,
cast(r54.indicatorvalue as double) as YWPT_ZHQI_CD_038_XT_00061,
cast(r55.indicatorvalue as double) as YWPT_ZHQI_CD_038_YW_00009,
localtimestamp as crtime,
'2021-03-24' as dt
from prod_mysql_bnpmp.r_biz_product r
left join raw_restapi_load.p_hcd r1 on r1.dt='2021-03-24' and r1.coltime =cast('2021-03-24 00:00:00' as timestamp) and r1.businessplatform=r.code and r1.indicatornumber='YWPT-ZHQI-CD-038-GZ-00002'
left join raw_restapi_load.p_hcd r2 on r2.dt='2021-03-24' and r2.coltime =cast('2021-03-24 00:00:00' as timestamp) and r2.businessplatform=r.code and r2.indicatornumber='YWPT-ZHQI-CD-038-YW-00007'
left join raw_restapi_load.p_hcd r3 on r3.dt='2021-03-24' and r3.coltime =cast('2021-03-24 00:00:00' as timestamp) and r3.businessplatform=r.code and r3.indicatornumber='YWPT-ZHQI-CD-038-YW-00005'
left join raw_restapi_load.p_hcd r4 on r4.dt='2021-03-24' and r4.coltime =cast('2021-03-24 00:00:00' as timestamp) and r4.businessplatform=r.code and r4.indicatornumber='YWPT-ZHQI-CD-038-YW-00006'
left join raw_restapi_load.p_hcd r5 on r5.dt='2021-03-24' and r5.coltime =cast('2021-03-24 00:00:00' as timestamp) and r5.businessplatform=r.code and r5.indicatornumber='YWPT-ZHQI-CD-038-XT-00029'
left join raw_restapi_load.p_hcd r6 on r6.dt='2021-03-24' and r6.coltime =cast('2021-03-24 00:00:00' as timestamp) and r6.businessplatform=r.code and r6.indicatornumber='YWPT-ZHQI-CD-038-XT-00028'
left join raw_restapi_load.p_hcd r7 on r7.dt='2021-03-24' and r7.coltime =cast('2021-03-24 00:00:00' as timestamp) and r7.businessplatform=r.code and r7.indicatornumber='YWPT-ZHQI-CD-038-XT-00015'
left join raw_restapi_load.p_hcd r8 on r8.dt='2021-03-24' and r8.coltime =cast('2021-03-24 00:00:00' as timestamp) and r8.businessplatform=r.code and r8.indicatornumber='YWPT-ZHQI-CD-038-XT-00014'
left join raw_restapi_load.p_hcd r9 on r9.dt='2021-03-24' and r9.coltime =cast('2021-03-24 00:00:00' as timestamp) and r9.businessplatform=r.code and r9.indicatornumber='YWPT-ZHQI-CD-038-XT-00011'
left join raw_restapi_load.p_hcd r10 on r10.dt='2021-03-24' and r10.coltime =cast('2021-03-24 00:00:00' as timestamp) and r10.businessplatform=r.code and r10.indicatornumber='YWPT-ZHQI-CD-038-XT-00010'
left join raw_restapi_load.p_hcd r11 on r11.dt='2021-03-24' and r11.coltime =cast('2021-03-24 00:00:00' as timestamp) and r11.businessplatform=r.code and r11.indicatornumber='YWPT-ZHQI-CD-038-XT-00013'
left join raw_restapi_load.p_hcd r12 on r12.dt='2021-03-24' and r12.coltime =cast('2021-03-24 00:00:00' as timestamp) and r12.businessplatform=r.code and r12.indicatornumber='YWPT-ZHQI-CD-038-XT-00012'
left join raw_restapi_load.p_hcd r13 on r13.dt='2021-03-24' and r13.coltime =cast('2021-03-24 00:00:00' as timestamp) and r13.businessplatform=r.code and r13.indicatornumber='YWPT-ZHQI-CD-038-XT-00027'
left join raw_restapi_load.p_hcd r14 on r14.dt='2021-03-24' and r14.coltime =cast('2021-03-24 00:00:00' as timestamp) and r14.businessplatform=r.code and r14.indicatornumber='YWPT-ZHQI-CD-038-XT-00026'
left join raw_restapi_load.p_hcd r15 on r15.dt='2021-03-24' and r15.coltime =cast('2021-03-24 00:00:00' as timestamp) and r15.businessplatform=r.code and r15.indicatornumber='YWPT-ZHQI-CD-038-XT-00046'
left join raw_restapi_load.p_hcd r16 on r16.dt='2021-03-24' and r16.coltime =cast('2021-03-24 00:00:00' as timestamp) and r16.businessplatform=r.code and r16.indicatornumber='YWPT-ZHQI-CD-038-XT-00047'
left join raw_restapi_load.p_hcd r17 on r17.dt='2021-03-24' and r17.coltime =cast('2021-03-24 00:00:00' as timestamp) and r17.businessplatform=r.code and r17.indicatornumber='YWPT-ZHQI-CD-038-XT-00049'
left join raw_restapi_load.p_hcd r18 on r18.dt='2021-03-24' and r18.coltime =cast('2021-03-24 00:00:00' as timestamp) and r18.businessplatform=r.code and r18.indicatornumber='YWPT-ZHQI-CD-038-XT-00048'
left join raw_restapi_load.p_hcd r19 on r19.dt='2021-03-24' and r19.coltime =cast('2021-03-24 00:00:00' as timestamp) and r19.businessplatform=r.code and r19.indicatornumber='YWPT-ZHQI-CD-038-XT-00024'
left join raw_restapi_load.p_hcd r20 on r20.dt='2021-03-24' and r20.coltime =cast('2021-03-24 00:00:00' as timestamp) and r20.businessplatform=r.code and r20.indicatornumber='YWPT-ZHQI-CD-038-XT-00025'
left join raw_restapi_load.p_hcd r21 on r21.dt='2021-03-24' and r21.coltime =cast('2021-03-24 00:00:00' as timestamp) and r21.businessplatform=r.code and r21.indicatornumber='YWPT-ZHQI-CD-038-XT-00022'
left join raw_restapi_load.p_hcd r22 on r22.dt='2021-03-24' and r22.coltime =cast('2021-03-24 00:00:00' as timestamp) and r22.businessplatform=r.code and r22.indicatornumber='YWPT-ZHQI-CD-038-XT-00023'
left join raw_restapi_load.p_hcd r23 on r23.dt='2021-03-24' and r23.coltime =cast('2021-03-24 00:00:00' as timestamp) and r23.businessplatform=r.code and r23.indicatornumber='YWPT-ZHQI-CD-038-XT-00054'
left join raw_restapi_load.p_hcd r24 on r24.dt='2021-03-24' and r24.coltime =cast('2021-03-24 00:00:00' as timestamp) and r24.businessplatform=r.code and r24.indicatornumber='YWPT-ZHQI-CD-038-XT-00055'
left join raw_restapi_load.p_hcd r25 on r25.dt='2021-03-24' and r25.coltime =cast('2021-03-24 00:00:00' as timestamp) and r25.businessplatform=r.code and r25.indicatornumber='YWPT-ZHQI-CD-038-XT-00033'
left join raw_restapi_load.p_hcd r26 on r26.dt='2021-03-24' and r26.coltime =cast('2021-03-24 00:00:00' as timestamp) and r26.businessplatform=r.code and r26.indicatornumber='YWPT-ZHQI-CD-038-XT-00032'
left join raw_restapi_load.p_hcd r27 on r27.dt='2021-03-24' and r27.coltime =cast('2021-03-24 00:00:00' as timestamp) and r27.businessplatform=r.code and r27.indicatornumber='YWPT-ZHQI-CD-038-XT-00053'
left join raw_restapi_load.p_hcd r28 on r28.dt='2021-03-24' and r28.coltime =cast('2021-03-24 00:00:00' as timestamp) and r28.businessplatform=r.code and r28.indicatornumber='YWPT-ZHQI-CD-038-XT-00052'
left join raw_restapi_load.p_hcd r29 on r29.dt='2021-03-24' and r29.coltime =cast('2021-03-24 00:00:00' as timestamp) and r29.businessplatform=r.code and r29.indicatornumber='YWPT-ZHQI-CD-038-XT-00051'
left join raw_restapi_load.p_hcd r30 on r30.dt='2021-03-24' and r30.coltime =cast('2021-03-24 00:00:00' as timestamp) and r30.businessplatform=r.code and r30.indicatornumber='YWPT-ZHQI-CD-038-XT-00050'
left join raw_restapi_load.p_hcd r31 on r31.dt='2021-03-24' and r31.coltime =cast('2021-03-24 00:00:00' as timestamp) and r31.businessplatform=r.code and r31.indicatornumber='YWPT-ZHQI-CD-038-XT-00043'
left join raw_restapi_load.p_hcd r32 on r32.dt='2021-03-24' and r32.coltime =cast('2021-03-24 00:00:00' as timestamp) and r32.businessplatform=r.code and r32.indicatornumber='YWPT-ZHQI-CD-038-XT-00042'
left join raw_restapi_load.p_hcd r33 on r33.dt='2021-03-24' and r33.coltime =cast('2021-03-24 00:00:00' as timestamp) and r33.businessplatform=r.code and r33.indicatornumber='YWPT-ZHQI-CD-038-XT-00017'
left join raw_restapi_load.p_hcd r34 on r34.dt='2021-03-24' and r34.coltime =cast('2021-03-24 00:00:00' as timestamp) and r34.businessplatform=r.code and r34.indicatornumber='YWPT-ZHQI-CD-038-XT-00016'
left join raw_restapi_load.p_hcd r35 on r35.dt='2021-03-24' and r35.coltime =cast('2021-03-24 00:00:00' as timestamp) and r35.businessplatform=r.code and r35.indicatornumber='YWPT-ZHQI-CD-038-GZ-00003'
left join raw_restapi_load.p_hcd r36 on r36.dt='2021-03-24' and r36.coltime =cast('2021-03-24 00:00:00' as timestamp) and r36.businessplatform=r.code and r36.indicatornumber='YWPT-ZHQI-CD-038-XT-00045'
left join raw_restapi_load.p_hcd r37 on r37.dt='2021-03-24' and r37.coltime =cast('2021-03-24 00:00:00' as timestamp) and r37.businessplatform=r.code and r37.indicatornumber='YWPT-ZHQI-CD-038-XT-00044'
left join raw_restapi_load.p_hcd r38 on r38.dt='2021-03-24' and r38.coltime =cast('2021-03-24 00:00:00' as timestamp) and r38.businessplatform=r.code and r38.indicatornumber='YWPT-ZHQI-CD-038-XT-00038'
left join raw_restapi_load.p_hcd r39 on r39.dt='2021-03-24' and r39.coltime =cast('2021-03-24 00:00:00' as timestamp) and r39.businessplatform=r.code and r39.indicatornumber='YWPT-ZHQI-CD-038-XT-00039'
left join raw_restapi_load.p_hcd r40 on r40.dt='2021-03-24' and r40.coltime =cast('2021-03-24 00:00:00' as timestamp) and r40.businessplatform=r.code and r40.indicatornumber='YWPT-ZHQI-CD-038-XT-00037'
left join raw_restapi_load.p_hcd r41 on r41.dt='2021-03-24' and r41.coltime =cast('2021-03-24 00:00:00' as timestamp) and r41.businessplatform=r.code and r41.indicatornumber='YWPT-ZHQI-CD-038-XT-00036'
left join raw_restapi_load.p_hcd r42 on r42.dt='2021-03-24' and r42.coltime =cast('2021-03-24 00:00:00' as timestamp) and r42.businessplatform=r.code and r42.indicatornumber='YWPT-ZHQI-CD-038-XT-00040'
left join raw_restapi_load.p_hcd r43 on r43.dt='2021-03-24' and r43.coltime =cast('2021-03-24 00:00:00' as timestamp) and r43.businessplatform=r.code and r43.indicatornumber='YWPT-ZHQI-CD-038-XT-00041'
left join raw_restapi_load.p_hcd r44 on r44.dt='2021-03-24' and r44.coltime =cast('2021-03-24 00:00:00' as timestamp) and r44.businessplatform=r.code and r44.indicatornumber='YWPT-ZHQI-CD-038-XT-00034'
left join raw_restapi_load.p_hcd r45 on r45.dt='2021-03-24' and r45.coltime =cast('2021-03-24 00:00:00' as timestamp) and r45.businessplatform=r.code and r45.indicatornumber='YWPT-ZHQI-CD-038-XT-00035'
left join raw_restapi_load.p_hcd r46 on r46.dt='2021-03-24' and r46.coltime =cast('2021-03-24 00:00:00' as timestamp) and r46.businessplatform=r.code and r46.indicatornumber='YWPT-ZHQI-CD-038-XT-00030'
left join raw_restapi_load.p_hcd r47 on r47.dt='2021-03-24' and r47.coltime =cast('2021-03-24 00:00:00' as timestamp) and r47.businessplatform=r.code and r47.indicatornumber='YWPT-ZHQI-CD-038-XT-00031'
left join raw_restapi_load.p_hcd r48 on r48.dt='2021-03-24' and r48.coltime =cast('2021-03-24 00:00:00' as timestamp) and r48.businessplatform=r.code and r48.indicatornumber='YWPT-ZHQI-CD-038-XT-00020'
left join raw_restapi_load.p_hcd r49 on r49.dt='2021-03-24' and r49.coltime =cast('2021-03-24 00:00:00' as timestamp) and r49.businessplatform=r.code and r49.indicatornumber='YWPT-ZHQI-CD-038-XT-00021'
left join raw_restapi_load.p_hcd r50 on r50.dt='2021-03-24' and r50.coltime =cast('2021-03-24 00:00:00' as timestamp) and r50.businessplatform=r.code and r50.indicatornumber='YWPT-ZHQI-CD-038-XT-00018'
left join raw_restapi_load.p_hcd r51 on r51.dt='2021-03-24' and r51.coltime =cast('2021-03-24 00:00:00' as timestamp) and r51.businessplatform=r.code and r51.indicatornumber='YWPT-ZHQI-CD-038-XT-00019'
left join raw_restapi_load.p_hcd r52 on r52.dt='2021-03-24' and r52.coltime =cast('2021-03-24 00:00:00' as timestamp) and r52.businessplatform=r.code and r52.indicatornumber='YWPT-ZHQI-CD-038-YW-00004'
left join raw_restapi_load.p_hcd r53 on r53.dt='2021-03-24' and r53.coltime =cast('2021-03-24 00:00:00' as timestamp) and r53.businessplatform=r.code and r53.indicatornumber='YWPT-ZHQI-CD-038-YW-00008'
left join raw_restapi_load.p_hcd r54 on r54.dt='2021-03-24' and r54.coltime =cast('2021-03-24 00:00:00' as timestamp) and r54.businessplatform=r.code and r54.indicatornumber='YWPT-ZHQI-CD-038-XT-00061'
left join raw_restapi_load.p_hcd r55 on r55.dt='2021-03-24' and r55.coltime =cast('2021-03-24 00:00:00' as timestamp) and r55.businessplatform=r.code and r55.indicatornumber='YWPT-ZHQI-CD-038-YW-00009'
where 1=1
and r.code='YWPT-ZHQI-CD-038'
and r.type='biz';{code}
 

2. get error 

 
{code:java}
[ERROR] Could not execute SQL statement. Reason:
java.io.IOException: Can not make a progress: all selected inputs are already finished
{code}
3. execute same sql under streaming mode and get expected output

 "	FLINK	Closed	1	1	10239	pull-request-available, stale-blocker, stale-critical
13505880	Allow FileStoreScan to read incremental changes from OVERWRITE snapshot in Table Store	Currently {{AbstractFileStoreScan}} can only read incremental changes from APPEND snapshots. However in OVERWRITE snapshots, users will also append new records to table. These changes must be discovered by compact job source so that the overwritten partition can be compacted.	FLINK	Closed	3	7	10239	pull-request-available
13431423	Allow ManifestFile to write a list of ManifestFileMeta into multiple files if too many.	Currently {{ManifestFile#write}} will only produce a single file no matter how large the given {{ManifestFileMeta}} list is. However batch jobs are likely to produce a lot of changes hence the list might be very long. We need to allow {{ManifestFile#write}} to produce multiple files for effective merging and filtering.	FLINK	Closed	3	7	10239	pull-request-available
13494210	Old record may overwrite new record in Table Store when snapshot committing is slow	"Consider the following scenario when snapshot committing is slow:
* A writer produces some records at checkpoint T.
* It produces no record at checkpoint T+1 and is closed.
* It produces some records at checkpoint T+2. It will be reopened and read the latest sequence number from disk. However snapshot at checkpoint T may not be committed so the sequence number it reads might be too small.

In this scenario, records from checkpoint T may overwrite records from checkpoint T+2 because they have larger sequence numbers."	FLINK	Closed	3	1	10239	pull-request-available
13516624	Table Store dedicated compact job may skip some records when checkpoint interval is long	Currently the sink for Table Store dedicated compact job only receives records about what buckets are changed, instead of what files are changed. If the writer is kept open, new files of this bucket may be skipped.	FLINK	Closed	3	1	10239	pull-request-available
13336872	FlinkRelMdDistinctRowCount#getDistinctRowCount(Calc) will always return 0 when number of rows are large	"Due to CALCITE-4351 {{FlinkRelMdDistinctRowCount#getDistinctRowCount(Calc)}} will always return 0 when number of rows are large.

What I would suggest is to introduce our own {{FlinkRelMdUtil#numDistinctVals}} to treat small and large inputs in different ways. For small inputs we use the more precise {{RelMdUtil#numDistinctVals}} and for large inputs we copy the old, approximated implementation of {{RelMdUtil#numDistinctVals}}.

This is a temporary solution. When CALCITE-4351 is fixed we should revert this commit."	FLINK	Closed	3	1	10239	pull-request-available
13473986	Add Tez execution engine test for Table Store Hive connector	Table Store Hive connector is supposed to support both MR and Tez engine. However current tests only runs on MR. We need to add Tez tests.	FLINK	Closed	3	4	10239	pull-request-available
13438090	Introduce snapshot.num-retained.min option	"Currently we retain at least 1 snapshot when expiring. However consider the following scenario:

A user writes a snapshot one day ago. Today he is reading this snapshot and meanwhile writing more records. If a new snapshot is created and the reading is not finished, the old snapshot created one day ago will be removed as it exceeds maximum retaining time. This will cause the reading to fail.

We should introduce {{snapshot.num-retained.min}} to at least retain a minimum number of snapshots to avoid this problem."	FLINK	Closed	3	7	10239	pull-request-available
13528331	Make SchemaChange serializable	"To avoid concurrent changes to table schema, CDC sinks for Flink Table Store should send all \{{SchemaChange}} to a special process function. This process function only has 1 parallelism and it is dedicated for schema changes.

 

To pass \{{SchemaChange}} through network, \{{SchemaChange}} must be serializable."	FLINK	Closed	3	7	10239	pull-request-available
13450194	Check Hive DDL against table store schema when creating table	As table store schema is supported, we should use this schema as the ground truth. Hive DDL should only be used for checking.	FLINK	Closed	3	7	10239	pull-request-available
13253401	The behavior of JobExecutionResult.getAccumulatorResult does not match its java doc	"The java doc of `JobExecutionResult.getAccumulatorResult` states that ""Returns \{@code null}, if no accumulator with that name was produced"", but actually an NPE will be triggered if no accumulator with that name is produced.

I'm going to rewrite the `getAccumulatorResult` method to the following:
{code:java}
public <T> T getAccumulatorResult(String accumulatorName) {
   OptionalFailure<Object> result = this.accumulatorResults.get(accumulatorName);
   if (result != null) {
      return (T) result.getUnchecked();
   } else {
      return null;
   }
}
{code}
Please assign this issue to me if this solution is acceptable.

 "	FLINK	Resolved	3	1	10239	pull-request-available
13449759	Refactor Flink connector for table store with FileStoreTable	We've prepared {{FileStoreTable}} for {{RowData}} reading and writing, so it's time to refactor Flink connector for table store with {{FileStoreTable}}.	FLINK	Closed	3	7	10239	pull-request-available
13385744	Introduce Java code splitter for code generation	In this first step we introduce a Java code splitter for the generated Java code. This splitter comes into effect after the original Java code is generated, hoping to solve the 64KB problem in one shot.	FLINK	Closed	3	7	10239	pull-request-available
13505882	Move split initialization and discovery logic fully into SnapshotEnumerator in Table Store	"It is possible that the separated compact job is started long after the write jobs. so compact job sources need a special split initialization logic: we will find the latest COMPACT snapshot, and start compacting right after this snapshot.

However, split initialization logic are currently coded into {{FileStoreSource}}. We should extract these logic into {{SnapshotEnumerator}} so that we can create our special {{SnapshotEnumerator}} for compact sources."	FLINK	Closed	3	7	10239	pull-request-available
13492154	PreAggregationITCase.LastValueAggregation and PreAggregationITCase.LastNonNullValueAggregation are unstable	{{PreAggregationITCase.LastValueAggregation}} and {{PreAggregationITCase.LastNonNullValueAggregation}} need to make sure that the order of input data is determined. However the default parallelism of {{FileStoreTableITCase}} is 2, so the order of input data might change across tests.	FLINK	Closed	3	1	10239	pull-request-available
13462031	Savepoint may corrupt file metas by repeat commit	"[https://github.com/apache/flink-table-store/runs/7020439369?check_suite_focus=true]
Error:  Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 46.953 s <<< FAILURE! - in org.apache.flink.table.store.connector.RescaleBucketITCase 
[32285|https://github.com/apache/flink-table-store/runs/7020439369?check_suite_focus=true#step:4:32286]Error:  testSuspendAndRecoverAfterRescaleOverwrite Time elapsed: 25.545 s <<< ERROR!
{code:java}
Caused by: java.lang.IllegalStateException: Trying to add file {org.apache.flink.table.data.binary.BinaryRowData@9c67b85d, 0, 0, data-4756dfaf-e14e-440e-b211-df2b25f2537a-0.orc} which is already added. Manifest might be corrupted.
32416	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
32417	at org.apache.flink.table.store.file.operation.AbstractFileStoreScan.plan(AbstractFileStoreScan.java:189)
32418	at org.apache.flink.table.store.table.source.TableScan.plan(TableScan.java:99)
32419	at org.apache.flink.table.store.connector.source.FileStoreSource.restoreEnumerator(FileStoreSource.java:117)
32420	at org.apache.flink.table.store.connector.source.FileStoreSource.createEnumerator(FileStoreSource.java:93)
32421	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197)
32422	... 33 more {code}"	FLINK	Closed	1	1	10239	pull-request-available
13404204	"org.codehaus.janino.InternalCompilerException: Compiling ""StreamExecValues$200"": Code of method ""nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""StreamExecValues$200"" grows beyond 64 KB"	"I build a large SQL in application, and meet the issue ""Code of method method  grows beyond 64 KB"". This bug should be fixed refer to #FLINK-22903.

 
{quote}{{ java.lang.RuntimeException: Could not instantiate generated class 'StreamExecValues$200'    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.operators.values.ValuesInputFormat.open(ValuesInputFormat.java:60) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.operators.values.ValuesInputFormat.open(ValuesInputFormat.java:35) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:84) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323) [flink-dist_2.11-1.14.0.jar:1.14.0]Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling ""StreamExecValues$200"": Code of method ""nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""StreamExecValues$200"" grows beyond 64 KB    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method ""nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""StreamExecValues$200"" grows beyond 64 KB    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.CodeContext.write(CodeContext.java:925) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.writeOpcode(UnitCompiler.java:12291) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.referenceThis(UnitCompiler.java:10103) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4488) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$10000(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference(UnitCompiler.java:4437) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$QualifiedThisReference.accept(Java.java:4407) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileContext2(UnitCompiler.java:4336) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess(UnitCompiler.java:4273) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess(UnitCompiler.java:4268) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4268) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4264) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileContext(UnitCompiler.java:4264) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5661) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5145) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$Block.accept(Java.java:2779) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.fakeCompile(UnitCompiler.java:1529) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2434) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2181) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$2400(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitSwitchStatement(UnitCompiler.java:1500) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitSwitchStatement(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$SwitchStatement.accept(Java.java:3391) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) ~[flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 more}}
{quote}
 "	FLINK	Closed	3	1	10239	pull-request-available
13448992	Extract AppendOnlyFileStore out of KeyValueFileStore	"Currently {{FileStore}} for append only records and key-values are mixed in one {{FileStoreImpl}} class. This makes the code base messy and also introduce bugs (for example, {{AppendOnlyFileStore}} should rely on a special reader implementation but it is not, causing failures when using avro format).

We need to extract {{AppendOnlyFileStore}} out of {{KeyValueFileStore}}."	FLINK	Closed	3	7	10239	pull-request-available
13505890	Introduce a TableStoreCompactJob class for users to submit compact jobs in Table Store	Currently Flink does not support SQL statements for compacting a table. So in this ticket we create a TableStoreCompactJob class for users to submit compact jobs in Table Store.	FLINK	Closed	3	7	10239	pull-request-available
13245128	JDBC source/sink should respect the conversion class of DataType	"Hi , when I use Flink 1.9  JDBCTableSource,and I create  TableSchema like this:
final TableSchema schema = TableSchema.builder()
			.field(""id"", DataTypes.INT())
			.field(""create"", DataTypes.DATE())
			.field(""update"", DataTypes.DATE())
			.field(""name"", DataTypes.STRING())
			.field(""age"", DataTypes.INT())
			.field(""address"", DataTypes.STRING())
			.field(""birthday"",DataTypes.DATE())
			.field(""likethings"", DataTypes.STRING())
			.build();
I use  JDBCTableSource.builder() to create JDBCTableSource, I run the program, and there is a exception :
{color:red}java.lang.IllegalArgumentException: Unsupported type: LocalDate{color}
I saw the src code , I find that in LegacyTypeInfoDataTypeConverter , DateType convert to Types.LOCAL_DATE,but in JDBCTypeUtil class, the HashMap  TYPE_MAPPING  doesn't have the key Types.LOCAL_DATE,so that throw the exception.
Does the JDBC dim table support the time data,Like Date? May it is bug for JDBCTableSource join."	FLINK	Resolved	3	7	10239	pull-request-available
13493443	FileStoreCommitTest is unstable and may stuck	{{FileStoreCommitTest}} may stuck because the {{FileStoreCommit}} in {{TestCommitThread}} does not commit APPEND snapshot when no new files are produced. In this case, if the following COMPACT snapshot conflicts with the current merge tree, the test will stuck.	FLINK	Closed	3	1	10239	pull-request-available
13438057	Reuse FileFormat instead of DecodingFormat/EncodingFormat to ensure thread safety.	"When testing table store I'm faced with the following warning messages:
{code}
org.apache.org.impl.MemoryManagerImpl [] - Owner thread expected Thread[Writer -> Local Committer (1/16)#0,5,Flink Task Threads], got Thread[pool-8-thread-1,5,Flink Task Threads]
{code}

This is because {{OrcBulkWriterFactory}} is not thread safe (it keeps {{WriterOptions}} as a class member and creates writers from the same {{WriterOptions}} object, which is not thread safe).

What we should do is to reuse {{FileFormat}} instead of {{DecodingFormat}}/{{EncodingFormat}} to ensure thread safety."	FLINK	Closed	3	7	10239	pull-request-available
13514838	Refactor Table Store Documentation	Currently the documents of table store are messy and are lacking basic concepts. This ticket refactors table store documentation and at the mean time adding basic concepts.	FLINK	Closed	3	4	10239	pull-request-available
13369783	FlinkLogicalRankRuleBase should check if name of rankNumberType already exists in the input	"Add the following test case to {{org.apache.flink.table.planner.plan.stream.sql.RankTest}} to reproduce this issue.

{code:scala}
@Test
def myTest(): Unit = {
  val sql =
    """"""
      |SELECT CAST(rna AS INT) AS rn1, CAST(rnb AS INT) AS rn2 FROM (
      |  SELECT *, row_number() over (partition by a order by b desc) AS rnb
      |  FROM (
      |    SELECT *, row_number() over (partition by a, c order by b desc) AS rna
      |    FROM MyTable
      |  )
      |  WHERE rna <= 100
      |)
      |WHERE rnb <= 100
      |"""""".stripMargin
  util.verifyExecPlan(sql)
}
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [w0$o0]

	at org.apache.flink.table.types.logical.RowType.validateFields(RowType.java:272)
	at org.apache.flink.table.types.logical.RowType.<init>(RowType.java:157)
	at org.apache.flink.table.types.logical.RowType.of(RowType.java:297)
	at org.apache.flink.table.types.logical.RowType.of(RowType.java:289)
	at org.apache.flink.table.planner.calcite.FlinkTypeFactory$.toLogicalRowType(FlinkTypeFactory.scala:632)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalRank.translateToExecNode(StreamPhysicalRank.scala:117)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:74)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:71)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:54)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToExecNodeGraph(PlannerBase.scala:314)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:895)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:583)
{code}

This is because currently names of rank fields are all {{w0$o0}}, so if the input of a Rank is another Rank the exception will occur. To solve this, we should check if name of rank field has occurred in the input in {{FlinkLogicalRankRuleBase}}."	FLINK	Closed	3	1	10239	pull-request-available
13433824	Delegate table planner dependencies with separate modules	FLINK-25128 introduces the planner loader module to hide scala versions. All modules relying on flink-table-planner should change to flink-table-planner-loader instead.	FLINK	Closed	3	7	10239	pull-request-available
13248241	Allow setting configuration in SQL CLI	"Currently, we provide a set of configurations in blink planner to support optimization or enable some advanced feature. However, all the configurations can't be set in SQL CLI. 

It would be great to allow set configurations in SQL CLI via {{SET}} command. 
This maybe a new feature, but considering the implementation effort is rather low (pass configurations to TableConfig), I would like to add it to 1.9 too, but I won't set it as blocker.


For example:


{code:sql}
SET table.exec.mini-batch.enabled = true;
SET table.exec.mini-batch.allow-latency = 5s;
{code}

Meanwhile, we may also need to add an entry in yaml file, we propose to add a {{config}} entry. However, this might be different with other entries. Because we don't need the {{config.}} prefix in {{SET}} command. 

{code}
config:
  table.exec.mini-batch.enabled: true
  table.exec.mini-batch.allow-latency: 5s
{code}
"	FLINK	Closed	2	4	10239	pull-request-available
13430382	Add statistics collecting to sst files	"Currently field statistics are not collected in sst files. With statistics we can do filter and other operations with better performance.

Some formats like orc already record statistics into file headers, so for these special formats we just need to read them directly from files. For others, however, we need to collect the statistics by hand."	FLINK	Closed	3	7	10239	pull-request-available
13481411	Change MergeFunction to produce not only KeyValues	{{MergeFunction}} of full compaction need to produce changelogs instead of single {{KeyValue}}. We need to modify {{MergeFunction}} into a generic class.	FLINK	Closed	3	7	10239	pull-request-available
13318934	ProjectionCodeGenerator#generateProjectionExpression should remove for loop optimization	"If too many fields of the same type are projected, {{ProjectionCodeGenerator#generateProjectionExpression}} currently performs a ""for loop optimization"" which, instead of generating code separately for each field, they'll be squashed into a for loop.

However, if the indices of the fields with the same type are not continuous, this optimization will not write fields in index ascending order. This is not acceptable because {{BinaryWriter}} expects the users to write fields in index ascending order (that is to say, we *have to* first write field 0, then field 1, then...), otherwise the variable length area of the two binary rows with same data might be different. Although we can use {{getXX}} methods of {{BinaryRow}} to get the fields correctly, states for streaming jobs compare state keys with binary bits, not with the contents of the keys. So we need to make sure the binary bits of the binary rows be the same if two rows contain the same data.

What's worse, as the current implementation of {{ProjectionCodeGenerator#generateProjectionExpression}} uses a scala {{HashMap}}, the key order of the map might be different on different workers; Even if the projection does not meet the condition to be optimized, it will still be affected by this bug.

What I suggest is to simply remove this optimization. Because if we still want this optimization, we have to make sure that the fields of the same type have continuous order, which is a very strict and rare condition."	FLINK	Closed	2	1	10239	pull-request-available
13427064	Implement FileStoreExpire	Currently FileStoreExpire does not have an implementation. We need an implementation to clean up old snapshots and related files.	FLINK	Closed	3	7	10239	pull-request-available
13478110	Fix Table Store Hive CDH support	Currently Table Store Hive catalog and connectors cannot run against Hive 2.1 CDH 6.3. We should fix this support.	FLINK	Closed	3	1	10239	pull-request-available
13473258	FileStoreExpireTest.testExpireWithTime failed	https://github.com/apache/flink-table-store/runs/7496947558?check_suite_focus=true	FLINK	Closed	1	1	10239	pull-request-available
13248484	Run TPC-H E2E test on travis cron job 	FLINK-13436 added a TPC-H e2e test but didn't include it in travis. We should add it to travis cron job. One place is {{split_misc.sh}}, another choice is {{split_heavy.sh}}.	FLINK	Resolved	1	4	10239	pull-request-available
13478148	RescaleBucketITCase#testSuspendAndRecoverAfterRescaleOverwrite is not stable	"[https://github.com/apache/flink-table-store/runs/7964774584?check_suite_focus=true]

!image-2022-08-23-15-35-59-499.png|width=576,height=370!"	FLINK	Closed	3	1	10239	pull-request-available
13509819	Change table property key 'log.scan' to 'startup.mode' and add a default startup mode in Table Store	"We're introducing time-travel reading of Table Store for batch jobs. However this reading mode is quite similar to the ""from-timestamp"" startup mode for streaming jobs, just that ""from-timestamp"" streaming jobs only consume incremental data but not history data.

We can support startup mode for both batch and streaming jobs. For batch jobs, ""from-timestamp"" startup mode will produce all records from the last snapshot before the specified timestamp. For streaming jobs the behavior doesn't change.

Previously, in order to use ""from-timestamp"" startup mode, users will have to specify ""log.scan"" and also ""log.scan.timestamp-millis"", which is a little inconvenient. We can introduce a ""default"" startup mode and its behavior will base on the execution environment and other configurations. In this way, to use ""from-timestamp"" startup mode, it is enough for users to specify just ""startup.timestamp-millis""."	FLINK	Closed	3	4	10239	pull-request-available
13368352	SQL filter containing OR and IS NULL will produce an incorrect result.	"Add the following test case to {{CalcITCase}} to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  checkResult(
    """"""
      |WITH myView AS (SELECT a, CASE
      |  WHEN a = 1 THEN '1'
      |  ELSE CAST(NULL AS STRING)
      |  END AS s
      |FROM SmallTable3)
      |SELECT a FROM myView WHERE s = '2' OR s IS NULL
      |"""""".stripMargin,
    Seq(row(2), row(3)))
}
{code}

However if we remove the {{s = '2'}} the result will be correct."	FLINK	Closed	2	1	10239	pull-request-available
13493693	"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow"	"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow.

This is due to a bug in {{FileStoreCommitImpl#filterCommitted}}. When this method finds an identifier, it removes the identifier from a map. However different snapshots may have the same identifier (for example an APPEND commit and the following COMPACT commit will have the same identifier), so we need to use another set to check for identifiers.

When snapshot committing is fast there is at most 1 identifier to check after the job restarts, so nothing happens. However when snapshot committing is slow, there will be multiple identifiers to check and some identifiers will be mistakenly kept."	FLINK	Closed	3	1	10239	pull-request-available
13292175	TableEnvironmentITCase is crashing on Travis	"Here is the instance and exception stack: https://api.travis-ci.org/v3/job/663408376/log.txt
But there is not too much helpful information there, maybe a accidental maven problem.

{code}
09:55:07.703 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-table-planner-blink_2.11: There are test failures.
09:55:07.703 [ERROR] 
09:55:07.703 [ERROR] Please refer to /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target/surefire-reports for the individual test results.
09:55:07.703 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
09:55:07.703 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
09:55:07.703 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target/surefire/surefirebooter714252487017838305.jar /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target/surefire 2020-03-17T09-34-41_826-jvmRun1 surefire4625103637332937565tmp surefire_43192129054983363633tmp
09:55:07.703 [ERROR] Error occurred in starting fork, check output in log
09:55:07.703 [ERROR] Process Exit Code: 137
09:55:07.703 [ERROR] Crashed tests:
09:55:07.703 [ERROR] org.apache.flink.table.api.TableEnvironmentITCase
09:55:07.703 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
09:55:07.703 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target/surefire/surefirebooter714252487017838305.jar /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target/surefire 2020-03-17T09-34-41_826-jvmRun1 surefire4625103637332937565tmp surefire_43192129054983363633tmp
09:55:07.703 [ERROR] Error occurred in starting fork, check output in log
09:55:07.703 [ERROR] Process Exit Code: 137
09:55:07.703 [ERROR] Crashed tests:
09:55:07.703 [ERROR] org.apache.flink.table.api.TableEnvironmentITCase
09:55:07.703 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
09:55:07.704 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
09:55:07.704 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
09:55:07.704 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
09:55:07.704 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
09:55:07.704 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
09:55:07.704 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
09:55:07.704 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
09:55:07.704 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
09:55:07.704 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
09:55:07.704 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
09:55:07.704 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
09:55:07.704 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
09:55:07.704 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
09:55:07.704 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
09:55:07.704 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
09:55:07.704 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
09:55:07.704 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
09:55:07.704 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
09:55:07.704 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
09:55:07.704 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
09:55:07.704 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
09:55:07.704 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
09:55:07.704 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target/surefire/surefirebooter714252487017838305.jar /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/target/surefire 2020-03-17T09-34-41_826-jvmRun1 surefire4625103637332937565tmp surefire_43192129054983363633tmp
09:55:07.704 [ERROR] Error occurred in starting fork, check output in log
09:55:07.704 [ERROR] Process Exit Code: 137
09:55:07.704 [ERROR] Crashed tests:
09:55:07.704 [ERROR] org.apache.flink.table.api.TableEnvironmentITCase
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)
09:55:07.704 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)
09:55:07.704 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
09:55:07.704 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
09:55:07.704 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
09:55:07.704 [ERROR] at java.lang.Thread.run(Thread.java:748)
09:55:07.704 [ERROR] -> [Help 1]
09:55:07.704 [ERROR] 
09:55:07.704 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
09:55:07.704 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
09:55:07.704 [ERROR] 
09:55:07.704 [ERROR] For more information about the errors and possible solutions, please read the following articles:
09:55:07.704 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
09:55:07.704 [ERROR] 
09:55:07.704 [ERROR] After correcting the problems, you can resume the build with the command
09:55:07.704 [ERROR]   mvn <goals> -rf :flink-table-planner-blink_2.11
{code}"	FLINK	Closed	1	1	10239	pull-request-available, test-stability
13448023	Introduce TableScan and TableRead as an abstraction layer above FileStore for reading RowData	In this step we introduce {{TableScan}} and {{TableRead}} They are an abstraction layer above {{FileStoreScan}} and {{FileStoreRead}} to provide {{RowData}} reading.	FLINK	Closed	3	7	10239	pull-request-available
13245397	Fix implementation of getString and getBinary method in NestedRow	The `getString` and `getBinary` method in `NestedRow` are not implemented correctly. Also there is no tests guarding these complex data formats.	FLINK	Resolved	2	1	10239	pull-request-available
13441046	Support reading external table store table in Hive	As the first step to support table store in Hive, we should allow Hive external table to read table store files created by other systems (for example Flink).	FLINK	Closed	3	7	10239	pull-request-available
13434330	Introduce flink-table-store-dist module for packaging	We need a flink-table-store-dist module to produce a fat connector jar for table store.	FLINK	Closed	3	7	10239	pull-request-available
13370377	Join & Select a part of composite primary key will cause ArrayIndexOutOfBoundsException	"Add the following test case to {{org.apache.flink.table.planner.plan.stream.sql.join.JoinTest}} to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  util.tableEnv.executeSql(
    """"""
      |CREATE TABLE MyTable (
      |  pk1 INT,
      |  pk2 BIGINT,
      |  PRIMARY KEY (pk1, pk2) NOT ENFORCED
      |) WITH (
      |  'connector'='values'
      |)
      |"""""".stripMargin)
  util.verifyExecPlan(""SELECT A.a1 FROM A LEFT JOIN MyTable ON A.a1 = MyTable.pk1"")
}
{code}

The exception stack is
{code}
java.lang.RuntimeException: Error while applying rule StreamPhysicalJoinRule, args [rel#141:FlinkLogicalJoin.LOGICAL.any.None: 0.[NONE].[NONE](left=RelSubset#139,right=RelSubset#140,condition==($0, $1),joinType=left), rel#138:FlinkLogicalCalc.LOGICAL.any.None: 0.[NONE].[NONE](input=RelSubset#137,select=a1), rel#121:FlinkLogicalTableSourceScan.LOGICAL.any.None: 0.[NONE].[NONE](table=[default_catalog, default_database, MyTable, project=[pk1]],fields=pk1)]

	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:281)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:583)
	at org.apache.flink.table.planner.plan.stream.sql.join.JoinTest.myTest(JoinTest.scala:300)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.lang.RuntimeException: Error occurred while applying rule StreamPhysicalJoinRule
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalJoinRuleBase.onMatch(StreamPhysicalJoinRuleBase.scala:90)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
	... 49 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.calcite.util.ImmutableBitSet.of(ImmutableBitSet.java:113)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.getTableUniqueKeys(FlinkRelMdUniqueKeys.scala:76)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.getUniqueKeys(FlinkRelMdUniqueKeys.scala:59)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys_$(Unknown Source)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getUniqueKeys(RelMetadataQuery.java:464)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueKeys.getUniqueKeys(FlinkRelMdUniqueKeys.scala:591)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys_$(Unknown Source)
	at GeneratedMetadataHandler_UniqueKeys.getUniqueKeys(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getUniqueKeys(RelMetadataQuery.java:464)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getUniqueKeys(RelMetadataQuery.java:445)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalJoin.getUniqueKeys(StreamPhysicalJoin.scala:111)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalJoin.explainTerms(StreamPhysicalJoin.scala:107)
	at org.apache.calcite.rel.AbstractRelNode.getDigestItems(AbstractRelNode.java:409)
	at org.apache.calcite.rel.AbstractRelNode.deepHashCode(AbstractRelNode.java:391)
	at org.apache.calcite.rel.AbstractRelNode$InnerRelDigest.hashCode(AbstractRelNode.java:443)
	at java.util.HashMap.hash(HashMap.java:339)
	at java.util.HashMap.get(HashMap.java:557)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1150)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
	... 53 more
{code}

This is because {{FlinkRelMdUniqueKeys#getTableUniqueKeys}} does not consider the case when projections are pushed down and cause only a part of the composite primary key to be selected."	FLINK	Closed	3	1	10239	pull-request-available
13447187	Add an abstraction layer for connectors to read and write row data instead of key-values	"Currently {{FileStore}} exposes an interface for reading and writing {{KeyValue}}. However connectors may have different ways to change a {{RowData}} to {{KeyValue}} under different {{WriteMode}}. This results in lots of {{if...else...}} branches and duplicated code.

We need to add an abstraction layer for connectors to read and write row data instead of key-values."	FLINK	Closed	3	4	10239	pull-request-available, stale-assigned
13486027	Extract changelog files out of DataFileMeta#extraFiles	"Currently changelog files are stored as extra files in {{DataFileMeta}}. However for the full compaction changelog we're about to introduce, it cannot be added as extra files because their statistics might be different from the corresponding merge tree files.

We need to extract changelog files out of DataFileMeta#extraFiles."	FLINK	Closed	3	7	10239	pull-request-available
13515681	Add documentation for writing Table Store with Spark3	Table Store 0.3 supports writing with Spark3. We need to add documentation.	FLINK	Closed	3	4	10239	pull-request-available
13368636	Could not find FLINSHED Flink job and can't submit job 	Could not find FLINSHED Flink job,  and aways can't submit job by insufficient slot	FLINK	Closed	3	1	10239	stale-assigned
13450118	PredicateBuilder.in should accept null parameters	"PredicateBuilder.in(int idx, List<Literal> literals).

Literals must be not null, but this is not enough to meet the needs of SQL `in`.

It is allowed to have null parameters in `in`."	FLINK	Closed	1	1	10239	pull-request-available
13338765	Multiple input creation algorithm will deduce an incorrect input order if the inputs are related under PIPELINED shuffle mode	"Consider the following SQL
{code:sql}
WITH
  T1 AS (SELECT x.a AS a, y.d AS b FROM y LEFT JOIN x ON y.d = x.a),
  T2 AS (SELECT a, b FROM (SELECT a, b FROM T1) UNION ALL (SELECT x.a AS a, x.b AS b FROM x))
SELECT * FROM T2 LEFT JOIN t ON T2.a = t.a
{code}

The multiple input creation algorithm will currently deduce the following plan under the PIPELINED shuffle mode:
{code}
MultipleInputNode(readOrder=[0,1,1,0], members=[\nNestedLoopJoin(joinType=[LeftOuterJoin], where=[=(a, a0)], select=[a, b, a0, b0, c], build=[right])\n:- Union(all=[true], union=[a, b])\n:  :- Calc(select=[a, CAST(d) AS b])\n:  :  +- NestedLoopJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, a], build=[right])\n:  :     :- [#3] Calc(select=[d])\n:  :     +- [#4] Exchange(distribution=[broadcast])\n:  +- [#2] Calc(select=[a, b])\n+- [#1] Exchange(distribution=[broadcast])\n])
:- Exchange(distribution=[broadcast])
:  +- BoundedStreamScan(table=[[default_catalog, default_database, t]], fields=[a, b, c])
:- Calc(select=[a, b])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c, nx)]]], fields=[a, b, c, nx], reuse_id=[1])
:- Calc(select=[d])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f, ny)]]], fields=[d, e, f, ny])
+- Exchange(distribution=[broadcast])
   +- Calc(select=[a])
      +- Reused(reference_id=[1])
{code}

It's obvious that the 2nd and the 4th input for the multiple input node should have the same input priority, otherwise a deadlock will occur.

This is because the current algorithm fails to consider the case when the inputs are related out of the multiple input node."	FLINK	Closed	3	1	10239	pull-request-available
13450582	Table Store Hive Reader supports projection pushdown	When the user declares fields in the DDL, we may not report an error when the declared fields are incomplete, at this time we can assume that the user only wants to read these fields, in fact, it is projection pushdown	FLINK	Closed	3	7	10239	pull-request-available
13296755	Improve handling of unexpected input in config.sh#extractExecutionParams	"In FLINK-15727 {{BashJavaUtils}} now returns multiple lines of results to avoid using {{BashJavaUtils}} twice. But now the format checking in {{extractExecutionParams}} for the last line is incorrect. Instead of
{code:bash}
if ! [[ $execution_config =~ ^${EXECUTION_PREFIX}.* ]]; then
    echo ""[ERROR] Unexpected result: $execution_config"" 1>&2
    echo ""[ERROR] The last line of the BashJavaUtils outputs is expected to be the execution result, following the prefix '${EXECUTION_PREFIX}'"" 1>&2
    echo ""$output"" 1>&2
    exit 1
fi
{code}
It should be
{code:bash}
last_line=`echo ""$execution_config"" | tail -n 1`
if ! [[ ""$last_line"" =~ ^${EXECUTION_PREFIX}.* ]]; then
# ...
{code}"	FLINK	Closed	3	4	10239	pull-request-available
13335325	Update deadlock break-up algorithm to cover more cases	"Current deadlock breakup algorithm fails to cover the following case:

 !pasted image 0.png! 

We're going to introduce a new deadlock breakup algorithm to cover this."	FLINK	Closed	3	7	10239	pull-request-available
13398489	"""Internal server error"" when quitting a SELECT query in the SqlClient"	"Release testing revealed an Internal server error being reported when quitting a SELECT statement in the SQL Client (see details on how to reproduce it in the [comment of FLINK-23850|https://issues.apache.org/jira/browse/FLINK-23850?focusedCommentId=17407235&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17407235]):
{code}
2021-08-31 12:34:37,091 WARN  org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (3df388d05273cc9782458228081ea5bf)
        at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:909)
        at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:923)
        at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:719)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

End of exception on server side>]
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_265]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:163) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:128) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370) [flink-table_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:74) [flink-sql-client_2.12-1.14.0.jar:1.14.0]
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (3df388d05273cc9782458228081ea5bf)
        at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:909)
        at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:923)
        at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:719)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

End of exception on server side>]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:532) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:512) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_265]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_265]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_265]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
{code}"	FLINK	Closed	3	1	10239	pull-request-available
13528332	Introduce CDC sink for Table Store	To directly consume changes from Flink CDC connectors, we need a special CDC sink for Flink Table Store.	FLINK	Closed	3	7	10239	pull-request-available
13517803	Table Store Hive catalog throws ClassNotFoundException when custom hive-site.xml is presented	"For Hive 2.3.9, if a custom {{hive-site.xml}} is presented in {{$HIVE_HOME/conf}}, when creating Table Store Hive catalog in Flink, the following exception will be thrown.

{code}
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2273) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2367) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.loadFilterHooks(HiveMetaStoreClient.java:250) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:145) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_352]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_352]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_352]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_352]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:415) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:82) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:106) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:66) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:435) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1426) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1172) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16.0.jar:1.16.0]
	... 10 more
{code}

This is because {{hive-default.xml.template}} contains a property named {{hive.metastore.filter.hook}}. Its default value is {{org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl}}. However all Hive packages in Table Store are shaded, so the class loader cannot find the original class.

we need to remove relocation of Hive classes."	FLINK	Closed	3	1	10239	pull-request-available
13445089	Table Store throws NullPointerException when pushing down NotEqual predicate to a column consisting of nulls	"Run the following Flink SQL to reproduce this issue.
{code}
Flink SQL> create table S ( a double ) with ( 'path' = '/tmp/store' );
[INFO] Execute statement succeed.

Flink SQL> insert into S values (cast(null as double)), (cast(null as double));
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: edb2ce383c00b2f635759dee70add73d


Flink SQL> select * from S where a <> 1;
[ERROR] Could not execute SQL statement. Reason:
java.util.concurrent.ExecutionException: java.lang.NullPointerException
{code}

The exception stack is
{code}
Caused by: java.lang.NullPointerException
	at java.lang.Double.compareTo(Double.java:978) ~[?:1.8.0_151]
	at java.lang.Double.compareTo(Double.java:49) ~[?:1.8.0_151]
	at org.apache.flink.table.store.file.predicate.Literal.compareValueTo(Literal.java:60) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at org.apache.flink.table.store.file.predicate.NotEqual.test(NotEqual.java:50) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at org.apache.flink.table.store.file.operation.FileStoreScanImpl.filterManifestEntry(FileStoreScanImpl.java:287) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174) ~[?:1.8.0_151]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[?:1.8.0_151]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721) ~[?:1.8.0_151]
	at java.util.stream.AbstractTask.compute(AbstractTask.java:316) ~[?:1.8.0_151]
	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_151]
	at org.apache.flink.table.store.file.operation.FileStoreScanImpl.lambda$plan$3(FileStoreScanImpl.java:221) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_151]
{code}"	FLINK	Closed	3	1	10239	pull-request-available
13392326	Add Flink 1.13 MigrationVersion	"Currently the largest MigrationVersion is 1.12. We need newer versions to add more serializer compatibility tests.

As stated in [https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release#CreatingaFlinkRelease-Checklisttoproceedtothenextstep.1] this should be the work of release manager."	FLINK	Closed	3	4	10239	pull-request-available
13427126	Implement ProjectableDecodingFormat for avro bulk format	Currently {{BulkDecodingFormat}} of avro does not also implement {{ProjectableDecodingFormat}} and the reader will have to read unused columns. We need to implement {{ProjectableDecodingFormat}} to optimize reading from avro files.	FLINK	Closed	3	4	10239	pull-request-available
13440133	Fix table store connector throws ClassNotFoundException: org.apache.flink.table.store.shaded.org.apache.flink.connector.file.table.RowDataPartitionComputer	"This is caused by FLINK-27172. Currently table store excludes file connector dependencies shading as follows:
{code}
<exclude>org.apache.flink.connector.base.*</exclude>
<exclude>org.apache.flink.connector.file.*</exclude>
{code}

However this only excludes classes in {{org.apache.flink.connector.base}} and {{org.apache.flink.connector.file}} packages and does not exclude classes in their sub-packages. The correct excluding pattern should be:
{code}
<exclude>org.apache.flink.connector.base.**</exclude>
<exclude>org.apache.flink.connector.file.**</exclude>
{code}

This change will also be checked by e2e tests in the near future."	FLINK	Closed	3	7	10239	pull-request-available
13513547	Rename 'full' to 'latest-full' and 'compacted' to 'compacted-full' for scan.mode in Table Store	"As discussed in the [dev mailing list|https://lists.apache.org/thread/t76f0ofl6k7mlvqotp57ot10x8o1x90p], we're going to rename some values for {{scan.mode}} in Table Store.

Specifically, {{full}} will be renamed to {{latest-full}} and {{compacted}} will be renamed to {{compacted-full}}, so user can understand that a full snapshot will always be produced, no matter for a batch job or a streaming job."	FLINK	Closed	3	4	10239	pull-request-available
13472200	Throw exception intentionally when new snapshots are committed during restore	"Currently snapshots are committed in {{notifyCheckpointComplete}}. If the job fails between a successful checkpoint and the call of {{notifyCheckpointComplete}}, these snapshots will be committed after job restarts.

However when the writer starts they also need to read from the latest snapshot (to build the latest structure of LSM tree). These two steps may happen concurrently and what the writers see may not be the latest snapshot.

To fix this problem, we can throw exception intentionally after new snapshots are committed during restore. In this way the job will be forcefully restarted and it is very likely that the writers can see the latest snapshot."	FLINK	Closed	3	4	10239	pull-request-available
13383835	NullPointerException when cast string literal to date or time	"sql:
{code:java}
CREATE TABLE source_table
(
    id               INT,
    score            INT,
    address          STRING,
    create_time      TIME,
    create_date      DATE,
    create_timestamp TIMESTAMP
) WITH (
      'connector' = 'datagen'
      );

CREATE TABLE console_table
(
    create_time      TIME,
    create_date      DATE,
    create_timestamp TIMESTAMP
) WITH (
      'connector' = 'print'
      );

INSERT INTO console_table
SELECT CASE
           WHEN A.create_time IS NULL
               OR A.create_time = '' THEN CURRENT_TIME
           ELSE A.create_time
           END
           AS create_time,
       CASE
           WHEN A.create_date IS NULL
               OR A.create_date = '' THEN CURRENT_DATE
           ELSE A.create_date
           END
           AS create_date,
       CASE
           WHEN A.create_timestamp IS NULL
               OR A.create_timestamp = '' THEN CURRENT_TIMESTAMP
           ELSE A.create_timestamp
           END
           AS create_timestamp
FROM source_table A;
{code}
exception:
{code:java}
java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23'java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:652) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:626) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:566) at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:181) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:548) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:537) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64) ... 12 moreCaused by: java.lang.NullPointerException at StreamExecCalc$23.<init>(Unknown Source) ... 16 more
{code}"	FLINK	Closed	3	1	10239	pull-request-available
13481412	Introduce a MergeFunction for full compaction	We need to introduce a special {{MergeFunction}} to produce changelogs.	FLINK	Closed	3	7	10239	pull-request-available
13305196	Add specialized collecting iterator	"Add specialized collecting iterator is needed to implement the algorithms in FLINK-14807

As legacy planner does not have data structures like {{ResettableExternalBuffer}} in Blink planner, we're not implementing the algorithms in legacy planner. Legacy planner just use the current collect implementation."	FLINK	Closed	3	7	10239	pull-request-available
13424812	TableITCase.testCollectWithClose failed on azure	"
{code:java}
2022-01-25T08:35:25.3735884Z Jan 25 08:35:25 [ERROR] TableITCase.testCollectWithClose  Time elapsed: 0.377 s  <<< FAILURE!
2022-01-25T08:35:25.3737127Z Jan 25 08:35:25 java.lang.AssertionError: Values should be different. Actual: RUNNING
2022-01-25T08:35:25.3738167Z Jan 25 08:35:25 	at org.junit.Assert.fail(Assert.java:89)
2022-01-25T08:35:25.3739085Z Jan 25 08:35:25 	at org.junit.Assert.failEquals(Assert.java:187)
2022-01-25T08:35:25.3739922Z Jan 25 08:35:25 	at org.junit.Assert.assertNotEquals(Assert.java:163)
2022-01-25T08:35:25.3740846Z Jan 25 08:35:25 	at org.junit.Assert.assertNotEquals(Assert.java:177)
2022-01-25T08:35:25.3742302Z Jan 25 08:35:25 	at org.apache.flink.table.api.TableITCase.testCollectWithClose(TableITCase.scala:135)
2022-01-25T08:35:25.3743327Z Jan 25 08:35:25 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-25T08:35:25.3744343Z Jan 25 08:35:25 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-25T08:35:25.3745575Z Jan 25 08:35:25 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-25T08:35:25.3746840Z Jan 25 08:35:25 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-25T08:35:25.3747922Z Jan 25 08:35:25 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-25T08:35:25.3749151Z Jan 25 08:35:25 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-25T08:35:25.3750422Z Jan 25 08:35:25 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-25T08:35:25.3751820Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-25T08:35:25.3753196Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-01-25T08:35:25.3754253Z Jan 25 08:35:25 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-01-25T08:35:25.3755441Z Jan 25 08:35:25 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
2022-01-25T08:35:25.3756656Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3757778Z Jan 25 08:35:25 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-01-25T08:35:25.3758821Z Jan 25 08:35:25 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-01-25T08:35:25.3759840Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-25T08:35:25.3760919Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-25T08:35:25.3762249Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-25T08:35:25.3763322Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-25T08:35:25.3764436Z Jan 25 08:35:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-25T08:35:25.3765907Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-25T08:35:25.3766957Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-25T08:35:25.3768104Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-25T08:35:25.3769128Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-25T08:35:25.3770125Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-25T08:35:25.3771118Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-25T08:35:25.3772264Z Jan 25 08:35:25 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-01-25T08:35:25.3773118Z Jan 25 08:35:25 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-01-25T08:35:25.3774092Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-25T08:35:25.3775056Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-25T08:35:25.3776144Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-25T08:35:25.3777125Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-25T08:35:25.3778190Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-25T08:35:25.3779234Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3780354Z Jan 25 08:35:25 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-01-25T08:35:25.3781583Z Jan 25 08:35:25 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-01-25T08:35:25.3782721Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-25T08:35:25.3783724Z Jan 25 08:35:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-25T08:35:25.3784663Z Jan 25 08:35:25 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-25T08:35:25.3785542Z Jan 25 08:35:25 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-25T08:35:25.3786641Z Jan 25 08:35:25 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-25T08:35:25.3787854Z Jan 25 08:35:25 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-25T08:35:25.3789028Z Jan 25 08:35:25 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-25T08:35:25.3790347Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-25T08:35:25.3791934Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-25T08:35:25.3793503Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-25T08:35:25.3794936Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-25T08:35:25.3796384Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-25T08:35:25.3797588Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-25T08:35:25.3798765Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-25T08:35:25.3799989Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-25T08:35:25.3801441Z Jan 25 08:35:25 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-25T08:35:25.3802916Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-01-25T08:35:25.3804358Z Jan 25 08:35:25 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-01-25T08:35:25.3805461Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-01-25T08:35:25.3806909Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-25T08:35:25.3808260Z Jan 25 08:35:25 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-01-25T08:35:25.3809487Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-25T08:35:25.3810614Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-25T08:35:25.3811838Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-25T08:35:25.3813068Z Jan 25 08:35:25 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-01-25T08:35:25.3813890Z Jan 25 08:35:25 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30101&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9693"	FLINK	Closed	2	1	10239	pull-request-available, stale-assigned, test-stability
13437402	Use '<format>.<option>' instead of 'file.<format>.<option>' in table properties of managed table	Currently if we want to set compression method of file store we need to use 'file.orc.compress'. It would be better to use 'orc.compress' directly, just like what we do for filesystem connectors.	FLINK	Closed	3	7	10239	pull-request-available
13305837	CollectSinkFunctionTest.testCheckpointedFunction fails with  expected:<50> but was:<0>	"CI https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1793&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d

{code}
2020-05-19T04:28:24.6400821Z [INFO] Running org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkBroadcastThroughputBenchmarkTest
2020-05-19T04:28:25.7508457Z java.util.concurrent.TimeoutException
2020-05-19T04:28:25.7511469Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
2020-05-19T04:28:25.7512331Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-05-19T04:28:25.7513238Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.sendRequest(CollectSinkFunctionTest.java:391)
2020-05-19T04:28:25.7514280Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.access$300(CollectSinkFunctionTest.java:60)
2020-05-19T04:28:25.7515499Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest$TestCollectRequestSender.sendRequest(CollectSinkFunctionTest.java:465)
2020-05-19T04:28:25.7516628Z 	at org.apache.flink.streaming.api.operators.collect.utils.TestCollectClient.run(TestCollectClient.java:77)
2020-05-19T04:28:25.7517231Z java.lang.InterruptedException
2020-05-19T04:28:25.7517907Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
2020-05-19T04:28:25.7518816Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)
2020-05-19T04:28:25.7519745Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.invoke(CollectSinkFunction.java:246)
2020-05-19T04:28:25.7520773Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest$CheckpointedDataFeeder.run(CollectSinkFunctionTest.java:574)
2020-05-19T04:28:25.7600809Z [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 53.526 s <<< FAILURE! - in org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest
2020-05-19T04:28:25.7602305Z [ERROR] testCheckpointedFunction(org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest)  Time elapsed: 26.262 s  <<< FAILURE!
2020-05-19T04:28:25.7603126Z java.lang.AssertionError: expected:<50> but was:<0>
2020-05-19T04:28:25.7603600Z 	at org.junit.Assert.fail(Assert.java:88)
2020-05-19T04:28:25.7604100Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-05-19T04:28:25.7604810Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2020-05-19T04:28:25.7605373Z 	at org.junit.Assert.assertEquals(Assert.java:631)
2020-05-19T04:28:25.7606127Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.assertResultsEqual(CollectSinkFunctionTest.java:430)
2020-05-19T04:28:25.7607201Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.assertResultsEqualAfterSort(CollectSinkFunctionTest.java:441)
2020-05-19T04:28:25.7608296Z 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.testCheckpointedFunction(CollectSinkFunctionTest.java:321)
2020-05-19T04:28:25.7609371Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-05-19T04:28:25.7610031Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-05-19T04:28:25.7610819Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-05-19T04:28:25.7611665Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-05-19T04:28:25.7612348Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-05-19T04:28:25.7613094Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-05-19T04:28:25.7613785Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-05-19T04:28:25.7614493Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-05-19T04:28:25.7615370Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-05-19T04:28:25.7616067Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-05-19T04:28:25.7616711Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-19T04:28:25.7617574Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-19T04:28:25.7618159Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-19T04:28:25.7618789Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-19T04:28:25.7619486Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-19T04:28:25.7620134Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-19T04:28:25.7620788Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-19T04:28:25.7621528Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-19T04:28:25.7622174Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-19T04:28:25.7622835Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-19T04:28:25.7623473Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-19T04:28:25.7624165Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-05-19T04:28:25.7625144Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-05-19T04:28:25.7625939Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-05-19T04:28:25.7626730Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-05-19T04:28:25.7627545Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-19T04:28:25.7628322Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-19T04:28:25.7629082Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-19T04:28:25.7629737Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-19T04:28:25.7630123Z 
2020-05-19T04:28:26.3490861Z [INFO] Running org.apache.flink.streaming.runtime.io.benchmark.StreamNetworkThroughputBenchmarkTest
{code}"	FLINK	Resolved	1	1	10239	test-stability
13369265	ResultType of GeneratedExpression in StringCallGen should be compatible with their definition in FlinkSqlOperatorTable	"Add the following test case to {{TableEnvironmentITCase}} to reproduce this bug:

{code:scala}
@Test
def myTest(): Unit = {
  tEnv.executeSql(
    """"""
      |CREATE TABLE my_source (
      |  a VARCHAR(10)
      |) WITH (
      |  'connector'='values',
      |  'bounded'='true'
      |)
      |"""""".stripMargin)
  tEnv.explainSql(""SELECT ifnull(substring(a, 2, 5), 'null') FROM my_source"")
}
{code}

The exception stack is
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Mismatch of function's argument data type 'VARCHAR(10)' and actual argument type 'STRING'.

	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:323)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$$anonfun$verifyArgumentTypes$1.apply(BridgingFunctionGenUtil.scala:320)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.verifyArgumentTypes(BridgingFunctionGenUtil.scala:320)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCallWithDataType(BridgingFunctionGenUtil.scala:95)
	at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCall(BridgingFunctionGenUtil.scala:65)
	at org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.generate(BridgingSqlFunctionCallGen.scala:73)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:832)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:529)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:56)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:155)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$2.apply(CalcCodeGenerator.scala:141)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$$anonfun$2.apply(CalcCodeGenerator.scala:141)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:141)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:167)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:50)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:127)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:71)
	at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:70)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:70)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:109)
	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:47)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:691)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainSql(TableEnvironmentImpl.java:677)
{code}

This is because the return type inference of {{SUBSTR}} in {{FlinkSqlOperatorTable}} is {{ARG0_VARCHAR_FORCE_NULLABLE}}, but the result type of the GeneratedExpression is {{VARCHAR(INT_MAX)}}."	FLINK	Closed	3	1	10239	pull-request-available
13368368	PushFilterIntoTableSourceScanRule fails to deal with NULLs	"Add the following test case to {{PushFilterIntoTableSourceScanRuleTest}} to reproduce this bug:

{code:java}
@Test
public void myTest() {
    String ddl =
            ""CREATE TABLE MTable (""
                    + ""  a STRING,""
                    + ""  b STRING""
                    + "") WITH (""
                    + ""  'connector' = 'values',""
                    + ""  'bounded' = 'true'""
                    + "")"";
    util().tableEnv().executeSql(ddl);
    util().verifyRelPlan(""WITH MView AS (SELECT CASE\n""
            + ""  WHEN a = b THEN a\n""
            + ""  ELSE CAST(NULL AS STRING)\n""
            + ""  END AS a\n""
            + ""  FROM MTable)\n""
            + ""SELECT a FROM MView WHERE a IS NOT NULL"");
}
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: Data type 'STRING NOT NULL' does not support null values.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:272)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:79)
	at org.apache.flink.table.expressions.ApiExpressionUtils.valueLiteral(ApiExpressionUtils.java:251)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:463)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1173)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:470)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter$$anonfun$8.apply(RexNodeExtractor.scala:471)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:470)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:361)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:138)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$$anonfun$extractConjunctiveConditions$1.apply(RexNodeExtractor.scala:137)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:137)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor.extractConjunctiveConditions(RexNodeExtractor.scala)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.pushFilterIntoScan(PushFilterIntoTableSourceScanRule.java:121)
	at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.onMatch(PushFilterIntoTableSourceScanRule.java:100)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:276)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:889)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:780)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyRelPlan(TableTestBase.scala:400)
{code}

It seems that this bug is related to commit 957c49d56c80416ae712ae79cdd2784bb2387c80 by [~dwysakowicz]. This commit is a hotfix related to no issue. It adds a {{notNull}} to the return value of RexNodeExtractor#visitLiteral."	FLINK	Closed	3	1	10239	pull-request-available
13335390	Optimize parallelism calculating of HiveTableSource by checking file number	"The current implementation of {{HiveTableSource#createBatchSource}} for calculating parallelism directly uses {{inputFormat.createInputSplits(0).length}} as the number of splits. However {{createInputSplits}} may be costly as it will read some data from all source files, especially when the table is not partitioned and the number of files are large.

Many Hive tables maintain the number of files in that table, and it's obvious that the number of splits is at least the number of files. So we can try to fetch the number of files (almost without cost) first and if the number of files already exceeds maximum parallelism we can directly use the maximum parallelism without calling {{createInputSplits}}.

This is a significant optimization on the current Flink TPCDS benchmark, which will create some table with 15000 files without partitioning. This optimization will improve the performance of the whole benchmark by 300s and more."	FLINK	Closed	3	4	10239	pull-request-available
13429298	Introduce a better expire strategy	We can add the snapshot id in which the file/manifest was added to the table. With this snapshot id, we can have better expire strategy. Instead of scanning all files of the snapshot.	FLINK	Closed	3	7	10239	pull-request-available
13448416	SearchArgumentToPredicateConverter can not handle partial filters	"For example: SLECT * FROM T WHERE a = 1 and ${unsupported_filters};

According to the reasonable path, the condition a=1 should be able to be pushed down to the table store, but currently the whole will throw UnsupportedOperationException, resulting in a=1 not being pushed down."	FLINK	Closed	3	7	10239	pull-request-available
13406713	Comparing timstamp_ltz with random string throws NullPointerException	"Add the following test case to {{org.apache.flink.table.planner.runtime.batch.sql.CalcITCase}} to reproduce this issue.
{code:scala}
@Test
def myTest(): Unit = {
  val data: Seq[Row] = Seq(row(
    LocalDateTime.of(2021, 10, 15, 0, 0, 0).toInstant(ZoneOffset.UTC)))
  val dataId = TestValuesTableFactory.registerData(data)
  val ddl =
    s""""""
       |CREATE TABLE MyTable (
       |  ltz TIMESTAMP_LTZ
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$dataId',
       |  'bounded' = 'true'
       |)
       |"""""".stripMargin
  tEnv.executeSql(ddl)

  checkResult(""SELECT ltz = uuid() FROM MyTable"", Seq(row(null)))
}
{code}

The exception stack is
{code}
java.lang.RuntimeException: Failed to fetch next result

	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:109)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest(CalcITCase.scala:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 41 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
	... 43 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:614)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1983)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
	... 43 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:678)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.lang.NullPointerException
	at BatchExecCalc$7.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:330)
{code}

This is because {{ScalarOperatorGens#generateCast}} will generate the following code when casting strings to timestamp_ltz.
{code:java}
result$6 = org.apache.flink.table.data.TimestampData.fromEpochMillis(org.apache.flink.table.runtime.functions.SqlDateTimeUtils.toTimestamp(result$3.toString(), timeZone));
{code}

{{org.apache.flink.table.runtime.functions.SqlDateTimeUtils.toTimestamp}} might returns {{null}} while {{org.apache.flink.table.data.TimestampData.fromEpochMillis}} only accepts primitive long values, thus causing this issue.

What we need to do is to check the result of {{toTimestamp}}."	FLINK	Closed	3	1	10239	pull-request-available
13388934	Collect Sink Operator Factory does not support to configure max bytes per batch	Current {{CollectSinkOperatorFactory}} only supports max bytes per batch as 2MB. However, this configuration might not be enough for production users and we should provide ability to make this configurable. 	FLINK	Closed	3	4	10239	pull-request-available
13379544	Lookup join condition with CURRENT_DATE fails to filter records	"Add the following test case to org.apache.flink.table.api.TableEnvironmentITCase to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  val id1 = TestValuesTableFactory.registerData(
    Seq(Row.of(""abc"", LocalDateTime.of(2000, 1, 1, 0, 0))))
  val ddl1 =
    s""""""
       |CREATE TABLE Ta (
       |  id VARCHAR,
       |  ts TIMESTAMP,
       |  proc AS PROCTIME()
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$id1',
       |  'bounded' = 'true'
       |)
       |"""""".stripMargin
  tEnv.executeSql(ddl1)

  val id2 = TestValuesTableFactory.registerData(
    Seq(Row.of(""abc"", LocalDateTime.of(2000, 1, 2, 0, 0))))
  val ddl2 =
    s""""""
       |CREATE TABLE Tb (
       |  id VARCHAR,
       |  ts TIMESTAMP
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$id2',
       |  'bounded' = 'true'
       |)
       |"""""".stripMargin
  tEnv.executeSql(ddl2)

  val it = tEnv.executeSql(
    """"""
      |SELECT * FROM Ta AS t1
      |INNER JOIN Tb FOR SYSTEM_TIME AS OF t1.proc AS t2
      |ON t1.id = t2.id
      |WHERE CAST(coalesce(t1.ts, t2.ts) AS VARCHAR) >= CONCAT(CAST(CURRENT_DATE AS VARCHAR), ' 00:00:00')
      |"""""".stripMargin).collect()

  while (it.hasNext) {
    System.out.println(it.next())
  }
}
{code}

The result is
{code}
+I[abc, 2000-01-01T00:00, 2021-05-20T14:30:47.735Z, abc, 2000-01-02T00:00]
{code}

which is obviously incorrect.

The generated operator is as follows

{code:java}
public class JoinTableFuncCollector$22 extends org.apache.flink.table.runtime.collector.TableFunctionCollector {

    org.apache.flink.table.data.GenericRowData out = new org.apache.flink.table.data.GenericRowData(2);
    org.apache.flink.table.data.utils.JoinedRowData joinedRow$9 = new org.apache.flink.table.data.utils.JoinedRowData();
    private static final java.util.TimeZone timeZone =
            java.util.TimeZone.getTimeZone(""Asia/Shanghai"");
    private org.apache.flink.table.data.TimestampData timestamp;
    private org.apache.flink.table.data.TimestampData localTimestamp;
    private int date;

    private final org.apache.flink.table.data.binary.BinaryStringData str$17 = org.apache.flink.table.data.binary.BinaryStringData.fromString("" 00:00:00"");


    public JoinTableFuncCollector$22(Object[] references) throws Exception {

    }

    @Override
    public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {

    }

    @Override
    public void collect(Object record) throws Exception {
        org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) getInput();
        org.apache.flink.table.data.RowData in2 = (org.apache.flink.table.data.RowData) record;
        org.apache.flink.table.data.binary.BinaryStringData field$7;
        boolean isNull$7;
        org.apache.flink.table.data.TimestampData field$8;
        boolean isNull$8;
        org.apache.flink.table.data.TimestampData field$10;
        boolean isNull$10;
        boolean isNull$13;
        org.apache.flink.table.data.binary.BinaryStringData result$14;
        boolean isNull$15;
        org.apache.flink.table.data.binary.BinaryStringData result$16;
        boolean isNull$18;
        org.apache.flink.table.data.binary.BinaryStringData result$19;
        boolean isNull$20;
        boolean result$21;
        isNull$8 = in2.isNullAt(1);
        field$8 = null;
        if (!isNull$8) {
            field$8 = in2.getTimestamp(1, 6);
        }
        isNull$7 = in2.isNullAt(0);
        field$7 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        if (!isNull$7) {
            field$7 = ((org.apache.flink.table.data.binary.BinaryStringData) in2.getString(0));
        }
        isNull$10 = in1.isNullAt(1);
        field$10 = null;
        if (!isNull$10) {
            field$10 = in1.getTimestamp(1, 6);
        }



        boolean result$11 = !isNull$10;
        org.apache.flink.table.data.TimestampData result$12 = null;
        boolean isNull$12;
        if (result$11) {

            isNull$12 = isNull$10;
            if (!isNull$12) {
                result$12 = field$10;
            }
        }
        else {

            isNull$12 = isNull$8;
            if (!isNull$12) {
                result$12 = field$8;
            }
        }
        isNull$13 = isNull$12;
        result$14 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        if (!isNull$13) {

            result$14 = org.apache.flink.table.data.binary.BinaryStringData.fromString(org.apache.flink.table.runtime.functions.SqlDateTimeUtils.timestampToString(result$12, 6));
            isNull$13 = (result$14 == null);
        }




        isNull$15 = false;
        result$16 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        if (!isNull$15) {

            result$16 = org.apache.flink.table.data.binary.BinaryStringData.fromString(org.apache.calcite.avatica.util.DateTimeUtils.unixDateToString(((int) date)));
            isNull$15 = (result$16 == null);
        }


        result$19 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(( isNull$15 ) ? null : (result$16), ( false ) ? null : (((org.apache.flink.table.data.binary.BinaryStringData) str$17)));
        isNull$18 = (result$19 == null);
        if (isNull$18) {
            result$19 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        }

        isNull$20 = isNull$13 || isNull$18;
        result$21 = false;
        if (!isNull$20) {

            result$21 = ((result$14 == null) ? ((result$19 == null) ? 0 : -1) : ((result$19 == null) ? 1 : (result$14.compareTo(result$19)))) >= 0;

        }

        if (result$21) {





            if (isNull$7) {
                out.setField(0, null);
            } else {
                out.setField(0, field$7);
            }



            if (isNull$8) {
                out.setField(1, null);
            } else {
                out.setField(1, field$8);
            }


            joinedRow$9.replace(in1, out);
            joinedRow$9.setRowKind(in1.getRowKind());
            outputResult(joinedRow$9);

        }

    }

    @Override
    public void close() throws Exception {

    }
}
{code}

The member variable {{date}} is not initialized before use, thus causing this bug.

This is because {{LookupJoinCodeGenerator#generateTableFunctionCollectorForJoinTable}} forgets to use {{${ctx.reusePerRecordCode()}}} when generating {{collect}} method."	FLINK	Closed	3	1	10239	pull-request-available
13427379	Support projection pushdown on keys and values in sst file readers	Projection pushdown is an optimization for sources. With this optimization, we can avoid reading useless columns and thus improve performance.	FLINK	Closed	3	7	10239	pull-request-available
13409604	Batch SQL file sink forgets to close the output stream	"I tried to write a large avro file into HDFS and discover that the displayed file size in HDFS is extremely small, but copying that file to local yields the correct size. If we create another Flink job and read that avro file from HDFS, the job will finish without outputting any record because the file size Flink gets from HDFS is the very small file size.

This is because the output format created in {{FileSystemTableSink#createBulkWriterOutputFormat}} only finishes the {{BulkWriter}}. According to the java doc of {{BulkWriter#finish}} bulk writers should not close the output stream and should leave them to the framework."	FLINK	Closed	3	1	10239	pull-request-available
13438552	Disable rolling file when writing level 0 sst files	Each level 0 sst file is a sorted run. If we write rolling files then we cannot decrease number of sorted runs.	FLINK	Closed	3	7	10239	pull-request-available
13447860	Refactor RecordReader and RecordIterator in table store into generic types	We'd like to introduce a `TableRead` class between file store and the users. This class should provide an iterator for `RowData` instead of `KeyValue`s. So we'll need an iterator not only for `KeyValue` but also for `RowData`.	FLINK	Closed	3	7	10239	pull-request-available
13360062	Add a SqlExpression in table-common	We introduced a dedicated function definition in FLINK-20971 to use SQL expression in Table API. However, for usability and clear Maven module design it is better add a dedicated `SqlExpression` in `table-common`. This allows catalogs to specify an unresolved SQL expression without a dependency on `table-api` furthermore we can add SQL specific functions to the new `Schema` class.	FLINK	Closed	3	7	10269	pull-request-available
13321301	Add StreamStatementSet.attachAsDataStream	"StatementSet solves use cases for pure SQL & Table API pipelines. However, currently there is no way of creating StatementSet for a DataStream API job.

We propose the following API:
{code}
StreamTableEnvironment.createStatementSet(): StreamStatementSet // return a stream-specific set

StreamStatementSet extends StatementSet {
  /**
   * Attaches the optimized statement set to the DataStream pipeline.
   */
  attachAsDataStream(): Unit
}
{code}

An example could look like:
{code}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

tEnv
  .createStatementSet()
  .addInsert(tEnv.from(""FromTable""))
  .attachAsDataStream();

tEnv.from(""OtherTable"").toDataStream(...) // continue with further operations
{code}"	FLINK	Closed	3	7	10269	pull-request-available, usability
13288321	Preserve nullability for nested types	"Currently, FlinkTypeFactory does not always preserve nullability attributes when handling nested types.

E.g. a table function that returns {{ROW<s STRING, sa ARRAY<STRING> NOT NULL>}} looses information to {{ROW<s STRING, sa ARRAY<STRING>>}}.

Same for built-in functions such as {{COLLECT}} which results in mismatches as mentioned in FLINK-14042."	FLINK	Closed	3	7	10269	pull-request-available
13409386	Validate partition columns for ResolvedCatalogTable	Currently, partition columns are not validated and might not exist in the schema.	FLINK	Closed	3	7	10269	pull-request-available
13269870	Relax structured types constraints	"As mentioned in FLIP-65:

In order to allow type extraction of structured types that are not registered in a catalog, we need to relax the structured type concept to ""inline or anonymous structured types"" that are not identified by an object identifier in a catalog but the fully qualified implementation class.

In order to support case classes and immutable types, we relax the constraint of enforcing a default constructor by the alternative of having a constructor that fully assigns all fields (same parameter names and types). Because we are already using code generation, the implementation of creating instances even without a default constructor is relatively easy."	FLINK	Closed	3	7	10269	pull-request-available
13373881	Type mismatch when declaring SOURCE_WATERMARK on TIMESTAMP_LTZ column	"The following schema cannot be resolved currently:

{code}
Schema.newBuilder()
    .columnByMetadata(""rowtime"", DataTypes.TIMESTAMP_LTZ(3))
    .watermark(""rowtime"", ""SOURCE_WATERMARK()"")
    .build()
{code}

It leads to:

{code}
The watermark output type TIMESTAMP(3) is different from input time filed type TIMESTAMP_LTZ(3).
{code}"	FLINK	Closed	2	7	10269	pull-request-available
13381722	Add possibility to call built-in functions in SpecializedFunction	This is the last missing piece to avoid code generation when developing built-in functions. Core operations such as CAST, EQUALS, etc. will still use code generation but other built-in functions should be able to use these core operations without the need for generating code. It should be possible to call other built-in functions via a SpecializedFunction.	FLINK	Closed	3	7	10269	auto-unassigned, pull-request-available
13195911	Interval join produces wrong result type in Scala API	"When stream is a Scala case class, the TypeInformation will fall back to GenericType in the process function which result in bad performance when union another DataStream.

In the union method of DataStream, the type is first checked for equality.

Here is an example:
{code:java}
object Test {

    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment

      val orderA: DataStream[Order] = env.fromCollection(Seq(
        Order(1L, ""beer"", 3),
         Order(1L, ""diaper"", 4),
         Order(3L, ""rubber"", 2)))

      val orderB: DataStream[Order] = env.fromCollection(Seq(
        new Order(2L, ""pen"", 3),
        new Order(2L, ""rubber"", 3),
        new Order(4L, ""beer"", 1)))

      val orderC: DataStream[Order] = orderA.keyBy(_.user)
        .intervalJoin(orderB.keyBy(_.user))
        .between(Time.seconds(0), Time.seconds(0))
        .process(new ProcessJoinFunction[Order, Order, Order] {
          override def processElement(left: Order, right: Order, ctx: ProcessJoinFunction[Order, Order, Order]#Context, out: Collector[Order]): Unit = {
            out.collect(left)
          }})

      println(""C: "" + orderC.dataType.toString)
      println(""B: "" + orderB.dataType.toString)

      orderC.union(orderB).print()

      env.execute()
    }

    case class Order(user: Long, product: String, amount: Int)
}{code}
Here is the Exception:
{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot union streams of different types: GenericType<com.manbuyun.awesome.flink.Test.Order> and com.manbuyun.awesome.flink.Test$Order(user: Long, product: String, amount: Integer)
 at org.apache.flink.streaming.api.datastream.DataStream.union(DataStream.java:219)
 at org.apache.flink.streaming.api.scala.DataStream.union(DataStream.scala:357)
 at com.manbuyun.awesome.flink.Test$.main(Test.scala:38)
 at com.manbuyun.awesome.flink.Test.main(Test.scala){code}
 "	FLINK	Closed	3	1	10269	pull-request-available
13366301	Create a utility to create DataStream API's DataType and Schema	"FLIP-136 defines the following behavior that should be supported by such a utility:
 * Since the type system of Table API is more powerful than DataStream API that uses TypeExtraction, it enables replacing GenericTypeInfo with a more meaningful DataType. E.g. for supporting immutable POJOs as StructuredType instead of RawType. Or Lists as ArrayTypes instead of RawTypes.
 * We will perform verification based on what is available in TypeInformation.
 * Type coercion between an explicitly specified Schema and DataStream will not happen (e.g. DataStream<Integer> != Schema.column(""f"", DataTypes.BIGINT())). Because the user specified the desired data type explicitly and expects correctness.
 * It allows to define a `system_rowtime` attribute for using the StreamRecord's timestamp and existing watermarks.
 * It allows to reorder fields and thus has similar functionality as the Expression-based API before but with a more consistent API.
 * Users can place the definition time attributes at arbitrary locations between existing fields.
 * We can give time attributes a name in the same way as in regular DDL.
 * By using `{{system_rowtime().as(""rowtime"")}}` the watermark would be assigned implicitly."	FLINK	Closed	3	7	10269	pull-request-available
13363717	DataTypeExtractor extracts wrong fields ordering for Tuple12	"The following test can reproduce the problem:

{code:java}
 /** Emit Tuple12 result. */
    public static class JavaTableFuncTuple12
            extends TableFunction<
                    Tuple12<
                            String,
                            String,
                            String,
                            String,
                            String,
                            String,
                            Integer,
                            Integer,
                            Integer,
                            Integer,
                            Integer,
                            Integer>> {
        private static final long serialVersionUID = -8258882510989374448L;

        public void eval(String str) {
            collect(
                    Tuple12.of(
                            str + ""_a"",
                            str + ""_b"",
                            str + ""_c"",
                            str + ""_d"",
                            str + ""_e"",
                            str + ""_f"",
                            str.length(),
                            str.length() + 1,
                            str.length() + 2,
                            str.length() + 3,
                            str.length() + 4,
                            str.length() + 5));
        }
    }
{code}

{code:scala}
@Test
  def testCorrelateTuple12(): Unit = {
    val util = streamTestUtil()
    util.addTableSource[(Int, Long, String)](""MyTable"", 'a, 'b, 'c)
    val function = new JavaTableFuncTuple12
    util.addTemporarySystemFunction(""func1"", function)
    val sql =
      """"""
        |SELECT *
        |FROM MyTable, LATERAL TABLE(func1(c)) AS T
        |"""""".stripMargin

    util.verifyExecPlan(sql)
  }
{code}

{code}
// output plan
Correlate(invocation=[func1($cor0.c)], correlate=[table(func1($cor0.c))], select=[a,b,c,f0,f1,f10,f11,f2,f3,f4,f5,f6,f7,f8,f9], rowType=[RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c, VARCHAR(2147483647) f0, VARCHAR(2147483647) f1, INTEGER f10, INTEGER f11, VARCHAR(2147483647) f2, VARCHAR(2147483647) f3, VARCHAR(2147483647) f4, VARCHAR(2147483647) f5, INTEGER f6, INTEGER f7, INTEGER f8, INTEGER f9)], joinType=[INNER])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}

Note that there is no problem if using the legacy {{tEnv.registerFunction}} to register function, becuase it uses {{TypeInformation}}. However, it has problem if using  {{tEnv.createTemporaryFunction}} or {{CREATE FUNCTION}} syntax, because it uses {{TypeInference}}.

Note this problem exists in latest 1.11, 1.12, and master branch. 

I think the problem might lay in this line: https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/extraction/DataTypeExtractor.java#L562

because it orders field names by alphabetical."	FLINK	Closed	2	1	10269	pull-request-available
13171689	End-to-end test: SQL Client with unified source/sink/format	"After FLINK-8858 is resolved we can add an end-to-end test for the SQL Client. The test should perform the following steps:

- Put JSON data into Kafka
- Submit and execute a {{INSERT INTO}} statement that reads from a Kafka connector with JSON format, does some ETL, and writes to Kafka with Avro format
- Validate Avro data "	FLINK	Resolved	1	7	10269	pull-request-available
13142467	Add support for INSERT INTO in SQL Client	The current design of SQL Client embedded mode doesn't support long running queries. It would be useful for simple jobs that can be expressed in a single sql statement if we can submit sql statements stored in files as long running queries. 	FLINK	Resolved	3	7	10269	pull-request-available
13244959	Allow switching planners in SQL Client	Once FLINK-13267 is resolved, we can also enable switching planners in the SQL Client via a execution property. Even though this is kind of a new feature, it had to be postponed after the feature-freeze as the relocation of FLINK-13267 would have created many merge conflicts otherwise.	FLINK	Closed	1	2	10269	pull-request-available
13432422	Support the new type inference in Scala Table API table functions	"Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used ""inline"". We should support them as well."	FLINK	Closed	3	7	10269	pull-request-available
13001945	Clearly define SQL operator table	Currently, we use {{SqlStdOperatorTable.instance()}} for setting all supported operations. However, not all of them are actually supported. {{FunctionCatalog}} should only return those operators that are tested and documented.	FLINK	Resolved	3	4	10269	starter
13089438	Merge the flink-java8 project into flink-core	This issue removes the flink-java8 module and merges some tests into flink-core/flink-runtime. It ensures to have the possibility for passing explicit type information in DataStream API as a fallback. Since the tycho compiler approach was very hacky and seems not to work anymore, this commit also removes all references in the docs and quickstarts.	FLINK	Resolved	3	4	10269	pull-request-available
13391864	Use a layered configuration in Executor and Planner	"The configuration story in Flink is not very consistent at the moment. Ideally, we should have a layered approach where pipeline executor, DataStream API, and Table API store their configuration and add it to a global configuration on request. However, this is a big change that we would like to avoid at this point.

Instead, we partially follow this approach by adding:
- {{Executor.getConfiguration}} to access the config of lower layers
- {{Planner.getConfiguration}} to access a global configuration during planning

This is required to access properties stored in {{StreamExecutionEnvironment.configuration}}."	FLINK	Closed	3	7	10269	pull-request-available
13367033	Add new StreamTableEnvironment.toDataStream	Implement StreamTableEnvironment.fromDataStream according to FLIP-136.	FLINK	Closed	3	7	10269	pull-request-available
13003279	Fix flaky test ScalarFunctionsTest.testCurrentTimePoint	"It seems that the test is still non deterministic.

{code}
org.apache.flink.api.table.expressions.ScalarFunctionsTest
testCurrentTimePoint(org.apache.flink.api.table.expressions.ScalarFunctionsTest)  Time elapsed: 0.083 sec  <<< FAILURE!
org.junit.ComparisonFailure: Wrong result for: AS(>=(CHAR_LENGTH(CAST(CURRENT_TIMESTAMP):VARCHAR(1) CHARACTER SET ""ISO-8859-1"" COLLATE ""ISO-8859-1$en_US$primary"" NOT NULL), 22), '_c0') expected:<[tru]e> but was:<[fals]e>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.apache.flink.api.table.expressions.utils.ExpressionTestBase$$anonfun$evaluateExprs$1.apply(ExpressionTestBase.scala:126)
	at org.apache.flink.api.table.expressions.utils.ExpressionTestBase$$anonfun$evaluateExprs$1.apply(ExpressionTestBase.scala:123)
	at scala.collection.mutable.LinkedHashSet.foreach(LinkedHashSet.scala:87)
	at org.apache.flink.api.table.expressions.utils.ExpressionTestBase.evaluateExprs(ExpressionTestBase.scala:123)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
{code}"	FLINK	Resolved	3	1	10269	starter
13425423	Introduce ContextResolvedFunction similar to ContextResolvedTable	The planner is currently unable to distinguish between a temporary function and persisted function. The problem is similar to what ContextResolvedTable tries to solve.	FLINK	Closed	3	7	10269	pull-request-available
13309174	Update the Row.toString method	"This updates the Row.toString method to provide a good summary string.

In particular it fixes the following issues:

Changeflag: According to FLIP-95, a row describes an entry in a
changelog. Therefore, it should visible whether a row is an insert,
delete, or update change. Now indicated with +I, -D, +U, -U.

Nested rows: In the old implementation it was not visible whether nested
rows exist or not due to missing start/end boundaries. Now indicated with
[...] or {...}.

Positioned rows vs. named rows: According to FLIP-136, it should be visible
whether a row operates in name-based or position-based field mode. Now
indicated with [...] or {...}.

Nested arrays in maps and lists: In the old implementation arrays in maps
or lists could not be represented.

Wrong formatting: Most programming languages use a space after a comma."	FLINK	Closed	3	7	10269	pull-request-available
13234504	Remove row interval type	The row interval type just adds additional complexity and prevents SQL queries from supporting count windows. A regular {{BIGINT}} type is sufficient to represent a count.	FLINK	Closed	3	7	10269	pull-request-available
13299144	Improve FieldsDataType	"The problem with {{FieldsDataType}} is that the method {{getFieldDataTypes}} does not keep the order of the fields stored in the logical type. Therefore at couple of locations we have to first iterate over the names in logical type and then get the DataTypes.

{code}
final RowType rowType = (RowType) fieldsDataType.getLogicalType();
final String[] fieldNames = rowType.getFields()
	.stream()
	.map(RowType.RowField::getName)
	.toArray(String[]::new);

final TypeInformation<?>[] fieldTypes = Stream.of(fieldNames)
	.map(name -> fieldsDataType.getFieldDataTypes().get(name))
	.map(LegacyTypeInfoDataTypeConverter::toLegacyTypeInfo)
	.toArray(TypeInformation[]::new);
{code}"	FLINK	Closed	4	7	10269	pull-request-available
13367784	Add a SupportsSourceWatermark ability interface	"FLINK-21899 added a dedicated function that can be used in watermark definitions. Currently, the generated watermark strategy is invalid because of the exception that we throw in the function’s implementation. We should integrate this concept deeper into the interfaces instead of the need to implement some expression analyzing utility for every source.

We propose the following interface:

{code}
SupportsSourceWatermark {
  void applySourceWatermark()
}
{code}

 "	FLINK	Closed	3	7	10269	pull-request-available
13391510	Simplify BlinkExecutorFactory stack	The {{BlinkExecutorFactory}} stack uses the old table factory stack and is not needed anymore as the old planner has been removed. We should simplify the logic there.	FLINK	Closed	3	7	10269	pull-request-available
13080698	Add E() supported in TableAPI	See FLINK-6960 for detail.	FLINK	Resolved	3	7	10269	starter
13398213	Let SinkUpsertMaterializer emit +U instead of only +I	Currently, {{SinkUpsertMaterializer}} is not able to emit +U's but will always emit +I's. Thus, resulting changelogs are incorrect strictly speaking and only valid when treating +U and +I as similar changes in downstream operators.	FLINK	Closed	1	1	10269	pull-request-available
12719957	The Hadoop Compatibility has been refactored and extended to support the new Java API.	"This PR requires ([#776|https://github.com/stratosphere/stratosphere/issues/776] | [FLINK-776|https://issues.apache.org/jira/browse/FLINK-776]). A description can be found in ([#611|https://github.com/stratosphere/stratosphere/issues/611] | [FLINK-611|https://issues.apache.org/jira/browse/FLINK-611]).

The complete hadoop-compatibility package has been refactored and adapted to the new Java API. Record specific classes are in a separate ```record``` package which can be thrown out later.

A new Word Count example shows how to use Input and Output formats. A test case uses the Word Count example for testing the implementation.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/777
Created by: [twalthr|https://github.com/twalthr]
Labels: enhancement, java api, user satisfaction, 
Milestone: Release 0.6 (unplanned)
Created at: Thu May 08 20:37:28 CEST 2014
State: open
"	FLINK	Closed	3	4	10269	github-import
13322483	Update documentation about user-defined aggregate functions	The documentation needs an update because all functions support the new type inference now.	FLINK	Closed	3	7	10269	pull-request-available
13441362	SQL CAST(' 1 ' as BIGINT) returns wrong result	"{code:sql}
Flink SQL> select
>                     cast(' 1 ' as tinyint),
>                     cast(' 1 ' as smallint),
>                     cast(' 1 ' as int),
>                     cast(' 1 ' as bigint),
>                     cast(' 1 ' as float),
>                     cast(' 1 ' as double);
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
| op | EXPR$0 | EXPR$1 |      EXPR$2 |               EXPR$3 |                         EXPR$4 |                         EXPR$5 |
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: ' 1 '. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toByte(BinaryStringDataUtil.java:568)
	at StreamExecCalc$392.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
{code}

Setting CAST behavior to legacy but got null result :

{code}
Flink SQL> set table.exec.legacy-cast-behaviour=enabled;
[INFO] Session property has been set.

Flink SQL> select
>                     cast(' 1 ' as tinyint),
>                     cast(' 1 ' as smallint),
>                     cast(' 1 ' as int),
>                     cast(' 1 ' as bigint),
>                     cast(' 1 ' as float),
>                     cast(' 1 ' as double);
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
| op | EXPR$0 | EXPR$1 |      EXPR$2 |               EXPR$3 |                         EXPR$4 |                         EXPR$5 |
+----+--------+--------+-------------+----------------------+--------------------------------+--------------------------------+
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Column 'EXPR$0' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='DROP' to suppress this exception and drop such records silently.
	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processNotNullConstraint(ConstraintEnforcer.java:261)
	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:241)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at StreamExecCalc$591.processElement_split1(Unknown Source)
	at StreamExecCalc$591.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
{code}


In 1.14 the result should be {{[1, 1, 1, 1, 1.0, 1.0]}}. 

In Postgres:
{code}
postgres=# select cast(' 1 ' as int), cast(' 1 ' as bigint), cast(' 1 ' as float);
 int4 | int8 | float8
------+------+--------
    1 |    1 |      1
(1 row)
{code}"	FLINK	Closed	2	1	10269	pull-request-available
13389854	Expose a consistent GlobalDataExchangeMode	"The Table API makes the {{GlobalDataExchangeMode}} configurable via {{table.exec.shuffle-mode}}.

In Table API batch mode the StreamGraph is configured with {{ALL_EDGES_BLOCKING}} and in DataStream API batch mode {{FORWARD_EDGES_PIPELINED}}.

I would vote for unifying the exchange mode of both APIs so that complex SQL pipelines behave identical in {{StreamTableEnvironment}} and {{TableEnvironment}}. Also the feedback a got so far would make {{ALL_EDGES_BLOCKING}} a safer option to run pipelines successfully with limited resources.

[~lzljs3620320]
{quote}
The previous history was like this:
- The default value is pipeline, and we find that many times due to insufficient resources, the deployment will hang. And the typical use of batch jobs is small resources running large parallelisms, because in batch jobs, the granularity of failover is related to the amount of data processed by a single task. The smaller the amount of data, the faster the fault tolerance. So most of the scenarios are run with small resources and large parallelisms, little by little slowly running.

- Later, we switched the default value to blocking. We found that the better blocking shuffle implementation would not slow down the running speed much. We tested tpc-ds and it took almost the same time.
{quote}

[~dwysakowicz]
{quote}
I don't see a problem with changing the default value for DataStream batch mode if you think ALL_EDGES_BLOCKING is the better default option.
{quote}

In any case, we should make this configurable for DataStream API users and make the specific Table API option obsolete."	FLINK	Closed	3	7	10269	pull-request-available
13303064	Support inline structured types	"Many locations in the code base already support structured types. The runtime treats them as row types. However, some final work is needed to support structured types though the stack. We start with inline structured types. Registered structured types in catalog are covered in a different issue.

Inline structured types are a prerequisite to enable aggregate functions in FLIP-65 again."	FLINK	Closed	3	7	10269	pull-request-available
13318705	Fix Scala code examples for UDF type inference annotations	"The Scala code examples for the [UDF type inference annotations|https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#type-inference] are not correct.

For example: the following {{FunctionHint}} annotation 

{code:scala}
@FunctionHint(
  input = Array(@DataTypeHint(""INT""), @DataTypeHint(""INT"")),
  output = @DataTypeHint(""INT"")
)
{code}


needs to be changed to

{code:scala}
@FunctionHint(
  input = Array(new DataTypeHint(""INT""), new DataTypeHint(""INT"")),
  output = new DataTypeHint(""INT"")
)
{code}"	FLINK	Closed	3	1	10269	pull-request-available
13296431	Add ability interfaces for table source/sink	"This will add the ability interfaces mentioned in FLIP-95:
 * SupportsWatermarkPushDown
 * SupportsProjectionPushDown
 * + already existing interfaces"	FLINK	Closed	3	7	10269	pull-request-available
13283964	Fully support RAW types in the API	"Currently, the Table API does not expose a way of creating RAW types without a {{DataTypeFactory}}. The reason for that is that RAW types need to be resolved at a later stage. This is similar to user-defined types that need to be resolved from the catalog.

API changes:

- Introduction of {{AbstractDataType}} as the highest type of resolved and unresolved data types

- Introduction of {{DataTypes.of()}} for classes and names and {{DataTypes.RAW(Class)}} for unresolved types

- Helper classes and methods for constructing unresolved types type-safe

We also need better support in the Blink planner for that."	FLINK	Closed	3	7	10269	pull-request-available
13377609	Drop usages of BatchTableEnvironment and old planner in Python	"This is a major cleanup of the Python module that drops support for BatchTableEnvironment and old planner.

Removes usages of:
 - DataSet
 - BatchTableEnvironment
 - Legacy planner
 - ExecutionEnvironment

 

Note: Batch processing is still possible via the unified \{{TableEnvironment}}."	FLINK	Closed	3	7	10269	pull-request-available
13317406	Invalid error message for overloaded methods with same parameter name	"If a function has overloaded evaluation methods but same argument names, this leads to a confusing error message where types are missing:

{code}
Caused by: org.apache.flink.table.api.ValidationException: Invalid input arguments. Expected signatures are:
test-catalog.TEST_DB.myScalarFunc(a => )
	at org.apache.flink.table.types.inference.TypeInferenceUtil.createInvalidInputException(TypeInferenceUtil.java:190)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker.checkOperandTypesOrError(TypeInferenceOperandChecker.java:131)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker.checkOperandTypes(TypeInferenceOperandChecker.java:89)
	... 79 common frames omitted
Caused by: org.apache.flink.table.api.ValidationException: Invalid input arguments.
	at org.apache.flink.table.types.inference.TypeInferenceUtil.inferInputTypes(TypeInferenceUtil.java:467)
	at org.apache.flink.table.types.inference.TypeInferenceUtil.adaptArguments(TypeInferenceUtil.java:123)
	at org.apache.flink.table.types.inference.TypeInferenceUtil.adaptArguments(TypeInferenceUtil.java:102)
	at org.apache.flink.table.planner.functions.inference.TypeInferenceOperandChecker.checkOperandTypesOrError(TypeInferenceOperandChecker.java:126)
	... 80 common frames omitted
{code}"	FLINK	Closed	3	7	10269	pull-request-available
12979735	FieldParsers should support empty strings	In order to parse CSV files using the new Table API that converts rows to Row objects (that support null values), FiledParser implementations should support emptry strings setting the parser state to ParseErrorState.EMPTY_STRING (for example FloatParser and DoubleParser doesn't respect this constraint)	FLINK	Resolved	3	1	10269	csvparser, table-api
13382142	"Remove ""blink"" suffix from table modules"	"In order to reduce confusion around ""What is Flink?/What is Blink?"" we should remove the term {{blink}} from the following modules:

{code}
flink-table-planner-blink
flink-table-runtime-blink
flink-table-uber-blink
{code}

It is up for discussion if we will:
- just perform the refactoring,
- keep the old modules for a while,
- or move the contents to new modules and link to those from the old modules

In any case we should make sure to keep the Git history."	FLINK	Closed	3	7	10269	pull-request-available
13175092	Support a custom FlinkKafkaPartitioner for a Kafka table sink factory	Currently, the Kafka table sink factory does not support a custom FlinkKafkaPartitioner. However, this is needed for many use cases.	FLINK	Resolved	3	7	10269	pull-request-available
13216882	Create Blink SQL planner and runtime modules	As mentioned in FLIP-32, we will create separate modules while performing the Blink SQL merge. As part of this issue we will create \{{flink-table-planner-blink}} and \{{flink-table-runtime-blink}} modules.	FLINK	Closed	3	2	10269	pull-request-available
13325553	No access to metric group in ScalarFunction when optimizing	"Under some circumstances, I cannot access {{context.getMetricGroup()}} in a {{ScalarFunction}} like this (full job attached):
{code:java}
  public static class MyUDF extends ScalarFunction {
    @Override
    public void open(FunctionContext context) throws Exception {
      super.open(context);
      context.getMetricGroup();
    }

    public Integer eval(Integer id) {
      return id;
    }
  }
{code}
which leads to this exception:
{code:java}
Exception in thread ""main"" java.lang.UnsupportedOperationException: getMetricGroup is not supported when optimizing
	at org.apache.flink.table.planner.codegen.ConstantFunctionContext.getMetricGroup(ExpressionReducer.scala:249)
	at com.ververica.MetricsGroupBug$MyUDF.open(MetricsGroupBug.java:57)
	at ExpressionReducer$2.open(Unknown Source)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:118)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:696)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:618)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:303)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:328)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:562)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:427)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:264)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:223)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:210)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:164)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:84)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:164)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1264)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:700)
	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:565)
	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:549)
	at com.ververica.MetricsGroupBug.main(MetricsGroupBug.java:50)
{code}
I also tried to work around this with a try-catch, assuming that this method is called once during optimisation and another time at runtime. However, it seems as if {{open()}} is actually only called once (during optimization) thus giving me no choice to access the metrics group.

It seems that removing the where condition before my UDF call also fixes it but it shouldn't be that way..."	FLINK	Closed	3	1	10269	pull-request-available
13197412	Make Kafka version definition more flexible for new Kafka table factory	"Currently, a user has to specify a specific version for a Kafka connector like:

{code}
connector:
  type: kafka
  version: ""0.11""     # required: valid connector versions are ""0.8"", ""0.9"", ""0.10"", and ""0.11""
  topic: ...          # required: topic name from which the table is read
{code}

However, the new Kafka connector aims to be universal, thus, at least for 1.x and 2.x versions which we should support those as parameters as well. Currently, {{2.0}} is the only accepted string for the factory."	FLINK	Resolved	3	1	10269	pull-request-available
13281554	SQLClientKafkaITCase.testKafka failed on Travis	"The end-to-end test {{SQLClientKafkaITCase.testKafka}} failed with

{code}
22:09:36.957 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 129.244 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
22:09:36.958 [ERROR] testKafka[2: kafka-version:universal kafka-sql-version:.*kafka.jar](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase)  Time elapsed: 45.954 s  <<< FAILURE!
org.junit.ComparisonFailure: 
expected:<...-03-12 09:00:00.000,[Bob,This was another warning.,1,Success constant folding.
2018-03-12 09:00:00.000,Steve,This was another info.,2],Success constant fo...> but was:<...-03-12 09:00:00.000,[Steve,This was another info.,2,Success constant folding.
2018-03-12 09:00:00.000,Bob,This was another warning.,1],Success constant fo...>
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.checkCsvResultFile(SQLClientKafkaITCase.java:226)
	at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:154)

22:09:37.287 [INFO] 
22:09:37.288 [INFO] Results:
22:09:37.288 [INFO] 
22:09:37.288 [ERROR] Failures: 
22:09:37.288 [ERROR]   SQLClientKafkaITCase.testKafka:154->checkCsvResultFile:226 expected:<...-03-12 09:00:00.000,[Bob,This was another warning.,1,Success constant folding.
{code}

https://api.travis-ci.org/v3/job/641346345/log.txt"	FLINK	Closed	2	1	10269	pull-request-available, test-stability
13336949	KafkaTableITCase.testKafkaSourceSinkWithMetadata fails on AZP	"The {{KafkaTableITCase.testKafkaSourceSinkWithMetadata}} seems to fail on AZP:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8197&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

{code}
Expected: <data 1,1,CreateTime,2020-03-08T13:12:11.123,0,0,{k1=[B@4ea4e0f3, k2=[B@7c9ecd9e},0,metadata_topic_avro,true>
<data 2,2,CreateTime,2020-03-09T13:12:11.123,1,0,{},0,metadata_topic_avro,false>
<data 3,3,CreateTime,2020-03-10T13:12:11.123,2,0,{k1=[B@27cd156a, k2=[B@4af44e42},0,metadata_topic_avro,true>
     but: was <data 1,1,CreateTime,2020-03-08T13:12:11.123,0,0,{k1=[B@4ea4e0f3, k2=[B@7c9ecd9e},0,metadata_topic_avro,true>
<data 2,2,CreateTime,2020-03-09T13:12:11.123,1,0,{},0,metadata_topic_avro,false>
<data 3,3,CreateTime,2020-03-10T13:12:11.123,2,0,{k1=[B@27cd156a, k2=[B@4af44e42},0,metadata_topic_avro,true>
{code}"	FLINK	Closed	1	1	10269	pull-request-available, test-stability
13382130	Allow prefix syntax for ConfigOption.mapType	"The current factory design does not allow placeholder options in {{EncodingFormatFactory}} or {{DecodingFormatFactory}}.

The past has shown that placeholder options are used at a couple of locations.
See FLINK-22475 or {{KafkaOptions#PROPERTIES_PREFIX}}.

We should think about adding an additional functionality to {{ReadableConfig}} or a special {{ConfigOption}} type to finally solve this problem. This could also be useful for FLIP-129. And would solve the [current shortcomings for Confluent Avro registry|https://github.com/apache/flink/pull/15808#discussion_r645494282]."	FLINK	Closed	3	7	10269	pull-request-available
13328004	Update parser module for FLIP-107	Makes expressing metadata possible both for the new column type and LIKE.	FLINK	Closed	3	7	10269	pull-request-available
13298471	Align Calcite's and Flink's SYMBOL types	"Flink's `SymbolType` stores the class of the symbol. Calcite does not. When converting from Flink's to Calcite's type we are loosing that information. We should either remove that from the Flink's type or add it to Calcite's.

I am in favor of removing it from the {{SymbolType}}. This type is a pseudo type which can be used only for literals. The symbol class is already part of the corresponding value, therefore it is a redundant information in the type itself.

Initial discussion: https://github.com/apache/flink/pull/11694#discussion_r408058640"	FLINK	Closed	3	7	10269	pull-request-available
13174320	Document unified table sources/sinks/formats	The recent unification of table sources/sinks/formats needs documentation. I propose a new page that explains the built-in sources, sinks, and formats as well as a page for customization of public interfaces.	FLINK	Resolved	3	4	10269	pull-request-available
13379224	Drop usages of EnvironmentSettings.useOldPlanner()	Remove EnvironmentSettings.useOldPlanner() usages outside of the legacy planner module.	FLINK	Closed	3	7	10269	stale-assigned
13318720	Calling ROW() in a UDF results in UnsupportedOperationException	"Given a UDF {{func}} that accepts a {{ROW(INT, STRING)}} as parameter, it cannot be called like this:
{code:java}
SELECT func(ROW(a, b)) FROM t{code}
while this works
{code:java}
SELECT func(r) FROM (SELECT ROW(a, b) FROM t){code}
 

The exception returned is:
{quote}org.apache.flink.table.api.ValidationException: SQL validation failed. null
{quote}
with an empty {{UnsupportedOperationException}} as cause."	FLINK	Closed	3	7	10269	pull-request-available
13177082	Document usage of INSERT INTO in SQL Client	Document the usage of {{INSERT INTO}} statements in SQL.	FLINK	Resolved	3	7	10269	pull-request-available
13328712	"AggregateITCase.testListAggWithDistinct failed with ""expected:<List(1,A, 2,B, 3,C#A, 4,EF)> but was:<List(1,A, 2,B, 3,C#A, 4,EF#EF)>"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6734&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29

{code}
2020-09-22T04:59:30.1229430Z [ERROR] testListAggWithDistinct[LocalGlobal=ON, MiniBatch=ON, StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase)  Time elapsed: 0.346 s  <<< FAILURE!
2020-09-22T04:59:30.1230120Z java.lang.AssertionError: expected:<List(1,A, 2,B, 3,C#A, 4,EF)> but was:<List(1,A, 2,B, 3,C#A, 4,EF#EF)>
2020-09-22T04:59:30.1232835Z 	at org.junit.Assert.fail(Assert.java:88)
2020-09-22T04:59:30.1233314Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2020-09-22T04:59:30.1233688Z 	at org.junit.Assert.assertEquals(Assert.java:118)
2020-09-22T04:59:30.1234034Z 	at org.junit.Assert.assertEquals(Assert.java:144)
2020-09-22T04:59:30.1234528Z 	at org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testListAggWithDistinct(AggregateITCase.scala:667)
{code}"	FLINK	Closed	1	1	10269	pull-request-available, test-stability
13354637	Remove deprecated CatalogTable.getProperties	{{CatalogTable.getProperties}} has been deprecated in 1.11. It is time to remove it, to reduce confusion and potential bugs by calling the wrong method.	FLINK	Closed	3	7	10269	pull-request-available
12833025	Add a SQL API	"From the mailing list:

Fabian: Flink's Table API is pretty close to what SQL provides. IMO, the best
approach would be to leverage that and build a SQL parser (maybe together
with a logical optimizer) on top of the Table API. Parser (and optimizer)
could be built using Apache Calcite which is providing exactly this.
Since the Table API is still a fairly new component and not very feature
rich, it might make sense to extend and strengthen it before putting
something major on top.

Ted: It would also be relatively simple (I think) to retarget drill to Flink if
Flink doesn't provide enough typing meta-data to do traditional SQL."	FLINK	Closed	3	2	10269	requires-design-doc
13182292	Elasticsearch 6 UpdateRequest fail because of binary incompatibility	"When trying to send UpdateRequest(s) to ElasticSearch6, and one gets the following
error:

{code}
Caused by: java.lang.NoSuchMethodError:
org.elasticsearch.action.bulk.BulkProcessor.add(Lorg/elasticsearch/action/ActionRequest;)Lorg/elasticsearch/action/bulk/BulkProcessor;
	at
org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer.add(BulkProcessorIndexer.java:76)
{code}

ElasticsearchSinkFunction:
{code}
	import org.elasticsearch.action.update.UpdateRequest
	def upsertRequest(element: T): UpdateRequest = {
		new UpdateRequest(
			""myIndex"",
			""record"",
			s""${element.id}"")
	        	.doc(element.toMap())
	}
	override def process(element: T, runtimeContext: RuntimeContext,
requestIndexer: RequestIndexer): Unit = {
		requestIndexer.add(upsertRequest(element))
	}
{code}

This is due to a binary compatibility issue between the base module (which is compiled against a very old ES version and the current Elasticsearch version).

As a work around you can simply copy org.apache.flink.streaming.connectors.elasticsearch.BulkProcessorIndexer to your project. This should ensure that the class is compiled correctly."	FLINK	Resolved	1	1	10269	pull-request-available
13324445	Fix validation of table functions in projections	"While working on another change, I realized that the {{FunctionITCase.testInvalidUseOfTableFunction()}} tests throws a NullPointerException during execution.

This error is not visible, because TableEnvironmentImpl.executeInternal() does not wait for the final job status.
It submits the job using the job client ({{JobClient jobClient = execEnv.executeAsync(pipeline);}}), and it doesn't wait for the job to complete before returning a result. 

This is the null pointer that is hidden:
{code}

Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'insert-into_default_catalog.default_database.SinkTable'.
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1823)
	at org.apache.flink.table.planner.delegation.ExecutorBase.executeAsync(ExecutorBase.java:57)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:681)
	... 34 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:148)
	at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$submitJob$2(PerJobMiniClusterFactory.java:92)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:195)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:188)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:182)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:523)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:422)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:284)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:199)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.runtime.collector.WrappingCollector.outputResult(WrappingCollector.java:43)
	at StreamExecCalc$245$TableFunctionResultConverterCollector$243.collect(Unknown Source)
	at org.apache.flink.table.functions.TableFunction.collect(TableFunction.java:201)
	at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase$RowTableFunction.eval(FunctionITCase.java:1024)
	at StreamExecCalc$245.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.pushToOperator(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:603)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:563)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:305)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:394)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:213)
{code}"	FLINK	Closed	3	1	10269	pull-request-available
13306718	Don't allow self referencing structured type	"Currently, the logical constraint ""A type cannot be defined so that one of its attribute types (transitively) uses itself."" is not enforced during extraction and leads to a stack overflow. We should throw a helpful exception instead."	FLINK	Closed	3	7	10269	pull-request-available
13228824	Expose the new type system through the API	"Exposes the new type system through API methods.

Introduces new methods, adds converters for backwards-compatibility, and deprecates old methods.

Adds checks to types that are not supported by the legacy planner."	FLINK	Closed	3	7	10269	pull-request-available, stale-assigned
13395617	Add documentation for batch mode in StreamTableEnvironment	The DataStream API Integration page needs an update.	FLINK	Closed	1	7	10269	pull-request-available
13300804	Support the NULL type for function calls	According to FLIP-37, the null type is used to represent untyped NULL literals. Those are in particular useful when it comes to passing them to a function call. The new type inference of FLIP-65 is able to process call such as `f(NULL)`.	FLINK	Closed	3	7	10269	pull-request-available
13544976	Insufficient validation for table.local-time-zone	"There are still cases where timezone information is lost silently due to the interaction between {{java.util.TimeZone}} and {{java.time.ZoneId}}.

This might be theoretical problem, but I would feel safer if we change the check to:
{code}
if (!java.util.TimeZone.getTimeZone(zoneId).toZoneId().equals(ZoneId.of(zoneId))) {
   throw new ValidationException(errorMessage);
}
{code}"	FLINK	Closed	3	1	10269	pull-request-available
13379066	Clean up examples to not use legacy planner anymore	Clean up the `flnk-examples-table` module to not reference the legacy planner anymore.	FLINK	Closed	3	7	10269	pull-request-available
13434389	Metadata keys should not conflict with physical columns	"If you have an field called timestamp and in addition want to read the timestamp from the metadata:

{code}
CREATE TABLE animal_sightings_with_metadata (
  `timestamp` TIMESTAMP(3),
  `name` STRING,
  `country` STRING,
  `number` INT,
  `append_time` TIMESTAMP(3) METADATA FROM 'timestamp',
  `partition` BIGINT METADATA VIRTUAL,
  `offset` BIGINT METADATA VIRTUAL,
  `headers` MAP<STRING, BYTES> METADATA,
  `timestamp-type` STRING METADATA,
  `leader-epoch` INT METADATA,
  `topic` STRING METADATA
)
{code}

This gives:
{code}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [timestamp]
{code}"	FLINK	Closed	3	7	10269	pull-request-available
13556647	Avoid CompiledPlan recompilation during loading	{{StreamPlanner.loadPlan}} recompiles the loaded plan. This causes unnecessary computational overhead and should be removed.	FLINK	Closed	3	7	10269	pull-request-available
13319108	Allow variables for column names in Scala Table API	"User have reported that the Scala API lacks a way to reference columns via a name that is stored in a variable. String interpolation is inconvenient in this case:

We should allow this also in Scala:
{code}
tab.select($(keyVar), $(valueVar))
{code}"	FLINK	Closed	3	4	10269	pull-request-available
13335732	Merge kafka-connector-base into flink-connector-kafka	Nowadays, we only offer one unified Kafka connector, so a base module is not required anymore. The base module also uses Kafka 0.10 at the moment. We should merge those two modules into one.	FLINK	Closed	3	4	10269	pull-request-available
13307343	Implement type inference for AS	Type information gets lost due to the legacy planner expressions. The user might experience unexpected exceptions.	FLINK	Closed	3	7	10269	pull-request-available
13381634	Drop remaining usages of legacy planner in E2E tests and Python	This removes the remaining usages of legacy planner outside of the {{flink-table}} module.	FLINK	Closed	3	7	10269	pull-request-available
13365258	Update use new schema in Table, TableResult, TableOperation	In order to complete FLIP-164, we should update important API parts to return the new `ResolvedSchema` and deprecate `TableSchema`.	FLINK	Closed	3	7	10269	pull-request-available
13429671	Add Expressions.col as a synonym for $	"`$` might not always be the right choice for referring to a column. Python API has already `col` for this. We should offer `col` as well for e.g. Kotlin users:

https://stackoverflow.com/questions/71145050/how-to-write-user-in-kotlin-with-flink"	FLINK	Closed	3	1	10269	pull-request-available
13328263	Support key and value formats in Kafka connector	"Introduce the following options for Kafka:
{code}
key.format, value.format, key.fields, value.fields-include, fields.verify-integrity
{code}

As described in FLIP-107."	FLINK	Closed	3	7	10269	pull-request-available
13313189	GenericArrayData cannot convert object arrays to primitive arrays	GenericArrayData.toBooleanArray throws a cast exception if it is backed by an object array.	FLINK	Closed	3	1	10269	pull-request-available
13143592	"Rowtime materialization causes ""mismatched type"" AssertionError"	"As raised in [this thread|https://lists.apache.org/thread.html/e2ea38aa7ae224d7481145334955d84243690e9aad10d58310bdb8e7@%3Cuser.flink.apache.org%3E], the query created by the following code will throw a calcite ""mismatch type"" ({{Timestamp(3)}} and {{TimeIndicator}}) exception.

{code:java}
String sql1 = ""select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1"";
String sql2 = ""select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2"";

Table left = tableEnv.sqlQuery(sql1);
Table right = tableEnv.sqlQuery(sql2);
left.join(right).where(""id === r_id && t1 === t2"").select(""id, t1"").writeToSink(...)
{code}
The logical plan is as follows.
{code}
LogicalProject(id=[$0], t1=[$1])
  LogicalFilter(condition=[AND(=($0, $3), =($1, $4))])
    LogicalJoin(condition=[true], joinType=[inner])
      LogicalAggregate(group=[{0, 1, 2}])
        LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])])
          LogicalProject(id=[$0], eventTs=[$3])
            LogicalTableScan(table=[[_DataStreamTable_0]])
      LogicalAggregate(group=[{0, 1, 2}])
        LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])])
          LogicalProject(id=[$0], eventTs=[$3])
            LogicalTableScan(table=[[_DataStreamTable_0]])
{code}
That is because the the rowtime field after an aggregation will be materialized while the {{RexInputRef}} type for the filter's operands ({{t1 === t2}}) is still {{TimeIndicator}}. We should make them unified.

"	FLINK	Resolved	3	1	10269	pull-request-available
13139454	Supported Data Types - Six or Seven	"See [Supported Data Types|https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/api_concepts.html#supported-data-types] in the online documentation. The text specifies that there are ""six different categories of data types,"" but the list below contains seven items. I suggest that you omit a specific number in the sentence preceding the list. Here is a suggestion: ""DataSet and DataStream support the following categories of data elements."" See the attached screenshot. Note: Please let me know if these types of issues are too trivial to report. Thx."	FLINK	Closed	4	1	10269	documentation
13359278	Implement Schema, ResolvedSchema, SchemaResolver	Introduces the main classes and utilities around schema mentioned in FLIP-164.	FLINK	Closed	3	7	10269	pull-request-available
13569230	Window TVFs with named parameters don't support column expansion	"It seems named parameters still have issues with column expansion of virtual metadata column:

{code}

SELECT * FROM TABLE(  TUMBLE( DATA => TABLE gaming_player_activity_source, TIMECOL => DESCRIPTOR(meta_col), SIZE => INTERVAL '10' MINUTES));

{code}"	FLINK	Closed	3	7	10269	pull-request-available
13221377	Update the year in NOTICE files	"The {{NOTICE}} files are still starting with:

{code}
flink-table-planner
Copyright 2014-2018 The Apache Software Foundation
{code}

We need to update the year to 2019."	FLINK	Closed	1	1	10269	pull-request-available
13242958	Add a type parser utility	For both the SQL Client YAML files as well as future type annotations, we need a parser that can create logical types from strings. It is the reverse operation of {{org.apache.flink.table.types.logical.LogicalType#asSerializableString}}.	FLINK	Closed	3	7	10269	pull-request-available
13237427	Fix ANY type serialization	"Every logical type needs to be string serializable. In old versions we used Java serialization logic for it. Since an any type has no type information anymore but just type serializer, we can use the snapshot to write out an any type into properties in a backward compatible way.

However, the current serialization logic is wrong."	FLINK	Closed	3	7	10269	pull-request-available
13173732	Add TO_BASE64 function for table/sql API	refer to mysql TO_BASE64 function : https://dev.mysql.com/doc/refman/5.6/en/string-functions.html#function_to-base64	FLINK	Resolved	4	7	10999	pull-request-available
13025095	Add a Thread default uncaught exception handler on the JobManager	"When some JobManager threads die because of uncaught exceptions, we should bring down the JobManager. If a thread dies from an uncaught exception, there is a high chance that the JobManager becomes dysfunctional.

The only sfae thing is to rely on the JobManager being restarted by YARN / Mesos / Kubernetes / etc.

I suggest to add this code to the JobManager launch:

{code}
Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler() {

    @Override
    public void uncaughtException(Thread t, Throwable e) {
        try {
            LOG.error(""Thread {} died due to an uncaught exception. Killing process."", t.getName());
        } finally {
            Runtime.getRuntime().halt(-1);
        }
    }
});
{code}"	FLINK	Resolved	3	7	10999	pull-request-available
13194561	Move modern kafka connector module into connector profile 	"The modern connector is run in the {{misc}} profile since it wasn't properly added to the {{connector profile in stage.sh click [here|https://github.com/apache/flink/pull/6890#issuecomment-431917344] to see more details.}}

*This issue is blocked by FLINK-10603.*"	FLINK	Closed	3	7	10999	pull-request-available
13254905	Keep hasDeprecatedKeys and deprecatedKeys methods in ConfigOption and mark it with @Deprecated annotation	"In our program based on Flink 1.7.2, we used method {{ConfigOption#hasDeprecatedKeys}}. But, this method was renamed to {{hasFallbackKeys}} in FLINK-10436. So after we bump our flink version to 1.9.0, we meet compile error.

 
It seems we replaced the deprecated key with an entity {{FallbackKey}}. However, I still see the method {{withDeprecatedKeys}}. Since we keep the method {{withDeprecatedKeys}}, why not keep the method {{hasDeprecatedKeys}}. Although, this public API did not marked as {{Public}} annotation. IMHO, {{ConfigOption}} is hosted in flink-core module, many users also use it, we should maintain the compatibility as far as possible."	FLINK	Resolved	3	4	10999	pull-request-available
13278401	Add usage of ProcessFunctionTestHarnesses for testing documentation	"Recently, we added {{ProcessFunctionTestHarness}} for testing {{ProcessFunction}}. However, except {{ProcessFunctionTestHarnessesTest}} I can not find anything about this test harness in the master codebase.

Considering {{ProcessFunction}} is the very important and frenquency-used UDF.

I suggest that we could add a test example in the [testing documentation|https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/testing.html#integration-testing]."	FLINK	Closed	3	1	10999	pull-request-available
13163886	Add shade plugin executions to package table example jar	this is a preparatory work for issue FLINK-9519, so that we can get those examples' fat jar and then move them to example dir like batch and streaming. Because, there is no table examples in flink binary package.	FLINK	Closed	4	1	10999	pull-request-available
13163570	Migrate integration tests for iterative aggregators	Migrate integration tests in org.apache.flink.test.iterative.aggregators to use collect() instead of temp files. Related to parent jira.	FLINK	Resolved	4	7	10999	pull-request-available
13144112	CheckpointingStatisticsHandler fails to return PendingCheckpointStats 	"{noformat}
2018-03-10 21:47:52,487 ERROR org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler  - Implementation error: Unhandled exception.
java.lang.IllegalArgumentException: Given checkpoint stats object of type org.apache.flink.runtime.checkpoint.PendingCheckpointStats cannot be converted.
	at org.apache.flink.runtime.rest.messages.checkpoints.CheckpointStatistics.generateCheckpointStatistics(CheckpointStatistics.java:276)
	at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:146)
	at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:54)
	at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:81)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}"	FLINK	Closed	1	1	10999	flip6
13228022	Fix IllegalArgumentException thrown by FlinkKinesisConsumerMigrationTest#writeSnapshot	"Currently, {{FlinkKinesisConsumerMigrationTest#writeSnapshot}} throws an exception : 
{code:java}
java.lang.IllegalArgumentException: Cannot create enum from null value!

at com.amazonaws.regions.Regions.fromName(Regions.java:79)
at org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.createKinesisClient(AWSUtil.java:93)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.createKinesisClient(KinesisProxy.java:203)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.<init>(KinesisProxy.java:138)
at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.create(KinesisProxy.java:213)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.<init>(KinesisDataFetcher.java:275)
at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.<init>(KinesisDataFetcher.java:237)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest$TestFetcher.<init>(FlinkKinesisConsumerMigrationTest.java:422)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.writeSnapshot(FlinkKinesisConsumerMigrationTest.java:332)
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerMigrationTest.writeSnapshot(FlinkKinesisConsumerMigrationTest.java:113)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runners.Suite.runChild(Suite.java:128)
at org.junit.runners.Suite.runChild(Suite.java:27)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{code}
This exception may make the upgrader confused.

The exception is because the exists code did not initialize TestFetcher correctly. More details see the commits : https://issues.apache.org/jira/browse/FLINK-10785?focusedCommentId=16778899&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16778899 under FLINK-10785"	FLINK	Resolved	3	1	10999	pull-request-available
13249304	Update TypeSerializerSnapshotMigrationITCase to restore from 1.9 savepoint	Update {{TypeSerializerSnapshotMigrationITCase}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13176952	Add LTRIM supported in Table API and SQL	refer to MYSQL ltrim function : https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_ltrim	FLINK	Resolved	4	7	10999	pull-request-available
13115609	Add Apache Kafka 1.0/1.1 connectors	"Kafka 1.0.0 is no mere bump of the version number. The Apache Kafka Project Management Committee has packed a number of valuable enhancements into the release. Here is a summary of a few of them:

* Since its introduction in version 0.10, the Streams API has become hugely popular among Kafka users, including the likes of Pinterest, Rabobank, Zalando, and The New York Times. In 1.0, the the API continues to evolve at a healthy pace. To begin with, the builder API has been improved (KIP-120). A new API has been added to expose the state of active tasks at runtime (KIP-130). The new cogroup API makes it much easier to deal with partitioned aggregates with fewer StateStores and fewer moving parts in your code (KIP-150). Debuggability gets easier with enhancements to the print() and writeAsText() methods (KIP-160). And if that’s not enough, check out KIP-138 and KIP-161 too. For more on streams, check out the Apache Kafka Streams documentation, including some helpful new tutorial videos.
* Operating Kafka at scale requires that the system remain observable, and to make that easier, we’ve made a number of improvements to metrics. These are too many to summarize without becoming tedious, but Connect metrics have been significantly improved (KIP-196), a litany of new health check metrics are now exposed (KIP-188), and we now have a global topic and partition count (KIP-168). Check out KIP-164 and KIP-187 for even more.
* We now support Java 9, leading, among other things, to significantly faster TLS and CRC32C implementations. Over-the-wire encryption will be faster now, which will keep Kafka fast and compute costs low when encryption is enabled.
* In keeping with the security theme, KIP-152 cleans up the error handling on Simple Authentication Security Layer (SASL) authentication attempts. Previously, some authentication error conditions were indistinguishable from broker failures and were not logged in a clear way. This is cleaner now.
* Kafka can now tolerate disk failures better. Historically, JBOD storage configurations have not been recommended, but the architecture has nevertheless been tempting: after all, why not rely on Kafka’s own replication mechanism to protect against storage failure rather than using RAID? With KIP-112, Kafka now handles disk failure more gracefully. A single disk failure in a JBOD broker will not bring the entire broker down; rather, the broker will continue serving any log files that remain on functioning disks.
* Since release 0.11.0, the idempotent producer (which is the producer used in the presence of a transaction, which of course is the producer we use for exactly-once processing) required max.in.flight.requests.per.connection to be equal to one. As anyone who has written or tested a wire protocol can attest, this put an upper bound on throughput. Thanks to KAFKA-5949, this can now be as large as five, relaxing the throughput constraint quite a bit."	FLINK	Closed	3	4	10999	pull-request-available
13175375	Add regexp_extract supported in TableAPI and SQL	"regex_extract is a very useful function, it returns a string based on a regex pattern and a index.

For example : 
{code:java}
regexp_extract('foothebar', 'foo(.*?)(bar)', 2) // returns 'bar.'
{code}

It is provided as a UDF in Hive, more details please see[1].

[1]: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"	FLINK	Resolved	4	7	10999	pull-request-available
13185642	Scala example in DataSet docs is broken	"The Scala example of [https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/batch/dataset_transformations.html#combinable-groupreducefunctions] is broken.

The {{asScala}} and the {{reduce}} call fetch the Java {{Iterator}} which may only fetched once.
{quote}The Iterable can be iterated over only once. Only the first call to 'iterator()' will succeed.{quote}

While we are on it, it would make sense to check the other examples as well."	FLINK	Resolved	3	4	10999	pull-request-available
13168671	Add setDescription to execution environment and provide description field for the rest api	"Currently you can provide a job name to {{execute}} in the execution environment.  In an environment where many version of a job may be executing, such as a development or test environment, identifying which running job is of a specific version via the UI can be difficult unless the version is embedded into the job name given the {{execute}}.  But the job name is uses for other purposes, such as for namespacing metrics.  Thus, it is not ideal to modify the job name, as that could require modifying metric dashboards and monitors each time versions change.

I propose a new method be added to the execution environment, {{setDescription}}, that would allow a user to pass in an arbitrary description that would be displayed in the dashboard, allowing users to distinguish jobs."	FLINK	Closed	3	4	10999	pull-request-available, stale-assigned
13164339	Flink Overview of Jobs Documentation Incorrect	"[Link|https://ci.apache.org/projects/flink/flink-docs-release-1.5/monitoring/rest_api.html#overview-of-jobs]

""Jobs, grouped by status, each with a small summary of its status.""

This statement is incorrect as per the new response format."	FLINK	Resolved	4	1	10999	Documentation, pull-request-available
13233702	Remove STATE_UPDATER in ExecutionGraph	Since {{ExecutionGraph}} can only be accessed from a single thread(FLINK-11417). We can remove the {{AtomicReferenceFieldUpdater<ExecutionGraph, JobStatus> STATE_UPDATER}} from {{ExecutionGraph}}.	FLINK	Closed	4	7	10999	pull-request-available
13249295	Update FlinkKafkaProducerMigrationOperatorTest to restore from 1.9 savepoint	Update {{FlinkKafkaProducerMigrationOperatorTest}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13249289	Add MigrationVersion.v1_9	Add {{MigrationVersion.v1_9}}	FLINK	Closed	1	7	10999	pull-request-available
13224102	Bump universal Kafka connector to Kafka dependency to 2.2.0	Update the Kafka client dependency to version 2.2.0.	FLINK	Closed	3	4	10999	pull-request-available
13100784	JDBCOutputFormat autoCommit	"Currently, if a connection is not created with autoCommit = true by default (e.g. Apache Phoenix), no data is written into the database.

So, in the JDBCOutputFormat.open() autoCommit should be forced on the created Connection, i.e.:

{code:java}
if (!conn.getAutoCommit()) {
  conn.setAutoCommit(true);
}
{code}

This should be well documented also.."	FLINK	Reopened	10200	4	10999	stale-assigned
13233703	Remove GLOBAL_VERSION_UPDATER in ExecutionGraph	Since {{ExecutionGraph}} can only be accessed from a single thread. We can remove {{AtomicLongFieldUpdater<ExecutionGraph> GLOBAL_VERSION_UPDATER}} from {{ExecutionGraph}}.	FLINK	Closed	4	7	10999	pull-request-available
13165918	Potentially unclosed ByteBufInputStream in RestClient#readRawResponse	"Here is related code:
{code}
          ByteBufInputStream in = new ByteBufInputStream(content);
          byte[] data = new byte[in.available()];
          in.readFully(data);
{code}
In the catch block, ByteBufInputStream is not closed."	FLINK	Closed	4	1	10999	pull-request-available
13223320	Remove warning about max fields in case class 	The [serialization documentation|https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/types_serialization.html#flinks-typeinformation-class] states that there is a limit of 22 fields in a case class. Since [Scala 2.11|https://github.com/scala/bug/issues/7296] this arity limit has been removed and therefore this limit should also be removed on this documentation page. 	FLINK	Closed	4	4	10999	pull-request-available
13111168	Consider using nio.Files for file deletion in TransientBlobCleanupTask	"nio.Files#delete() provides better clue as to why the deletion may fail:

https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#delete(java.nio.file.Path)


Depending on the potential exception (FileNotFound), the call to localFile.exists() may be skipped."	FLINK	Closed	4	4	10999	pull-request-available
13242054	Elasticsearch 7.x support	"Elasticsearch 7.0.0 was released in April of 2019: [https://www.elastic.co/blog/elasticsearch-7-0-0-released]
The latest elasticsearch connector is [flink-connector-elasticsearch6|https://github.com/apache/flink/tree/master/flink-connectors/flink-connector-elasticsearch6]"	FLINK	Closed	3	2	10999	pull-request-available
13280694	Add MigrationVersion.v1_10	Add MigrationVersion.v1_10	FLINK	Closed	3	7	10999	pull-request-available
13195344	Misleading clean_log_files() in common.sh	"In the `common.sh` base script of the end-to-end tests, there is a `clean_stdout_files` which cleans only the `*.out` files and a `clean_log_files` which cleans *both* `*.log` and `*.out` files.

Given the current behavior that at the end of a test, the logs are checked and if there are exceptions (even expected ones but not whitelisted), the tests fails, some tests chose to call the `clean_log_files` so that exceptions are ignored. In this case, also `*.out` files are cleaned so if a test was checking for errors in the `.out` files, then the test will falsely pass.

The solution is as simple as renaming the method to something more descriptive like `clean_logs_and_output_files`, but doing so, also includes checking if any existing tests were falsely passing."	FLINK	Closed	3	1	10999	pull-request-available
13155736	Add method Writer[A]#contramap[B](f: B => A): Writer[B]	"Similar to https://issues.apache.org/jira/browse/FLINK-9221, it would be very handy to have a `Add method Writer[A]#contramap[B](f: B => A): Writer[B]` method, which would allow reuse of existing writers, with a ""formatting"" function placed infront of it. For example:

val stringWriter = new StringWriter[String]()
val intWriter: Writer[Int] = stringWriter.contraMap[Int](_.toString) "	FLINK	Reopened	10200	4	10999	stale-assigned
13249297	Update ContinuousFileProcessingMigrationTest to restore from 1.9 savepoint	Update {{ContinuousFileProcessingMigrationTest}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13186853	Add Tanh math function supported in Table API and SQL	refer to : https://www.techonthenet.com/oracle/functions/tanh.php	FLINK	Closed	4	7	10999	pull-request-available
13266715	Change Type of Field currentExecutions from ConcurrentHashMap to HashMap	After FLINK-11417, we made ExecutionGraph be a single-thread mode. It will no longer be plagued by concurrency issues. So, we can degenerate the current ConcurrentHashMap type of currentExecutions to a normal HashMap type.	FLINK	Closed	3	7	10999	pull-request-available
13178717	Certain cluster-level metrics are no longer exposed	"In [the documentation for metrics|https://ci.apache.org/projects/flink/flink-docs-release-1.5/monitoring/metrics.html#cluster] in the Flink 1.5.0 release, it says that the following metrics are reported by the JobManager:
{noformat}
numRegisteredTaskManagers
numRunningJobs
taskSlotsAvailable
taskSlotsTotal
{noformat}

In the job manager REST endpoint ({{http://<job-manager>:8081/jobmanager/metrics}}), those metrics don't appear."	FLINK	Closed	2	1	10999	pull-request-available
13166051	Logger in ZooKeeperStateHandleStore is public and non-final	The logger in {{ZooKeeperStateHandleStore}} should be private and final.	FLINK	Closed	4	1	10999	pull-request-available
13269008	Cleanup the description about container number config option in Scala and python shell doc	Currently, the config option {{-n}} for Flink on Yarn has not been supported since Flink 1.8+. FLINK-12362 did the cleanup job about this config option. However, the scala shell and python doc still contains some description about {{-n}} which may make users confused. This issue used to track the cleanup work.	FLINK	Closed	3	4	10999	pull-request-available
13266242	Change Type of Field tasks from ConcurrentHashMap to HashMap	After FLINK-11417, we made ExecutionGraph be a single-thread mode. It will no longer be plagued by concurrency issues. So, we can degenerate the current ConcurrentHashMap type of tasks to a normal HashMap type.	FLINK	Closed	3	7	10999	pull-request-available
13187272	Create and drop view in sql client should check the view created based on the configuration.	"Currently, just checked current session : 
{code:java}
private void callCreateView(SqlCommandCall cmdCall) {
   final String name = cmdCall.operands[0];
   final String query = cmdCall.operands[1];

   //here
   final String previousQuery = context.getViews().get(name);
   if (previousQuery != null) {
      printExecutionError(CliStrings.MESSAGE_VIEW_ALREADY_EXISTS);
      return;
   }
{code}
 "	FLINK	Closed	4	1	10999	stale-assigned, stale-minor
13170719	YARN: JM and TM Memory must be specified with Units 	"FLINK-6469 breaks backwards compatibility because the JobManager and TaskManager memory must be specified with units (otherwise bytes are assumed). The command to start a YARN session as documented ([https://github.com/apache/flink/blob/9f736d1927c62d220a82931c4f5ffa4955910f27/docs/ops/deployment/yarn_setup.md) |https://github.com/apache/flink/blob/9f736d1927c62d220a82931c4f5ffa4955910f27/docs/ops/deployment/yarn_setup.md] would not work because 1024 bytes and 4096 bytes are not enough for the heap size. The command finishes with the following exception:
{noformat}
java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1854)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:802)
Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:420)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:599)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.lambda$main$2(FlinkYarnSessionCli.java:802)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	... 2 more
Caused by: org.apache.flink.util.FlinkException: Cannot fulfill the minimum memory requirements with the provided cluster specification. Please increase the memory of the cluster.
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.validateClusterSpecification(AbstractYarnClusterDescriptor.java:453)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:475)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:413)
	... 7 more
Caused by: java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:123)
	at org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters.calculateCutoffMB(ContaineredTaskManagerParameters.java:115)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.validateClusterSpecification(AbstractYarnClusterDescriptor.java:450)
	... 9 more
{noformat}
 "	FLINK	Resolved	1	1	10999	pull-request-available
13163763	Implement TTL config	"`TtlConfig` has to be in flink core module.

Another option is to consider adding TtlConfig builder."	FLINK	Closed	3	7	10999	pull-request-available
13196192	Update AbstractNonKeyedOperatorRestoreTestBase for 1.7	Update {{AbstractNonKeyedOperatorRestoreTestBase}} and subclasses so that it covers restoring from 1.7.	FLINK	Closed	3	7	10999	pull-request-available
13244023	Remove dispatcherRetrievalService and dispatcherLeaderRetriever from RestClusterClient	In {{RestClusterClient}}, it contains two components: dispatcherRetrievalService which is the instance of {{LeaderRetrievalService}} and dispatcherLeaderRetriever which is the instance of {{LeaderRetriever}}. Both of them start in {{startLeaderRetrievers}} method and stop in {{shutdown}} method. It seems they are not necessary. IMO, we can remove them.	FLINK	Closed	4	4	10999	pull-request-available
13187172	toString field in AbstractID should be transient to avoid been serialized	"The toString field in AbstractID will be serialized currently, which makes RPC messages body like InputChannelDeploymentDescriptor and PartitionInfo larger (50%+).

It adds more pressure to JM memory especially in large scale job scheduling (10000x10000 ALL-to-ALL connection)."	FLINK	Resolved	3	4	10999	deploy,deployment, pull-request-available, serialization
13171934	Web UI only show partial taskmanager log 	" 

In the web UI, we select a task manager and click the ""log"" tab, but the UI only show the partial log (first part), can never update even if we click the ""refresh"" button.

However, the job manager is always OK.

The reason is the resource be closed twice."	FLINK	Closed	3	1	10999	pull-request-available
13266314	Change Type of Field intermediateResults from ConcurrentHashMap to HashMap	After FLINK-11417, we made ExecutionGraph be a single-thread mode. It will no longer be plagued by concurrency issues. So, we can degenerate the current ConcurrentHashMap type of intermediateResults to a normal HashMap type.	FLINK	Closed	3	7	10999	pull-request-available
13181841	Handle oversized response messages in AkkaRpcActor	The {{AkkaRpcActor}} should check whether an RPC response which is sent to a remote sender does not exceed the maximum framesize of the underlying {{ActorSystem}}. If this is the case we should fail fast instead. We can achieve this by serializing the response and sending the serialized byte array.	FLINK	Closed	3	4	10999	pull-request-available
13156154	Improve error message when TaskManager fails	"When a TaskManager fails, we frequently get a message

{code}
org.apache.flink.util.FlinkException: Releasing TaskManager container_1524853016208_0001_01_000102
{code}

This message is misleading in that it sounds like an intended operation, when it really is a failure of a container that the {{ResourceManager}} reports to the {{JobManager}}."	FLINK	Closed	2	4	10999	pull-request-available
13223770	Wrong check message about heartbeat interval for HeartbeatServices	"I am seeing:
{code:java}
The heartbeat timeout should be larger or equal than the heartbeat timeout{code}
due to bad configuration. I guess it should be instead:
{code:java}
The heartbeat interval should be larger or equal than the heartbeat timeout{code}
at:

https://github.com/apache/flink/blob/1f0e036bbf6a37bb83623fb62d4900d7c28a5e1d/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatServices.java#L43"	FLINK	Closed	4	1	10999	pull-request-available
13196191	Update CEPMigrationTest for 1.7	Update {{CEPMigrationTest}} so that it covers restoring from 1.7.	FLINK	Closed	3	7	10999	pull-request-available
13279832	Remove useless JobRetrievalException	Currently, the exception class {{JobRetrievalException}} has not been used anywhere in Flink codebase. IMO, we can remove it.	FLINK	Resolved	4	4	10999	pull-request-available
13249301	Update StatefulJobWBroadcastStateMigrationITCase to restore from 1.9 savepoint	Update {{StatefulJobWBroadcastStateMigrationITCase}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13176696	Migrate module flink-gelly-examples to flink-examples and rename it	I think we can put all the example modules into flink-examples module.	FLINK	Closed	4	4	10999	stale-minor
13169361	Provide connector for modern Kafka	"Kafka 2.0.0 would be released soon.

Here is vote thread:

[http://search-hadoop.com/m/Kafka/uyzND1vxnEd23QLxb?subj=+VOTE+2+0+0+RC1]

We should provide connector for Kafka 2.0.0 once it is released.

Upgrade to 2.0 documentation : http://kafka.apache.org/20/documentation.html#upgrade_2_0_0"	FLINK	Closed	3	2	10999	pull-request-available
13249292	Update BucketingSinkMigrationTest to restore from 1.9 savepoint	Update {{BucketingSinkMigrationTest}} to restore from 1.9 savepoint.	FLINK	Resolved	1	7	10999	pull-request-available
13232589	[State TTL] Consider setting a default background cleanup strategy in StateTtlConfig	"At the moment we have two efficient background cleanup strategies: incremental for heap and compaction filter for RocksDB. *StateTtlConfig* has 2 methods to activate them: *cleanupIncrementally* and *cleanupInRocksdbCompactFilter*. Each is activated only for certain backend type and inactive for other. They have different tuning parameters.

The idea is to add method *cleanupInBackground* which would activate default background cleanup. User does not need to think then about details or used backend if not needed. Depending on actually used backend, the corresponding cleanup will kick in. If original strategy is not set with *cleanupIncrementally* and *cleanupInRocksdbCompactFilter* then backends should check whether default background cleanup is activated and if so, use it with default parameters.

We can also deprecate the parameterless *cleanupInRocksdbCompactFilter()* in favour of this new method."	FLINK	Closed	3	4	10999	pull-request-available
13011615	"Checkpoint Coordinator should fail ExecutionGraph after ""n"" unsuccessful checkpoints"	"The Checkpoint coordinator should track the number of consecutive unsuccessful checkpoints.

If more than {{n}} (configured value) checkpoints fail in a row, it should call {{fail()}} on the execution graph to trigger a recovery.

The design document is here : https://docs.google.com/document/d/1ce7RtecuTxcVUJlnU44hzcO2Dwq9g4Oyd8_biy94hJc/edit?usp=sharing"	FLINK	Closed	3	7	10999	pull-request-available
13198522	Upgrade Kafka client version to 2.0.1	Since the modern kafka connector only keeps track of the latest version of the kafka client. With the release of Kafka 2.0.1, we should upgrade the version of the kafka client maven dependency.	FLINK	Closed	3	7	10999	pull-request-available
13230611	Remove legacy container number config option for Flink on yarn	The {{-n}} config option for Flink on YARN has been deprecated and removed from documentation. However, there are still some legacy code exists in the codebase. We need to clean up them.	FLINK	Resolved	3	4	10999	pull-request-available
13167163	Extending 'KafkaJsonTableSource' according to comments will result in NPE	"According to the comments what is needed to extend the 'KafkaJsonTableSource' looks as follows:

 
{code:java}
A version-agnostic Kafka JSON {@link StreamTableSource}.
*
* <p>The version-specific Kafka consumers need to extend this class and
* override {@link #createKafkaConsumer(String, Properties, DeserializationSchema)}}.
*
* <p>The field names are used to parse the JSON file and so are the types.{code}
This will cause an NPE, since there is no default value for startupMode in the abstract class itself only in the builder of this class. 
For the 'getKafkaConsumer' method the switch statement will be executed on non-initialized 'startupMode' field:
{code:java}
switch (startupMode) {
case EARLIEST:
kafkaConsumer.setStartFromEarliest();
break;
case LATEST:
kafkaConsumer.setStartFromLatest();
break;
case GROUP_OFFSETS:
kafkaConsumer.setStartFromGroupOffsets();
break;
case SPECIFIC_OFFSETS:
kafkaConsumer.setStartFromSpecificOffsets(specificStartupOffsets);
break;
}{code}
 

 "	FLINK	Closed	3	1	10999	pull-request-available
13181842	Handle oversized metric messages	Since the {{MetricQueryService}} is implemented as an Akka actor, it can only send messages of a smaller size then the current {{akka.framesize}}. We should check similarly to FLINK-10251 whether the payload exceeds the maximum framesize and fail fast if it is true.	FLINK	Closed	2	7	10999	pull-request-available
13218235	Update FlinkKafkaConsumerBaseMigrationTest for 1.8	Update {{FlinkKafkaConsumerBaseMigrationTest}} so that it covers restoring from 1.8.	FLINK	Resolved	1	7	10999	pull-request-available
13277923	Remove the wrong doc of ExecutionVertexSchedulingRequirements#getExecutionVertexId	"Currently, {{ExecutionVertexSchedulingRequirements#getExecutionVertexId}}'s doc is:


{code:java}
/**
 * a {@link ExecutionVertex#MAX_DISTINCT_LOCATIONS_TO_CONSIDER} test.
 *
 * @return
 */
{code}

It has a {{@return}} but missed relevant descriptions. However, when analysis of the call chain, it shows that it is called via general code(not only for testing purposes).

So, IMO, we either correct the java doc or remove it directly.

Based on the style in the same file, I suggest that we can remove it.
"	FLINK	Resolved	3	4	10999	pull-request-available
13280496	Remove legacy ExecutionAndAllocationFuture class	Currently, {{ExecutionAndAllocationFuture}} has not be used anywhere in Flink codebase. IMO, we can remove it.	FLINK	Resolved	3	4	10999	pull-request-available
13170418	Documentation of Hadoop API outdated	"It looks like the documentation of the [Hadoop Compatibility|https://ci.apache.org/projects/flink/flink-docs-release-1.5/dev/batch/hadoop_compatibility.html] is somewhat outdated? At least the text and examples in section [Using Hadoop InputFormats|https://ci.apache.org/projects/flink/flink-docs-release-1.5/dev/batch/hadoop_compatibility.html#using-hadoop-inputformats] mention methods

{{env.readHadoopFile}} and {{env.createHadoopInput}}

which do not exist anymore since 1.4.0. 

 

 "	FLINK	Closed	4	1	10999	pull-request-available
13174745	Add CHR function for table/sql API	"This function convert ASCII code to a character,

refer to : [https://doc.ispirer.com/sqlways/Output/SQLWays-1-071.html]

Considering ""CHAR"" always is a keyword in many database, so we use ""CHR"" keyword."	FLINK	Closed	4	2	10999	pull-request-available
13249299	Update StatefulJobSavepointMigrationITCase.scala to restore from 1.9 savepoint	Update {{StatefulJobSavepointMigrationITCase.scala}} to restore from 1.9 savepoint.	FLINK	Closed	1	7	10999	pull-request-available
13267648	Mark ExecutionVertex#deployToSlot with @VisibleForTesting annotation	From tracking the call chain of {{ExecutionVertex#deployToSlot}}, it seems this method is only been called in the test code. IMO, we'd better bring down {{ExecutionVertex#deployToSlot}} access modifier and mark it with @VisibleForTesting annotation to reduce the risk of incorrect calls	FLINK	Closed	4	4	10999	pull-request-available
13266821	Change Type of Field jobStatusListeners from CopyOnWriteArrayList to ArrayList	After FLINK-11417, we made ExecutionGraph be a single-thread mode. It will no longer be plagued by concurrency issues. So, we can degenerate the current CopyOnWriteArrayList type of jobStatusListeners to a normal ArrayList type.	FLINK	Closed	3	7	10999	pull-request-available
13187324	taskmanager.host is not respected	"The documentation states that taskmanager.host can be set to override the discovered hostname, however, setting this value has no effect.

Looking at the code, the value never seems to be used.  Instead, the deprecated taskmanager.hostname is still used."	FLINK	Closed	3	1	10999	pull-request-available
13243563	Fix documentation error about stop job with restful api	"Currently, [https://ci.apache.org/projects/flink/flink-docs-master/monitoring/rest_api.html#jobs-jobid-1] does not support ""stop"" mode. If users use stop mode will throw an exception:
{code:java}
The ""stop"" command has been removed. Please use ""stop-with-savepoint"" instead.
{code}
IMO, We should remove it.

 "	FLINK	Closed	3	4	10999	pull-request-available
13173734	Add FROM_BASE64 function for table/sql API	refer to mysql FROM_BASE64 function : https://dev.mysql.com/doc/refman/5.6/en/string-functions.html#function_from-base64	FLINK	Resolved	4	7	10999	pull-request-available
13170005	Potential resource leak in RocksDBStateBackend#getDbOptions	"Here is related code:
{code}
    if (optionsFactory != null) {
      opt = optionsFactory.createDBOptions(opt);
    }
{code}
opt, an DBOptions instance, should be closed before being rewritten.

getColumnOptions has similar issue."	FLINK	Closed	4	1	10999	pull-request-available
13159022	In Standalone checkpoint recover mode many jobs with same checkpoint interval cause IO pressure	"currently, the periodic checkpoint coordinator startCheckpointScheduler uses *baseInterval* as the initialDelay parameter. the *baseInterval* is also the checkpoint interval. 

In standalone checkpoint mode, many jobs config the same checkpoint interval. When all jobs being recovered (the cluster restart or jobmanager leadership switched), all jobs' checkpoint period will tend to accordance. All jobs' CheckpointCoordinator would start and trigger in a approximate time point.

This caused the high IO cost in the same time period in our production scenario.

I suggest let the scheduleAtFixedRate's initial delay parameter as a API config which can let user scatter checkpoint in this scenario.

 

cc [~StephanEwen] [~Zentol]"	FLINK	Resolved	3	4	10999	pull-request-available
13183326	Generate JobGraph with fixed/configurable JobID in StandaloneJobClusterEntrypoint	The {{StandaloneJobClusterEntrypoint}} currently generates the {{JobGraph}} from the user code when being started. Due to the nature of how the {{JobGraph}} is generated, it will get a random {{JobID}} assigned. This is problematic in case of a failover because then, the {{JobMaster}} won't be able to detect the checkpoints. In order to solve this problem, we need to either fix the {{JobID}} assignment or make it configurable.	FLINK	Closed	2	4	10999	pull-request-available
13172232	Add a string to the print method to identify output for DataStream	The output of the print method of {[DataSet}} allows the user to supply a String to identify the output(see [FLINK-1486|https://issues.apache.org/jira/browse/FLINK-1486]). But {[DataStream}} doesn't support now. It is valuable to add this feature for {{DataStream}}	FLINK	Closed	3	2	10999	pull-request-available
13175377	Add regexp_replace supported in TableAPI and SQL	"regexp_replace is a very userful function to process String. 
 For example :
{code:java}
regexp_replace(""foobar"", ""oo|ar"", """") //returns 'fb.'
{code}
It is supported as a UDF in Hive, more details please see[1].

[1]: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF

 "	FLINK	Resolved	4	7	10999	pull-request-available
13196193	Update ContinuousFileProcessingMigrationTest for 1.7	Update {{ContinuousFileProcessingMigrationTest}} so that it covers restoring from 1.7.	FLINK	Closed	3	7	10999	pull-request-available
13221284	Allow FileSystem Configs to be altered at Runtime	"This stems from a need to be able to pass in S3 auth keys at runtime in order to allow users to specify the keys they want to use. Based on the documentation it seems that currently S3 keys need to be part of the Flink cluster configuration, in a hadoop file (which the cluster needs to pointed to) or JVM args.


This only seems to apply to the streaming API. Also Feel free to correct the following if I am wrong, as there may be pieces I have no run across, or parts of the code I have misunderstood.


Currently it seems that FileSystems are inferred based on the extension type and a set of cached Filesystems that are generated in the background. These seem to use the config as defined at the time they are stood up. Unfortunately there is no way to tap into this control mechanism or override this behavior as many places in the code pulls from this cache. This is particularly painful in the sink instance as there are places where this is used that are not accessible outside the package it is implemented.

Through a pretty hacky mechanism I have proved out that this is a self imposed limitation, as I was able to change the code to pass in a Filesystem from the top level and have it read and write to S3 given keys I set at runtime.

The current methodology is convenient, however there should be finer grain controls for instances where the cluster is in a multitenant environment.

As a final note it seems like both the FileSystem and FileSystemFactory classes are not Serializable. I can see why this would be the case in former, but I am not clear as to why a factory class would not be Serializable (like in the case of BucketFactory). If this can be made serializable this should make this a much cleaner process."	FLINK	Reopened	10200	4	10999	stale-assigned
13174041	Simplify taskmanager memory default values	"The default value for {{NETWORK_BUFFERS_MEMORY_MIN}} is currently defined is {{String.valueOf(64L << 20)}}, which in the documentation is represented as {{""67108864""}}.

Now that we have the {{MemorySize}} utility we can change the default to {{""64 mb""}}.

The same applies to {{NETWORK_BUFFERS_MEMORY_MAX}}."	FLINK	Closed	4	4	10999	pull-request-available
13190808	Cleanup constant isNewMode in YarnTestBase	"This seems to be a residual problem with FLINK-10396. It is set to true in that PR. Currently it has three usage scenarios:

1. assert, caused an error
{code:java}
assumeTrue(""The new mode does not start TMs upfront."", !isNewMode);
{code}
2. if (!isNewMode) the logic in the block would not have invoked, the if block can be removed

3. if (isNewMode) always been invoked, the if statement can be removed."	FLINK	Resolved	3	7	10999	pull-request-available
13224313	Remove ASSIGNED_SLOT_UPDATER in Execution.tryAssignResource	After making access to ExecutionGraph single-threaded in FLINK-11417, we can simplify execution slot assignment in Execution.tryAssignResource and get rid of ASSIGNED_SLOT_UPDATER as it happens now only in one JM main thread. Though, it seems that we have to keep `assignedResource` as volatile at the moment which could be further investigated.	FLINK	Closed	4	7	10999	pull-request-available
13369727	Setup .asf.yaml in flink-web	"Infra is making some changes to the hosting of websites from git, in 2 months and flink-web  appears to require a small change.

We need to add a .asf.yaml file, with these contents:
{code}
publish:
 whoami: asf-site
{code}

https://lists.apache.org/thread.html/r8d023c0f5afefca7f6ce4e26d02404762bd6234fbe328011e1564249%40%3Cusers.infra.apache.org%3E"	FLINK	Closed	3	4	11245	pull-request-available
13214194	Missing configuration sections since 1.5 docs	"It seems that during the merge of FLINK-8475, some of the content from the TM options didn't make it into the new doc generators, e.g. the notes in {{taskmanager.memory.preallocate}}. Before [commit eaff4da15b|https://github.com/apache/flink/commit/eaff4da15bdd7528dcc0d8a37fd59642cee53850], there were several sections with introduction, meaningful summary of important settings (not just an alphabetical list) and the option texts actually also contained some more information than the new descriptions.

I'd like to see those back again, in particular the (advanced) managed memory configuration notes for batch users."	FLINK	Closed	3	1	11245	pull-request-available
13235064	Flink-shaded's shade-sources profile does not work anymore	"{code}
> mvn clean package -Pshade-sources
...
[INFO] --- maven-shade-plugin:3.0.0:shade (shade-flink) @ flink-shaded-hadoop-2 ---
[INFO] Excluding org.apache.commons:commons-compress:jar:1.18 from the shaded jar.
[INFO] Excluding org.apache.avro:avro:jar:1.8.2 from the shaded jar.
[INFO] Including org.codehaus.jackson:jackson-core-asl:jar:1.9.13 in the shaded jar.
[INFO] Including org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13 in the shaded jar.
[INFO] Excluding com.thoughtworks.paranamer:paranamer:jar:2.7 from the shaded jar.
[INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.4 from the shaded jar.
[INFO] Excluding org.tukaani:xz:jar:1.5 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.7 from the shaded jar.
[INFO] Including org.apache.hadoop:hadoop-common:jar:2.4.1 in the shaded jar.
[INFO] Including org.apache.hadoop:hadoop-annotations:jar:2.4.1 in the shaded jar.
[INFO] Including com.google.guava:guava:jar:11.0.2 in the shaded jar.
[INFO] Excluding commons-cli:commons-cli:jar:1.3.1 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-math3:jar:3.5 from the shaded jar.
[INFO] Excluding xmlenc:xmlenc:jar:0.52 from the shaded jar.
[INFO] Including commons-httpclient:commons-httpclient:jar:3.1 in the shaded jar.
[INFO] Excluding commons-codec:commons-codec:jar:1.10 from the shaded jar.
[INFO] Excluding commons-io:commons-io:jar:2.4 from the shaded jar.
[INFO] Excluding commons-net:commons-net:jar:3.1 from the shaded jar.
[INFO] Excluding commons-collections:commons-collections:jar:3.2.2 from the shaded jar.
[INFO] Excluding javax.servlet:servlet-api:jar:2.5 from the shaded jar.
[INFO] Excluding commons-el:commons-el:jar:1.0 from the shaded jar.
[INFO] Excluding commons-logging:commons-logging:jar:1.1.3 from the shaded jar.
[INFO] Excluding log4j:log4j:jar:1.2.17 from the shaded jar.
[INFO] Including net.java.dev.jets3t:jets3t:jar:0.9.0 in the shaded jar.
[INFO] Including org.apache.httpcomponents:httpclient:jar:4.5.3 in the shaded jar.
[INFO] Including org.apache.httpcomponents:httpcore:jar:4.4.6 in the shaded jar.
[INFO] Excluding com.jamesmurty.utils:java-xmlbuilder:jar:0.4 from the shaded jar.
[INFO] Excluding commons-lang:commons-lang:jar:2.6 from the shaded jar.
[INFO] Excluding commons-configuration:commons-configuration:jar:1.7 from the shaded jar.
[INFO] Excluding commons-digester:commons-digester:jar:1.8.1 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-log4j12:jar:1.7.15 from the shaded jar.
[INFO] Including com.google.protobuf:protobuf-java:jar:2.5.0 in the shaded jar.
[INFO] Including org.apache.hadoop:hadoop-auth:jar:2.4.1 in the shaded jar.
[INFO] Excluding com.jcraft:jsch:jar:0.1.42 from the shaded jar.
[INFO] Including com.google.code.findbugs:jsr305:jar:1.3.9 in the shaded jar.
[WARNING] Could not get sources for com.google.code.findbugs:jsr305:jar:1.3.9:compile
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] flink-shaded 7.0 ................................... SUCCESS [  0.771 s]
[INFO] flink-shaded-force-shading 7.0 ..................... SUCCESS [  0.951 s]
[INFO] flink-shaded-asm-6 6.2.1-7.0 ....................... SUCCESS [  1.469 s]
[INFO] flink-shaded-guava-18 18.0-7.0 ..................... SKIPPED
[INFO] flink-shaded-netty-4 4.1.32.Final-7.0 .............. SKIPPED
[INFO] flink-shaded-netty-tcnative-dynamic 2.0.25.Final-7.0 SUCCESS [  2.195 s]
[INFO] flink-shaded-jackson-parent 2.9.8-7.0 .............. SUCCESS [  0.161 s]
[INFO] flink-shaded-jackson-2 2.9.8-7.0 ................... SKIPPED
[INFO] flink-shaded-jackson-module-jsonSchema-2 2.9.8-7.0 . SKIPPED
[INFO] flink-shaded-hadoop-2 2.4.1-7.0 .................... FAILURE [  2.597 s]
[INFO] flink-shaded-hadoop-2-uber 2.4.1-7.0 ............... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  3.499 s (Wall Clock)
[INFO] Finished at: 2019-05-23T09:22:04+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade (shade-flink) on project flink-shaded-hadoop-2: Execution shade-flink of goal org.apache.maven.plugins:maven-shade-plugin:3.0.0:shade failed.: NullPointerException -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-shaded-hadoop-2
{code}"	FLINK	Closed	2	1	11245	pull-request-available
13232585	CommonTestUtils#printLog4jDebugConfig() should include timestamp	"{{CommonTestUtils#printLog4jDebugConfig()}} is used by a few tests to generate a log4j configuration for external processes.
We should include a timestamp in each message. This would make it easier to figure out in which order operations between different processes occurred."	FLINK	Closed	3	4	11245	pull-request-available
13397314	Add additional availability timing metrics to Job lifecycle events	"Flink currently contains a number of availability lifecycle metrics[1] showing how long it takes to move through different job status'. We propose adding two additional metrics; startingTime, and cancellingTime (open to bikeshedding on the metric names). 

 
 * startingTime is the time it takes a job to get to running. 
 * cancellingTime is the time spent in status CANCELLING 

 

 [1]https://ci.apache.org/projects/flink/flink-docs-master/docs/ops/metrics/#availability"	FLINK	Closed	4	4	11245	pull-request-available
13501074	Provide more convenient way to programmatically configure reporters	"Configuring reporters programmatically is an uncommon task in tests, but is currently not convenient and error prone since you need to manually assemble the config key as it must contain the reporter name.

Add some factory methods to the `MetricOptions` to generate a config options at runtime given a reporter name."	FLINK	Closed	3	4	11245	pull-request-available
13253705	OSS FS NOTICE file is placed in wrong directory	The NOTICE file for the OSS filesystem is directly in the resources directory, and not in META-INF where it belongs. As a result the contained dependencies are not properly listed in NOTICE-binary.	FLINK	Closed	1	1	11245	pull-request-available
13202543	Skip deployment for flnk-storm-examples	"Similar to FLINK-10987 we should also update the {{LICENSE}} and {{NOTICE}} for {{flink-storm-examples}}.

 

This project creates several fat example jars that are deployed to maven central.

Alternatively we could about dropping these examples."	FLINK	Closed	3	4	11245	pull-request-available
13197075	ConnectedComponents test instable on Travis	"The ""ConnectedComponents iterations with high parallelism end-to-end test"" succeeds on Travis but the log contains with the following exception:

{code}
2018-11-08 10:15:13,698 ERROR org.apache.flink.runtime.taskexecutor.rpc.RpcResultPartitionConsumableNotifier  - Could not schedule or update consumers at the JobManager.
org.apache.flink.runtime.executiongraph.ExecutionGraphException: Cannot find execution for execution Id 5b02c2f51e51f68b66bfab07afc1bf17.
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.scheduleOrUpdateConsumers(ExecutionGraph.java:1635)
	at org.apache.flink.runtime.jobmaster.JobMaster.scheduleOrUpdateConsumers(JobMaster.java:637)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:247)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:162)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:70)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor.aroundReceive(Actor.scala:502)
	at akka.actor.Actor.aroundReceive$(Actor.scala:500)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
{code}"	FLINK	Closed	2	1	11245	pull-request-available
13204405	RestClusterClientTest#testSendIsNotRetriableIfHttpNotFound with BindException	"{quote}
03:14:22.321 [ERROR] Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.189 s <<< FAILURE! - in org.apache.flink.client.program.rest.RestClusterClientTest
03:14:22.322 [ERROR] testSendIsNotRetriableIfHttpNotFound(org.apache.flink.client.program.rest.RestClusterClientTest) Time elapsed: 0.043 s <<< ERROR!
java.net.BindException: Address already in use
{quote}

https://api.travis-ci.org/v3/job/467812798/log.txt"	FLINK	Closed	2	4	11245	pull-request-available, test-stability
13397331	FieldAccessor has direct scala dependency	"The FieldAccessor class in flink-streaming-java has a hard dependency on scala. It would be ideal if we could restrict this dependencies to flink-streaming-scala.

We could move the SimpleProductFieldAccessor & RecursiveProductFieldAccessor to flink-streaming-scala, and load them in the FieldAccessorFactory via reflection.

This is one of a few steps that would allow the Java Datastream API to be used without scala being on the classpath."	FLINK	Closed	3	7	11245	pull-request-available
13533787	Bad address construction in SqlClientTest	"The SqlClientTest constructs a host:port pair with this:

{code}
InetSocketAddress.createUnresolved(
                                    SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetAddress(),
                                    SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetPort())
                            .toString()
{code}

This is unnecessarily complicated and fails on Java 17 because the toString representation is _not_ guaranteed to return something of the form host:port."	FLINK	Closed	3	7	11245	pull-request-available
13405384	DispatcherTest  is unstable	"https://dev.azure.com/pnowojski/Flink/_build/results?buildId=534&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20

testCancellationOfNonCanceledTerminalJobFailsWithAppropriateException from DispatcherTest can fail with:

{noformat}
Oct 07 10:31:18 Expected: A CompletableFuture that failed with: org.apache.flink.runtime.messages.FlinkJobTerminatedWithoutCancellationException
Oct 07 10:31:18      but: Future is not completed.
Oct 07 10:31:18 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Oct 07 10:31:18 	at org.junit.Assert.assertThat(Assert.java:964)
Oct 07 10:31:18 	at org.junit.Assert.assertThat(Assert.java:930)
Oct 07 10:31:18 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationOfNonCanceledTerminalJobFailsWithAppropriateException(DispatcherTest.java:442)
Oct 07 10:31:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Oct 07 10:31:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Oct 07 10:31:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Oct 07 10:31:18 	at java.lang.reflect.Method.invoke(Method.java:498)
(...)
{noformat}
"	FLINK	Closed	3	1	11245	pull-request-available, test-stability
13247662	Release partitions for FINISHED tasks if they are cancelled/suspended	"With FLINK-12615 we removed that partitions are being explicitly released from the JM if an {{Execution}} which is in state {{FINISHED}} or {{FAILED}} is being cancelled. In order to not have resource leak when using pipelined result partitions whose consumers fail before start consuming, we should re-introduce the deleted else branch (removed via 408f6b67aefaccfc76708b2d4772eb7f0a8fd984).

Once we properly wait that a {{Task}} does not finish until its produced results have been either persisted or sent to a consumer, then we should be able to remove this branch again."	FLINK	Closed	1	1	11245	pull-request-available
13541911	Tests revoke leadership too early	There are a few tests issue a request to the dispatcher and immediately revoke leadership. In this case there is no guarantee that the request arrived before leadership was revoked, so it could fail if it arrives afterwards since we reject requests if we aren't the leader anymore.	FLINK	Closed	3	11500	11245	pull-request-available
13288731	Flink-dist bundles ZK 3.5 as JDK11-exclusive dependency	"This is the output from the CI system (https://travis-ci.org/apache/flink/jobs/656931001)
{code}
16:35:30.798 [ERROR] testKillYarnSessionClusterEntrypoint(org.apache.flink.yarn.YARNHighAvailabilityITCase)  Time elapsed: 10.363 s  <<< ERROR!
org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.deploySessionCluster(YARNHighAvailabilityITCase.java:296)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.lambda$testKillYarnSessionClusterEntrypoint$0(YARNHighAvailabilityITCase.java:165)
	at org.apache.flink.yarn.YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint(YARNHighAvailabilityITCase.java:157)
Caused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: 
The YARN application unexpectedly switched to state FAILED during deployment. 
Diagnostics from YARN: Application application_1583080501498_0002 failed 2 times in previous 10000 milliseconds due to AM Container for appattempt_1583080501498_0002_000002 exited with  exitCode: 1
Failing this attempt.Diagnostics: Exception from container-launch.
Container id: container_1583080501498_0002_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972)
	at org.apache.hadoop.util.Shell.run(Shell.java:869)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

... snip ...

16:44:14.840 [INFO] Results:
16:44:14.840 [INFO] 
16:44:14.840 [ERROR] Errors: 
16:44:14.840 [ERROR]   YARNHighAvailabilityITCase.testJobRecoversAfterKillingTaskManager:187->YarnTestBase.runTest:242->lambda$testJobRecoversAfterKillingTaskManager$1:191->deploySessionCluster:296 Â» ClusterDeployment
16:44:14.840 [ERROR]   YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint:157->YarnTestBase.runTest:242->lambda$testKillYarnSessionClusterEntrypoint$0:165->deploySessionCluster:296 Â» ClusterDeployment
16:44:14.840 [INFO] 
16:44:14.840 [ERROR] Tests run: 25, Failures: 0, Errors: 2, Skipped: 4
{code}

Digging deeper into the problem, this seems to be the root cause:
{code}
2020-03-01 16:35:14,444 INFO  org.apache.flink.shaded.curator4.org.apache.curator.utils.Compatibility [] - Using emulated InjectSessionExpiration
2020-03-01 16:35:14,466 WARN  org.apache.flink.shaded.curator4.org.apache.curator.CuratorZookeeperClient [] - session timeout [1000] is less than connection timeout [15000]
2020-03-01 16:35:14,491 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting YarnSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.<init>(Ljava/util/Map;)V
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.<init>(EnsembleTracker.java:57)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:159)
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165)
	at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138)
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518)
	at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80)
.
2020-03-01 16:35:14,502 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2020-03-01 16:35:14,512 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2020-03-01 16:35:14,514 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2020-03-01 16:35:14,548 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2020-03-01 16:35:14,604 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2020-03-01 16:35:14,592 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Could not start cluster entrypoint YarnSessionClusterEntrypoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint YarnSessionClusterEntrypoint.
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.yarn.entrypoint.YarnSessionClusterEntrypoint.main(YarnSessionClusterEntrypoint.java:80) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
Caused by: java.lang.NoSuchMethodError: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.server.quorum.flexible.QuorumMaj.<init>(Ljava/util/Map;)V
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.EnsembleTracker.<init>(EnsembleTracker.java:57) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:159) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:165) ~[flink-shaded-zookeeper-3.4.10.jar:3.4.10-10.0]
	at org.apache.flink.runtime.util.ZooKeeperUtils.startCuratorFramework(ZooKeeperUtils.java:138) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:128) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:305) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:263) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:207) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
	at javax.security.auth.Subject.doAs(Subject.java:423) ~[?:?]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	... 2 more
{code}"	FLINK	Closed	1	1	11245	pull-request-available, test-stability
13271429	Pass all reporter-specific settings via the FrontMetricGroup	"For the assembly of scopes there currently exist 3 parameters that are defined per reporter; the reporter index, used to cache scopes separately per reporter, and the delimiter/filter pair that is applied to each scope.

Currently, the FrontMetricGroup only provides the index, which the AbstractMetricGroup uses to figure out which delimiter should be used.
Instead we can store the delimiter directly in the FrontMetricGroup; this simplifies the metric group implementation and further decouples the scope generation from the MetricRegistry."	FLINK	Closed	3	4	11245	pull-request-available
13213306	Port ClientTest	Check and port {{ClientTest}} to new code base if necessary	FLINK	Closed	3	7	11245	pull-request-available
13433369	AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI failed with a timeout	"{{AdaptiveSchedulerITCase.}} failed in [this build|https://dev.azure.com/mapohl/flink/_build/results?buildId=855&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=5778] due to a timeout.
{code}
Mar 11 14:41:36 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 76.177 s <<< FAILURE! - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Mar 11 14:41:36 [ERROR] org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI  Time elapsed: 60.146 s  <<< ERROR!
Mar 11 14:41:36 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
Mar 11 14:41:36 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:167)
Mar 11 14:41:36 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Mar 11 14:41:36 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:137)
Mar 11 14:41:36 	at org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testExceptionHistoryIsRetrievableFromTheRestAPI(AdaptiveSchedulerITCase.java:268)
{code}"	FLINK	Closed	2	1	11245	pull-request-available
13200028	Move flink-shaded-hadoop to flink-shaded	"To allow reasonable dependency management we should move flink-shaded-hadoop to flink-shaded, with each supported version having it's own module and dependency management.


"	FLINK	Closed	3	4	11245	pull-request-available
13247581	Setup travis cron builds for Java 11	"Setup Jdk 11 cron builds, excluding updates to the dockerized tests to run on Java 11 (follow-up: FLINK-13719) which we also skipped for Java 9.

As part of this, all passing profiles will subsume the respective java 9 variant. Once all tests pass, the java 9 builds will be removed."	FLINK	Closed	3	7	11245	pull-request-available
13257113	Make flink-runtime scala-free	"As the consensus among our community(please link dedicated thread if there is) we keep in mind that {{flink-runtime}} will be eventually scala-free. It is because of maintenance concerns, release concerns and so on.

This is an umbrella issue to track all efforts that towards a scala-free {{flink-runtime}}."	FLINK	Closed	4	4	11245	auto-deprioritized-major
13413183	Setup required --add-opens/--add-exports	"Java 17 actually enforces the encapsulation of the JDK (opposed to Java 11 which just printed warnings), requiring us to explicitly open/export any package that we access illegally.

The following is a list of opens/exports that I needed to get most tests to pass, also with some comments which component needed them. Overall the ClosureCleaner and FieldSerializer result in the most offenses, as they try to access private fields.

These properties need to be set _for all JVMs in which we run Flink_, including surefire forks, other tests processes (TestJvmProcess/TestProcessBuilder/Yarn) and the distribution.
This needs some thought on how we can share this list across poms (surefire), code (test processes / yarn) and the configuration (distribution).

{code:xml}
<surefire.module.config> <!--
	-->--add-exports java.base/sun.net.util=ALL-UNNAMED <!--
	required by JmxServer
	-->--add-exports java.rmi/sun.rmi.registry=ALL-UNNAMED <!--
	-->--add-exports jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED <!--
	-->--add-exports jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED <!--
	-->--add-exports jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED <!--
	-->--add-exports jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED <!--
	-->--add-exports jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED <!--
	HadoopUtilsTest
	-->--add-exports java.security.jgss/sun.security.krb5=ALL-UNNAMED <!--
	-->--add-opens java.base/java.lang=ALL-UNNAMED <!--
	RocksDBAsyncSnapshotTest Whitebox SerializedThrowable
	-->--add-opens java.base/java.lang.invoke=ALL-UNNAMED <!--
	RoundingMode
	-->--add-opens java.base/java.math=ALL-UNNAMED <!--
	-->--add-opens java.base/java.net=ALL-UNNAMED <!--
	InitOutputPathTest
	-->--add-opens java.base/java.io=ALL-UNNAMED <!--
	InitOutputPathTest
	-->--add-opens java.base/java.lang.reflect=ALL-UNNAMED <!--
	ClosureCleaner Timestamp
	-->--add-opens java.sql/java.sql=ALL-UNNAMED <!--
	lz4 Buffer
	-->--add-opens java.base/java.nio=ALL-UNNAMED <!--
	ClosureCleaner SimpleDateFormat
	-->--add-opens java.base/java.text=ALL-UNNAMED <!--
	FieldSerializer LocalDate
	-->--add-opens java.base/java.time=ALL-UNNAMED <!--
	ExpressionKeysTest
	-->--add-opens java.base/java.util=ALL-UNNAMED <!--
	AsynchronousFileIOChannelTest
	-->--add-opens java.base/java.util.concurrent=ALL-UNNAMED <!--
	Kryo FieldSerializer
	-->--add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED <!--
	ClosureCleaner ReentrantLock
	-->--add-opens java.base/java.util.concurrent.locks=ALL-UNNAMED <!--
	Whitebox ReferencePipeline$Head
	-->--add-opens java.base/java.util.stream=ALL-UNNAMED <!--
	ClosureCleaner ZoneInfo
	-->--add-opens java.base/sun.util.calendar=ALL-UNNAMED
</surefire.module.config>
{code}

Additionally, the following JVM arguments must be supplied when running Maven:

{code}
export MAVEN_OPTS=""\
--add-exports jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED \
--add-exports jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED \
--add-exports jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED \
--add-exports jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED \
--add-exports jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED \
--add-exports java.security.jgss/sun.security.krb5=ALL-UNNAMED""
{code}"	FLINK	Closed	3	7	11245	pull-request-available
13515049	Change groupId for flink-connector-parent	"During the migration of the connectors to the external connector framework, we've used io.github.zentol.flink as groupId for the artifact flink-connector-parent to have the ability to quickly iterate on the connector parent. 

With the first wave of migrations and releases being completed, we should change this back to org.apache.flink"	FLINK	Closed	3	11500	11245	pull-request-available
13404788	Bundle netty in flink-shaded-zookeeper 3.5+	The client-server SSL support added in ZK 3.5 requires Netty, but we currently exclude it as we assumed it to not be necessary.	FLINK	Closed	3	4	11245	pull-request-available
13442831	Drop flink-yarn test-jar	We could do just fine without this test-jar.	FLINK	Closed	3	11500	11245	pull-request-available
13442762	Remove CliFrontendTestBase	The class isn't really providing any value.	FLINK	In Progress	3	11500	11245	pull-request-available, stale-assigned
13253040	Update Execution Plan docs	"The *Execution Plans* section is totally outdated and refers to the old {{tools/planVisalizer.html}} file that has been removed for two years.

https://ci.apache.org/projects/flink/flink-docs-master/dev/execution_plans.html"	FLINK	Closed	4	1	11245	pull-request-available
13477378	Print a log message if a Pojo/Tuple contains a generic type	"Users are encouraged to use POJO types, that will be serialized by the PojoSerializer which supports schema evolution.
If a user does not use a POJO we print an info message, linking to the docs and citing potential performance issues.

However, no such message is printed if a POJO _contains_ a generic type.

As a result there may be users out there believing to have optimal performance and support for schema evolution since, after all, they are able to use the POJO serializer, when this may not be the case."	FLINK	Closed	3	4	11245	pull-request-available
13244882	Skip hive connector test for JDK9 profile	The Hive binary we depend upon has issues when running with JDK9, e.g. it assumes application class loader is URL class loader, which is no longer true in JDK9.	FLINK	Closed	3	1	11245	pull-request-available
13397546	flink-dist NOTICE not properly checked by NoticeFileChecker	"com.github.scopt:scopt_2.11:3.5.0 is still bundled by flink-dist because of the scala-shell, but is not mentioned in the NOTICE file.

We should add it, and check why the notice check did not catch it."	FLINK	Closed	1	1	11245	pull-request-available
13278482	Apache Camel not bundled but listed in flink-dist NOTICE	Apache Camel dependencies are listed in the flink-dist NOTICE, but we removed the dependency in 1.9.0 (see FLINK-12040).	FLINK	Closed	1	1	11245	pull-request-available
13474026	Deprecate host/web-ui-port parameter of jobmanager.sh	"If we fix FLINK-28733 we could while we're at it deprecate these 2 parameters, since you can then also control them via dynamic properties.

This would also subsume FLINK-21038."	FLINK	Closed	4	4	11245	pull-request-available
13540036	Add FailsOnJava17 annotation	Add an annotation for disabling specific tests on Java 17, similar to FailsOnJava11.	FLINK	Closed	3	7	11245	pull-request-available
13509284	Make datadog reporter url configurable	"Make the URL configurable to support agents or other services supporting the datadog protocol.

Deprecate the data center option since it is subsumed."	FLINK	Open	3	2	11245	pull-request-available
13443577	Drop flink-clients test-jar	The test-jar is actually unused and could be removed entirely.	FLINK	Closed	3	11500	11245	pull-request-available
13424700	Generated SubtaskExecutionAttemptDetailsInfo contains duplicate property	Swaggers converts both start-time and start_time (kept for backwards-compatibility) into a startTime property, resulting in 2 properties with the same name in one class.	FLINK	Closed	3	1	11245	pull-request-available
13242733	FailoverStrategies should respect restart constraints	"RestartStrategies can define their own restrictions for whether job can be restarted or not. For example, they could count the number of total failures or observe failure rates.

FailoverStrategies are used for partial restarts of jobs, and currently largely bypass the restrictions defined by the restart strategies.

My proposal is the following:

Introduce a new method into the {{RestartStrategy}} interface to notify the strategy of failed task executions. Currently, strategies implicitly handle this in {{RestartStrategy#restart}}, as such the migration of our existing strategies should be trivial.

Next, before calling {{RestartStrategy#restart}}, inform the strategy about the task failure. This retains existing behavior.
Additionally, the {{FailoverStrategy}} implementation may additionally inform the restart strategy about task failures, if and when they perform a local failover. Additionally, all implementation have to check {{RestartStrategy#canRestart}} before attempting a failover.

"	FLINK	Closed	3	4	11245	pull-request-available
13370678	Rest client shutdown on failure runs in netty thread	Then using the CLI to run any command, if the request fails then the RestClient is shut down from the netty thread, which causes problems because when shutting down we try to shut down netty, but the netty thread is still busy shutting things down.	FLINK	Closed	3	1	11245	pull-request-available
13353983	Add SlotAllocator	"We should add the {{SlotAllocator}} as defined in [FLIP-160|https://cwiki.apache.org/confluence/x/mwtRCg]. The {{SlotAllocator}} implementation should support 
* slot sharing
* respect the configured parallelism of operators
* respect the max parallelism of operators (this point might already be covered if the configured parallelism cannot exceed the max parallelism, we should double check this)"	FLINK	Closed	3	7	11245	pull-request-available
13313597	Allow reporter factories to intercept reflection-based instantiation attempts	"Before 1.11 to use a reporter its class was configured, and the instance instantiated via reflection.
We then introduced reporter factories, and added an annotation for redirection instantiation attempts from the reporter class to factories, by annotating the reporter class with {{InstantiateViaFactory}}.
However, when we migrated reporters to plugins, this approach stopped working, the reason being that it required the reporter class to be accessible. The plugin system only exposes the factories however.

To ensure that existing configurations continue to work, I propose to add a new {{InterceptInstantiationViaReflection}} annotation for factories, with which they can specify a class name to intercept reflection-based instantiation attempts.

Basically, we just invert the {{InstantiateViaFactory}} logic.
Instead of the reporter saying ""This factory should be used to instantiate me."", the factory now say ""I can instantiate that reporter."""	FLINK	Closed	3	4	11245	pull-request-available
13286495	Migrate existing java e2e tests away from FlinkDistribution	"A few prototype e2e tests work directly against the {{FlinkDistribution}}. These should be migrated to work against a {{FlinkResource}} instead.

This also entails providing some setup instructions for the distribution, e.g., copying jars from opt to lib."	FLINK	Closed	3	4	11245	pull-request-available
13302791	StreamingKafkaITCase. testKafka timeouts on downloading Kafka	"CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=585&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}
2020-05-05T00:06:49.7268716Z [INFO] -------------------------------------------------------
2020-05-05T00:06:49.7268938Z [INFO]  T E S T S
2020-05-05T00:06:49.7269282Z [INFO] -------------------------------------------------------
2020-05-05T00:06:50.5336315Z [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-05-05T00:11:26.8603439Z [ERROR] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 276.323 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase
2020-05-05T00:11:26.8604882Z [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 120.024 s  <<< ERROR!
2020-05-05T00:11:26.8605942Z java.io.IOException: Process ([wget, -q, -P, /tmp/junit2815750531595874769/downloads/1290570732, https://archive.apache.org/dist/kafka/0.11.0.2/kafka_2.11-0.11.0.2.tgz]) exceeded timeout (120000) or number of retries (3).
2020-05-05T00:11:26.8606732Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:132)
2020-05-05T00:11:26.8607321Z 	at org.apache.flink.tests.util.cache.AbstractDownloadCache.getOrDownload(AbstractDownloadCache.java:127)
2020-05-05T00:11:26.8607826Z 	at org.apache.flink.tests.util.cache.LolCache.getOrDownload(LolCache.java:31)
2020-05-05T00:11:26.8608343Z 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.setupKafkaDist(LocalStandaloneKafkaResource.java:98)
2020-05-05T00:11:26.8608892Z 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.before(LocalStandaloneKafkaResource.java:92)
2020-05-05T00:11:26.8609602Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:46)
2020-05-05T00:11:26.8610026Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-05T00:11:26.8610553Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-05T00:11:26.8610958Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-05T00:11:26.8611388Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-05T00:11:26.8612214Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-05T00:11:26.8612706Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-05T00:11:26.8613109Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-05T00:11:26.8613551Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-05T00:11:26.8614019Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-05T00:11:26.8614442Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-05T00:11:26.8614869Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-05T00:11:26.8615251Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-05T00:11:26.8615654Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-05T00:11:26.8616060Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-05T00:11:26.8616465Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-05T00:11:26.8616893Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-05T00:11:26.8617893Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-05T00:11:26.8618490Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-05T00:11:26.8619056Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-05T00:11:26.8619589Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-05T00:11:26.8620073Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-05T00:11:26.8620745Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-05T00:11:26.8621172Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-05T00:11:26.8621585Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-05T00:11:26.8622006Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-05T00:11:26.8622403Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-05T00:11:26.8622811Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-05T00:11:26.8623212Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-05-05T00:11:26.8623698Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-05-05T00:11:26.8635914Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-05-05T00:11:26.8636538Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-05-05T00:11:26.8637056Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-05-05T00:11:26.8637543Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-05-05T00:11:26.8638072Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-05T00:11:26.8638590Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-05T00:11:26.8639065Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-05T00:11:26.8639502Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-05T00:11:26.8639902Z Caused by: java.io.IOException: Process failed due to timeout.
2020-05-05T00:11:26.8640420Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:114)
2020-05-05T00:11:26.8641189Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:125)
2020-05-05T00:11:26.8641598Z 	... 42 more
2020-05-05T00:11:26.8641915Z 
2020-05-05T00:11:26.8642783Z [ERROR] testKafka[2: kafka-version:2.2.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 120.004 s  <<< ERROR!
2020-05-05T00:11:26.8643829Z java.io.IOException: Process ([wget, -q, -P, /tmp/junit4015087902971183123/downloads/-1713883086, https://archive.apache.org/dist/kafka/2.2.2/kafka_2.11-2.2.2.tgz]) exceeded timeout (120000) or number of retries (3).
2020-05-05T00:11:26.8644556Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:132)
2020-05-05T00:11:26.8645138Z 	at org.apache.flink.tests.util.cache.AbstractDownloadCache.getOrDownload(AbstractDownloadCache.java:127)
2020-05-05T00:11:26.8645623Z 	at org.apache.flink.tests.util.cache.LolCache.getOrDownload(LolCache.java:31)
2020-05-05T00:11:26.8646140Z 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.setupKafkaDist(LocalStandaloneKafkaResource.java:98)
2020-05-05T00:11:26.8646720Z 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.before(LocalStandaloneKafkaResource.java:92)
2020-05-05T00:11:26.8647201Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:46)
2020-05-05T00:11:26.8647610Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-05-05T00:11:26.8648025Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-05-05T00:11:26.8648401Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-05-05T00:11:26.8648823Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-05-05T00:11:26.8649310Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-05-05T00:11:26.8649736Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-05T00:11:26.8650123Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-05T00:11:26.8650560Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-05T00:11:26.8650968Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-05T00:11:26.8651367Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-05T00:11:26.8652115Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-05T00:11:26.8652517Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-05T00:11:26.8654347Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-05T00:11:26.8654809Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-05T00:11:26.8655235Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-05T00:11:26.8655692Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-05T00:11:26.8656141Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-05T00:11:26.8656544Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-05T00:11:26.8656956Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-05T00:11:26.8657353Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-05-05T00:11:26.8657727Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-05-05T00:11:26.8658110Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-05-05T00:11:26.8658525Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-05-05T00:11:26.8658923Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-05-05T00:11:26.8659343Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-05-05T00:11:26.8659740Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-05-05T00:11:26.8660148Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-05-05T00:11:26.8660740Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-05-05T00:11:26.8661213Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-05-05T00:11:26.8661734Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-05-05T00:11:26.8662334Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-05-05T00:11:26.8662808Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-05-05T00:11:26.8663278Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-05-05T00:11:26.8663817Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-05-05T00:11:26.8664315Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-05-05T00:11:26.8664791Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-05-05T00:11:26.8665264Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-05-05T00:11:26.8665735Z Caused by: java.io.IOException: Process failed due to timeout.
2020-05-05T00:11:26.8667503Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:114)
2020-05-05T00:11:26.8668023Z 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:125)
2020-05-05T00:11:26.8668347Z 	... 42 more
2020-05-05T00:11:26.8668440Z 
2020-05-05T00:11:26.8671270Z [INFO] Running org.apache.flink.tests.util.kafka.SQLClientKafkaITCase
{code}"	FLINK	Closed	2	7	11245	pull-request-available, test-stability
13436181	Document reporter behavior w.r.t. scopes & push/pull	The docs are lacking information for whether a reporter uses a metric identifier or the logical scope + tags, and whether they are push or pull based.	FLINK	Closed	3	4	11245	pull-request-available
13180902	Bump mockito to 2.0+	"Mockito only properly supports java 9 with version 2. We have to bump the dependency and fix various API incompatibilities.

Additionally we could investigate whether we still need powermock after bumping the dependency (which we'd also have to bump otherwise)."	FLINK	Closed	3	7	11245	pull-request-available
13264463	Capture partition meta info on TaskExecutor	We need store some meta information (initially the IntermediateDataSetID, later on things libe createdAtDate, jobname, taskname etc.) in the TaskExecutor book-keeping so we can forward this later to the RM in case of promotions.	FLINK	Closed	3	7	11245	pull-request-available
13283900	Harden jackson dependency constraints	Replace the individual jackson dependency management entries with the jackson bom, and introduce an enforcer check to ban older jackson dependencies.	FLINK	Closed	3	4	11245	pull-request-available
13248094	Kinesis connector missing jaxb-api dependency	KPL makes use of {{javax.xml.bind.DatatypeConverter}} but does not declare a dependency on {{jaxb-api}} and relies on the JDK containing this class. This is no longer the case on Java 11; we have to add it as a dependency and bundle it in the jar.	FLINK	Closed	3	7	11245	pull-request-available
13458566	Generalize utils around dependency-plugin	"We'll be adding another safeguard against developer mistakes which also parses the output of the dependency plugin, like the scala suffix checker.

We should generalize this parsing such that both checks can use the same code."	FLINK	Closed	3	7	11245	pull-request-available
13409113	AkkaInvocationHandler silently ignores deserialization errors	"In AkkaInvocationHandler#invokeRpc we create a new CompletableFuture to return to the caller, and setup a forwarding routine that runs when the response is received via akka.

{code}
final CompletableFuture<Object> completableFuture = new CompletableFuture<>();
resultFuture.whenComplete(
        (resultValue, failure) -> {
            if (failure != null) {
                completableFuture.completeExceptionally(
                        resolveTimeoutException(
                                failure, callStackCapture, address, rpcInvocation));
            } else {
                completableFuture.complete(
                        deserializeValueIfNeeded(resultValue, method));
            }
        });
{code}

If {{deserializeValueIfNeeded}} fails then {{completableFuture}} will never be completed, and we exception will not be logged anywhere."	FLINK	Closed	3	1	11245	pull-request-available
13481527	Set timeout on CI	With the repo using apache resources we should ensure tests that are stuck arent consuming unnecessary resources.	FLINK	Closed	3	11500	11245	pull-request-available
13477188	flink-end-to-end-tests-sql compile failed in hadoop3	"{code:java}
2022-08-17T00:39:13.9082097Z Dependency convergence error for com.nimbusds:nimbus-jose-jwt:4.41.1 paths to dependency are:
2022-08-17T00:39:13.9082987Z +-org.apache.flink:flink-end-to-end-tests-sql:1.16-SNAPSHOT
2022-08-17T00:39:13.9083712Z   +-org.apache.hadoop:hadoop-common:3.1.3
2022-08-17T00:39:13.9084340Z     +-org.apache.hadoop:hadoop-auth:3.1.3
2022-08-17T00:39:13.9084963Z       +-com.nimbusds:nimbus-jose-jwt:4.41.1
2022-08-17T00:39:13.9085616Z and
2022-08-17T00:39:13.9086212Z +-org.apache.flink:flink-end-to-end-tests-sql:1.16-SNAPSHOT
2022-08-17T00:39:13.9086864Z   +-org.apache.hadoop:hadoop-common:3.1.3
2022-08-17T00:39:13.9087499Z     +-org.apache.kerby:kerb-simplekdc:1.0.1
2022-08-17T00:39:13.9088125Z       +-org.apache.kerby:kerb-client:1.0.1
2022-08-17T00:39:13.9088753Z         +-org.apache.kerby:token-provider:1.0.1
2022-08-17T00:39:13.9089381Z           +-com.nimbusds:nimbus-jose-jwt:3.10
2022-08-17T00:39:13.9089596Z 
2022-08-17T00:39:13.9090061Z [WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
2022-08-17T00:39:13.9090651Z Failed while enforcing releasability. See above detailed error message.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691
"	FLINK	Closed	2	11500	11245	pull-request-available, test-stability
13483666	Add a uid(hash) remapping function	Expose functionality for modifying the uid[hash] of a state.	FLINK	Closed	3	7	11245	pull-request-available
13382775	Migrate to flink-shaded-force-shading	Migrate to flink-shaded-force-shading, allowing us to drop force-shading removing an annoying bit of our build setup.	FLINK	Closed	3	11500	11245	pull-request-available
13520065	IPv6HostnamesITCase should create remote rpc service	"The test wants to ensure that Akka's Netty can bind to IPv6 addresses, but uses a local rpc service which doesn't use netty.
This was incorrectly migrated in FLINK-23090."	FLINK	Closed	3	11500	11245	pull-request-available
13272806	Drop Kafka 0.8/0.9	As discussed on the [mailing list|https://lists.apache.org/thread.html/faf03a7ddb151e2b311894e57b9dc950dc21b10333c5d01be763ed74%40%3Cdev.flink.apache.org%3E], remove the Kafka 0.8/0.9 connectors.	FLINK	Closed	3	4	11245	pull-request-available
13247618	Bump powermock to 2.0.2	"We have to bump powermock to 2.0.2 to resolve this issue in the {{InitOutputPathTest:}}
{code:java}
java.lang.IllegalStateException: Failed to transform class with name org.apache.flink.core.fs.InitOutputPathTest. Reason: [source error] the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock

    at org.powermock.core.classloader.javassist.JavassistMockClassLoader.defineAndTransformClass(JavassistMockClassLoader.java:119)
    at org.powermock.core.classloader.MockClassLoader.loadMockClass(MockClassLoader.java:174)
    at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:102)
    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
    at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Class.java:398)
    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:154)
    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.createDelegatorFromClassloader(JUnit4TestSuiteChunkerImpl.java:47)
    at org.powermock.tests.utils.impl.AbstractTestSuiteChunkerImpl.createTestDelegators(AbstractTestSuiteChunkerImpl.java:107)
    at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.<init>(JUnit4TestSuiteChunkerImpl.java:69)
    at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.<init>(AbstractCommonPowerMockRunner.java:36)
    at org.powermock.modules.junit4.PowerMockRunner.<init>(PowerMockRunner.java:34)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:104)
    at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:86)
    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
    at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)
    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
    at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:33)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:49)
    at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: javassist.CannotCompileException: [source error] the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock
    at javassist.expr.NewExpr.replace(NewExpr.java:214)
    at org.powermock.core.transformers.javassist.support.PowerMockExpressionEditor.edit(PowerMockExpressionEditor.java:73)
    at javassist.expr.ExprEditor.loopBody(ExprEditor.java:212)
    at javassist.expr.ExprEditor.doit(ExprEditor.java:91)
    at javassist.CtClassType.instrument(CtClassType.java:1431)
    at org.powermock.core.transformers.javassist.InstrumentMockTransformer.transform(InstrumentMockTransformer.java:41)
    at org.powermock.core.transformers.javassist.AbstractJavaAssistMockTransformer.transform(AbstractJavaAssistMockTransformer.java:40)
    at org.powermock.core.transformers.support.DefaultMockTransformerChain.transform(DefaultMockTransformerChain.java:43)
    at org.powermock.core.classloader.MockClassLoader.transformClass(MockClassLoader.java:184)
    at org.powermock.core.classloader.javassist.JavassistMockClassLoader.defineAndTransformClass(JavassistMockClassLoader.java:102)
    ... 27 more
Caused by: compile error: the called constructor is private in org.apache.flink.core.fs.InitOutputPathTest$NoOpLock
    at javassist.compiler.MemberCodeGen.getAccessibleConstructor(MemberCodeGen.java:709)
    at javassist.compiler.MemberCodeGen.atMethodCallCore2(MemberCodeGen.java:610)
    at javassist.compiler.MemberCodeGen.atMethodCallCore(MemberCodeGen.java:589)
    at javassist.expr.NewExpr$ProceedForNew.doit(NewExpr.java:237)
    at javassist.compiler.JvstCodeGen.atCallExpr(JvstCodeGen.java:235)
    at javassist.compiler.ast.CallExpr.accept(CallExpr.java:46)
    at javassist.compiler.CodeGen.atAssignCore(CodeGen.java:877)
    at javassist.compiler.CodeGen.atVariableAssign(CodeGen.java:810)
    at javassist.compiler.CodeGen.atAssignExpr(CodeGen.java:764)
    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:332)
    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)
    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:351)
    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)
    at javassist.compiler.CodeGen.atIfStmnt(CodeGen.java:411)
    at javassist.compiler.CodeGen.atStmnt(CodeGen.java:355)
    at javassist.compiler.ast.Stmnt.accept(Stmnt.java:50)
    at javassist.compiler.Javac.compileStmnt(Javac.java:569)
    at javassist.expr.NewExpr.replace(NewExpr.java:208)
    ... 36 more{code}"	FLINK	Closed	3	7	11245	pull-request-available
13253672	HistoryServerTest failed on Travis	"{code}
16:56:29.548 [ERROR] testHistoryServerIntegration[Flink version less than 1.4: false](org.apache.flink.runtime.webmonitor.history.HistoryServerTest)  Time elapsed: 0.69 s  <<< ERROR!
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: 
Cannot construct instance of `org.apache.flink.runtime.messages.webmonitor.MultipleJobsDetails`, problem: `java.lang.NullPointerException`
 at [Source: (String)""{""errors"":[""File not found.""]}""; line: 1, column: 30]
	at org.apache.flink.runtime.webmonitor.history.HistoryServerTest.testHistoryServerIntegration(HistoryServerTest.java:142)
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.webmonitor.history.HistoryServerTest.testHistoryServerIntegration(HistoryServerTest.java:142)
{code}

https://api.travis-ci.org/v3/job/577860508/log.txt"	FLINK	Closed	3	1	11245	pull-request-available
13397725	Remove Scala dependencies from flink-cep/flink-streaming-java	To finalize the Scala isolation we should remove the Scala dependencies from flink-cep/flink-streaming-java, and resolve the resulting cascade of unnecessary scala suffixes.	FLINK	Closed	3	7	11245	pull-request-available
13531610	FlameGraphTypeQueryParameter#Type clashes with java.reflect.Type in generated clients	"Generating a client with the openapi generators causes compile errors because the generated file imports java.reflect.Type, but also the generated ""Type"" model.

For convenience it would be neat to give this enum a slightly different name, because working around this issue is surprisingly annoying."	FLINK	Resolved	3	1	11245	pull-request-available
13239501	Add travis profile for gelly/kafka	The misc/tests profiles frequently hit timeouts; we can resolve this by moving gelly and the universal kafka connector into a separate profile.	FLINK	Closed	3	4	11245	pull-request-available
13364973	JobManagerMG should not accept JobGraph	JobManagerMetricGroup#addJob currently accepts a JobGraph; this should be refactored to accept the job name and ID to ease usage of this API in tests.	FLINK	Closed	3	7	11245	auto-unassigned, pull-request-available
13344534	Enable log4j2 monitor interval by default	"Log4j2 has a feature where the configuration can be updated at runtime, but it needs the {{monitorInterval}} to be configured in the log4j configuration file.
We should enable this by default."	FLINK	Closed	3	4	11245	pull-request-available
13450399	Replace IOUtils dependency on oss filesystem	"The oss fs has an undeclared dependency on commons-io for a single call to IOUtils.
We can make our lives a little bit easier by using the Flink IOUtils instead."	FLINK	Closed	3	7	11245	pull-request-available
13328496	Add compatibility layer for new declarative SlotPool	Add a compatibility layer that maps fulfilled requirements to matching SlotRequests issued by the ExecutionGraph.	FLINK	Closed	3	4	11245	pull-request-available
13500707	Rework connector docs integration	"The current connector integration doesn't work properly with branches and is virtually impossible to maintain because of how obscure go modules are.

Re-implement it by checking out specific commits and copying files as necessary into a pseudo local module instead."	FLINK	Closed	3	11500	11245	pull-request-available
13337169	Add ServiceConnectionManager	"The Slotpool has to interact with the ResourceManager to declare the resource requirements.

We do not want to provide full access to the ResourceManagerGateway (and as such should wrap it in some form), but we also have to handle the case where no ResourceManager is connected.

Introduce a component for handling this."	FLINK	Closed	3	4	11245	pull-request-available
13413144	Upgrade lombok to 1.18.22	Our current Lombok version fails on Java 17 due to illegal accesses to JDK internals. Newer versions of Lombok do some hacks at runtime to resolve the issue...	FLINK	Closed	3	7	11245	pull-request-available
13400145	Use ContextClassLoaderExtension in ReporterSetupTest	"Migrate the ReporterSetupTests to use ContextClassLoaderExtension to generate service entries at runtime, scoped to the test, instead of having them as a test resource.

See https://github.com/apache/flink/commit/8081dfbcc2c63dfda385a68f4615ddf5d51ccc26 as an example."	FLINK	Closed	3	11500	11245	pull-request-available, stale-assigned, starter
13434085	SessionDispatcherLeaderProcess#onAddJobGraph crashes if the job is marked as deleted	Under certain situations it can happen that a dispatcher is informed about an added jobgraph while that JobGraph is already marked for deletion. This case isn't properly handled, causing an NPE that crashes the cluster.	FLINK	Closed	1	1	11245	pull-request-available
13170835	scala-maven-plugin fails on java 9	"https://travis-ci.org/zentol/flink/jobs/401711258

{code}
11:10:02.157 [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-runtime_2.11 ---
11:10:04.861 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/java:-1: info: compiling
11:10:04.862 [INFO] /home/travis/build/zentol/flink/flink-runtime/src/main/scala:-1: info: compiling
11:10:04.862 [INFO] Compiling 1486 source files to /home/travis/build/zentol/flink/flink-runtime/target/classes at 1531134604862
11:10:06.135 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider
11:10:06.135 [INFO] 	at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)
11:10:06.135 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
11:10:06.135 [INFO] 	at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)
11:10:06.135 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)
11:10:06.135 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
11:10:06.136 [INFO] 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
11:10:06.136 [INFO] 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
11:10:06.136 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
11:10:06.136 [INFO] 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)
11:10:06.136 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)
11:10:06.136 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)
11:10:06.136 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
11:10:06.136 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
11:10:06.136 [INFO] 	at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)
11:10:06.136 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)
11:10:06.136 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)
11:10:06.136 [INFO] 	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)
11:10:06.136 [INFO] 	at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)
11:10:06.136 [INFO] 	at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)
11:10:06.136 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:71)
11:10:06.136 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:39)
11:10:06.136 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)
11:10:06.136 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)
11:10:06.136 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)
11:10:06.136 [INFO] 	at scala.tools.nsc.Global$Run.<init>(Global.scala:1242)
11:10:06.136 [INFO] 	at scala.tools.nsc.Driver.doCompile(Driver.scala:31)
11:10:06.136 [INFO] 	at scala.tools.nsc.MainClass.doCompile(Main.scala:23)
11:10:06.136 [INFO] 	at scala.tools.nsc.Driver.process(Driver.scala:51)
11:10:06.136 [INFO] 	at scala.tools.nsc.Driver.main(Driver.scala:64)
11:10:06.136 [INFO] 	at scala.tools.nsc.Main.main(Main.scala)
11:10:06.136 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
11:10:06.136 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
11:10:06.136 [INFO] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
11:10:06.137 [INFO] 	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
11:10:06.137 [INFO] 	at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)
11:10:06.137 [INFO] 	at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)
11:10:06.137 [INFO] java.lang.reflect.InvocationTargetException
11:10:06.137 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
11:10:06.137 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
11:10:06.137 [INFO] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
11:10:06.137 [INFO] 	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
11:10:06.137 [INFO] 	at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)
11:10:06.137 [INFO] 	at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)
11:10:06.137 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider
11:10:06.137 [INFO] 	at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)
11:10:06.137 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
11:10:06.137 [INFO] 	at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
11:10:06.138 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
11:10:06.138 [INFO] 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
11:10:06.138 [INFO] 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
11:10:06.138 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
11:10:06.138 [INFO] 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
11:10:06.148 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)
11:10:06.148 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)
11:10:06.149 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)
11:10:06.149 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
11:10:06.149 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
11:10:06.149 [INFO] 	at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)
11:10:06.149 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)
11:10:06.149 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)
11:10:06.149 [INFO] 	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)
11:10:06.149 [INFO] 	at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)
11:10:06.149 [INFO] 	at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)
11:10:06.149 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:71)
11:10:06.149 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:39)
11:10:06.151 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)
11:10:06.153 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)
11:10:06.153 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)
11:10:06.153 [INFO] 	at scala.tools.nsc.Global$Run.<init>(Global.scala:1242)
11:10:06.153 [INFO] 	at scala.tools.nsc.Driver.doCompile(Driver.scala:31)
11:10:06.153 [INFO] 	at scala.tools.nsc.MainClass.doCompile(Main.scala:23)
11:10:06.172 [INFO] 	at scala.tools.nsc.Driver.process(Driver.scala:51)
11:10:06.172 [INFO] 	at scala.tools.nsc.Driver.main(Driver.scala:64)
11:10:06.172 [INFO] 	at scala.tools.nsc.Main.main(Main.scala)
11:10:06.172 [INFO] 	... 6 more
11:10:06.196 [INFO] ------------------
{code}"	FLINK	Closed	3	7	11245	pull-request-available
13268341	Process ClusterPartitionReport on ResourceManager	Setup a data-structure for storing the cluster partition report that the ResourceManager receives via TaskExecutor heartbeats.	FLINK	Closed	3	7	11245	pull-request-available
13407710	flink-rpc-akka uses wrong Scala version property for parser-combinators	"dependencyManagement entry:
{code}
<dependency>
	<groupId>org.scala-lang.modules</groupId>
	<artifactId>scala-parser-combinators_${scala.binary.version}</artifactId>
	<version>1.1.2</version>
</dependency>
{code}

Conceptually this should use {{${akka.scala.binary.version}}}.

Interestingly enough, changing this breaks the build because maven then ignores the entry entirely, only using the existing entry from the root pom. I think there's some bug where if properties are used in artifactIds, then the properties references themselves must match in order to override them. Amazingly, this even fails if you hard-code the scala version.

Adding a direct dependency on parser-combinators does fix the packaging problem, but then we run into dependency convergence errors.
What we can try is overriding {{scala.binary.version}} instead; maybe that does the trick...
"	FLINK	Closed	3	11500	11245	pull-request-available
13207663	Do not include hadoop in flink-dist by default	"In order to build a hadoop-free Flink it is currently necessary to activate the {{without-hadoop}} profile.
We should revert this so that flink-dist is hadoop-free by default."	FLINK	Closed	3	7	11245	pull-request-available
13296168	flink-runtime tests are crashing the JVM on Java11 because of PowerMock	"Nightly travis run: https://travis-ci.org/github/apache/flink/jobs/670686286?utm_medium=notification&utm_source=slack

{code}
22:11:49.063 [INFO] Reactor Summary:
22:11:49.063 [INFO] 
22:11:49.068 [INFO] flink-annotations .................................. SUCCESS [  4.733 s]
22:11:49.069 [INFO] flink-metrics ...................................... SUCCESS [  0.250 s]
22:11:49.069 [INFO] flink-metrics-core ................................. SUCCESS [  3.012 s]
22:11:49.069 [INFO] flink-core ......................................... SUCCESS [01:34 min]
22:11:49.069 [INFO] flink-java ......................................... SUCCESS [ 30.494 s]
22:11:49.074 [INFO] flink-runtime ...................................... FAILURE [25:01 min]
22:11:49.074 [INFO] flink-scala ........................................ SKIPPED
22:11:49.074 [INFO] flink-optimizer .................................... SKIPPED
22:11:49.074 [INFO] flink-clients ...................................... SKIPPED
22:11:49.074 [INFO] flink-streaming-java ............................... SKIPPED
22:11:49.074 [INFO] flink-test-utils ................................... SKIPPED
22:11:49.074 [INFO] flink-runtime-web .................................. SKIPPED
22:11:49.074 [INFO] flink-statebackend-rocksdb ......................... SKIPPED
22:11:49.074 [INFO] flink-streaming-scala .............................. SKIPPED
22:11:49.074 [INFO] flink-scala-shell .................................. SKIPPED
22:11:49.074 [INFO] ------------------------------------------------------------------------
22:11:49.074 [INFO] BUILD FAILURE
22:11:49.074 [INFO] ------------------------------------------------------------------------
22:11:49.074 [INFO] Total time: 27:20 min
22:11:49.077 [INFO] Finished at: 2020-04-03T22:11:49+00:00
22:11:49.355 [INFO] Final Memory: 97M/330M
22:11:49.355 [INFO] ------------------------------------------------------------------------
22:11:49.361 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-runtime_2.11: There are test failures.
22:11:49.362 [ERROR] 
22:11:49.362 [ERROR] Please refer to /home/travis/build/apache/flink/flink-runtime/target/surefire-reports for the individual test results.
22:11:49.362 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
22:11:49.362 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.362 [ERROR] Error occurred in starting fork, check output in log
22:11:49.362 [ERROR] Process Exit Code: 239
22:11:49.362 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.362 [ERROR] Error occurred in starting fork, check output in log
22:11:49.362 [ERROR] Process Exit Code: 239
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
22:11:49.362 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
22:11:49.363 [ERROR] at java.base/java.lang.reflect.Method.invoke(Method.java:566)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
22:11:49.363 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.363 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.363 [ERROR] Error occurred in starting fork, check output in log
22:11:49.363 [ERROR] Process Exit Code: 239
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
22:11:49.363 [ERROR] at java.base/java.lang.Thread.run(Thread.java:834)
22:11:49.363 [ERROR] -> [Help 1]
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:11:49.363 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:11:49.363 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] After correcting the problems, you can resume the build with the command
22:11:49.364 [ERROR]   mvn <goals> -rf :flink-runtime_2.11
{code}"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13437466	Upgrade checkstyle plugin	Newer versions of the checkstyle plugin allow running checkstyle:check without requiring dependency resolution. This allows it to be used in a fresh environment.	FLINK	Closed	3	11500	11245	pull-request-available
13310047	Add human readable summary for configured and derived memory sizes.	"FLIP-49 & FLIP-116 introduces sophisticated memory configurations for TaskManager and Master processes. Before the JVM processes are started, Flink derives the accurate sizes for all necessary components, based on both explicit user configurations and implicit defaults.

To make the configuration results (especially those implicitly derived) clear to users, it would be helpful to print them in the beginning of the process logs. Currently, we only have printed JVM parameters (TM & Master) dynamic memory configurations (TM only). They are incomplete (jvm overhead for both processes and off-heap memory for the master process are not presented) and unfriendly (in bytes).

Therefore, I propose to add a human readable summary at the beginning of process logs.

See also this [PR discussion|https://github.com/apache/flink/pull/11445#discussion_r435721169]."	FLINK	Closed	3	4	11245	pull-request-available
13285048	maven-dependency-plugin not fully compatible with Java 11	The maven-dependency-plugin 3.1.1 is not fully compatible with Java 11; dependency analysis and listing of dependencies is currently failing.	FLINK	Closed	3	4	11245	pull-request-available
13236178	Add JobID argument to TaskExecutorGateway#releasePartitions	"The TM is informed about partitions that should be releases via {{TaskExecutorGateway#releasePartitions(Collection<ResultPartitionID>)}}.
This signature makes it a bit tricky to keep track of which partitions still exist for a given job, as the TM would have to maintain a map (partition -> jobID), in addition to a map (jobID -> partitions)

We could include the JobID as a parameter to simplify this. This doens't interfere with current use-cases."	FLINK	Closed	3	7	11245	pull-request-available
13437579	Remove shared TestingUtils executor	The testing utils provide a shared executor. Sharing such an executor across tests can cause odd behaviors, like unexpected shutdowns or blocking calls effectively halting the executor.	FLINK	Closed	3	11500	11245	pull-request-available
13296746	Implements bulk allocation for physical slots	"SlotProvider should support bulk slot allocation so that we can check to see if the resource requirements of a slot request bulk can be fulfilled at the same time.

The SlotProvider interface should be extended with an bulk slot allocation method which accepts a bulk of slot requests as one of the parameters.

{code:java}
CompletableFuture<Collection<PhysicalSlotRequest.Result>> allocatePhysicalSlots(
  Collection<PhysicalSlotRequest> slotRequests,
  Time timeout);
 
class PhysicalSlotRequest {
  SlotRequestId slotRequestId;
  SlotProfile slotProfile;
  boolean slotWillBeOccupiedIndefinitely;

  class Result {
    SlotRequestId slotRequestId;
    PhysicalSlot physicalSlot;
  }
}
{code}

More details see [FLIP-119#Bulk Slot Allocation|https://cwiki.apache.org/confluence/display/FLINK/FLIP-119+Pipelined+Region+Scheduling#FLIP-119PipelinedRegionScheduling-BulkSlotAllocation]
"	FLINK	Closed	3	7	11355	pull-request-available
13363447	Job is possible to hang when restarting a FINISHED task with POINTWISE BLOCKING consumers	"Job is possible to hang when restarting a FINISHED task with POINTWISE BLOCKING consumers. This is because {{PipelinedRegionSchedulingStrategy#onExecutionStateChange()}} will try to schedule all the consumer tasks/regions of the finished *ExecutionJobVertex*, even though the regions are not the exact consumers of the finished *ExecutionVertex*. In this case, some of the regions can be in non-CREATED state because they are not connected to nor affected by the restarted tasks. However, {{PipelinedRegionSchedulingStrategy#maybeScheduleRegion()}} does not allow to schedule a non-CREATED region and will throw an Exception and breaks the scheduling of all the other regions. One example to show this problem case can be found at [PipelinedRegionSchedulingITCase#testRecoverFromPartitionException |https://github.com/zhuzhurk/flink/commit/1eb036b6566c5cb4958d9957ba84dc78ce62a08c].

To fix the problem, we can add a filter in {{PipelinedRegionSchedulingStrategy#onExecutionStateChange()}} to only trigger the scheduling of regions in CREATED state."	FLINK	Closed	1	1	11355	pull-request-available
13474618	Assign speculative execution attempt with correct CREATED timestamp	Currently, newly created speculative execution attempt is assigned with a wrong CREATED timestamp in SpeculativeScheduler. We need to fix it.	FLINK	Closed	3	1	11355	pull-request-available
13260758	Reset vertices right after they transition to terminated states	"Currently in DefaultScheduler, tasks to restart will remain in terminated state until they are re-scheduled by the SchedulingStrategy.
This behavior may cause 2 problems:
1. Failed/Canceled tasks are possibly not be able to be restarted in lazy scheduling. e.g. The job A1--pipelined-->B1 fails. And only A1 will be re-scheduled on restartTasks() since the inputs of B1 are not ready. B1 should be scheduled later on the partition consumable event from restarted A1. But the terminal state of B1 will prevent B1 from being scheduled.
2. Keeping a task in FAILED/CANCELED state for a long time can happen if it takes a long time for its inputs to become ready again. This is also not friendly to users, which may cause confusions.

That's why I'd propose to reset vertices right after they transition to terminated states.
"	FLINK	Closed	3	7	11355	pull-request-available
13256316	Enable TimeUtils to parse all time units labels supported by scala Duration	"Currently we are using scala Duration to parse duration configs. 
The supported time unit labels are
{
DAYS -> ""d day"",
HOURS -> ""h hour"",
MINUTES -> ""min minute"",
SECONDS -> ""s sec second"",
MILLISECONDS -> ""ms milli millisecond"",
MICROSECONDS -> ""µs micro microsecond"",
NANOSECONDS -> ""ns nano nanosecond""
}

We want to use Flink {{TimeUtils}} to parse the duration configuration, as a step to let flink core get rid of scala dependencies. 
In order not to break existing jobs, {{TimeUtils}} must be able to parse all time unit labels supported by scala Duration.

Current TimeUtils supported time unit labels are ""h"", ""min"", ""s"" and ""ms"".
We need to enrich it.
"	FLINK	Resolved	3	7	11355	pull-request-available
13268515	Replace Java Streams with for-loops in vertex input checking	"Vertex input checking is invoked in lazily triggered scheduling by a FINISHED vertex state update RPC or a {{scheduleOrUpdateConsumers}} RPC. 
Java Streams is used in it, but it should be avoided since it is performance critical code. See ref [1] and [2].
We should refactor these Java Streams to improve the performance, for both legacy scheduler and NG schedulers.

cc [~sewen] [~gjy]


[1] [flink code style guide|https://flink.apache.org/contributing/code-style-and-quality-java.html]
[2] [discussion & performance test|https://issues.apache.org/jira/browse/FLINK-14735?focusedCommentId=16974209&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16974209] which compares the performance."	FLINK	Closed	3	4	11355	pull-request-available
13261514	Change DefaultSchedulingResultPartition to return correct partition state	"Currently {{DefaultSchedulingResultPartition#getState()}} returns the state of partitions based on the partition producer's state. The state is used to make scheduling decision.

However, it does not correctly reflect the true state of a partition.
For example, when producer task turns to RUNNING but has not produced any data yet, it's consumers should not be scheduled to reduce unnecessary resource cost in lazy scheduling mode. However, the partition state will be RUNNING in {{DefaultSchedulingResultPartition}} and will trigger the scheduling of its consumers. This may lead to some vertices scheduled earlier than expected with no data to consume, which means a waste of resources.

I'd propose to change the enums in {{ResultPartitionState}} to be:
* CREATED // partition is just created or is just reset
* CONSUMABLE // pipelined partition has data produced or blocking partition's parent result finishes. Corresponds to IntermediateResultPartition#isConsumable.

The CONSUMABLE state is what the scheduler really cares to make scheduling decisions.
The DefaultResultPartition should be fixed to return {{ResultPartitionState}} based the true partition state, rather than the vertex state."	FLINK	Closed	3	7	11355	pull-request-available
13472264	Cancel pending slot allocation after canceling executions	Canceling pending slot allocation before canceling executions will result in execution failures  and pollute the logs. It will also result in an execution to be FAILED even if the execution vertex has FINISHED, which breaks the assumption of SpeculativeScheduler#isExecutionVertexPossibleToFinish().	FLINK	Closed	3	7	11355	pull-request-available
13338374	Remove legacy scheduling in ExecutionGraph components	"This is one step towards making ExecutionGraph a pure data structure.
Note that this task mainly targets to remove the legacy codes of scheduling and failover. Codes of Execution state transition and task deployment will be factored out in follow-up tasks."	FLINK	Closed	3	7	11355	pull-request-available
13296748	Allocates slots in bulks on pipelined region scheduling	"The DefaultExecutionSlotAllocator should allocate slot in bulks, that means the bulks of slot requests will be sent together and fail if any of the request fails.

Note this is a first step to fully functional bulk slot allocation. Current limitations would be:
1. Slot sharing will be ignored
2. Co-location constraints are not allowed
3. intra-bulk input location preferences will be ignored"	FLINK	Closed	3	7	11355	pull-request-available
13264699	Make clear the way to aggregate specified cpuCores resources	"I'm raising this question because I find {{cpuCores}} in {{ResourceSpec#merge}} are aggregated with {{max()}}, while in {{ResourceProfile#merge}} it is {{sum()}}.

This means that when calculating resources of a vertex from within operators, the {{cpuCores}} is the max value. While it is a sum(or subtraction in {{ResourceProfile#subtract}}) when dealing with shared slot bookkeeping and related checks. 
This is confusing to me, especially when I'm considering how to generate a shared slot resource spec merged from all vertices in it(see FLINK-14314).

I'm not pretty sure if we already have a concise definition for it?
If there is, we need to respect it and change {{ResourceSpec}} or {{ResourceProfile}} accordingly.
If not, we need to decide it first before we can move on with fine grained resources.

Need to mention that if we take the {{max()}} way, we need to re-consider how we can conduct a correct {{ResourceProfile#subtract}} regarding {{cpuCores}}, since there is not a clear way to reverse a {{max()}} computation.

cc [~trohrmann] [~xintongsong] [~azagrebin]
"	FLINK	Resolved	3	7	11355	pull-request-available
13263084	Keep only one execution topology in scheduler	"Currently there are 3 failover topology instances created, 2 permanently kept in JM. 2 scheduling topology instances created permanently kept in JM. It a waste of computation to build the topologies and memory to keep these topologies. Which may be a significant issue when the job scale is large.

With FLINK-14450 and FLINK-14451, the SchedulingTopology and FailoverTopology are able to share one default implementation. 
We can change the scheduler to create and keep only one such an execution topology instance to reduce the cost to build and host execution topologies.

More details see FLINK-14330 and the [design doc|https://docs.google.com/document/d/1f88luAOfUQ6Pm4JkxYexLXpfH-crcXJdbubi1pS2Y5A/edit#]."	FLINK	Resolved	3	7	11355	pull-request-available
13451181	Add metrics for speculative execution	"Following two metrics will be added to expose job problems and show the effectiveness of speculative execution:
 # {*}numSlowExecutionVertices{*}: Number of slow execution vertices at the moment.
 # {*}numEffectiveSpeculativeExecutions{*}: Number of speculative executions which finish before their corresponding original executions finish."	FLINK	Closed	3	7	11355	pull-request-available
13275451	Ignores the input locations of a ConsumePartitionGroup if the corresponding ConsumerVertexGroup is too large	"When running TPC-DS jobs in a session cluster, we observed that sometimes tasks are not evenly distributed in TMs. The root cause turned out to be that the downstream tasks tend to be TM or host local with its input tasks. This helps to reduce network shuffle. 
However, in certain cases, like the topology presented in the attached image, jamming the input task's TM and machine with downstream tasks would affect the performance. In this case, respecting input location preferences is causing troubles more than bringing benefits.
So I'm wondering whether we should introduce a config so that users can disable input location preferences?"	FLINK	Closed	3	4	11355	pull-request-available
13242690	Leverage JM side partition state to improve region failover experience	"In current region failover process, most of the input result partition states are unknown. Even though the failure cause is a PartitionException, only one unhealthy partition can be identified.

The may lead to multiple unsuccessful failovers before all the unhealthy but needed partitions are identified and their producers are involved in the failover as well. (unsuccessful failover here means the recovered tasks get failed again soon due to some missing input partitions.)

Using JM side tracked partition states to help the region failover to identify unhealthy(missing) partitions earlier can help with this case.

The basic idea is to build RestartPipelinedRegionStrategy with a ResultPartitionAvailabilityChecker which can query the JM side tracked partition states."	FLINK	Closed	3	7	11355	pull-request-available
13263082	Refactor FailoverTopology to extend base topology	"This task is to change FailoverTopology to extend the base topology 
introduced in FLINK-14330. ExecutionGraphToSchedulingTopologyAdapter(default implementation of SchedulingTopology) should also implements and replace DefaultFailoverTopology.

More details see FLINK-14330 and the [design doc|https://docs.google.com/document/d/1f88luAOfUQ6Pm4JkxYexLXpfH-crcXJdbubi1pS2Y5A/edit#]."	FLINK	Resolved	3	7	11355	pull-request-available
13378702	Scheduler should invoke ShuffleMaster#registerPartitionWithProducer by a real asynchronous fashion	Current scheduler enforces a synchronous registration though the API of ShuffleMaster#registerPartitionWithProducer returns a CompletableFuture. In scenario of remote shuffle service, the talk between ShuffleMaster and remote cluster tends to be expensive. A synchronous registration risks to block main thread potentially and might cause negative side effects like heartbeat timeout. Additionally, expensive synchronous invokes to remote could bottleneck the throughput for applying shuffle resource, especially for batch jobs with complicated DAGs;	FLINK	Closed	3	7	11355	pull-request-available
13457204	Enable to identify whether a job vertex contains source/sink operators	"Speculative execution does not support sources/sinks in the first version. Therefore, it will not create speculation instances for vertices which contains source/sink operators.

Note that a job vertex with no input/output does not mean it is a source/sink vertex. Multi-input sources can have input. And it's possible that the vertex with no output edge does not contain any sink operator. Besides that, a new sink with topology can spread the sink logic into multiple job vertices connected with job edges."	FLINK	Closed	3	7	11355	pull-request-available
13468516	Introduce ExecutionHistory to host historical executions for each execution vertex	"With speculative execution, tracking prior executions in an {{EvictingBoundedList}} does not work. This is because when using {{EvictingBoundedList}} relies on the assumption that the historical executions are added in ascending order of attempt number successively. This is no longer true if speculative execution is enabled. e.g. 3 speculative execution attempts #1, #2, #3 are running concurrently, later #3 failed, and then #1 failed, and execution attempt #2 keeps running.
The broken assumption may result in exceptions in REST, job archiving and so on.
We propose to introduce an {{ExecutionHistory}} to replace {{EvictingBoundedList}}. It hosts the historical executions in a {{LinkedHashMap}} with a size bound. When the map grows beyond the size bound, elements are dropped from the head of the map (FIFO order)."	FLINK	Closed	3	7	11355	pull-request-available
13472017	Speculative execution for InputFormat sources	This task enables InputFormat sources for speculative execution.	FLINK	Closed	3	7	11355	pull-request-available
13280036	Remove legacy scheduler	"This umbrella ticket is to track the tickets to remove the legacy scheduler and related components.
So that we can have a much cleaner scheduler framework which significantly simplifies our next development work on job scheduling.

Here's a rough plan:
1. remove LegacyScheduler class
  * related tests should also be removed, including AlsoRunWithLegacyScheduler which is used to enable integration tests for legacy scheduler

2. remove legacy failover strategy AdaptedRestartPipelinedRegionStrategyNG and RestartIndividualStrategy. These implementations and their tests heavily relies on the legacy scheduling logics in ExecutionGraph so removing them can make #3 much easier.

3. remove legacy scheduling/failover logics in ExecutionGraph
  * note that many tests still relies on these logics(e.g. ExecutionGraph#scheduleForExecution()), we may need to retain and rewrite tests which are still needed. So before removing the legacy logics, it's better to cleanup all the tests relying on the legacy scheduling first, i.e. remove tests that are no needed and rewrite the tests which are still needed.

4. remove legacy FailoverStrategy and RestartStrategy

5. Factoring out the scheduling logics in ExecutionGraph to make it a plain data structure. This can be a long term goal though. There can be several pieces and we may need to examine them one by one."	FLINK	Closed	2	4	11355	Umbrella
13259037	Support global failure handling for DefaultScheduler (SchedulerNG)	"Global failure handling(full restarts) is widely used in ExecutionGraph components and even other components to recover the job from an inconsistent state. 

We need to support it for DefaultScheduler to not break the safety net. More details see [here|https://github.com/apache/flink/pull/9663/files#r326892524].

There can be follow ups of this task to replace usages of full restarts with JVM termination, in cases that are considered as bugs/unexpected to happen.

Implementation plan:
1. Add {{getGlobalFailureHandlingResult(Throwable)}} in {{ExecutionFailureHandler}}
2. Add an interface {{handleGlobalFailure(Throwable)}} in {{SchedulerNG}} and implement it in {{DefaultScheduler}}
3. Add an interface {{notifyGlobalFailure(Throwable)}} in {{InternalTaskFailuresListener}} and rework the implementations to use {{SchedulerNG#handleGlobalFailure}}
4. Rework {{ExecutionGraph#failGlobal}} to invoke {{InternalTaskFailuresListener#notifyGlobalFailure}} for ng scheduler"	FLINK	Closed	3	7	11355	pull-request-available
13245941	Remove the legacy batch fault tolerance page and redirect it to the new task failure recovery page	"The batch fault tolerance page([https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/fault_tolerance.html]) is describing a deprecated way to configure restart strategies.

We should remove it and redirect it to the task failure recovery page ([https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/restart_strategies.html]) for latest description of the fault tolerance configurations, including restart strategies and failover strategies."	FLINK	Closed	4	4	11355	pull-request-available
13338973	All vertices in an DataSet iteration job will be eagerly scheduled	"After switching to pipelined region scheduling, all vertices in an DataSet iteration job will be eagerly scheduled, which means BLOCKING result consumers can be deployed even before the result finishes and resource waste happens. This is because all vertices will be put into one pipelined region if the job contains {{ColocationConstraint}}, see [PipelinedRegionComputeUtil|https://github.com/apache/flink/blob/c0f382f5f0072441ef8933f6993f1c34168004d6/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/failover/flip1/PipelinedRegionComputeUtil.java#L52].

IIUC, this {{makeAllOneRegion()}} behavior was introduced to ensure co-located iteration head and tail to be restarted together in pipelined region failover. However, given that edges within an iteration will always be PIPELINED ([ref|https://github.com/apache/flink/blob/0523ef6451a93da450c6bdf5dd4757c3702f3962/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java#L1188]), co-located iteration head and tail will always be in the same region. So I think we can drop the {{PipelinedRegionComputeUtil#makeAllOneRegion()}} code path and build regions in the the same way no matter if there is co-location constraints or not."	FLINK	Closed	1	1	11355	pull-request-available
13270976	Managed memory fractions should be rounded properly to not summed up to be more than 1.0	"Managed memory fractions should be rounded to floor at a certain precision when divided by the number of operators, otherwise the fractions can be summed up to be more than 1.0 due to the double precision issue, and the last operator may fail to allocate managed memory it is supposed to be able to acquire.

To achieve that, I think we should change {{StreamingJobGraphGenerator#setManagedMemoryFractionForOperator}} to use BigDecimal#divide(otherValue, scale, RoundMode.ROUND_DOWN) to calculate the fractions. In this way, the sum of the fractions will not exceed 1.0.
The scale can be a bit larger (maybe 16) so that we only lose little managed memory.

cc [~chesnay]"	FLINK	Closed	3	7	11355	pull-request-available
13431016	Release Testing: Explicit shutdown signalling from TaskManager to JobManager	"FLINK-25277 introduces explicit signalling between a TaskManager and the JobManager when the TaskManager shuts down. This reduces the time it takes for a reactive cluster to down-scale & restart.

 

*Setup*
 # Add the following line to your flink config to enable reactive mode:
{code}
taskmanager.host: localhost # a workaround
scheduler-mode: reactive
restart-strategy: fixeddelay
restart-strategy.fixed-delay.attempts: 100
{code}
 # Create a “usrlib” folder and place the TopSpeedWindowing jar into it
{code:bash}
$ mkdir usrlib
$ cp examples/streaming/TopSpeedWindowing.jar usrlib/
{code}
 # Start the job 
{code:bash}
$ bin/standalone-job.sh start  --main-class org.apache.flink.streaming.examples.windowing.TopSpeedWindowing
{code}
 # Start three task managers
{code:bash}
$ bin/taskmanager.sh start
$ bin/taskmanager.sh start
$ bin/taskmanager.sh start
{code}
 # Wait for the job to stabilize. The log file should show that three tasks start for every operator.
{code}
 GlobalWindows -> Sink: Print to Std. Out (3/3) (d10339d5755d07f3d9864ed1b2147af2) switched from INITIALIZING to RUNNING.{code}

*Test*

Stop one taskmanager

{code:bash}
$ bin/taskmanager.sh stop
{code}

Success condition: You should see that the job cancels and re-runs after a few seconds. In the logs you should find a line with the text “The TaskExecutor is shutting down”.


*Teardown*

Stop all taskmanagers and the jobmanager:

{code:bash}
$ bin/standalone-job.sh stop
$ bin/taskmanager.sh stop-all
{code}"	FLINK	Resolved	1	4	11355	release-testing
13296744	Integrate pipelined region scheduling	"This task is to integrate PipelinedRegionSchedulingStrategy with DefaultScheduler. A config option ""jobmanager.scheduler.scheduling-strategy"" should be introduced to control whether to use the new ""region"" scheduling or to use the ""legacy"" eager/lazy-from-sources scheduling."	FLINK	Closed	3	7	11355	pull-request-available
13267495	Introduce full restarts failover strategy for NG scheduler	"This strategy allows users to force all the tasks to be restarted if any task fails.
It may help if it's non-sense or even worse if only part of the tasks can acquire resources and process data. In such cases, checkpoints cannot be completed and progress cannot be persisted.

Supporting this strategy also enables the NG scheduler to be backward compatible for existing failover strategy configuration."	FLINK	Closed	3	7	11355	pull-request-available
13297034	Simplify SchedulingStrategy#onPartitionConsumable(...) parameters	"I'd propose to simplify SchedulingStrategy#onPartitionConsumable(...) parameters as below:
1. take IntermediateResultPartitionID instead of ResultPartitionID
    ResultPartitionID is a composition of IntermediateResultPartitionID and ExecutionAttemptID. SchedulingStrategy is not aware of ExecutionAttemptID so there is no need to expose it.
2. drop the executionVertexId param. executionVertexId does not provide extra information. The check in LazyFromSourcesSchedulingStrategy does not make much sense since the executionVertexId is just retrieved by the partitionId in an earlier stage. It makes things more complex since a blocking result partition can become consumable when a vertex who is not its producer finishes.

This simplification also eases the work of FLINK-14234."	FLINK	Closed	3	7	11355	pull-request-available
13258205	Add a metric to show failover count regarding fine grained recovery	"Previously Flink uses restart all strategy to recover jobs from failures. And the metric ""fullRestart"" is used to show the count of failovers.

However, with fine grained recovery introduced in 1.9.0, the ""fullRestart"" metric only reveals how many times the entire graph has been restarted, not including the number of fine grained failure recoveries.

As many users want to build their job alerting based on failovers, I'd propose to add such a new metric {{numberOfRestarts}} which also respects fine grained recoveries. The metric should be a Gauge."	FLINK	Closed	3	7	11355	pull-request-available
13261720	Annotate MiniCluster tests in core modules with AlsoRunWithSchedulerNG	"This task is to annotate MiniCluster tests with AlsoRunWithSchedulerNG in flink core modules, so that we can know breaking changes in time when further improving the new generation scheduler.

Core modules are the basic flink modules as defined in {{MODULES_CORE}} in flink/travis/stage.sh.

MODULES_CORE=""\
flink-annotations,\
flink-test-utils-parent/flink-test-utils,\
flink-state-backends/flink-statebackend-rocksdb,\
flink-clients,\
flink-core,\
flink-java,\
flink-optimizer,\
flink-runtime,\
flink-runtime-web,\
flink-scala,\
flink-streaming-java,\
flink-streaming-scala,\
flink-metrics,\
flink-metrics/flink-metrics-core""

Note that the test bases in flink-test-utils will not be annotated in this task, since it enables MiniCluster tests in flink-tests and other non-core modules."	FLINK	Resolved	3	7	11355	pull-request-available
13299928	Refactor the ExecutionAttemptID to consist of ExecutionVertexID and attemptNumber	Make the ExecutionAttemptID being composed of (ExecutionVertexID, attemptNumber).	FLINK	Closed	3	7	11355	pull-request-available
13279300	Enable batch scheduling tests in LegacySchedulerBatchSchedulingTest for DefaultScheduler as well	"{{testSchedulingOfJobWithFewerSlotsThanParallelism}} is a common case but it is only tested with legacy scheduler in {{LegacySchedulerBatchSchedulingTest}} at the moment.
We should enable it for DefaultScheduler as well. 
This also allows us to safely remove {{LegacySchedulerBatchSchedulingTest}} when we are removing the LegacyScheduler and related components without loosing test coverage."	FLINK	Closed	3	7	11355	pull-request-available
13256245	Introduce option allVerticesInSameSlotSharingGroupByDefault in ExecutionConfig	"* Introduce option -{{allSourcesInSamePipelinedRegion}}- {{allVerticesInSameSlotSharingGroupByDefault}} in {{ExecutionConfig}}
 * Set it to {{true}} by default
 * Set it to {{false}} for SQL/Table API bounded batch jobs by the Blink planner

This step should not introduce any behavior changes. "	FLINK	Closed	3	7	11355	pull-request-available
13261721	Annotate MiniCluster tests in flink-tests with AlsoRunWithSchedulerNG	"This task is to annotate all MiniCluster tests with AlsoRunWithSchedulerNG in flink-tests, so that we can know breaking changes in time when further improving the new generation scheduler.

We should also guarantee the annotated tests to pass, either by fixing failed tests, or not annotating a failed test and opening a ticket to track it.
The tickets for failed tests should be linked in this task."	FLINK	Resolved	3	7	11355	pull-request-available
13256244	FLIP-53 Fine-grained Operator Resource Management	This is the umbrella issue of 'FLIP-53: Fine Grained Operator Resource Management'.	FLINK	Closed	3	2	11355	Umbrella
13252626	The Document about Cluster on yarn have some problems	"Read the flink 1.9 documentation, YARN Setup section, there have some issues with download the installation package. There is no flink-1.10-SNAPSHOT-bin-hadoop2.tgz package in the https://flink.apache.org/downloads.html download page.

!image-2019-08-23-17-48-22-123.png!"	FLINK	Closed	3	1	11355	documentation, wish
13346347	Canceling a job when it is failing will result in job hanging in CANCELING state	If user manually cancels a job when the job is failing(here failing means the job encounters unrecoverable failure and is about to fail),  the job will hang in CANCELING state and cannot terminate. The cause is that DefaultScheduler currently will always try to transition from `FAILING` to `FAILED` to terminate the job. However, job canceling will change job status to `CANCELING` so that the transition to `FAILED` will not success.	FLINK	Closed	2	1	11355	pull-request-available
13256248	Set managed memory fractions according to slot sharing groups	"* For operators with specified {{ResourceSpecs}}, calculate fractions according to operators {{ResourceSpecs}}
 * For operators with unknown {{ResourceSpecs}}, calculate fractions according to number of operators -using managed memory-

This step should not introduce any behavior changes."	FLINK	Closed	3	7	11355	pull-request-available
13266305	Move allVerticesInSameSlotSharingGroupByDefault from ExecutionConfig to StreamGraph	"allVerticesInSameSlotSharingGroupByDefault is currently for internal use only.
It's better to not add it in ExecutionConfig which is for user configurations.
More details see discussion [this|https://issues.apache.org/jira/browse/FLINK-14059?focusedCommentId=16967392&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16967392] and [this|https://github.com/apache/flink/pull/10007#discussion_r342481078].

I'd propose to move it to StreamGraph."	FLINK	Closed	1	7	11355	pull-request-available
13337144	Job may try to leave SUSPENDED state in ExecutionGraph#failJob()	"{{SUSPENDED}} is a terminal state which a job is not supposed to leave this state once entering. However, {{ExecutionGraph#failJob()}} did not check it and may try to transition a job out from {{SUSPENDED}} state. This will cause unexpected errors and may lead to JM crash.
The problem can be visible if we rework {{ExecutionGraphSuspendTest}} to be based on {{DefaultScheduler}}.
We should harden the check in {{ExecutionGraph#failJob()}}."	FLINK	Closed	3	1	11355	pull-request-available
13267897	Introduce ResourceProfile builder to enable flexible building	"The ResourceProfile constructors may accept values in raw types (double/int) or advanced types(CPUResource/MemorySize). Thus it had to maintain different kinds of constructors for the combinations. Or one may need to build advanced values types from raw types to use construct ResourceProfile.

I'd propose to introduce a builder to enable flexible building of ResourceProfile. It allows setting a resource with advanced value types as well as raw value types.

The builder will also make it easier to see what resource is set."	FLINK	Closed	3	4	11355	pull-request-available
13451140	Introduce SpeculativeExecutionVertex	SpeculativeExecutionVertex will be used if speculative execution is enabled, as a replacement of ExecutionVertex to form an ExecutionGraph. The core difference is that a SpeculativeExecutionVertex can have multiple current executions running at the same time.	FLINK	Closed	3	7	11355	pull-request-available
13472018	Speculative execution for new sources	This task enables new sources(FLIP-27) for speculative execution.	FLINK	Closed	3	7	11355	pull-request-available
13255825	Enable NG scheduler testing in per-commit tests	"Before making {{SchedulerNG}} the default scheduler, we need to guarantee all the {{MiniCluster}} tests can pass with it. 
FLINK-13953 introduces a way to run existing {{MiniCluster}} tests against {{SchedulerNG}}.

I'd propose to enable NG scheduler testing in per-commit tests, given that the travis currently have capacity to run per-commit tests for scheduler NG.
"	FLINK	Closed	3	7	11355	pull-request-available
13472251	Enable speculative execution of sources	Currently speculative execution of sources is disabled. It can be enabled with the improvement done to support InputFormat sources and new sources to work correctly with speculative execution.	FLINK	Closed	3	7	11355	pull-request-available
13231734	Implement ExecutionFailureHandler	"Implement ExecutionFailureHandler and related classes.

*Acceptance criteria*
* Implementation or definitions exist for:
** {{ExecutionFailureHandler}}
** {{FailureHandlingResult}}
** {{RestartBackoffTimeStrategy}} (interface definition is enough)
* {{ExecutionFailureHandler}} is unit tested in isolation
* {{FailureHandlingResult}} is unit tested in isolation
"	FLINK	Closed	3	7	11355	pull-request-available
13259215	Use NoResourceAvailableException to wrap TimeoutException on slot allocation (Scheduler NG) 	"This makes the error to be more user friendly.
It also helps {{MiniClusterITCases}} to pass."	FLINK	Closed	3	7	11355	pull-request-available
13305384	Remove RestartIndividualStrategy	"It was used by the legacy scheduler and is not used anymore.
Removing it can ease the work to further remove the legacy scheduling logics in ExecutionGraph."	FLINK	Closed	3	7	11355	pull-request-available
13275390	JobManager crashes in the standalone model when cancelling job which subtask' status is scheduled	"Use start-cluster.sh to start a standalone cluster, and then submit a job from the streaming's example which name is TopSpeedWindowing, parallelism is 20. Wait for one minute, cancel the job, jobmanager will crash. The exception stack is:

{noformat}
2019-12-19 10:12:11,060 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL: Thread 'flink-akka.actor.default-dispatcher-2' produced an uncaught exception. Stopping the process...2019-12-19 10:12:11,060 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL: Thread 'flink-akka.actor.default-dispatcher-2' produced an uncaught exception. Stopping the process...java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at org.apache.flink.runtime.scheduler.DefaultScheduler.propagateIfNonNull(DefaultScheduler.java:387) at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$deployAll$4(DefaultScheduler.java:372) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:705) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:170) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFulfillSlotRequestOrMakeAvailable(SlotPoolImpl.java:534) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseSingleSlot(SlotPoolImpl.java:479) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseSlot(SlotPoolImpl.java:390) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:557) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.releaseChild(SlotSharingManager.java:607) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.access$700(SlotSharingManager.java:352) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:716) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.releaseSharedSlot(SchedulerImpl.java:552) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.cancelSlotRequest(SchedulerImpl.java:184) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.returnLogicalSlot(SchedulerImpl.java:195) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:181) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:778) at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2140) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:178) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:125) at org.apache.flink.runtime.executiongraph.Execution.releaseAssignedResource(Execution.java:1451) at org.apache.flink.runtime.executiongraph.Execution.finishCancellation(Execution.java:1170) at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1150) at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1129) at org.apache.flink.runtime.executiongraph.Execution.cancelAtomically(Execution.java:1111) at org.apache.flink.runtime.executiongraph.Execution.cancel(Execution.java:804) at org.apache.flink.runtime.executiongraph.ExecutionVertex.cancel(ExecutionVertex.java:729) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.mapExecutionVertices(ExecutionJobVertex.java:505) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.cancelWithFuture(ExecutionJobVertex.java:494) at org.apache.flink.runtime.executiongraph.ExecutionGraph.cancelVerticesAsync(ExecutionGraph.java:952) at org.apache.flink.runtime.executiongraph.ExecutionGraph.cancel(ExecutionGraph.java:903) at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:432) at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:364) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:824) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ... 69 moreCaused by: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at org.apache.flink.runtime.executiongraph.ExecutionVertex.tryAssignResource(ExecutionVertex.java:701) at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$5(DefaultScheduler.java:409) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ... 70 more2019-12-19 10:12:11,066 INFO  org.apache.flink.runtime.blob.BlobServer                      - Stopped BLOB server at 0.0.0.0:54944
{noformat}"	FLINK	Resolved	1	1	11355	pull-request-available
13334948	Execution graph related tests are possibly broken due to registering duplicated ExecutionAttemptID	"Since FLINK-17295, many tests encounters unexpected global failure due to registering duplicated ExecutionAttemptID. Although these tests do not appear to be broken yet, they are potentially broken/unstable. And it further blocks to rework these tests to be based on the new scheduer (FLINK-17760). 

Below is a sample error which happens in ExecutionTest#testAllPreferredLocationCalculation():

{code:java}
2194 [main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job (unnamed job) (a22afb832b5f94b075d7ffb32fbc9023) switched from state CREATED to FAILING.
java.lang.Exception: Trying to register execution Attempt #0 (TestVertex (1/1)) @ (unassigned) - [CREATED] for already used ID a22afb832b5f94b075d7ffb32fbc9023_146968a4de2df0b2fef1e4b2e8297993_0_0
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.registerExecution(ExecutionGraph.java:1621) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.<init>(ExecutionVertex.java:181) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:211) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:139) [classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:448) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:419) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:411) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecutionJobVertex(ExecutionGraphTestUtils.java:452) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.getExecution(ExecutionGraphTestUtils.java:477) [test-classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionTest.testAllPreferredLocationCalculation(ExecutionTest.java:298) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_261]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_261]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_261]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_261]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12]
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) [junit-4.12.jar:4.12]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) [junit-4.12.jar:4.12]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) [junit-4.12.jar:4.12]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) [junit-4.12.jar:4.12]
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) [junit-4.12.jar:4.12]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363) [junit-4.12.jar:4.12]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) [junit-4.12.jar:4.12]
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) [junit-rt.jar:?]
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) [junit-rt.jar:?]
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220) [junit-rt.jar:?]
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53) [junit-rt.jar:?]

{code}

This is because these tests improperly creates Execution/ExecutionVertex/ExecutionJobVertex from an existing ExecutionGraph which already contains the Execution/ExecutionVertex/ExecutionJobVertex. FLINK-17295 reveals this problem because it makes the attemptID no longer random. 

Below is an example of improper ExecutionJobVertex creation in ExecutionGraphTestUtils#getExecutionJobVertex():

{code:java}

		JobGraph jobGraph = new JobGraph(ajv);
		jobGraph.setScheduleMode(scheduleMode);

		ExecutionGraph graph = TestingExecutionGraphBuilder
			.newBuilder()
			.setJobGraph(jobGraph)
			.setIoExecutor(executor)
			.setFutureExecutor(executor)
			.build();

		graph.start(ComponentMainThreadExecutorServiceAdapter.forMainThread());

		return new ExecutionJobVertex(graph, ajv, 1, AkkaUtils.getDefaultTimeout());
{code}

We should get rid of such improper usages. 
Therefore, I would like to change these tests to get existing Execution/ExecutionVertex/ExecutionJobVertex from the generated ExecutionGraph, instead of invoking their constructors to create new ones."	FLINK	Closed	2	1	11355	pull-request-available
13269902	Let tasks in a batch get scheduled in topological order and subtaskIndex ascending pattern	"I'd propose to let tasks in a batch get scheduled in topological order and subtaskIndex ascending pattern. 
There can be 2 benefits:
1. there would be less chance for a task to get launched before its upstream tasks, which reduces {{requestPartitionState}} RPCs to JobMaster.
2. logs could be more readable, e.g.
ordered:
Source: source (1/20) ... switched from CREATED to SCHEDULED.
Source: source (2/20) ... switched from CREATED to SCHEDULED.
...
Source: source (20/20) ... switched from CREATED to SCHEDULED.
Flat Map (1/20) ... switched from CREATED to SCHEDULED.
...
Flat Map (20/20) ... switched from CREATED to SCHEDULED.

disordered:
Source: source (1/20) ... switched from CREATED to SCHEDULED.
Flat Map (11/20) ... switched from CREATED to SCHEDULED.
Source: source (19/20) ... switched from CREATED to SCHEDULED.
Flat Map (2/20) ... switched from CREATED to SCHEDULED.
...

The detailed proposal is:
1. change scheduling related methods to take and return tasks as {{List}} instead of {{Collection}} in {{DefaultScheduler}} and related classes
2. sort the tasks received in {{DefaultScheduler#allocateSlotsAndDeploy}}  to be topological sorted (primary) and subtaskIndex ascending (secondary) order before scheduling them. The tasks scheduled by {{EagerSchedulingStrategy}} can be in order with this change.
3. Change {{LazyFromSourcesSchedulingStrategy}} to schedule tasks in the original order it receives the tasks, which is usually in the desired order. We do this because in FLINK-14162 we may invoke #allocateSlotsAndDeploy on each vertex individually in this scheduling strategy, so that the ordering in {{DefaultScheduler}} would not work.
Note that it's just best effort since we always receives a Set of tasks in #restartTasks. But it should be Ok since the disordering does not result in more {{requestPartitionState}} RPCs with this scheduling strategy, and batch jobs are usually in small regions so that the log disordering is not that obvious. "	FLINK	Closed	3	7	11355	pull-request-available
13451131	Rework DefaultScheduler to directly deploy executions	"Currently, the DefaultScheduler(base of AdaptiveBatchScheduler) can only perform ExecutionVertex level deployment. However, in this case, the scheduler is actually deploying the current execution attempt of the ExecutionVertex.

Therefore, we need to rework the DefaultScheduler to directly deploy executions."	FLINK	Closed	3	7	11355	pull-request-available
13342031	LazyFromSourcesSchedulingStrategy is possible to schedule non-CREATED vertices	"LazyFromSourcesSchedulingStrategy is possible to schedule vertices which are not in CREATED state. This will lead result in unexpected check failure and result in fatal error (see attached error).

The reason is that the status of a vertex to schedule was changed in LazyFromSourcesSchedulingStrategy#allocateSlotsAndDeployExecutionVertices() during the invocation of schedulerOperations.allocateSlotsAndDeploy(...) on other vertices.

e.g. ev1 and ev2 are in the same pipelined region and are restarted one by one in the scheduling loop in LazyFromSourcesSchedulingStrategy#allocateSlotsAndDeployExecutionVertices(). They are all CREATED at the moment. ev1 is scheduled first but it immediately fails due to some slot allocation error and ev2 will be canceled as a result. So when ev2 is scheduled, its state would be CANCELED and the state check failed.

More details see FLINK-20220.

{code:java}
2020-11-19 13:34:17,231 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.lang.IllegalStateException: expected vertex aafcbb93173905cec9672e46932d7790_3 to be in CREATED state, was: CANCELED
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:708) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:687) ~[?:1.8.0_222]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) ~[?:1.8.0_222]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11.2.jar:1.11.2]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11.2.jar:1.11.2]
Caused by: java.lang.IllegalStateException: expected vertex aafcbb93173905cec9672e46932d7790_3 to be in CREATED state, was: CANCELED
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:217) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$validateDeploymentOptions$3(DefaultScheduler.java:326) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_222]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_222]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_222]
        at java.util.Collections$2.tryAdvance(Collections.java:4719) ~[?:1.8.0_222]
        at java.util.Collections$2.forEachRemaining(Collections.java:4727) ~[?:1.8.0_222]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_222]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_222]
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_222]
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_222]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_222]
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_222]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.validateDeploymentOptions(DefaultScheduler.java:326) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:297) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.strategy.LazyFromSourcesSchedulingStrategy.allocateSlotsAndDeployExecutionVertices(LazyFromSourcesSchedulingStrategy.java:140) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.strategy.LazyFromSourcesSchedulingStrategy.restartTasks(LazyFromSourcesSchedulingStrategy.java:93) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$restartTasks$2(DefaultScheduler.java:265) ~[flink-dist_2.11-1.11.2.jar:1.11.2]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_222]
        ... 24 more
{code}
"	FLINK	Closed	1	1	11355	pull-request-available
13269984	SchedulingStrategyFactory#createInstance might not need to know JobGraph	[~zhuzh] [~GJL] I just notice that {{SchedulingStrategyFactory#createInstance}} take a parameter {{JobGraph}} but neither the parameter is in use nor this class/method should know {{JobGraph}}. Could you explain why we need it or we can safely remove the parameter so that we get rid of confusing parameter?	FLINK	Closed	3	4	11355	pull-request-available
13522075	SortAggITCase.testLeadLag failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45389&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12560

{code}
Jan 30 11:03:32 [ERROR] Tests run: 72, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.42 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase
Jan 30 11:03:32 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.testLeadLag  Time elapsed: 0.547 s  <<< FAILURE!
Jan 30 11:03:32 java.lang.AssertionError: 
Jan 30 11:03:32 
Jan 30 11:03:32 Results do not match for query:
Jan 30 11:03:32   
Jan 30 11:03:32 SELECT
Jan 30 11:03:32   a,
Jan 30 11:03:32   b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
Jan 30 11:03:32   c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
Jan 30 11:03:32   d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
Jan 30 11:03:32   e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
Jan 30 11:03:32   f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
Jan 30 11:03:32   g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
Jan 30 11:03:32   h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
Jan 30 11:03:32   i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
Jan 30 11:03:32   j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
Jan 30 11:03:32   k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
Jan 30 11:03:32   l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
Jan 30 11:03:32   m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
Jan 30 11:03:32   n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag
Jan 30 11:03:32 
Jan 30 11:03:32 FROM UnnamedTable$230
Jan 30 11:03:32 order by a
Jan 30 11:03:32 
Jan 30 11:03:32 
Jan 30 11:03:32 Results
Jan 30 11:03:32  == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
Jan 30 11:03:32  +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]
Jan 30 11:03:32  +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]
Jan 30 11:03:32 !+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null]
Jan 30 11:03:32         
Jan 30 11:03:32 Plan:
Jan 30 11:03:32   == Abstract Syntax Tree ==
Jan 30 11:03:32 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
Jan 30 11:03:32 +- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
Jan 30 11:03:32    +- LogicalUnion(all=[true])
[...]
{code}"	FLINK	Closed	2	1	11355	pull-request-available, test-stability
13395438	StackOverflowException can happen if a large scale job failed to acquire enough slots in time	"When requested slots are not fulfilled in time, task failure will be triggered and all related tasks will be canceled and restarted. However, in this process, if a task is already assigned a slot, the slot will be returned to the slot pool and it will be immediately used to fulfill pending slot requests of the tasks which will soon be canceled. The execution version of those tasks are already bumped in {{DefaultScheduler#restartTasksWithDelay(...)}} so that the assignment will fail immediately and the slot will be returned to the slot pool and again used to fulfill pending slot requests. StackOverflow can happen in this way when there are many vertices, and fatal error can happen and lead to JM crash. A sample call stack is attached below.

To fix the problem, one way is to cancel the pending requests of all the tasks which will be canceled soon(i.e. tasks with version bumped) before canceling these tasks.

{panel}
...
        at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.cancelSlotRequest(PhysicalSlotProviderImpl.java:112) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.releaseSharedSlot(SlotSharingExecutionSlotAllocator.java:242) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.releaseExternally(SharedSlot.java:281) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.removeLogicalSlotRequest(SharedSlot.java:242) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.returnLogicalSlot(SharedSlot.java:234) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:203) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:200) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:130) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.releaseSlotIfPresent(DefaultScheduler.java:542) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$8(DefaultScheduler.java:505) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.fulfill(DeclarativeSlotPoolBridge.java:552) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequestSlotMatching.fulfillPendingRequest(DeclarativeSlotPoolBridge.java:587) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.newSlotsAreAvailable(DeclarativeSlotPoolBridge.java:171) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.lambda$freeReservedSlot$0(DefaultDeclarativeSlotPool.java:316) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.Optional.ifPresent(Optional.java:159) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeReservedSlot(DefaultDeclarativeSlotPool.java:313) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseSlot(DeclarativeSlotPoolBridge.java:335) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.cancelSlotRequest(PhysicalSlotProviderImpl.java:112) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
...

{panel}
"	FLINK	Closed	2	1	11355	pull-request-available
13303105	Unify slot request timeout handling for streaming and batch tasks	"There are 2 different slot request timeout handling mechanism for batch and streaming tasks.
For streaming tasks, the slot request will fail if it is not fulfilled within slotRequestTimeout.
For batch tasks, the slot request will be checked periodically to see whether it is fulfillable, and only fails if it has been unfulfillable for a certain period(slotRequestTimeout).

With slot marked with whether they will be occupied indefinitely, we can unify these handling. See [FLIP-119|https://cwiki.apache.org/confluence/display/FLINK/FLIP-119+Pipelined+Region+Scheduling#FLIP-119PipelinedRegionScheduling-ExtendedSlotProviderInterface] for more details."	FLINK	Closed	3	7	11355	pull-request-available
13278193	Misleading root cause exception when cancelling the job	"When cancelling a Flink job, the following stack trace gets displayed

{code}
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: d0e8c2026709385166bcc0253c30742e)
        at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:262)
        at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:338)
        at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:60)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1507)
        at org.apache.flink.streaming.examples.statemachine.StateMachineExample.main(StateMachineExample.java:142)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:576)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:438)
        at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:274)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:746)
        at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:273)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:205)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1010)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1083)
        at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1083)
Caused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.
        at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:148)
        at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:259)
        ... 18 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: No pooled slot available and request to ResourceManager for new slot failed
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.slotRequestToResourceManagerFailed(SlotPoolImpl.java:357)
        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.lambda$requestSlotFromResourceManager$1(SlotPoolImpl.java:345)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not fulfill slot request 7ab196daeb73e353c460455899a7622f.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:871)
        at akka.dispatch.OnComplete.internal(Future.scala:263)
        at akka.dispatch.OnComplete.internal(Future.scala:261)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
        at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
        at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
        at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
        at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
        at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
        at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
        at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
        at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
        at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
        ... 4 more
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not fulfill slot request 7ab196daeb73e353c460455899a7622f.
        at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.registerSlotRequest(SlotManagerImpl.java:315)
        at org.apache.flink.runtime.resourcemanager.ResourceManager.requestSlot(ResourceManager.java:443)
        at sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        ... 4 more
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.UnfulfillableSlotRequestException: Could not fulfill slot request 7ab196daeb73e353c460455899a7622f. Requested resource profile (ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1}) is unfulfillable.
        at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.internalRequestSlot(SlotManagerImpl.java:768)
        at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl.registerSlotRequest(SlotManagerImpl.java:310)
        ... 26 more
{code}

The reported root cause is {{UnfulfillableSlotRequestException}} which is a bit misleading. If the user cancels a running job then the root cause should be the cancellation."	FLINK	Closed	2	1	11355	pull-request-available
13267114	Enable AbstractTaskManagerProcessFailureRecoveryTest to pass with new DefaultScheduler	Investigate the reason {{testTaskManagerProcessFailure()}} fails, and fix issues.	FLINK	Closed	3	7	11355	pull-request-available
13259214	Annotate all MiniCluster tests in flink-runtime with AlsoRunWithSchedulerNG	"This task is to annotate all MiniCluster tests with AlsoRunWithSchedulerNG in flink-runtime, so that we can know breaking changes in time when further improving the new generation scheduler. 

We should also guarantee the annotated tests to pass, either by fixing failed tests, or not annotating a failed test and opening a ticket to track it.
"	FLINK	Closed	3	7	11355	pull-request-available
13470071	RemoveCachedShuffleDescriptorTest#testRemoveOffloadedCacheForPointwiseEdgeAfterFailover causes fatal error on CI	"{code:java}
Jul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in log
Jul 05 03:30:03 [ERROR] Process Exit Code: 239
Jul 05 03:30:03 [ERROR] Crashed tests:
Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest
Jul 05 03:30:03 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Jul 05 03:30:03 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter4932865857415988980.jar /__w/1/s/flink-runtime/target/surefire 2022-07-05T03-23-25_404-jvmRun1 surefire8916732512419442726tmp surefire_2130262314165063415tmp
Jul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in log
Jul 05 03:30:03 [ERROR] Process Exit Code: 239
Jul 05 03:30:03 [ERROR] Crashed tests:
Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8147"	FLINK	Closed	1	11500	11355	pull-request-available
13300424	Avoid scheduling deadlocks caused by cyclic input dependencies between regions	"Imagine a job like this:
A -- (pipelined FORWARD) --> B -- (blocking ALL-to-ALL) --> D
A -- (pipelined FORWARD) --> C -- (pipelined FORWARD) --> D
parallelism=2 for all vertices.

We will have 2 execution pipelined regions:
R1 = {A1, B1, C1, D1}
R2 = {A2, B2, C2, D2}

R1 has a cross-region input edge (B2->D1).
R2 has a cross-region input edge (B1->D2).

Scheduling deadlock will happen since we schedule a region only when all its inputs are consumable (i.e. blocking partitions to be finished). This is because R1 can be scheduled only if R2 finishes, while R2 can be scheduled only if R1 finishes.

To avoid this, one solution is to force a logical pipelined region with intra-region ALL-to-ALL blocking edges to form one only execution pipelined region, so that there would not be cyclic input dependency between regions.
Besides that, we should also pay attention to avoid cyclic cross-region POINTWISE blocking edges. "	FLINK	Closed	3	7	11355	pull-request-available
13255411	Remove legacy ProcessShutDownThread	"The class _org.apache.flink.runtime.util.ProcessShutDownThread_ is not referenced by any other classes/configs/scripts any more.

I think it's dead code and we can remove it."	FLINK	Resolved	3	7	11355	pull-request-available
13328177	Improve pipelined region scheduling performance	"In my recent TPCDS benchmark, pipelined region scheduling is slower than lazy-from-sources scheduling. 
The regression is due to some suboptimal implementation of {{PipelinedRegionSchedulingStrategy}}, including:
1. topologically sorting of vertices to deploy
2. unnecessary O(V) loop when sorting an empty set of regions

After improving these implementations, pipelined region scheduling turned to be 10% faster in the previous benchmark setup."	FLINK	Closed	3	7	11355	pull-request-available
13258474	Remove unnecessary scala Duration usages in flink-runtime	This ticket is to remove all usages of scala {{Duration/FiniteDuration}} in {{flink-runtime}}, except for those usages for {{Akka}} components (in AkkaUtils, AkkaRpcActor and ActorSystemScheduledExecutorAdapter).	FLINK	Resolved	3	7	11355	pull-request-available
13296752	Blink Planner set GlobalDataExchangeMode	"Blink planner config option ""table.exec.shuffle-mode"" should be extended to set GlobalDataExchangeMode for a job, values supported are:
 * ALL_EDGES_BLOCKING/batch
 * FORWARD_EDGES_PIPELINED
 * POINTWISE_EDGES_PIPELINED
 * ALL_EDGES_PIPELINED/pipelined

Note that values 'pipelined' and 'batch' are still supported to be compatible:
 * ‘pipelined’ will be treated the same as ‘ALL_EDGES_PIPELINED’
 * ‘batch’ will be treated the same as as ‘ALL_EDGES_BLOCKING’

Blink planner needs to set GlobalDataExchangeMode to StreamGraph according to the config value."	FLINK	Closed	3	7	11355	pull-request-available
13314235	Add a code contribution section about how to look for what to contribute	This section is to give general advices about browsing open Jira issues and starter tasks.	FLINK	Closed	3	4	11355	pull-request-available
13257717	Support configurable failover strategy for scheduler NG	"FLINK-10429 introduces new version failover strategies for scheduler NG.
There are 2 failover strategies and can be more in the future.
Users should be able to choose proper strategies for their jobs via configuration."	FLINK	Closed	3	7	11355	pull-request-available
13262881	RestartPipelinedRegionStrategy leverage tracked partition availability for better failover experience in DefaultScheduler 	"In current region failover when using DefaultScheduler, most of the input result partition states are unknown. Even though the failure cause is a PartitionException, only one unhealthy partition can be identified.

The may lead to multiple unsuccessful failovers before all the unhealthy but needed partitions are identified and their producers are involved in the failover as well. (unsuccessful failover here means the recovered tasks get failed again soon due to some missing input partitions.)

Using JM side tracked partition states to help the region failover to identify unhealthy(missing) partitions earlier can help with this case.

To achieve it, I'd propose as follows:
1. Change {{FailoverStrategy.Factory#create(FailoverTopology)}} to {{FailoverStrategy.Factory#create(FailoverTopology, ResultPartitionAvailabilityChecker)}}.
2. Add {{schedulerBase#getResultPartitionAvailabilityChecker}} which returns {{getExecutionGraph().getResultPartitionAvailabilityChecker()}}
3. In DefaultScheduler use the ResultPartitionAvailabilityChecker from SchedulerBase to create the failover strategy from the factory

It also fails BatchFineGrainedRecoveryITCase due to unexpected failover counts. This is because the legacy scheduler already has similar optimization in FLINK-13055."	FLINK	Closed	3	7	11355	pull-request-available
13311439	Remove the hack logics of result consumers	"Currently an {{IntermediateDataSet}} can have multiple consumer {{JobEdge}}. That's why the consumers of an {{IntermediateResultPartition}} is in the form of {{List<List<ExecutionEdge>>}}.

However, in scheduler/{{ExecutionGraph}} there is assumption that one {{IntermediateResultPartition}} can be consumed by one only {{ExecutionJobVertex}}. This results in a lot of hack logics which assumes partition consumers to contain a single list.

Given that there is no plan yet to support multiple consumer {{JobEdge}} of one {{IntermediateDataSet}}. I propose to refactor {{IntermediateDataSet}} to have one only consumer {{JobEdge}}. Thus the scheduler can get rid of these hack logics. "	FLINK	Closed	3	11500	11355	pull-request-available
13287950	Rework SchedulerTestUtils with testing classes to replace mockito usages	"Mockito is used in SchedulerTestUtils to mock ExecutionVertex and Execution for testing. It fails to mock every getter so that other tests use it may encounter NPE issues, e.g. ExecutionVertex#getID().
Mockito is also discouraged to be used in Flink tests. So I'd propose to rework the utils with testing classes."	FLINK	Closed	3	7	11355	pull-request-available
13336001	A result partition is not untracked after its producer task failed in TaskManager	{{Execution#maybeReleasePartitionsAndSendCancelRpcCall(...)}} will be not invoked when a task is reported to be failed in TaskManager, which results in its partitions to still be tacked by the job manager partition tracker. 	FLINK	Closed	3	1	11355	pull-request-available
13261745	Enable ClassLoaderITCase to pass with scheduler NG	"ClassLoaderITCase, EventTimeWindowCheckpointingITCase and WindowCheckpointingITCase now fail with scheduler NG.
There are 3 reasons for the failure:
1. state restore is not supported in scheduler NG yet
2. the cause of the expected exception is a bit different
3. there are multiples tasks in multiple regions, which will result in more failovers than expected as scheduler NG is using region failover

We need to support the state restore in scheduler NG and the fix the case issues. And then annotate them with AlsoRunWithSchedulerNG."	FLINK	Closed	3	7	11355	pull-request-available
13286367	Replacing vertexExecution in ScheduledUnit with executionVertexID	"{{ScheduledUnit#vertexExecution}} is nullable but {{ProgrammedSlotProvider}} requires it to be non-null to work. This makes {{ProgrammedSlotProvider}} not able to be used by new scheduler tests since {{vertexExecution}} is never set in the new scheduler code path. It blocks us from reworking tests which are based legacy scheduling to base on the new scheduler.

Besides that, there are 2 other problems caused by the nullable vertexExecution:
1. The log printed in SchedulerImpl#allocateSlotInternal(...) may contain no useful info since the vertexExecution can be null.
2. NPE issue reported in FLINK-16145.

Thus I would propose to replace the nullable vertexExecution with a non-null executionVertexID.
"	FLINK	Resolved	3	7	11355	pull-request-available
13475827	PipelinedRegionSchedulingITCase.testRecoverFromPartitionException failed with AssertionError	"
{code:java}
2022-08-08T20:38:43.3934646Z Aug 08 20:38:43 [ERROR] org.apache.flink.test.scheduling.PipelinedRegionSchedulingITCase.testRecoverFromPartitionException  Time elapsed: 20.288 s  <<< FAILURE!
2022-08-08T20:38:43.3935309Z Aug 08 20:38:43 java.lang.AssertionError: 
2022-08-08T20:38:43.3937070Z Aug 08 20:38:43 
2022-08-08T20:38:43.3938015Z Aug 08 20:38:43 Expected: is <false>
2022-08-08T20:38:43.3940277Z Aug 08 20:38:43      but: was <true>
2022-08-08T20:38:43.3940927Z Aug 08 20:38:43 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
2022-08-08T20:38:43.3941571Z Aug 08 20:38:43 	at org.junit.Assert.assertThat(Assert.java:964)
2022-08-08T20:38:43.3942120Z Aug 08 20:38:43 	at org.junit.Assert.assertThat(Assert.java:930)
2022-08-08T20:38:43.3943202Z Aug 08 20:38:43 	at org.apache.flink.test.scheduling.PipelinedRegionSchedulingITCase.testRecoverFromPartitionException(PipelinedRegionSchedulingITCase.java:98)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39652&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9994"	FLINK	Closed	3	1	11355	pull-request-available, test-stability
13262095	Restore task state in new DefaultScheduler	The new {{DefaultScheduler}} should restore the state of restarted tasks.	FLINK	Closed	3	7	11355	pull-request-available
13267928	Add a ResourceSpec in SlotSharingGroup to describe its overall resources	"To enable FLINK-14314 to allocate task slot regarding the share slot resources. We need 
a ResourceSpec for SlotSharingGroup. Its value is a merge of the resources of all operators in it.

This work was part of FLINK-14314 (step 1). However, as FLINK-14062 also relies on it to calculate managed memory fractions in a slot sharing group, it's better to make it a separate task to not block FLINK-14062 on FLINK-14314."	FLINK	Closed	3	7	11355	pull-request-available
13451167	Introduce SpeculativeScheduler	"A SpeculativeScheduler will be used if speculative execution is enabled. It extends AdaptiveBatchScheduler so that speculative execution can work along with the feature to adaptively tuning parallelisms for batch jobs.

The major differences of SpeculativeScheduler are:
 * SpeculativeScheduler needs to be able to directly deploy an Execution, while AdaptiveBatchScheduler can only perform ExecutionVertex level deployment.
 * SpeculativeScheduler does not restart the ExecutionVertex if an execution fails when any other current execution is still making progress
 * SpeculativeScheduler listens on slow tasks. Once there are slow tasks, it will block the slow nodes and deploy speculative executions of the slow tasks on other nodes.
 * Once any execution finishes, SpeculativeScheduler will cancel all the remaining executions of the same execution vertex."	FLINK	Closed	3	7	11355	pull-request-available
13267935	Improve batch schedule check input consumable performance	"Now if we launch batch job with 1000+ parallelism:

Even if we set the akka timeout of 2 minutes, the heartbeat is likely to timeout.

 JobMaster is buzy:
{code:java}
java.lang.Thread.State: RUNNABLE
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958)
        at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
        at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.anyMatch(ReferencePipeline.java:449)
        at org.apache.flink.runtime.executiongraph.ExecutionVertex.isInputConsumable(ExecutionVertex.java:824)
        at org.apache.flink.runtime.executiongraph.ExecutionVertex$$Lambda$257/564237119.test(Unknown Source)
        at java.util.stream.MatchOps$2MatchSink.accept(MatchOps.java:119)
        at java.util.stream.Streams$RangeIntSpliterator.tryAdvance(Streams.java:89)
        at java.util.stream.IntPipeline.forEachWithCancel(IntPipeline.java:162)
        at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230)
        at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.IntPipeline.allMatch(IntPipeline.java:482)
        at org.apache.flink.runtime.executiongraph.ExecutionVertex.checkInputDependencyConstraints(ExecutionVertex.java:811)
        at org.apache.flink.runtime.executiongraph.Execution.scheduleOrUpdateConsumers(Execution.java:889)
        at org.apache.flink.runtime.executiongraph.Execution.markFinished(Execution.java:1074)
        at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateStateInternal(ExecutionGraph.java:1597)
        at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:1570)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:424)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
{code}"	FLINK	Closed	3	7	11355	pull-request-available
