id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13197594	[Packaging] Zstd configure error on linux package builds	"Ubuntu Xenial https://travis-ci.org/kszucs/crossbow/builds/453054759
Ubuntu Bionic https://travis-ci.org/kszucs/crossbow/builds/453054805
Ubuntu Trusty https://travis-ci.org/kszucs/crossbow/builds/453054811
Debian Stretch https://travis-ci.org/kszucs/crossbow/builds/453054727

Perhaps this commit is related: https://github.com/apache/arrow/commit/394b334bba1199bd2d98a158736a6652efce629f

cc [~kou]"	ARROW	Resolved	3	1	6256	pull-request-available
13261223	[Website] merge_pr.py is published	We can download merge_pr.py at https://arrow.apache.org/merge_pr.py	ARROW	Resolved	4	4	6256	pull-request-available
13415436	[CI][Go] Use Go 1.16	Because Go 116 reached EOL.	ARROW	Resolved	3	4	6256	pull-request-available
13486530	[C++] Can't use bundled ORC with CMake 3.10	"test-ubuntu-18.04-cpp
https://github.com/ursacomputing/crossbow/actions/runs/3257736505/jobs/5349223302

{noformat}
-- Building Apache ORC from source
CMake Error at cmake_modules/ThirdpartyToolchain.cmake:4341 (target_link_libraries):
  Cannot specify link libraries for target ""orc::liborc"" which is not built
  by this project.
Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:183 (build_orc)
  cmake_modules/ThirdpartyToolchain.cmake:278 (build_dependency)
  cmake_modules/ThirdpartyToolchain.cmake:4357 (resolve_dependency)
  CMakeLists.txt:496 (include)
{noformat}"	ARROW	Resolved	1	4	6256	pull-request-available
13311520	[GLib][Ruby] Allow to read Parquet files in chunks (by RowGroup)	At the moment only `gparquet_arrow_file_reader_read_table` is exported which requires the reader to load all row-groups. Additionally, it would be extremely helpful to have access to the `ReadRowGroup` method which would allow reading parts of the file without having to load everything into memory.	ARROW	Resolved	3	4	6256	pull-request-available
13382504	[CI] Travis ARM builds often crash	"Those builds often crash with a compiler crash, for example:
https://travis-ci.com/github/apache/arrow/jobs/511663690

{code}
FAILED: CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o 

/usr/bin/c++  -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -DHAVE_ZLIB -I. -I/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src -fdiagnostics-color=always -ggdb -O0 -g -fPIC   -fdiagnostics-color=always -ggdb -O0 -g -fPIC   -std=c++11 -MD -MT CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o -MF CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o.d -o CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o -c /build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc

c++: fatal error: Killed signal terminated program cc1plus
{code}

It seems this is a problem with the Travis-CI ARM execution environment."	ARROW	Resolved	3	1	6256	pull-request-available
13450327	[CI][C++] xsimd is missing for vcpkg build, minimal example and R with system packages	"test-build-vcpkg-win

https://github.com/ursacomputing/crossbow/runs/6892291430?check_suite_focus=true

{noformat}
  Could not find a configuration file for package ""xsimd"" that is compatible
  with requested version ""8.1.0"".
  The following configuration files were considered but not accepted:
    D:/a/crossbow/crossbow/arrow/cpp/vcpkg_installed/x64-windows/share/xsimd/xsimdConfig.cmake, version: 7.4.9
Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:253 (find_package)
  cmake_modules/ThirdpartyToolchain.cmake:2209 (resolve_dependency)
  CMakeLists.txt:567 (include)
{noformat}

 example-cpp-minimal-build-static-system-dependency 

https://github.com/ursacomputing/crossbow/runs/6892331642?check_suite_focus=true

{noformat}
CMake Error at cmake_modules/ThirdpartyToolchain.cmake:253 (find_package):
  By not providing ""Findxsimd.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""xsimd"", but
  CMake did not find one.
  Could not find a package configuration file provided by ""xsimd"" (requested
  version 8.1.0) with any of the following names:
    xsimdConfig.cmake
    xsimd-config.cmake
  Add the installation prefix of ""xsimd"" to CMAKE_PREFIX_PATH or set
  ""xsimd_DIR"" to a directory containing one of the above files.  If ""xsimd""
  provides a separate development package or SDK, be sure it has been
  installed.
Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:2209 (resolve_dependency)
  CMakeLists.txt:567 (include)
{noformat}

test-r-depsource-system

https://github.com/ursacomputing/crossbow/runs/6892324900?check_suite_focus=true

{noformat}
CMake Error at cmake_modules/ThirdpartyToolchain.cmake:253 (find_package):
  By not providing ""Findxsimd.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""xsimd"", but
  CMake did not find one.
  Could not find a package configuration file provided by ""xsimd"" (requested
  version 8.1.0) with any of the following names:
    xsimdConfig.cmake
    xsimd-config.cmake
  Add the installation prefix of ""xsimd"" to CMAKE_PREFIX_PATH or set
  ""xsimd_DIR"" to a directory containing one of the above files.  If ""xsimd""
  provides a separate development package or SDK, be sure it has been
  installed.
Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:2209 (resolve_dependency)
  CMakeLists.txt:567 (include)
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13388045	[C++] Don't use .pc only in CMake paths for Requires.private	Because they can't be found by raw pkg-config usage.	ARROW	Resolved	3	4	6256	pull-request-available
13475946	[Release] R libarrow binaries have the wrong version number	The libarrow binaries that are uploaded during the release process have the wrong version number. This is an issue with the submit binaries script/r-binary-packages job. The arrow version should be picked up by the job even if not passed explicitly as a custom param.	ARROW	Resolved	1	1	6256	pull-request-available
13209221	[Plasma][Python] PlasmaClient.list doesn't work with CUDA enabled Plasma	"{{plasma::ObjectTableEntry}} layout is changed with CUDA enabled build: https://github.com/apache/arrow/blob/master/cpp/src/plasma/common.h#L97-L100
But it's not cared in Cython code: https://github.com/apache/arrow/blob/master/python/pyarrow/_plasma.pyx#L77-L78"	ARROW	Resolved	3	1	6256	pull-request-available
13322791	[Python] Failed to install on aarch64	"My team is attempting to migrate some workloads from x86-64 to ARM64, a blocker for this is PyArrow failing to install. `pip install pyarrow` fails to build the wheel as -march isn't correctly resolved:

{noformat}
 -- System processor: aarch64
 -- Performing Test CXX_SUPPORTS_ARMV8_ARCH
 -- Performing Test CXX_SUPPORTS_ARMV8_ARCH - Failed
 -- Arrow build warning level: PRODUCTION
 CMake Error at cmake_modules/SetupCxxFlags.cmake:338 (message):
 Unsupported arch flag: -march=.
{noformat}

It's possible to get the build to work after editing `cmake_modules/SetupCxxFlags.cmake` to force ARROW_ARMV8_ARCH_FLAG to end up as an architecture such as 'armv8-a' - although some more elaborate logic is really needed to pick up the correct extensions.

I can see that there  have been a number of items discussed in the past both on Jira and in GitHub issues ranging from simple fixes to the cmake script to more elaborate fixes cross-product for arch detection - but I wasn't able to discern how the project wishes to proceed.

With AWS pushing their ARM-based instances heavily at this point I would advocate for picking a direction before an influx of new issues.

 "	ARROW	Resolved	3	1	6256	pull-request-available
13405956	[C++][CI] LLVM 13 cannot be used on macOS GHA builds	"See 
https://github.com/apache/arrow/pull/11372/checks?check_run_id=3859972940
https://github.com/apache/arrow/pull/11372/checks?check_run_id=3859973472
https://github.com/apache/arrow/pull/11372/checks?check_run_id=3859973399
"	ARROW	Resolved	2	1	6256	pull-request-available
13402796	[GLib][Ruby][Dataset] Add support for scanning from directory	"in python dataset = ds.dataset (base / ""parquet_dataset"", format = ""parquet"")
dataset.files
dataset.to_table (filter = ds.field ('a')> = 7) .to_pandas ()

Want to do equivalent in ruby"	ARROW	Resolved	3	4	6256	pull-request-available
13293868	[CI][GLib] Meson install fails in the macOS build	"It also happens in the pull request builds, see build log https://github.com/apache/arrow/runs/533168517#step:5:1230

cc @kou"	ARROW	Resolved	3	4	6256	pull-request-available
13102848	[GLib] Support build append in builder	It improves performance.	ARROW	Resolved	3	2	6256	pull-request-available
13383385	[Release][Packaging] Update the java post release task to use the crossbow artifacts	"We produce java jars using a crossbow tasks. Ideally we should download and deploy these packages instead of compiling them locally during the java post release task.

See the produced jars at: https://github.com/ursacomputing/crossbow/releases/tag/actions-496-github-java-jars
See more context at: https://github.com/apache/arrow/pull/10411
"	ARROW	Resolved	3	4	6256	pull-request-available
13218810	[C++] Python not being built nor test under MinGW builds	"Follow up to needed for [arrow/pull/3693/files|https://github.com/apache/arrow/pull/3693/files].

appveyor-cpp-build-mingw.bat has not yet enabled Python tests, need to revert,

-DARROW_PYTHON=OFF

Suggestion was to use,
{code:java}
diff --git a/ci/appveyor-cpp-build-mingw.bat b/ci/appveyor-cpp-build-mingw.bat
index 06e8b7f7..3a853031 100644
--- a/ci/appveyor-cpp-build-mingw.bat
+++ b/ci/appveyor-cpp-build-mingw.bat
@@ -24,6 +24,15 @@ set INSTALL_DIR=%HOMEDRIVE%%HOMEPATH%\install
set PATH=%INSTALL_DIR%\bin;%PATH%
set PKG_CONFIG_PATH=%INSTALL_DIR%\lib\pkgconfig

+for /f ""usebackq"" %%v in (`python3 -c ""import sys; print('.'.join(map(str, sys.version_info[0:2])))""`) do (
+ set PYTHON_VERSION=%%v
+)
+
+set PYTHONHOME=%MINGW_PREFIX%\lib\python%PYTHON_VERSION%
+set PYTHONPATH=%PYTHONHOME%
+set PYTHONPATH=%PYTHONPATH%;%MINGW_PREFIX%\lib\python%PYTHON_VERSION%\lib-dynload
+set PYTHONPATH=%PYTHONPATH%;%MINGW_PREFIX%\lib\python%PYTHON_VERSION%\site-packages
+
{code}
However, this suggestion currently trigger a built error in Travis,
{code:java}
[ 43%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/json-simple.cc.obj
[ 44%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/message.cc.obj
[ 44%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/metadata-internal.cc.obj
[ 45%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/reader.cc.obj
[ 45%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/writer.cc.obj
[ 45%] Built target arrow_objlib
make: *** [Makefile:141: all] Error 2
C:\projects\arrow\cpp\build>goto scriptexit{code}
Therefore, additional investigation is needed."	ARROW	Resolved	3	6	6256	pull-request-available
13416172	[Packaging][CentOS] Drop support for CentOS 8	Because it'll reach EOL at 2021-12.	ARROW	Resolved	3	4	6256	pull-request-available
13281806	[Packaging][deb][RPM] Can't build repository packages for RC	"apache-arrow-archive-keyring failure:

https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=5737&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&l=13284

{noformat}
2020-01-27T16:02:31.2221451Z /host/build.sh: 27: cd: can't cd to apache-arrow-archive-keyring-0.16.0/
{noformat}

apache-arrow-release failure:

https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=5774&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&l=10330

{noformat}
/var/tmp/rpm-tmp.IfEC8a: line 39: cd: apache-arrow-release-0.16.0: No such file or directory
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13340747	[Ruby] Support Decimal256 type	The C++ implementation now support it.  We need to ensure Ruby/Gobject bindings do as well.	ARROW	Resolved	3	4	6256	pull-request-available
13194594	[GLib] cuda.cpp compile error	"Build errors:
- https://travis-ci.org/kszucs/crossbow/builds/446856145
- https://travis-ci.org/kszucs/crossbow/builds/446855716"	ARROW	Resolved	3	1	6256	pull-request-available
13282274	[Release][Yum] Ignore some arm64 verifications	Because we can't build some arm64 binaries by Crossbow for now.	ARROW	Resolved	3	4	6256	pull-request-available
13413731	[CI][Homebrew] Nightly build is failed by apache-arrow-glib version mismatch	"https://github.com/ursacomputing/crossbow/runs/4308415772?check_suite_focus=true#step:4:3924

{noformat}
apache-arrow:
Error: 1 problem in 1 formula detected
  * Version of `apache-arrow-glib` (6.0.1) should match version of `apache-arrow` (7.0.0)
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13186678	"[C++] ""redeclared without dllimport attribute after being referenced with dll linkage"" with MinGW"	"The following warning is reported by MinGW when we build Arrow GLib with Arrow C++:

{code}
../apache-arrow-0.11.0/c_glib/../cpp/src/arrow/status.h:265:8: warning: 'arrow::Status::Status(arrow::Status&&)' redeclared without dllimport attribute after being referenced with dll linkage
 inline Status::Status(Status&& s) noexcept : state_(s.state_) \{ s.state_ = NULL; }
 ^~~~~~
{code}"	ARROW	Resolved	4	4	6256	pull-request-available
13427584	[Release][MSYS2] Update reverse dependencies too	Reverse dependencies should be rebuilt when arrow package is updated.	ARROW	Resolved	3	4	6256	pull-request-available
13488264	[Release][CI] Use Ubuntu 22.04 for verifying binaries	"APT/Yum verifications use Docker. If we use old libseccomp on host, some operations may cause errors:

e.g.:  https://github.com/ursacomputing/crossbow/actions/runs/3294870946/jobs/5432835953#step:7:5437

{noformat}
  + valac --pkg arrow-glib --pkg posix build.vala
  error: Failed to close file descriptor for child process (Operation not permitted)
  error: Failed to close file descriptor for child process (Operation not permitted)
  error: Failed to close file descriptor for child process (Operation not permitted)
  error: Failed to close file descriptor for child process (Operation not permitted)
  error: Failed to close file descriptor for child process (Operation not permitted)
  error: Failed to close file descriptor for child process (Operation not permitted)
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13193068	[Gandiva][C++] Build error with g++ 8.2.0	"Error message1:
{noformat}
In file included from /home/kou/work/cpp/arrow.kou/cpp/src/gandiva/expr_decomposer.cc:27:
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:46:27: error: 'function' in namespace 'std' does not name a template type
   using maker_type = std::function<Status(const FunctionNode&, FunctionHolderPtr*)>;
                           ^~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:46:22: note: 'std::function' is defined in header '<functional>'; did you forget to '#include <functional>'?
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:30:1:
+#include <functional>
 
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:46:22:
   using maker_type = std::function<Status(const FunctionNode&, FunctionHolderPtr*)>;
                      ^~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:52: error: 'maker_type' was not declared in this scope
   using map_type = std::unordered_map<std::string, maker_type>;
                                                    ^~~~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:52: note: suggested alternative: 'decltype'
   using map_type = std::unordered_map<std::string, maker_type>;
                                                    ^~~~~~~~~~
                                                    decltype
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:62: error: template argument 2 is invalid
   using map_type = std::unordered_map<std::string, maker_type>;
                                                              ^
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:62: error: template argument 5 is invalid
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:60:10: error: 'map_type' does not name a type; did you mean 'iswctype'?
   static map_type& makers() {
          ^~~~~~~~
          iswctype
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h: In static member function 'static gandiva::Status gandiva::FunctionHolderRegistry::Make(const string&, const gandiva::FunctionNode&, gandiva::FunctionHolderPtr*)':
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:51:18: error: 'makers' was not declared in this scope
     auto found = makers().find(name);
                  ^~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:51:18: note: suggested alternative: 'Make'
     auto found = makers().find(name);
                  ^~~~~~
                  Make
{noformat}

Error message2:

{noformat}
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc: In static member function 'static gandiva::NodePtr gandiva::TreeExprBuilder::MakeNull(gandiva::DataTypePtr)':
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:78:70: error: 'float_t' was not declared in this scope
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((float_t)0), true);
                                                                      ^~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:78:70: note: suggested alternative: 'float'
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((float_t)0), true);
                                                                      ^~~~~~~
                                                                      float
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:80:70: error: 'double_t' was not declared in this scope
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((double_t)0), true);
                                                                      ^~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:80:70: note: suggested alternative: 'double'
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((double_t)0), true);
                                                                      ^~~~~~~~
                                                                      double
{noformat}"	ARROW	Resolved	3	1	6256	pull-request-available
13497448	[C++][Windows] External/shared libthrift requires IMPORTED_IMPLIB in FindThrift.cmake 	"As mentioned in ARROW-18255, over at [https://github.com/JuliaPackaging/Yggdrasil/pull/5425,] we cross-compile Arrow to make it easily available within the Julia ecosystem. 

When compiling with `DARROW_THRIFT_USE_SHARED=ON` and using an external libthrift, CMAKE warns with:

 
{quote}[12:00:54] CMake Warning (dev) in CMakeLists.txt:
[12:00:54]   Policy CMP0111 is not set: An imported target missing its location property
[12:00:54]   fails during generation.  Run ""cmake --help-policy CMP0111"" for policy
[12:00:54]   details.  Use the cmake_policy command to set the policy and suppress this
[12:00:54]   warning.
[12:00:54]
[12:00:54]   IMPORTED_IMPLIB not set for imported target ""thrift::thrift"" configuration
[12:00:54]   ""RELEASE"".
[12:00:54] This warning is for project developers.  Use -Wno-dev to suppress it.
{quote}
This will lead to linking errors later on. I've fixed the warning with the following patch for v10.

 
{quote}{{diff --git a/cpp/cmake_modules/FindThrift.cmake b/cpp/cmake_modules/FindThrift.cmake}}
{{index 2f20a5cb5..2d1e728aa 100644}}
{{--- a/cpp/cmake_modules/FindThrift.cmake}}
{{+++ b/cpp/cmake_modules/FindThrift.cmake}}
{{@@ -146,6 +146,7 @@ if(Thrift_FOUND)}}
{{   endif()}}
{{   set_target_properties(thrift::thrift}}
{{                         PROPERTIES IMPORTED_LOCATION ""${THRIFT_LIB}""}}
{{+                                   IMPORTED_IMPLIB ""${THRIFT_LIB}""}}
{{                                    INTERFACE_INCLUDE_DIRECTORIES ""${THRIFT_INCLUDE_DIR}"")}}
{{   if(WIN32 AND NOT MSVC_TOOLCHAIN)}}
{{     # We don't need this for Visual C++ because Thrift uses}}
{quote}
 "	ARROW	Resolved	3	1	6256	pull-request-available
13402598	[CI] Use Debian 11	Debian 11 is the current stable.	ARROW	Resolved	3	4	6256	pull-request-available
13173215	[Packaging] Options to build packages from apache source archive	See conversation: https://github.com/apache/arrow/pull/2288	ARROW	Resolved	3	3	6256	pull-request-available
13257818	[Packaging][RPM] Add support for CentOS 7 on aarch64	"apt:build rake task supports architecture to run [1], but it is not true
 for yum task.

 [1] [https://github.com/apache/arrow/blob/master/dev/tasks/linux-packages/package-task.rb#L276]

It is useful yum task also supports architecture (ex. i386) too. (even though CentOS 6 i386 EOL reaches 2020/11)

 "	ARROW	Resolved	3	1	6256	pull-request-available
13108963	[GLib] Support Meson	It's a base work for ARROW-1642. 	ARROW	Resolved	3	2	6256	pull-request-available
13388258	[CI] Require docker-compose 1.27.0 or later	"We need it for ""extends"".

See also:

  * https://issues.apache.org/jira/browse/ARROW-13199
  * https://github.com/apache/arrow/pull/10611
  * https://github.com/docker/compose/pull/7588"	ARROW	Resolved	3	4	6256	pull-request-available
13369069	[C++] CMake's find_package(Parquet) does not find Parquet with Arrow 3.0.0	"Hello,

I just updated a small utility that converts binary data to Parquet from Arrow 0.15 to 3.0, and noticed that CMake could not resolve {{find_package(Parquet REQUIRED)}}, as my installation tree of Arrow contained {{…/share/cmake/arrow/ParquetConfig.cmake}}, but CMake seems to be expecting {{…/share/cmake/*parquet*/ParquetConfig.cmake.}}

Creating a symbolic link from the {{arrow}} directory to a {{parquet}} directory solved CMake's find procedure. Alternatively, I ended up at this work-around that did not require me modifying my local install tree:
{code:java}
find_package(Arrow REQUIRED)
get_filename_component(MY_SEARCH_DIR ${Arrow_CONFIG} DIRECTORY)
find_package(Parquet REQUIRED HINTS ${MY_SEARCH_DIR})
{code}
Is this expected behavior? I went through the CMake code and found that there is only one installation directory for all CMake files: {{ARROW_CMAKE_INSTALL_DIR}} — I would expect this to be specific to the libraries exported.

Thanks,
Matthias"	ARROW	Resolved	3	1	6256	pull-request-available
13407391	[Release][Ruby] Check Homebrew/MSYS2 package version before releasing	{{gem install red-arrow}} is failed on macOS and Windows if Homebrew/MSYS2 packages aren't updated yet.	ARROW	Resolved	3	4	6256	pull-request-available
13409387	[GLib][Ruby] Segfault when trying to create Decimal128DataType with wrong precision	"{code:java}
Arrow::Decimal128DataType.new(50, 48)
 /tmp/apache-arrow-20211019-6348-1ltpz75/apache-arrow-5.0.0/cpp/src/arrow/type.cc:813:  Check failed: (precision) <= (kMaxPrecision) 
 0   libarrow.500.dylib                  0x000000011170b8e2 _ZN5arrow4util7CerrLogD2Ev + 204
 1   libarrow.500.dylib                  0x000000011170b808 _ZN5arrow4util7CerrLogD0Ev + 14
 2   libarrow.500.dylib                  0x000000011170b7b0 _ZN5arrow4util8ArrowLogD1Ev + 34
 3   libarrow.500.dylib                  0x00000001116a7eba _ZN5arrow14Decimal128TypeC2Eii + 176
 4   libarrow.500.dylib                  0x00000001116b21bc _ZN5arrow10decimal128Eii + 78
 5   libarrow-glib.500.dylib             0x000000011147eb68 garrow_decimal128_data_type_new + 24
 6   libffi.8.dylib                      0x0000000111122d8a ffi_call_unix64 + 82
 7   ???                                 0x00007ffee21c3818 0x0 + 140732691920920
{code}"	ARROW	Resolved	3	1	6256	pull-request-available
13149473	[GLib] Travis-CI failures	"See this (empty) PR:
https://github.com/apache/arrow/pull/1822

It results in failures on the GLib builds:
https://travis-ci.org/apache/arrow/builds/361145322

{code}================================================================================
/home/travis/build/apache/arrow/c_glib/test/test-list-array.rb:24:in `test_new'
     21:   def test_new
     22:     value_offsets = Arrow::Buffer.new([0, 2, 5, 5].pack(""l*""))
     23:     data = Arrow::Buffer.new([1, 2, 3, 4, 5].pack(""c*""))
  => 24:     values = Arrow::Int8Array.new(5, data, nil, 0)
     25:     assert_equal(build_list_array(Arrow::Int8DataType.new,
     26:                                   [[1, 2], [3, 4, 5], nil]),
     27:                  Arrow::ListArray.new(3,
/home/travis/build/apache/arrow/c_glib/test/test-list-array.rb:24:in `new'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:328:in `block in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `block (2 levels) in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `invoke'
Error: test_new(TestListArray): ArgumentError: <Arrow::Int8Array#new>: the 2nd argument must not nil: <null_bitmap>
================================================================================
...........................................E
================================================================================
/home/travis/build/apache/arrow/c_glib/test/test-struct-array.rb:41:in `test_new'
     38: 
     39:     data_type = Arrow::StructDataType.new(fields)
     40:     children = [
  => 41:       Arrow::Int8Array.new(2, Arrow::Buffer.new([-29, 2].pack(""C*"")), nil, 0),
     42:       Arrow::BooleanArray.new(2, Arrow::Buffer.new([0b01].pack(""C*"")), nil, 0),
     43:     ]
     44:     assert_equal(struct_array1,
/home/travis/build/apache/arrow/c_glib/test/test-struct-array.rb:41:in `new'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:328:in `block in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `block (2 levels) in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `invoke'
Error: test_new(TestStructArray): ArgumentError: <Arrow::Int8Array#new>: the 2nd argument must not nil: <null_bitmap>
================================================================================
{code}

"	ARROW	Resolved	3	1	6256	ci-failure, pull-request-available
13271586	[C++][R] read_parquet() freezes on Windows with Japanese locale	"The following example on read_parquet()'s doc freezes (seems to wait for the result forever) on my Windows.

df <- read_parquet(system.file(""v0.7.1.parquet"", package=""arrow""))

The CRAN checks are all fine, which means the example is successfully executed on the CRAN Windows. So, I have no idea why it doesn't work on my local.

[https://cran.r-project.org/web/checks/check_results_arrow.html]

Here's my session info in case it helps:
{code:java}
> sessioninfo::session_info()

- Session info ---------------------------------------------------------------------------------
 setting  value
 version  R version 3.6.1 (2019-07-05)
 os       Windows 10 x64
 system   x86_64, mingw32
 ui       RStudio
 language en
 collate  Japanese_Japan.932
 ctype    Japanese_Japan.932
 tz       Asia/Tokyo
 date     2019-12-01

- Packages -------------------------------------------------------------------------------------
 package     * version  date       lib source
 arrow       * 0.15.1.1 2019-11-05 [1] CRAN (R 3.6.1)
 assertthat    0.2.1    2019-03-21 [1] CRAN (R 3.6.0)
 bit           1.1-14   2018-05-29 [1] CRAN (R 3.6.0)
 bit64         0.9-7    2017-05-08 [1] CRAN (R 3.6.0)
 cli           1.1.0    2019-03-19 [1] CRAN (R 3.6.0)
 crayon        1.3.4    2017-09-16 [1] CRAN (R 3.6.0)
 fs            1.3.1    2019-05-06 [1] CRAN (R 3.6.0)
 glue          1.3.1    2019-03-12 [1] CRAN (R 3.6.0)
 magrittr      1.5      2014-11-22 [1] CRAN (R 3.6.0)
 purrr         0.3.3    2019-10-18 [1] CRAN (R 3.6.1)
 R6            2.4.1    2019-11-12 [1] CRAN (R 3.6.1)
 Rcpp          1.0.3    2019-11-08 [1] CRAN (R 3.6.1)
 reprex        0.3.0    2019-05-16 [1] CRAN (R 3.6.0)
 rlang         0.4.2    2019-11-23 [1] CRAN (R 3.6.1)
 rstudioapi    0.10     2019-03-19 [1] CRAN (R 3.6.0)
 sessioninfo   1.1.1    2018-11-05 [1] CRAN (R 3.6.0)
 tidyselect    0.2.5    2018-10-11 [1] CRAN (R 3.6.0)
 withr         2.1.2    2018-03-15 [1] CRAN (R 3.6.0)

[1] C:/Users/hiroaki-yutani/Documents/R/win-library/3.6
[2] C:/Program Files/R/R-3.6.1/library
{code}"	ARROW	Resolved	2	1	6256	parquet, pull-request-available
13191862	[Ruby] Import Red Parquet	I want to donate Ruby bindings for Apache Parquet GLib. It's developed by me at [https://github.com/red-data-tools/red-parquet] .	ARROW	Resolved	3	2	6256	pull-request-available
13250697	[C++] Rename Argsort kernel to SortToIndices	"""Argsort"" is NumPy specific name. Other languages/libraries use
different name:

  * R: order
    * https://cran.r-project.org/doc/manuals/r-release/fullrefman.pdf#Rfn.order

  * MATLAB: sort
    * https://mathworks.com/help/matlab/ref/sort.html
    * ""sort"" returns sorted array and indices to sort array

  * Julia: sortperm
    * https://pkg.julialang.org/docs/julia/THl1k/1.1.1/base/sort.html#Base.sortperm

It's better that we use general name because Arrow C++ isn't a NumPy
compatible library.

""SortToIndices"" means ""sort that returns indices array""."	ARROW	Resolved	3	4	6256	pull-request-available
13218811	[C++] Dictionary tests disabled under MinGW builds	"Follow up to needed for [arrow/pull/3693/files|https://github.com/apache/arrow/pull/3693/files].

Under cpp/src/arrow/CMakeLists.txt, PR disabled array-dict-test.cc test, by adding:

 
{code:java}
if(WIN32)
	 add_arrow_test(array-test
			SOURCES
			array-test.cc
			array-binary-test.cc
			array-list-test.cc
			array-struct-test.cc)
else()
	 add_arrow_test(array-test
			SOURCES
			array-test.cc
			array-binary-test.cc
			array-dict-test.cc
			array-list-test.cc
			array-struct-test.cc)
endif(){code}
 

Which should be reverted and investigated further. The build error that including this test triggers is the following:
{code:java}
/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0xb9a2): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0xcb8a): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0xeef8): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0x10240): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0x104fc): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::AppendArray(arrow::Array const&)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0x108ef): undefined reference to `arrow{code}"	ARROW	Resolved	3	6	6256	pull-request-available
13371908	[Packaging][deb] Rename -archive-keyring to -apt-source	"Because lintian recommends that a package that puts files to
/etc/apt/sources.list.d/ uses -apt-source suffix.

See also: https://lintian.debian.net/tags/package-installs-apt-sources

This also changes repository URL to
https://apache.jfrog.io/artifactory/ from https://apache.bintray.com/ ."	ARROW	Resolved	3	4	6256	pull-request-available
13445542	[Dev] Use GitHub API to merge pull request	"We use local ""git merge"" to merge a pull request in {{dev/merge_arrow_pr.py}}.

If we use ""git merge"" to merge a pull request, GitHub's Web UI shows ""Closed"" mark not ""Merged"" mark in a pull request page. This sometimes confuses new contributors. ""Why was my pull request closed without merging?"" See https://github.com/apache/arrow/pull/12004#issuecomment-1031619771 for example.

If we use GitHub API https://docs.github.com/en/rest/pulls/pulls#merge-a-pull-request to merge a pull request, GitHub's Web UI shows ""Merged"" mark not ""Closed"" mark. See https://github.com/apache/arrow/pull/13180 for example. I used GitHub API to merge the pull request.

And we don't need to create a local branch on local repository to merge a pull request. But we must specify {{ARROW_GITHUB_API_TOKEN}} to run {{dev/merge_arrow_pr.py}}."	ARROW	Resolved	3	4	6256	pull-request-available
13469266	[Packaging][deb] Drop support for Ubuntu impish	"It will reach EOL at 2022-07-14.

See also: https://wiki.ubuntu.com/Releases"	ARROW	Resolved	3	4	6256	pull-request-available
13479266	[C++] Backward compatible ${PACKAGE}_shared CMake target isn't provided	"This is a follow-up of ARROW-12175.

We introduced {{${PACKAGE}::}} prefix to all exported CMake targets such as {{Arrow::arrow_shared}} and {{Arrow::arrow_static}} but we also provides no namespaced CMake targets such as {{arrow_shared}} and {{arrow_static}} as aliases of namespaced CMake targets. But the backward compatibility feature isn't worked for {{_shared}}."	ARROW	Resolved	3	1	6256	pull-request-available
13405275	[C++] find_package(CURL) in build_google_cloud_cpp_storage fails	"{code}
CMake Error at /usr/share/cmake-3.18/Modules/FindCURL.cmake:163 (message):
  CURL: Required feature 7.47.0 is not found
Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:3585 (find_package)
  cmake_modules/ThirdpartyToolchain.cmake:156 (build_google_cloud_cpp_storage)
  cmake_modules/ThirdpartyToolchain.cmake:232 (build_dependency)
  cmake_modules/ThirdpartyToolchain.cmake:3697 (resolve_dependency)
  CMakeLists.txt:535 (include)
{code}"	ARROW	Resolved	3	1	6256	pull-request-available
13209314	[Release] Update release verification script to check binaries on Bintray	The release verification script still expects the binaries to be on the ASF dist server	ARROW	Resolved	3	4	6256	pull-request-available
13275908	[Python] Documentation lint is failed	"This is included by e519853fbfd0241d87572d8b333a8ab34f02b401.

I missed this. Sorry."	ARROW	Resolved	3	4	6256	pull-request-available
13443878	[Release][R] Version bump for r/NEWS.md doesn't work on macOS	"https://github.com/apache/arrow/commit/c3d031250a7fdcfee5e576833bf6f39097602c30 generated by {{dev/release/01-prepare.sh}} doesn't include any change for {{r/NEWS.md}} and
https://github.com/apache/arrow/commit/4c21fd12f93e4853c03c05919ffb22c6bb8f09b0 generated by {{dev/release/post-12-bump-versions.sh}} doesn't include any change for {{r/NEWS.md}}."	ARROW	Resolved	3	4	6256	pull-request-available
13171325	[GLib] Add GArrowORCFileReader	It's not a required change for 0.10.0. It's a nice-to-have change.	ARROW	Resolved	3	4	6256	pull-request-available
13391158	[C++] conda-forge benchmark library rejected	"It seems that our detection routine for the C++ benchmark library got broken recently:
{code}
CMake Error at cmake_modules/ThirdpartyToolchain.cmake:235 (find_package):
  Could not find a configuration file for package ""benchmark"" that is
  compatible with requested version ""0.0.0"".

  The following configuration files were considered but not accepted:

    /home/antoine/miniconda3/envs/pyarrow/lib/cmake/benchmark/benchmarkConfig.cmake, version: 1.5.4

Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:1852 (resolve_dependency)
  CMakeLists.txt:515 (include)

{code}
"	ARROW	Resolved	3	1	6256	pull-request-available
13477077	[C++] AppVeyor build fails due to Boost/S3	"Observed on master

{noformat}
[182/351] Building CXX object src\arrow\filesystem\CMakeFiles\arrow-s3fs-test.dir\Unity\unity_0_cxx.cxx.obj
FAILED: src/arrow/filesystem/CMakeFiles/arrow-s3fs-test.dir/Unity/unity_0_cxx.cxx.obj 
C:\Miniconda37-x64\Scripts\clcache.exe  /nologo /TP -DARROW_HAVE_RUNTIME_AVX2 -DARROW_HAVE_RUNTIME_AVX512 -DARROW_HAVE_RUNTIME_BMI2 -DARROW_HAVE_RUNTIME_SSE4_2 -DARROW_HAVE_SSE4_2 -DARROW_HDFS -DARROW_MIMALLOC -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_RE2 -DARROW_WITH_SNAPPY -DARROW_WITH_UTF8PROC -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DAWS_CAL_USE_IMPORT_EXPORT -DAWS_CHECKSUMS_USE_IMPORT_EXPORT -DAWS_COMMON_USE_IMPORT_EXPORT -DAWS_EVENT_STREAM_USE_IMPORT_EXPORT -DAWS_IO_USE_IMPORT_EXPORT -DAWS_SDK_VERSION_MAJOR=1 -DAWS_SDK_VERSION_MINOR=8 -DAWS_SDK_VERSION_PATCH=186 -DAWS_USE_IO_COMPLETION_PORTS -DBOOST_ALL_DYN_LINK -DBOOST_ALL_NO_LIB -DBOOST_ATOMIC_DYN_LINK -DBOOST_ATOMIC_NO_LIB -DBOOST_FILESYSTEM_DYN_LINK -DBOOST_FILESYSTEM_NO_LIB -DBOOST_SYSTEM_DYN_LINK -DBOOST_SYSTEM_NO_LIB -DPROTOBUF_USE_DLLS -DURI_STATIC_BUILD -DUSE_IMPORT_EXPORT -DUSE_IMPORT_EXPORT=1 -DUSE_WINDOWS_DLL_SEMANTICS -D_CRT_SECURE_NO_WARNINGS -D_ENABLE_EXTENDED_ALIGNED_STORAGE -IC:\projects\arrow\cpp\build\src -IC:\projects\arrow\cpp\src -IC:\projects\arrow\cpp\src\generated -IC:\projects\arrow\cpp\thirdparty\flatbuffers\include -IC:\Miniconda37-x64\envs\arrow\Library\include -IC:\projects\arrow\cpp\thirdparty\hadoop\include -IC:\projects\arrow\cpp\build\mimalloc_ep\src\mimalloc_ep\include\mimalloc-2.0 /DWIN32 /D_WINDOWS  /GR /EHsc /D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING   /EHsc /wd5105 /bigobj /utf-8 /W3 /wd4800 /wd4996 /wd4065  /WX /MP /MD /Od /UNDEBUG /showIncludes /Fosrc\arrow\filesystem\CMakeFiles\arrow-s3fs-test.dir\Unity\unity_0_cxx.cxx.obj /Fdsrc\arrow\filesystem\CMakeFiles\arrow-s3fs-test.dir\ /FS -c C:\projects\arrow\cpp\build\src\arrow\filesystem\CMakeFiles\arrow-s3fs-test.dir\Unity\unity_0_cxx.cxx
Please define _WIN32_WINNT or _WIN32_WINDOWS appropriately. For example:
- add -D_WIN32_WINNT=0x0601 to the compiler command line; or
- add _WIN32_WINNT=0x0601 to your project's Preprocessor Definitions.
Assuming _WIN32_WINNT=0x0601 (i.e. Windows 7 target).
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/environment.hpp(266): error C2220: warning treated as error - no 'object' file generated
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/environment.hpp(261): note: while compiling class template member function 'boost::iterators::transform_iterator<boost::process::detail::make_entry<Char,boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>>,Char **,boost::process::detail::entry<Char,boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>>,boost::process::detail::entry<Char,boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>>> boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>::find(const std::basic_string<char,std::char_traits<char>,std::allocator<char>> &)'
        with
        [
            Char=char
        ]
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/environment.hpp(361): note: see reference to function template instantiation 'boost::iterators::transform_iterator<boost::process::detail::make_entry<Char,boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>>,Char **,boost::process::detail::entry<Char,boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>>,boost::process::detail::entry<Char,boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>>> boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>::find(const std::basic_string<char,std::char_traits<char>,std::allocator<char>> &)' being compiled
        with
        [
            Char=char
        ]
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/environment.hpp(632): note: see reference to class template instantiation 'boost::process::basic_environment_impl<Char,boost::process::detail::windows::basic_environment_impl>' being compiled
        with
        [
            Char=char
        ]
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/env.hpp(176): note: see reference to class template instantiation 'boost::process::basic_environment<char>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/env.hpp(183): note: see reference to class template instantiation 'boost::process::detail::env_init<char>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/relationship.hpp(595): note: see reference to class template instantiation 'boost::asio::execution::detail::relationship_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/outstanding_work.hpp(597): note: see reference to class template instantiation 'boost::asio::execution::detail::outstanding_work_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/occupancy.hpp(163): note: see reference to class template instantiation 'boost::asio::execution::detail::occupancy_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/mapping.hpp(764): note: see reference to class template instantiation 'boost::asio::execution::detail::mapping_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/context.hpp(170): note: see reference to class template instantiation 'boost::asio::execution::detail::context_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/bulk_guarantee.hpp(852): note: see reference to class template instantiation 'boost::asio::execution::detail::bulk_guarantee_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/blocking_adaptation.hpp(787): note: see reference to class template instantiation 'boost::asio::execution::detail::blocking_adaptation_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/asio/execution/blocking.hpp(998): note: see reference to class template instantiation 'boost::asio::execution::detail::blocking_t<0>' being compiled
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/environment.hpp(266): warning C4267: 'initializing': conversion from 'size_t' to 'int', possible loss of data
C:\Miniconda37-x64\envs\arrow\Library\include\boost/process/environment.hpp(266): warning C4267: 'initializing': conversion from 'size_t' to 'const int', possible loss of data
[183/351] Building CXX object src\arrow\flight\CMakeFiles\arrow_flight_shared.dir\Unity\unity_1_cxx.cxx.obj
ninja: build stopped: subcommand failed.
(arrow) C:\projects\arrow\cpp\build>set lastexitcode=1 
{noformat}"	ARROW	Resolved	3	1	6256	pull-request-available
13313691	[GLib][CUDA] Add support for dictionary memo on reading record batch from buffer	This is a follow up task for ARROW-8927.	ARROW	Resolved	3	4	6256	pull-request-available
13486519	[Dev][Archery][Crossbow] Show body from server on upload error	"https://app.travis-ci.com/github/ursacomputing/crossbow/builds/256719193#L8074 failed with:

{quote}
curl: (22) The requested URL returned error: 422 
{quote}

It's helpful if we get more details on error."	ARROW	Closed	3	4	6256	pull-request-available
13290957	[GLib] Build error with configure	This is introduced by ARROW-7444.	ARROW	Resolved	3	1	6256	pull-request-available
13224471	[Release][Rust] arrow-testing is missing in verification script	"{noformat}
failures:

---- execution::projection::tests::project_first_column stdout ----
thread 'execution::projection::tests::project_first_column' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5
note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace.

---- execution::aggregate::tests::max_f64_group_by_string stdout ----
thread 'execution::aggregate::tests::max_f64_group_by_string' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5

---- execution::aggregate::tests::test_min_max_sum_f64_group_by_uint32 stdout ----
thread 'execution::aggregate::tests::test_min_max_sum_f64_group_by_uint32' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5

---- execution::aggregate::tests::min_f64_group_by_string stdout ----
thread 'execution::aggregate::tests::min_f64_group_by_string' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5


failures:
    execution::aggregate::tests::max_f64_group_by_string
    execution::aggregate::tests::min_f64_group_by_string
    execution::aggregate::tests::test_min_max_sum_f64_group_by_uint32
    execution::projection::tests::project_first_column

test result: FAILED. 36 passed; 4 failed; 0 ignored; 0 measured; 0 filtered out

error: test failed, to rerun pass '-p datafusion --lib'
{noformat}"	ARROW	Resolved	3	1	6256	pull-request-available
13445505	[C++][FlightRPC] Don't enforcing static link with static GoogleTest for arrow_flight_testing	The link problem was fixed by ARROW-16588.	ARROW	Resolved	3	4	6256	pull-request-available
13441947	[Packaging][RPM] Artifacts pattern list is outdated	This is a follow up of ARROW-15631.	ARROW	Resolved	3	4	6256	pull-request-available
13392898	[C++] Mingw-w64 + Clang (lld) doesn't support --version-script	"See also: https://github.com/msys2/MINGW-packages/pull/9255
"	ARROW	Resolved	3	4	6256	pull-request-available
13388466	[GLib][CI] Require gobject-introspection 3.4.5 or later	It's needed for Flight tests.	ARROW	Resolved	3	4	6256	pull-request-available
13239752	[C++] get_apache_mirror.py doesn't work with Python 3.5	"{noformat}
% python3 --version
Python 3.5.3
% python3 cpp/build-support/get_apache_mirror.py 
Traceback (most recent call last):
  File ""cpp/build-support/get_apache_mirror.py"", line 31, in <module>
    print(json.loads(suggested_mirror)['preferred'])
  File ""/usr/lib/python3.5/json/__init__.py"", line 312, in loads
    s.__class__.__name__))
TypeError: the JSON object must be str, not 'bytes'
{noformat}

Debian stretch ships Python 3.5 as python3."	ARROW	Resolved	3	1	6256	pull-request-available
13488261	[Release][Dev] Automate running binaries/wheels verifications	"We have a script (02-source.sh) that runs source verifications.
But we don't have a script that runs binaries/wheels verifications yet."	ARROW	Resolved	3	4	6256	pull-request-available
13473961	[Crossbow] Outdated artifact patterns for certain linux jobs	"almalinux-8-arm64 and almalinux-9-arm64:
{code}
                  arrow-flight-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
             arrow-flight-glib-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
               arrow-flight-glib-doc-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
              arrow-flight-sql-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
         arrow-flight-sql-glib-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
           arrow-flight-sql-glib-doc-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-flight-glib-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
        arrow[0-9]+-flight-glib-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
   arrow[0-9]+-flight-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
             arrow[0-9]+-flight-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-flight-sql-glib-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
    arrow[0-9]+-flight-sql-glib-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-flight-sql-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
         arrow[0-9]+-flight-sql-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
                             arrow-glib-devel-9.0.0-1.el8.aarch64.rpm [     OK]
                               arrow-glib-doc-9.0.0-1.el8.aarch64.rpm [     OK]
                   arrow9-glib-libs-debuginfo-9.0.0-1.el8.aarch64.rpm [     OK]
                             arrow9-glib-libs-9.0.0-1.el8.aarch64.rpm [     OK]
                        arrow9-libs-debuginfo-9.0.0-1.el8.aarch64.rpm [     OK]
                                  arrow9-libs-9.0.0-1.el8.aarch64.rpm [     OK]
                           arrow-python-devel-9.0.0-1.el8.aarch64.rpm [     OK]
           arrow-python-flight-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-python-flight-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
      arrow[0-9]+-python-flight-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
{code}


centos-7-amd64
{code}
                  arrow-python-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
             arrow[0-9]+-python-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
{code}

centos-8-arm64 and centos-9-arm64:
{code}
                 arrow-flight-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
             arrow-flight-glib-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
               arrow-flight-glib-doc-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
              arrow-flight-sql-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
         arrow-flight-sql-glib-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
           arrow-flight-sql-glib-doc-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-flight-glib-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
        arrow[0-9]+-flight-glib-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
   arrow[0-9]+-flight-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
             arrow[0-9]+-flight-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-flight-sql-glib-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
    arrow[0-9]+-flight-sql-glib-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-flight-sql-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
         arrow[0-9]+-flight-sql-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
                             arrow-glib-devel-9.0.0-1.el8.aarch64.rpm [     OK]
                               arrow-glib-doc-9.0.0-1.el8.aarch64.rpm [     OK]
                   arrow9-glib-libs-debuginfo-9.0.0-1.el8.aarch64.rpm [     OK]
                             arrow9-glib-libs-9.0.0-1.el8.aarch64.rpm [     OK]
                        arrow9-libs-debuginfo-9.0.0-1.el8.aarch64.rpm [     OK]
                                  arrow9-libs-9.0.0-1.el8.aarch64.rpm [     OK]
                           arrow-python-devel-9.0.0-1.el8.aarch64.rpm [     OK]
           arrow-python-flight-devel-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
arrow[0-9]+-python-flight-libs-debuginfo-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
      arrow[0-9]+-python-flight-libs-9.0.0-1.[a-z0-9]+.[a-z0-9_]+.rpm [MISSING]
{code}

ubuntu-bionic-amd64 / ubuntu-bionic-arm64:
{code}
                            libarrow-python-dev_9.0.0-1_[a-z0-9]+.deb [MISSING]
                     libarrow-python-flight-dev_9.0.0-1_[a-z0-9]+.deb [MISSING]
             libarrow-python-flight900-dbgsym_9.0.0-1_[a-z0-9]+.d?deb [MISSING]
                      libarrow-python-flight900_9.0.0-1_[a-z0-9]+.deb [MISSING]
                    libarrow-python900-dbgsym_9.0.0-1_[a-z0-9]+.d?deb [MISSING]
                             libarrow-python900_9.0.0-1_[a-z0-9]+.deb [MISSING]
{code}

cc [~kou]"	ARROW	Resolved	3	1	6256	pull-request-available
13437567	[C++][Gandiva] Add support for LLVM 14	Ubuntu 22.04 ships LLVM 14.	ARROW	Resolved	3	4	6256	pull-request-available
13270584	[CI] Homebrew formula is failed by openssl formula name update	"https://travis-ci.org/ursa-labs/crossbow/builds/616575964#L3501

{noformat}
Error: 2 problems in 1 formula detected
apache-arrow:
  * Dependency 'openssl' is an alias; use the canonical name 'openssl@1.1'.
The command ""brew audit $ARROW_FORMULA"" failed and exited with 1 during .
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13443561	[C++][Parquet] parquet::Statistics::Equals() with minmax returns wrong result	For example, the same statistics that doesn't minmax always returns {{false}} with {{statistics.Equals(statistics)}}.	ARROW	Resolved	3	4	6256	pull-request-available
13419588	[GLib] Add arrow::compute::RoundOptions bindings	Currently it is not possible to pass options to `round`/`round_to_multiple` compute functions	ARROW	Resolved	4	4	6256	pull-request-available
13477624	[Release][Packaging] Make binary uploader reusable from datafusion-c	"I want to reuse APT/Yum repositories at https://apache.jfrog.io/artifactory/arrow/ for https://github.com/datafusion-contrib/datafusion-c .

There is no objection on the mailing list:
""Can we add out of ASF packages to our APT/Yum repositories?""
https://lists.apache.org/thread/f47nmpmmydmr9613jty21sgbv83pr9tq"	ARROW	Resolved	3	4	6256	pull-request-available
13503863	[Packaging][RPM][Gandiva] Failed to link on AlmaLinux 9 	"https://github.com/ursacomputing/crossbow/actions/runs/3502784911/jobs/5867407921#step:6:4748

{noformat}
FAILED: gandiva-glib/Gandiva-1.0.gir 
env PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig:/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/meson-uninstalled /usr/bin/g-ir-scanner --quiet --no-libtool --namespace=Gandiva --nsversion=1.0 --warn-all --output gandiva-glib/Gandiva-1.0.gir --c-include=gandiva-glib/gandiva-glib.h --warn-all --include-uninstalled=./arrow-glib/Arrow-1.0.gir -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/gandiva-glib -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/gandiva-glib -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/. -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/. -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/../cpp/redhat-linux-build/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../cpp/redhat-linux-build/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/../cpp/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../cpp/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/. -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/. -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/../cpp/redhat-linux-build/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../cpp/redhat-linux-build/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/../cpp/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../cpp/src --filelist=/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/gandiva-glib/libgandiva-glib.so.1100.0.0.p/Gandiva_1.0_gir_filelist --include=Arrow-1.0 --symbol-prefix=ggandiva --identifier-prefix=GGandiva --pkg-export=gandiva-glib --cflags-begin -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/. -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/. -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/../cpp/redhat-linux-build/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../cpp/redhat-linux-build/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/../cpp/src -I/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../cpp/src -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include -I/usr/include/sysprof-4 -I/usr/include/gobject-introspection-1.0 --cflags-end --add-include-path=/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/arrow-glib --add-include-path=/usr/share/gir-1.0 -L/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/gandiva-glib --library gandiva-glib -L/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/arrow-glib -L/build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../../cpp/redhat-linux-build/release --extra-library=gobject-2.0 --extra-library=glib-2.0 --extra-library=girepository-1.0 --sources-top-dirs /build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/ --sources-top-dirs /build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/ --warn-error
/usr/bin/ld: /build/rpmbuild/BUILD/apache-arrow-11.0.0.dev130/c_glib/build/../../cpp/redhat-linux-build/release/libgandiva.so.1100: undefined reference to `std::__glibcxx_assert_fail(char const*, int, char const*, char const*)'
collect2: error: ld returned 1 exit status
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13309976	[GLib] Add support for building Apache Arrow Datasets GLib with non-installed Apache Arrow Datasets	"It's required for packaging: https://travis-ci.org/github/ursa-labs/crossbow/builds/695595159

{noformat}
  CXX      libarrow_dataset_glib_la-scanner.lo
scanner.cpp:24:33: fatal error: arrow/util/iterator.h: No such file or directory
 #include <arrow/util/iterator.h>
                                 ^
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13498030	[C++][CMake] Add support for x64 for CMAKE_SYSTEM_PROCESSOR	"vcpkg uses {{x64}}:

https://vcpkg.readthedocs.io/en/latest/users/triplets/

bq. Valid options are x86, x64, arm, arm64 and wasm32."	ARROW	Resolved	3	4	6256	pull-request-available
13275102	[C++] Can't build with g++ 5.4.0 on Ubuntu 16.04	"Full log: https://circleci.com/gh/ursa-labs/crossbow/6109

Formatted error message:

{noformat}
FAILED: /usr/bin/ccache /usr/lib/ccache/g++ \
  -DARROW_JEMALLOC \
  -DARROW_JEMALLOC_INCLUDE_DIR="""" \
  -DARROW_USE_GLOG \
  -DARROW_USE_SIMD \
  -DARROW_WITH_BOOST_FILESYSTEM \
  -DARROW_WITH_BROTLI \
  -DARROW_WITH_BZ2 \
  -DARROW_WITH_LZ4 \
  -DARROW_WITH_SNAPPY \
  -DARROW_WITH_ZLIB \
  -DGTEST_LINKED_AS_SHARED_LIBRARY=1 \
  -DURI_STATIC_BUILD \
  -isystem /arrow/cpp/thirdparty/flatbuffers/include \
  -isystem boost_ep-prefix/src/boost_ep \
  -isystem thrift_ep/src/thrift_ep-install/include \
  -isystem /arrow/cpp/thirdparty/protobuf_ep-install/include \
  -isystem jemalloc_ep-prefix/src \
  -isystem googletest_ep-prefix/src/googletest_ep/include \
  -isystem rapidjson_ep/src/rapidjson_ep-install/include \
  -isystem /arrow/cpp/thirdparty/hadoop/include \
  -Isrc \
  -I/arrow/cpp/src \
  -I/arrow/cpp/src/generated \
  -fdiagnostics-color=always \
  -ggdb \
  -O0 \
  -Wall \
  -Wno-conversion \
  -Wno-sign-conversion \
  -Wno-unused-variable \
  -Werror \
  -Wno-attributes \
  -msse4.2 \
  -g \
  -fPIE \
  -pthread \
  -std=gnu++11 \
  -MMD \
  -MT src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o \
  -MF src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o.d \
  -o src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o \
  -c /arrow/cpp/src/arrow/dataset/dataset_test.cc
/arrow/cpp/src/arrow/dataset/dataset_test.cc: In member function
  'virtual void arrow::dataset::TestSchemaUnification_SelectStar_Test::TestBody()':
/arrow/cpp/src/arrow/dataset/dataset_test.cc:531:3: error:
  converting to '
    std::tuple<nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int> >'
  from initializer list would use explicit constructor '
    constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...)
      [with
       _UElements = {
         int,
         int,
         const nonstd::optional_lite::nullopt_t&,
         const nonstd::optional_lite::nullopt_t&,
         int,
         int
       };
       <template-parameter-2-2> = void;
       _Elements = {
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>
       }]'
   };
   ^
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13111171	[GLib] Go example in test suite is broken	"see e.g. https://travis-ci.org/apache/arrow/jobs/290802235

{code}
~/build/apache/arrow/c_glib/example/go ~/build/apache/arrow/c_glib ~/build/apache/arrow
$GOPATH/bin/gir-generator	\
	-o $GOPATH/src/gir/arrow-1.0	\
	-config arrow-1.0/config.json	\
	arrow-1.0/arrow.go.in
go build -o read-batch read-batch.go
# gir/glib-2.0
/home/travis/gopath/src/gir/glib-2.0/glib.go:166: cannot convert (*(*[999999]*C.uint8_t)(unsafe.Pointer(ret1)))[i0] (type *C.uint8_t) to type uint8
/home/travis/gopath/src/gir/glib-2.0/glib.go:166: cannot use uint8((*(*[999999]*C.uint8_t)(unsafe.Pointer(ret1)))[i0]) (type uint8) as type *uint8 in assignment
make: *** [read-batch] Error 2
{code}"	ARROW	Resolved	3	1	6256	ci-failure, pull-request-available
13448064	[C++] Drop support for bundled Thrift < 0.13	We bundle Thrift 0.16.0. Users can use other version but will not use 0.13 or earlier.	ARROW	Resolved	3	4	6256	pull-request-available
13477283	[CI][Java] Move away from Debian 9	Java JNI CI builds currently use Debian 9, which is EOL (see https://www.debian.org/releases/stretch/index.en.html). We should switch that build to Debian 10 or 11.	ARROW	Resolved	3	5	6256	pull-request-available
13272600	[CI][Rust] Remove duplicated nightly job	"We run the same test on each push with GitHub Actions.
"	ARROW	Resolved	3	4	6256	pull-request-available
13284424	[Release] Remove SSH keys for internal use	"{{dev/release/binary/id_rsa*}} SSH keys are only used to login to local Docker container for
releasing binary artifacts. They aren't used over network. So putting
them to this public repository is safe.

But there are people who report ""they may be danger"" to us. We should
remove them from this repository and generate them locally instead of
describing why they aren't danger to reporters."	ARROW	Resolved	4	4	6256	pull-request-available
13469263	[Packaging][RPM] Disable GCS for Amazon Linux 2	"Because it hits an RPM bug: https://bugzilla.redhat.com/show_bug.cgi?id=304121

It was fixed in RPM 4.14.0 by https://github.com/rpm-software-management/rpm/commit/88989572fff1f31e0c4f972a6895585e4742ef4b but Amazon Linux 2 ships 4.11.3.

This is caused by the latest Google Cloud C++ SDK updated by ARROW-16510."	ARROW	Resolved	3	4	6256	pull-request-available
13282088	[C#] Date32 test depends on system timezone	"The following failure was occurred on 2020-01-29:08:47:33+09:00:

{noformat}
Starting test execution, please wait...
[xUnit.net 00:00:00.53]     Apache.Arrow.Tests.Date32ArrayTests+Set.SetAndGet [FAIL]
  X Apache.Arrow.Tests.Date32ArrayTests+Set.SetAndGet [19ms]
  Error Message:
   Assert.Equal() Failure
Expected: 2020-01-28T00:00:00.0000000
Actual:   2020-01-27T00:00:00.0000000
  Stack Trace:
     at Apache.Arrow.Tests.Date32ArrayTests.Set.SetAndGet() in /tmp/arrow-0.16.0.mrKfP/apache-arrow-0.16.0/csharp/test/Apache.Arrow.Tests/Date32ArrayTests.cs:line 38
{noformat}"	ARROW	Resolved	3	4	6256	pull-request-available
13474013	[Release][CI] Exercise the binary verification tasks on a nightly basis	"As discussed during the Release of the 9.0.0 Release the binary verification tasks are not exercised on a nightly basis. This caused some required fixes for the Release which should have been caught prior to the release.

Link to the relevant [discussion on Zulip|https://ursalabs.zulipchat.com/#narrow/stream/180245-dev/topic/Release.209.2E0.2E0.20status/near/291181703]

We have to take into account that we would need to download the binaries from crossbow instead of artifactory and only trigger the verification tasks after the binaries build tasks have finished."	ARROW	Resolved	2	4	6256	pull-request-available
13191435	[Packaging] Nightly tests for docker-compose images	We need to ensure that the developer containers are working.	ARROW	Resolved	3	4	6306	pull-request-available
13159777	[Python] Segmentation fault when writing empty ListType column to Parquet	"Context Is the following: I am currently dealing with sparse column serialization in parquet. In some cases, many lines are empty I can also have columns containing only empty lists.
However I got a segmentation fault when I try to write in parquet thoses columns filled only with empty lists.

Here is a simple code snipet reproduces the segmentation fault I had:


{noformat}

In [1]: import pyarrow as pa

In [2]: import pyarrow.parquet as pq

In [3]: pa_ar = pa.array([[],[]],pa.list_(pa.int32()))

In [4]: table = pa.Table.from_arrays([pa_ar],[""test""])

In [5]: pq.write_table(
   ...:     table=table,
   ...:     where=""test.parquet"",
   ...:     compression=""snappy"",
   ...:     flavor=""spark""
   ...: )
Segmentation fault

{noformat}

May I have it fixed?


Best

Jacques"	ARROW	Resolved	3	1	6306	parquet, pull-request-available
13322506	[Packaging][Python] Update wheel dependency files	Should not include pandas and keras peprocessing.	ARROW	Resolved	3	2	6306	pull-request-available
13172070	[Packaging] Fix artifact name matching for conda forge packages	"With the recently introduced conda recipe changes and version pinning the artifact names are not known before the build, they contain a hash too.

`arrow-cpp-0.9.1.dev366-py36_0.tar.bz2` => `arrow-cpp-0.9.1.dev366-py36_vc14h12fa3ca_0.tar.bz2`

We need to either remove the pins or use pattern matching in the status and sign commands."	ARROW	Resolved	3	4	6306	pull-request-available
13423589	[C++][Release] Missing libsqlite-dev from the verification docker images	See build error https://github.com/ursacomputing/crossbow/runs/4870407487?check_suite_focus=true#step:5:4852	ARROW	Resolved	3	1	6306	pull-request-available
13319729	[Dev][Release] Bump next snapshot versions to 2.0.0	The upcoming major release will have version 2.0.0, update the hardcoded version numbers.	ARROW	Resolved	3	4	6306	pull-request-available
13407283	[Packaging][Python] Python 3.9 installation fails in macOS wheel build	Due to a trailing comma in the script https://github.com/ursacomputing/crossbow/runs/3938860251#step:8:19	ARROW	Resolved	3	1	6306	pull-request-available
13179029	[INTEGRATION] Fix spark and hdfs dockerfiles	Spark and HDFS integration tests are failing to build.	ARROW	Resolved	3	3	6306	pull-request-available
13286536	[Packaging] Temporarily disable artifact uploading until we fix the deployment issues	This will filter out the false negatives from the nightly build report until we fix the deployment errors in https://github.com/apache/arrow/pull/6458	ARROW	Resolved	3	3	6306	pull-request-available
13130737	[Python] Test against Pandas master	We have seen recently a lot of breakage with Pandas master. This is an annoyance to our users and should already break in our builds instead of their chains. There is no need to add another entry to matrix, just in one of them to re-run the tests with the Pandas master after they ran successfully.	ARROW	Resolved	3	4	6306	pull-request-available
13307774	[Python][Documentation] Pyarrow documentation for pip nightlies references 404'd location	"The pyarrow documentation gives to options for nightly builds, one for use with anaconda, one for use with pip. While the anaconda command works, the command for pip sends users to

[https://repo.fury.io/arrow-nightlies/,] a url which 404s. Sphinx docs need updated for correct url of gemfury.com/arrow-nightlies/"	ARROW	Resolved	4	3	6306	pull-request-available
13295860	[CI] Fix C# workflow file syntax	"The github actions expression requires the enclosing ""${{ }}"""	ARROW	Resolved	3	3	6306	pull-request-available
13407819	[Packaging][Python] Disable windows wheel testing for python 3.6	"Two layers of the official python 3.6 windows image are not available for download.
Docker pull returns with unexpected status resolving reader: 403 Forbidden.

While this is a transient error, it blocks the release process.

See build log https://github.com/ursacomputing/crossbow/runs/3967178530?check_suite_focus=true#step:8:368"	ARROW	Resolved	3	1	6306	pull-request-available
13302151	[Python] Switch to VS2017 in the windows wheel builds	Since the recent conda-forge compiler migrations the wheel builds are failing https://mail.google.com/mail/u/0/#label/ARROW/FMfcgxwHNCsqSGKQRMZxGlWWsfmGpKdC	ARROW	Closed	3	4	6306	pull-request-available
13309900	[C++] Add sum/mean kernels for Boolean type	See https://github.com/apache/arrow/pull/7308 (ARROW-6978)	ARROW	Resolved	3	4	6306	pull-request-available
13370150	[Dev][Packaging] Move Crossbow to Archery	Crossbow should be part of the archery toolset enabling tighter integration and easier installation.	ARROW	Resolved	3	4	6306	pull-request-available
13210977	[Packaging] Fix failing OSX clang conda forge builds	Log: https://travis-ci.org/kszucs/crossbow/builds/482871537	ARROW	Resolved	3	1	6306	pull-request-available
13315984	[Python] Support pickling of Scalars	"Scalars don't currently support pickling.

Could this be as implemented with {{Scalar, (self.type, self.as_py())}}?"	ARROW	Resolved	3	4	6306	pull-request-available
13290897	[Dev] Implement Comment bot via Github actions	Ala {{@ursabot}}.	ARROW	Resolved	3	4	6306	pull-request-available
13304352	[Python] pyarrow schema.empty_table() does not preserve nullability of fields	"Introduced by PR: [https://github.com/apache/arrow/pull/2589]

 

When a field in a schema is marked as not-nullable, calling empty_table() on the schema returns a table with nullable fields.

 

reproduction
{code:java}
>>> import pyarrow as pa
>>> s = pa.schema([pa.field('a', pa.int64(), nullable=False), pa.field('b', pa.int64())])

>>> s

a: int64 not null
b: int64

>>> e = s.empty_table()
>>> e

pyarrow.Table
a: int64
b: int64

>>> e.schema

a: int64
b: int64

>>> assert s == e.schema
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
AssertionError

{code}"	ARROW	Resolved	3	1	6306	pull-request-available
13240820	[Python] Missing pandas pytest markers from test_parquet.py	Resulting failures in the nopandas tests: https://circleci.com/gh/ursa-labs/crossbow/123	ARROW	Resolved	3	1	6306	pull-request-available
13244692	[Python] Bundle arrow's LICENSE with the wheels	"Guide to bundle LICENSE files with the wheels: https://wheel.readthedocs.io/en/stable/user_guide.html#including-license-files-in-the-generated-wheel-file

We also need to ensure, that all thirdparty dependencies' license are attached to it, especially because we're statically linking multiple 3rdparty dependencies, and for example uriparser is missing from the LICENSE file.

cc [~wesmckinn]"	ARROW	Resolved	3	3	6306	pull-request-available
13284694	[Release] Post release task for updating the documentations	Use the ubuntu-docs docker container to build the documentations and add a post-release script.	ARROW	Resolved	3	4	6306	pull-request-available
13378193	[CI] Fix arguments in Conda Windows builds	"azure-conda-win-vs2017-py36-r36 (and azure-conda-win-vs2017-py37-r40, azure-conda-win-vs2017-py38, azure-conda-win-vs2017-py39) are affected by this new error

_Error: Got unexpected extra arguments (D:\bld\win-64\arrow-cpp-proc-3.1.0.dev834-cpu.tar.bz2 D:\bld\win-64\parquet-cpp-1.5.1-h57928b3_0.tar.bz2 D:\bld\win-64\pyarrow-3.1.0.dev834-py39hf9247be_0_cpu.tar.bz2 D:\bld\win-64\pyarrow-tests-3.1.0.dev834-py39hf9247be_0_cpu.tar.bz2)_"	ARROW	Resolved	3	3	6306	pull-request-available
13292467	[CI] Cmake 3.2 nightly build fails	In the LLVM 8 Migration PR wget was [removed|https://github.com/apache/arrow/commit/58ec1bc3984b8453011ba6ca45c727ff6ceed78c#diff-0a4bf63085865017969bbbdac6f66880L29] so the build is [missing|https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-03-18-0-circle-test-ubuntu-18.04-cpp-cmake32] wget.	ARROW	Resolved	3	1	6306	pull-request-available
13127267	[Python/C++] Add option to Array.from_pandas and pyarrow.array to perform unsafe casts	Per mailing list thread	ARROW	Resolved	3	4	6306	pull-request-available
13272217	[Python] Expose HDFS implementation for pyarrow.fs	There's a C++ implementation for HDFS (see {{arrow/filesystem/hdfs.h}}) but it's not exposed from Python.	ARROW	Resolved	3	4	6306	pull-request-available
13237295	[Python] Create FileSystem bindings	Now that we have a C++ filesystem API, it should be usable from Python as well.	ARROW	Resolved	3	4	6306	filesystem, pull-request-available
13298378	[C++] Prefer the original mirrors for the bundled thirdparty dependencies	Currently the ursa-labs mirrors have the highest priority but we should consider the original mirrors more reliable.	ARROW	Resolved	3	3	6306	pull-request-available
13260141	[Packaging][Python] Missing pytest dependency from conda and wheel builds	"Multiple python packaging nightlies are failing:

{code}
Failed Tasks:
- conda-osx-clang-py36:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-osx-clang-py36
- conda-osx-clang-py37:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-osx-clang-py37
- conda-win-vs2015-py36:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-win-vs2015-py36
- wheel-manylinux1-cp27mu:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-wheel-manylinux1-cp27mu
- conda-linux-gcc-py27:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-linux-gcc-py27
- wheel-osx-cp27m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-wheel-osx-cp27m
- docker-spark-integration:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-circle-docker-spark-integration
- wheel-win-cp35m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-appveyor-wheel-win-cp35m
- conda-win-vs2015-py37:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-win-vs2015-py37
- conda-linux-gcc-py37:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-linux-gcc-py37
- wheel-manylinux2010-cp27mu:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-wheel-manylinux2010-cp27mu
- conda-linux-gcc-py36:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-linux-gcc-py36
- wheel-win-cp37m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-appveyor-wheel-win-cp37m
- wheel-win-cp36m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-appveyor-wheel-win-cp36m
- gandiva-jar-osx:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-gandiva-jar-osx
- conda-osx-clang-py27:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-osx-clang-py27
{code}

Because of missing, recently introduced pytest-lazy-fixture test dependency:
{code}
+ pytest -m 'not requires_testing_data' --pyargs pyarrow
============================= test session starts ==============================
platform linux -- Python 3.7.3, pytest-5.2.0, py-1.8.0, pluggy-0.13.0
hypothesis profile 'default' ->
database=DirectoryBasedExampleDatabase('$SRC_DIR/.hypothesis/examples')
rootdir: $SRC_DIR
plugins: hypothesis-4.38.1
collected 1437 items / 1 errors / 3 deselected / 5 skipped / 1428 selected

==================================== ERRORS ====================================
______________________ ERROR collecting tests/test_fs.py _______________________
../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol/lib/python3.7/site-packages/pyarrow/tests/test_fs.py:91:
in <module>
pytest.lazy_fixture('localfs'),
E AttributeError: module 'pytest' has no attribute 'lazy_fixture'
=============================== warnings summary ===============================
$PREFIX/lib/python3.7/site-packages/_pytest/mark/structures.py:324
$PREFIX/lib/python3.7/site-packages/_pytest/mark/structures.py:324:
PytestUnknownMarkWarning: Unknown pytest.mark.s3 - is this a typo? You
can register custom marks to avoid this warning - for details, see
https://docs.pytest.org/en/latest/mark.html
PytestUnknownMarkWarning,

-- Docs: https://docs.pytest.org/en/latest/warnings.html
!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!
{code}"	ARROW	Resolved	3	4	6306	pull-request-available
13244084	[Python][Packaging] Bundle uriparser.dll in windows wheels 	"The windows nightly wheel builds are failing: https://ci.appveyor.com/project/Ursa-Labs/crossbow/builds/25688922 probably caused by 88fcb09, but it's hard to tell because of the error message ""ImportError: DLL load failed: The specified module could not be found."" is not very descriptive.

Theoretically it shouldn't affect the 0.14 release because 88fcb09 was added afterwards."	ARROW	Resolved	3	1	6306	pull-request-available
13255974	[Packaging][Python] Flight failing in OSX Python wheel builds	"See example failure

https://travis-ci.org/ursa-labs/crossbow/builds/583167489?utm_source=github_status&utm_medium=notification

{code}
[ 30%] Generating Flight.pb.cc, Flight.pb.h, Flight.grpc.pb.cc, Flight.grpc.pb.h
dyld: Library not loaded: /usr/local/opt/gperftools/lib/libprofiler.0.dylib
  Referenced from: /usr/local/Cellar/grpc/1.23.0_2/bin/grpc_cpp_plugin
  Reason: image not found
--grpc_out: protoc-gen-grpc: Plugin killed by signal 6.
make[2]: *** [src/arrow/flight/Flight.pb.cc] Error 1
make[2]: *** Deleting file `src/arrow/flight/Flight.pb.cc'
make[1]: *** [src/arrow/flight/CMakeFiles/flight_grpc_gen.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
{code}

I suggest disabling Flight in the wheel builds"	ARROW	Resolved	1	4	6306	pull-request-available
13387262	[Java][CI] Consistent timeout in the Java JNI build	The JNI build consistently gets killed on github actions.	ARROW	Resolved	3	1	6306	pull-request-available
13328207	[C++] Workaround to force find AWS SDK to look for shared libraries 	Since the recent conda forge feedstock update find aws sdk fails to locate the proper aws sdk cmake files: [https://github.com/apache/arrow/runs/1131824712#step:8:3700]	ARROW	Resolved	3	3	6306	pull-request-available
13206238	[Packaging] Missing glog dependency from arrow-cpp conda recipe	Follow up for https://github.com/apache/arrow/commit/700bd40afab973d00229a43dff5ce764ed996873	ARROW	Resolved	3	1	6306	pull-request-available
13281377	[Python][Dataset] Add bindings for the DatasetFactory	depends on https://issues.apache.org/jira/browse/ARROW-7380	ARROW	Resolved	3	4	6306	pull-request-available
13297684	[C++] Fail to compile aggregate_test.cc on Ubuntu 16.04	See build log https://app.circleci.com/pipelines/github/ursa-labs/crossbow/31122/workflows/b250d378-52a8-4d15-9909-96474fa38482/jobs/10840	ARROW	Resolved	3	1	6306	pull-request-available
13103674	[Python] Set up + document nightly conda builds for macOS	It's already been great to be able to test the nightlies on Linux in conda; it would be great to be able to do the same on macOS	ARROW	Resolved	3	4	6306	nightly
13327731	[CI] Disable Sphinx and API documentation build since it takes 6 hours on master	The reason is unclear from the build logs, but I suggest to turn it off until it gets resolved.	ARROW	Resolved	3	3	6306	pull-request-available
13254506	[OSX][Python][Wheel] Turn off ORC feature in the wheel building scripts	See https://travis-ci.org/ursa-labs/crossbow/builds/580191793, https://travis-ci.org/ursa-labs/crossbow/builds/580192018, https://travis-ci.org/ursa-labs/crossbow/builds/580192255. Something fails while doing something with thrift, it appears. 	ARROW	Resolved	1	1	6306	nightly, pull-request-available, wheel
13293271	[Packaging] Document the available nightly wheels and conda packages	"The packaging scripts are uploading the artifacts to package manager specific hosting services like Anaconda and Gemfury. We should document this in a form which conforms the [ASF Policy|https://www.apache.org/dev/release-distribution.html#unreleased].

For more information see the conversation at https://github.com/apache/arrow/pull/6669#issuecomment-601947006"	ARROW	Resolved	3	4	6306	pull-request-available
13237937	[Packaging][Documentation] Comments out of date in python/manylinux1/build_arrow.sh	"The script has this comment:
{code:java}
# Usage:
#   docker run --rm -v $PWD:/io arrow-base-x86_64 /io/build_arrow.sh
{code}

However, I get:
{code}
Unable to find image 'arrow-base-x86_64:latest' locally
docker: Error response from daemon: pull access denied for arrow-base-x86_64, repository does not exist or may require 'docker login'.
See 'docker run --help'.
{code}"	ARROW	Resolved	3	1	6306	pull-request-available, wheel
13251061	"[Python] Expose ""enable_buffered_stream"" option from parquet::ReaderProperties in pyarrow.parquet.read_table"	See also PARQUET-1370	ARROW	Resolved	3	4	6306	pull-request-available
13298421	[Packaging][Python] Windows py35 wheel build fails because of boost	See build log https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-04-14-3-appveyor-wheel-win-cp35m	ARROW	Resolved	3	1	6306	pull-request-available
13188468	[C++] Add double-conversion to cpp/thirdparty/download_dependencies.sh	For offline builds	ARROW	Resolved	3	4	6306	pull-request-available
13185568	"[C++] Do not hard code the ""v"" part of versions in thirdparty toolchain"	"When I changed Flatbuffers from ""v1.8.0"" to a git hash, it broke the dependency download script. We should move all the version string to versions.txt rather than having some ""v${FOO_URL}"" in ThirdpartyToolchain.cmake"	ARROW	Resolved	3	4	6306	pull-request-available
13185826	[Python] Implement __getitem__ with integers on pyarrow.Column	This would improve interactive usability	ARROW	Resolved	3	4	6306	pull-request-available, usability
13151045	[Python] PyArrow datatypes raise ValueError on equality checks against non-PyArrow objects	"Checking a PyArrow datatype object for equality with non-PyArrow datatypes causes a `ValueError` to be raised, rather than either returning a True/False value, or returning [NotImplemented|https://docs.python.org/3/library/constants.html#NotImplemented] if the comparison isn't implemented.

E.g. attempting to call:
{code:java}
import pyarrow
pyarrow.int32() == 'foo'
{code}
results in:
{code:java}
Traceback (most recent call last):
  File ""types.pxi"", line 1221, in pyarrow.lib.type_for_alias
KeyError: 'foo'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""t.py"", line 2, in <module>
    pyarrow.int32() == 'foo'
  File ""types.pxi"", line 90, in pyarrow.lib.DataType.__richcmp__
  File ""types.pxi"", line 113, in pyarrow.lib.DataType.equals
  File ""types.pxi"", line 1223, in pyarrow.lib.type_for_alias
ValueError: No type alias for foo
{code}
The expected outcome for the above would be for the comparison to return `False`, as that's the general behaviour for comparisons between objects of different types (e.g. `1 == 'foo'` or `object() == 12.4` both return `False`)."	ARROW	Resolved	4	1	6306	beginner, pull-request-available
13285185	[Python] 0.16.0 wheels not compatible with older numpy	"Using python 3.7.5 and numpy 1.14.6, I am unable to import pyarrow 0.16.0 (see below for error). Updating numpy to the most recent version fixes this, and I'm wondering if pyarrow needs update its requirements.txt.

 
{code:java}
➜  ~ ipython
Python 3.7.5 (default, Nov  7 2019, 10:50:52)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.9.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import numpy as npIn [2]: np.__version__
Out[2]: '1.14.6'

In [3]: import pyarrow
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-3-f1048abcb32d> in <module>
----> 1 import pyarrow~/.local/lib/python3.7/site-packages/pyarrow/__init__.py in <module>
     47 import pyarrow.compat as compat
     48
---> 49 from pyarrow.lib import cpu_count, set_cpu_count
     50 from pyarrow.lib import (null, bool_,
     51                          int8, int16, int32, int64,~/.local/lib/python3.7/site-packages/pyarrow/lib.pyx in init pyarrow.lib()ImportError: numpy.core.multiarray failed to import

In [4]: import pyarrow
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-f1048abcb32d> in <module>
----> 1 import pyarrow~/.local/lib/python3.7/site-packages/pyarrow/__init__.py in <module>
     47 import pyarrow.compat as compat
     48
---> 49 from pyarrow.lib import cpu_count, set_cpu_count
     50 from pyarrow.lib import (null, bool_,
     51                          int8, int16, int32, int64,~/.local/lib/python3.7/site-packages/pyarrow/ipc.pxi in init pyarrow.lib()AttributeError: type object 'pyarrow.lib.Message' has no attribute '__reduce_cython__'

{code}"	ARROW	Resolved	1	1	6306	pull-request-available
13383902	[Release] Generate the API docs in ubuntu 20.10	I noticed that the ubuntu build variable is passed as a runtime environment variable to the container: https://github.com/apache/arrow/blob/master/dev/release/post-09-docs.sh#L49	ARROW	Resolved	3	1	6306	pull-request-available
13196550	[Crossbow][Python] Run nightly tests against pandas master	Follow-up of [https://github.com/apache/arrow/pull/2758] and https://github.com/apache/arrow/pull/2755	ARROW	Resolved	3	4	6306	pull-request-available
13274462	[Packaging] Add conda packaging tasks for python 3.8	Conda-forge now supports python 3.8 so we should build the appropriate packages.	ARROW	Resolved	3	4	6306	pull-request-available
13221691	[Python] Keep backward compatibility for ParquetDatasetPiece	"See https://github.com/apache/arrow/commit/f2fb02b82b60ba9a90c8bad6e5b11e37fc3ea9d3#r32722497

and 

https://github.com/dask/dask/pull/4587"	ARROW	Resolved	1	1	6306	parquet, pull-request-available
13292557	[Packaging] Update the conda feedstock files and upload artifacts to Anaconda	"The windows builds were failing, so the feedstock files must be updated.

Under the same hat add support for uploading the produced artifacts to Anaconda labeled as nightly."	ARROW	Resolved	3	4	6306	pull-request-available
13404947	[Python][CI] Add support for python 3.10 	Python 3.10 has just been released, so exercise builds and ship packages for it.	ARROW	Resolved	3	2	6306	pull-request-available
13414592	[CI] Prefer mamba over conda 	Mamba should provide quicker docker image builds compared to conda.	ARROW	Resolved	3	4	6306	pull-request-available
13211809	[C++] Embed precompiled bitcode in the gandiva library	"We were not running the pyarrow tests after installing the manylinux wheels, which can lead to uncaught issues, like: [https://travis-ci.org/kszucs/crossbow/builds/484284104]

"	ARROW	Resolved	3	4	6306	pull-request-available
13319515	[Dev][Release] Use archery's changelog generator when creating release notes for the website 	The previous changelog generation script has been removed.	ARROW	Resolved	3	4	6306	pull-request-available
13160860	[Python] Random schema and data generator for Arrow conversion and Parquet testing	"See discussion in https://github.com/apache/arrow/issues/2067

Being able to generate random complex schemas and corresponding example data sets will help with exercising edge cases in many different parts of the codebase. One practical example: reading and writing nested data to Parquet format"	ARROW	Resolved	3	4	6306	parquet, pull-request-available
13303675	[Python][Packaging] Keep VS2015 with for the windows wheels	The windows wheels needs to be fixed for the release.	ARROW	Resolved	3	1	6306	pull-request-available
13308748	"[Python][C++] Non-deterministic segfault in ""AMD64 MacOS 10.15 Python 3.7"" build"	"I've been seeing this segfault periodically the last week, does anyone have an idea what might be wrong?

https://github.com/apache/arrow/pull/7273/checks?check_run_id=717249862"	ARROW	Closed	3	1	6306	dataset
13383930	[CI] Forward R argument to ubuntu-docs build	The R version set from environment variable is not propagated to the linux-apt-docs.dockerfile	ARROW	Resolved	3	4	6306	pull-request-available
13315656	[C++][Python] Expose MakeArrayFromScalar	Currently there is no efficient way to create a pyarrow array with identical values.	ARROW	Resolved	3	4	6306	pull-request-available
13349197	[CI] Use pip to install crossbow's dependencies for the comment bot	setup-conda action is no longer enabled on github actions, so use an alternative way to install archery bot.	ARROW	Resolved	3	1	6306	pull-request-available
13197221	[Python] Merging Parquet Files - Pandas Meta in Schema Mismatch	"From: https://stackoverflow.com/questions/53214288/merging-parquet-files-pandas-meta-in-schema-mismatch
 
I am trying to merge multiple parquet files into one. Their schemas are identical field-wise but my {{ParquetWriter}} is complaining that they are not. After some investigation I found that the pandas meta in the schemas are different, causing this error.
 
Sample-

{code:python}
import pyarrow.parquet as pq

pq_tables=[]
for file_ in files:
    pq_table = pq.read_table(f'{MESS_DIR}/{file_}')
    pq_tables.append(pq_table)
    if writer is None:
        writer = pq.ParquetWriter(COMPRESSED_FILE, schema=pq_table.schema, use_deprecated_int96_timestamps=True)
    writer.write_table(table=pq_table)
{code}

The error-

{code}
Traceback (most recent call last):
  File ""{PATH_TO}/main.py"", line 68, in lambda_handler
    writer.write_table(table=pq_table)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyarrow/parquet.py"", line 335, in write_table
    raise ValueError(msg)
ValueError: Table schema does not match schema used to create file:
{code}
"	ARROW	Resolved	3	1	6306	parquet, pull-request-available
13236874	[Crossbow] Support writing submitted job definition yaml to a file	In similar fashion like archery benchmark does. Required to consume the command's output from a buildbot build step.	ARROW	Resolved	3	4	6306	pull-request-available
13143215	[Python] Allow subscripting pyarrow.lib.StructValue	"{code:python}
>>> obj
{'x': 42, 'y': True}
>>> type(obj)
pyarrow.lib.StructValue
>>> obj['x']
Traceback (most recent call last):
  File ""<ipython-input-96-93ad3443b3c6>"", line 1, in <module>
    obj['x']
TypeError: 'pyarrow.lib.StructValue' object is not subscriptable
{code}"	ARROW	Resolved	3	5	6306	pull-request-available
13182422	[Packaging] Fix broken nightly package builds introduced with recent cmake changes and orc tests	"See: https://github.com/kszucs/crossbow/branches/all?utf8=%E2%9C%93&query=nightly-32
"	ARROW	Resolved	3	3	6306	pull-request-available
13407469	[C++] Bundled gRPC misses bundled Abseil include path	"{code}
CMake Error in src/arrow/flight/CMakeLists.txt:
  Imported target ""gRPC::grpc++"" includes non-existent path

    ""/tmp/arrow-6.0.0.v1qFD/apache-arrow-6.0.0/cpp/build/absl_ep-install/include""

  in its INTERFACE_INCLUDE_DIRECTORIES.  Possible reasons include:

  * The path was deleted, renamed, or moved to another location.

  * An install or uninstall procedure did not complete successfully.

  * The installation package was faulty and references files it does not
  provide.



CMake Error in src/arrow/flight/CMakeLists.txt:
  Imported target ""gRPC::grpc++"" includes non-existent path

    ""/tmp/arrow-6.0.0.v1qFD/apache-arrow-6.0.0/cpp/build/absl_ep-install/include""

  in its INTERFACE_INCLUDE_DIRECTORIES.  Possible reasons include:

  * The path was deleted, renamed, or moved to another location.

  * An install or uninstall procedure did not complete successfully.

  * The installation package was faulty and references files it does not
  provide.
{code}"	ARROW	Resolved	1	1	6306	pull-request-available
13240482	[Python] Table.from_pydict/from_arrays not using types in specified schema correctly 	"Example with {{from_pydict}} (from https://github.com/apache/arrow/pull/4601#issuecomment-503676534):

{code:python}
In [15]: table = pa.Table.from_pydict(
    ...:     {'a': [1, 2, 3], 'b': [3, 4, 5]},
    ...:     schema=pa.schema([('a', pa.int64()), ('c', pa.int32())]))

In [16]: table
Out[16]: 
pyarrow.Table
a: int64
c: int32

In [17]: table.to_pandas()
Out[17]: 
   a  c
0  1  3
1  2  0
2  3  4
{code}

Note that the specified schema has 1) different column names and 2) has a non-default type (int32 vs int64) which leads to corrupted values.

This is partly due to {{Table.from_pydict}} not using the type information in the schema to convert the dictionary items to pyarrow arrays. But then it is also {{Table.from_arrays}} that is not correctly casting the arrays to another dtype if the schema specifies as such.

Additional question for {{Table.pydict}} is whether it actually should override the 'b' key from the dictionary as column 'c' as defined in the schema (this behaviour depends on the order of the dictionary, which is not guaranteed below python 3.6).
"	ARROW	Resolved	3	1	6306	pull-request-available
13375466	[Python] Pyarrow 4.0.0 dependency numpy 1.19.4 throws errors on Apple silicon/M1 compilation	"Hi team! I've been unable to install older numpy versions (including 1.19.4 as specified here f[or aarch64 machine|https://github.com/apache/arrow/blob/master/python/requirements-wheel-build.txt]s) on my Apple Silicon machine because the build process throws a number of numpy compilation errors. I've been able to successfully install numpy 1.20.2 however - is it possible to bump up the numpy acceptable version number to enable Apple silicon installations?

Thanks!"	ARROW	Closed	4	1	6306	build-failure
13254499	[CI][Crossbow] Nightly conda osx builds fail	"See https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=684, https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=686, https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=688. Reported failure is

{code}
Traceback (most recent call last):
  File ""/usr/local/miniconda/bin/conda-build"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/cli/main_build.py"", line 445, in main
    execute(sys.argv[1:])
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/cli/main_build.py"", line 436, in execute
    verify=args.verify, variants=args.variants)
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/api.py"", line 209, in build
    notest=notest, need_source_download=need_source_download, variants=variants)
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/build.py"", line 2343, in build_tree
    notest=notest,
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/build.py"", line 1407, in build
    create_build_envs(top_level_pkg, notest)
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/build.py"", line 1261, in create_build_envs
    channel_urls=tuple(m.config.channel_urls))
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/environ.py"", line 758, in get_install_actions
    raise DependencyNeedsBuildingError(exc, subdir=subdir)
conda_build.exceptions.DependencyNeedsBuildingError: Unsatisfiable dependencies for platform osx-64: {'libcxx==4.0.1=h579ed51_0.conda'}
##[error]Bash exited with code '1'.
{code}"	ARROW	Resolved	1	1	6306	pull-request-available
13299985	[Release] Fix checksum url in the website post release script	The issue was captured here https://github.com/apache/arrow-site/pull/53#discussion_r411728907	ARROW	Resolved	3	4	6306	pull-request-available
13267691	[CI] Fedora cron jobs are failing because of wrong fedora version	The requested fedora version is 10 (Debian) instead of 29: https://github.com/apache/arrow/runs/299223601	ARROW	Resolved	3	1	6306	pull-request-available
13317111	[Python] Segmentation fault on ChunkedArray.take	"This leads to a segementation fault with the latest conda nigthlies on Python 3.8 / macOS

{code}
import pyarrow as pa
import numpy as np

arr = pa.chunked_array([
  [
    ""m"",
    ""J"",
    ""q"",
    ""k"",
    ""t""
  ],
  [
    ""m"",
    ""J"",
    ""q"",
    ""k"",
    ""t""
  ]
])

indices = np.array([0, 5, 1, 6, 2, 7, 3, 8, 4, 9])
arr.take(indices)
{code}"	ARROW	Resolved	2	1	6306	pull-request-available
13183633	[Python] Table.from_arrays segfaults if lists and schema are passed	"{code:python}
    data = [
        list(range(5)),
        [-10, -5, 0, 5, 10]
    ]

    schema = pa.schema([
        pa.field('a', pa.uint16()),
        pa.field('b', pa.int64())
    ])

    pa.Table.from_arrays(data, schema=schema)
{code}

Whereas it should raise a `TypeError`"	ARROW	Resolved	3	1	6306	pull-request-available
13372293	[Release] Remove rebase post-release scripts	We're going to release from a maintenance branch from now on, so we won't rebase neither the master branch nor the pull requests again.	ARROW	Resolved	3	4	6306	pull-request-available
13378290	[CI] Merge script test fails due to missing dependency	{{six}} is missing, perhaps github has updated the virtual environment of the hosted actions machines.	ARROW	Resolved	3	1	6306	pull-request-available
13291408	[Dev] Comment bot's crossbow command acts on the master branch	Rather than the branch of the PR which has triggered it.	ARROW	Resolved	3	4	6306	pull-request-available
13417448	[Dev][Website] Changelog generation should use commit messages	"Currently, {{archery release changelog generate}} will get issue titles from JIRA.
This is suboptimal as often the title of a JIRA points to a general problem, but doesn't describe the specific changes. Sometimes even, the JIRA title is misleading as the chosen solution is different as the one envisioned by the submitter.

It would be better to fetch the git commit titles and use the first line as the change summary."	ARROW	Resolved	3	4	6306	pull-request-available
13276891	[FlightRPC][Java] Flight gRPC service is missing reflection info	When setting up the gRPC service, we mangle the gRPC [service descriptor|https://github.com/apache/arrow/blob/master/java/flight/src/main/java/org/apache/arrow/flight/FlightBindingService.java], removing reflection information. This means things like gRPC reflection don't work, which is necessary for debugging/development tools like [grpcurl|https://github.com/fullstorydev/grpcurl/]. Reflection information is also useful to do things like authorization/access control based on RPC method.	ARROW	Resolved	3	4	6496	pull-request-available
13378366	[CI][C++] MinGW builds failing when trying to build Gandiva	"clang is failing to run because of a missing DLL. The trick is to use strace which will tell you the exact missing package, in this case it happens to be libxml2.

[https://github.com/msys2/MINGW-packages/issues/544#issuecomment-802725054]"	ARROW	Resolved	3	1	6496	pull-request-available
13425206	[C++][FlightRPC] Add FlightClient::Close	"We should add an explicit Close() method to Flight clients. For backwards compatibility, the client destructor should call Close().

For gRPC, we do not have an explicit Close() method provided by gRPC, so it will just be a no-op (or perhaps explicitly drop the reference to the underlying gRPC client). When ARROW-15473 is implemented, Close() could cancel all calls and drop the reference.

For UCX, we will need an explicit Close() to clean up resources (and return error codes), so this will be useful then."	ARROW	Resolved	3	4	6496	pull-request-available
13310682	[FlightRPC][C++][Python] Allow setting gRPC client options	There's no way to set generic gRPC options which are useful for tuning behavior (e.g. round-robin load balancing). Rather than bind all of these one by one, gRPC allows setting arguments as generic string-string or string-integer pairs; we could expose this (and leave the interpretation implementation-dependent).	ARROW	Resolved	3	4	6496	pull-request-available
13474544	[Dev][Java] Running integration tests on JDK 18 with archery fails	"Currently all our nightly integration tests are running with JDK 11. If we try to run them with JDK 18 the integration tests fail with:
{code:java}
2022-08-01T14:13:46.7801482Z Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field long java.nio.Buffer.address accessible: module java.base does not ""opens java.nio"" to unnamed module @47b35fc0
2022-08-01T14:13:46.7802091Z 	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
2022-08-01T14:13:46.7802604Z 	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
2022-08-01T14:13:46.7803059Z 	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:180)
2022-08-01T14:13:46.7803450Z 	at java.base/java.lang.reflect.Field.setAccessible(Field.java:174)
2022-08-01T14:13:46.7803837Z 	at org.apache.arrow.memory.util.MemoryUtil.<clinit>(MemoryUtil.java:84)
2022-08-01T14:13:46.7804123Z 	... 8 more {code}
and
{code:java}
2022-08-01T14:13:46.7807862Z subprocess.CalledProcessError: Command '['java', '-Dio.netty.tryReflectionSetAccessible=true', '-Darrow.struct.conflict.policy=CONFLICT_APPEND', '-cp', '/arrow/java/tools/target/arrow-tools-9.0.0-SNAPSHOT-jar-with-dependencies.jar', 'org.apache.arrow.tools.Integration', '-a', '/tmp/tmpx0isaswn/abf94267_generated_decimal256.json_as_file', '-j', '/tmp/arrow-integration-2cw7c4nq/generated_decimal256.json', '-c', 'VALIDATE']' returned non-zero exit status 1. {code}
We seem to be missing on the archery command:
{code:java}
--add-opens {code}
[https://github.com/apache/arrow/blob/546c3771a209cbcac5e03cf26e07bcd8c9601d5a/dev/archery/archery/integration/tester_java.py#L34-L37]

as specified on:

[https://arrow.apache.org/docs/dev/java/install.html#java-compatibility]

There is a PR that reproduces the issue on ubuntu 22.04:
[https://github.com/apache/arrow/pull/13762]

and the job failure:

[https://github.com/ursacomputing/crossbow/runs/7611294787?check_suite_focus=true]

 "	ARROW	Resolved	3	1	6496	pull-request-available
13368402	[C++] Add regex string match kernel	We have a basic {{match_substring}} kernel already but not a regular expression one.	ARROW	Resolved	3	2	6496	pull-request-available
13404996	[R] [CI] R sanitizer build failing	"https://dev.azure.com/ursacomputing/crossbow/_build/results?buildId=12665&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181&l=3124 which might be another out-of-memory error 

{code}
Error: `docker-compose --file /home/vsts/work/1/s/arrow/docker-compose.yml run --rm -e SETUPTOOLS_SCM_PRETEND_VERSION=6.0.0.dev391 ubuntu-r-sanitizer` exited with a non-zero exit code -9, see the process log above.
{code}

"	ARROW	Resolved	3	1	6496	pull-request-available
13361902	[C++][Dataset] Expose originating fragment as a property of ScanTask	This is generally useful for debugging and tracing ScanTasks, and can be used to remove the [explicit mapping in FileSystemDataset::Write|https://github.com/michalursa/arrow/commit/ae396b9d4c26621cba2cce955f1d55f43e8faab9#diff-2caf4e9bd3f139e05e55dca80725d8a9c436f5ccf65c76a37cebfa6ee9b36a6aR278]	ARROW	Resolved	3	4	6496	dataset, pull-request-available
13402386	[C++] Add appx_median, hash_appx_median functions	"The Arrow C++ library has {{tdigest}} and {{hash_tdigest}} kernels that return arrays/lists. We call these from bindings with {{q = 0.5}} to find the approximate median, but the output is difficult to work with. It would be nice to create kernels wrapping these that would produce scalar output per array/group:

{{appx_median}} --> {{tdigest(q = 0.5)}}

{{hash_appx_median}} --> {{hash_tdigest(q = 0.5)}}"	ARROW	Resolved	2	4	6496	kernel, pull-request-available
13399048	[C++] Uniform null handling in compute functions	"The compute functions today have mixed support for null types.

Unary arithmetic functions (e.g. abs) don't support null arrays

Binary arithmetic functions (e.g. add) support one null array (e.g. int32 + null) but not both null arrays (i.e. null + null) but they do support both values being null (e.g. [null] + [null] = [null] if dtype=int32 but not supported if dtype=null)

sort_indices should support null arrays.

Some functions do forward null arrays:
 - unique

Some functions output a non-null type given null inputs

- is_null (=> boolean)
- is_valid (=> boolean)
- value_counts (=> struct)
- dictionary_encode (=> dictionary<null>)
- count (=> int64)


Some functions throw an error other than ""not implemented""

 - list_parent_indices"	ARROW	Resolved	3	1	6496	kernel, pull-request-available, types
13182553	[Python] Enable Flight servers to be implemented in pure Python	While it will be straightforward to offer a Flight client to Python users, enabling _servers_ to be written _in Python_ will require a glue class to invoke methods on a provided server implementation, coercing to and from various Python objects and Arrow wrapper classes	ARROW	Resolved	3	2	6496	flight, pull-request-available
13361955	[C++] Add asynchronous read to parquet::arrow::FileReader	Allow reading 	ARROW	Resolved	3	7	6496	pull-request-available
13377287	[C++][Python][FlightRPC] Support export_to_c in DoGet/inherit from RecordBatchReader	{{MetadataRecordBatchReader}} should probably inherit from RecordBatchReader to make reuse easier and to support common APIs like the C Data Interface.	ARROW	Resolved	3	4	6496	pull-request-available
13367655	[C++] Fix sort_indices, array_sort_indices timestamp support discrepancy	"{{sort_indices}} supports sorting by timestamp arrays, but {{array_sort_indices}} does not. Here's some example R code to demonstrate this (but this example code depends on ARROW-11703 to run):
{code:java}
tbl <- tibble::tibble(
  dttm = lubridate::ymd_hms(c(""2021-01-01 00:00:00"", ""1900-01-01 00:00:00"")),
)
rb <- arrow::record_batch(tbl)

# this fails:
arrow:::call_function(
  ""array_sort_indices"",
  rb$dttm,
  options = list(order = F)
)
## Error: NotImplemented: Function array_sort_indices has no kernel matching input types (array[timestamp[us, tz=UTC]])

# this fails because it internally calls array_sort_indices
arrow:::call_function(
  ""sort_indices"",
  rb,
  options = list(names = ""dttm"", orders = 0L)
)
## Error: NotImplemented: Function array_sort_indices has no kernel matching input types (array[timestamp[us, tz=UTC]])

# this succeeds
arrow:::call_function(
  ""sort_indices"",
  rb,
  options = list(names = c(""dttm"", ""dttm""), orders = 0L)
) 
## Array
## <uint64>
## [
##   1,
##   0
## ]{code}"	ARROW	Resolved	3	1	6496	kernel, pull-request-available, types
13397568	[C++] Implement ScalarAggregateOptions for count_distinct (grouped) 	"I'm writing the R bindings for the grouped {{count_distinct}} kernel, but the current implementation counts nulls as their own group.  To match the R behaviour,  I need to be able to specify whether or not to remove NA/NULL values.

Please could we have ScalarAggregateOptions implemented for {{count_distinct}}?

"	ARROW	Resolved	3	4	6496	kernel, pull-request-available
13395545	[C++] Concatenate with an empty dictionary segfaults (ASan failure in TestFilterKernelWithString/0.FilterDictionary)	"{noformat}
[ RUN      ] TestFilterKernelWithString/0.FilterDictionary
=================================================================
==31836==ERROR: AddressSanitizer: global-buffer-overflow on address 0x7f3788bac6c0 at pc 0x7f377c13382b bp 0x7ffe135dddb0 sp 0x7ffe135ddda8
READ of size 4 at 0x7f3788bac6c0 thread T0
    #0 0x7f377c13382a in void arrow::internal::TransposeInts<signed char, signed char>(signed char const*, signed char*, long, int const*) /home/lidavidm/Code/upstream/merging/cpp/src/arrow/util/int_util.cc:434
    #1 0x7f377c1243f2 in Visit<arrow::Int8Type> /home/lidavidm/Code/upstream/merging/cpp/src/arrow/util/int_util.cc:482
    #2 0x7f377c11a5f9 in VisitTypeInline<arrow::internal::(anonymous namespace)::TransposeIntsDest<signed char> > /home/lidavidm/Code/upstream/merging/cpp/src/arrow/visitor_inline.h:89
    #3 0x7f377c119b51 in operator() /home/lidavidm/Code/upstream/merging/cpp/src/arrow/util/int_util.cc:491
    #4 0x7f377c102ac9 in Visit<arrow::Int8Type> /home/lidavidm/Code/upstream/merging/cpp/src/arrow/util/int_util.cc:508
    #5 0x7f377c0f7c88 in VisitTypeInline<arrow::internal::(anonymous namespace)::TransposeIntsSrc> /home/lidavidm/Code/upstream/merging/cpp/src/arrow/visitor_inline.h:89
    #6 0x7f377c0f479d in operator() /home/lidavidm/Code/upstream/merging/cpp/src/arrow/util/int_util.cc:515
    #7 0x7f377c0f49f3 in arrow::internal::TransposeInts(arrow::DataType const&, arrow::DataType const&, unsigned char const*, unsigned char*, long, long, long, int const*) /home/lidavidm/Code/upstream/merging/cpp/src/arrow/util/int_util.cc:525
    #8 0x7f377b6e72fd in ConcatenateDictionaryIndices /home/lidavidm/Code/upstream/merging/cpp/src/arrow/array/concatenate.cc:289
    #9 0x7f377b6e85f0 in Visit /home/lidavidm/Code/upstream/merging/cpp/src/arrow/array/concatenate.cc:320
    #10 0x7f377b6ef4cf in VisitTypeInline<arrow::(anonymous namespace)::ConcatenateImpl> /home/lidavidm/Code/upstream/merging/cpp/src/arrow/visitor_inline.h:89
    #11 0x7f377b6e11a2 in Concatenate /home/lidavidm/Code/upstream/merging/cpp/src/arrow/array/concatenate.cc:193
    #12 0x7f377b6ed681 in arrow::Concatenate(std::vector<std::shared_ptr<arrow::Array>, std::allocator<std::shared_ptr<arrow::Array> > > const&, arrow::MemoryPool*) /home/lidavidm/Code/upstream/merging/cpp/src/arrow/array/concatenate.cc:481
    #13 0x55adf99765b9 in arrow::compute::TestFilterKernel::AssertFilter(std::shared_ptr<arrow::Array> const&, std::shared_ptr<arrow::Array> const&, std::shared_ptr<arrow::Array> const&) /home/lidavidm/Code/upstream/merging/cpp/src/arrow/compute/kernels/vector_selection_test.cc:221
    #14 0x55adf9c12be2 in arrow::compute::TestFilterKernelWithString<arrow::BinaryType>::AssertFilterDictionary(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /home/lidavidm/Code/upstream/merging/cpp/src/arrow/compute/kernels/vector_selection_test.cc:517
    #15 0x55adf9bb251d in arrow::compute::TestFilterKernelWithString_FilterDictionary_Test<arrow::BinaryType>::TestBody() /home/lidavidm/Code/upstream/merging/cpp/src/arrow/compute/kernels/vector_selection_test.cc:531
    #16 0x7f378ac3698d in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/lidavidm/miniconda3/envs/arrow4/lib/libgtest.so+0x4c98d)
    #17 0x7f378ac36be0 in testing::Test::Run() (/home/lidavidm/miniconda3/envs/arrow4/lib/libgtest.so+0x4cbe0)
    #18 0x7f378ac36f0e in testing::TestInfo::Run() (/home/lidavidm/miniconda3/envs/arrow4/lib/libgtest.so+0x4cf0e)
    #19 0x7f378ac37035 in testing::TestSuite::Run() (/home/lidavidm/miniconda3/envs/arrow4/lib/libgtest.so+0x4d035)
    #20 0x7f378ac375eb in testing::internal::UnitTestImpl::RunAllTests() (/home/lidavidm/miniconda3/envs/arrow4/lib/libgtest.so+0x4d5eb)
    #21 0x7f378ac37858 in testing::UnitTest::Run() (/home/lidavidm/miniconda3/envs/arrow4/lib/libgtest.so+0x4d858)
    #22 0x7f378ac9707e in main (/home/lidavidm/miniconda3/envs/arrow4/lib/libgtest_main.so+0x107e)
    #23 0x7f377180cbf6 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x21bf6)
    #24 0x55adf9431658  (/home/lidavidm/Code/upstream/merging/build/debug/arrow-compute-vector-test+0xbaa658)0x7f3788bac6c1 is located 0 bytes to the right of global variable 'zero_size_area' defined in '/home/lidavidm/Code/upstream/merging/cpp/src/arrow/memory_pool.cc:188:36' (0x7f3788bac6c0) of size 1
  'zero_size_area' is ascii string ''
SUMMARY: AddressSanitizer: global-buffer-overflow /home/lidavidm/Code/upstream/merging/cpp/src/arrow/util/int_util.cc:434 in void arrow::internal::TransposeInts<signed char, signed char>(signed char const*, signed char*, long, int const*)
Shadow bytes around the buggy address:
  0x0fe77116d880: f9 f9 f9 f9 00 f9 f9 f9 f9 f9 f9 f9 00 00 f9 f9
  0x0fe77116d890: f9 f9 f9 f9 00 f9 f9 f9 f9 f9 f9 f9 00 00 f9 f9
  0x0fe77116d8a0: f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9 00 00 00 00
  0x0fe77116d8b0: 01 f9 f9 f9 f9 f9 f9 f9 00 00 00 f9 f9 f9 f9 f9
  0x0fe77116d8c0: 00 f9 f9 f9 f9 f9 f9 f9 02 f9 f9 f9 f9 f9 f9 f9
=>0x0fe77116d8d0: 00 f9 f9 f9 f9 f9 f9 f9[01]f9 f9 f9 f9 f9 f9 f9
  0x0fe77116d8e0: 00 00 00 00 00 00 00 f9 f9 f9 f9 f9 00 00 00 00
  0x0fe77116d8f0: 01 f9 f9 f9 f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9
  0x0fe77116d900: 00 00 00 00 00 00 00 00 00 00 00 00 01 f9 f9 f9
  0x0fe77116d910: f9 f9 f9 f9 01 f9 f9 f9 f9 f9 f9 f9 01 f9 f9 f9
  0x0fe77116d920: f9 f9 f9 f9 01 f9 f9 f9 f9 f9 f9 f9 01 f9 f9 f9
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==31836==ABORTING
fish: Job 1, './debug/arrow-compute-vector-te…' terminated by signal SIGABRT (Abort) {noformat}"	ARROW	Resolved	3	1	6496	pull-request-available
13363938	[C++] GZip codec hangs if flushed twice	"{code:java}
    // ""If deflate returns with avail_out == 0, this function must be called
    //  again with the same value of the flush parameter and more output space
    //  (updated avail_out), until the flush is complete (deflate returns
    //  with non-zero avail_out).""
    return FlushResult{bytes_written, (bytes_written == 0)}; {code}
But contrary to the comment, we're checking bytes_written. So if we flush twice, the second time, we won't write any bytes, but we'll erroneously interpret that as zlib asking for a larger buffer, rather than zlib telling us there's no data to decompress. Then we'll enter a loop where we keep doubling the buffer size forever, hanging the program."	ARROW	Resolved	3	1	6496	pull-request-available
13434357	[Java][Docs] Document HistoricalLog for debugging	"{{BufferLedger}} has a {{HistoricalLog}} which records stack traces at various events in the buffer's life, such as when it was first allocated and upon incrementing/decrementing reference counts. While somewhat of an implementation detail, this information is useful for debugging, especially for tracking down memory leaks. We could document it (with a suitable warning).

However, the stack trace is less useful than it first appears because it accidentally truncates the log. We should fix that too."	ARROW	Resolved	3	4	6496	pull-request-available
13380115	[C++] hash_aggregate_test not building on master	The test was recently updated but looks like it needed a rebase	ARROW	Resolved	3	1	6496	pull-request-available
13245858	[FlightRPC] Expose (de)serialization of protocol types	It would be nice to be able to serialize/deserialize Flight types (e.g. FlightInfo) to/from the binary representations, in order to interoperate with systems that might want to provide (say) Flight tickets or FlightInfo without using the Flight protocol. For instance, you might have a search server that exposes a REST interface and wants to provide FlightInfo objects for Flight clients, without having to listen on a separate port.	ARROW	Resolved	3	2	6496	pull-request-available
13481159	[Format][FlightRPC][C++][Java] Add Substrait for Flight SQL	See ML: https://lists.apache.org/thread/3k3np6314dwb0n7n1hrfwony5fcy7kzl	ARROW	Resolved	3	2	6496	pull-request-available
13261952	[FlightRPC][Java] Flight server can hang JVM on shutdown	"I noticed this while working on Flight integration tests. FlightService keeps an executor, which can hang the JVM on shutdown if the executor itself is not shut down.

It's used by Handshake and DoPut.

I think this surfaced because I wrote an AuthHandler that threw an exception."	ARROW	Resolved	3	1	6496	pull-request-available
13472091	[Java] JSONFileWriter throws IOOBE writing an empty list	"Hey folks,

I'm trying to write an empty ListVector out through the `JsonFileWriter`, and am getting an IOOBE. Stack trace is as follows:

 

```

java.lang.IndexOutOfBoundsException: index: 0, length: 4 (expected: range(0, 0))
 at org.apache.arrow.memory.ArrowBuf.checkIndexD (ArrowBuf.java:318)
    org.apache.arrow.memory.ArrowBuf.chk (ArrowBuf.java:305)
    org.apache.arrow.memory.ArrowBuf.getInt (ArrowBuf.java:424)
    org.apache.arrow.vector.ipc.JsonFileWriter.writeValueToGenerator (JsonFileWriter.java:270)
    org.apache.arrow.vector.ipc.JsonFileWriter.writeFromVectorIntoJson (JsonFileWriter.java:237)
    org.apache.arrow.vector.ipc.JsonFileWriter.writeFromVectorIntoJson (JsonFileWriter.java:253)
    org.apache.arrow.vector.ipc.JsonFileWriter.writeFromVectorIntoJson (JsonFileWriter.java:253)
    org.apache.arrow.vector.ipc.JsonFileWriter.writeFromVectorIntoJson (JsonFileWriter.java:253)
    org.apache.arrow.vector.ipc.JsonFileWriter.writeBatch (JsonFileWriter.java:200)
    org.apache.arrow.vector.ipc.JsonFileWriter.write (JsonFileWriter.java:190)

```

It's trying to write the offset buffer of the list, which is empty. L224 of JFW.java sets `bufferValueCount` to 1 (because we're not a DUV), so we enter the `for` loop. We don't hit the `valueCount=0` condition in L230 (because we're not a varbinary or a varchar vector). So we fall into the `else`, which tries to write the 0th element in the offset vector, and IOOBE.

Could we include 'list' in either the L224 or the L230 checks?

Admittedly, I'm not aware of the history of this section, but it seems that, by the time we hit L230 (i.e. excluding DUV), any empty vector should yield a single 0?

Let me know if there's any more info I can provide!

Cheers,

James"	ARROW	Resolved	4	1	6496	pull-request-available
13398766	[C++] Add min and max aggregation functions	We have min_max but that returns a struct array. At some point we will want compute kernels that we can call in a ProjectNode that will allow us to get at struct array elements, but for now it would be nice to have (hash_)min and max that just give the scalar we want.	ARROW	Resolved	2	2	6496	kernel, pull-request-available
13365870	[C++] Move csv::ReadOptions::skip_rows to csv::ParseOptions::skip_rows	"This properly affects how the CSV is parsed and in turn the schema and data, so it belongs in ParseOptions. This means it can also be configured in Datasets.

For now, we'll duplicate the field and deprecate the original spot."	ARROW	Closed	3	3	6496	dataset, datasets
13416311	[C++] Add simple stdout/JSON exporter for OpenTelemetry	"While it is odd for a library to configure OpenTelemetry exporters, there is a reason for this: the actual end-user is unable to configure it directly (as we use the C++ implementation, and the user is likely in Python or R), and often the ""important"" bits are all in C++, so it would be nice to have a way to dump the information collected.

On top of that, it would be nice to dump the data in an easily parseable format for further consumption.

We should enable the ostream exporter, as well as a custom exporter that dumps JSON. See [https://github.com/open-telemetry/opentelemetry-cpp/pull/1111] for an example. See discussion on [https://github.com/apache/arrow/pull/11906] as well."	ARROW	Resolved	3	4	6496	pull-request-available, query-engine
13472149	[Java] All static initializers should catch and report exceptions	"As reported on the mailing list: https://lists.apache.org/thread/gysn25gsm4v1fvvx9l0sjyr627xy7q65

All static initializers should catch and report exceptions, or else they will get swallowed by the JVM."	ARROW	Resolved	3	4	6496	good-first-issue, good-second-issue, pull-request-available
13412554	[Go] Errors from MessageReader.Message don't get surfaced by Reader.Read	"Noticed while playing with a Flight client. This snippet inadvertently suppresses all errors that come from the underlying MessageReader since {{next}} sets {{done}} after an error is encountered.
{code:go}
func (r *Reader) Read() (array.Record, error) {
	if r.rec != nil {
		r.rec.Release()
		r.rec = nil
	}

	if !r.next() {
                // r.done is set if next() errors, so we always
                // return EOF instead of the actual error
		if r.done {
			return nil, io.EOF
		}
		return nil, r.err
	}

	return r.rec, nil
}{code}"	ARROW	Resolved	3	1	6496	pull-request-available
13502192	[C++] Flight client may crash due to improper Result/Status conversion	"Reported on user@ https://lists.apache.org/thread/84z329t1djhnbr5bq936v4hr8cyngj2l 

{noformat}
I have an issue on my project, we have a query execution engine that
returns result data as a flight stream and c++ client that receives the
stream. In case a query has no results but the result schema implies
dictionary encoded fields in results we have client app crushed.

The cause is in cpp/src/arrow/flight/client.cc:461:

::arrow::Result<std::unique_ptr<ipc::Message>> ReadNextMessage() override {
if (stream_finished_) {
return nullptr;
}
internal::FlightData* data;
{
auto guard = read_mutex_ ? std::unique_lock<std::mutex>(*read_mutex_)
: std::unique_lock<std::mutex>();
peekable_reader_->Next(&data);
}
if (!data) {
stream_finished_ = true;
return stream_->Finish(Status::OK()); // Here the issue
}
// Validate IPC message
auto result = data->OpenMessage();
if (!result.ok()) {
return stream_->Finish(std::move(result).status());
}
*app_metadata_ = std::move(data->app_metadata);
return result;
}

The method returns Result object while stream_Finish(..) returns a Status.
So there is an implicit conversion from Status to Result that causes
Result(Status) constructor to be called, but the constructor expects only
error statuses which in turn causes the app to be failed:

/// Constructs a Result object with the given non-OK Status object. All
/// calls to ValueOrDie() on this object will abort. The given `status` must
/// not be an OK status, otherwise this constructor will abort.
///
/// This constructor is not declared explicit so that a function with a
return
/// type of `Result<T>` can return a Status object, and the status will be
/// implicitly converted to the appropriate return type as a matter of
/// convenience.
///
/// \param status The non-OK Status object to initialize to.
Result(const Status& status) noexcept // NOLINT(runtime/explicit)
: status_(status) {
if (ARROW_PREDICT_FALSE(status.ok())) {
internal::DieWithMessage(std::string(""Constructed with a non-error status: "")
+
status.ToString());
}
}

Is there a way to workaround or fix it? We use Arrow 6.0.0, but it seems
that the issue exists in all future versions.
{noformat}"	ARROW	Resolved	3	1	6496	pull-request-available
13400124	[C++] index_in/is_in kernels missing support for timestamp with timezone	The index_in and is_in kernels should support all equatable value types.  At the moment it supports all except for timestamp types that have a timezone.	ARROW	Resolved	3	1	6496	good-first-issue, kernel, pull-request-available, query-engine
13348321	[Java] Is there a bug in flight AddWritableBuffer	"[https://github.com/apache/arrow/blob/9bab12f03ac486bb8270f031b83f0a0411766b3e/java/flight/flight-core/src/main/java/org/apache/arrow/flight/grpc/AddWritableBuffer.java#L94]

buf.readBytes(stream, buf.readableBytes());

is this line redundant
In my perf.svg, this will copy the data from buf to OutputStream, which can not realize zero-copy."	ARROW	Resolved	3	1	6496	pull-request-available
13400239	[C++] iso_calendar may be uninitialized	"{code}
/arrow/cpp/src/arrow/scalar.h:137:64: warning: ‘*((void*)& iso_calendar +8)’ may be used uninitialized in this function [-Wmaybe-uninitialized]
  137 |       : PrimitiveScalarBase(std::move(type), true), value(value) {}
      |                                                                ^
In file included from /tmp/RtmpoS4YCn/file8773f4430f/src/arrow/CMakeFiles/arrow_objlib.dir/Unity/unity_17_cxx.cxx:7:
/arrow/cpp/src/arrow/compute/kernels/scalar_temporal.cc:697:30: note: ‘*((void*)& iso_calendar +8)’ was declared here
  697 |       std::array<int64_t, 3> iso_calendar;
{code}

fyi [~rokm]"	ARROW	Resolved	3	1	6496	pull-request-available
13248880	[FlightRPC][Java] Don't double-close response stream	DoPut in Java double-closes the metadata response stream: if the service implementation sends an error down that channel, the Flight implementation will unconditionally try to complete the stream, violating the gRPC semantics (either an error or a completion may be sent, never both).	ARROW	Resolved	3	1	6496	pull-request-available
13300623	[Python] Expose UnionArray.array and other fields	"Currently in Python, you can construct a UnionArray easily, but getting the data back out (without copying) is near-impossible. We should expose the getter for UnionArray.array so we can pull out the constituent arrays. We should also expose fields like mode while we're at it.

The use case is: in Flight, we'd like to write multiple distinct datasets (with distinct schemas) in a single logical call; using UnionArrays lets us combine these datasets into a single logical dataset."	ARROW	Resolved	3	4	6496	pull-request-available
13377637	[C++] Add variadic string join kernel	"Similar to SQL's {{concat}} and {{concat_ws}}. Should take 0, 1, 2, ... string arrays and an optional separator (default empty string) and concatenate them together, returning a string array.

For example, in the case of 2 input arrays and with the separator {{""-""}}, this would take inputs:
{code}
Array<string>        Array<string>
[                    [
  ""foo"",               ""bar"",
  ""push""               ""pop""
]                    ]
{code}
and return output:
{code}
Array<string>
[ 
  ""foo-bar"",
  ""push-pop""
] 
{code}

Should also accept scalar strings and recycle their values.
"	ARROW	Resolved	3	2	6496	pull-request-available
13375498	[C++][FlightRPC] Benchmark compression with real data	"Flight benchmark program is using random test data as payload. Random data is hard to compress. Per my test, the compressed payload (zstd default level) is actually a bit larger than the original uncompressed payload. So compression is a pure loss in current benchmark.

It's better to use real world test set for a more reasonable benchmark about how compression influences FlightRPC performance. Perhaps the benchmark (both client/server) could accept a path to an IPC file as an option."	ARROW	Resolved	3	4	6496	pull-request-available
13500873	[Java][FlightRPC] FlightSQL error: 'Parameter ordinal out of range' executing a prepared stmt with params	"Hey again :) 

I'm getting a 'parameter ordinal 1 out of range' error trying to set a parameter on the returned AvaticaPreparedStatement. Repro:

* Open a FlightSQL JDBC connection
* {{conn.prepareStatement}} with a SQL query containing params (e.g. {{INSERT INTO users (id, name) VALUES (?, ?)}})
* `ps.setString(1, ""foo"")` -> above error, thrown from {{AvaticaPreparedStatement.getParameter(int)}}

I had a bit of a dig to try to identify a potential cause:
* the {{Meta.Signature}} passed to the {{AvaticaPreparedStatement}} on creation has an empty parameter list - this is what causes the out-of-bounds error.
*  in {{ArrowFlightMetaImpl.prepare}}, it calls {{newSignature}}, but this only takes the SQL query, and so {{newSignature}} creates the signature with the empty list. The call to {{ArrowFlightSqlClientHandler.prepare}} happens on the line after - could we pass the param Schema from this result to {{newSignature}}?

Let me know if I can help narrow this down further or help with the fix :)

James"	ARROW	Open	3	1	6496	pull-request-available
13378359	[Python][FlightRPC] Flight server segfaults with certain data	"This do_get RPC handler segfaults when invoked:
{code:python}
def do_get(...):
    schema = pa.schema([])
    return flight.GeneratorStream(schema, itertools.repeat(schema.empty_table()))
{code}
A similar one for do_exchange also segfaults.

Confirmed using the Linux Conda package for 4.0.0."	ARROW	Resolved	3	1	6496	pull-request-available
13400878	[C++] Support binary-like types in hash_min_max, hash_min, hash_max	An extension to ARROW-13882. Non-fixed-width types will need a separate approach, so this was split out to a new JIRA.	ARROW	Resolved	3	4	6496	kernel, pull-request-available
13393476	[C++] Implement datediff kernel	"Add a kernel to compute the number of years, months, weeks, days, hours, minutes, (micro/milli/nano)seconds, or quarters between two timestamps. 

This should act like SQL's DATEDIFF ([SQL Server|https://docs.microsoft.com/en-us/sql/t-sql/functions/datediff-transact-sql?view=sql-server-ver15]). Pandas doesn't have a convenient equivalent except in the case of days (pd.Timedelta.days) but it can be [calculated using Timestamp.to_period|https://stackoverflow.com/questions/54171674/calculating-the-amount-of-full-months-between-two-dates].

We have hinnant's date library vendored and this should hopefully be implementable with that."	ARROW	Resolved	3	4	6496	compute, kernel, pull-request-available
13481738	[Packaging] Add JDBC driver to release tasks	The java-jars task has a list of artifacts to upload, the JDBC driver needs to be included there: https://github.com/apache/arrow/blob/7cfdfbb0d5472f8f8893398b51042a3ca1dd0adf/dev/tasks/tasks.yml#L816-L820	ARROW	Resolved	3	7	6496	pull-request-available
13337931	[R] Feather reader/writer should accept a MemoryPool	"https://github.com/apache/arrow/pull/8533#issuecomment-717516187

Currently feather readers and writers will allocate all buffers from the default memory pool. This is contrary to the Arrow convention of allowing control over memory allocation.
"	ARROW	Resolved	3	4	6496	pull-request-available
13386954	Add CSV Writer documentation	The new CSV writer in C++, Python and possibly other languages does not have much documentation. A user who checks out our site would probably still believe that we don't have an CSV writer in C++.	ARROW	Resolved	3	4	6496	pull-request-available
13425405	[C++][FlightRPC] Ensure system gRPC is only used with system Protobuf	"See Kou's post on the ML: [https://lists.apache.org/thread/dg2nm7r9vpo42toygg8o8rzf8gkg6knb]

We should ensure system gRPC doesn't get mixed with bundled Protobuf which can cause test failures (also this is not really a valid combination, this will likely link in two copies of Protobuf)."	ARROW	Resolved	3	4	6496	pull-request-available
13417943	[Doc] Start Glossary of common terms	"Add Annotation Glossary for Apache Arrow project.

Example from the scikit-learn project: https://scikit-learn.org/stable/glossary.html"	ARROW	Resolved	3	4	6496	pull-request-available
13308043	[FlightRPC][C++] Fix flaky MacOS tests	"The gRPC MacOS tests have been flaking again.

Looking at [https://github.com/grpc/grpc/issues/20311] they may possibly have been fixed except [https://github.com/grpc/grpc/issues/13856] reports they haven't (in some configurations?) so I will try a few things in CI, or just disable the tests on MacOS."	ARROW	Resolved	3	1	6496	pull-request-available
13376524	[C++][Dataset] Support reading date/time-partitioned datasets accounting for URL encoding (Spark)	"I'm using Spark (3.1.1) to write a dataframe to a partitioned parquet dataset (using delta.io) which is partitioned by a timestamp field.

The relevant Spark code:
{code:java}
// code placeholder
(
  df.withColumn(
                ""Date"",
                sf.date_trunc(
                    ""DAY"",
                    sf.from_unixtime(
                        (sf.col(""MyEpochField"")),
                    ),
                ),
            )
    .write.format(""delta"")
    .mode(""append"")
    .partitionBy(""Date"")
    .save(""..."")

{code}
This gives a structure like following:
{code:java}
// code placeholder
/tip
/tip/Date=2021-05-04 00%3A00%3A00
/tip/Date=2021-05-04 00%3A00%3A00/Time=2021-05-04 07%3A27%3A00
/tip/Date=2021-05-04 00%3A00%3A00/Time=2021-05-04 07%3A27%3A00/part-00000-8846eb80-a369-43f6-a715-fec9cf1adf95.c000.snappy.parquet

{code}
Notice the : character is (url?) encoded because of fs protocol violation.

When i try to open this dataset using delta-rs ([https://github.com/delta-io/delta-rs)] which uses Arrow below the hood, then an error is raised trying to parse the Date (folder) value.
{code:java}
// code placeholder
pyarrow.lib.ArrowInvalid: error parsing '2021-05-03 00%3A00%3A00' as scalar of type timestamp[ns]
{code}
It seems this error is raised in ScalarParseImpl => ParseValue => StringConverter<TimestampType>::Convert => ParseTimestampISO8601

The mentioned parse method does support for format:
{code:java}
// code placeholder
static inline bool ParseTimestampISO8601(const char* s, size_t length,
                                         TimeUnit::type unit,
                                         TimestampType::c_type* out) {
  using seconds_type = std::chrono::duration<TimestampType::c_type>;  // We allow the following formats for all units:
  // - ""YYYY-MM-DD""
  // - ""YYYY-MM-DD[ T]hhZ?""
  // - ""YYYY-MM-DD[ T]hh:mmZ?""
  // - ""YYYY-MM-DD[ T]hh:mm:ssZ?""
<...>{code}
But may not support (url?) decoding the value upfront?

Questions we have:
 * Should Arrow support timestamp fields when used as partitioned field?
 * Where to decode?

 

Some more information from the writing side.

The writing is initiated using FileFormatWriter.write that eventually uses a DynamicPartitionDataWriter (passing in the partitionColumns through the job description).

Here the actual ""value"" is rendered and concatennated.
{code:java}
// code placeholder
  /** Expression that given partition columns builds a path string like: col1=val/col2=val/... */
  private lazy val partitionPathExpression: Expression = Concat(
    description.partitionColumns.zipWithIndex.flatMap { case (c, i) =>
      val partitionName = ScalaUDF(
        ExternalCatalogUtils.getPartitionPathString _,
        StringType,
        Seq(Literal(c.name), Cast(c, StringType, Option(description.timeZoneId))))
      if (i == 0) Seq(partitionName) else Seq(Literal(Path.SEPARATOR), partitionName)
    })

{code}
Where the encoding is done in:

[https://github.com/apache/spark/blob/v3.0.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala#L66]

If i understand correct, then Arrow should provide the equivalent of unescapePathName for fields used as partitioned columns.

 "	ARROW	Resolved	3	1	6496	dataset, datasets, delta, parquet, pull-request-available, spark
13476386	[Integration] Re-enable disabled Rust Flight middleware test	Follow-up for ARROW-10961. The linked Rust issue was fixed, so we should re-enable the integration test case.	ARROW	Resolved	3	4	6496	pull-request-available
13382295	[C++][Python] Converter::Extend gets stuck in infinite loop causing OOM if values don't fit in single chunk	"_Apologies if this is a duplicate, I haven't found anything related_

When creating an arrow table via the python api, the following code runs out of memory after using all the available resources on a box with 512GB of ram. This happens with pyarrow 4.0.0 and 4.0.1. However when running the same code with pyarrow 3.0.0, the memory usage only reaches 5GB (which seems like the appropriate ballpark for the table size).
 The code generates a table with a single string column with 1m rows, each string being 3000 characters long.

Not sure whether the issue is python related or not, I haven't tried replicating it from the C++ api.

 
{code:python}
import os, string
import numpy as np
import pyarrow as pa

print(pa.__version__)
np.random.seed(42)

alphabet = list(string.ascii_uppercase)

_col = []
for _n in range(1000):
  k = ''.join(np.random.choice(alphabet, 3000))
  _col += [k] * 1000

table = pa.Table.from_pydict({'col': _col})
{code}"	ARROW	Resolved	3	1	6496	pull-request-available
13425833	[C++][FlightRPC] Add CUDA MemoryManager to Flight benchmark	Continuation of ARROW-15374.	ARROW	Resolved	3	4	6496	pull-request-available
13390759	[C++][Compute] Document out-of-source addition to the FunctionRegistry	The FunctionRegistry class provides support for consumers of the library to define their own compute functions and add them to the registry for usage in filter expressions, projections, etc. We don't have any documentation on how to do this, and it'd be worthwhile to add an example too.	ARROW	Resolved	3	4	6496	pull-request-available
13389170	[C++][Documentation] List hash aggregate kernels somewhere	Hash aggregate kernels are not listed in compute.rst with the rest of the functions, presumably because they're not intended to be directly callable. However, once ARROW-12759 goes in, we should find some place to list what aggregations are supported with group by.	ARROW	Resolved	3	4	6496	kernel, pull-request-available
13480897	[C++][Python][FlightRPC] Add Flight SQL ADBC driver and Python bindings	"Pending ADBC acceptance.

This will finally make Flight SQL accessible in Python, though it will rely on having the ADBC driver manager available to provide the Python bindings.  "	ARROW	In Progress	3	2	6496	pull-request-available
13440173	[Python][FlightRPC] Fix test_flight.py when flight is not available	"https://github.com/apache/arrow/pull/12749#discussion_r851671770

{{flight}} is {{None}} when not building flight so don't use the module at module level"	ARROW	Resolved	3	1	6496	pull-request-available
13399868	[C++] Coalesce kernel missing support for list/struct types	List and struct types aren't sortable and one could debate whether they should be equatable or not but coalesce should only require nullable and all data types should be nullable.  So I would expect coalesce to work on any data type.  At the moment it does not support list/struct data types even when all arguments are the same type.	ARROW	Closed	3	1	6496	kernel, query-engine
13397774	[C++] Add option to handle NAs to TDigest, Index, Mode, Quantile aggregates	"When calling the ""tdigest"" kernel, there is no option to handle NA values and they are currently ignored - please could we have an option added, so that if there are NA values, we get an NA back?
"	ARROW	Resolved	3	4	6496	kernel, pull-request-available
13375829	[C++] Implement OptionalParallelForAsync	In ARROW-11843 we found that we still can't enable parallel column conversion for Parquet datasets because the Arrow Parquet reader uses OptionalParallelFor, which does a parallel wait. We should provide an async version to avoid nested parallelism.	ARROW	Resolved	3	4	6496	pull-request-available
13376009	[C++] Add split_pattern_regex function	Currently the split_pattern compute kernel can split strings by a given pattern, but it'd be really helpful if the specified pattern could be a regular expression	ARROW	Resolved	3	4	6496	pull-request-available
13322594	"[C++][Dataset] Port ""head"" method from R to C++ Dataset Scanner"	"ARROW-9665 (https://github.com/apache/arrow/pull/7913) added amongst other things a {{head}} method for Dataset in R:

https://github.com/apache/arrow/blob/586c060c8b1851f1077911fae6d02a10ed83e7fb/r/src/dataset.cpp#L266-L282

It might be nice to move this to C++ and expose it on the python side as well (and since it's written already in C++ on the R side, it should be relatively straightforward to port I assume)"	ARROW	Resolved	3	4	6496	dataset, pull-request-available
13482431	[Java] Flakiness in JDBC driver test ArrowFlightJdbcConnectionCookieTest.testCookies	"I think we should just suppress this kind of exception in Flight SQL as it's not really actionable

{noformat}
 Error:  org.apache.arrow.driver.jdbc.ArrowFlightJdbcConnectionCookieTest.testCookies  Time elapsed: 0.805 s  <<< ERROR!
java.sql.SQLException: While closing statement
	at org.apache.calcite.avatica.Helper.createException(Helper.java:56)
	at org.apache.calcite.avatica.Helper.createException(Helper.java:41)
	at org.apache.calcite.avatica.AvaticaStatement.close(AvaticaStatement.java:254)
	at org.apache.arrow.driver.jdbc.ArrowFlightJdbcConnectionCookieTest.testCookies(ArrowFlightJdbcConnectionCookieTest.java:51)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.arrow.driver.jdbc.FlightServerTestRule$1.evaluate(FlightServerTestRule.java:166)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Sep 19, 2022 12:52:16 AM io.grpc.netty.NettyServerHandler onStreamError
WARNING: Stream Error
io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place
	at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:173)
	at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:481)
	at io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:105)
	at io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:357)
	at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:1007)
	at io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:963)
	at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:515)
	at io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:521)
	at io.netty.handler.codec.http2.Http2ConnectionHandler.closeStream(Http2ConnectionHandler.java:628)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onRstStreamRead(DefaultHttp2ConnectionDecoder.java:444)
	at io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onRstStreamRead(Http2InboundFrameLogger.java:80)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readRstStreamFrame(DefaultHttp2FrameReader.java:509)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:259)
	at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:159)
	at io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41)
	at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:173)
	at io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:393)
	at io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:453)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:510)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:449)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:279)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:55)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:234)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:228)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:175)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:131)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:456)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:169)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:595)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:581)
Caused by: org.apache.arrow.flight.FlightRuntimeException: INTERNAL: Connection closed after GOAWAY. HTTP/2 error code: INTERNAL_ERROR, debug data: byte distribution write error
	at org.apache.arrow.flight.CallStatus.toRuntimeException(CallStatus.java:131)
	at org.apache.arrow.flight.grpc.StatusUtils.fromGrpcRuntimeException(StatusUtils.java:164)
	at org.apache.arrow.flight.grpc.StatusUtils$1.hasNext(StatusUtils.java:241)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:132)
	at org.apache.arrow.flight.sql.FlightSqlClient$PreparedStatement.close(FlightSqlClient.java:1084)
	at org.apache.arrow.driver.jdbc.client.ArrowFlightSqlClientHandler$1.close(ArrowFlightSqlClientHandler.java:192)
	at org.apache.arrow.driver.jdbc.ArrowFlightMetaImpl.closeStatement(ArrowFlightMetaImpl.java:73)
	at org.apache.calcite.avatica.AvaticaStatement.close_(AvaticaStatement.java:268)
	at org.apache.calcite.avatica.AvaticaStatement.close(AvaticaStatement.java:252)
	... 46 more

	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}"	ARROW	Resolved	3	1	6496	pull-request-available
13500690	[Java][FlightRPC] 'Signature not found' calling `executeUpdate` on FlightSQL prepared statement	"Hey folks :)

I'm hitting the 'signature not found' NPE precondition in {{ArrowFlightMetaImpl.execute}} when I prepare a DML statement and call {{executeUpdate}} (the prepare step seems to work fine):


{code}
           Preconditions.java:  801  cfjd.org.apache.arrow.util.Preconditions/checkNotNull
     ArrowFlightMetaImpl.java:   86  org.apache.arrow.driver.jdbc.ArrowFlightMetaImpl/execute
     ArrowFlightMetaImpl.java:   96  org.apache.arrow.driver.jdbc.ArrowFlightMetaImpl/execute
       AvaticaConnection.java:  548  cfjd.org.apache.calcite.avatica.AvaticaConnection/executeQueryInternal
AvaticaPreparedStatement.java:  152  cfjd.org.apache.calcite.avatica.AvaticaPreparedStatement/executeLargeUpdate
AvaticaPreparedStatement.java:  147  cfjd.org.apache.calcite.avatica.AvaticaPreparedStatement/executeUpdate
{code}

It seems that this will always be null in this situation because {{AvaticaPreparedStatement.executeLargeUpdate}} calls {{executeQueryInternal}} with a null signature, regardless of the signature in the prepared statement itself (which is non-null)?

Otherwise, enjoying playing with FlightSQL so far - thanks!

James"	ARROW	Resolved	3	1	6496	pull-request-available
13395909	[C++][Compute] Remove `seq` as a parameter of ExecNode::InputReceived	`seq` is a parameter on which no useful guarantees can be provided and which causes confusion since it could be construed to imply ordering, uniqueness of the tag, etc. Let's simplify the API by simply removing it	ARROW	Resolved	3	4	6496	pull-request-available, query-engine
13295294	[Format][Flight] Add DoExchange RPC to Flight protocol	Per mailing list discussion and vote	ARROW	Resolved	3	2	6496	pull-request-available
13483066	[Java] Remove flaky JaCoCo check in JDBC driver	It doesn't seem to bring much value + can make builds flaky (e.g. a branch may or may not be hit depending on when exactly an exception occurs)	ARROW	Resolved	3	4	6496	pull-request-available
13374245	[C++][Dataset] Consolidate similar tests for file formats	"Between CSV/Parquet/IPC we have a number of very similar or in some cases essentially identical tests. As we're doing more refactoring and development it would be nice to consolidate these tests so that we can ensure all formats behave consistently and get the same level of testing. For instance, ARROW-11772 now adds more comprehensive tests for scanning IPC which don't yet apply to Parquet/CSV.

This sort of consolidation may also be nice to do in Python."	ARROW	Resolved	3	4	6496	dataset, datasets, pull-request-available
13315987	[Python] Provide configurable MetadataVersion in IPC API and environment variable to set default to V4 when needed	This is a follow up to ARROW-9265 and must be implemented in order to release 1.0.0	ARROW	Resolved	1	4	6496	pull-request-available
13217249	[Format] Flight Location should be more flexible than a (host, port) pair	"The more future-proof solution is probably to define a URI format. gRPC already has something like that, though we might want to define our own format:
https://grpc.io/grpc/cpp/md_doc_naming.html
"	ARROW	Resolved	3	1	6496	pull-request-available
13424410	[C++][GLib] Update bindings for MemoryManager::AllocateBuffer	Caused by ARROW-15373.	ARROW	Resolved	3	1	6496	pull-request-available
13407977	[C++][FlightRPC] Add example of registering gRPC service on a Flight server	"Some applications may want to use gRPC in conjunction with Flight to benefit from gRPC's tooling. This is fairly straightforward to do in Java, not possible in Python, and possible in gRPC but needs some demonstration.

This may also be good to have as a cookbook example."	ARROW	Resolved	3	4	6496	pull-request-available
13382163	[C++] match_substring doesn't match empty needle to empty haystack	"{noformat}
>>> import pyarrow as pa, pyarrow.compute as pc
>>> pa.__version__
'4.0.0'
>>> pc.match_substring(["""", ""a""], """")
<pyarrow.lib.BooleanArray object at 0x7fd8a77888e0>
[
  false,
  true
]
>>> """" in """"
True {noformat}
Also confirmed with PyArrow 4.0.1."	ARROW	Resolved	3	1	6496	pull-request-available
13301902	[C++][Dataset] Add ConvertOptions and ReadOptions to CsvFileFormat	https://github.com/apache/arrow/pull/7033 does not add ConvertOptions (including alternate spellings for null/true/false, etc) or ReadOptions (block_size, column name customization, etc). These will be helpful but will require some discussion to find the optimal way to integrate them with dataset::	ARROW	Resolved	3	4	6496	dataset, pull-request-available
13473378	[FlightRPC][C++][Java] Fix example Flight SQL servers	"There are a number of small bugs in the Java Flight SQL example (e.g. binding parameters to the wrong index, not handling null parameter values, not properly reporting errors, things throwing SQLException that should use Arrow/general Java exceptions) that should be fixed. 

Also, CommandPreparedStatementQuery is implemented wrong in both C++ and Java servers: only the last parameter value will be bound/executed when executing a prepared statement that returns a result set. But this contradicts the spec, which states ""All of the bound parameter sets will be executed as a single atomic execution"". The server(s) need to cache the uploaded Arrow data and execute using all input rows."	ARROW	Resolved	3	1	6496	pull-request-available
13372297	[Dev] archery trigger-bot should use logger.exception	"[https://github.com/apache/arrow/blob/9c85e5465a5738f5ba9a2455d1b566948f89d0f3/dev/archery/archery/bot.py#L145-L150]

We should use {{logger.exception}} not {{logger.error}} to get the full traceback, else it's harder to tell what happened."	ARROW	Resolved	3	4	6496	pull-request-available
13365315	[C++][Dataset] Extract IpcFragmentScanOptions, ParquetFragmentScanOptions	Follow-up to ARROW-9749.	ARROW	Resolved	3	3	6496	dataset, datasets, pull-request-available
13327668	[C++][CI] Flight test failure in TestFlightClient.GenericOptions	"This failure seems non-deterministic?
On AppVeyor (Windows): https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/35214577/job/7aolb47v3s80pdgb
{code}
[ RUN      ] TestFlightClient.GenericOptions
C:/projects/arrow/cpp/src/arrow/flight/flight_test.cc(1429): error: Failed
Expected 'status' to fail with Invalid, but got OK
[  FAILED  ] TestFlightClient.GenericOptions (5 ms)
{code}

On GitHub Actions (macOS): https://github.com/apache/arrow/pull/8193/checks?check_run_id=1117619662#step:6:217
{code}
[ RUN      ] TestFlightClient.GenericOptions
/Users/runner/work/arrow/arrow/cpp/src/arrow/flight/flight_test.cc:1429: Failure
Failed
Expected 'status' to fail with Invalid, but got OK
[  FAILED  ] TestFlightClient.GenericOptions (6 ms)
{code}
"	ARROW	Resolved	3	1	6496	pull-request-available
13315466	[Integration] Reconsider generated_large_batch.json	"The dominant part of the time spent running the integration tests is one test case named ""generated_large_batch.json"". It is not obvious how useful this test case is. Ideally, integration test cases should be small enough to only take a fraction of a second."	ARROW	Resolved	3	5	6496	pull-request-available
13480155	[Doc] Remove experimental marker for Flight RPC in feature matrix	In https://arrow.apache.org/docs/dev/status.html, Flight RPC is still marked experimental. We should probably remove that mention as it was already removed from the corresponding format specification page (https://arrow.apache.org/docs/dev/format/Flight.html)	ARROW	Resolved	2	3	6496	pull-request-available
13279561	[C++][Flight] Auth handler tests fragile on Windows	"This occurs often on AppVeyor:
{code}
[----------] 3 tests from TestAuthHandler
[ RUN      ] TestAuthHandler.PassAuthenticatedCalls
[       OK ] TestAuthHandler.PassAuthenticatedCalls (4 ms)
[ RUN      ] TestAuthHandler.FailUnauthenticatedCalls
..\src\arrow\flight\flight_test.cc(1126): error: Value of: status.message()
Expected: has substring ""Invalid token""
  Actual: ""Could not write record batch to stream: ""
[  FAILED  ] TestAuthHandler.FailUnauthenticatedCalls (3 ms)
[ RUN      ] TestAuthHandler.CheckPeerIdentity
[       OK ] TestAuthHandler.CheckPeerIdentity (2 ms)
[----------] 3 tests from TestAuthHandler (10 ms total)
[----------] 3 tests from TestBasicAuthHandler
[ RUN      ] TestBasicAuthHandler.PassAuthenticatedCalls
[       OK ] TestBasicAuthHandler.PassAuthenticatedCalls (4 ms)
[ RUN      ] TestBasicAuthHandler.FailUnauthenticatedCalls
..\src\arrow\flight\flight_test.cc(1224): error: Value of: status.message()
Expected: has substring ""Invalid token""
  Actual: ""Could not write record batch to stream: ""
[  FAILED  ] TestBasicAuthHandler.FailUnauthenticatedCalls (4 ms)
[ RUN      ] TestBasicAuthHandler.CheckPeerIdentity
[       OK ] TestBasicAuthHandler.CheckPeerIdentity (3 ms)
[----------] 3 tests from TestBasicAuthHandler (11 ms total)
{code}

See e.g. https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/30110376/job/vbtd22813g5hlgfl#L2252"	ARROW	Resolved	4	1	6496	pull-request-available
13387394	[C++][FlightRPC] Segfault when sending record batch >2GB	"When sending a record batch > 2GiB, the server will segfault. Although Flight checks for this case and returns an error, it turns out that gRPC always tries to increment the refcount of the result buffer whether the serialization handler returned successfully or not:
{code:cpp}
// From gRPC 1.36
Status CallOpSendMessage::SendMessagePtr(const M* message,
                                         WriteOptions options) {
  msg_ = message;
  write_options_ = options;
  // Store the serializer for later since we have access to the message
  serializer_ = [this](const void* message) {
    bool own_buf;
    // TODO(vjpai): Remove the void below when possible
    // The void in the template parameter below should not be needed
    // (since it should be implicit) but is needed due to an observed
    // difference in behavior between clang and gcc for certain internal users
    Status result = SerializationTraits<M, void>::Serialize(
        *static_cast<const M*>(message), send_buf_.bbuf_ptr(), &own_buf);
    if (!own_buf) {
      // XXX(lidavidm): This should perhaps check result.ok(), or Serialize should
      // unconditionally initialize send_buf_
      send_buf_.Duplicate();
    }
    return result;
  };
  return Status();
}
{code}
Hence when Flight returns an error without initializing the buffer, we get a segfault.

Originally reported on StackOverflow: [https://stackoverflow.com/questions/68230146/pyarrow-flight-do-get-segfault-when-pandas-dataframe-over-3gb]"	ARROW	Resolved	3	1	6496	pull-request-available
13433410	[Format][FlightRPC] Clarify FlightEndpoint.locations	"[2022/03/11 ""[Java] [Flight] Usage of Locations on FlightEndpoint"" on user@arrow.apache.org|https://lists.apache.org/thread/1668fk1myqf8168xh296qck5fn8ztcmn]

The {{locations}} field of {{FlightEndpoint}} was meant to have a particular interpretation, we should make this clear in the protocol comments."	ARROW	Resolved	3	4	6496	pull-request-available
13364996	[C++] arrow/util/io_util.cc does not compile on Solaris	"Looks similar to ARROW-11740

{code}
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc: In function ‘arrow::Status arrow::internal::MemoryMapRemap(void*, std::size_t, std::size_t, int, void**)’:
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1089:48: error: ‘MREMAP_MAYMOVE’ was not declared in this scope
*new_addr = mremap(addr, old_size, new_size, MREMAP_MAYMOVE);
 ^
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1089:62: error: ‘mremap’ was not declared in this scope
*new_addr = mremap(addr, old_size, new_size, MREMAP_MAYMOVE);
 ^
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc: In function ‘arrow::Status arrow::internal::MemoryAdviseWillNeed(const std::vector&)’:
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1144:59: error: ‘POSIX_MADV_WILLNEED’ was not declared in this scope
int err = posix_madvise(aligned.addr, aligned.size, POSIX_MADV_WILLNEED);
 ^
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1144:78: error: ‘posix_madvise’ was not declared in this scope
int err = posix_madvise(aligned.addr, aligned.size, POSIX_MADV_WILLNEED);
 ^
{code}"	ARROW	Resolved	3	7	8135	pull-request-available
13195164	[C++] Allow whitespace in numeric CSV fields	"Pandas allows whitespace before and after numbers in CSV files, but Arrow doesn't:
{code:python}
>>> s = b""a,b,c\n12 , 34 , 56\n""
>>> pd.read_csv(io.BytesIO(s))
    a   b   c
0  12  34  56
>>> csv.read_csv(io.BytesIO(s)).to_pandas()
        a        b       c
0  b'12 '  b' 34 '  b' 56'
{code}
"	ARROW	Resolved	3	4	8135	pull-request-available
13259938	[CI] ccache doesn't cache on Travis-CI	Looks like our ccache setup on Travis is broken. This is seen by the cache hits statistics printed at the end of each job.	ARROW	Resolved	3	1	8135	pull-request-available
13278273	[C++][CI] Build parquet support in the VS2019 GitHub Actions job	Enable ARROW_PARQUET cmake flag. Additional patching might be required, see https://github.com/microsoft/vcpkg/pull/8263/files	ARROW	Resolved	3	4	8135	pull-request-available
13372017	"[Archery] Error running ""crossbow submit ..."""	"{code:python}
$ archery crossbow submit -g cpp
/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!
  RequestsDependencyWarning)
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/bin/archery"", line 33, in <module>
    sys.exit(load_entry_point('archery', 'console_scripts', 'archery')())
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/decorators.py"", line 27, in new_func
    return f(get_current_context().obj, *args, **kwargs)
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/cli.py"", line 113, in submit
    config = Config.load_yaml(config_path)
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/core.py"", line 1060, in load_yaml
    params={})
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/core.py"", line 122, in _render_jinja_template
    loader = jinja2.FileSystemLoader(searchpath)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/jinja2/loaders.py"", line 163, in __init__
    self.searchpath = list(searchpath)
TypeError: 'PosixPath' object is not iterable
{code}
"	ARROW	Resolved	3	1	8135	pull-request-available
13283027	[C++] Link some more tests together	With unity builds (ARROW-7725) it may become more beneficial to reduce the number of test executables, as several C++ files could be compiled together so as to reduce build times.	ARROW	Resolved	4	4	8135	pull-request-available
13188884	[C++] Make CSV chunker faster	Currently the CSV chunker can be the bottleneck in multi-threaded reads (starting from 6 threads, according to my experiments). One way to make it faster is to consider by default that CSV values cannot contain newline characters (overridable via a setting), and then simply search for the last newline character in each block of data.	ARROW	Resolved	3	4	8135	pull-request-available
13413192	[C++] Fix crash when validating corrupt list array (OSS-Fuzz)	As found by OSS-Fuzz: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=41143	ARROW	Resolved	3	1	8135	pull-request-available
13283607	[C++] Support nested dictionaries in JSON integration format	The {{generate_nested_dictionary_case}} is disabled for all library implementations. We support dictionaries-within-dictionaries in IPC (I believe) and so need to integration test this	ARROW	Resolved	3	4	8135	pull-request-available
13208092	[Python] TestConvertStructTypes.test_from_numpy_large failing	"This is half of ARROW-4179 (this test should not be running in Travis CI at all). This failure appears to be a regression, and was not caught because we do not regularly run the large_memory tests

[~kszucs] now that we have these large memory DGX machines we should run the {{large_memory}} unit tests at least once a day"	ARROW	Resolved	3	1	8135	pull-request-available
13334145	[C++] String split kernels do not propagate nulls correctly on sliced input	"I am not sure if this is a specific test issue or valid behavior, but when writing a test in [https://github.com/apache/arrow/pull/8271] 

The following test fails:
{code:java}
this->CheckUnary(""split_pattern"", R""([""foo bar"", ""foo"", null])"", list(this->type()),  //                  R""([[""foo"", ""bar""], [""foo""], null])"", &options);
{code}
with the following output
{code:java}
Failed:
Got: 
  [
    [
      [
        ""foo"",
        ""bar""
      ]
    ],
    [
      [
        ""foo""
      ],
      null
    ]
  ]
Expected: 
  [
    [
      [
        ""foo"",
        ""bar""
      ]
    ],
    [
      [
        ""foo""
      ],
      null
    ]
  ]
{code}
while the outputs are the same, the arrays are seen as unequal."	ARROW	Resolved	4	4	8135	pull-request-available
13142827	[Python] test_plasma spams /tmp	{{test_plasma}} creates a new socket in {{/tmp}} for each test and never cleans up.	ARROW	Resolved	5	1	8135	pull-request-available
13149995	[C++/Python] CheckPyError() could inspect exception type	"Current {{CheckPyError}} always chooses an ""unknown error"" status. But it could inspect the Python exception and choose, e.g. ""type error"" for a {{TypeError}} exception, etc.

See also ARROW-2389"	ARROW	Resolved	3	5	8135	pull-request-available
13184141	[C++] Rename libarrow_gpu to libarrow_cuda	I'm proposing to rename this library since we could conceivably have OpenCL bindings in the repository also	ARROW	Resolved	3	4	8135	pull-request-available
13316728	[Python] Make more objects weakrefable	Currently, some PyArrow objects (like Array) are weakrefable, but others (like Buffer) are not. There's no reason not to allow that, it just needs the required (short) boilerplate.	ARROW	Resolved	3	5	8135	pull-request-available
13212898	[Python] Cannot create empty StructArray via pa.StructArray.from_arrays	"{code:python}
In [5]: pa.StructArray.from_arrays([], names=[])
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-d6fa9cf39f31> in <module>
----> 1 pa.StructArray.from_arrays([], names=[])

~/Workspace/arrow/python/pyarrow/array.pxi in pyarrow.lib.StructArray.from_arrays()
   1326         num_arrays = len(arrays)
   1327         if num_arrays == 0:
-> 1328             raise ValueError(""arrays list is empty"")
   1329
   1330         length = len(arrays[0])

ValueError: arrays list is empty
{code}

however

{code:python}
pa.array([], type=pa.struct([]))
{code}

works"	ARROW	Resolved	3	1	8135	pull-request-available
13344823	[C++] Add async filesystem operations	It would probably be useful to have Future-returning variants of some filesystem operations (at least {{GetFileInfo}} and {{OpenInput(File|Stream)}}).	ARROW	Resolved	3	4	8135	pull-request-available
13406173	"[Dev][CI] ""linux-apt-r"" dockerfile reinstalls Minio"	Minio is already installed in the base (e.g. ubuntu-cpp) image, there's no point in reinstalling it (especially as the download server can be slow from some places).	ARROW	Resolved	5	1	8135	pull-request-available
13168279	[Python] Writing to parquet crashes when writing a ListArray of empty lists 	"When writing a ListArray which contains only empty lists to Parquet, Pyarrow crashes. Here is a minimal code snippet which reproduces the crash:
{code:java}
import pyarrow as pa
from pyarrow import parquet as pq

array = pa.array([[]], type=pa.list_(pa.int32()))
table = pa.Table.from_arrays([array], [""A""])
pq.write_table(table, ""tmp.parq""){code}
When the ListArray has at least one non-empty list, the issue disappears.

 "	ARROW	Resolved	3	1	8135	parquet, pull-request-available
13420575	[CI][Python] Broken PyArrow install on some crossbow jobs	"See example here:
https://dev.azure.com/ursacomputing/crossbow/_build/results?buildId=18133&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181

{code}
============================= test session starts ==============================
platform linux -- Python 3.9.2, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /usr/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/.hypothesis/examples')
rootdir: /
plugins: lazy-fixture-0.6.3, hypothesis-6.34.1
collecting ... collected 0 items

============================ no tests ran in 0.48s =============================
ERROR: module or package not found: pyarrow (missing __init__.py?)
{code}
"	ARROW	Resolved	3	1	8135	pull-request-available
13357725	[CI] Cancel stale Github Actions workflow runs	"When a new changeset is pushed in a PR, we should probably cancel previous GHA builds in that PR. This is not done automatically by  Github, but can be enabled using a third-party action:

https://github.com/potiuk/cancel-workflow-runs

Here is an example of use:

[https://github.com/apache/pulsar/blob/master/.github/workflows/ci-cancel-duplicate-workflows.yaml]

 "	ARROW	Resolved	4	5	8135	pull-request-available
13236508	[C++] Local filesystem implementation: investigate Windows UNC paths	"Followup to ARROW-5378: Windows paths to networked files (e.g. ""\\server\share\path\file.txt"") and extended-length paths (e.g. ""\\?\c:\some\absolute\path.txt"") should be checked for compatibility with the LocalFileSystem implementation."	ARROW	Resolved	3	3	8135	pull-request-available
13384150	[C++] CreateDir should fail if the target exists and is not a directory	As discussed in https://github.com/apache/arrow/pull/10540#issuecomment-862284472 .	ARROW	Resolved	4	4	8135	pull-request-available
13391162	[Dev][Archery] Archery import pandas which imports pyarrow	"Just got this error when trying to run benchmarks on a PR:
{code}
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/bin/archery"", line 33, in <module>
    sys.exit(load_entry_point('archery', 'console_scripts', 'archery')())
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/antoine/arrow/dev/dev/archery/archery/cli.py"", line 634, in benchmark_diff
    no_counters, ren_counters)
  File ""/home/antoine/arrow/dev/dev/archery/archery/cli.py"", line 650, in _format_comparisons_with_pandas
    import pandas as pd
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/__init__.py"", line 50, in <module>
    from pandas.core.api import (
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/core/api.py"", line 29, in <module>
    from pandas.core.arrays import Categorical
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/core/arrays/__init__.py"", line 20, in <module>
    from pandas.core.arrays.string_arrow import ArrowStringArray
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/core/arrays/string_arrow.py"", line 65, in <module>
    import pyarrow.compute as pc
  File ""/home/antoine/arrow/dev/python/pyarrow/compute.py"", line 18, in <module>
    from pyarrow._compute import (  # noqa
ImportError: cannot import name 'ProjectOptions' from 'pyarrow._compute' (/home/antoine/arrow/dev/python/pyarrow/_compute.cpython-37m-x86_64-linux-gnu.so)
{code}

Since Archery is a tool for developing Arrow, current PyArrow may be broken or incompatible with the currently available Arrow C++."	ARROW	Resolved	3	1	8135	pull-request-available
13204084	[CI] Run Valgrind and C++ code coverage in different bulds	"Currently, we run Valgrind on a coverage-enabled C++ build on Travis-CI. This means the slowness of Valgrind acts as a multiplier of the overhead of outputting coverage information using the instrumentation added by the compiler.

Instead we should probably emit C++ (and Python) coverage information in a different Travis-CI build without Valgrind enabled."	ARROW	Resolved	3	4	8135	pull-request-available
13249322	[Python][C++] UnionArray with invalid data passes validation / leads to segfaults	"From the Python side, you can create an ""invalid"" UnionArray:

{code}
binary = pa.array([b'a', b'b', b'c', b'd'], type='binary') 
int64 = pa.array([1, 2, 3], type='int64') 
types = pa.array([0, 1, 0, 0, 2, 1, 0], type='int8')   # <- value of 2 is out of bound for number of childs
value_offsets = pa.array([0, 0, 2, 1, 1, 2, 3], type='int32')

a = pa.UnionArray.from_dense(types, value_offsets, [binary, int64])
{code}

Eg on conversion to python this leads to a segfault:

{code}
In [7]: a.to_pylist()
Segmentation fault (core dumped)
{code}

On the other hand, doing an explicit validation does not give an error:

{code}
In [8]: a.validate()
{code}

Should the validation raise errors for this case? (the C++ {{ValidateVisitor}} for UnionArray does nothing) 

(so that this can be called from the Python API to avoid creating invalid arrays / segfaults there)
"	ARROW	Resolved	3	1	8135	pull-request-available
13268102	[C++] Cleanup warnings in cmake_modules/SetupCxxFlags.cmake	For clang we currently disable a lot of warnings explicitly. This dates back to when we enabled {{-Weverything}}. We should probably remove most or all of these flags now.	ARROW	Resolved	4	5	8135	pull-request-available
13377070	[C++] extract_regex gives bizarre behavior after nulls or non-matches	"After a non-match, the *subsequent* string may match ... but its data is in the wrong array element.

{code}
>>> pa.compute.extract_regex(pa.array([""a"", ""b"", ""c"", ""d""]), pattern=""(?P<x>[^b])"")
<pyarrow.lib.StructArray object at 0x7f80de918ee0>
-- is_valid:
  [
    true,
    false,
    true,
    true
  ]
-- child 0 type: string
  [
    ""a"",
    """",
    """",
    ""c""
  ]
{code}

Same if trying to match after {{null}}:

{code}
>>> pa.compute.extract_regex(pa.array([""a"", None, ""c"", ""d"", ""e""]), pattern=""(?P<x>[^b])"")
<pyarrow.lib.StructArray object at 0x7f80de918ee0>
-- is_valid:
  [
    true,
    false,
    true,
    true,
    true
  ]
-- child 0 type: string
  [
    ""a"",
    """",
    """",
    ""c"",
    ""d""
  ]
{code}

Workaround: 1) filter out non-matches; 2) extract only the matching strings; 3) interpolate nulls:

{code:python}
def _extract_regex_workaround_arrow_12670(
    array: pa.StringArray, *, pattern: str
) -> pa.StructArray:
    ok = pa.compute.match_substring_regex(array, pattern=pattern)
    good = array.filter(ok)
    good_matches = pa.compute.extract_regex(good, pattern=pattern)

    # Build array that looks like [None, 1, None, 2, 3, 4, None, 5]
    # ... ok_nonnull: [False, True, False, True, True, True, False, True]
    # (not ok.fill_null(False).cast(pa.int8()) because of ARROW-12672 segfault)
    ok_nonnull = pa.compute.and_kleene(ok.is_valid(), ok)
    # ... np_ok: [0, 1, 0, 1, 1, 1, 0, 1]
    np_ok = ok_nonnull.cast(pa.int8()).to_numpy(zero_copy_only=False)
    # ... np_index: [0, 1, 1, 2, 3, 4, 4, 5]
    np_index = np.cumsum(np_ok, dtype=np.int64) - 1
    # ...index_or_null: [None, 1, None, 3, 4, 5, None, 5]
    valid = ok_nonnull.buffers()[1]
    index_or_null = pa.Array.from_buffers(
        pa.int64(), len(array), [valid, pa.py_buffer(np_index)]
    )

    return good_matches.take(index_or_null)
{code}"	ARROW	Resolved	3	1	8135	pull-request-available
13181410	[Python] Update ASV instructions	"The ability to define custom install / build / uninstall commands was added in mainline ASV in https://github.com/airspeed-velocity/asv/pull/699
We don't need to use our own fork / PR anymore."	ARROW	Resolved	3	1	8135	pull-request-available
13251262	[C++][Fuzzing] Add fuzzer for parquet->arrow read path	The parquet to arrow read path is likely the most commonly used one (esp. by pyarrow) and is a closed step that should allow us to fuzz the reading of untrusted parquet files into memory. This complements the existing arrow ipc fuzzer.	ARROW	Closed	3	1	8135	fuzzer, pull-request-available
13278830	[C++] TestSlowInputStream is flaky	"See https://github.com/apache/arrow/pull/6160/checks?check_run_id=384146741#step:5:1556 for example

{code}
[ RUN      ] TestSlowInputStream.Basics
/arrow/cpp/src/arrow/io/memory_test.cc:308: Failure
Expected: (dt) < (latency * 3), actual: 4.96068 vs 1.8
[  FAILED  ] TestSlowInputStream.Basics (4961 ms)
[----------] 1 test from TestSlowInputStream (4961 ms total)
{code}

Tests that rely on timing are pretty tough to do on public CI. We should consider moving this somewhere that doesn't run on CI."	ARROW	Resolved	3	1	8135	pull-request-available
13162387	[C++] Investigate spurious memset() calls	"{{builder.cc}} has TODO statements of the form:

{code:c++}
  // TODO(emkornfield) valgrind complains without this
  memset(data_->mutable_data(), 0, static_cast<size_t>(nbytes));
{code}

Ideally we shouldn't have to zero-initialize a data buffer before writing to it.
"	ARROW	Resolved	3	4	8135	pull-request-available
13242891	[C++] Factor out status copying code from cast.cc	FUNC_RETURN_NOT_OK reconstructs a status object including line and status information.  This should be replaced with suitable methods from Status instead.   The macro is probably still useful since it returns by setting Status on the context instead of directly returning from a function	ARROW	Resolved	4	4	8135	pull-request-available
13405970	[C++] Fix crashes when pretty-printing data from valid IPC file (OSS-Fuzz)	"Fix the following issues found by OSS-Fuzz:
* https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=39677
* https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=39703
* https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=39763
* https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=39773
"	ARROW	Resolved	3	1	8135	pull-request-available
13398254	"[C++] Deprecate Parquet pseudo-version ""2.0"""	"{{ParquetVersion::type}} currently has two members: {{PARQUET_1_0}} and {{PARQUET_2_0}}. The latter enables a hodgepodge of features which were actually added in Parquet format 2.4 and 2.6 (and others?).

To allow more fine-grained selection of the format version, we should deprecate this value and add {{PARQUET_2_4}} and {{PARQUET_2_6}} values."	ARROW	Resolved	3	3	8135	pull-request-available
13288983	[C++] ListBuilder.Finish fails if underlying value builder is empty and .Reserve'd	"Here's a reproduction:
{code:java}
#include <arrow/builder.h>
#include <arrow/status.h>
#include <iostream>

int main() {
        arrow::ListBuilder lb(arrow::default_memory_pool(), std::unique_ptr<arrow::ArrayBuilder>(new arrow::Int32Builder()));
        lb.value_builder()->Reserve(100); // bug
        lb.Append();
        std::shared_ptr<arrow::Array> ar;
        arrow::Status st = lb.Finish(&ar);
        if (!st.ok()) {
                std::cerr << st << '\n';
                return 1;
        }
}
{code}
The output is
{noformat}
Invalid: Resize cannot downsize{noformat}
The Resize call is made at builder_nested.h, line 115. There's a note there about ARROW-2744. Perhaps the fix is to look at capacity rather than length?"	ARROW	Resolved	3	1	8135	pull-request-available
13491310	"[Dev][Archery] ""archery docker run"" sets env var to None when inherited"	"When an environment variable is marked inherited in {{docker-compose.yml}} and it's not set in the calling environment, Archery sets it to ""None"" in the container."	ARROW	Resolved	3	1	8135	pull-request-available
13215574	[Python] pa.decimal128 should validate inputs	"The precision shouldn't be higher than 38, but 39 is happily accepted currently:
{code:python}
>>> ty = pa.decimal128(39, 0)                                                                            
>>> arr = pa.array([2**127], type=ty)                                                                  
>>> arr                                                                                                  
<pyarrow.lib.Decimal128Array object at 0x7f9b89444138>
[
  -170141183460469231731687303715884105728
]
{code}

"	ARROW	Resolved	3	1	8135	pull-request-available
13381466	[C++][R][pyarrow] Failure importing some decimal types using the C data interface	"In my R notebook, I try to read data from a Db2 database (""SELECT CAST (15 as decimal(5,0)) FROM sysibm.sysdummy1"") into an R dataframe, leveraging arrow/flight.
integers, varchars, etc can be loaded without issues, but when I use a decimal type, an error is thrown.

 

Here is the code I'm running:

 
{code:java}
library(""reticulate"")
library(""arrow"")
itcfs <- import(""..."")
readClient <- itcfs$get_flight_client()
Manual_data_request = ....
flightInfo <- itcfs$get_flight_info(readClient,data_request=Manual_data_request)

tables <- itcfs$read_tables(readClient, flightInfo)
...
 
{code}
 

{{The itcfs package is implemented in Python, and here is the read_tables method:}}

 
{color:#0000ff}def{color}{color:#000000} read_tables(read_client, flight_info):{color}
{color:#a31515}""""""Read a list of pyarrow.Table""""""{color}
{color:#000000} tables = []{color}
{color:#0000ff}for{color}{color:#000000} endpoint {color}{color:#0000ff}in{color}{color:#000000} flight_info.endpoints:{color}
{color:#000000} reader = read_client.do_get(endpoint.ticket){color}
{color:#000000} batches = [b.data {color}{color:#0000ff}for{color}{color:#000000} b {color}{color:#0000ff}in{color}{color:#000000} reader]{color}
{color:#000000} tables.append(pa.Table.from_batches(batches)){color}
{color:#0000ff}return{color}{color:#000000} tables{color}
 
This is the erro message:

 
{code:java}
Error: Invalid: Invalid or unsupported format string: 'd:5,0'
Traceback:
1. itcfs$read_tables(readClient, flightInfo)
2. py_to_r(result)
3. py_to_r.python.builtin.list(result)
4. lapply(converted, function(object) {
 . if (inherits(object, ""python.builtin.object"")) 
 . py_to_r(object)
 . else object
 . })
5. FUN(X[[i]], ...)
6. py_to_r(object)
7. py_to_r.pyarrow.lib.Table(object)
8. maybe_py_to_r(x$columns)
9. x$columns
10. `$.python.builtin.object`(x, ""columns"")
11. py_get_attr_or_item(x, name, TRUE)
12. py_maybe_convert(object, py_has_convert(x))
13. py_to_r(x)
14. py_to_r.python.builtin.list(x)
15. lapply(converted, function(object) {
 . if (inherits(object, ""python.builtin.object"")) 
 . py_to_r(object)
 . else object
 . })
16. FUN(X[[i]], ...)
17. py_to_r(object)
18. py_to_r.pyarrow.lib.ChunkedArray(object)
19. ChunkedArray$create(!!!maybe_py_to_r(x$chunks))
20. ChunkedArray__from_list(list2(...), type)
21. list2(...)
22. maybe_py_to_r(x$chunks)
23. x$chunks
24. `$.python.builtin.object`(x, chunks)
25. py_get_attr_or_item(x, name, TRUE)
26. py_maybe_convert(object, py_has_convert(x))
27. py_to_r(x)
28. py_to_r.python.builtin.list(x)
29. lapply(converted, function(object) {
 . if (inherits(object, ""python.builtin.object"")) 
 . py_to_r(object)
 . else object
 . })
30. FUN(X[[i]], ...)
31. py_to_r(object)
32. py_to_r.pyarrow.lib.Array(object)
33. ImportArray(array_ptr, schema_ptr)
{code}
 

 

In a pure python envrionment, decimal data can be read without issues."	ARROW	Resolved	3	1	8135	pull-request-available
13224158	[C++][Gandiva] Split Gandiva-related conda packages for builds into separate .yml conda env file	These installs are large and should not be required unconditionally in CI and elsewhere	ARROW	Resolved	3	4	8135	pull-request-available
13205418	[C++] Add machine benchmarks	I wonder if it may be useful to add machine benchmarks. I have a cache/memory latency benchmark lying around, we could also add e.g. memory bandwidth benchmarks.	ARROW	Resolved	4	5	8135	pull-request-available
13135132	[Python] Expose Array's buffers to Python users	This amounts to converting {{arr->data()->buffers}} to a list of {{pyarrow.Buffer}} objects	ARROW	Resolved	3	4	8135	pull-request-available
13246415	[C++] CSV reader ignore_empty_lines option doesn't handle empty lines	"Followup to https://issues.apache.org/jira/browse/ARROW-5747. If {{ignore_empty_lines}} is false and there are empty lines, it fails to parse (again, with {{Invalid: Empty CSV file}}).

Correct behavior should be to fill those empty lines with missing data for all columns."	ARROW	Resolved	4	1	8135	csv, pull-request-available
13380816	"[CI] Use ""concurrency"" setting on Github Actions"	"We're currently using a dedicated Github Actions to cancel previous jobs when a new job is queued. It seems this now can be done better using the ""concurrency"" setting (unfortunately in beta):
https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#concurrency
"	ARROW	Resolved	4	3	8135	pull-request-available
13280609	[C++] Update generated flatbuffers files	The field added in  ARROW-6836 should be reflected in the generated C++ code.	ARROW	Resolved	3	3	8135	pull-request-available
13237672	[C++] Printer for uint64 shows wrong values	"From the example in ARROW-5430:

{code}
In [16]: pa.array([14989096668145380166, 15869664087396458664], type=pa.uint64())                                                                                                                                   
Out[16]: 
<pyarrow.lib.UInt64Array object at 0x7ff7c51bdf48>
[
  -3457647405564171450,
  -2577079986313092952
]
{code}

I _think_ the actual conversion is correct, and it's only the printer that is going wrong, as {{to_numpy}} gives the correct values:

{code}
In [17]: pa.array([14989096668145380166, 15869664087396458664], type=pa.uint64()).to_numpy()                                                                                                                        
Out[17]: array([14989096668145380166, 15869664087396458664], dtype=uint64)
{code}"	ARROW	Resolved	4	1	8135	pull-request-available
13346706	[C++] Reading empty json lists results in invalid non-nullable null type	"We're using Arrow to convert from JSON to Parquet and occasionally have empty lists in our json. Reading such JSON into an Arrow table and writing it to Parquet currently fails. We noticed this issue in our C++ Arrow code, but it also happens from Python.

Minimal repro:

input.json:

{""foo"": []}

 

convert.py:
 import pyarrow.json
 import pyarrow.parquet

t = pyarrow.json.read_json(""input.json"")
 pyarrow.parquet.write_table(t, ""out.parquet"")
  

Produces:

Traceback (most recent call last):
 File ""repro.py"", line 5, in <module>
 pyarrow.parquet.write_table(t, ""out.parquet"")
env/lib/python3.8/site-packages/pyarrow/parquet.py"", line 1717, in write_table
 with ParquetWriter(
 File ""env/lib/python3.8/site-packages/pyarrow/parquet.py"", line 554, in __init__
 self.writer = _parquet.ParquetWriter(
 File ""pyarrow/_parquet.pyx"", line 1409, in pyarrow._parquet.ParquetWriter.__cinit__
 File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
 pyarrow.lib.ArrowInvalid: NullType Arrow field must be nullable

 "	ARROW	Resolved	3	1	8135	json, pull-request-available
13316737	[CI][Java] Run Maven in parallel	"It looks like Maven nowadays supports multi-threaded builds, but we're not using them:
https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3
"	ARROW	Resolved	4	4	8135	pull-request-available
13260446	"[CI] Decommission ""C++ with clang 7 and system packages"" Travis CI job"	"Now that this is running in GitHub Actions, we can probably skip it in Travis CI?

Any other barriers to turning this off and saving the CI build time?"	ARROW	Resolved	3	4	8135	pull-request-available
13208060	[C++] Fix TSan and UBSan errors	clang's Thread Sanitizer and Undefined Behaviour Sanitizer report a number of alerts. We should strive to fix or workaround them.	ARROW	Resolved	3	1	8135	pull-request-available
13180240	[C++] Add benchmark for number parsing	Number parsing will become important once we have a CSV reader (or possibly other text-based formats). We should add benchmarks for the internal conversion routines.	ARROW	Resolved	3	5	8135	pull-request-available
13163376	[Python] Experiment with zero-copy pickling	"PEP 574 has an implementation ready and a PyPI-available backport (at [https://pypi.org/project/pickle5/] ). Adding experimental support for it would allow for zero-copy pickling of Arrow arrays, columns, etc.

I think it mainly involves implementing {{reduce_ex}} on the {{Buffer}} class, as described in [https://www.python.org/dev/peps/pep-0574/#producer-api]

In addition, the consumer API added by PEP 574 could be used in Arrow's serialization layer, to avoid or minimize copies when serializing foreign objects."	ARROW	Resolved	3	5	8135	pull-request-available
13433913	[C++] Allow setting IO thread pool size with an environment variable	See https://issues.apache.org/jira/browse/ARROW-14354 and https://github.com/apache/arrow/pull/12624#discussion_r827088337 for discussion.	ARROW	Resolved	3	4	8135	good-second-issue, pull-request-available
13367627	[C++] offline builds does not use ARROW_$LIBRARY_URL to search for packages	"I am following the instructions in order to build the parquet libraries, and there are some firewall restrictions in our dvpmt environment.

I have followed the instructions mentioned in [https://github.com/apache/arrow/blob/master/docs/source/developers/cpp/building.rst]

regarding Offline Builds: 
{code:java}
./thirdparty/download_dependencies.sh $HOME/arrow-thirdparty >env.sh
. ./env.sh
cd cpp
mkdir debug
cd debug
cmake -DCMAKE_BUILD_TYPE=Debug -DARROW_PARQUET=ON -DARROW_DATASET=ON ..
make parquet arrow_dataset{code}
The result is that the process simply ignores the environment variables and tries to download the packages from the cloud - facing the firewall restrictions, and then it fails after a number of attempts at trying to download the packages.
h3.  "	ARROW	Resolved	3	1	8135	pull-request-available
13353283	[C++] Spurious test failure when creating temporary dir	"When running the release verification script, I sometimes get this error:
{code}
[----------] 5 tests from TestInt8/TestSparseTensorRoundTrip/0, where TypeParam = arrow::Int8Type
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexRowMajor
/tmp/arrow-3.0.0.4SRpe/apache-arrow-3.0.0/cpp/src/arrow/ipc/tensor_test.cc:53: Failure
Failed
'_error_or_value8.status()' failed with IOError: Path already exists: '/tmp/ipc-test-qj6ng827/'
[  FAILED  ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexRowMajor, where TypeParam = arrow::Int8Type (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexColumnMajor
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexColumnMajor (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSRIndex
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSRIndex (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSCIndex
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSCIndex (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSFIndex
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSFIndex (1 ms)
[----------] 5 tests from TestInt8/TestSparseTensorRoundTrip/0 (1 ms total)
{code}

It seems that in some fringe cases, the random generation of temporary directory names produces duplicates. Most likely this means the random generator is getting the same seed from different processes."	ARROW	Resolved	4	1	8135	pull-request-available
13192653	[CI] Disable optimizations on Windows	Disabling compiler optimizations, even in release mode, should allow builds to become a bit faster.	ARROW	Resolved	4	4	8135	pull-request-available
13336278	[C++] Parquet decompresses DataPageV2 pages even if is_compressed==0	"According to the parquet-format specification, DataPageV2 pages have an is_compressed flag. Even if the column chunk has a decompression codec set, the page is only compressed if this flag is true (this likely enables not compressing some pages where the compression wouldn't save memory).

Here is the relevant excerpt from parquet.thrift describing the semantics of the is_compressed flag in a DataPageV2:

_/** whether the values are compressed._
 _Which means the section of the page between_
 _definition_levels_byte_length + repetition_levels_byte_length + 1 and compressed_page_size (included)_
 _is compressed with the compression_codec._
 _If missing it is considered compressed */_
 _7: optional bool is_compressed = 1;_

 

It seems that the apache parquet cpp library (haven't checked other languages but might have the bug as well) totally disregards this flag and decompresses the page in all cases if a decompressor is set for the column chunk.

The erroneous code is in column_reader.cc: 

std::shared_ptr<Page> SerializedPageReader::NextPage() 

This method first decompresses the page if there is a decompressor set and only then does a case distinction on whether this page is a DataPageV2 and has the is_compressed flag. Thus, even if the page would have this flag set to 0, the page would be decompressed anyway.

The method that should use the is_compressed flag but doesn't is:

std::shared_ptr<Buffer> SerializedPageReader::DecompressPage

This method doesn't look at the is_compressed flag at all.

 

The reason why this bug probably doesn't show in any unit test is that the write implementation seems to do the same mistake: It always compresses the page, even if the page has its is_compressed flag set to false.

 "	ARROW	Resolved	3	1	8135	pull-request-available
13292478	[C++] Rename GetTargetInfos	"Sorry, but I think I'm irked by the new ""GetTargetInfos"" spelling.
I suggest either ""GetTargetInfo"" or ""GetFileInfo"" (both singular)."	ARROW	Resolved	5	5	8135	pull-request-available
13325786	[C++] Add hand-crafted Parquet to Arrow reconstruction test for nested reading	"We should write tests where definition and repetition levels are explicitly written out for a particular Parquet schema, then read as a Arrow column.

Sketch here:
https://gist.github.com/pitrou/282dd790cac0eb2c1b59e8c9ab1941d8"	ARROW	Resolved	3	7	8135	pull-request-available
13135463	[Python] Create StructArray from sequence of tuples given a known data type	"Following ARROW-1705, we should support calling {{pa.array}} with a sequence of tuples, presuming a struct type is passed for the {{type}} parameter.

We also probably want to disallow mixed inputs, e.g. a sequence of both dicts and tuples. The user should use only one idiom at a time."	ARROW	Resolved	3	4	8135	pull-request-available
13505430	[C++] Avoid global variables for thread pools and at-fork handlers	"Investigation revealed an issue where the global IO thread pool could be constructed before the at-fork handler internal state. The IO thread pool, created on library load, would register an at-fork handler; then, the at-fork handler state would be initialized and clobber the handler registered just before.
"	ARROW	Resolved	3	1	8135	pull-request-available
13368930	[C++] Fix compressed file reading with an empty stream at end of file	"Initially reported on the arrow-user mailing-list:
https://mail-archives.apache.org/mod_mbox/arrow-user/202103.mbox/%3Cb0a27ab4f8387e41b313fbfaa7ffb4e5764155c2.camel%40cnrgh.fr%3E

(with private access to the actual file) 

 "	ARROW	Resolved	3	1	8135	pull-request-available
13180002	[Python] Allow lighter construction of pa.Schema / pa.StructType	"One shouldn't have to call {{pa.field}} explicitly. See this example:
https://github.com/apache/arrow/pull/2449/files#diff-a01a3e7cbe0d7dd0ec300a725ac0c0c6R148
"	ARROW	Resolved	3	5	8135	pull-request-available
13423296	[C++] Compilation error	"I get the following error:
{code}
In file included from /home/antoine/arrow/dev/cpp/src/arrow/status_test.cc:25:
/home/antoine/arrow/dev/cpp/src/arrow/testing/matchers.h:33:8: error: expected constructor, destructor, or type conversion before '(' token
   33 | MATCHER(PointeesEquals, """") { return std::get<0>(arg)->Equals(*std::get<1>(arg)); }
      |        ^
{code}"	ARROW	Resolved	1	1	8135	pull-request-available
13228642	[Python] Allow creating Table from Python dict	There's already {{Table.to_pydict()}}, we should probably have the reverse {{Table.from_pydict()}} method.	ARROW	Resolved	3	4	8135	pull-request-available
13498762	[Python] Remove gcc 4.9 compatibility code	"Since we now require a C++17-compliant compiler, we don't support gcc 4.9 anymore. The following code can probably be simplified:
https://github.com/apache/arrow/blob/619b034bd3e14937fa5d12f8e86fa83e7444b886/python/pyarrow/src/arrow/python/datetime.cc#L41"	ARROW	Resolved	4	3	8135	pull-request-available
13300834	[C++] Compilation error when linking arrow-flight-perf-server	"I wanted to play around with Flight benchmark after seeing the discussion regarding Flight's throughput in arrow dev mailing list today.

I met the following error when trying to build the benchmark from latest source code:
{code:java}
[ 95%] Linking CXX executable ../../../debug/arrow-flight-perf-server
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::detail::canonical(boost::filesystem::path const&, boost::filesystem::path const&, boost::system::error_code*)'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::system::system_category()'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::path::parent_path() const'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `deflate'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `deflateEnd'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::system::generic_category()'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::detail::current_path(boost::system::error_code*)'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `inflateInit2_'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `inflate'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `deflateInit2_'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `inflateEnd'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::path::operator/=(boost::filesystem::path const&)'
collect2: error: ld returned 1 exit status
src/arrow/flight/CMakeFiles/arrow-flight-perf-server.dir/build.make:154: recipe for target 'debug/arrow-flight-perf-server' failed
make[2]: *** [debug/arrow-flight-perf-server] Error 1
CMakeFiles/Makefile2:2609: recipe for target 'src/arrow/flight/CMakeFiles/arrow-flight-perf-server.dir/all' failed
make[1]: *** [src/arrow/flight/CMakeFiles/arrow-flight-perf-server.dir/all] Error 2
Makefile:140: recipe for target 'all' failed
make: *** [all] Error 2

{code}
I was using {{cmake .. -DCMAKE_BUILD_TYPE=Debug -DARROW_DEPENDENCY_SOURCE=AUTO -DARROW_FLIGHT=ON -DARROW_BUILD_BENCHMARKS=ON -DARROW_CXXFLAGS=""-lboost_filesystem -lboost_system""}} to configure the build.
 I noticed that there was a {{ARROW_BOOST_BUILD_VERSION: 1.71.0}} in the output, but the Boost library that I installed from the package manger was of this version: {{1.65.1.0ubuntu1}}. Could this be the cause of the problem?

PS:
I was able to build the benchmark [before|https://issues.apache.org/jira/browse/ARROW-7200]. It was on AWS with the OS being ubuntu-bionic-18.04-amd64-server-20191002, which should be very similar to the one I'm using on my laptop."	ARROW	Resolved	4	1	8135	pull-request-available
13143795	[Python] More consistent / intuitive name for pyarrow.frombuffer	Now that we have {{pyarrow.foreign_buffer}}, things are a bit odd. We could call {{frombuffer}} something like {{py_buffer}} instead?	ARROW	Resolved	3	4	8135	pull-request-available
13247899	[C++] Implement casting List <-> LargeList	"We should implement bidirectional casts from List to LargeList and vice-versa.

In the narrowing direction, the offset width should be checked.
"	ARROW	Resolved	4	4	8135	kernel, pull-request-available
13247898	[C++] Implement casting Binary <-> LargeBinary	"We should implement bidirectional casts from Binary to LargeBinary and vice-versa. Also including String and LargeString.

In the narrowing direction, the offset width should be checked."	ARROW	Resolved	4	4	8135	pull-request-available
13375158	Allow duplicates in the value_set for compute::is_in  	"In the arrow release-4.0.0 branch, the `compute::is_in` operation rejects duplicate values in the `value_set` [1]. This was not the case in arrow 2.0 >=.
 
I was wondering if this strict restriction is required? Because ultimately, a hash set would be created from the value_set values, and there's no harm in having duplicates while doing so, isn't it?
PS: I understand that the param name ""value_set"" indicates that the values need to be unique, but in the useability perspective, this can be relaxed IMO. ex: Pandas isin [2].
 
 
[1] [https://github.com/apache/arrow/blob/master/cpp/src/arrow/compute/kernels/scalar_set_lookup.cc#L53]
[2] [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html]"	ARROW	Resolved	3	4	8135	pull-request-available
13241736	[CI] Add daily / weekly Valgrind build	A daily or weekly Valgrind build on the ursa-labs machines would further check sanity of the C++ code base, though with ASAN and UBSAN builds on Travis-CI we're already well covered.	ARROW	Resolved	4	5	8135	pull-request-available
13061285	[C++] Adopt FileSystem abstraction	See, e.g. in TensorFlow: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.h	ARROW	Resolved	3	4	8135	filesystem, pull-request-available
13156414	[C++] libarrow.so leaks zlib symbols	"I get the following here:

{code:bash}
$ nm -D -C /home/antoine/miniconda3/envs/pyarrow/lib/libarrow.so.0.0.0 | \grep ' T ' | \grep -v arrow
000000000025bc8c T adler32_z
000000000025c4c9 T crc32_z
00000000002ad638 T _fini
0000000000078ab8 T _init
{code}"	ARROW	Resolved	3	1	8135	pull-request-available
13139539	[Python] ASV benchmark setup does not account for C++ library changing	"See https://github.com/apache/arrow/blob/master/python/README-benchmarks.md

Perhaps we could create a helper script that will run all the currently-defined benchmarks for a specific commit, and ensure that we are running against pristine, up-to-date release builds of Arrow (and any other dependencies, like parquet-cpp) at that commit? 

cc [~pitrou]"	ARROW	Resolved	3	1	8135	pull-request-available
13125303	[C++] Add benchmarks comparing performance of internal::BitmapReader/Writer with naive approaches	The performance may also vary across platforms/compilers. This would be helpful to know how much they help	ARROW	Resolved	3	4	8135	pull-request-available
13188933	[C++] Add option to CSV reader to dictionary encode individual columns or all string / binary columns	"For many datasets, dictionary encoding everything can result in drastically lower memory usage and subsequently better performance in doing analytics

One difficulty of dictionary encoding in multithreaded conversions is that ideally you end up with one dictionary at the end. So you have two options:

* Implement a concurrent hashing scheme -- for low cardinality dictionaries, the overhead associated with mutex contention will not be meaningful, for high cardinality it can be more of a problem

* Hash each chunk separately, then normalize at the end

My guess is that a crude concurrent hash table with a mutex to protect mutations and resizes is going to outperform the latter"	ARROW	Resolved	3	2	8135	csv, dataset, pull-request-available
13328838	[C++] Make sure that default AWS region is respected	See https://aws.amazon.com/blogs/developer/aws-sdk-for-c-version-1-8-developer-preview/. The sdk will detect the region from EC2 metadata if present; it will also (as of 1.8 at least) let you specify a default region via environment variable. So we should make sure that we don't override it with us-east-1 if the SDK pulls a region from one of those. (Sounds like how access_key/secret_key are currently handled.)	ARROW	Resolved	3	4	8135	pull-request-available
13149590	[Python] Correct issues in numpy_to_arrow conversion routines	"Following the discussion at [https://github.com/apache/arrow/pull/1689,] there are a few issues with conversion of various types to arrow that are incorrect or could be improved:
 * PyBytes_GET_SIZE is being casted to the wrong type, for example 
{{const int32_t length = static_cast<int32_t>(PyBytes_GET_SIZE(obj));}}

 * Handle the possibility with the statement
{{builder->value_data_length() + length > kBinaryMemoryLimit}}
if length is larger than kBinaryMemoryLimit

 * Look into using common code for binary object conversion to avoid duplication, and allow support for bytes and bytearray objects in other places than numpy_to_arrow.  (possibly put in src/arrow/python/helpers.h)"	ARROW	Resolved	3	1	8135	pull-request-available
13222109	[Python] Hypothesis test failures	"I don't think these are being run regularly anywhere (?)

{code}
==================================== FAILURES =====================================
__________________________________ test_pickling __________________________________

    @h.given(
>       past.arrays(
            past.all_types,
            size=st.integers(min_value=0, max_value=10)
        )
    )
    def test_pickling(arr):

pyarrow/tests/test_array.py:822: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:145: in arrays
    type = draw(type)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(150345453957525051493869399118570182513) to this test or run pytest with --hypothesis-seed=150345453957525051493869399118570182513 to reproduce this failure.
__________________________________ test_schemas ___________________________________

    @h.given(past.all_schemas)
>   def test_schemas(schema):

pyarrow/tests/test_strategies.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(275336988088025852485731858331440131646) to this test or run pytest with --hypothesis-seed=275336988088025852485731858331440131646 to reproduce this failure.
___________________________________ test_arrays ___________________________________

    @h.given(past.all_arrays)
>   def test_arrays(array):

pyarrow/tests/test_strategies.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:145: in arrays
    type = draw(type)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(112717218077417468999288153449881179589) to this test or run pytest with --hypothesis-seed=112717218077417468999288153449881179589 to reproduce this failure.
_______________________________ test_chunked_arrays _______________________________

    @h.given(past.all_chunked_arrays)
>   def test_chunked_arrays(chunked_array):

pyarrow/tests/test_strategies.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:206: in chunked_arrays
    type = draw(type)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(249910746870640720519016487591535359287) to this test or run pytest with --hypothesis-seed=249910746870640720519016487591535359287 to reproduce this failure.
_______________________________ test_record_batches _______________________________

    @h.given(past.all_record_batches)
>   def test_record_batches(record_bath):

pyarrow/tests/test_strategies.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:233: in record_batches
    schema = draw(schemas(type, max_fields=max_fields))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(224188657977816444687824754926551268633) to this test or run pytest with --hypothesis-seed=224188657977816444687824754926551268633 to reproduce this failure.
___________________________________ test_tables ___________________________________

    @h.given(past.all_tables)
>   def test_tables(table):

pyarrow/tests/test_strategies.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:249: in tables
    schema = draw(schemas(type, max_fields=max_fields))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(11590233605635440545759465827826616216) to this test or run pytest with --hypothesis-seed=11590233605635440545759465827826616216 to reproduce this failure.
__________________________________ test_pickling __________________________________

    @h.given(
>       past.all_types |
        past.all_fields |
        past.all_schemas
    )
    @h.example(
        pa.field(name='', type=pa.null(), metadata={'0': '', '': ''})
    )
    def test_pickling(field):

pyarrow/tests/test_types.py:546: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(29668570043146860011453262620485865691) to this test or run pytest with --hypothesis-seed=29668570043146860011453262620485865691 to reproduce this failure.
__________________________________ test_hashing ___________________________________

    @h.given(
>       st.lists(past.all_types) |
        st.lists(past.all_fields) |
        st.lists(past.all_schemas)
    )
    def test_hashing(items):

pyarrow/tests/test_types.py:559: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(154619181152052597443025012079016131068) to this test or run pytest with --hypothesis-seed=154619181152052597443025012079016131068 to reproduce this failure.
{code}"	ARROW	Resolved	1	1	8135	pull-request-available
13163265	[Python] Error with errno 22 when loading 3.6 GB Parquet file	"I saved a file using pandas to_parquet method, but can't read it back in. Here's the full stack trace:

 
{code:java}
Traceback (most recent call last):
File ""src/data/CLXP_pull.py"", line 214, in <module>
 main()
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 722, in _call_
 return self.main(*args, **kwargs)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 697, in main
 rv = self.invoke(ctx)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 895, in invoke
 return ctx.invoke(self.callback, **ctx.params)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 535, in invoke
 return callback(*args, **kwargs)
 File ""src/data/CLXP_pull.py"", line 188, in main
 results[fullname] = pd.read_parquet(os.path.join(project_dir, ""data"", ""raw"", fullname+"".parquet""), engine=""pyarrow"")
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pandas/io/parquet.py"", line 257, in read_parquet
 return impl.read(path, columns=columns, **kwargs)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pandas/io/parquet.py"", line 130, in read
 **kwargs).to_pandas()
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pyarrow/parquet.py"", line 939, in read_table
 pf = ParquetFile(source, metadata=metadata)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pyarrow/parquet.py"", line 64, in _init_
 self.reader.open(source, metadata=metadata)
 File ""_parquet.pyx"", line 651, in pyarrow._parquet.ParquetReader.open
 File ""error.pxi"", line 79, in pyarrow.lib.check_status
 pyarrow.lib.ArrowIOError: Arrow error: IOError: [Errno 22] Invalid argument
{code}
Any ideas what could cause this? The file itself is 3.6GB.

I'm running pandas==0.22.0."	ARROW	Closed	3	1	10714	parquet
13248641	[C++][Gandiva] including some headers causes decimal_test to fail	"It seems this is due to precompiled code being contaminated with undesired headers

For example, {{#include <iostream>}} in {{arrow/compare.h}} causes:

{code}
[ RUN      ] TestDecimal.TestCastFunctions
../../src/gandiva/tests/decimal_test.cc:478: Failure
Value of: (array_dec)->Equals(outputs[2], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  0.00,
  0.00,
  0.00,
  0.00
]
../../src/gandiva/tests/decimal_test.cc:481: Failure
Value of: (array_dec)->Equals(outputs[2], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  0.00,
  0.00,
  0.00,
  0.00
]
../../src/gandiva/tests/decimal_test.cc:484: Failure
Value of: (array_dec)->Equals(outputs[3], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  0.00,
  0.00,
  0.00,
  0.00
]
../../src/gandiva/tests/decimal_test.cc:497: Failure
Value of: (array_float64)->Equals(outputs[6], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  inf,
  inf,
  -inf,
  -inf
]
[  FAILED  ] TestDecimal.TestCastFunctions (134 ms)
{code}
"	ARROW	Resolved	3	1	10714	pull-request-available
13203160	[Python] Better error message when user connects to HDFS cluster with wrong port	"I'm trying to connect to HDFS using libhdfs and Kerberos.

I have JAVA_HOME and HADOOP_HOME set and {{pyarrow.hdfs.connect}} sets CLASSPATH correctly.

My connect call looks like:

{{import pyarrow.hdfs}}

{{c = pyarrow.hdfs.connect(host='MYHOST', port=42424,}}

{{                         user='ME', kerb_ticket=""/tmp/krb5cc_498970"")}}

This doesn't error but the resulting connection can't do anything. They either error like this:

{{ArrowIOError: HDFS list directory failed, errno: 255 (Unknown error 255) }}

Or swallow errors (e.g. {{exists}} returning {{False}}).

Note that {{connect}} errors if the host is wrong but doesn't error if the port, user, or kerb_ticket are wrong. I have no idea how to debug this, because no useful errors.

Note that I _can_ connect using the hdfs Python package. (Of course, that doesn't provide the API I need to read Parquet files.).

Any help would be appreciated greatly."	ARROW	Resolved	3	1	10714	hdfs
13304959	[C++] Split arrow::ChunkedArray into arrow/chunked_array.h	There are plenty of scenarios where ChunkedArray is used separate from Table, it would probably make sense to split up the headers, implementation, and unit tests	ARROW	Resolved	3	4	10714	pull-request-available
13303254	[C++] Do not require struct-initialization of StringConverter<T> to parse strings to other types	"I ran into this issue while working on refactoring kernels. {{StringConverter<T>}} must be initialized to be able to support parametric types like Timestamp, but this produces an awkwardness and possibly a performance penalty (I haven't measured yet) in inlined functions. 

In any case, I'm refactoring everything to be static non-stateful"	ARROW	Resolved	3	4	10714	pull-request-available
13042885	Script to easily verify release in all languages	Having a script as in {{parquet-cpp}} that downloads the tarball, verifies the signature and builds/tests in all 3 three languages would be very helpful for future releaes.	ARROW	Resolved	3	5	10714	pull-request-available
13207730	[Dev] Allow maintainers to use a GitHub API token when merging pull requests	I rate limited today on unauthenticated requests for some reason -- the failure mode for dev/merge_arrow_pr.py was pretty bad. I'm making the output more helpful and adding an option to use an API token set via environment variable	ARROW	Resolved	3	4	10714	pull-request-available
13092244	"[C++] Define ""virtual table"" interface"	"The idea is that a virtual table may reference Arrow data that is not yet available in memory. The implementation will define the semantics of how columns are loaded into memory. 

A virtual column interface will need to accompany this. For example:

{code:language=c++}
std::shared_ptr<VirtualTable> vtable = ...;
std::shared_ptr<VirtualColumn> vcolumn = vtable->column(i);
std::shared_ptr<Column> = vcolumn->Materialize();
std::shared_ptr<Table> = vtable->Materialize();
{code}"	ARROW	Closed	3	2	10714	dataframe
13117387	[Format] List expected on-wire buffer layouts for each kind of Arrow physical type in specification	see ARROW-1693, ARROW-1785	ARROW	Resolved	3	4	10714	columnar-format-1.0
13115552	[C++] unittest failure for language environment	"After clone the project and run ""make unittest"" ,the ""io-file-test"" has error:  

locale::facet::_S_create_c_locale name not valid 

need to add LC_ALL=""en_US.UTF-8"" to pass this test"	ARROW	Resolved	5	1	10714	pull-request-available
13225328	[Python] Space leak in  ParquetFile.read_row_group()	"I have a code pattern like this:

 

reader = pq.ParquetFile(path)

for ix in range(0, reader.num_row_groups):
    table = reader.read_row_group(ix, columns=self._columns)
    # operate on table

 

But it leaks memory over time, only releasing it when the reader object is collected. Here's a workaround

 

num_row_groups = pq.ParquetFile(path).num_row_groups

for ix in range(0, num_row_groups):
    table = pq.ParquetFile(path).read_row_group(ix, columns=self._columns)
    # operate on table

 

This puts an upper bound on memory usage and is what I'd  expect from the code. I also put gc.collect() to the end of every loop.

 

I charted out memory usage for a small benchmark that just copies a file, one row group at a time, converting to pandas and back to arrow on the writer path. Line in black is the first one, using a single reader object. Blue is instantiating a fresh reader in every iteration."	ARROW	Resolved	3	1	10714	parquet, pull-request-available
13247988	[C++][Parquet] Build logical schema tree mapping Arrow fields to Parquet schema levels	"In several places in cpp/src/parquet/arrow, the {{FromParquetSchema}} function is used to construct fields using a filtered ""view"" of the Parquet schema. This is a hack caused by the lack of some kind of a ""schema tree"" which maps Parquet concepts to Arrow {{Field}} objects. 

One manifestation of this issue is that I was unable to implement dictionary encoded subfields in cases like {{list<string>}}, where you want the inner field to be dictionary-encoded. 

Patch forthcoming"	ARROW	Resolved	3	4	10714	pull-request-available
13249219	[C++][Parquet] Write arrow::Array directly into parquet::TypedColumnWriter<T>	This is an initial refactoring task to enable the Arrow write layer to access some of the internal implementation details of {{parquet::TypedColumnWriter<T>}}. See discussion in ARROW-3246	ARROW	Resolved	3	1	10714	pull-request-available
13208358	[C++] conda_env_* files cannot be used to create a fresh conda environment on Windows	"See

{code}
λ conda create -n arrow-dev python=3.7 --file=ci\conda_env_cpp.yml --file=ci\conda_env_python.yml -c conda-forge
Solving environment: failed                                                                                     
                                                                                                                
PackagesNotFoundError: The following packages are not available from current channels:                          
                                                                                                                
  - rsync                                                                                                       
  - nomkl                                                                                                       
                                                                                                                
Current channels:                                                                                               
                                                                                                                
  - https://conda.anaconda.org/conda-forge/win-64                                                               
  - https://conda.anaconda.org/conda-forge/noarch                                                               
  - https://repo.anaconda.com/pkgs/main/win-64                                                                  
  - https://repo.anaconda.com/pkgs/main/noarch                                                                  
  - https://repo.anaconda.com/pkgs/free/win-64                                                                  
  - https://repo.anaconda.com/pkgs/free/noarch                                                                  
  - https://repo.anaconda.com/pkgs/r/win-64                                                                     
  - https://repo.anaconda.com/pkgs/r/noarch                                                                     
  - https://repo.anaconda.com/pkgs/pro/win-64                                                                   
  - https://repo.anaconda.com/pkgs/pro/noarch                                                                   
  - https://repo.anaconda.com/pkgs/msys2/win-64                                                                 
  - https://repo.anaconda.com/pkgs/msys2/noarch                                                                 
                                                                                                                
To search for alternate channels that may provide the conda package you're                                      
looking for, navigate to                                                                                        
                                                                                                                
    https://anaconda.org                                                                                        
                                                                                                                
and use the search bar at the top of the page.                                                                  
{code}

The Linux/macOS-specific dependencies should be put in conda_env_unix.yml"	ARROW	Resolved	3	1	10714	pull-request-available
13178862	[C++] Detect ORC system packages	See https://github.com/apache/arrow/blob/master/cpp/cmake_modules/ThirdpartyToolchain.cmake#L155. After the CMake refactor it is possible to use built ORC packages with {{$ORC_HOME}} but not detected like the other toolchain dependencies	ARROW	Resolved	3	4	10714	pull-request-available
13102443	[C++] Fix valgrind warnings in cuda-test if possible	Running cuda-test with {{-DARROW_TEST_MEMCHECK=on}} fails with RC0. Not a big issue for 0.7.0 but may be worth investigating	ARROW	Resolved	3	1	10714	pull-request-available
13275741	[CI][C++] test-ubuntu-18.04-cpp-static failing with linking error in arrow-io-hdfs-test	"see https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-12-20-0-circle-test-ubuntu-18.04-cpp-static

{code}
FAILED: debug/arrow-io-hdfs-test
: && /usr/bin/ccache /usr/bin/c++  -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -g  -rdynamic src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o  -o debug/arrow-io-hdfs-test  -Wl,-rpath,/build/cpp/debug debug/libarrow_testing.a debug/libarrow.a /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib/x86_64-linux-gnu/libbrotlienc.so /usr/lib/x86_64-linux-gnu/libbrotlidec.so /usr/lib/x86_64-linux-gnu/libbrotlicommon.so /usr/lib/x86_64-linux-gnu/libprotobuf.so orc_ep-install/lib/liborc.a /usr/lib/x86_64-linux-gnu/libglog.so -ldl debug//libgtest_maind.so debug//libgtestd.so debug//libgmockd.so /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libbz2.so /usr/lib/x86_64-linux-gnu/liblz4.so /usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.7 /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libzstd.so jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt && :
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `__static_initialization_and_destruction_0(int, int)':
/usr/include/boost/system/error_code.hpp:206: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:208: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:210: undefined reference to `boost::system::system_category()'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::system::error_category::std_category::equivalent(int, std::error_condition const&) const':
/usr/include/boost/system/error_code.hpp:656: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:659: undefined reference to `boost::system::generic_category()'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::system::error_category::std_category::equivalent(std::error_code const&, int) const':
/usr/include/boost/system/error_code.hpp:686: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:689: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:701: undefined reference to `boost::system::generic_category()'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::filesystem::operator/(boost::filesystem::path const&, boost::filesystem::path const&)':
/usr/include/boost/filesystem/path.hpp:792: undefined reference to `boost::filesystem::path::operator/=(boost::filesystem::path const&)'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::filesystem::temp_directory_path()':
/usr/include/boost/filesystem/operations.hpp:716: undefined reference to `boost::filesystem::detail::temp_directory_path(boost::system::error_code*)'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::filesystem::unique_path(boost::filesystem::path const&)':
/usr/include/boost/filesystem/operations.hpp:723: undefined reference to `boost::filesystem::detail::unique_path(boost::filesystem::path const&, boost::system::error_code*)'
collect2: error: ld returned 1 exit status
{code}

possibly related to ARROW-6742"	ARROW	Resolved	3	1	10714	pull-request-available
13269344	[Python] Passing directory to ParquetFile class gives confusing error message	"Somehow have the same errors. We are working with pyarrow 0.15.1, trying to access a folder of `parquet` files generated with Amazon Athena.

```python
table2 = pq.read_table('C:/Data/test-parquet')
```

works fine in contrast to

```python
parquet_file = pq.ParquetFile('C:/Data/test-parquet')
# parquet_file.read_row_group(0)
```

which raises

`ArrowIOError: Failed to open local file 'C:/Data/test-parquet', error: Access is denied.`"	ARROW	Resolved	3	1	10714	parquet, pull-request-available
13204543	[CI] Use travis_terminate to halt builds when a step fails	"I noticed that Travis CI will soldier onward if a step in its {{script:}} block fails. This wastes build time when there is an error somewhere early on in the testing process

For example, in the main C++ build, if {{travis_script_cpp.sh}} fails, then the subsequent steps will continue.

It seems the way to deal with this is to add {{|| travis_terminate 1}} to lines that can fail

see

https://medium.com/@manjula.cse/how-to-stop-the-execution-of-travis-pipeline-if-script-exits-with-an-error-f0e5a43206bf

I also found this discussion

https://github.com/travis-ci/travis-ci/issues/1066"	ARROW	Resolved	3	4	10714	pull-request-available
13219791	[C++] Prototype scalar and array expression types for developing deferred operator algebra	I am beginning to develop a C++ API for describing analytical expressions on scalars, arrays, and tables in the general style of Ibis (https://docs.ibis-project.org/sql.html) which has been used to successfully model SQL relational algebra as well as a broader variety of analytical operations. This is a large project so I'm starting small	ARROW	Resolved	3	2	10714	pull-request-available
13104025	[Python] Fix package dependency issues causing build failures	"We are installing package requirements for the Python build in two steps, and the second step is causing conda to downgrade NumPy, resulting in an ABI conflict and broken build. I'm not sure why this suddenly started happening, but installing the packages all at once and pinning the NumPy version should fix it

https://travis-ci.org/apache/arrow/jobs/278202858#L9106"	ARROW	Resolved	3	1	10714	ci-failure, pull-request-available
13242043	[C++] TestDictionary.Validate test is crashed with release build	"Here is a backtrace:

{noformat}
(gdb) bt
#0  0x00007ffff76b3bba in arrow::DictionaryArray::DictionaryArray(std::shared_ptr<arrow::DataType> const&, std::shared_ptr<arrow::Array> const&, std::shared_ptr<arrow::Array> const&) ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow.so.14
#1  0x00005555557ba6c3 in arrow::TestDictionary_Validate_Test::TestBody() ()
#2  0x00007ffff7fa725a in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#3  0x00007ffff7f9db5a in testing::Test::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#4  0x00007ffff7f9dca8 in testing::TestInfo::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#5  0x00007ffff7f9dd85 in testing::TestCase::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#6  0x00007ffff7f9e29c in testing::internal::UnitTestImpl::RunAllTests() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#7  0x00007ffff7fa776a in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#8  0x00007ffff7f9e3cc in testing::UnitTest::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#9  0x000055555568eb90 in main ()
{noformat}

It's not occurred with debug build.

Here are CMake options I used:

{noformat}
rm -rf build
mkdir -p build
cd build
CUDA_TOOLKIT_ROOT=/usr \
  cmake .. \
  -G Ninja \
  -DCMAKE_INSTALL_PREFIX=/tmp/local \
  -DCMAKE_BUILD_TYPE=release \
  -DARROW_PYTHON=on \
  -DPythonInterp_FIND_VERSION=on \
  -DPythonInterp_FIND_VERSION_MAJOR=3 \
  -DARROW_PLASMA=on \
  -DARROW_CUDA=on \
  -DARROW_EXTRA_ERROR_CONTEXT=on \
  -DARROW_ORC=on \
  -DARROW_PARQUET=on \
  -DARROW_GANDIVA=on \
  -DARROW_BUILD_TESTS=on \
  -DARROW_FLIGHT=on
{noformat}"	ARROW	Resolved	1	1	10714	pull-request-available
13262042	"[C++] Dictionary ""delta"" building logic in builder_dict.h produces invalid arrays"	Looking at the unit tests for the dictionary delta logic -- the arrays that are produced by subsequent invocations of {{Finish}} yield DictionaryArray instances with partial dictionaries. I think this is misleading (I was surprised to find this while working on ARROW-6861). We should develop a different approach to computing dictionary delta. 	ARROW	Resolved	1	1	10714	pull-request-available
13290771	[Python] Don't check Schema metadata in __eq__ and __ne__	"When performing schema roundtrips, the equality check for fields break. This is a regression from PyArrow 0.16.0

The equality check for entire schemas has never worked (but should from my POV)
{code:python}
import pyarrow.parquet as pq
import pyarrow as pa
print(pa.__version__)
fields = [
    pa.field(""bool"", pa.bool_()),
    pa.field(""byte"", pa.binary()),
    pa.field(""date"", pa.date32()),
    pa.field(""datetime64"", pa.timestamp(""us"")),
    pa.field(""float32"", pa.float64()),
    pa.field(""float64"", pa.float64()),
    pa.field(""int16"", pa.int64()),
    pa.field(""int32"", pa.int64()),
    pa.field(""int64"", pa.int64()),
    pa.field(""int8"", pa.int64()),
    pa.field(""null"", pa.null()),
    pa.field(""uint16"", pa.uint64()),
    pa.field(""uint32"", pa.uint64()),
    pa.field(""uint64"", pa.uint64()),
    pa.field(""uint8"", pa.uint64()),
    pa.field(""unicode"", pa.string()),
    pa.field(""array_float32"", pa.list_(pa.float64())),
    pa.field(""array_float64"", pa.list_(pa.float64())),
    pa.field(""array_int16"", pa.list_(pa.int64())),
    pa.field(""array_int32"", pa.list_(pa.int64())),
    pa.field(""array_int64"", pa.list_(pa.int64())),
    pa.field(""array_int8"", pa.list_(pa.int64())),
    pa.field(""array_uint16"", pa.list_(pa.uint64())),
    pa.field(""array_uint32"", pa.list_(pa.uint64())),
    pa.field(""array_uint64"", pa.list_(pa.uint64())),
    pa.field(""array_uint8"", pa.list_(pa.uint64())),
    pa.field(""array_unicode"", pa.list_(pa.string())),
]

schema = pa.schema(fields)

buf = pa.BufferOutputStream()
pq.write_metadata(schema, buf)
reader = pa.BufferReader(buf.getvalue().to_pybytes())
reconstructed_schema = pq.read_schema(reader)

assert reconstructed_schema == reconstructed_schema
assert reconstructed_schema[0] == reconstructed_schema[0]
# This breaks on master / regression from 0.16.0 
assert schema[0] == reconstructed_schema[0]

# This never worked but should
assert reconstructed_schema == schema
assert schema == reconstructed_schema
{code}"	ARROW	Resolved	3	1	10714	pull-request-available
13118965	[C++] Implement hash kernel specialization for BooleanType	Follow up to ARROW-1559	ARROW	Resolved	3	4	10714	pull-request-available
13199734	[Gandiva] Build on Windows	I started briefly looking at this. As an initial matter, our FindLLVM.cmake is inadequate. There's one (BSD-licensed) we can pull in from the LLVM-based D compiler: https://github.com/ldc-developers/ldc/blob/master/cmake/Modules/FindLLVM.cmake. If someone can point out another one, I can take a look. 	ARROW	Resolved	3	4	10714	pull-request-available
13120677	[Python] Improve performance of serializing object dtype ndarrays	"I haven't looked carefully at the hot path for this, but I would expect these statements to have roughly the same performance (offloading the ndarray serialization to pickle)

{code}
In [1]: import pickle

In [2]: import numpy as np

In [3]: import pyarrow as pa
a
In [4]: arr = np.array(['foo', 'bar', None] * 100000, dtype=object)

In [5]: timeit serialized = pa.serialize(arr).to_buffer()
10 loops, best of 3: 27.1 ms per loop

In [6]: timeit pickled = pickle.dumps(arr)
100 loops, best of 3: 6.03 ms per loop
{code}

[~robertnishihara] [~pcmoritz] I encountered this while working on ARROW-1783, but it can likely be resolved independently"	ARROW	Resolved	3	4	10714	pull-request-available
12947906	[C++] Consider adding a scalar type object model	Just did this on the Python side. In later analytics routines, passing in scalar values (example: Array + Scalar) requires some kind of container. Some systems, like the R language, solve this problem with length-1 arrays, but we should do some analysis of use cases and figure out what will work best for Arrow.	ARROW	Resolved	3	2	10714	Analytics, pull-request-available
13307840	[C++] Reduce generated code in compute/kernels/scalar_compare.cc	"We are instantiating multiple versions of templates in this module for cases that, byte-wise, do the exact same comparison. For example:

* For equals, not_equals, we can use the same 32-bit/64-bit comparison kernels for signed int / unsigned int / floating point types of the same byte width
* TimestampType can reuse int64 kernels, similarly for other date/time types
* BinaryType/StringType can share kernels

etc."	ARROW	Resolved	3	4	10714	pull-request-available
13186689	[C++] Convenience API for constructing arrow::io::BufferReader from std::string	"See motivating code example:

https://github.com/apache/arrow/commit/db0ef22dd68ae00e11f09da40b6734c1d9770b57#diff-6dc1b0b53e71627dfb98c60b1fd2d45cR39"	ARROW	Resolved	3	4	10714	pull-request-available
13129962	[Python] Return parquet statistics min/max as values instead of strings	"Currently `min` and `max` column statistics are returned as formatted strings of the _physical type_. This makes using them in python a bit tricky, as the strings need to be parsed as the proper _logical type_. Observe:


{code}
In [20]: import pandas as pd

In [21]: df = pd.DataFrame({'a': [1, 2, 3],
    ...:                    'b': ['a', 'b', 'c'],
    ...:                    'c': [pd.Timestamp('1991-01-01')]*3})
    ...:

In [22]: df.to_parquet('temp.parquet', engine='pyarrow')

In [23]: from pyarrow import parquet as pq

In [24]: f = pq.ParquetFile('temp.parquet')

In [25]: rg = f.metadata.row_group(0)

In [26]: rg.column(0).statistics.min  # string instead of integer
Out[26]: '1'

In [27]: rg.column(1).statistics.min  # weird space added after value due to formatter
Out[27]: 'a '

In [28]: rg.column(2).statistics.min  # formatted as physical type (int) instead of logical (datetime)
Out[28]: '662688000000'
{code}

Since the type information is known, it should be possible to convert these to arrow values instead of strings."	ARROW	Resolved	3	1	10714	pull-request-available
13144764	[C++] MultipleClients test in io-hdfs-test fails on trunk	"This fails for me locally:

{code}
[ RUN      ] TestHadoopFileSystem/0.MultipleClients
../src/arrow/io/io-hdfs-test.cc:192: Failure
Value of: s.ok()
  Actual: false
Expected: true
{code}"	ARROW	Resolved	2	1	10714	pull-request-available
13313777	[C++] Compact generated code in compute/kernels/scalar_set_lookup.cc using same method as vector_hash.cc	This module can be made to compile smaller and faster by using common kernels for types having the same binary representation	ARROW	Resolved	3	4	10714	pull-request-available
13220564	"[C++] An incorrect dependency leads ""ninja"" to re-evaluate steps unnecessarily on subsequent calls"	"Not sure about the root cause yet but here are the 5 steps that are re-executing

{code}
$ ninja -v
[1/5] /usr/bin/ccache /usr/bin/g++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR=/home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist//include -DARROW_NO_DEPRECATED_API -DARROW_PYTHON_EXPORTING -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -Isrc -I../src -isystem /home/wesm/cpp-toolchain/include -isystem gbenchmark_ep/src/gbenchmark_ep-install/include -isystem jemalloc_ep-prefix/src -isystem ../thirdparty/hadoop/include -isystem orc_ep-install/include -isystem /home/wesm/cpp-toolchain/include/thrift -isystem /home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/numpy/core/include -isystem /home/wesm/miniconda/envs/arrow-3.7/include/python3.7m -Wno-noexcept-type  -fdiagnostics-color=always -O3 -DNDEBUG  -Wall -Wno-unused-variable -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG -fPIC   -std=gnu++11 -MD -MT src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o -MF src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o.d -o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o -c ../src/arrow/python/flight.cc
[2/5] : && /usr/bin/ccache /home/wesm/miniconda/envs/arrow-3.7/bin/cmake -E remove release/libarrow_python.a && /usr/bin/ccache /usr/bin/ar qc release/libarrow_python.a  src/arrow/python/CMakeFiles/arrow_python_objlib.dir/arrow_to_pandas.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/benchmark.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/common.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/config.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/decimal.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/deserialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/helpers.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/inference.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/init.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/io.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_convert.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/python_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/pyarrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/serialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o && /usr/bin/ccache /usr/bin/ranlib release/libarrow_python.a && :
[3/5] : && /usr/bin/ccache /usr/bin/g++ -fPIC -Wno-noexcept-type  -fdiagnostics-color=always -O3 -DNDEBUG  -Wall -Wno-unused-variable -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG   -shared -Wl,-soname,libarrow_python.so.13 -o release/libarrow_python.so.13.0.0 src/arrow/python/CMakeFiles/arrow_python_objlib.dir/arrow_to_pandas.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/benchmark.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/common.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/config.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/decimal.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/deserialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/helpers.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/inference.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/init.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/io.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_convert.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/python_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/pyarrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/serialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o  -Wl,-rpath,/home/wesm/code/arrow/cpp/build/release:/home/wesm/cpp-toolchain/lib: -lpthread -ldl -lutil -lrt release/libarrow_flight.so.13.0.0 release/libarrow.so.13.0.0 -ldl jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt /home/wesm/cpp-toolchain/lib/libprotobuf.a /home/wesm/cpp-toolchain/lib/libgrpc++_unsecure.a /home/wesm/cpp-toolchain/lib/libgrpc_unsecure.a /home/wesm/cpp-toolchain/lib/libgpr.a /home/wesm/cpp-toolchain/lib/libaddress_sorting.a /home/wesm/cpp-toolchain/lib/libcares_static.a -Wl,-rpath-link,/home/wesm/cpp-toolchain/lib && :
[4/5] /home/wesm/miniconda/envs/arrow-3.7/bin/cmake -E cmake_symlink_library release/libarrow_python.so.13.0.0  release/libarrow_python.so.13 release/libarrow_python.so && :
[5/5] : && /usr/bin/ccache /usr/bin/g++  -Wno-noexcept-type  -fdiagnostics-color=always -O3 -DNDEBUG  -Wall -Wno-unused-variable -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG  -rdynamic src/arrow/python/CMakeFiles/arrow-python-test.dir/python-test.cc.o  -o release/arrow-python-test  -Wl,-rpath,/home/wesm/code/arrow/cpp/build/release:/home/wesm/miniconda/envs/arrow-3.7/lib:/home/wesm/cpp-toolchain/lib release/libarrow_python_test_main.a release/libarrow_python.so.13.0.0 release/libarrow_testing.so.13.0.0 /home/wesm/miniconda/envs/arrow-3.7/lib/libpython3.7m.so -lpthread -lpthread -ldl -lutil -lrt release/libarrow_flight.so.13.0.0 /home/wesm/cpp-toolchain/lib/libprotobuf.a /home/wesm/cpp-toolchain/lib/libgrpc++_unsecure.a /home/wesm/cpp-toolchain/lib/libgrpc_unsecure.a /home/wesm/cpp-toolchain/lib/libgpr.a /home/wesm/cpp-toolchain/lib/libaddress_sorting.a /home/wesm/cpp-toolchain/lib/libcares_static.a release/libarrow.so.13.0.0 -ldl jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt /home/wesm/cpp-toolchain/lib/libgtest.a -Wl,-rpath-link,/home/wesm/cpp-toolchain/lib && :
{code}"	ARROW	Resolved	3	1	10714	pull-request-available
13211413	[Python] Run Python Travis CI unit tests on Linux when Java codebase changed	"The Java library is also dependency of the Python tests, but the tests aren't triggered if there is a change to the {{java/}} subtree. This blind spot was introduced when the CI jobs were split apart

https://github.com/apache/arrow/blob/master/.travis.yml#L133"	ARROW	Resolved	3	1	10714	pull-request-available
13307004	[C++] Slicing a ChunkedArray with zero chunks segfaults	"{code:python}
import pyarrow as pa
arr = pa.chunked_array([[1]])
empty = arr.filter(pa.array([False]))
print(empty)
print(empty[:]) # <- crash
{code}"	ARROW	Resolved	2	1	10714	pull-request-available
13251142	[Python] RecordBatch.from_arrays does not check array types against a passed schema	"Example came from ARROW-6038

{code}
In [4]: pa.RecordBatch.from_arrays([pa.array([])], schema)                                                  
Out[4]: <pyarrow.lib.RecordBatch at 0x7fc36fa18db8>

In [5]: rb = pa.RecordBatch.from_arrays([pa.array([])], schema)                                             

In [6]: rb                                                                                                  
Out[6]: <pyarrow.lib.RecordBatch at 0x7fc37d9c69f8>

In [7]: rb.schema                                                                                           
Out[7]: col: string

In [8]: rb[0]                                                                                               
Out[8]: 
<pyarrow.lib.NullArray object at 0x7fc36fa8ce08>
0 nulls

{code}"	ARROW	Resolved	3	1	10714	beginner, pull-request-available
13242887	[Release] Migrate and improve binary release verification script	"Requirements:
* parallel downloads
* resumable operation on error downloading
* python with no external dependencies"	ARROW	Resolved	3	4	10714	pull-request-available
13131683	[C++] Add shrink_to_fit option in BufferBuilder::Resize	See discussion in https://github.com/apache/arrow/pull/1481#discussion_r162157558	ARROW	Resolved	3	4	10714	pull-request-available
13294535	[C++] Add -Wa,-mbig-obj when compiling with MinGW to avoid linking errors	"See https://digitalkarabela.com/mingw-w64-how-to-fix-file-too-big-too-many-sections/

This seems to be the MinGW equivalent of {{/bigobj}} in MSVC"	ARROW	Resolved	3	4	10714	pull-request-available
13317413	[C++] Variadic template unpack inside lambda doesn't compile with gcc	Gandiva nightlies have been failing for a past couple of days	ARROW	Resolved	3	1	10714	pull-request-available
13187263	[Python] Support reading Parquet binary/string columns directly as DictionaryArray	"Requires PARQUET-1324 and probably quite a bit of extra work  

Properly implementing this will require dictionary normalization across row groups. When reading a new row group, a fast path that compares the current dictionary with the prior dictionary should be used. This also needs to handle the case where a column chunk ""fell back"" to PLAIN encoding mid-stream"	ARROW	Resolved	3	4	10714	parquet, pull-request-available
13294888	"[Python] Failure in ""nopandas"" build in test_parquet_row_group_fragments"	https://github.com/apache/arrow/runs/544301393	ARROW	Resolved	3	1	10714	pull-request-available
13185372	[C++] Use coarser-grained dispatch to SIMD hash functions	"The way that we dispatch to SSE4 hash functions is a remnant from the Impala codebase, which checks CpuInfo on every iteration in debug builds:

https://github.com/apache/arrow/blob/master/cpp/src/arrow/util/hash-util.h#L43

However, the static {{model_name_}} member is causing some non-determinism related to static member lifetime as reported in ARROW-3241. 

I'm proposing to refactor CpuInfo into a singleton pattern and handle SIMD vs non-SIMD dispatch at a higher level rather than at the lowest level like it is now. This should hopefully make the issue in ARROW-3241 go away"	ARROW	Resolved	3	4	10714	pull-request-available
13124167	[Website] Add org affiliations to committers.html	see e.g. http://hadoop.apache.org/who.html	ARROW	Resolved	3	1	10714	pull-request-available
13206465	[Python] Clarify in development.rst that virtualenv cannot be used with miniconda/Anaconda	Per mailing list thread on 2018-12-25	ARROW	Resolved	3	4	10714	pull-request-available
13177211	[Python] Do not build unit tests other than python-test in travis_script_python.sh	This is inflating CI runtime. This is a bit of work as we do not currently have granular control over which unit tests get built. We may have to add some logic to tag tests with extra metadata that can be used for conditional compilation (beyond the all-or-nothing {{NO_TESTS}} CMake variable being used now)	ARROW	Resolved	3	4	10714	pull-request-available
13184198	[C++] Create deterministic IPC metadata	Currently, the amount of padding bytes written after the IPC metadata header depends on the current position of the {{OutputStream}} passed. So if the message begins on an unaligned (not multiple of 8) offset, then the content of the metadata will be different than if it did. This seems like a leaky abstraction -- aligning the stream should probably be handled separately from writing the IPC protocol.	ARROW	Resolved	3	4	10714	pull-request-available
13207095	[Python] setuptools_scm customization does not work for versions above 0.9.0 on Windows	"{code}
C:\Users\wesm\code\arrow\python (ARROW-3910 -> origin)
(pyarrow-dev) λ git describe --dirty --tags --long --match 'apache-arrow-[0-9].*'
fatal: No names found, cannot describe anything.

C:\Users\wesm\code\arrow\python (ARROW-3910 -> origin)
(pyarrow-dev) λ git describe --dirty --tags --long
apache-arrow-0.11.0-499-gf77c2967-dirty
{code}

It's possible this is a Windows-specific issue"	ARROW	Resolved	3	1	10714	pull-request-available
13020923	Python: Add support for conversion of Pandas.Categorical	"At the moment conversion from {{pandas.Categorical}} columns fails with {{ArrowException: Invalid: only handle 1-dimensional arrays}}. As a better alternative, we should provide one of the following solutions:

 * Convert the categorical column to a string (Pandas type {{object}}) column, then use the conversion routines for strings. Add some metadata to the Arrow column that it was initially a Pandas string column so that in the case of a roundtrip, it will be a categorical column again.
 * Implement the conversion of the column to a dictionary-encoded Arrow column. This is the preferred solution but may be more complicated to implement as certain requirements have not yet been implemented."	ARROW	Resolved	4	2	10714	newbie
13294314	[Python] Review Developer build instructions for conda and non-conda users	See discussion in ARROW-3329. These instructions should be reviewed and either fixed (if possible) or removed if they cannot be maintained	ARROW	Resolved	3	1	10714	pull-request-available
13117346	[Python] Read and write pandas.DataFrame in pyarrow.serialize by decomposing the BlockManager rather than coercing to Arrow format	"See discussion in https://github.com/dask/distributed/pull/931

This will permit zero-copy reads for DataFrames not containing Python objects. In the event of an {{ObjectBlock}} these arrays will not be worse than pickle to reconstruct on the receiving side"	ARROW	Resolved	3	2	10714	pull-request-available
13208076	[Python] Tests crashing on all platforms in CI	Since there have been some conda-forge library updates I would guess this is toolchain-related	ARROW	Resolved	1	1	10714	pull-request-available
13317047	[C++][Parquet] Fix failure caused by malformed repetition/definition levels	Fix a case discovered by OSS-Fuzz	ARROW	Resolved	3	1	10714	pull-request-available
13111173	[Python] StructArray.from_arrays should handle sequences that are coercible to arrays	Currently the arrays passed must be `pyarrow.Array` objects already.	ARROW	Resolved	3	4	10714	pull-request-available
13188437	[Python] Dictionary has out-of-bound index when creating DictionaryArray from Pandas with NaN	"Minimal reproducer:

{code:python}
import pandas as pd
import pyarrow as pa

pa.array(pd.Categorical(['a', 'b', 'c'], categories=['a', 'b']))
{code}"	ARROW	Resolved	3	1	10714	pull-request-available
13128044	[Python] Writing Parquet file with flavor='spark' loses pandas schema metadata	"You can see the issue in the {{_sanitize_schema}} method

https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L201

see https://github.com/apache/arrow/issues/1452"	ARROW	Resolved	3	1	10714	pull-request-available
13109566	[Python] Use RecordBatch.from_pandas in FeatherWriter.write	In addition to making the implementation simpler, we will also benefit from multithreaded conversions, so faster write speeds	ARROW	Resolved	3	4	10714	pull-request-available
13262735	[Python] pyarrow.parquet.read_table(...) takes up lots of memory which is not released until program exits	"I realize that when I read up a lot of Parquet files using pyarrow.parquet.read_table(...), my program's memory usage becomes very bloated, although I don't keep the table objects after converting them to Pandas DFs.

You can try this in an interactive Python shell to reproduce this problem:

```{python}
from tqdm import tqdm
from pyarrow.parquet import read_table

PATH = '/tmp/big.snappy.parquet'

for _ in tqdm(range(10)):
    read_table(PATH, use_threads=False, memory_map=False)
    (note that I'm not assigning the read_table(...) result to anything, so I'm not creating any new objects at all)

```

During the For loop above, if you view the memory usage (e.g. using htop program), you'll see that it keeps creeping up. Either the program crashes during the 10 iterations, or if the 10 iterations complete, the program will still occupy a huge amount of memory, although no objects are kept. That memory is only released when you exit() from Python.

This problem means that my compute jobs using PyArrow currently need to use bigger server instances than I think is necessary, which translates to significant extra cost.

"	ARROW	Resolved	2	1	10714	pull-request-available
13474277	[C++] Use shared_ptr<DataType> less throughout arrow/compute	It turns out we generate a ton of code just copying and manipulating {{shared_ptr<DataType>}} throughput arrow/compute, and especially in the configuration of the function/kernels registry. One function {{RegisterScalarArithmetic}} generates around 300kb of code, which on looking at disassembly contains a significant amount of inlined shared_ptr template code. I made an attempt to refactoring things to use {{const DataType*}} for function signatures which removes quite a bit of code bloat, and puts us on a path to using fewer shared_ptr's in general	ARROW	In Progress	3	4	10714	pull-request-available
13311142	[C++] Add BinaryArray::total_values_length()	"It's often useful to compute the total data size of a binary array.
Sample implementation:
{code:c++}
  int64_t total_values_length() const {
    return raw_value_offsets_[length() + data_->offset] - raw_value_offsets_[data_->offset];
  }
{code}
"	ARROW	Resolved	4	4	10714	pull-request-available
13310273	[C++] Optimize Filter implementation	I split this off from ARROW-5760 	ARROW	Resolved	3	4	10714	pull-request-available
13168585	[Python] When installing pyarrow via pip, a debug build is created	I noticed this in the log for https://github.com/apache/arrow/issues/2173. We should probably change the default build type to {{release}}	ARROW	Resolved	3	1	10714	pull-request-available
13144414	[Python] Add option to not consider NaN to be null when converting to an integer Arrow type	Follow-on work to ARROW-2135	ARROW	Resolved	3	4	10714	pull-request-available
13186983	[R] Run clang-format, cpplint checks on R C++ code	Comment to ARROW-3282	ARROW	Resolved	3	4	10714	pull-request-available
13261489	[C++][Parquet][Python] List<scalar type> columns read broken with 0.15.0	"Columns of type {{array<primitive type>}} (such as `array<int32>`, `array<int64>`...) are not readable anymore using {{pyarrow == 0.15.0}} (but were with {{pyarrow == 0.14.1}}) when the original writer of the parquet file is {{parquet-mr 1.9.1}}.

{code}
import pyarrow.parquet as pq

pf = pq.ParquetFile('sample.gz.parquet')

print(pf.read(columns=['profile_ids']))
{code}

with 0.14.1:

{code}
pyarrow.Table
profile_ids: list<element: int64>
 child 0, element: int64

...
{code}

with 0.15.0:

{code}
Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
 File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyarrow/parquet.py"", line 253, in read
 use_threads=use_threads)
 File ""pyarrow/_parquet.pyx"", line 1131, in pyarrow._parquet.ParquetReader.read_all
 File ""pyarrow/error.pxi"", line 78, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Column data for field 0 with type list<item: int64> is inconsistent with schema list<element: int64>
{code}

I've tested parquet files coming from multiple tables (with various schemas) created with `parquet-mr`, couldn't read any `array<primitive type>` column anymore.

 

I _think_ the bug was introduced with [this commit|[https://github.com/apache/arrow/commit/06fd2da5e8e71b660e6eea4b7702ca175e31f3f5]].

I think the root of the issue comes from the fact that `parquet-mr` writes the inner struct name as `""element""` by default (see [here|[https://github.com/apache/parquet-mr/blob/b4198be200e7e2df82bc9a18d54c8cd16aa156ac/parquet-column/src/main/java/org/apache/parquet/schema/ConversionPatterns.java#L33]]), whereas `parquet-cpp` (or `pyarrow`?) assumes `""item""` (see for example [this test|[https://github.com/apache/arrow/blob/c805b5fadb548925c915e0e130d6ed03c95d1398/python/pyarrow/tests/test_schema.py#L74]]). The round-tripping tests write/read in pyarrow only obviously won't catch this.

 

 "	ARROW	Resolved	3	1	10714	parquet, pull-request-available
13297910	[Python] Add memory_map= toggle to pyarrow.feather.read_feather	I missed this in my prior patch	ARROW	Resolved	3	4	10714	pull-request-available
13310494	[C++][CI] Appveyor CI test failures	"See https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/33417919

These seem to have been introduced by 

https://github.com/apache/arrow/commit/b058cf0d1c26ad7984c104bb84322cc7dcc66f00"	ARROW	Resolved	1	1	10714	pull-request-available
13256117	[C++] Feather: slow writing of NullArray	"From https://stackoverflow.com/questions/57877017/pandas-feather-format-is-slow-when-writing-a-column-of-none

Smaller example with just using pyarrow, it seems that writing an array of nulls takes much longer than an array of for example ints, which seems a bit strange:

{code}
In [93]: arr = pa.array([None]*1000, type='int64')

In [94]: %%timeit 
    ...: w = pyarrow.feather.FeatherWriter('__test.feather') 
    ...: w.writer.write_array('x', arr) 
    ...: w.writer.close() 

31.4 µs ± 464 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [95]: arr = pa.array([None]*1000)  

In [96]: arr    
Out[96]: 
<pyarrow.lib.NullArray object at 0x7fa47a23ca40>
1000 nulls

In [97]: %%timeit 
    ...: w = pyarrow.feather.FeatherWriter('__test.feather') 
    ...: w.writer.write_array('x', arr) 
    ...: w.writer.close() 

3.75 ms ± 64.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
{code}

So writing the same length NullArray takes ca 100x more time compared to an array of nulls but with Integer type."	ARROW	Closed	3	1	10714	feather
13208324	[Format] Metadata.rst does not specify timezone for Timestamp type	Schema.fbs states that Timestamp type consists of unit and timezone, but Metadata.rst just has unit. This seems like an error.	ARROW	Resolved	3	4	10714	pull-request-available
13254082	[C++] jemalloc_ep fails for offline build	"Seems we have some slippage between the dependency download script and ThirdpartyToolchain.cmake

{code}
-- Build files have been written to: /home/wesm/code/arrow/cpp/build
[2/8] Performing download step (verify and extract) for 'jemalloc_ep'
-- jemalloc_ep download command succeeded.  See also /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-download-*.log
[5/8] Performing configure step for 'jemalloc_ep'
FAILED: jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure 
cd /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep && /home/wesm/cpp-toolchain/bin/cmake -P /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-DEBUG.cmake && /home/wesm/cpp-toolchain/bin/cmake -E touch /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure
CMake Error at /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-DEBUG.cmake:49 (message):
  Command failed: No such file or directory

   './configure' 'AR=/usr/bin/ar' 'CC=/usr/bin/clang-7' '--prefix=/home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist/' '--with-jemalloc-prefix=je_arrow_' '--with-private-namespace=je_arrow_private_' '--without-export' '--disable-cxx' '--disable-libdl' '--disable-initial-exec-tls'

  See also

    /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-*.log


ninja: build stopped: subcommand failed.
{code}"	ARROW	Resolved	3	1	10714	pull-request-available
13282685	[Developer] Install locally a new enough version of Go for release verification script	This will ensure that if a developer has a too-old version of Go installed on their system that the release verification will still work	ARROW	Resolved	3	4	10714	pull-request-available
13064310	[Python] Handle more kinds of null sentinel objects from pandas 0.x	Follow-on work to ARROW-707. See https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/lib.pyx#L193 and discussion in https://github.com/apache/arrow/pull/554	ARROW	Resolved	3	4	10714	pull-request-available
13064302	[Python] Efficient construction of arrays from non-pandas 1D NumPy arrays	This is follow on work to ARROW-825	ARROW	Resolved	3	2	10714	pull-request-available
13250590	[Python] pyarrow.array() shouldn't coerce np.nan to string	pa.array() by default regards np.nan as float value and fails on pa.array([np.nan, 'string']). It should also fail on pa.array(['string', np.nan]) instead of coercing it to null value.	ARROW	Resolved	3	1	10714	pull-request-available
13137279	[Python] Improve error message when calling parquet.read_table on an empty file	Currently it raises an exception about memory mapping failing	ARROW	Resolved	3	4	10714	pull-request-available
13215882	[C++] gbenchmark_ep is a dependency of unit tests when ARROW_BUILD_BENCHMARKS=ON	"I hit this issue when trying to use clang-7 from conda-forge, and wasn't sure why gbenchmark_ep is getting built when I'm building only a single unit test executable like arrow-array-test

https://github.com/google/benchmark/issues/351"	ARROW	Resolved	3	1	10714	pull-request-available
13263652	[C++] Clarify what signatures are preferred for compute kernels	Many of the compute kernels feature functions which accept only array inputs in addition to functions which accept Datums. The former seems implicitly like a convenience wrapper around the latter but I don't think this is explicit anywhere. Is there a preferred overload for bindings to use? Is it preferred that C++ implementers provide convenience wrappers for different permutations of argument type? (for example, Filter now provides an overload for record batch input as well as array input)	ARROW	Closed	4	4	10714	compute
13296666	[C++] is_nullable maybe not initialized in parquet writer	"From the Rtools build:

{code}
[ 84%] Building CXX object src/parquet/CMakeFiles/parquet_static.dir/column_reader.cc.obj
In file included from D:/a/arrow/arrow/cpp/src/arrow/io/concurrency.h:23:0,
                 from D:/a/arrow/arrow/cpp/src/arrow/io/memory.h:25,
                 from D:/a/arrow/arrow/cpp/src/parquet/platform.h:25,
                 from D:/a/arrow/arrow/cpp/src/parquet/arrow/writer.h:23,
                 from D:/a/arrow/arrow/cpp/src/parquet/arrow/writer.cc:18:
D:/a/arrow/arrow/cpp/src/arrow/result.h: In member function 'virtual arrow::Status parquet::arrow::FileWriterImpl::WriteColumnChunk(const std::shared_ptr<arrow::ChunkedArray>&, int64_t, int64_t)':
D:/a/arrow/arrow/cpp/src/arrow/result.h:428:28: warning: 'is_nullable' may be used uninitialized in this function [-Wmaybe-uninitialized]
   auto result_name = (rexpr);                               \
                            ^
D:/a/arrow/arrow/cpp/src/parquet/arrow/writer.cc:430:10: note: 'is_nullable' was declared here
     bool is_nullable;
          ^
{code}

I'd give it a default value, but IDK that it's that simple."	ARROW	Resolved	4	1	10714	pull-request-available
13046637	Add JIRA fix version to merge tool	Like parquet-mr's tool. This will make releases less painful	ARROW	Resolved	3	2	10714	pull-request-available
