id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13505448	[ confignode ] The RegionRouteMap doesn't update after the leader Node is Unknown	"m_1111_600e376
1. 启动3副本3C3D集群，BM 写入查询 7*24小时
2. 验证各节点数据是否一致：只保留1节点在线，查询，查询报错，
报错是因为当前的查询路由策略是Leader优先级最高，但是show regions中Leader 有Unknown。
{color:#DE350B}*存在2个问题：
~ Leader是Unknown ，但是RegionRouteMap没更新。
~ 如果Leader 是Unknown，应该路由给Running的follower。*{color}
查询报错信息：
2022-11-23 10:03:07,158 [pool-80-IoTDB-ClientRPC-Processor-5] ERROR o.a.i.d.u.ErrorHandlingUtils:90 - Status code: 310, Query Statement: fetchResults failed
org.apache.iotdb.commons.exception.IoTDBException: can't connect to node {}TEndPoint(ip:192.168.10.76, port:9003)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getResult(QueryExecution.java:394)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getByteBufferBatchResult(QueryExecution.java:421)
        at org.apache.iotdb.db.utils.QueryDataSetUtils.convertQueryResultByFetchSize(QueryDataSetUtils.java:387)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.fetchResultsV2(ClientRPCServiceImpl.java:423)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3528)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)

ConfigNode 日志：
 !image-2022-11-23-10-26-35-175.png! 

 !screenshot-1.png! 
测试环境
https://apache-iotdb.feishu.cn/docs/doccn0blJR3zeJgEVB7PsGwyC5e#J8fSiJ"	IOTDB	Closed	3	1	3357	pull-request-available
13554344	Reject Node startup when loading configuration file failed	"*Updates*

1. Reject Node startup when loading configuration failed. For instance, the Node can't startup if the following parameter occurs parsing error: 
 * cn_seed_config_node
 * region_group_allocate_policy
 * leader_distribution_policy
 * route_priority_policy
 * dn_seed_config_node
 * dn_multi_dir_strategy

2. Ensure the `timestamp_precision` can't be modified after first startup"	IOTDB	Closed	3	4	3357	pull-request-available
13469484	Get latest RegionRouteMap	"ConfigNode will create a new interface to get the latest RegionRouteMap:

 

*RegionRouteMap:* map<TConsensusGroupId, TRegionReplicaSet>"	IOTDB	Closed	3	2	3357	pull-request-available
13422230	Linear fill should log warning for unsupported data type	"Such as:
IoTDB> show timeseries root.sg.d1.*
{+}------------{-}{-}{+}---{-}++{-}-----------{-}{-}------{-}++{-}------{-}{-}---------{-}++{-}--{-}{-}---------+
|  timeseries|alias|storage group|dataType|encoding|compression|tags|attributes|

{+}------------{-}{-}{+}---{-}++{-}-----------{-}{-}------{-}++{-}------{-}{-}---------{-}++{-}--{-}{-}---------+
|root.sg.d1.s3|null|     root.sg|  INT32|    RLE|    SNAPPY|null|     null|
|root.sg.d1.s4|null|     root.sg|BOOLEAN|    RLE|    SNAPPY|null|     null|
|root.sg.d1.s5|null|     root.sg|  INT64|    RLE|    SNAPPY|null|     null|
|root.sg.d1.s6|null|     root.sg| DOUBLE|    RLE|    SNAPPY|null|     null|
|root.sg.d1.s7|null|     root.sg|   TEXT|  PLAIN|    SNAPPY|null|     null|
|root.sg.d1.s2|null|     root.sg|  FLOAT|GORILLA|    SNAPPY|null|     null|

{+}------------{-}{-}{+}---{-}++{-}-----------{-}{-}------{-}++{-}------{-}{-}---------{-}++{-}--{-}{-}---------+

Execute fill query:
IoTDB> select s4 from root.sg.d1 where time = 2020-12-12T14:10:10.000+08:00 fill(linear,20m,100m)
{+}----------------------------{-}{-}{+}------------+
|                        Time|root.sg.d1.s4|

{+}----------------------------{-}{-}{+}------------+
|2020-12-12T14:10:10.000+08:00|        null|

{+}----------------------------{-}{-}{+}------------+

So, timeseries root.sg.d1.s4 is a BOOLEAN data type. Even you don't have to throw the UnsupportedDataTypeException, but it's better to write a log in the server.

!image-2022-01-12-20-30-33-097.png!

 "	IOTDB	Closed	3	1	3357	0.13.0, pull-request-available
13561001	Enhance IoTDBPartitionCreationIT	The checking time in IoTDBPartitionCreationIT should be extended to 30s because the interval of RegionCleaner is 10s.	IOTDB	Closed	3	1	3357	pull-request-available
13471884	Clear up Non-Seed-ConfigNode register process	Use TConfigNodeConfiguration, including TConfigNodeLocation and TNodeResource, to maintain registeredConfigNodes. Therefore we can get an overview of cluster ConfigNodes' resource.	IOTDB	Closed	3	2	3357	pull-request-available
13441041	Optimize the serialization and deserialization of thrift data structures	In ThriftCommonsSerDeUtils and ThriftConfigNodeSerDeUtils the serialization and deserialization of various data structure interface is implemented by artificial. This is not conducive to the follow-up extension to upgrade	IOTDB	Closed	3	4	3357	pull-request-available
13471789	Fix genRealTimeRegionRoutingMap bug	We found that the realTimeRegionRoutingMap is incorrect when some DataNodes fall down. I then discovered that this bug was caused by float not being able to express the timestamp of long. Therefore I replace the unit of loadScore by long for temporary fix this bug. Finally, I also reinforce the router test to simulate the real production environment as much as possible.	IOTDB	Closed	3	1	3357	pull-request-available
13493711	[Broadcast partitionCache on removed datanode] ConsensusGroupNotExistException: The consensus group DataRegion[11] doesn't exist	"m_1031_76b947f
3rep , 3C5D
schema region : ratis
data region : multiLeader

{color:#DE350B}*This issue contains 3 bugs*{color}

benchmark runs for 1 hour and execute remove (ip72) datanode ,  ip72 datanode brushes ERROR logs ：
2022-10-31 17:48:06,277 [pool-25-IoTDB-ClientRPC-Processor-85$20221031_094806_31457_3.1.0] ERROR o.a.i.d.m.p.s.FragmentInstanceDispatcherImpl:234 - write locally failed. TSStatus: TSStatus(code:412, message:org.apache.iotdb.consensus.exception.ConsensusGroupNotExistException: The consensus group DataRegion[13] doesn't exist), message: org.apache.iotdb.consensus.exception.ConsensusGroupNotExistException: The consensus group DataRegion[13] doesn't exist
2022-10-31 17:48:06,285 [pool-25-IoTDB-ClientRPC-Processor-93$20221031_094806_31458_3.1.0] ERROR o.a.i.d.m.e.e.RegionWriteExecutor$WritePlanNodeExecutionVisitor:235 - {color:#DE350B}*Something wrong happened while calling consensus layer's write API.
org.apache.iotdb.consensus.exception.ConsensusGroupNotExistException: The consensus group DataRegion[11] doesn't exist*{color}
        at org.apache.iotdb.consensus.multileader.MultiLeaderConsensus.write(MultiLeaderConsensus.java:155)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor.fireTriggerAndInsert(RegionWriteExecutor.java:101)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.executeDataInsert(RegionWriteExecutor.java:215)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.visitInsertTablet(RegionWriteExecutor.java:163)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.visitInsertTablet(RegionWriteExecutor.java:117)
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.write.InsertTabletNode.accept(InsertTabletNode.java:1085)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor.execute(RegionWriteExecutor.java:83)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchLocally(FragmentInstanceDispatcherImpl.java:232)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:137)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchWriteSync(FragmentInstanceDispatcherImpl.java:119)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatch(FragmentInstanceDispatcherImpl.java:90)
        at org.apache.iotdb.db.mpp.plan.scheduler.ClusterScheduler.start(ClusterScheduler.java:102)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.schedule(QueryExecution.java:283)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.start(QueryExecution.java:201)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:146)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:160)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.insertTablet(ClientRPCServiceImpl.java:1198)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:4078)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:4058)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)


{color:#DE350B}**See the attachment for region information before and after the remove operation .
An incorrect phenomenon**{color}
 !screenshot-1.png! 

{color:#DE350B}*When  removing,  new dataregion was created,but no data ：*{color}
 !screenshot-2.png! 

Test ENV:
1. 192.168.10.72、73、74、75、76         48CPU 384GB
ConfigNode
MAX_HEAP_SIZE=""8G""
cn_connection_timeout_ms=120000

Common ：
connection_timeout_ms=120000
max_connection_for_internal_service=200
query_timeout_threshold=36000000
multi_leader_throttle_threshold_in_byte=536870912000
max_waiting_time_when_insert_blocked=120000
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
data_replication_factor=3


Datanode ：
MAX_HEAP_SIZE=""256G""
MAX_DIRECT_MEMORY_SIZE=""32G""

2. benchmark configuration
See the attachment 
3. remove cmd ：
{color:#DE350B}*fit-72*{color}:/data/mpp_test/m_1031_76b947f$ cat rm.sh 
#!/bin.bash
sleep 1h
./sbin/start-cli.sh -h 192.168.10.76 -e ""show cluster"" >> bef_rm_info.out
./sbin/start-cli.sh -h 192.168.10.76 -e ""show regions"" >> bef_rm_info.out

./sbin/remove-datanode.sh  192.168.10.72:6667 > rm_ip72.out


"	IOTDB	Closed	3	1	3357	pull-request-available
13523418	Rename StorageGroup to Database in ConfigNode	To match our new code framework	IOTDB	Closed	3	4	3357	pull-request-available
13563308	Alter Procedure timeout	Alter the procedure timeout equal to internal rpc timeout. Therefore the procedure will have enough time to process.	IOTDB	Closed	3	4	3357	pull-request-available
13477996	Record DataNode's read-only status in ConfigNode	"The DataNode has status ReadOnly which indicates itself has persistence problem. It's the duty of ConfigNode-leader to monitor DataNodes' running status and take corrective measures when some DataNodes run abnormally. The ConfigNode-leader will have the ability to monitor DataNode's ReadOnly and Error status after the following updates:

 
 # Merge SystemStatus into NodeStatus
 # DataNodes will report their status during each heartbeat
 # ConfigNode-leader will broadcast the latest RegionRouteMap as soon as some DataNodes change their status."	IOTDB	Closed	3	2	3357	pull-request-available
13517891	Move parameters into correct ConfigClass	"Make the ConfigClasses corresponding to the .properties files is better for our future parameters management.

 

And the redundant parameters in the IoTDBConfig should also be removed."	IOTDB	Closed	3	4	3357	pull-request-available
13524866	ConfigNode counter dashboard	Make the ConfigNode-Dashboard ponderable and comprehensible when exhibiting cluster metrics.	IOTDB	Closed	3	4	3357	pull-request-available
13468303	Reinforce ConfigNode startup process	"The startup process of non-seed ConfigNode exists the following hidden dangers:
 # The register request should be send to ConfigNode-leader, but user can pick a random ConfigNode as register target. Therefore, we should add a retrying logic, and protect the redirecting logic.
 # The hidden parameter _configNodeList_ in ConfigNodeConfig is used in more than one place(ConfigNodeStartupCheck, ConsensusManager and NodeInfo) and can be confusing.
 # We should create a static class to manage all read/write opeartions involving the configNode-system.properties file"	IOTDB	Closed	3	4	3357	pull-request-available
13444865	Printing logs when get/getOrCreate Partition in ConfigNode	Currently, we don't know what PartitionTable that ConfigNode returns to DataNode, which is not conducive to development debug. So I have temporarily added logs for getSchemaPartition getOrCreateSchemaPartition getDataPartition, getOrCreateDataPartition  interfaces.	IOTDB	Closed	3	4	3357	pull-request-available
13470973	Create read_consistency_level parameter	"We need a new global config parameter to indicate the read consistency level:

 
 # strong: the read requests are always sent to leader replica
 # weak: the read requests are sent to random replica"	IOTDB	Closed	3	2	3357	pull-request-available
13523881	Optimized the logic that Datanodes can't restart before 20 seconds	"[start] It's not a good idea to wait 20s to restart a datanode 

 !image-2023-02-09-11-43-44-065.png|width=800! 

suppose: 
I change some configurations and need to restart to make sense, then I want to restart immediately, rather than waiting 20s

It's not a good designation.
"	IOTDB	Closed	3	4	3357	pull-request-available
13516112	Add cluster_name parameter	"We need the parameter `cluster_name` to distinguish different clusters.

 

Add `cluster_name` parameter in iotdb-common.property

 "	IOTDB	Closed	3	2	3357	pull-request-available
13445407	"Msg: 500: [INTERNAL_SERVER_ERROR(500)] Exception occurred: ""create timeseries. executeStatement failed. An error occurred when executing getOrCreateSchemaPartition():Error in calling method getOrCreateSchemaPartition"	"Msg: 500: [INTERNAL_SERVER_ERROR(500)] Exception occurred: ""create timeseries root.sg.d.s1 with datatype=INT32,encoding=RLE,compression=snappy"". executeStatement failed. An error occurred when executing getOrCreateSchemaPartition():Error in calling method getOrCreateSchemaPartition

复现步骤：

1.编译复制distribution内文件（默认配置）

2.启动confignode（正常）

3.启动datanode（正常）

4.iotdb-cli连接datanode（正常）

5.复制输入以下语句：
set storage group to root.sg;
create timeseries root.sg.d.s1 with datatype=INT32,encoding=RLE,compression=snappy;
create timeseries root.sg.d.s2 with datatype=INT32,encoding=RLE,compression=snappy;
create timeseries root.sg.d.s3 with datatype=INT32,encoding=RLE,compression=snappy;
insert into root.sg.d(time,s1,s2,s3) values(1,1,2,3);
insert into root.sg.d(time,s1,s2,s3) values(2,1,2,3);
6.报错CODE500
!image-2022-05-17-19-46-50-123.png!

 "	IOTDB	Closed	3	1	3357	pull-request-available
13470610	The Non-Seed-ConfigNode should suicide if waiting leader's scheduling for too long	It's obviously that this new feature is necessary. I decide to complete this feature by judging if the current Non-Seed-ConfigNode joins in the ConsensusGroup sucessfully.	IOTDB	Closed	3	2	3357	pull-request-available
13524190	[ confignode] ERROR o.a.i.c.m.p.PartitionManager:320 - Lacked some data partition allocation result in the response of getOrCreateDataPartition method	"master_0210_8757400
问题描述：
启动3副本3C5D集群，benchmark创建元数据，写入数据。
设置集群为readonly （为了避免 insert + drop database 并发报错）
drop database root.** ; 全部drop成功。
这时客户端还在继续写入，集群设置为running
应该能继续自动创建元数据，写入数据，实际是写入全部失败
confignode报错：
2023-02-10 17:50:54,128 [pool-11-IoTDB-ConfigNodeRPC-Processor-33] ERROR o.a.i.c.m.p.PartitionManager:320 - Lacked some data partition allocation result in the response of getOrCreateDataPartition method

DataNode报错:
2023-02-10 17:54:37,461 [pool-25-IoTDB-ClientRPC-Processor-155] WARN  o.a.i.d.u.ErrorHandlingUtils:61 - Status code: EXECUTE_STATEMENT_ERROR(301), operation: insertTablet failed
java.lang.RuntimeException: Failed to get replicaSet of consensus group[id= TConsensusGroupId(type:DataRegion, id:14)]
        at org.apache.iotdb.db.mpp.plan.analyze.cache.PartitionCache.getRegionReplicaSet(PartitionCache.java:419)
        at org.apache.iotdb.db.mpp.plan.analyze.cache.PartitionCache.getTimeSlotDataPartition(PartitionCache.java:727)
        at org.apache.iotdb.db.mpp.plan.analyze.cache.PartitionCache.getDeviceDataPartition(PartitionCache.java:695)
        at org.apache.iotdb.db.mpp.plan.analyze.cache.PartitionCache.getStorageGroupDataPartition(PartitionCache.java:647)
        at org.apache.iotdb.db.mpp.plan.analyze.cache.PartitionCache.getDataPartition(PartitionCache.java:607)
        at org.apache.iotdb.db.mpp.plan.analyze.ClusterPartitionFetcher.getOrCreateDataPartition(ClusterPartitionFetcher.java:260)
        at org.apache.iotdb.db.mpp.plan.analyze.AnalyzeVisitor.getAnalysisForWriting(AnalyzeVisitor.java:2002)
        at org.apache.iotdb.db.mpp.plan.analyze.AnalyzeVisitor.visitInsertTablet(AnalyzeVisitor.java:1828)
        at org.apache.iotdb.db.mpp.plan.analyze.AnalyzeVisitor.visitInsertTablet(AnalyzeVisitor.java:183)
        at org.apache.iotdb.db.mpp.plan.statement.crud.InsertTabletStatement.accept(InsertTabletStatement.java:121)
        at org.apache.iotdb.db.mpp.plan.statement.StatementVisitor.process(StatementVisitor.java:113)
        at org.apache.iotdb.db.mpp.plan.analyze.Analyzer.analyze(Analyzer.java:48)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.analyze(QueryExecution.java:263)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.<init>(QueryExecution.java:147)
        at org.apache.iotdb.db.mpp.plan.Coordinator.createQueryExecution(Coordinator.java:107)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:140)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:167)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.insertTablet(ClientRPCServiceImpl.java:1357)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:4279)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:4259)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)


测试环境 私有云1期
1.3副本3C5D
3confignode ：172.16.2.23/24 /25 /data/iotdb/m_0210_8757400/logs
5datanode : 172.16.2.2/3/4/5/6 /data1/iotdb/m_0210_8757400/logs
benchmark 在 172.16.2.26 /data/iotdb/benchmark_v1

数据库配置
ConfigNode
MAX_HEAP_SIZE=""20G""
MAX_DIRECT_MEMORY_SIZE=""6G""
DataNode
MAX_HEAP_SIZE=""20G""
MAX_DIRECT_MEMORY_SIZE=""6G""
Common配置
schema_replication_factor=3
data_replication_factor=3

2. 启动benchmark 创建元数据，写入数据

3. 设置集群整体状态为readonly

4.drop database root.**

删除所有数据库

5.此时benchmark还在insert数据

设置集群整体状态为running，benchmark写入失败。

查看集群日志。"	IOTDB	Resolved	2	1	3357	pull-request-available
13558533	Greedy CopySet allocation algorithm	"New RegionGroup allocation algorithm based on the [Copyset theory|https://www.usenix.org/system/files/conference/atc13/atc13-cidon.pdf].

The implementation uses the greedy and dfs algorithm to iterate every possible allocation plan and select a random one among those optimal result, which is more suitable for small and medium-sized cluster.

This new algorithm ensures that each DataNode will still manage the same number of Regions, while the scatter width of each DataNode will be as large as possible. As a result, the cluster will gain the following benifits:
 # The RegionGroup-leader can be distributed more evenly among the cluster when some of the DataNodes is shutdown.
 # A DataNode can be restored more quickly when restarted. Because it's data replication is distributed to more other DataNodes in the cluster.

 "	IOTDB	Closed	3	2	3357	pull-request-available
13533125	Construct Cluster-LoadPublisher-Thread and IClusterStatusSubscriber	"New feature in ConfigNode-leader's LoadManager:

 
 * Cluster-LoadPublisher-Thread: publish cluster events
 * IClusterStatusSubscriber: subscribe cluster status events"	IOTDB	Closed	3	2	3357	pull-request-available
13480095	No disk space, no load balancing to new datanode	"master_0904_2db66c6
ConfigNode开启时间分区
 !image-2022-09-05-16-46-15-829.png! 
启动3副本3C3D干净集群，启动benchmark写入数据，元数据创建完成，写入一些数据后，增加2个datanode。
ip3(follower) 磁盘满，{color:#DE350B}*并没有路由新分区到新datanode，没有负载均衡*{color}。
{color:#DE350B}*20分钟后，ip5（leader）写入停止（multiLeader的限流，wal 51GB）*{color}（客户端bm并没有停止写入）

ip3 报错：
2022-09-05 16:30:04,560 [pool-29-IoTDB-WAL-Sync(node-root.test.g0_0-1)-1] ERROR o.a.i.d.w.b.WALBuffer$SyncBufferTask:427 - Fail to sync wal node-root.test.g0_0-1's buffer, change system mode to error.
java.io.IOException: No space left on device
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
        at sun.nio.ch.IOUtil.write(IOUtil.java:51)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
        at org.apache.iotdb.db.wal.io.LogWriter.write(LogWriter.java:58)
        at org.apache.iotdb.db.wal.io.WALWriter.write(WALWriter.java:50)
        at org.apache.iotdb.db.wal.buffer.WALBuffer$SyncBufferTask.run(WALBuffer.java:425)
        at org.apache.iotdb.commons.concurrent.WrappedRunnable$1.runMayThrow(WrappedRunnable.java:44)
        at org.apache.iotdb.commons.concurrent.WrappedRunnable.run(WrappedRunnable.java:29)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2022-09-05 16:30:04,637 [pool-19-IoTDB-MultiLeaderConsensusRPC-Client-15] ERROR o.a.i.c.m.t.MultiLeaderConsensusIService$AsyncProcessor$syncLog$1:215 - Exception inside handler
org.apache.iotdb.commons.exception.IoTDBException: Fail to sync log because system is read-only.
        at org.apache.iotdb.consensus.multileader.service.MultiLeaderRPCServiceProcessor.syncLog(MultiLeaderRPCServiceProcessor.java:76)
        at org.apache.iotdb.consensus.multileader.thrift.MultiLeaderConsensusIService$AsyncProcessor$syncLog.start(MultiLeaderConsensusIService.java:234)
        at org.apache.iotdb.consensus.multileader.thrift.MultiLeaderConsensusIService$AsyncProcessor$syncLog.start(MultiLeaderConsensusIService.java:177)
        at org.apache.thrift.TBaseAsyncProcessor.process(TBaseAsyncProcessor.java:103)
        at org.apache.thrift.server.AbstractNonblockingServer$AsyncFrameBuffer.invoke(AbstractNonblockingServer.java:603)
        at org.apache.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

测试流程：
1. 3C3D 192.168.130.3/4/5   16核32G

2. benchmark在ip2，配置文件见附件
/home/benchmark/bm_0620_7ec96c1
元数据创建完成，开始写入数据。
集群和regions信息：
 !image-2022-09-05-16-51-07-405.png! 
3. 增加datanode ip1和ip2

集群和regions信息
 !image-2022-09-05-16-52-07-143.png! 

4. 往ip3机器，数据所在磁盘，复制一些数据，让ip3的磁盘空间满。

ip3 的磁盘满，节点read-only（报错信息见问题描述），详细日志见附件。

集群并没有负载均衡，集群和region信息：
 !image-2022-09-05-16-54-09-763.png! 

ip5（leader） wal大小达到限流 阈值：
 !screenshot-1.png! "	IOTDB	Closed	3	1	3357	pull-request-available
13471414	Optimize leader routing policy	"Currently, the ConfigNode-leader will broadcast the RegionRouteMap continuously when the routing_policy is set to ""leader"" even if the leadership of all Regions don't change"	IOTDB	Closed	3	4	3357	pull-request-available
13468842	Create parameter schema_region_per_data_node and data_region_per_processor in ConfigNode	We add two parameters schema_region_per_data_node and  data_region_per_processor which makes the maxRegionCount configurable	IOTDB	Closed	3	4	3357	pull-request-available
13535901	Fix route balancing not trigger bug	The route balancing will not be triggered if the leader election is faster than ConfigNode persist new created RegionGroup.	IOTDB	Closed	3	1	3357	pull-request-available
13486862	Consensus heartbeat statistics	"Currently, all heartbeat samples are cached above the consensus layer of ConfigNode, which connotes that all load statistics will be discarded as long as the leader of ConfigNode is switched. Obviously, the following kinds of loadStatistics are worth to consensus:

 
 * The status of cluster Nodes and RegionGroups
 * The leader of RegionGroup
 * The loadScore of cluster Nodes and RegionGroups(will be implemented in the future)

 

Therefore, I refactor the framework of cluster heartbeat related threads, and you can find the detailed desgin in the [LoadBalancing Framework|https://apache-iotdb.feishu.cn/docx/doxcnZ2tquzJKINQWi9nnYECPCb] doc."	IOTDB	Closed	3	4	3357	pull-request-available
13529162	Replace data_region_per_processor by data_region_per_data_node	The parameter data_region_per_data_node is a empirical value from reall production environmennt. Replace data_region_per_processor by data_region_per_data_node will improve the efficiency of default cluster and simplify testing process.	IOTDB	Closed	3	4	3357	pull-request-available
13444997	Region expansion based on the total number of cpu cores	We first allocated Regions with 50% of the cluster's CPU cores to facilitate testing.	IOTDB	Closed	3	4	3357	pull-request-available
13462607	Optimize getOrCreatePartition process	The PartitionManager has poor judgment on triggering conditions for Region expansion. Therefore, optimize and record a document	IOTDB	Closed	3	4	3357	pull-request-available
13482720	[DOC] [cluster]SET_SYSTEM_TO_READONLY mode, data operation restrictions are not clear	"In the current 0.14 cluster version, after setting the SET SYSTEM TO READONLY

mode in CLI, the restrictions on data are not very clear. For example, you can create storage groups, but you cannot create time series and insert data.

 

!image-2022-09-22-16-12-04-886.png|width=557,height=222!"	IOTDB	Closed	4	4	3357	pull-request-available
13541898	Top-K DTW algorithm	The Top-K DTW algorithm can select the top-k DTW substring in the target time-series compare to the pattern time-series	IOTDB	In Progress	3	2	3357	pull-request-available
13554371	Rename target_config_node_list to seed_config_node	Rename target_config_node_list to seed_config_node to prevent misunderstanding.	IOTDB	Closed	3	4	3357	pull-request-available
13482855	[DOC] Add load balance UserGuide	Add the relationship of MaxRegionCout, CurrentRegionCount, seriesslot, datanode CPU cores and DataNodes.	IOTDB	Closed	4	4	3357	pull-request-available
13445306	[ConfigNode  Startup] ConfigNode failed to start, restarting again is considered a “restart”	"master_0516_7942b02
iotdb-confignode.properties配置文件中的target_confignode参数配置的不正确，如：
target_confignode=192.168.130.3:22277{color:#DE350B}*;*{color}192.168.130.4:22277

start-confignode.sh失败，却生成了 ./confignode/data/system/confignode-system.properties文件，里面记录的confignode_list=是空：

 !image-2022-05-17-11-19-03-968.png! 

iotdb-confignode.properties配置正确的target_confignode，再次启动confignode，失败：
 !image-2022-05-17-11-17-39-917.png! 

根据异常信息，去代码查看，发现走了restart ：
 !image-2022-05-17-11-22-09-195.png! 


 "	IOTDB	Closed	3	1	3357	pull-request-available
13416269	Change the fill syntax	"Change the fill syntax from:
{code:java}
Fill (typeClause (, typeClause)*){code}
to:
{code:java}
Fill (linearClause | previousClause | specificValueClause | previousUntilLastClause | oldTypeClause (COMMA oldTypeClause)*){code}
The new syntax is compatible with the old syntax. But only one fill method can be specified in one SQL. In order to unify the extra query time range within the query and reduce the difficulty of working with the result dataset."	IOTDB	Closed	3	4	3357	pull-request-available
13540224	Control the RegionGroup number of system Database	The number of SchemaRegionGroups and DataRegionGroups in system Database should always be 1.	IOTDB	Closed	3	4	3357	pull-request-available
13471298	DataNode cannot connect ConfigNode and join Cluster after restart cluster	"The registeredConfigNode info is not recorded in configNode snapshot, as a result of which the info will lose after restart configNode. 

The dataNode need the registeredConfigNode info to join cluster."	IOTDB	Closed	3	1	3357	pull-request-available
13469485	Update RegionRouteMap based on heartbeat sample	"Update the RegionRouteMap cache in PartitionCache based on the heartbeat sample. Update algorithms are distinguished according to different routing policies:

 
 # For greedy policy, update RegionRouteMap when there exists some DataNodes fail down.
 # For leader policy, update RegionRouteMap when the leadership of Regions changed."	IOTDB	Closed	3	2	3357	pull-request-available
13541874	Format LoadStatistics logs and add ClusterIT	Format LoadStatistics logs to improve readability. Add ClusterIT to make sure the leader of DataRegionGroup will re-distribute as long as some DataNodes turn into ReadOnly Status.	IOTDB	Closed	3	4	3357	pull-request-available
13563650	Alter default disk_space_warning_threshold in ClusterIT environment	Alter default disk_space_warning_threshold in ClusterIT environment from 5% to 1%. Therefore the test DataNodes will not fall into ReadOnly status so easily.	IOTDB	Closed	3	4	3357	pull-request-available
13474794	Latent routing policy for MultiLeader protocol	See this [doc|https://apache-iotdb.feishu.cn/docx/doxcnMg51Z44hC0IbzD26TCWtne?appStyle=UI4&domain=www.feishu.cn&locale=zh-CN&refresh=1&sidebarOpen=1&tabName=space&theme=light&userId=7078098607744024580] for more details.	IOTDB	Closed	3	2	3357	pull-request-available
13473370	Show DataNodes shows nothing	"I start 1C1D and use sql show datanodes, but result is nothing:

!image-2022-07-26-00-31-29-638.png!"	IOTDB	Closed	3	1	3357	pull-request-available
13494201	【1】 Implement TestOnly RPC interface setDataNodeStatus	"Implement a TestOnly RPC interface setDataNodeStatus as following:

 
{code:java}
/** Set the target DataNode to the specified status */
common.TSStatus setSystemStatus(TSetDataNodeStatusReq req) {code}
Will be helpful for ClusterIT. i.e. we can change some DataNodes' status more easily so that the ClusterIT can cover more test cases."	IOTDB	Closed	3	4	3357	pull-request-available
13503324	Greedy leader balancing policy	Although we currently have MinCostFlowLeaderBalancer to deal with the cluster leader balancing problem. We still need a simple GreedyBalancer to compare with the MinCostFlowLeaderBalancer.	IOTDB	Closed	3	2	3357	pull-request-available
13562928	Parallel RegionGroup activation	The RegionGroup activation process should be started simultaneously. Otherwise, the CreateRegionGroupProcedure might stuck at leader election wait process if there are many RegionGroups created concurrently.	IOTDB	Closed	3	4	3357	pull-request-available
13440013	Extend TDataNodeLocation	Currently, the endpoint that identifies each DataNode in the new cluster only has two variables IP and port. However, there are many ports in each DataNode, and only one port is not enough to identify the DataNodes. So we need to extend the identifier of the DataNode.	IOTDB	Closed	3	4	3357	pull-request-available
13476110	Protecting Region creation process by CreateRegionGroupsProcedure	"There is a potential bug within the creation process of RegionGroups. That is, the ConfigNode-leader always chooses some online DataNodes as the holder of new RegionReplicas. However, if a DataNode was shutdown suddenly and there occurs a requirement for creating new RegionGroups in the same time, the DataNode that just been shutdown recently will still have the probability to be chosen as the holder of the new RegionReplica. Because it takes 20s for the ConfigNode-leader to mark that DataNode as ""UNKNOWN"" status.

 

Therefore, I use the Procedure framework to create the CreateRegionGroupsProcedure to prevent this potential bug:

 
 # A batch of RegionGroups are considered as created successfully only if they are all physically created on those chosen DataNodes.
 # As long as there exists one RegionReplica failed to create, this batch of RegionGroups will be considered as fail to create. In the same time, the corresponding getOrCreatePartition request will be rejected and receive error code ""CREATE_REGION_FAIL"". Finally, these redundant ReionReplicas that fail to construct new RegionGroups will be physically removed asynchronously.

You can see the [Partition doc|https://apache-iotdb.feishu.cn/docs/doccnzlKpkxhuRw52P9pp6SfELf#kJavEE] and the [RegionGroup management doc|https://apache-iotdb.feishu.cn/docx/doxcnMQppTJCURUuWZ5LfWA5dK6] for more details."	IOTDB	Closed	3	2	3357	pull-request-available
13565022	Use nanotime in cluster heartbeat	To prevent the heartbeat bug caused by modifying server clock, use System.nanotime() in cluster heartbeat.	IOTDB	Closed	3	4	3357	pull-request-available
13440113	ConfigNode supports ClientPool	Including StorageGroupSchema, templete, etc.	IOTDB	Closed	3	2	3357	pull-request-available
13408157	Distinguish between zero and null values in sum aggregation	[ISSUE-4224|https://github.com/apache/iotdb/issues/4224]	IOTDB	Closed	1	2	3357	pull-request-available
13476292	Decoupling heartbeat ScheduledExecutorService from LoadManager	"The following changes will bring NodeManager, PartitionManager and LoadManager more in line with their respective functional definitions:

 
 # Let NodeManager maintains the heartbeat ScheduledExecutorService
 # Let NodeManager maintains the NodeCache
 # Let PartitionManager maintains the RegionGroupCache

Such that the LoadManager can focus more on the load balancing policies.

 

You can see the [ConfigNode framwork|https://apache-iotdb.feishu.cn/docs/doccnUzGLXivqOoNjAo8Ole3cQe#] doc for more details"	IOTDB	Closed	3	4	3357	pull-request-available
13539769	ConfigNode leader changing causes lacking some DataPartition allocation result in the response of getOrCreateDataPartition method	"We will get `Lacked some DataPartition allocation result in the response of getOrCreateDataPartition method` error message if confignode leader changing happens just after confirm leader done.

!image-2023-06-13-11-00-27-605.png!

!image-2023-06-13-10-58-12-517.png!"	IOTDB	Closed	3	1	3357	pull-request-available
13525357	Enable modify external RPC EndPoint of DataNode	"Including:
 * dn_rpc_address
 * dn_rpc_port"	IOTDB	Closed	3	2	3357	pull-request-available
13432272	Region allocation policy	"This Jira should wait IOTDB-2730 and IOTDB-2761. For the following reasons:

 
 # IOTDB-2730: ConfigNode will use the RPCServer to construct the ConsensusLayer
 # IOTDB-2761: ConfigNode will use the RPC interfaces in DataNode's Coordinator to transmit Region informations."	IOTDB	Closed	3	2	3357	pull-request-available
13557674	Remove redundant error log when restarting simple consensus ConfigNode	"When restarting simple consensus ConfigNode, the following logs will be printed:

 
{code:java}
2023-11-13 16:21:07,180 [main] ERROR o.a.i.c.m.c.ConsensusManager:222 - Something wrong happened while calling consensus layer's createLocalPeer API.
org.apache.iotdb.consensus.exception.ConsensusGroupAlreadyExistException: The consensus group 0 already exists
at org.apache.iotdb.consensus.simple.SimpleConsensus.createLocalPeer(SimpleConsensus.java:181)
at org.apache.iotdb.confignode.manager.consensus.ConsensusManager.createPeerForConsensusGroup(ConsensusManager.java:279)
at org.apache.iotdb.confignode.manager.consensus.ConsensusManager.setConsensusLayer(ConsensusManager.java:218)
at org.apache.iotdb.confignode.manager.consensus.ConsensusManager.<init>(ConsensusManager.java:79)
at org.apache.iotdb.confignode.manager.ConfigManager.initConsensusManager(ConfigManager.java:302)
at org.apache.iotdb.confignode.service.ConfigNode.active(ConfigNode.java:130)
at org.apache.iotdb.confignode.service.ConfigNodeCommandLine.activeConfigNodeInstance(ConfigNodeCommandLine.java:97)
at org.apache.iotdb.confignode.service.ConfigNodeCommandLine.run(ConfigNodeCommandLine.java:79)
at org.apache.iotdb.commons.ServerCommandLine.doMain(ServerCommandLine.java:58)
at org.apache.iotdb.confignode.service.ConfigNode.main(ConfigNode.java:110) {code}
 

However, the ConfigNode can still restart successfully and running healthily."	IOTDB	Closed	3	1	3357	pull-request-available
13479119	Heartbeat error after setting a DataNode's status to Read-Only	"The ConfigNode will report the following error after setting DataNode's status to Read-Only:

 

!image-2022-08-29-22-00-11-391.png!"	IOTDB	Closed	3	1	3357	pull-request-available
13555265	Remove rpc_thrift_compression_enable hard code	"Some thrift services use hard code parameter of rpc_thrift_compression_enable. Replaced by the .properties file now.


 !screenshot-1.png! "	IOTDB	Closed	3	1	3357	pull-request-available
13564647	Simplify GetNodePathsPartition log	The previous log is complex and redundant.	IOTDB	Closed	3	4	3357	pull-request-available
13544520	Cluster computing resource balance	"*Before Updation:*

Users could only either turn on the inherit policy to speed up queries or abandon the inherit policy for better DataPartition load balancing

 

*After Updation:*

Now users don't need to care the inherit policy. The cluster will decide the best DataPartition allocation while pay attention to both the inherit policy and load balance."	IOTDB	Closed	3	2	3357	pull-request-available
13540845	Heterogeneous Database document	Add document for heterogeneous Database function based on our former design.	IOTDB	Closed	3	4	3357	pull-request-available
13555695	Node error detection through broken thrift pipe	Detect shutdown Node(i.e. by ctrl+c or kill -9) through broken thrift pipe, and mark them to Unknown status immediately.	IOTDB	Closed	3	4	3357	pull-request-available
13476113	Use online DataNodes to allocate new Regions	"We currently use registered DataNodes to allocate new Regions due to the influence of ConfigNodeRPCProcessorTest framework. Actually, we should use online DataNodes to do the Region allocation.

 

!image-2022-08-10-15-55-11-351.png!"	IOTDB	Closed	3	4	3357	pull-request-available
13541507	Improve efficiency of ConfigNode PartitionInfo takeSnapshot	Use BufferedOutputStream(with 32MB buffer) instead of FileInputStream in PartitionInfo	IOTDB	Resolved	3	4	3357	pull-request-available
13490520	Object has already been returned to this pool or is invalid error in CI	"There are a lot of error logs like the following while running CI.

!image-2022-10-24-15-14-01-894.png|width=808,height=454!"	IOTDB	Closed	3	1	3357	pull-request-available
13473751	Shielding heartbeat error log	"There is no need to print heartbeat error logs:

 

!image-2022-07-27-18-04-31-955.png!"	IOTDB	Closed	3	4	3357	pull-request-available
13480791	Fix TTimePartitionSlot count and DataPartition inherit policy bug	"I've fixed two bugs in this PR:
 # The SeriesSlot count in the result of show regions are incorrect.
 # The DataPartitions with larger TTimePartitionSlot can't inherit their predecessor when a batch of contiguous DataPartition are demanded to create synchronously.

I solved these two bugs with one PR because it's too difficult to write stronger test for me if I break them apart."	IOTDB	Closed	3	1	3357	pull-request-available
13517018	[datanode]datanode would use the environment variable of IOTDB_HOME, rather than the directory where the start-datanode.sh is	"datanode would use the environment variable of IOTDB_HOME, rather than the directory where the start-datanode.sh is.

reproduction:
1. start 3C3D cluster with nodes 44, 45, 46
2. stop confignode and datanode of node 44
3. start a 1C1D in another directory with the default configurations, directory is different from $IOTDB_HOME.

actual result:
1. start 1C1D , but the logs of datanode write to $IOTDB_HOME/logs, rather than the directory where the start-datanode.sh is.
 
!image-2023-01-06-17-25-12-894.png|width=600! 
log_datanode_error.log
{code}
2023-01-06 17:15:19,324 [main] ERROR o.a.i.db.service.DataNode:255 - Cannot pull system configurations from ConfigNode-leader after 10 retries
2023-01-06 17:15:19,329 [main] ERROR o.a.i.db.service.DataNode:177 - Fail to start server
org.apache.iotdb.commons.exception.StartupException: Cannot pull system configurations from ConfigNode-leader
        at org.apache.iotdb.db.service.DataNode.pullAndCheckSystemConfigurations(DataNode.java:258)
        at org.apache.iotdb.db.service.DataNode.doAddNode(DataNode.java:154)
        at org.apache.iotdb.db.service.DataNodeServerCommandLine.run(DataNodeServerCommandLine.java:79)
        at org.apache.iotdb.commons.ServerCommandLine.doMain(ServerCommandLine.java:58)
        at org.apache.iotdb.db.service.DataNode.main(DataNode.java:141)
{code}

log_datanode_warn.log
{code}
023-01-06 17:14:54,315 [main] WARN  o.a.i.db.service.DataNode:237 - Cannot pull system configurations from ConfigNode-leader, because: Fail to connect to any config node. Please check status of ConfigNodes
2023-01-06 17:14:59,316 [main] WARN  o.a.i.d.c.ConfigNodeClient:268 - The current node may have been down TEndPoint(ip:iotdb-44, port:10710),try next node
{code}

注意IP， 1C1D 我使用的是默认配置，所以它的IP应该是127.0.0.1，为什么会是iotdb-44内，因为它使用的是 $IOTDB_HOME路径下的配置，日志也写在了$IOTDB_HOME路径下。这是不对的。应该使用我启动命令所在的文件夹。
ps: confignode没有这个问题。


2. start-cli.sh -h iotdb-45 -e ""show cluster"", the result is correct.
 !image-2023-01-06-17-27-25-078.png|width=600! 

"	IOTDB	Closed	3	1	3357	pull-request-available
13524182	[ insert + drop database ] Msg: 305: Error in calling method deleteStorageGroups	"master 0210_8757400
问题描述：
benchmark 执行创建元数据，写入数据，开始insert后，cli执行drop database root.**,  
返回
Msg: 305: Error in calling method deleteStorageGroups, because: Fail to connect to any config node. Please check status of ConfigNodes
再之后，执行show databases 操作，立即返回Msg:305:null
/data/iotdb/m_0210_8757400/sbin/start-cli.sh -h 172.16.2.2 -e 'show databases'
Msg: 305: null

测试环境 私有云1期
1.3副本3C5D
3confignode ：172.16.2.23/24 /25    /data/iotdb/m_0210_8757400/logs
5datanode : 172.16.2.2/3/4/5/6      /data1/iotdb/m_0210_8757400/logs
benchmark 在 172.16.2.26 /data/iotdb/benchmark_v1

数据库配置
ConfigNode
MAX_HEAP_SIZE=""20G""
MAX_DIRECT_MEMORY_SIZE=""6G""
DataNode
MAX_HEAP_SIZE=""20G""
MAX_DIRECT_MEMORY_SIZE=""6G""
Common配置
schema_replication_factor=3
data_replication_factor=3

2. 启动benchmark
创建元数据完成，开始写入数据 ：

/data/iotdb/m_0210_8757400/sbin/start-cli.sh -h 172.16.2.2 -e 'drop database root.**'
Msg: 305: Error in calling method deleteStorageGroups, because: Fail to connect to any config node. Please check status of ConfigNodes

[root@i-66xazbht deploy_mpp_scripts_3c5d]#  /data/iotdb/m_0210_8757400/sbin/start-cli.sh -h 172.16.2.2 -e 'show databases'
立即返回如下信息
Msg: 305: null

 !image-2023-02-10-17-25-55-548.png! 

3. 详细日志见附件

"	IOTDB	Open	2	1	3357	pull-request-available
13525360	Gracefully exit Cluster Nodes through stop script	We can take some operation to ensure cluster security if we shutdown DataNode by stop-datanode script.	IOTDB	Closed	3	2	3357	pull-request-available
13438700	ConfigNode response Leader Redirect	When Datanode send a request to ConfigNode, such as registerDataNode(). Only the leader of ConfigNode could handle this request. So, if the received ConfigNode is not leader, return a redirect response with leader address, otherwise return the execution result.	IOTDB	Closed	3	2	3357	pull-request-available
13468881	Rename config_nodes to target_config_nodes	The current name config_nodes is misleading	IOTDB	Closed	3	4	3357	pull-request-available
13448243	Serialize expansion ConfigNodeGroup	We must serialize the expansion process of ConfigNodeGroup in the ConfigNode manager module	IOTDB	Closed	2	4	3357	pull-request-available
13547167	Fix Region creation bugs	"*Updations:*
 # Both the Running and Unknown DataNodes can be used when making CreateRegionGroupsPlan. Since we have IoTDB-Region-Maintainer thread in PartitionManager to ensure the creation at Unknown DataNodes will finally successed.
 # Distinguish re-creation and re-deletion in IoTDB-Region-Maintainer. So that these corner cases wouldn't stuck into infinite loop.
 # A RegionGroup will always been Removing status as soon as it has been removed."	IOTDB	Resolved	3	1	3357	pull-request-available
13470439	Add getPartitionTable and getOrCreatePartitionTable interfaces	We've created SchemaPartitionTable and DataPartitionTable whose last level points to the TConsensusGroupId rather than a specific TRegionReplicaSet, in order to reduce the memory overhead for PartitionTable in both ConfigNode and DataNode. Therefore we should add getPartitionTable and getOrCreatePartitionTable interfaces to correspond these updates. And these interfaces will eventually take the place of getPartition and getOrCreatePartition.	IOTDB	Closed	3	2	3357	pull-request-available
13537953	Enable DataPartition inherit policy	"We've found in real production environment that queries across different DataRegionGroups are time-consuming. Therefore we should enable and enhance the DataPartition inherit policy. The new created DataPartition will inherit its neighbor(predecessor or successor)'s allocate result if the neighbor is allocated before.

see [doc|https://timechor.feishu.cn/docx/P2FIdliXeohLNqxS9Krc1iR3ngb]"	IOTDB	Closed	3	4	3357	pull-request-available
13570666	Lager interval for auto ratis leader balance	The interval for ratis leader balance is extend to 60s when there exists failed balance attempt. Because the ratis protocol has the ability for leader switch only after the replicas are synchronized.	IOTDB	Closed	3	4	3357	pull-request-available
13461985	[ confignode ] ConfigNode getLeader NPE	"master_0623_cb77e2c , 3C3D 启动confignode 报错：
2022-06-23 17:12:26,192 [pool-3-IoTDB-ConfigNodeRPC-Client-1]{color:red}* ERROR o.a.t.ProcessFunction:47 - Internal error processing registerConfigNode
java.lang.NullPointerException: null*{color}
        at org.apache.iotdb.confignode.manager.ConsensusManager.lambda${color:red}*getLeader*{color}$2(ConsensusManager.java:166)
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
        at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1359)
        at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
        at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464)
        at org.apache.iotdb.confignode.manager.ConsensusManager.getLeader(ConsensusManager.java:167)
        at org.apache.iotdb.confignode.manager.ConfigManager.confirmLeader(ConfigManager.java:479)
        at org.apache.iotdb.confignode.manager.ConfigManager.registerConfigNode(ConfigManager.java:562)
        at org.apache.iotdb.confignode.service.thrift.ConfigNodeRPCServiceProcessor.registerConfigNode(ConfigNodeRPCServiceProcessor.java:370)
        at org.apache.iotdb.confignode.rpc.thrift.ConfigIService$Processor$registerConfigNode.getResult(ConfigIService.java:2441)
        at org.apache.iotdb.confignode.rpc.thrift.ConfigIService$Processor$registerConfigNode.getResult(ConfigIService.java:2421)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
"	IOTDB	Closed	3	1	3357	pull-request-available
13505938	[Atmos][cluster]low performance of the unseq_rw test (many errors in log files)	"1.Start a 1C1D wit default config

2.using a bm to do write and read operations(config as the attachment unse

Please refer to below picture：

[http://111.202.73.147:13000/d/391PQsXnk/weekly-test?orgId=1&viewPanel=10]

!image-2022-11-25-21-10-58-796.png|width=783,height=342!

 

2022-11-25 17:29:03,515 [pool-25-IoTDB-ClientRPC-Processor-11] ERROR o.a.i.d.u.ErrorHandlingUtils:90 - Status code: 301, Query Statement: ""SELECT count(s_266) FROM root.test.g_6.d_46 WHERE  root.test.g_6.d_46.s_266 > -5"". executeStatement failed
org.apache.iotdb.commons.exception.IoTDBException: org.apache.iotdb.db.mpp.execution.schedule.FragmentInstanceAbortedException: FragmentInstance 20221125_092803_09770_1.1.0 is aborted by timeout
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.dealWithException(QueryExecution.java:430)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getResult(QueryExecution.java:413)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getByteBufferBatchResult(QueryExecution.java:448)
        at org.apache.iotdb.db.utils.QueryDataSetUtils.convertQueryResultByFetchSize(QueryDataSetUtils.java:387)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.lambda$static$0(ClientRPCServiceImpl.java:160)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementInternal(ClientRPCServiceImpl.java:236)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementV2(ClientRPCServiceImpl.java:390)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeQueryStatementV2(ClientRPCServiceImpl.java:380)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeQueryStatementV2.getResult(IClientRPCService.java:3403)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeQueryStatementV2.getResult(IClientRPCService.java:3383)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.iotdb.db.mpp.execution.schedule.FragmentInstanceAbortedException: FragmentInstance 20221125_092803_09770_1.1.0 is aborted by timeout
        at org.apache.iotdb.db.mpp.execution.schedule.DriverScheduler.clearDriverTask(DriverScheduler.java:246)
        at org.apache.iotdb.db.mpp.execution.schedule.DriverScheduler.access$900(DriverScheduler.java:54)
        at org.apache.iotdb.db.mpp.execution.schedule.DriverScheduler$Scheduler.toAborted(DriverScheduler.java:390)
        at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskTimeoutSentinelThread.execute(DriverTaskTimeoutSentinelThread.java:77)
        at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:74)
2022-11-25 17:30:04,355 [pool-25-IoTDB-ClientRPC-Processor-11] ERROR o.a.i.d.u.ErrorHandlingUtils:90 - Status code: 301, Query Statement: ""SELECT count(s_268) FROM root.test.g_0.d_0 WHERE  root.test.g_0.d_0.s_268 > -5"". executeStatement failed

FragmentInstance 20221125_093007_10213_1.1.0 is aborted by timeout"	IOTDB	Closed	3	1	5267	pull-request-available
13479460	[ mem leak ] Request metadata timed out, causing memory leak	"master_0830_42fcbfc
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.standalone.{color:#DE350B}*StandAloneConsensus*{color}
{color:#DE350B}*schemaregion 1副本*{color}
dataregion 3副本
先启动1confignode，3datanode，启动benchmark，只有写入，{color:#DE350B}15小时后，大量写入失败{color}，下图是统计每小时的写入数据量：
  !screenshot-1.png! 

ip5 error
2022-08-30 18:01:04,256 [20220830_094446_42966_3.1.0-1068] ERROR o.a.i.d.m.e.f.FragmentInstanceManager:157 - Execute error caused by
org.apache.iotdb.db.mpp.exception.MemoryNotEnoughException: There is not enough memory to execute current fragment instance, current remaining free memory is 1014007, estimated memory usage for current fragment instance is 1048576
        at org.apache.iotdb.db.mpp.plan.planner.LocalExecutionPlanner.checkMemory(LocalExecutionPlanner.java:132)
        at org.apache.iotdb.db.mpp.plan.planner.LocalExecutionPlanner.plan(LocalExecutionPlanner.java:104)
        at org.apache.iotdb.db.mpp.execution.fragment.FragmentInstanceManager.lambda$execSchemaQueryFragmentInstance$3(FragmentInstanceManager.java:147)
        at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
        at org.apache.iotdb.db.mpp.execution.fragment.FragmentInstanceManager.execSchemaQueryFragmentInstance(FragmentInstanceManager.java:133)
        at org.apache.iotdb.db.consensus.statemachine.SchemaRegionStateMachine.read(SchemaRegionStateMachine.java:94)
        at org.apache.iotdb.consensus.standalone.StandAloneServerImpl.read(StandAloneServerImpl.java:72)
        at org.apache.iotdb.consensus.standalone.StandAloneConsensus.read(StandAloneConsensus.java:135)
        at org.apache.iotdb.db.service.thrift.impl.DataNodeInternalRPCServiceImpl.sendFragmentInstance(DataNodeInternalRPCServiceImpl.java:169)
        at org.apache.iotdb.mpp.rpc.thrift.IDataNodeRPCService$Processor$sendFragmentInstance.getResult(IDataNodeRPCService.java:2136)
        at org.apache.iotdb.mpp.rpc.thrift.IDataNodeRPCService$Processor$sendFragmentInstance.getResult(IDataNodeRPCService.java:2116)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)


复现流程
1. 172.20.70.3/4/5   8c32G  datanode
172.20.70.31 confignode  8c32G
benchmark在ip15  /data/benchmark/bm_0620_7ec96c1
集群与regions信息
 !screenshot-2.png! 
2. 数据库配置参数
confignode
MAX_HEAP_SIZE=""16G""
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.standalone.StandAloneConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=1
data_replication_factor=3

datanode
MAX_HEAP_SIZE=""16G""
wal_buffer_size_in_byte=1048576
max_waiting_time_when_insert_blocked=3600000

3. 启动bm 配置文件见附件

4. 后续还有add datanode（20分钟 加1个datanode，共加6个）
ip 2/13/14/16/18/19
但是没有新的写入操作，所以这些新的datanode上没有数据。
"	IOTDB	Closed	3	1	5267	pull-request-available
13502642	Optimize PartitonFetch Process in query	"Currently, in query process, no matter there is time filter in sql or not, PartitionFetcher will choose to fetch that from confignode through rpc instead of trying to use cache.

However, if query sql has timefilter like `where time >= XX and time <= XXX`, we may get data partition info in local cache."	IOTDB	Closed	3	4	5267	pull-request-available
13283368	Support Fill function in Group by query	"Hi, we need to extend the group by query to support fill function. 

such as:

select count(s1) from root.sg.d1 group by ([1,100], 10ms) fill (int32[previous,20ms])"	IOTDB	Closed	4	2	5267	pull-request-available
13450796	DeadLock in LocalSInkHandle and LocalSourceHandle	"There will be thread stuck in current CI, and query result won't be returned in 10s.

After analyzing logs, I found that there may be potential deadlock in LocalSinkHandle and LocalSourceHandle. 

!image-2022-06-19-16-55-19-344.png!

""set noMoreTsBlocks."" has been printed, but another log ""noMoreTsBlocks has been set."" is not printed until the query timeout."	IOTDB	Closed	3	1	5267	pull-request-available
13590405	Null values are not filtered out by in filter	"```
CREATE DATABASE root.testNullFilter;
CREATE ALIGNED TIMESERIES root.testNullFilter.d2(s1 INT32, s2 BOOLEAN, s3 DOUBLE);
INSERT INTO root.testNullFilter.d2(timestamp,s2,s3) values(1, false, 11.1);
INSERT INTO root.testNullFilter.d2(timestamp,s1,s2) values(2, 22, true);
INSERT INTO root.testNullFilter.d2(timestamp,s1,s2) values(3, 22, false);
select * from root.testNullFilter.d2 where s1 in (22, 23);
```

!image-2024-08-29-15-17-58-608.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13378432	Auto Close the dataset while there is no more data	Some users may forget to close the dataset manually after iterating the dataset which will cause resource leaky. We can automatically close it for users while there is no more data.	IOTDB	Closed	3	4	5267	pull-request-available
13578541	Column Headers of auth statement is unstandard	"ColumnHeaders of list privileges of user/role and list user/role are not standard:

!image-2024-05-08-12-00-49-693.png!

We expect them to be like: User, Role, Path, Privileges, GrantOption"	IOTDB	Closed	3	1	5267	pull-request-available
13303543	Cached page not consumed while doing aggregation query	!image-2020-05-08-13-23-27-411.png!	IOTDB	Closed	3	1	5267	pull-request-available
13561322	Construct SessionPool using empty nodeUrls should throw exception	"while we use an empty nodeUrls to construct SessionPool, we will succeed(using localhost and 6667 as default host and port) which is not we want. We expect it throw exception to tell us that nodeUrls shouldn't be empty.

```

SessionPool sessionPool = new SessionPool(Collections.emptyList(), ""root"", ""root"", 3);

```"	IOTDB	Open	3	1	5267	pull-request-available
13351014	Print the file name while error happened in th process of reading	Currently, if BufferOverflowException happened, the corresponding file name will not be printed in log, and it's diffcult to find the file which cause this.	IOTDB	Closed	3	4	5267	pull-request-available
13572405	Correct the implementation of FILL(LINEAR)	more details can be seen in https://apache-iotdb.feishu.cn/docx/OlrNd4oSmos8QVxCGm2cxThanBg	IOTDB	Open	3	1	5267	pull-request-available
13476729	NPE in StateTracker for StandAlone mode	"If we still fetch the state for one FI after the query is finished, there will exist NPE in AbstractFragInsStateTracker.

!image-2022-08-15-10-07-20-297.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13534376	Concurrent bug caused by non-atomic operation in QueryExecution	!image-2023-04-28-09-24-08-392.png!	IOTDB	Open	3	1	5267	pull-request-available
13310588	Lazy Contruct Alias Map in InternalMNode	"Currently, we maintain an alias map for each InternalMNode. However, firstly, only the device node will have data in that map; secondly, not all of the device node will have alias.

And the map has initial capacity of 16, that will waste some memory footprint.

So, we can defer the construction to when the alias map is really used, i.e. there are alias added to this node."	IOTDB	Closed	3	4	5267	pull-request-available
13305122	Make LRUCache more accurate	Currently, the ChunkCache, ChunkMetadataCache and TimeSeriesMetadataCache's calculation is wrong, and will cause OOM	IOTDB	Closed	3	4	5267	pull-request-available
13447587	timeseries id is not right while doing last query	!image-2022-05-31-08-35-58-463.png!	IOTDB	Closed	3	1	5267	pull-request-available
13329625	The query result set of JDBC will be covered by next query	"{code:java}

public static void main(String[] args) throws ClassNotFoundException, SQLException {
 Class.forName(""org.apache.iotdb.jdbc.IoTDBDriver"");
 try (Connection connection = DriverManager.getConnection(""jdbc:iotdb://127.0.0.1:6667/"", ""root"", ""root"");
 Statement statement = connection.createStatement()) {
 try {
 statement.execute(""SET STORAGE GROUP TO root.sg1"");
 statement.execute(""CREATE TIMESERIES root.sg1.d0.s0 WITH DATATYPE=INT64, ENCODING=RLE, COMPRESSOR=SNAPPY"");
 statement.execute(""CREATE TIMESERIES root.sg1.d0.s1 WITH DATATYPE=INT64, ENCODING=RLE, COMPRESSOR=SNAPPY"");
 statement.execute(""CREATE TIMESERIES root.sg1.d1.s0 WITH DATATYPE=INT64, ENCODING=RLE, COMPRESSOR=SNAPPY"");
 statement.execute(""CREATE TIMESERIES root.sg1.d1.s1 WITH DATATYPE=INT64, ENCODING=RLE, COMPRESSOR=SNAPPY"");
 } catch (IoTDBSQLException e) {
 System.out.println(e.getMessage());
 }

 statement.execute(""insert into root.sg1.d0(timestamp,s0,s1) values(1,1,1)"");
 statement.execute(""insert into root.sg1.d1(timestamp,s0,s1) values(1000,1000,1000)"");

 List<ResultSet> resultSetList = new ArrayList<>();
 resultSetList.add(statement.executeQuery(""select * from root.sg1.d0 where time <= 1""));
 resultSetList.add(statement.executeQuery(""select * from root.sg1.d1 where s0 >= 100""));

 for (ResultSet resultSet : resultSetList) {
 outputResult(resultSet);
 }

 } catch (IoTDBSQLException e){
 System.out.println(e.getMessage());
 }
}

{code}

 

The result will be like:

 

!image-2020-09-27-21-23-32-742.png!

 

Obviously, the result of `d0` is covered by `d1`!!! It does not have the timestamp 1000 at all."	IOTDB	Closed	3	1	5267	pull-request-available
13422698	Query result is not right while setting the page size to a low size	"change the iotdb-engine.properties:

```
max_number_of_points_in_page=2
enable_seq_space_compaction=false
enable_unseq_space_compaction=false
enable_cross_space_compaction=false
```

 

And then open a iotdb-cli, do the following commands:

```
SET STORAGE GROUP TO root.sg1;
create aligned timeseries root.sg1.d1(s1 FLOAT encoding=RLE, s2 INT32 encoding=Gorilla compression=SNAPPY, s3 INT64, s4 BOOLEAN, s5 TEXT);
create timeseries root.sg1.d2.s1 WITH DATATYPE=FLOAT, encoding=RLE;
create timeseries root.sg1.d2.s2 WITH DATATYPE=INT32, encoding=Gorilla;
create timeseries root.sg1.d2.s3 WITH DATATYPE=INT64;
create timeseries root.sg1.d2.s4 WITH DATATYPE=BOOLEAN;
create timeseries root.sg1.d2.s5 WITH DATATYPE=TEXT;
insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(1, 1.0, 1, 1, TRUE, 'aligned_test1');
insert into root.sg1.d1(time, s1, s2, s3, s5) aligned values(2, 2.0, 2, 2, 'aligned_test2');
insert into root.sg1.d1(time, s1, s3, s4, s5) aligned values(3, 3.0, 3, FALSE, 'aligned_test3');
insert into root.sg1.d1(time, s1, s2, s4, s5) aligned values(4, 4.0, 4, TRUE, 'aligned_test4');
insert into root.sg1.d1(time, s1, s2, s4, s5) aligned values(5, 5.0, 5, TRUE, 'aligned_test5');
insert into root.sg1.d1(time, s1, s2, s3, s4) aligned values(6, 6.0, 6, 6, TRUE);
insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(7, 7.0, 7, 7, FALSE, 'aligned_test7');
insert into root.sg1.d1(time, s1, s2, s3, s5) aligned values(8, 8.0, 8, 8, 'aligned_test8');
insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(9, 9.0, 9, 9, FALSE, 'aligned_test9');
insert into root.sg1.d1(time, s2, s3, s4, s5) aligned values(10, 10, 10, TRUE, 'aligned_test10');
insert into root.sg1.d2(time, s1, s2, s3, s4, s5) values(1, 1.0, 1, 1, TRUE, 'non_aligned_test1');
insert into root.sg1.d2(time, s1, s2, s3, s5) values(2, 2.0, 2, 2, 'non_aligned_test2');
insert into root.sg1.d2(time, s1, s3, s4, s5) values(3, 3.0, 3, FALSE, 'non_aligned_test3');
insert into root.sg1.d2(time, s1, s2, s4, s5) values(4, 4.0, 4, TRUE, 'non_aligned_test4');
insert into root.sg1.d2(time, s1, s2, s4, s5) values(5, 5.0, 5, TRUE, 'non_aligned_test5');
insert into root.sg1.d2(time, s1, s2, s3, s4) values(6, 6.0, 6, 6, TRUE);
insert into root.sg1.d2(time, s1, s2, s3, s4, s5) values(7, 7.0, 7, 7, FALSE, 'non_aligned_test7');
insert into root.sg1.d2(time, s1, s2, s3, s5) values(8, 8.0, 8, 8, 'non_aligned_test8');
insert into root.sg1.d2(time, s1, s2, s3, s4, s5) values(9, 9.0, 9, 9, FALSE, 'non_aligned_test9');
insert into root.sg1.d2(time, s2, s3, s4, s5) values(10, 10, 10, TRUE, 'non_aligned_test10');
flush;
insert into root.sg1.d1(time, s1, s3, s4, s5) aligned values(3, 30000.0, 30000, TRUE, 'aligned_unseq_test3');
insert into root.sg1.d1(time, s1, s2, s3) aligned values(11, 11.0, 11, 11);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(12, 12.0, 12, 12);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(13, 13.0, 13, 13)
insert into root.sg1.d1(time, s1, s2, s3) aligned values(14, 14.0, 14, 14);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(15, 15.0, 15, 15);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(16, 16.0, 16, 16);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(17, 17.0, 17, 17);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(18, 18.0, 18, 18);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(19, 19.0, 19, 19);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(20, 20.0, 20, 20);
insert into root.sg1.d2(time, s1, s2, s3) values(11, 11.0, 11, 11);
insert into root.sg1.d2(time, s1, s2, s3) values(12, 12.0, 12, 12);
insert into root.sg1.d2(time, s1, s2, s3) values(13, 13.0, 13, 13);
insert into root.sg1.d2(time, s1, s2, s3) values(14, 14.0, 14, 14);
insert into root.sg1.d2(time, s1, s2, s3) values(15, 15.0, 15, 15);
insert into root.sg1.d2(time, s1, s2, s3) values(16, 16.0, 16, 16);
insert into root.sg1.d2(time, s1, s2, s3) values(17, 17.0, 17, 17);
insert into root.sg1.d2(time, s1, s2, s3) values(18, 18.0, 18, 18);
insert into root.sg1.d2(time, s1, s2, s3) values(19, 19.0, 19, 19);
insert into root.sg1.d2(time, s1, s2, s3) values(20, 20.0, 20, 20);
flush;
insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(13, 130000.0, 130000, 130000, TRUE, 'aligned_unseq_test13');
insert into root.sg1.d1(time, s3, s4) aligned values(21, 21, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(22, 22, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(23, 23, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(24, 24, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(25, 25, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(26, 26, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(27, 27, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(28, 28, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(29, 29, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(30, 30, FALSE);
insert into root.sg1.d2(time, s3, s4) values(21, 21, TRUE);
insert into root.sg1.d2(time, s3, s4) values(22, 22, TRUE);
insert into root.sg1.d2(time, s3, s4) values(23, 23, TRUE);
insert into root.sg1.d2(time, s3, s4) values(24, 24, TRUE);
insert into root.sg1.d2(time, s3, s4) values(25, 25, TRUE);
insert into root.sg1.d2(time, s3, s4) values(26, 26, FALSE);
insert into root.sg1.d2(time, s3, s4) values(27, 27, FALSE);
insert into root.sg1.d2(time, s3, s4) values(28, 28, FALSE);
insert into root.sg1.d2(time, s3, s4) values(29, 29, FALSE);
insert into root.sg1.d2(time, s3, s4) values(30, 30, FALSE);
flush;
insert into root.sg1.d1(time, s1, s3, s4) aligned values(23, 230000.0, 230000, FALSE);
insert into root.sg1.d1(time, s2, s5) aligned values(31, 31, 'aligned_test31');
insert into root.sg1.d1(time, s2, s5) aligned values(32, 32, 'aligned_test32');
insert into root.sg1.d1(time, s2, s5) aligned values(33, 33, 'aligned_test33');
insert into root.sg1.d1(time, s2, s5) aligned values(34, 34, 'aligned_test34');
insert into root.sg1.d1(time, s2, s5) aligned values(35, 35, 'aligned_test35');
insert into root.sg1.d1(time, s2, s5) aligned values(36, 36, 'aligned_test36');
insert into root.sg1.d1(time, s2, s5) aligned values(37, 37, 'aligned_test37');
insert into root.sg1.d1(time, s2, s5) aligned values(38, 38, 'aligned_test38');
insert into root.sg1.d1(time, s2, s5) aligned values(39, 39, 'aligned_test39');
insert into root.sg1.d1(time, s2, s5) aligned values(40, 40, 'aligned_test40');
insert into root.sg1.d2(time, s2, s5) values(31, 31, 'non_aligned_test31');
insert into root.sg1.d2(time, s2, s5) values(32, 32, 'non_aligned_test32');
insert into root.sg1.d2(time, s2, s5) values(33, 33, 'non_aligned_test33');
insert into root.sg1.d2(time, s2, s5) values(34, 34, 'non_aligned_test34');
insert into root.sg1.d2(time, s2, s5) values(35, 35, 'non_aligned_test35');
insert into root.sg1.d2(time, s2, s5) values(36, 36, 'non_aligned_test36');
insert into root.sg1.d2(time, s2, s5) values(37, 37, 'non_aligned_test37');
insert into root.sg1.d2(time, s2, s5) values(38, 38, 'non_aligned_test38');
insert into root.sg1.d2(time, s2, s5) values(39, 39, 'non_aligned_test39');
insert into root.sg1.d2(time, s2, s5) values(40, 40, 'non_aligned_test40');
```

 

Then do the query:

 

```
select s1,s4 from root.sg1.d1 where s1 < 19 and s4 = false
```

 

The result will be like:

!image-2022-01-14-17-03-40-315.png!

However the correct result should be like:

!image-2022-01-14-17-06-39-183.png!

 "	IOTDB	Resolved	3	1	5267	pull-request-available
13422941	iotdb：The file handle is not released	"master 0114  302e9b9153ec1d2c39187151a425ffac9275f6b6
问题现象：
长测运行（读写混合，配置文件见附件） 45小时
iotdb Too many open files
lsof -p iotdb_pid大量tsfile文件句柄没释放。

 cat 1.out |grep ""0.tsfile""|wc -l
65213

2022-01-16 15:34:56,838 [pool-170-IoTDB-Query-1] ERROR o.a.i.d.q.d.RawQueryDataSetWithoutValueFilter:571 - exception happened in producer thread 
java.lang.NullPointerException: null
	at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:137)
	at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:117)
	at org.apache.iotdb.db.query.control.FileReaderManager.get(FileReaderManager.java:121)
	at org.apache.iotdb.db.engine.cache.BloomFilterCache.lambda$new$1(BloomFilterCache.java:73)
	at com.github.benmanes.caffeine.cache.LocalLoadingCache.lambda$newMappingFunction$2(LocalLoadingCache.java:141)
	at com.github.benmanes.caffeine.cache.LocalCache.lambda$statsAware$0(LocalCache.java:139)
	at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2405)
	at java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1853)
	at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2403)
	at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2386)
	at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
	at com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:54)
	at org.apache.iotdb.db.engine.cache.BloomFilterCache.get(BloomFilterCache.java:99)
	at org.apache.iotdb.db.engine.cache.TimeSeriesMetadataCache.get(TimeSeriesMetadataCache.java:177)
	at org.apache.iotdb.db.utils.FileLoaderUtils.loadTimeSeriesMetadata(FileLoaderUtils.java:107)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.loadTimeSeriesMetadata(SeriesReader.java:1103)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.unpackSeqTsFileResource(SeriesReader.java:1069)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.tryToUnpackAllOverlappedFilesToTimeSeriesMetadata(SeriesReader.java:993)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.hasNextFile(SeriesReader.java:263)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.hasNextChunk(SeriesReader.java:336)
	at org.apache.iotdb.db.query.reader.series.SeriesRawDataBatchReader.readChunkData(SeriesRawDataBatchReader.java:170)
	at org.apache.iotdb.db.query.reader.series.SeriesRawDataBatchReader.hasNextBatch(SeriesRawDataBatchReader.java:118)
	at org.apache.iotdb.db.query.dataset.RawQueryDataSetWithoutValueFilter$ReadTask.runMayThrow(RawQueryDataSetWithoutValueFilter.java:96)
	at org.apache.iotdb.db.concurrent.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2022-01-16 15:34:56,841 [pool-170-IoTDB-Query-8] ERROR o.a.i.t.f.f.LocalFSInputFactory:40 - Failed to get TsFile input of file: /data/iotdb_data/data/sequence/root.test.g_41/0/0/1642317771628-4999-0-0.tsfile,  
java.nio.file.FileSystemException: /data/iotdb_data/data/sequence/root.test.g_41/0/0/1642317771628-4999-0-0.tsfile: Too many open files
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
	at java.nio.channels.FileChannel.open(FileChannel.java:287)
	at java.nio.channels.FileChannel.open(FileChannel.java:335)
	at org.apache.iotdb.tsfile.read.reader.LocalTsFileInput.<init>(LocalTsFileInput.java:42)
	at org.apache.iotdb.tsfile.fileSystem.fileInputFactory.LocalFSInputFactory.getTsFileInput(LocalFSInputFactory.java:38)
	at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:131)
	at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:117)
	at org.apache.iotdb.db.query.control.FileReaderManager.get(FileReaderManager.java:121)
	at org.apache.iotdb.db.engine.cache.BloomFilterCache.lambda$new$1(BloomFilterCache.java:73)
	at com.github.benmanes.caffeine.cache.LocalLoadingCache.lambda$newMappingFunction$2(LocalLoadingCache.java:141)
	at com.github.benmanes.caffeine.cache.LocalCache.lambda$statsAware$0(LocalCache.java:139)
	at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2405)
	at java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1892)
	at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2403)
	at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2386)
	at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
	at com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:54)
	at org.apache.iotdb.db.engine.cache.BloomFilterCache.get(BloomFilterCache.java:99)
	at org.apache.iotdb.db.engine.cache.TimeSeriesMetadataCache.get(TimeSeriesMetadataCache.java:177)
	at org.apache.iotdb.db.utils.FileLoaderUtils.loadTimeSeriesMetadata(FileLoaderUtils.java:107)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.loadTimeSeriesMetadata(SeriesReader.java:1103)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.unpackSeqTsFileResource(SeriesReader.java:1069)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.tryToUnpackAllOverlappedFilesToTimeSeriesMetadata(SeriesReader.java:993)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.hasNextFile(SeriesReader.java:263)
	at org.apache.iotdb.db.query.reader.series.SeriesReader.hasNextChunk(SeriesReader.java:336)
	at org.apache.iotdb.db.query.reader.series.SeriesRawDataBatchReader.readChunkData(SeriesRawDataBatchReader.java:170)
	at org.apache.iotdb.db.query.reader.series.SeriesRawDataBatchReader.hasNextBatch(SeriesRawDataBatchReader.java:118)
	at org.apache.iotdb.tsfile.read.query.timegenerator.node.LeafNode.hasNext(LeafNode.java:51)
	at org.apache.iotdb.tsfile.read.query.timegenerator.TimeGenerator.hasNext(TimeGenerator.java:53)
	at org.apache.iotdb.db.query.executor.AggregationExecutor.aggregateWithValueFilter(AggregationExecutor.java:713)
	at org.apache.iotdb.db.query.executor.AggregationExecutor.executeWithValueFilter(AggregationExecutor.java:661)
	at org.apache.iotdb.db.query.executor.QueryRouter.aggregate(QueryRouter.java:152)
	at org.apache.iotdb.db.qp.executor.PlanExecutor.processDataQuery(PlanExecutor.java:617)
	at org.apache.iotdb.db.qp.executor.PlanExecutor.processQuery(PlanExecutor.java:254)
	at org.apache.iotdb.db.service.basic.ServiceProvider.createQueryDataSet(ServiceProvider.java:252)
	at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeQueryPlan(TSServiceImpl.java:751)
	at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.access$000(TSServiceImpl.java:160)
	at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl$QueryTask.call(TSServiceImpl.java:218)
	at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl$QueryTask.call(TSServiceImpl.java:162)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


"	IOTDB	Closed	3	1	5267	0.13.0, pull-request-available
13298207	Add MetaMarker.SEPARATOR offset in TsFileMetadata	When we ForceOpen a TsFile and want to truncate its all metadata, we could directly get the position of MetaMarder.SEPARATOR and do truncate. No need to read each TimeseriesMetadata one by one.	IOTDB	Closed	3	4	5267	pull-request-available
13447523	Index Out of bound while doing raw query	"SET STORAGE GROUP TO root.sg1;
create aligned timeseries root.sg1.d1(s1 FLOAT encoding=RLE, s2 INT32 encoding=Gorilla compression=SNAPPY, s3 INT64, s4 BOOLEAN, s5 TEXT);
create timeseries root.sg1.d2.s1 WITH DATATYPE=FLOAT, encoding=RLE;
create timeseries root.sg1.d2.s2 WITH DATATYPE=INT32, encoding=Gorilla;
create timeseries root.sg1.d2.s3 WITH DATATYPE=INT64;
create timeseries root.sg1.d2.s4 WITH DATATYPE=BOOLEAN;
create timeseries root.sg1.d2.s5 WITH DATATYPE=TEXT;
insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(1, 1.0, 1, 1, TRUE, 'aligned_test1');
insert into root.sg1.d1(time, s1, s2, s3, s5) aligned values(2, 2.0, 2, 2, 'aligned_test2');
insert into root.sg1.d1(time, s1, s3, s4, s5) aligned values(3, 3.0, 3, FALSE, 'aligned_test3');
insert into root.sg1.d1(time, s1, s2, s4, s5) aligned values(4, 4.0, 4, TRUE, 'aligned_test4');
insert into root.sg1.d1(time, s1, s2, s4, s5) aligned values(5, 5.0, 5, TRUE, 'aligned_test5');
insert into root.sg1.d1(time, s1, s2, s3, s4) aligned values(6, 6.0, 6, 6, TRUE);
insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(7, 7.0, 7, 7, FALSE, 'aligned_test7');
insert into root.sg1.d1(time, s1, s2, s3, s5) aligned values(8, 8.0, 8, 8, 'aligned_test8');
insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(9, 9.0, 9, 9, FALSE, 'aligned_test9');
insert into root.sg1.d1(time, s2, s3, s4, s5) aligned values(10, 10, 10, TRUE, 'aligned_test10');
insert into root.sg1.d2(time, s1, s2, s3, s4, s5) values(1, 1.0, 1, 1, TRUE, 'non_aligned_test1');
insert into root.sg1.d2(time, s1, s2, s3, s5) values(2, 2.0, 2, 2, 'non_aligned_test2');
insert into root.sg1.d2(time, s1, s3, s4, s5) values(3, 3.0, 3, FALSE, 'non_aligned_test3');
insert into root.sg1.d2(time, s1, s2, s4, s5) values(4, 4.0, 4, TRUE, 'non_aligned_test4');
insert into root.sg1.d2(time, s1, s2, s4, s5) values(5, 5.0, 5, TRUE, 'non_aligned_test5');
insert into root.sg1.d2(time, s1, s2, s3, s4) values(6, 6.0, 6, 6, TRUE);
insert into root.sg1.d2(time, s1, s2, s3, s4, s5) values(7, 7.0, 7, 7, FALSE, 'non_aligned_test7');
insert into root.sg1.d2(time, s1, s2, s3, s5) values(8, 8.0, 8, 8, 'non_aligned_test8');
insert into root.sg1.d2(time, s1, s2, s3, s4, s5) values(9, 9.0, 9, 9, FALSE, 'non_aligned_test9');
insert into root.sg1.d2(time, s2, s3, s4, s5) values(10, 10, 10, TRUE, 'non_aligned_test10');

restart


insert into root.sg1.d1(time, s1, s3, s4, s5) aligned values(3, 30000.0, 30000, TRUE, 'aligned_unseq_test3');
insert into root.sg1.d1(time, s1, s2, s3) aligned values(11, 11.0, 11, 11);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(12, 12.0, 12, 12);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(13, 13.0, 13, 13);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(14, 14.0, 14, 14);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(15, 15.0, 15, 15);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(16, 16.0, 16, 16);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(17, 17.0, 17, 17);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(18, 18.0, 18, 18);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(19, 19.0, 19, 19);
insert into root.sg1.d1(time, s1, s2, s3) aligned values(20, 20.0, 20, 20);
insert into root.sg1.d2(time, s1, s2, s3) values(11, 11.0, 11, 11);
insert into root.sg1.d2(time, s1, s2, s3) values(12, 12.0, 12, 12);
insert into root.sg1.d2(time, s1, s2, s3) values(13, 13.0, 13, 13);
insert into root.sg1.d2(time, s1, s2, s3) values(14, 14.0, 14, 14);
insert into root.sg1.d2(time, s1, s2, s3) values(15, 15.0, 15, 15);
insert into root.sg1.d2(time, s1, s2, s3) values(16, 16.0, 16, 16);
insert into root.sg1.d2(time, s1, s2, s3) values(17, 17.0, 17, 17);
insert into root.sg1.d2(time, s1, s2, s3) values(18, 18.0, 18, 18);
insert into root.sg1.d2(time, s1, s2, s3) values(19, 19.0, 19, 19);
insert into root.sg1.d2(time, s1, s2, s3) values(20, 20.0, 20, 20);

restart

insert into root.sg1.d1(time, s1, s2, s3, s4, s5) aligned values(13, 130000.0, 130000, 130000, TRUE, 'aligned_unseq_test13');
insert into root.sg1.d1(time, s3, s4) aligned values(21, 21, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(22, 22, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(23, 23, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(24, 24, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(25, 25, TRUE);
insert into root.sg1.d1(time, s3, s4) aligned values(26, 26, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(27, 27, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(28, 28, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(29, 29, FALSE);
insert into root.sg1.d1(time, s3, s4) aligned values(30, 30, FALSE);
insert into root.sg1.d2(time, s3, s4) values(21, 21, TRUE);
insert into root.sg1.d2(time, s3, s4) values(22, 22, TRUE);
insert into root.sg1.d2(time, s3, s4) values(23, 23, TRUE);
insert into root.sg1.d2(time, s3, s4) values(24, 24, TRUE);
insert into root.sg1.d2(time, s3, s4) values(25, 25, TRUE);
insert into root.sg1.d2(time, s3, s4) values(26, 26, FALSE);
insert into root.sg1.d2(time, s3, s4) values(27, 27, FALSE);
insert into root.sg1.d2(time, s3, s4) values(28, 28, FALSE);
insert into root.sg1.d2(time, s3, s4) values(29, 29, FALSE);
insert into root.sg1.d2(time, s3, s4) values(30, 30, FALSE);

restart

insert into root.sg1.d1(time, s1, s3, s4) aligned values(23, 230000.0, 230000, FALSE);
insert into root.sg1.d1(time, s2, s5) aligned values(31, 31, 'aligned_test31');
insert into root.sg1.d1(time, s2, s5) aligned values(32, 32, 'aligned_test32');
insert into root.sg1.d1(time, s2, s5) aligned values(33, 33, 'aligned_test33');
insert into root.sg1.d1(time, s2, s5) aligned values(34, 34, 'aligned_test34');
insert into root.sg1.d1(time, s2, s5) aligned values(35, 35, 'aligned_test35');
insert into root.sg1.d1(time, s2, s5) aligned values(36, 36, 'aligned_test36');
insert into root.sg1.d1(time, s2, s5) aligned values(37, 37, 'aligned_test37');
insert into root.sg1.d1(time, s2, s5) aligned values(38, 38, 'aligned_test38');
insert into root.sg1.d1(time, s2, s5) aligned values(39, 39, 'aligned_test39');
insert into root.sg1.d1(time, s2, s5) aligned values(40, 40, 'aligned_test40');
insert into root.sg1.d2(time, s2, s5) values(31, 31, 'non_aligned_test31');
insert into root.sg1.d2(time, s2, s5) values(32, 32, 'non_aligned_test32');
insert into root.sg1.d2(time, s2, s5) values(33, 33, 'non_aligned_test33');
insert into root.sg1.d2(time, s2, s5) values(34, 34, 'non_aligned_test34');
insert into root.sg1.d2(time, s2, s5) values(35, 35, 'non_aligned_test35');
insert into root.sg1.d2(time, s2, s5) values(36, 36, 'non_aligned_test36');
insert into root.sg1.d2(time, s2, s5) values(37, 37, 'non_aligned_test37');
insert into root.sg1.d2(time, s2, s5) values(38, 38, 'non_aligned_test38');
insert into root.sg1.d2(time, s2, s5) values(39, 39, 'non_aligned_test39');
insert into root.sg1.d2(time, s2, s5) values(40, 40, 'non_aligned_test40');

restart

select * from root.sg1.*"	IOTDB	Closed	3	1	5267	pull-request-available
13553321	Call fsync after writing file	"Currently, lots of IO operations in IoTDB doesn't call fsync after doing writing which may cause file content lost if users restarting computer.

More details can be seen in https://apache-iotdb.feishu.cn/docx/L6XNd0HBbo57xFx1DQ0cjkNenhg"	IOTDB	Open	3	1	5267	pull-request-available
13501836	[Trigger] Update content about trigger in UserGuide	In 0.14.0 version, the trigger permission function is not currently implemented, and it is planned to be optimized in a later version, so it is necessary to update the relevant content in the relevant version UserGuide, prompting the user that the current version does not support permission management, and the permission list needs Remove trigger-related permissions from the list.	IOTDB	Closed	4	4	5267	pull-request-available
13427911	change max_deduplicated_path_num does not take effect	"If we change meta_data_cache_enable=false and change max_deduplicated_path_num in iotdb-engine.properties meanwhile, max_deduplicated_path_num won't take effect.

 

Change the following two items in iotdb-engine.properties

```

meta_data_cache_enable=false

max_deduplicated_path_num=2

```

And then start iotdb-server.

And then execute the following commands

```

insert into root.sg.d1(time,s1) values(1,1);

insert into root.sg.d1(time,s2) values(1,1);

insert into root.sg.d1(time,s3) values(1,1);

select last s1, s2, s3 from root.sg.d1;

```

We expect to get `PathNumOverLimitException: Too many paths in one query!` exeception, because we already change max_deduplicated_path_num to 2, but it can be executed successfully."	IOTDB	Closed	3	1	5267	pull-request-available
13433575	[JDBC] ResultSetMetaData.getColumnTypeName  ，the result is incorrect	"问题描述：
iotdb jdbc ResultSetMetaData.getColumnTypeName  返回的结果不正确。

测试用例：
下面例子中创建的时间序列类型是text，获取到的列类型是INT64.

package jdbcPro;

import org.apache.iotdb.jdbc.IoTDBSQLException;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.ResultSetMetaData;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.Properties;

public class JDBCTest extends Thread{
	public static Connection connection = null;
	public static Statement statement = null;
	ResultSet rs = null;
	public void run()
	{
		 try {
			Class.forName(""org.apache.iotdb.jdbc.IoTDBDriver"");
		} catch (ClassNotFoundException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}
		    try{ 
//		    	connection = DriverManager.getConnection(""jdbc:iotdb://172.20.70.4:6667/"", ""root"", ""root"");
		    	Properties p = new Properties();
		        p.setProperty(""user"", ""root"");
		        p.setProperty(""password"", ""root"");
		    	connection = DriverManager.getConnection(""jdbc:iotdb://172.20.70.31:6667/"", p);
		    	statement = connection.createStatement() ;


	      // set JDBC fetchSize
	      statement.setFetchSize(10000);

	      try {

	    	  statement.execute(""delete storage group root.**"");
	    	  statement.execute(""create timeseries root.sg.dev.status with datatype={color:#DE350B}*text*{color},encoding=PLAIN ;"");
	    	  statement.execute(""insert into root.sg.dev(time,status) values(1,3.14);"");
	       rs = statement.executeQuery(""select status from root.sg.dev"");
	       ResultSetMetaData rsmd = rs.getMetaData();
	       System.out.println(""列数:""+rsmd.getColumnCount());
	       System.out.println(""列名:""+rsmd.getColumnName(1));
	       System.out.println(""列类型号:""+rsmd.getColumnType(1));
	       System.out.println(""列类型名:""+rsmd.getColumnTypeName(1));
	       System.out.println(""列名:""+rsmd.getColumnName(2));
	       System.out.println(""列类型号:""+rsmd.getColumnType(2));
	    {color:#DE350B}   System.out.println(""列类型名:""+rsmd.getColumnTypeName(2));{color}
	        if(rs.next()){

	        	System.out.print(rs.getString(1));
	        	System.out.println("",""+rs.getString(2));
	        
	        }

	      } catch (IoTDBSQLException e) {
	        System.out.println(e.getMessage());
	        statement.close();
	      }

	     
	    } catch (SQLException e) {
	      System.out.println(e.getMessage());
	    }finally{
	    	try {
				close();
			} catch (SQLException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
	    }
	  }
      public void close() throws SQLException, InterruptedException{
    	 
    	  if(statement != null)
    	     statement.close();
    	  if(connection != null)
    	     connection.close();
//    	  System.out.println(""finished."");
      }

  public static void main(String[] args) throws ClassNotFoundException, SQLException, InterruptedException {
    
	  JDBCTest t1 = new JDBCTest();
	  t1.start();

  }
  }
"	IOTDB	Closed	2	1	5267	pull-request-available
13578818	Random choosing available nodes to send sql requests	Now even if we construct session with only one node url, session will automatically fetch all available node urls in the cluster. However, when we use `executeQueryStatement`, we can only send the sql query to the first node url ( we can see that in the metrics panel that only one DN has query OPS)	IOTDB	Closed	3	4	5267	pull-request-available
13554700	Change schema template to device template	Change all schema template to device template, to be compatible forwards, we need to keep the schema template, but in docs we will all use device template to replace schema template.	IOTDB	Open	3	3	5267	pull-request-available
13427740	Move some contents in `Show Timeseries` subSection to `Tag and Attribute Management` subSection	We need move `SHOW TIMESERIES (<{{{}PathPattern{}}}>)? WhereClause` contents in `Show Timeseries` subSection to `Tag and Attribute Management` subSection, because the tag info is never explained until there.	IOTDB	Closed	4	4	5267	pull-request-available
13572188	Print correct error msg while using wrong password in session	"If we write a wrong password in Session, we now get an error msg like the following:
!image-2024-03-18-14-17-11-463.png!

It never tells us that we write a wrong password."	IOTDB	Closed	3	4	5267	pull-request-available
13483600	NPE in SourceHandle	While upstream FI failed, stateTracker may detect its state and then abort the SourceHandle in downstream, however meanwhile there may still exist GetTsBlock request in thread pool, it will fail because SinkHandle in upstream is already released. Then the GetTsBlock request will rertry 3 times and when it reach max retry times, it will try to release memory that reserved by itself, but this part of memory has already been released while this SourceHandle being aborted before.	IOTDB	Closed	3	1	5267	pull-request-available
13303825	Move the same construct dataset logic to service-rpc	There are duplicated code in both JDBC module and session module about constructing dataset. We can put it into service-roc module.	IOTDB	Closed	3	4	5267	pull-request-available
13506170	[udf]create udf with bad uri failed with 305 null: NPE	"[udf]create udf with bad uri failed with 305 null: NPE

environment:
3C3D cluster , Nov.28th master 

expect:
1. not NPE
2. not error log 

reproduction:
{code:sql}
create function my_compare2 as ""com.timecho.udf.upload.TestObjectRow"" using uri ""/data/nginx/upload-udf-test-0.14-SNAPSHOT.jar""
{code}

datanode log:
{code:java}
2022-11-28 11:44:06,666 [pool-4-IoTDB-timedQuerySqlCount-1] INFO  o.a.i.d.s.b.QueryFrequencyRecorder:44 - Query count in current 1 minute 1  
2022-11-28 11:44:08,711 [pool-26-IoTDB-ClientRPC-Processor-4$20221128_034408_00069_3] ERROR o.a.i.d.m.p.e.c.ConfigExecution:132 - Failures happened during running ConfigExecution. 
java.lang.NullPointerException: null
	at org.apache.iotdb.db.mpp.plan.execution.config.executor.ClusterConfigTaskExecutor.createFunction(ClusterConfigTaskExecutor.java:310)
	at org.apache.iotdb.db.mpp.plan.execution.config.metadata.CreateFunctionTask.execute(CreateFunctionTask.java:40)
	at org.apache.iotdb.db.mpp.plan.execution.config.ConfigExecution.start(ConfigExecution.java:107)
	at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:150)
	at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementInternal(ClientRPCServiceImpl.java:215)
	at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementV2(ClientRPCServiceImpl.java:390)
	at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeStatementV2.getResult(IClientRPCService.java:3453)
	at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeStatementV2.getResult(IClientRPCService.java:3433)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
	at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}"	IOTDB	Closed	3	1	5267	pull-request-available
13520014	Concurrent bug caused by using synchronizedSet() in DriverScheduler	"synchronizedSet() return a faked thread-safe Set, because its iterator method is not thread-safe.

If we use for-each syntax for that Set, it may cause ConcurrentModificationException.

!image-2023-01-18-17-43-53-454.png!

 

!image-2023-01-18-17-44-16-595.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13491761	Support fetching all connection info in session api	We want a new api in Session & SessionPool to fetch information of all the connections.	IOTDB	Closed	3	2	5267	pull-request-available
13477871	LocalSourceHandle Throw NPE while calling receive	!image-2022-08-22-10-21-59-854.png!	IOTDB	Closed	3	1	5267	pull-request-available
13571692	Optimize for query resource init	"we use timefilter to filter useless timepartition instead of getting all of them and then filter tsfile in them one by one.

We insert `0~9999` points whose timestamp is also `0~9999` and execute `flush` after each point insert and then insert one point with timestamp now(). As so, we will get two partitions, one contains 10,000 tsfiles, another one contains only one tsfile.

execute the following sql: (only the last tsfile satisfies)
```
select s1 from root.db.d1 where time > now() - 1h;
```

before opt, the time for query init tsfile list are:(about 2.5ms)

```
3057000
2356292
3258834
2415417
2876041
2395750
2564833
```

after opt, the time for query init tsfile list are:(about 0.047 ms)
```
47500
39792
33875
31958
37584
56666
45709
```

 "	IOTDB	Closed	3	4	5267	pull-request-available
13300801	IoTDB support alter one time series's tag/attribute property	"Now, we can only add tag/attribute for one timeseries while creating it. After that, we cannot update it.

So, we should support the alter timeseries syntax"	IOTDB	Closed	3	2	5267	pull-request-available
13307423	Error message is not intuitive while inserting when the load is too heavy	"While inserting when the load is too heavy, the insert operation is denied and the auto created mnode should also be deleted too. However, the deletion failed because it can not get the write lock of the node.

In the client, you will get the deletion failed exception instead of load too heavy exception which is not intuitive"	IOTDB	Closed	3	4	5267	pull-request-available
13477150	result error in group by 	"INSERT INTO root.sgcc.wf01.wt01(timestamp, ts0, ts1, ts2, ts3, ts4, ts5) VALUES (2022-05-27T23:50:00.000+08:00, true, 5, 23, 15.88, 8888.88, ""IOTDB!IOTDB12345"");
INSERT INTO root.sgcc.wf01.wt01(timestamp, ts0, ts1, ts2, ts3, ts4, ts5) VALUES (2022-05-26T23:50:00.000+08:00, 1, 11, 8, 88.88, 8888.88, ""IOTDB!@#B12345"");

 

select count(ts0),max_value(ts1)from root.sgcc.wf01.wt01 group by ((2022-05-26T23:50:00.000+08:00, 2022-05-27T23:50:00.000+08:00],4h)

 

!image-2022-08-17-10-52-44-389.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13468778	NPE while calling QueryExecution.getBatchResult	!image-2022-06-28-11-49-20-288.png!	IOTDB	Closed	3	1	5267	pull-request-available
13582862	Optimize the time slice control of SeriesScanOperator and AlignedSeriesScanOperator	"Previously, we will scan until we get at least one satisfied row and then we have chance to do time slice control.
However, when we do the predicate push down optimization, we may need much more time to get a satisfied row from tsfile, so we need to change the control processing of SeriesScanOperator and AlignedSeriesScanOperator.

We will do time check after we got one page data from SeriesScanUtil, even if it's an empty TsBlock.

more details can be seen in [https://timechor.feishu.cn/docx/IMHxdHhrRoPg88x69wTc4RqynFf]"	IOTDB	Closed	3	4	5267	pull-request-available
13549869	Optimize the large time range raw query performance	"Use the iot-write.properties in attachment to generate data.

Then use the  iot-query.properties in attachment to test query performance, you can get the following result:

!image-2023-09-07-14-56-02-013.png!

After profiling the DataNode, we found that much time wasting in Array.growCapacity which can be totally avoided, because we can estimate the TsBlock size using page statistics.

!image-2023-09-07-14-51-04-689.png!"	IOTDB	Open	3	4	5267	pull-request-available
13505873	[privilege] show triggers expect 803 rather than 301 when there is no enough privilege	"actual:
show trigger and show functions require privilege of root.**
 !image-2022-11-25-15-22-00-669.png! 
 !image-2022-11-25-15-23-28-273.png!

expect:
Suppose 2 company share 1 IoTDB service and they own their own databse,  now grant them privileges on root.sg1.** or root.sg2.** and they own different triggers and UDF. When using show triggers or show functions, they would like to get different result what the actual triggers or UDFs are rather than an error with privilage.
"	IOTDB	Closed	3	4	5267	pull-request-available
13499676	Memory leak in query processing	"In read and write mixed load, after about 90 mins, the query will be much slower than before and the memory keeps highing and gc will keep active.

After dumping the memory heap, I found that instanceExecution Map in FragmentInstanceManager is too large. Previously, it can be removed by cancel rpc call from Coordinator, but after optimizing, Coordinator won't call cancel rpc if the FI is already in done state. Unfortunately, when FI's state changing to done, it doesn't remove itself from instanceExecution Map in FragmentInstanceManager which causes this memory leak

!image-2022-11-08-20-13-21-385.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13434778	[Query] java.io.IOException: overlapped data should be consumed first	"iotdb版本 0.13  rc1

问题描述：
    关合并0.12/0.13.0 rc1 生成的数据，有乱序。拿到0.12.5 rc1 和0.13 rc1 上分别执行查询（查询的时候，iotdb也都关合并），0.12.5 rc1 查询正确。
    0.13 rc1 查询报错，报错信息为：
2022-03-21 09:17:57,026 [pool-13-IoTDB-RPC-Client-1] {color:#DE350B}*WARN  *{color}o.a.i.d.u.ErrorHandlingUtils:61 - Status code: INTERNAL_SERVER_ERROR(500), operation: ""SELECT max_time(s_213) FROM root.test.g_6.d_26 WHERE time >= 1537372800000 AND time <= 1537422800000"". executeStatement failed
java.util.concurrent.ExecutionException: java.io.IOException: overlapped data should be consumed first
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.submitQueryTask(TSServiceImpl.java:785)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeStatement(TSServiceImpl.java:619)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeStatement.getResult(TSIService.java:2853)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeStatement.getResult(TSIService.java:2833)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: overlapped data should be consumed first
        at org.apache.iotdb.db.query.reader.series.SeriesReader.isPageOverlapped(SeriesReader.java:657)
        at org.apache.iotdb.db.query.reader.series.SeriesAggregateReader.canUseCurrentPageStatistics(SeriesAggregateReader.java:148)
        at org.apache.iotdb.db.query.executor.AggregationExecutor.aggregatePages(AggregationExecutor.java:528)
        at org.apache.iotdb.db.query.executor.AggregationExecutor.aggregateFromReader(AggregationExecutor.java:419)
        at org.apache.iotdb.db.query.executor.AggregationExecutor.aggregateOneSeries(AggregationExecutor.java:318)
        at org.apache.iotdb.db.query.executor.AggregationExecutor.aggregateOneSeries(AggregationExecutor.java:195)
        at org.apache.iotdb.db.query.executor.AggregationExecutor.executeWithoutValueFilter(AggregationExecutor.java:149)
        at org.apache.iotdb.db.query.executor.QueryRouter.aggregate(QueryRouter.java:123)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.processDataQuery(PlanExecutor.java:630)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.processQuery(PlanExecutor.java:258)
        at org.apache.iotdb.db.service.basic.ServiceProvider.createQueryDataSet(ServiceProvider.java:264)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeQueryPlan(TSServiceImpl.java:821)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.access$000(TSServiceImpl.java:163)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl$QueryTask.call(TSServiceImpl.java:221)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl$QueryTask.call(TSServiceImpl.java:165)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        ... 3 common frames omitted

0.12.5 rc1 执行结果是
 !image-2022-03-21-09-52-02-258.png! 

问题2，这个日志，应该是error级别，不应该是warn级别。

测试环境：
192.168.10.68
iotdb路径
/data/liuzhen_test/weekly_0127/13_rc1
数据备份在/data/benchmark/weekly_shell/rel_13_rc1_data_bk/data_1_luanxu
查询语句：SELECT max_time(s_212) FROM root.test.g_2.d_12 WHERE time >= 1537372800000 AND time <= 1537422800000"	IOTDB	Closed	3	1	5267	pull-request-available
13472025	Q8 in 0.14 new standalone is much slower than 0.13	"In R2(R1 should already cache all last values in last cache), Q8 in 0.14 new standalone is much slower than 0.13,  it seems that 0.14 didn't use last cache.

After debugging, I found that even though I call the DataNodeSchemaCache.updateLastCache() in UpdateLastCacheOperator after first query from disk, DataNodeSchemaCache doesn't contain the corresponding key entry, because there is no more write operation.(BTW, even though, there are more write operation coming, DataNodeSchemaCache still won't contain the corresponding key entry in standalone mode becasue StandAloneSchemaFetcher will never use DataNodeSchemaCache)."	IOTDB	Closed	3	1	5267	pull-request-available
13516607	Forget to init lastEnterReadyQueueTime of DriverTask while creating it	"When we create a new DriverTask, we forget to init its lastEnterReadyQueueTime, and directly push it into ready queue.

This will cause ready queued time cost became abnormal like the following:

!image-2023-01-05-16-04-34-503.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13505674	DeadLock casued by memory pool	"There exists a deadlock case in the current MemoryPool implementation.

 
{panel}
Found one Java-level deadlock: ============================= ""pool-25-IoTDB-ClientRPC-Processor-50$20221123_152410_87128_5.0.0.0"": waiting to lock monitor 0x00007ff16c00ec78 (object 0x00000005cbe177d8, a org.apache.iotdb.db.mpp.execution.memory.MemoryPool), which is held by ""pool-25-IoTDB-ClientRPC-Processor-43$20221123_152410_87142_5.0.0.0"" ""pool-25-IoTDB-ClientRPC-Processor-43$20221123_152410_87142_5.0.0.0"": waiting to lock monitor 0x00007ff16c010278 (object 0x00000005dc0ba760, a org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue), which is held by ""pool-25-IoTDB-ClientRPC-Processor-22$20221123_152410_87114_5.0.0.0"" ""pool-25-IoTDB-ClientRPC-Processor-22$20221123_152410_87114_5.0.0.0"": waiting to lock monitor 0x00007ff16c00ec78 (object 0x00000005cbe177d8, a org.apache.iotdb.db.mpp.execution.memory.MemoryPool), which is held by ""pool-25-IoTDB-ClientRPC-Processor-43$20221123_152410_87142_5.0.0.0"" Java stack information for the threads listed above: =================================================== ""pool-25-IoTDB-ClientRPC-Processor-50$20221123_152410_87128_5.0.0.0"": at org.apache.iotdb.db.mpp.execution.memory.MemoryPool.free(MemoryPool.java:184) - waiting to lock <0x00000005cbe177d8> (a org.apache.iotdb.db.mpp.execution.memory.MemoryPool) at org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue.remove(SharedTsBlockQueue.java:129) at org.apache.iotdb.db.mpp.execution.exchange.LocalSourceHandle.receive(LocalSourceHandle.java:99) - locked <0x00000005d2b97308> (a org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue) at org.apache.iotdb.db.mpp.execution.exchange.LocalSourceHandle.getSerializedTsBlock(LocalSourceHandle.java:115) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getSerializedTsBlock(QueryExecution.java:452) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution$$Lambda$1005/498612315.get(Unknown Source) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getResult(QueryExecution.java:404) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getByteBufferBatchResult(QueryExecution.java:448) at org.apache.iotdb.db.utils.QueryDataSetUtils.convertQueryResultByFetchSize(QueryDataSetUtils.java:387) at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.fetchResultsV2(ClientRPCServiceImpl.java:423) at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3528) at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3508) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ""pool-25-IoTDB-ClientRPC-Processor-43$20221123_152410_87142_5.0.0.0"": at org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue.lambda$add$0(SharedTsBlockQueue.java:161) - waiting to lock <0x00000005dc0ba760> (a org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue) at org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue$$Lambda$1068/1953903521.run(Unknown Source) at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31) at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1270) at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038) at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:783) at org.apache.iotdb.db.mpp.execution.memory.MemoryPool$MemoryReservationFuture.set(MemoryPool.java:64) at org.apache.iotdb.db.mpp.execution.memory.MemoryPool.free(MemoryPool.java:216) - locked <0x00000005cbe177d8> (a org.apache.iotdb.db.mpp.execution.memory.MemoryPool) at org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue.remove(SharedTsBlockQueue.java:129) at org.apache.iotdb.db.mpp.execution.exchange.LocalSourceHandle.receive(LocalSourceHandle.java:99) - locked <0x00000005d2c187f8> (a org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue) at org.apache.iotdb.db.mpp.execution.exchange.LocalSourceHandle.getSerializedTsBlock(LocalSourceHandle.java:115) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getSerializedTsBlock(QueryExecution.java:452) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution$$Lambda$1005/498612315.get(Unknown Source) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getResult(QueryExecution.java:404) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getByteBufferBatchResult(QueryExecution.java:448) at org.apache.iotdb.db.utils.QueryDataSetUtils.convertQueryResultByFetchSize(QueryDataSetUtils.java:387) at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.fetchResultsV2(ClientRPCServiceImpl.java:423) at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3528) at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3508) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ""pool-25-IoTDB-ClientRPC-Processor-22$20221123_152410_87114_5.0.0.0"": at org.apache.iotdb.db.mpp.execution.memory.MemoryPool.free(MemoryPool.java:184) - waiting to lock <0x00000005cbe177d8> (a org.apache.iotdb.db.mpp.execution.memory.MemoryPool) at org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue.remove(SharedTsBlockQueue.java:129) at org.apache.iotdb.db.mpp.execution.exchange.LocalSourceHandle.receive(LocalSourceHandle.java:99) - locked <0x00000005dc0ba760> (a org.apache.iotdb.db.mpp.execution.exchange.SharedTsBlockQueue) at org.apache.iotdb.db.mpp.execution.exchange.LocalSourceHandle.getSerializedTsBlock(LocalSourceHandle.java:115) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getSerializedTsBlock(QueryExecution.java:452) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution$$Lambda$1005/498612315.get(Unknown Source) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getResult(QueryExecution.java:404) at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getByteBufferBatchResult(QueryExecution.java:448) at org.apache.iotdb.db.utils.QueryDataSetUtils.convertQueryResultByFetchSize(QueryDataSetUtils.java:387) at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.fetchResultsV2(ClientRPCServiceImpl.java:423) at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3528) at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$fetchResultsV2.getResult(IClientRPCService.java:3508) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock.
{panel}
 
Thread-A: LocalSourceHandle.receive() -> A-SharedTsBlockQueue.remove() -> MemoryPool.free() (hold MemoryPool's lock) -> future.set(null) -> try to get B-SharedTsBlockQueue's lock

Thread-B: LocalSourceHandle.receive() -> B-SharedTsBlockQueue.remove() (hold B-SharedTsBlockQueue's lock) -> try to get MemoryPool's lock"	IOTDB	Closed	3	1	5267	pull-request-available
13298243	Use value in timeGenerator instead of constructing another reader	"In IoTDB server, while we are doing a RawDataQueryWithValueFilter, like

`select s0 from root.ln.d0 where root.ln.d0.s0 > 5`

We will firstly construct a timeGenerator to get next time that satisfy the filter and then construct another `IReaderByTimestamp` to read by timestamp for each time series.

However, in this case, the value of s0 has been in the timeGenerator, so we don't need to query again."	IOTDB	Closed	3	4	5267	pull-request-available
13310301	Throw an exception while select count(*)	"h1. select count(*) from root align by device
h1. Msg: 500: java.lang.Float cannot be cast to java.lang.Long

 "	IOTDB	Closed	3	1	5267	pull-request-available
13447848	Internal error processing fetchFragmentInstanceState	"master_0530_95884ad
3副本3节点，benchmark先创建好元数据，再启动benchmark写入数据，
1小时后，node3 持续抛异常：
2022-05-31 18:31:41,843 [pool-1-IoTDB-InternalServiceRPC-Client-32] ERROR o.a.t.ProcessFunction:47 -{color:red}* Internal error processing fetchFragmentInstanceState
java.lang.NullPointerException: null*{color}
        at org.apache.iotdb.db.service.thrift.impl.InternalServiceImpl.fetchFragmentInstanceState(InternalServiceImpl.java:177)
        at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$fetchFragmentInstanceState.getResult(InternalService.java:1220)
        at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$fetchFragmentInstanceState.getResult(InternalService.java:1200)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2022-05-31 18:31:41,845 [pool-6-IoTDB-MPPCoordinatorScheduled-1] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:65 - level-0 Exception class org.apache.thrift.TApplicationException, message Internal error processing fetchFragmentInstanceState
2022-05-31 18:31:41,853 [pool-6-IoTDB-MPPCoordinatorScheduled-1] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:80 - root cause message Internal error processing fetchFragmentInstanceState, LocalizedMessage Internal error processing fetchFragmentInstanceState,
org.apache.thrift.TApplicationException: Internal error processing fetchFragmentInstanceState
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$4ea7c99a.CGLIB$receiveBase$60(<generated>)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$4ea7c99a$$FastClassByCGLIB$$47a8a47d.invoke(<generated>)
        at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
        at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$4ea7c99a.receiveBase(<generated>)
        at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.recv_fetchFragmentInstanceState(InternalService.java:228)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$4ea7c99a.CGLIB$recv_fetchFragmentInstanceState$10(<generated>)


17.5小时后，node3 OOM：
2022-06-01 11:00:48,432 [pool-1-IoTDB-InternalServiceRPC-Client-1340] ERROR o.a.t.ProcessFunction:47 - Internal error processing sendFragmentInstance
java.lang.RuntimeException: cannot fetch schema, status is: 411, msg is: {color:red}*java.lang.OutOfMemoryError: Java heap space*{color}
        at org.apache.iotdb.db.mpp.plan.analyze.ClusterSchemaFetcher.executeSchemaFetchQuery(ClusterSchemaFetcher.java:105)
        at org.apache.iotdb.db.mpp.plan.analyze.ClusterSchemaFetcher.fetchSchema(ClusterSchemaFetcher.java:92)
        at org.apache.iotdb.db.mpp.plan.analyze.ClusterSchemaFetcher.fetchSchemaWithAutoCreate(ClusterSchemaFetcher.java:154)
        at org.apache.iotdb.db.mpp.plan.analyze.SchemaValidator.validate(SchemaValidator.java:52)
        at org.apache.iotdb.db.service.thrift.impl.InternalServiceImpl.sendFragmentInstance(InternalServiceImpl.java:151)
        at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$sendFragmentInstance.getResult(InternalService.java:1195)
        at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$sendFragmentInstance.getResult(InternalService.java:1175)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

测试流程：
1. 192.168.130.3/4/5   16C32G
记为node1，node2，node3
2. benchmark创建元数据
配置文件见附件，运行2次benchmark创建元数据的操作，因为第1次创建有异常，部分失败。
3. benchmark连node1 执行写入

2022-05-31 17:25:39,199 benchmark开始运行。
{color:red}*node3：2022-05-31 18:31:41,843*{color} [pool-1-IoTDB-InternalServiceRPC-Client-32] ERROR o.a.t.ProcessFunction:47 - Internal error processing fetchFragmentInstanceState
java.lang.NullPointerException: null
{color:red}*node2：2022-06-01 09:54:03,432*{color} [pool-1-IoTDB-InternalServiceRPC-Client-40] ERROR o.a.t.ProcessFunction:47 - Internal error processing fetchFragmentInstanceState
java.lang.NullPointerException: null

{color:red}*node3：2022-06-01 11:00:48,432*{color} [pool-1-IoTDB-InternalServiceRPC-Client-1340] ERROR o.a.t.ProcessFunction:47 - Internal error processing sendFragmentInstance
java.lang.RuntimeException: cannot fetch schema, status is: 411, msg is: java.lang.OutOfMemoryError: Java heap space
"	IOTDB	Closed	3	1	5267	pull-request-available
13447429	Meet IndexOutOfBoundsException when executing linear fill in group by query	!image-2022-05-30-10-20-50-171.png|width=924,height=264!	IOTDB	Closed	3	1	5267	pull-request-available
13443969	Calculating allSensors fields in LocalExecutionPlanner	We should calculating allSensors fields in LocalExecutionPlanner instead of calculating them in Coordinator. In previous way, we may need to serialize and deserialize these sensors' name multiple times for each ScanNode and transfer them in network.	IOTDB	Closed	3	3	5267	pull-request-available
13554680	Session automatically fetch all available DataNodes	To let user session still be alive while some nodes shut down in cluster, we need to save all available datanodes' urls in Session, if one failed to connect, we can try next node.	IOTDB	Open	3	4	5267	pull-request-available
13573505	Concurrent bug between load and compaction	"The case is that we may get a tsfile resource without tsfile in disk which is caused by concurrent bug between compaction and load.

More details about the lock analysis can be seen in https://apache-iotdb.feishu.cn/docx/RrBrdXHPgoJTcxxoPxMc8eNXndd"	IOTDB	Open	3	1	5267	pull-request-available
13406311	Execute time range query : Msg: 500: [INTERNAL_SERVER_ERROR] 	"rel/0.12  24cc07dffdaf79fda8290c24e8329e20a53015df

Exception message:
Msg: 500: [INTERNAL_SERVER_ERROR] Exception occurred while executing ""{color:#DE350B}SELECT s_0 FROM root.test.g_15.d_2890 WHERE time >= 1537377860000 AND time <= 1537377865000{color}"". null

Reproduce steps :
1. iotdb-engine.properties
compaction_strategy=NO_COMPACTION
enable_unseq_compaction=false
enable_last_cache=false
enable_wal=false
max_deduplicated_path_num=10000
query_timeout_threshold=60000000

2. Start iotdb

3. Run benchmark （See attachment config.properties for configuration parameters）
cd ${benchmark_dir}
// First test  第1轮测试（读写混合/乱序）
./benchmark.sh > 1013_1.out
sleep 2s
// 删除所有存储组
${iotdb_dir}/sbin/start-cli.sh -e ""delete storage group root.*""
sleep 2s
// show.out 结果显示存储组全部删除成功
${iotdb_dir}/sbin/start-cli.sh -e ""show storage group"" > show.out
sleep 2s
mv data data_1

// Second test 第2轮测试（读写混合/乱序），bm配置同第1轮测试
./benchmark.sh > 1013_2.out

{color:#DE350B}*第2轮测试，查询报错*{color}(1013_2.out )： 
2021-10-13 15:54:25,978 ERROR cn.edu.tsinghua.iotdb.benchmark.iotdb012.IoTDB:608 - exception occurred when execute query=SELECT s_0 FROM root.test.g_5.d_930 WHERE time >= 1537377805000 AND time <= 1537377810000
org.apache.iotdb.jdbc.IoTDBSQLException: 500: [INTERNAL_SERVER_ERROR] Exception occurred while executing executeQueryStatement. null




"	IOTDB	Closed	3	1	5267	pull-request-available
13545949	M4 will output zero while meeting null	"using the data in the attachment and runing a 1D1D IoTDB, then execute the following sql:

```

select M4(t21,’timeInterval'='2000000000'),M4(t10,’timeInterval'='2000000000') fronroot.ZYJ_test.test where time<=1690882738999999999 and time>=1690882708000000000

```

!image-2023-08-03-14-30-14-936.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13318506	Bug in selfCheck() function of TsFileSequenceReader	While doing self check for a tsfile, we should retain the version_number. That means we should initialize the truncatedPosition to headerLength, not only the length of MAGIC_STRING	IOTDB	Closed	3	1	5267	pull-request-available
13549249	Fill statement field of show queries for inner schema fetch	"Currently, our inner query like `fetch schema` won't display its sql while we show queries which may be  confusing.
!image-2023-08-31-16-43-35-277.png!"	IOTDB	Closed	3	2	5267	pull-request-available
13471527	Last cache didn't consider TTL	"We set ttl to a storage group, the last query can still show the result that should be deleted.

!image-2022-07-14-14-58-50-517.png!

!image-2022-07-14-14-59-03-158.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13301173	Stale LeafMNodes are not removed in tag inverted index map	"When I delete storage group, all the leafMNodes in that storage group should be removed from tag inverted index map and can never be queried out.

However, currently, even though I delete the storage group, I still use the following sql:

```

show timeseries where tag1=v1

```

to get result."	IOTDB	Closed	3	1	5267	pull-request-available
13381593	OOM caused by ChunkCache	"Test environment

IoTDB：fit4,fit5,fit7 （3 nodes and 3 replica）

benchmark：fit4,fit5,fit7（each benchmark instance connect to the local server）

IoTDB version: 31f077ed752b759442c7f95aa4f003dbdb229260

benchmark barnch: 0.12

benchmark config:

IoTDB config:

 

Phenomenon

2021-05-24 11:52:22,226 server started
2021-05-24 17:39:37,642  throw connection reset exception in server
2021-05-25 04:44:47,377 throw OOM Exception in server"	IOTDB	Closed	3	1	5267	pull-request-available
13447433	Last Query return data which are not satisfied with time filter	!image-2022-05-30-10-51-19-082.png!	IOTDB	Closed	3	1	5267	pull-request-available
13377976	TsFileResource memory control	"For each data file TsFile, we generate a TsFileResource, which contains the time interval of paths in this file.  The TsFileResource is used as a time filter to locate data.

Currently, all TsFileResources with device-level indexes are cached in memory (TsFileManagement.java).

This may cause OOM. We need to control its memory.

 

设计文档见[https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=184617885]"	IOTDB	Closed	3	4	5267	pull-request-available
13448046	Align by device: Error in calling method sendFragmentInstance	" 

datanode1
{code:java}
2022-06-02 14:45:44,182 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:186 - Query[20220602_064544_00093_0]: start to analyze query 
2022-06-02 14:45:44,182 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.a.Analyzer$AnalyzeVisitor:159 - Query[20220602_064544_00093_0]: fetch query schema... 
2022-06-02 14:45:44,184 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:186 - Query[20220602_064544_00094_0]: start to analyze query 
2022-06-02 14:45:44,184 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:216 - Query[20220602_064544_00094_0]: do logical plan... 
2022-06-02 14:45:44,184 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:219 - Query[20220602_064544_00094_0]: logical plan is: 
 SchemaFetchNode-1
  └──org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.read.SchemaFetchScanNode@51
 
2022-06-02 14:45:44,184 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:227 - Query[20220602_064544_00094_0]: do distribution plan... 
SchemaFetchNode-1
  └──org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.read.SchemaFetchScanNode@52


SchemaFetchNode-1
  └──org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.read.SchemaFetchScanNode@52


2022-06-02 14:45:44,185 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:230 - Query[20220602_064544_00094_0]: distribution plan done. Fragment instance count is 1, details is: 
 [FragmentInstance-20220602_064544_00094_0.0.0:Host: 2 Region: TConsensusGroupId(type:SchemaRegion, id:0) 
---- Plan Node Tree ----
FragmentSinkNode-4:[SendTo: (127.0.0.1/20220602_064544_00094_0.0.0/0)]
  └──SchemaFetchNode-1
      └──org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.read.SchemaFetchScanNode@52
] 
2022-06-02 14:45:44,185 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.s.ClusterScheduler:97 - Query[20220602_064544_00094_0] transit to DISPATCHING 
2022-06-02 14:45:44,186 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.s.ClusterScheduler:127 - Query[20220602_064544_00094_0] transit to RUNNING 
2022-06-02 14:45:44,187 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.s.ClusterScheduler:135 - Query[20220602_064544_00094_0] state tracker starts 
2022-06-02 14:45:44,187 [pool-6-IoTDB-MPPCoordinatorScheduled-1] INFO  o.a.i.d.m.p.s.FixedRateFragInsStateTracker:75 - Instance 20220602_064544_00094_0.0.0's State is RUNNING 
2022-06-02 14:45:44,188 [pool-20-IoTDB-DataBlockManagerRPC-Client-1] INFO  o.a.i.d.m.e.d.SourceHandle:207 - Query[20220602_064544_00094_0]-[0-0-SourceHandle-0]: receive newDataBlockEvent. [0, 1) 
2022-06-02 14:45:44,188 [pool-19-IoTDB-data-block-manager-task-executors-1] INFO  o.a.i.d.m.e.d.SourceHandle$GetDataBlocksTask:318 - Query[20220602_064544_00094_0]-[0-0-SourceHandle-0]: try to get data blocks [0, 1) 
2022-06-02 14:45:44,188 [pool-20-IoTDB-DataBlockManagerRPC-Client-2] INFO  o.a.i.d.m.e.d.SourceHandle:196 - Query[20220602_064544_00094_0]-[0-0-SourceHandle-0]: receive NoMoreTsBlock event. 
2022-06-02 14:45:44,188 [pool-19-IoTDB-data-block-manager-task-executors-1] INFO  o.a.i.d.m.e.d.SourceHandle$GetDataBlocksTask:336 - Query[20220602_064544_00094_0]-[0-0-SourceHandle-0]: got data blocks. count: 1 
2022-06-02 14:45:44,188 [pool-19-IoTDB-data-block-manager-task-executors-1] INFO  o.a.i.d.m.e.d.SourceHandle$SendAcknowledgeDataBlockEventTask:391 - Query[20220602_064544_00094_0]-[0-0-SourceHandle-0]: send ack data block event [0, 1). 
2022-06-02 14:45:44,188 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.e.d.DataBlockManager$SourceHandleListenerImpl:188 - Query[20220602_064544_00094_0]-[0-0-SourceHandle-0] finished and release resources 
2022-06-02 14:45:44,189 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.a.Analyzer$AnalyzeVisitor:161 - Query[20220602_064544_00093_0]: fetch schema done 
2022-06-02 14:45:44,190 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:216 - Query[20220602_064544_00093_0]: do logical plan... 
2022-06-02 14:45:44,191 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:219 - Query[20220602_064544_00093_0]: logical plan is: 
 DeviceView-8
  ├──AlignedSeriesScanNode-1:[SeriesPath: root.sg1.d1, DataRegion: null]
  └──TimeJoinNode-7
      ├──SeriesScanNode-2:[SeriesPath: root.sg1.d2.s3, DataRegion: null]
      ├──SeriesScanNode-3:[SeriesPath: root.sg1.d2.s4, DataRegion: null]
      ├──SeriesScanNode-4:[SeriesPath: root.sg1.d2.s5, DataRegion: null]
      ├──SeriesScanNode-5:[SeriesPath: root.sg1.d2.s1, DataRegion: null]
      └──SeriesScanNode-6:[SeriesPath: root.sg1.d2.s2, DataRegion: null]
 
2022-06-02 14:45:44,191 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:227 - Query[20220602_064544_00093_0]: do distribution plan... 
DeviceMerge-9
  ├──DeviceView-16
  │   └──TimeJoinNode-15
  │       ├──SeriesScanNode-10:[SeriesPath: root.sg1.d2.s3, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       ├──SeriesScanNode-11:[SeriesPath: root.sg1.d2.s4, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       ├──SeriesScanNode-12:[SeriesPath: root.sg1.d2.s5, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       ├──SeriesScanNode-13:[SeriesPath: root.sg1.d2.s1, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       └──SeriesScanNode-14:[SeriesPath: root.sg1.d2.s2, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  └──DeviceView-18
      └──AlignedSeriesScanNode-17:[SeriesPath: root.sg1.d1, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:1), dataNodeLocations:[TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010)), TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012))])]


DeviceMerge-9
  ├──DeviceView-16
  │   └──TimeJoinNode-15
  │       ├──SeriesScanNode-10:[SeriesPath: root.sg1.d2.s3, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       ├──SeriesScanNode-11:[SeriesPath: root.sg1.d2.s4, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       ├──SeriesScanNode-12:[SeriesPath: root.sg1.d2.s5, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       ├──SeriesScanNode-13:[SeriesPath: root.sg1.d2.s1, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  │       └──SeriesScanNode-14:[SeriesPath: root.sg1.d2.s2, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
  └──ExchangeNode-19: [SourceAddress:Not assigned]
      └──DeviceView-18
          └──AlignedSeriesScanNode-17:[SeriesPath: root.sg1.d1, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:1), dataNodeLocations:[TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010)), TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012))])]


2022-06-02 14:45:44,192 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.e.QueryExecution:230 - Query[20220602_064544_00093_0]: distribution plan done. Fragment instance count is 2, details is: 
 [FragmentInstance-20220602_064544_00093_0.0.0:Host: 1 Region: TConsensusGroupId(type:DataRegion, id:2) 
---- Plan Node Tree ----
FragmentSinkNode-21:[SendTo: (127.0.0.1/20220602_064544_00093_0.0.0/0)]
  └──DeviceMerge-9
      ├──DeviceView-16
      │   └──TimeJoinNode-15
      │       ├──SeriesScanNode-10:[SeriesPath: root.sg1.d2.s3, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
      │       ├──SeriesScanNode-11:[SeriesPath: root.sg1.d2.s4, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
      │       ├──SeriesScanNode-12:[SeriesPath: root.sg1.d2.s5, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
      │       ├──SeriesScanNode-13:[SeriesPath: root.sg1.d2.s1, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
      │       └──SeriesScanNode-14:[SeriesPath: root.sg1.d2.s2, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:2), dataNodeLocations:[TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012)), TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010))])]
      └──ExchangeNode-19: [SourceAddress:127.0.0.1/20220602_064544_00093_0.1.0/20]
, FragmentInstance-20220602_064544_00093_0.1.0:Host: 0 Region: TConsensusGroupId(type:DataRegion, id:1) 
---- Plan Node Tree ----
FragmentSinkNode-20:[SendTo: (127.0.0.1/20220602_064544_00093_0.0.0/19)]
  └──DeviceView-18
      └──AlignedSeriesScanNode-17:[SeriesPath: root.sg1.d1, DataRegion: TRegionReplicaSet(regionId:TConsensusGroupId(type:DataRegion, id:1), dataNodeLocations:[TDataNodeLocation(dataNodeId:0, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6667), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9003), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8777), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40010), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50010)), TDataNodeLocation(dataNodeId:1, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6668), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9004), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8778), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40011), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50011)), TDataNodeLocation(dataNodeId:2, externalEndPoint:TEndPoint(ip:127.0.0.1, port:6669), internalEndPoint:TEndPoint(ip:127.0.0.1, port:9005), dataBlockManagerEndPoint:TEndPoint(ip:127.0.0.1, port:8779), dataRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:40012), schemaRegionConsensusEndPoint:TEndPoint(ip:127.0.0.1, port:50012))])]
] 
2022-06-02 14:45:44,193 [pool-7-IoTDB-RPC-Client-1] INFO  o.a.i.d.m.p.s.ClusterScheduler:97 - Query[20220602_064544_00093_0] transit to DISPATCHING 
2022-06-02 14:45:44,195 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:65 - level-0 Exception class org.apache.thrift.TApplicationException, message Internal error processing sendFragmentInstance 
2022-06-02 14:45:44,195 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:80 - root cause message Internal error processing sendFragmentInstance, LocalizedMessage Internal error processing sendFragmentInstance, 
org.apache.thrift.TApplicationException: Internal error processing sendFragmentInstance
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$receiveBase$60(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.receiveBase(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.recv_sendFragmentInstance(InternalService.java:205)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$recv_sendFragmentInstance$10(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.recv_sendFragmentInstance(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.sendFragmentInstance(InternalService.java:192)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$sendFragmentInstance$40(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.sendFragmentInstance(<generated>)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchRemote(FragmentInstanceDispatcherImpl.java:160)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:141)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.lambda$dispatchRead$0(FragmentInstanceDispatcherImpl.java:100)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2022-06-02 14:45:44,195 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:65 - level-0 Exception class org.apache.thrift.TException, message Error in calling method receiveBase 
2022-06-02 14:45:44,195 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:65 - level-1 Exception class org.apache.thrift.TApplicationException, message Internal error processing sendFragmentInstance 
2022-06-02 14:45:44,195 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:80 - root cause message Internal error processing sendFragmentInstance, LocalizedMessage Internal error processing sendFragmentInstance, 
org.apache.thrift.TApplicationException: Internal error processing sendFragmentInstance
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$receiveBase$60(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.receiveBase(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.recv_sendFragmentInstance(InternalService.java:205)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$recv_sendFragmentInstance$10(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.recv_sendFragmentInstance(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.sendFragmentInstance(InternalService.java:192)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$sendFragmentInstance$40(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.sendFragmentInstance(<generated>)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchRemote(FragmentInstanceDispatcherImpl.java:160)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:141)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.lambda$dispatchRead$0(FragmentInstanceDispatcherImpl.java:100)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2022-06-02 14:45:44,196 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:65 - level-0 Exception class org.apache.thrift.TException, message Error in calling method recv_sendFragmentInstance 
2022-06-02 14:45:44,196 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:65 - level-1 Exception class org.apache.thrift.TException, message Error in calling method receiveBase 
2022-06-02 14:45:44,196 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:65 - level-2 Exception class org.apache.thrift.TApplicationException, message Internal error processing sendFragmentInstance 
2022-06-02 14:45:44,196 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:80 - root cause message Internal error processing sendFragmentInstance, LocalizedMessage Internal error processing sendFragmentInstance, 
org.apache.thrift.TApplicationException: Internal error processing sendFragmentInstance
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$receiveBase$60(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.receiveBase(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.recv_sendFragmentInstance(InternalService.java:205)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$recv_sendFragmentInstance$10(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.recv_sendFragmentInstance(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.sendFragmentInstance(InternalService.java:192)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$sendFragmentInstance$40(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.sendFragmentInstance(<generated>)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchRemote(FragmentInstanceDispatcherImpl.java:160)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:141)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.lambda$dispatchRead$0(FragmentInstanceDispatcherImpl.java:100)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2022-06-02 14:45:44,196 [pool-4-IoTDB-MPPCoordinator-5] ERROR o.a.i.d.m.p.s.FragmentInstanceDispatcherImpl:163 - can't connect to node TEndPoint(ip:127.0.0.1, port:9004) 
org.apache.thrift.TException: Error in calling method sendFragmentInstance
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:94)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.sendFragmentInstance(<generated>)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchRemote(FragmentInstanceDispatcherImpl.java:160)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:141)
	at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.lambda$dispatchRead$0(FragmentInstanceDispatcherImpl.java:100)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.thrift.TException: Error in calling method recv_sendFragmentInstance
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:94)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.recv_sendFragmentInstance(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.sendFragmentInstance(InternalService.java:192)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$sendFragmentInstance$40(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	... 8 common frames omitted
Caused by: org.apache.thrift.TException: Error in calling method receiveBase
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:94)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.receiveBase(<generated>)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.recv_sendFragmentInstance(InternalService.java:205)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$recv_sendFragmentInstance$10(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	... 14 common frames omitted
Caused by: org.apache.thrift.TApplicationException: Internal error processing sendFragmentInstance
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e.CGLIB$receiveBase$60(<generated>)
	at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$35b0e10e$$FastClassByCGLIB$$f5ef5929.invoke(<generated>)
	at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
	at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
	... 20 common frames omitted
2022-06-02 14:45:44,197 [pool-7-IoTDB-RPC-Client-1] WARN  o.a.i.d.u.ErrorHandlingUtils:62 - Status code: INTERNAL_SERVER_ERROR(500), operation: ""select * from root.sg1.** where time >= 9 and time <= 33 align by device"". executeStatement failed 
java.lang.RuntimeException: error code: TSStatus(code:411, message:org.apache.iotdb.db.exception.mpp.FragmentInstanceDispatchException: org.apache.thrift.TException: Error in calling method sendFragmentInstance)
	at org.apache.iotdb.db.service.thrift.impl.DataNodeTSIServiceImpl.executeStatement(DataNodeTSIServiceImpl.java:561)
	at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeStatement.getResult(TSIService.java:2853)
	at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeStatement.getResult(TSIService.java:2833)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
	at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2022-06-02 14:45:44,691 [pool-6-IoTDB-MPPCoordinatorScheduled-1] INFO  o.a.i.d.m.p.s.FixedRateFragInsStateTracker:75 - Instance 20220602_064544_00094_0.0.0's State is FINISHED 
2022-06-02 14:45:45,201 [pool-4-IoTDB-MPPCoordinator-10] INFO  o.a.i.d.m.e.d.DataBlockManager$SourceHandleListenerImpl:207 - Query[20220602_064544_00093_0]-[0-0-SourceHandle-0]: onAborted is invoked 
2022-06-02 14:45:45,202 [pool-4-IoTDB-MPPCoordinator-10] INFO  o.a.i.d.m.e.d.DataBlockManager$SourceHandleListenerImpl:188 - Query[20220602_064544_00093_0]-[0-0-SourceHandle-0] finished and release resources  {code}
datanode2
{code:java}
2022-06-02 14:45:44,194 [pool-1-IoTDB-InternalServiceRPC-Client-6] INFO  o.a.i.d.s.t.i.InternalServiceImpl:117 - receive FragmentInstance to group[TConsensusGroupId(type:DataRegion, id:2)] 
2022-06-02 14:45:44,194 [pool-1-IoTDB-InternalServiceRPC-Client-6] ERROR o.a.i.d.c.s.DataRegionStateMachine:146 - Invalid node type: 35 
2022-06-02 14:45:44,194 [pool-1-IoTDB-InternalServiceRPC-Client-6] ERROR o.a.t.ProcessFunction:47 - Internal error processing sendFragmentInstance 
java.lang.NullPointerException: null
	at org.apache.iotdb.db.service.thrift.impl.InternalServiceImpl.sendFragmentInstance(InternalServiceImpl.java:141)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$sendFragmentInstance.getResult(InternalService.java:1195)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$sendFragmentInstance.getResult(InternalService.java:1175)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750) {code}
Confignode-2
{code:java}
2022-06-02 14:42:11,677 [pool-2-IoTDB-ConfigNodeRPC-Client-1] ERROR o.a.t.s.TThreadPoolServer$WorkerProcess:258 - Thrift Error occurred during processing of message. 
org.apache.thrift.protocol.TProtocolException: Required field 'userInfo' was not present! Struct: TPermissionInfoResp(userInfo:null, roleInfo:null, status:TSStatus(code:707, message:The current ConfigNode is not leader. And ConfigNodeGroup is in leader election. Please redirect with a random ConfigNode.))
	at org.apache.iotdb.confignode.rpc.thrift.TPermissionInfoResp.validate(TPermissionInfoResp.java:458)
	at org.apache.iotdb.confignode.rpc.thrift.ConfigIService$login_result.validate(ConfigIService.java:18415)
	at org.apache.iotdb.confignode.rpc.thrift.ConfigIService$login_result$login_resultStandardScheme.write(ConfigIService.java:18474)
	at org.apache.iotdb.confignode.rpc.thrift.ConfigIService$login_result$login_resultStandardScheme.write(ConfigIService.java:18441)
	at org.apache.iotdb.confignode.rpc.thrift.ConfigIService$login_result.write(ConfigIService.java:18392)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:58)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2022-06-02 14:42:23,449 [pool-2-IoTDB-ConfigNodeRPC-Client-2] INFO  o.a.i.c.s.t.ConfigNodeRPCServiceProcessor:193 - Execute SetStorageGroupRequest TSetStorageGroupReq(storageGroup:TStorageGroupSchema(name:root.sg1, TTL:9223372036854775807, schemaReplicationFactor:3, dataReplicationFactor:3, timePartitionInterval:604800, maximumSchemaRegionCount:-1, maximumDataRegionCount:-1, dataRegionGroupIds:[], schemaRegionGroupIds:[])) with result TSStatus(code:707, message:The current ConfigNode is not leader. And ConfigNodeGroup is in leader election. Please redirect with a random ConfigNode.) 
2022-06-02 14:42:23,478 [0.0.0.0_22280@group-000000000000-StateMachineUpdater] WARN  o.a.i.d.m.r.MemoryStatistics:80 - Current series number 224 is too large... 
2022-06-02 14:42:23,479 [0.0.0.0_22280@group-000000000000-StateMachineUpdater] INFO  o.a.i.c.p.ClusterSchemaInfo:113 - Successfully set StorageGroup: TStorageGroupSchema(name:root.sg1, TTL:9223372036854775807, schemaReplicationFactor:3, dataReplicationFactor:3, timePartitionInterval:604800, maximumSchemaRegionCount:-1, maximumDataRegionCount:-1, dataRegionGroupIds:[], schemaRegionGroupIds:[])  {code}
 

confignode3
{code:java}
type:DataRegion, id:2)] 
2022-06-02 14:42:50,243 [pool-1-IoTDB-InternalServiceRPC-Client-5] INFO  o.a.i.d.s.t.i.InternalServiceImpl:117 - receive FragmentInstance to group[TConsensusGroupId(type:DataRegion, id:2)] 
2022-06-02 14:42:53,739 [Service Thread] INFO  o.a.i.m.m.MicrometerMetricManager:200 - create getOrCreateTimer jvm.gc.pause 
2022-06-02 14:43:03,344 [pool-1-IoTDB-InternalServiceRPC-Client-5] INFO  o.a.i.d.s.t.i.InternalServiceImpl:117 - receive FragmentInstance to group[TConsensusGroupId(type:DataRegion, id:2)] 
2022-06-02 14:43:03,346 [pool-1-IoTDB-InternalServiceRPC-Client-5] ERROR o.a.i.d.c.s.DataRegionStateMachine:146 - Invalid node type: 35 
2022-06-02 14:43:03,348 [pool-1-IoTDB-InternalServiceRPC-Client-5] ERROR o.a.t.ProcessFunction:47 - Internal error processing sendFragmentInstance 
java.lang.NullPointerException: null
	at org.apache.iotdb.db.service.thrift.impl.InternalServiceImpl.sendFragmentInstance(InternalServiceImpl.java:141)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$sendFragmentInstance.getResult(InternalService.java:1195)
	at org.apache.iotdb.mpp.rpc.thrift.InternalService$Processor$sendFragmentInstance.getResult(InternalService.java:1175)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

 {code}
 "	IOTDB	Closed	3	1	5267	pull-request-available
13475825	Implemation of partial operators for memory control	"* 
FillOperator (org.apache.iotdb.db.mpp.execution.operator.process)

 * 
LastQueryCollectOperator (org.apache.iotdb.db.mpp.execution.operator.process.last)

 * 
LastQueryMergeOperator (org.apache.iotdb.db.mpp.execution.operator.process.last)

 * 
LastQueryOperator (org.apache.iotdb.db.mpp.execution.operator.process.last)

 * 
LastQuerySortOperator (org.apache.iotdb.db.mpp.execution.operator.process.last)

 * 
LimitOperator (org.apache.iotdb.db.mpp.execution.operator.process)

 * 
OffsetOperator (org.apache.iotdb.db.mpp.execution.operator.process)

 * 
RowBasedTimeJoinOperator (org.apache.iotdb.db.mpp.execution.operator.process.join)

 * 
SortOperator (org.apache.iotdb.db.mpp.execution.operator.process)

 * 
TimeJoinOperator (org.apache.iotdb.db.mpp.execution.operator.process.join)

 * 
UpdateLastCacheOperator (org.apache.iotdb.db.mpp.execution.operator.process.last)

 * 
LinearFillOperator (org.apache.iotdb.db.mpp.execution.operator.process)
 ** 
需要单独讨论，是否需要加上限制，最多往后探多少个TsBlock，如果还未遇到非空的，则报错

 * 
AlignedSeriesScanOperator (org.apache.iotdb.db.mpp.execution.operator.source)

 * 
SeriesScanOperator (org.apache.iotdb.db.mpp.execution.operator.source)

 * 
ExchangeOperator (org.apache.iotdb.db.mpp.execution.operator.source)

 * 
LastCacheScanOperator (org.apache.iotdb.db.mpp.execution.operator.source)"	IOTDB	Closed	3	4	5267	pull-request-available
13474413	Improve the performance of Raw Query Without ValueFilter for nonAligned	"Test Configuration:

1 device, 30sensors(non aligned), each sensor contains 1,000,000 points

 

Test Machine:

Apple M1 Pro 

 

Test sql:

We do `selelect * from root.**` query, time cost for old standalone and new standalone:

old standalone: 2852ms

new standalone: 5035ms

 "	IOTDB	Closed	3	4	5267	pull-request-available
13348050	New TsFile Format for 0.12	"We find that the former tsfile format waste some disk space, some information was stored duplicately and some legacy fields are no longer useful.

So a new TsFile format should be designed to make our tsfile more dense.

Design: https://cwiki.apache.org/confluence/display/IOTDB/New+TsFile+Format"	IOTDB	Closed	3	2	5267	pull-request-available
13434714	Definition of operators interface and  PlanVisitor	"Definition of Operator interface and all cuurent operators corresponding to each PlanNode and  PlanVisitor in LocalExecutionPlanner to generate executable operator tree from PlamNode tree:
 # Add all current useful operators add filter operator implementation and visitor in LocalExecutionPlanner.
 # change the package structure.
 # add some necessary fields in each concrete PlanNode to generate corresponding oeprator.
 # define IScheduler interface, add two implemetation: StandaloneScheduler and ClusterScheduler."	IOTDB	Closed	3	3	5267	pull-request-available
13546535	tmp directory won't be cleaned after udf query end	"While using udf to query, and when the memory is not enough, we will spill the data to disk. However when  the query end, the tmp directory is not cleaned completely.

!image-2023-08-08-20-47-38-670.png!

 

To replay, you can  use the UDF in the attachment, and  then create function with that. change the `udf_memory_budget_in_mb` to `0.01 `and  then insert 1000000 points data. After that,  you can use your registered udf to do the query."	IOTDB	Closed	3	1	5267	pull-request-available
13493868	"[Atmos][Query Performance] Compare performance of 1C1D with ""start-server.sh” "	"commit_id：6e089fb

Reproduce Steps：

1.Git pull the latest master code,then build it with command ""mvn clean package -pl distribution -am -DskipTests""

2.Modify the config as bellow:

MAX_HEAP_SIZE=""20G""

enable_partition=false

enable_seq_space_compaction=false

enable_unseq_space_compaction=false

enable_cross_space_compaction=false

enableMetric: true

3.Start 1C1D

4.Load data with cli

5.Execute the query test

!image-2022-11-01-09-39-04-146.png|width=549,height=268!

B.R

 "	IOTDB	Closed	3	4	5267	pull-request-available
13301759	Add a jdbc-like way to fetch data in session	"Currently, we can only fetch result through RowRecord in session.

That means each time we get a row, we will create a RowRecord object. We can add a jdbc-like way to fetch data in session, with which we can reduce the cost of creating RowRecord."	IOTDB	Closed	3	4	5267	pull-request-available
13324249	Optimize value filter query	"Now, while doing query with value filter, we won't use the time filter condition in filter to unpack the chunk to pages. For exmaple:

 

select s1 from root.sg1.d1 where time > 0 and time < 10 and s1 > 0 and s1 < 9;

 

We will get an AndFilter as valueFilter whose left node is 'time > 0 and time < 10'  and right node is 's1 > 0 and s1 < 9'. While unpacking chunk, we only use timeFilter(which is null in this case) to filter the page. However, timeFilter is null in this case, so we need to unpack all the page data in this chunk whatever it is really satisfied."	IOTDB	Closed	3	4	5267	pull-request-available
13311406	Lost precision when using PLAIN encoding for floating data	"create timeseries root.turbine1.d1.s2 with datatype=DOUBLE, encoding=PLAIN, compression=SNAPPY

insert into root.turbine1.d1(timestamp,s2) values(1,1.2345678);

 

select s2 from root.*.*


+-----------------------------+-------------------+
| Time|root.turbine1.d1.s2|
+-----------------------------+-------------------+
|1970-01-01T08:00:00.001+08:00| 1.23|
+-----------------------------+-------------------+"	IOTDB	Closed	3	1	5267	pull-request-available
13309919	Handle select * from root OOM	"In this query, select * from root

the server will cache [the total time series * fetch size] points. This may cause OOM, we need to handle this."	IOTDB	Closed	3	4	5267	pull-request-available
13336215	Align by device doesn't support '*' in path.	!Screenshot from 2020-10-20 15-06-33.png!	IOTDB	Closed	3	1	5267	pull-request-available
13310132	Throw NullPointer while deleting timeseries and storage group	"Sometimes, it will throw nullpointer exception while deleting timeseries or storage group

!image-2020-06-08-20-56-07-171.png!

 

!image-2020-06-08-20-57-38-783.png!"	IOTDB	Closed	3	1	5267	pull-request-available
13411987	iotdb-engine.properties doesn't take effect	"In current IoTDB, if we uncomment the 

chunk_timeseriesmeta_free_memory_proportion=1:100:200:300:400

and don't uncomment

max_deduplicated_path_num=1000

server will throw exception while starting and all the following config changes will not take effect."	IOTDB	Closed	3	1	5267	pull-request-available
13592792	Don't retry if operation has already succeed even if the DN is not running status	Previously, we will set needRetry to true even if operation has already succeed(statusCode is 200) if he DN is not running status. It's not reasonable in some cases, like dropping model.	IOTDB	Open	3	1	5267	pull-request-available
13421194	Count timeseries Group by level Statistics error	"Step1: start server
Step2: Enter cli

Step3: count timeseries root.** group by level=1

!image-2022-01-07-10-34-12-458.png!"	IOTDB	Closed	3	1	6861	0.13.0, pull-request-available
13534320	Drop database won't delete totally files in disk	While dropping database and inserting into that database concurrently, I found that there may exist stale data region related directories in disk like in consensus sub directory and wal sub directory.	IOTDB	Resolved	3	1	6861	pull-request-available
13421001	Tag recover bug after tag upsert	"Currently, the tagIndex in metadata module uses mlog to recover.

However, if user alters one timeseries' tag, the tag will only be updated to the tagLogFile rather than mlog.

Thus, after tag update operation, the recover won't use the up-to-date tag information in tagLogFile but use the outdated tag information in mlog."	IOTDB	Closed	3	1	6861	pull-request-available
13514654	Add Schema Memory Usage metric monitor	The remaining memory for schema module is a critical factor considered for series registering and resource updating.	IOTDB	Closed	3	4	6861	pull-request-available
13470021	The failure RPC is not handled when getOrCreateDataPartition	"!image-2022-07-05-10-49-39-033.png|width=626,height=243!

See the code above. If the RPC is failed, there is no logic to handle that and a null will be returned.

It will lead to the following code throws NPE when executing."	IOTDB	Closed	1	1	6861	pull-request-available
13473147	[Atmos][Performance] Compare insert tablet between start-server.sh and start-new-server.sh	"Produce Steps：

1.git clone and build iotdb-master 

2.using start-server.sh to start iotd instance

3.using iotdb-benchmark to test sequence insert(get a result:{color:#00875a}green{color} line)

4.clean the test env and using start-new-server.sh to start a NewIoTDB instance

5.using iotdb-benchmark to test sequence insert(get a result:{color:#ff8b00}yellow{color} line)

!image-2022-07-25-09-09-42-767.png|width=581,height=249!"	IOTDB	Closed	3	1	6861	pull-request-available
13527758	Accelerate Delete Schema via AsyncClient	Currently, DataNodes will be processed one by one during deleting schema. It's better to use async client to implement concurrent process among DataNodes.	IOTDB	Closed	3	4	6861	pull-request-available
13420531	The user granted the permission to query data still has no permission to query data	"User: xzh_01
GRANT USER xzh_01 PRIVILEGES READ_TIMESERIES on root.ln;

execute sql:
select * from root.ln;

!image-2022-01-04-15-31-01-108.png!

!image-2022-01-04-15-28-58-560.png|width=691,height=218!"	IOTDB	Closed	2	1	6861	0.13.0, pull-request-available
13421231	[DOC]Wrong SQL instance	"[https://iotdb.apache.org/zh/UserGuide/Master/IoTDB-SQL-Language/DML-Data-Manipulation-Language.html#%E4%BD%BF%E7%94%A8-insert-%E8%AF%AD%E5%8F%A5]

!image-2022-01-07-15-25-19-513.png!"	IOTDB	Closed	4	1	6861	0.13.0
13516735	Eliminate stale code in LocalConfigNode and LocalSchemaProcessor to simplify interfaces of ISchemaRegion	Since LocalConfigNode and LocalSchemaProcessor are deprecated, SchemaRegion has no need to provide specific interfaces for them. Thus eliminate the stale code if possible and refactor the implementation of interfaces still used in UT, to simplify the interfaces of ISchemaRegion	IOTDB	Closed	3	4	6861	pull-request-available
13405102	Debug MManager Serialization on 0.12 for APM scenario test	"To achieve a better storage capacity score in APM scenario test, we are trying to merge rel/0.12 with MManager serialization 0.11 based version.

There are many CI failures and potential bugs after merge. It takes time to detect and fix these bugs."	IOTDB	Closed	3	1	6861	pull-request-available
13467071	[ schema ] ERROR o.a.i.d.m.e.o.s.SchemaFetchScanOperator:113 - The size of schemaTree's binary data is too large. 3	"master_0624_9cfc0f0_1c1d
1C1D ,72C256G机器，集群启动参数默认， StandAloneConsensus：
10万设备，每个设备6个序列，每个序列1个点。
select count(s_4) from root.** align by device;
2022-06-24 16:55:15,397 [20220624_085515_15089_1.1.0-37] {color:red}*ERROR o.a.i.d.m.e.o.s.SchemaFetchScanOperator:113 - The size of schemaTree's binary data is too large. 3
java.nio.BufferOverflowException: null*{color}
        at java.nio.Buffer.nextPutIndex(Buffer.java:547)
        at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:172)
        at org.apache.iotdb.tsfile.utils.ReadWriteIOUtils.write(ReadWriteIOUtils.java:228)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaMeasurementNode.serialize(SchemaMeasurementNode.java:86)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaEntityNode.serialize(SchemaEntityNode.java:103)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.SchemaTree.serialize(SchemaTree.java:218)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.fetchSchema(SchemaFetchScanOperator.java:111)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:80)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchMergeOperator.next(SchemaFetchMergeOperator.java:54)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processInternal(Driver.java:188)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.lambda$processFor$1(Driver.java:127)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.tryWithLock(Driver.java:274)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processFor(Driver.java:120)
        at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskThread.execute(DriverTaskThread.java:58)
        at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:64)
2022-06-24 16:55:15,398 [20220624_085515_15089_1.1.0-37] ERROR o.a.i.d.m.e.driver.Driver:195 - Failed to execute fragment instance 20220624_085515_15089_1.1.0
java.nio.BufferOverflowException: null
        at java.nio.Buffer.nextPutIndex(Buffer.java:547)
        at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:172)
        at org.apache.iotdb.tsfile.utils.ReadWriteIOUtils.write(ReadWriteIOUtils.java:228)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaMeasurementNode.serialize(SchemaMeasurementNode.java:86)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaEntityNode.serialize(SchemaEntityNode.java:103)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.SchemaTree.serialize(SchemaTree.java:218)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.fetchSchema(SchemaFetchScanOperator.java:111)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:80)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchMergeOperator.next(SchemaFetchMergeOperator.java:54)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processInternal(Driver.java:188)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.lambda$processFor$1(Driver.java:127)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.tryWithLock(Driver.java:274)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processFor(Driver.java:120)
        at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskThread.execute(DriverTaskThread.java:58)
        at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:64)
2022-06-24 16:55:15,398 [Worker-Thread-1] ERROR o.a.i.d.m.e.s.AbstractDriverThread:66 - execute failed
java.nio.BufferOverflowException: null
        at java.nio.Buffer.nextPutIndex(Buffer.java:547)
        at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:172)
        at org.apache.iotdb.tsfile.utils.ReadWriteIOUtils.write(ReadWriteIOUtils.java:228)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaMeasurementNode.serialize(SchemaMeasurementNode.java:86)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaEntityNode.serialize(SchemaEntityNode.java:103)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serializeChildren(SchemaInternalNode.java:88)
        at org.apache.iotdb.db.mpp.common.schematree.node.SchemaInternalNode.serialize(SchemaInternalNode.java:79)
        at org.apache.iotdb.db.mpp.common.schematree.SchemaTree.serialize(SchemaTree.java:218)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.fetchSchema(SchemaFetchScanOperator.java:111)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:80)
        at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchMergeOperator.next(SchemaFetchMergeOperator.java:54)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processInternal(Driver.java:188)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.lambda$processFor$1(Driver.java:127)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.tryWithLock(Driver.java:274)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processFor(Driver.java:120)
        at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskThread.execute(DriverTaskThread.java:58)
        at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:64)
"	IOTDB	Closed	3	1	6861	pull-request-available
13482849	delete timeseries  root.** ( with template ) :NPE	"master_0922_0d65058
benchmark开启模板
delete timeseries root.**

2022-09-22 18:19:28,366 [172.20.70.43_50010@group-00020000000E-StateMachineUpdater] ERROR o.a.i.c.r.ApplicationStateMachineProxy:153 - application statemachine throws a runtime excep
tion:
{color:#DE350B}*java.lang.NullPointerException: null
        at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.processMultiLevelWildcard(Traverser.java:255)*{color}
        at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.traverse(Traverser.java:164)
        at org.apache.iotdb.db.metadata.mtree.traverser.collector.CollectorTraverser.traverse(CollectorTraverser.java:63)
        at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.processMultiLevelWildcard(Traverser.java:233)
        at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.traverse(Traverser.java:164)
        at org.apache.iotdb.db.metadata.mtree.traverser.collector.CollectorTraverser.traverse(CollectorTraverser.java:63)
        at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.traverse(Traverser.java:144)
        at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.getMeasurementPathsWithAlias(MTreeBelowSGMemoryImpl.java:692)
        at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.getMeasurementPaths(MTreeBelowSGMemoryImpl.java:649)
        at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.getMeasurementPaths(MTreeBelowSGMemoryImpl.java:660)
        at org.apache.iotdb.db.metadata.schemaregion.SchemaRegionMemoryImpl.constructSchemaBlackList(SchemaRegionMemoryImpl.java:850)
        at org.apache.iotdb.db.metadata.visitor.SchemaExecutionVisitor.visitConstructSchemaBlackList(SchemaExecutionVisitor.java:313)
        at org.apache.iotdb.db.metadata.visitor.SchemaExecutionVisitor.visitConstructSchemaBlackList(SchemaExecutionVisitor.java:65)
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.write.ConstructSchemaBlackListNode.accept(ConstructSchemaBlackListNode.java:71)
        at org.apache.iotdb.db.consensus.statemachine.SchemaRegionStateMachine.write(SchemaRegionStateMachine.java:74)
        at org.apache.iotdb.consensus.ratis.ApplicationStateMachineProxy.applyTransaction(ApplicationStateMachineProxy.java:135)
        at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1588)
        at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:239)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:182)
        at java.lang.Thread.run(Thread.java:748)
2022-09-22 18:19:28,366 [pool-20-IoTDB-DataNodeInternalRPC-Processor-2] ERROR o.a.t.ProcessFunction:47 - Internal error processing constructSchemaBlackList
java.lang.ClassCastException: org.apache.ratis.protocol.Message$1 cannot be cast to org.apache.iotdb.consensus.ratis.ResponseMessage
        at org.apache.iotdb.consensus.ratis.RatisConsensus.write(RatisConsensus.java:238)
        at org.apache.iotdb.db.service.thrift.impl.DataNodeRegionManager.executePlanNodeInConsensusLayer(DataNodeRegionManager.java:119)
        at org.apache.iotdb.db.service.thrift.impl.DataNodeRegionManager.executeSchemaPlanNode(DataNodeRegionManager.java:180)
        at org.apache.iotdb.db.service.thrift.impl.DataNodeInternalRPCServiceImpl.constructSchemaBlackList(DataNodeInternalRPCServiceImpl.java:359)
        at org.apache.iotdb.mpp.rpc.thrift.IDataNodeRPCService$Processor$constructSchemaBlackList.getResult(IDataNodeRPCService.java:3787)
        at org.apache.iotdb.mpp.rpc.thrift.IDataNodeRPCService$Processor$constructSchemaBlackList.getResult(IDataNodeRPCService.java:3767)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

在这个环境执行
delete timeseries root.test3.g_6.d_166.*
约30分钟返回Path [root.test3.g_6.d_166.*] does not exist

2022-09-23 10:20:32,974 [pool-24-IoTDB-ClientRPC-Processor-10$20220923_022033_00003_7] INFO  o.a.i.d.m.p.Coordinator:123 - [QueryStart] sql: delete timeseries root.test3.g_6.d_166.*
2022-09-23 10:50:06,117 [pool-24-IoTDB-ClientRPC-Processor-10$20220923_022033_00003_7] ERROR o.a.i.d.m.p.e.c.e.ClusterConfigTaskExecutor:815 - Failed to execute delete timeseries [root.test3.g_6.d_166.*] in config node, status is TSStatus(code:304, message:Path [root.test3.g_6.d_166.*] does not exist).
2022-09-23 10:50:06,119 [pool-24-IoTDB-ClientRPC-Processor-10] INFO  o.a.i.d.s.t.i.ClientRPCServiceImpl:601 - Cost: {color:#DE350B}*1773145 ms*{color}, sql is delete timeseries root.test3.g_6.d_166.*
 !screenshot-1.png! 
测试流程
1. 172.20.70.34..43 共10台机器
confignode在34，35，36  3台机器
datanode在37，38..43   7台机器
benchmark在ip35

2. 集群配置参数
ConfigNode
MAX_HEAP_SIZE=""8G""
MAX_DIRECT_MEMORY_SIZE=""4G""
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
data_replication_factor=3

DataNode
MAX_HEAP_SIZE=""20G""
MAX_DIRECT_MEMORY_SIZE=""6G""

max_waiting_time_when_insert_blocked=3600000
query_timeout_threshold=36000000

3. bm配置文件

见附件

4. 运行脚本
cat run_del.sh
#!/bin/bash
sed -i 's/^START_TIME.*/START_TIME=2016-8-30T00:00:00+08:00/g' conf/down_delete_ts.conf
sed -i 's/^DB_NAME.*/DB_NAME=test1/g' conf/down_delete_ts.conf
./del_ts.sh > d1.out
/data/iotdb/master_0919_3348a06/datanode/sbin/start-cli.sh -h 172.20.70.43 -e 'delete timeseries root.**' &
sleep 5
sed -i 's/START_TIME.*/START_TIME=2017-8-30T00:00:00+08:00/g' conf/down_delete_ts.conf
sed -i 's/^DB_NAME.*/DB_NAME=test2/g' conf/down_delete_ts.conf
./del_ts.sh > d2.out
wait
/data/iotdb/master_0919_3348a06/datanode/sbin/start-cli.sh -h 172.20.70.43 -e 'delete timeseries root.**' &
sleep 5
sed -i 's/START_TIME.*/START_TIME=2018-8-30T00:00:00+08:00/g' conf/down_delete_ts.conf
sed -i 's/^DB_NAME.*/DB_NAME=test3/g' conf/down_delete_ts.conf
./del_ts.sh > d3.out
wait

5. 查看iotdb日志"	IOTDB	Closed	3	1	6861	pull-request-available
13446296	[ cluster metadata ]  WARN  o.a.i.d.m.r.MemoryStatistics:80 - Current series number 201 is too large...	"master_0523_51e9703
3C3D
创建30sg，300dev，1k ts/dev，共30万序列。
datanode内存：-Xmx7968M 
创建元数据失败：
confignode 没有error，有WARN：
2022-05-23 13:53:31,618 [192.168.130.3-22278@group-000000000000->192.168.130.5-22278-GrpcLogAppender-LogAppenderDaemon] WARN  o.a.r.g.s.GrpcLogAppender:177 - 192.168.130.3-22278@group-000000000000->192.168.130.5-22278-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2022-05-23 13:58:21,715 [192.168.130.3-22278@group-000000000000-StateMachineUpdater] WARN  o.a.i.d.m.r.MemoryStatistics:80 - Current series number 201 is too large...
2022-05-23 13:58:21,718 [192.168.130.3-22278@group-000000000000-StateMachineUpdater] WARN  o.a.i.d.m.r.MemoryStatistics:80 - Current series number 432 is too large...
2022-05-23 13:58:21,743 [192.168.130.3-22278@group-000000000000-StateMachineUpdater] WARN  o.a.i.d.m.r.MemoryStatistics:80 - Current series number 663 is too large...
 !image-2022-05-23-14-42-29-889.png! 
datanode 报错：
2022-05-23 13:58:25,843 [pool-18-IoTDB-MPPCoordinatorWrite-5] ERROR o.a.i.c.c.s.SyncThriftClientWithErrorHandler:80 - root cause message Internal error processing sendFragmentInstance, LocalizedMessage Internal error processing sendFragmentInstance,
org.apache.thrift.TApplicationException: Internal error processing sendFragmentInstance
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446.CGLIB$receiveBase$51(<generated>)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446$$FastClassByCGLIB$$76030a70.invoke(<generated>)
        at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
        at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446.receiveBase(<generated>)
        at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.recv_sendFragmentInstance(InternalService.java:170)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446.CGLIB$recv_sendFragmentInstance$12(<generated>)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446$$FastClassByCGLIB$$76030a70.invoke(<generated>)
        at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
        at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446.recv_sendFragmentInstance(<generated>)
        at org.apache.iotdb.mpp.rpc.thrift.InternalService$Client.sendFragmentInstance(InternalService.java:157)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446.CGLIB$sendFragmentInstance$39(<generated>)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446$$FastClassByCGLIB$$76030a70.invoke(<generated>)
        at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
        at org.apache.iotdb.commons.client.sync.SyncThriftClientWithErrorHandler.intercept(SyncThriftClientWithErrorHandler.java:55)
        at org.apache.iotdb.commons.client.sync.SyncDataNodeInternalServiceClient$$EnhancerByCGLIB$$fa724446.sendFragmentInstance(<generated>)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchRemote(FragmentInstanceDispatcherImpl.java:157)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:138)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.lambda$dispatchWrite$1(FragmentInstanceDispatcherImpl.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
 !image-2022-05-23-14-40-36-059.png! 

复现流程：
1. 机器信息
192.168.130.3/4/5
16C32G
2. benchmark 配置见附件
使用的bm版本：/home/benchmark/bm_20220322
3. iotdb配置文件见附件"	IOTDB	Closed	3	1	6861	pull-request-available
13451417	Support deacitvate and unset template	"1. Deactivate template: deactivate template on paths matched by given pathPattern, and the according data will be deleted at same time.
2. Unset template: if this template is activated on some path, it should not be unset."	IOTDB	Closed	3	4	6861	pull-request-available
13442896	SchemaFetcher not support multi paths across storage group	"The existing implementation of schemaFetcher doesn't work when execute queries like 

select *from root.sg1.d, root.sg2.d

The reason is that the schemaFetchNode is not split by storage group during analyzing stage."	IOTDB	Closed	3	1	6861	pull-request-available
13404272	Illegal String ending with . being parsed to PartialPath 	"When parsing strings, like root.sg.d. , to partialPath, an illegalPathException should be thrown rather than parse it to \{""root"", ""sg"", ""d""}."	IOTDB	Closed	3	1	6861	pull-request-available
13538340	Syntax error when specifying placeholders in create view statement.	"测试版本：iotdb_master_0601_81f541d
有2个问题：
cli执行报错不对。
指定占位符 ，创建view 不成功。
研发功能定义文档描述：
!image-2023-06-01-10-06-09-132.png! 
实际创建失败，测试用例：
drop database root.db;
drop database root.view;
create database root.db;
create database root.view;
create timeseries root.db.d01.temperature with datatype=INT32;
create timeseries root.db.d01.speed with datatype=INT64;
create timeseries root.db.d02.temperature with datatype=INT32;
create timeseries root.db.d02.s02 with datatype=INT64;
create timeseries root.db.d02.s03 with datatype=INT64;

create timeseries root.db.d03.temperature with datatype=TEXT;
create timeseries root.db.d03.c02 with datatype=INT64;
create timeseries root.db.d03.c03 with datatype=INT64;

create timeseries root.db.d04.temperature with datatype=TEXT;
create timeseries root.db.d04.d02 with datatype=INT64;
create timeseries root.db.d04.d03 with datatype=INT64;

create timeseries root.db.d05.temperature with datatype=float;
create timeseries root.db.d06.temperature with datatype=double;
insert into root.db.d01(time,temperature,speed) values(1,20,80);
insert into root.db.d06(time,temperature) values(1685583924366,30);
{color:#de350b}*//下面这个语句是在cli中执行，这个bug 需要cli来解决*{color}
{color:#de350b}{*}CREATE VIEW root.db.view(${2}_temperature) AS SELECT temperature FROM root.db.*{*};
CREATE VIEW root.db.view(${2}_${3}) AS SELECT temperature from root.db.*{*};{*}{color}

Msg: 701: ${2}_temperature is illegal, unquoted node name can only consist of digits, characters and underscore, or start or end with wildcard

!image-2023-06-01-10-07-36-404.png! 
{color:#de350b}*// 指定占位符的创建方式，怎么才能执行成功*{color}
./sbin/start-cli.sh -h 172.20.70.3 -e "" CREATE VIEW root.db.view(${2}_temperature) AS SELECT temperature FROM root.db.*;""
Msg: 300: The number of target and source paths are miss matched! Please check your SQL.

!screenshot-1.png!"	IOTDB	Closed	3	1	6861	view
13503037	[DispatchFailed] NPE at org.apache.iotdb.db.mpp.plan.planner.plan.node.write.InsertNode.selfCheckDataTypes(InsertNode.java:251)	"master_1117_92c6a57
1. 启动3副本3C3D集群
2. benchmark创建元数据，写入数据
3.（ip62） datanode ERROR ，所有的数据写入失败（预期每个序列写入10个点，共5000万序列）：
{color:red}*2022-11-17 16:32:59,456 [pool-26-IoTDB-ClientRPC-Processor-35$20221117_083256_00512_3] ERROR o.a.i.d.m.p.s.FragmentInstanceDispatcherImpl:123 - [DispatchFailed]
java.lang.NullPointerException: null
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.write.InsertNode.selfCheckDataTypes(InsertNode.java:251)*{color}
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.write.InsertTabletNode.validateAndSetSchema(InsertTabletNode.java:201)
        at org.apache.iotdb.db.mpp.plan.analyze.SchemaValidator.validate(SchemaValidator.java:64)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.executeDataInsert(RegionWriteExecutor.java:191)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.visitInsertTablet(RegionWriteExecutor.java:163)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.visitInsertTablet(RegionWriteExecutor.java:117)
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.write.InsertTabletNode.accept(InsertTabletNode.java:1086)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor.execute(RegionWriteExecutor.java:83)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchLocally(FragmentInstanceDispatcherImpl.java:232)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:137)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchWriteSync(FragmentInstanceDispatcherImpl.java:119)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatch(FragmentInstanceDispatcherImpl.java:90)
        at org.apache.iotdb.db.mpp.plan.scheduler.ClusterScheduler.start(ClusterScheduler.java:106)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.schedule(QueryExecution.java:287)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.start(QueryExecution.java:205)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:150)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:164)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.insertTablet(ClientRPCServiceImpl.java:1234)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:4078)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:4058)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

复现流程
1. 192.168.10.62/66/68  72C256GB   3C3D
ConfigNode配置文件：
MAX_HEAP_SIZE=""8G""
cn_connection_timeout_ms=3600000

DataNode配置文件：
MAX_HEAP_SIZE=""192G""
MAX_DIRECT_MEMORY_SIZE=""32G""

Common配置文件：
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
data_replication_factor=3
connection_timeout_ms=3600000
max_connection_for_internal_service=1100
enable_timed_flush_seq_memtable=true
seq_memtable_flush_interval_in_ms=3600000
seq_memtable_flush_check_interval_in_ms=600000
enable_timed_flush_unseq_memtable=true
unseq_memtable_flush_interval_in_ms=3600000
unseq_memtable_flush_check_interval_in_ms=600000
max_waiting_time_when_insert_blocked=3600000
query_timeout_threshold=36000000
enable_auto_create_schema=false
schema_region_ratis_request_timeout_ms=3600000

2. benchmark配置见附件
运行，查看ip62 datanode 日志。"	IOTDB	Closed	3	1	6861	pull-request-available
13514726	Missing schemaTemplateId in SchemaRegion snapshot	"Currently, the schemaTemplateId of device node is not stored in schemaRegion snapshot, which results in failure schema query and fetch after DataNode restart.

Add template id to schema region snapshot"	IOTDB	Closed	3	1	6861	pull-request-available
13432243	Add SchemaEngine above the MManager	"In 0.13, we have a global singleton MManager for all schemas in IoTDB.

 

Since 0.14, we need to degradation the MManager for each storage group. And they are managed by a global SchemaEngine, which holds a field of Map<storage_group(String), MManager>.

This field will be leveraged in the new cluster, each MManager holds a partition of schema in one storage group.

The field will be Map<schema_partition_Id(String), MManager> in each DataNode."	IOTDB	Closed	3	4	6861	pull-request-available
13440130	After enabling persistence,  report that MNode has not been pinned when writing datas by benchmark 	"step 1:  Modify parameters in the iotdb-engine.properties
{code:java}
schema_engine_mode=Schema_File {code}
step 2：start iotdb
step 3: Change the benchmark running mode to generate a dataset，running benchmark
{code:java}
BENCHMARK_WORK_MODE=generateDataMode 
LOOP=1000
BIG_BATCH_SIZE=100
GROUP_NUMBER=10
DEVICE_NUMBER=100
SENSOR_NUMBER=100
BATCH_SIZE_PER_WRITE=100{code}
step 4: Change the benchmark running mode again，running benchmark
{code:java}
BENCHMARK_WORK_MODE=verificationWriteMode {code}
step 5：check log

then view error:

!image-2022-04-18-15-55-02-466.png!

 "	IOTDB	Closed	3	1	6861	pull-request-available
13424264	[IoTDB][0.13.0] NPE: executeNonQueryPlan failed	"version: 0.13.0

commitId: 5c2aa6b6df12ccffd1ec61648e6396919ed35eb3

 
{code:java}
2022-01-24 10:30:01,121 [pool-83-IoTDB-RPC-Client-349] ERROR o.a.i.d.u.ErrorHandlingUtils:59 - Status code: INTERNAL_SERVER_ERROR(500), operation: executeNonQueryPlan failed 
java.lang.NullPointerException: null
        at org.apache.iotdb.db.metadata.mtree.MTree.createTimeseries(MTree.java:335)
        at org.apache.iotdb.db.metadata.MManager.createTimeseries(MManager.java:577)
        at org.apache.iotdb.db.metadata.MManager.createTimeseries(MManager.java:558)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.createTimeSeries(PlanExecutor.java:1744)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.processNonQuery(PlanExecutor.java:298)
        at org.apache.iotdb.db.service.basic.StandaloneServiceProvider.executeNonQuery(StandaloneServiceProvider.java:53)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeNonQueryPlan(TSServiceImpl.java:2001)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.createTimeseries(TSServiceImpl.java:1657)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$createTimeseries.getResult(TSIService.java:3093)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$createTimeseries.getResult(TSIService.java:3073)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
2022-01-24 10:37:00,985 [pool-83-IoTDB-RPC-Client-373] ERROR o.a.i.d.u.ErrorHandlingUtils:59 - Status code: INTERNAL_SERVER_ERROR(500), operation: executeNonQueryPlan failed 
java.lang.NullPointerException: null
        at org.apache.iotdb.db.metadata.mtree.MTree.getMountedNodeIndexOnMeasurementPath(MTree.java:1646)
        at org.apache.iotdb.db.metadata.MManager.getSeriesSchemasAndReadLockDevice(MManager.java:1956)
        at org.apache.iotdb.db.engine.StorageEngine.getSeriesSchemas(StorageEngine.java:1048)
        at org.apache.iotdb.db.engine.StorageEngine.insertTablet(StorageEngine.java:644)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.insertTablet(PlanExecutor.java:1657)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.insertTabletSerial(PlanExecutor.java:1572)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.insertTablet(PlanExecutor.java:1560)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.processNonQuery(PlanExecutor.java:276)
        at org.apache.iotdb.db.service.basic.StandaloneServiceProvider.executeNonQuery(StandaloneServiceProvider.java:53)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeNonQueryPlan(TSServiceImpl.java:2001)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.insertTabletsInternally(TSServiceImpl.java:1590)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.insertTablets(TSServiceImpl.java:1540)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$insertTablets.getResult(TSIService.java:3293)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$insertTablets.getResult(TSIService.java:3273)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
2022-01-24 10:40:17,492 [pool-83-IoTDB-RPC-Client-219] ERROR o.a.i.d.u.ErrorHandlingUtils:59 - Status code: INTERNAL_SERVER_ERROR(500), operation: executeNonQueryPlan failed 
java.lang.NullPointerException: null
        at org.apache.iotdb.db.metadata.mtree.MTree.getNodeByPathWithStorageGroupCheck(MTree.java:1406)
        at org.apache.iotdb.db.metadata.MManager$1.load(MManager.java:250)
        at org.apache.iotdb.db.metadata.MManager$1.load(MManager.java:245)
        at com.github.benmanes.caffeine.cache.LocalLoadingCache.lambda$newMappingFunction$2(LocalLoadingCache.java:141)
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2405)
        at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2403)
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2386)
        at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
        at com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:54)
        at org.apache.iotdb.db.metadata.MManager.getDeviceNodeWithAutoCreate(MManager.java:926)
        at org.apache.iotdb.db.metadata.MManager.getDeviceNodeWithAutoCreate(MManager.java:970)
        at org.apache.iotdb.db.metadata.MManager.getSeriesSchemasAndReadLockDevice(MManager.java:1970)
        at org.apache.iotdb.db.engine.StorageEngine.getSeriesSchemas(StorageEngine.java:1048)
        at org.apache.iotdb.db.engine.StorageEngine.insertTablet(StorageEngine.java:644)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.insertTablet(PlanExecutor.java:1657)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.insertTabletSerial(PlanExecutor.java:1572)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.insertTablet(PlanExecutor.java:1560)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.processNonQuery(PlanExecutor.java:276)
        at org.apache.iotdb.db.service.basic.StandaloneServiceProvider.executeNonQuery(StandaloneServiceProvider.java:53)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeNonQueryPlan(TSServiceImpl.java:2001)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.insertTabletsInternally(TSServiceImpl.java:1590)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.insertTablets(TSServiceImpl.java:1540)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$insertTablets.getResult(TSIService.java:3293)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$insertTablets.getResult(TSIService.java:3273)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
2022-01-24 10:51:21,208 [pool-83-IoTDB-RPC-Client-248] ERROR o.a.i.d.u.ErrorHandlingUtils:59 - Status code: INTERNAL_SERVER_ERROR(500), operation: executeNonQueryPlan failed 
java.lang.NullPointerException: null
        at org.apache.iotdb.db.metadata.mtree.MTree.createTimeseries(MTree.java:335)
        at org.apache.iotdb.db.metadata.MManager.createTimeseries(MManager.java:577)
        at org.apache.iotdb.db.metadata.MManager.createTimeseries(MManager.java:558)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.createTimeSeries(PlanExecutor.java:1744)
        at org.apache.iotdb.db.qp.executor.PlanExecutor.processNonQuery(PlanExecutor.java:298)
        at org.apache.iotdb.db.service.basic.StandaloneServiceProvider.executeNonQuery(StandaloneServiceProvider.java:53)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeNonQueryPlan(TSServiceImpl.java:2001)
        at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.createTimeseries(TSServiceImpl.java:1657)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$createTimeseries.getResult(TSIService.java:3093)
        at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$createTimeseries.getResult(TSIService.java:3073)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834) {code}
 "	IOTDB	Closed	2	1	6861	0.13.0, pull-request-available
13439779	Persistent implementation of new feature metadata does not match the design documentation	" 

Q1：The detailed description of the Rocksdb_based operating mode is not mentioned in the design document.
Q2：The persistence is turned on for the first time, and the memory mode is changed again. There is no abnormality in the startup system, and its implementation does not match the design document.
!image-2022-04-15-15-22-08-489.png!

 "	IOTDB	Closed	3	1	6861	pull-request-available
13516604	Refactor Count TimeSeries Group By Level based on SchemaReader	"Currently, the LevelTimeSeriesCountOperator, serving feature ""count timeseries group by level"", is implemented by invoking SchemaRegion interfaces directly, which is unable to be interrupt and brings burden to SchemaRegion business logic.

Since we want SchemaRegion to only provide the basic SchemaReader, we refactor the implementation of LevelTimeSeriesCountOperator to extract the group by and counting logic from SchemaRegion. Based on SchemaReader, it is more flexible to interrupt the process and release resources."	IOTDB	Closed	3	4	6861	pull-request-available
13515304	Incorrect result of PatternTree.getAllDevicePatterns	"The device patterns of timeseries pattern root.sg.d.\*\* are root.sg.d and root.sg.d.\*\*, while current result of PatternTree.getAllDevicePatterns is only root.sg.d.\*\*.

This may cause residual data after data deletion, since some devices are missed out due to this bug. 
For example, the data of root.sg.d.s may residual when executing ""delete data from root.sg.d.**"" and the data of root.sg.d.a.s will be deleted successfully."	IOTDB	Closed	3	1	6861	pull-request-available
13450951	[cluster]Got a wrong result data set when using 'show child nodes root'	"Reproduce steps:
 # Setup a cluster with 3C3D
 # create timeseries like below picture
 # using 'show child nodes root' , got a wrong result data set like below:

!image-2022-06-20-17-49-51-365.png!"	IOTDB	Closed	3	1	6861	cluster, pull-request-available
13490986	[Schema Creation][DispatchFailed]  java.lang.IndexOutOfBoundsException: Index: 3, Size: 3	"master_1023_2fea011
schema region : ratis
data region : multiLeader

3rep , 3C3D . 
benchmark：
IS_DELETE_DATA=false
CREATE_SCHEMA=true

benchmark runs the metadata creation operation twice, (ip62） datanode ERROR LOG :
2022-10-25 09:53:12,210 [pool-26-IoTDB-ClientRPC-Processor-8$20221025_015312_00103_3] {color:#DE350B}*ERROR o.a.i.d.m.p.s.FragmentInstanceDispatcherImpl:123 - [DispatchFailed]
java.lang.IndexOutOfBoundsException: Index: 3, Size: 3*{color}
        at java.util.ArrayList.rangeCheck(ArrayList.java:659)
        at java.util.ArrayList.remove(ArrayList.java:498)
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.write.MeasurementGroup.removeMeasurement(MeasurementGroup.java:122)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.visitCreateMultiTimeSeries(RegionWriteExecutor.java:349)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor$WritePlanNodeExecutionVisitor.visitCreateMultiTimeSeries(RegionWriteExecutor.java:113)
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.write.CreateMultiTimeSeriesNode.accept(CreateMultiTimeSeriesNode.java:141)
        at org.apache.iotdb.db.mpp.execution.executor.RegionWriteExecutor.execute(RegionWriteExecutor.java:79)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchLocally(FragmentInstanceDispatcherImpl.java:232)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:137)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchWriteSync(FragmentInstanceDispatcherImpl.java:119)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatch(FragmentInstanceDispatcherImpl.java:90)
        at org.apache.iotdb.db.mpp.plan.scheduler.ClusterScheduler.start(ClusterScheduler.java:102)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.schedule(QueryExecution.java:277)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.start(QueryExecution.java:195)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:146)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:160)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.createMultiTimeseries(ClientRPCServiceImpl.java:467)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$createMultiTimeseries.getResult(IClientRPCService.java:3387)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$createMultiTimeseries.getResult(IClientRPCService.java:3367)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Test environment ：
1. 192.168.10.62 / 66 /68 72CPU 256GB
benchmark : ip64 /data/liuzhen_test/weektest/benchmark_tool

ConfigNode
MAX_HEAP_SIZE=""8G""
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
data_replication_factor=3
partition_region_ratis_request_timeout_ms = 1200000
schema_region_ratis_request_timeout_ms = 1200000
data_region_ratis_request_timeout_ms = 1200000
partition_region_ratis_max_retry_attempts = 1
schema_region_ratis_max_retry_attempts = 1
data_region_ratis_max_retry_attempts = 1

DataNode
MAX_HEAP_SIZE=""192G""
MAX_DIRECT_MEMORY_SIZE=""32G""
query_timeout_threshold=36000000

2. benchmark configuration
see attachment .
Run twice

3. View ip62 datanode logs"	IOTDB	Closed	4	1	6861	pull-request-available
13402853	Slow creation of timeseries with tag	"Problem:

When user creates a timeseries with tag or attribute, the tag/attribute record will be persisted to TagLogFile.

Currently, every TagLogFile writing operation will be force flushed to  disk, and too many disk I/O will cause poor performance.

A example case is that, one timeseries creation costs 48ms and around 8000 timeseries with tag creation costs 5-6min.

 

Solution:

Set the force flush mode of FileChannel to false and execute force flush every N writing operations. N is an customized interval value that user can config."	IOTDB	Closed	3	4	6861	pull-request-available
13445295	Batch creation of timeseries	"Implement batch creation to speed up timeseries creation.

Supporting creating batch timeseries of one device or multi device"	IOTDB	Closed	3	4	6861	pull-request-available
13342374	support Path with **	"root.sg.** could match

 

root.sg

root.sg.*

root.sg.*.*

...

 

This is helpful when integrating with Promethues"	IOTDB	Closed	3	2	6861	pull-request-available
13401830	MManager slow recover with tag 	"IoTDB support tagIndex in memory for tag based timeseries query.

Current tagIndex is based on CopyOnWriteArraySet and the time complexity to construct each set is O(n2).

An existing case is that the time for recovering tagIndex for 50000 timeseries with tags costs 5min or even more, based on working environment.

We should use a more effective way to construct tagIndex."	IOTDB	Closed	3	4	6861	pull-request-available
13486556	create storage group failed with 303 but it exists at the result of show storage group	"create storage group failed with 303 but it exists at the result of show storage group

命令行窗口创建下面值的storage group, 创建后报错303（具体日志信息见附件），但是show storage group 可以看到刚刚创建失败的这个storage group.

 

storage group: 

root.level2.level3.level4.level5.level6.level7.level8.level9.level10.level11.level12.level13.level14.level15.level16.level17.level18.level19.level20.level21.level22.level23.level24.level25.level26.level27.level28.level29.level30.level31.level32.level33.level34.level35.level36.level37.level38.level39.level40.level41.level42.level43.level44.level45.level46.level47.level48.level49.level50.level51.level52.level53.level54.level55.level56.level57.level58.level59.level60.level61.level62.level63.level64.level65.level66.level67.level68.level69.level70.level71.level72.level73.level74.level75.level76.level77.level78.level79.level80.level81.level82.level83.level84.level85.level86.level87.level88.level89.level90.level91.level92.level93.level94.level95.level96.level97.level98.level99.level100.level101.level102.level103.level104.level105.level106.level107.level108.level109.level110.level111.level112.level113.level114.level115.level116.level117.level118.level119.level120.level121.level122.level123.level124.level125.level126.level127.level128.level129.level130.level131.level132.level133.level134.level135.level136.level137.level138.level139.level140.level141.level142.level143.level144.level145.level146.level147.level148.level149.level150.level151.level152.level153.level154.level155.level156.level157.level158.level159.level160.level161.level162.level163.level164.level165.level166.level167.level168.level169.level170.level171.level172.level173.level174.level175.level176.level177.level178.level179.level180.level181

 

environment:

1. git pull origin master

2. mvn clean package -pl server -am -DskipTests

3. iotdb/server/target/iotdb-server-0.14.0-SNAPSHOT/sbin/start-server.sh

2022-10-17  172.20.70.44

 

failed info:

!image-2022-10-17-15-32-41-422.png!

 

reproduction:
 # build the newest source code of master branch, start-server.sh
 # start-cli.sh
 # show storage group: no such storage group with upstairs value
 # create storage group with the value upstairs, failed
 # show storage group, it shows the storage group with upstairs value

 

说明：

使用java原生接口测试，发现该问题。

然后使用命令行测试，复现。

在0.13.3版本上复测，没有该问题。

*在分布式3C3D上，没有这个问题。可以创建成功，可以查询成功。*

建议：
linux对文件名长度有限制，ext2~ext4是255 bytes, ntfs是255字符。路径长度4096 bytes
或许应该全面考虑名称长度问题。可以检查用户指定的data路径，可以限制storage group本身长度等。"	IOTDB	Closed	4	1	6861	pull-request-available
13514144	Correct CountMergeOperatorTest SchemaCountOperatorTest and SchemaQueryScanOperatorTest	Change these two to correct UT style.	IOTDB	Closed	3	3	6861	pull-request-available
13450884	Adjust the capacity of SchemaCache according to schema memory	"In the new cluster version, we have a SchemaCahce module, which acts as the MManager in the previous Standalone version.

 

Currently, the number of cached entries in SchemaCache is 10000, this will lead to a lot of cache miss when we write larger than 10000 series one by one.

 

Note that we have a parameter write_read_schema_free_memory_proportion=4:3:1:2 in iotdb-engine.properties.

 

We could use the schema memory to store schema and partition info in DataNode."	IOTDB	Closed	3	4	6861	pull-request-available
13498263	Default configuration, the data query report is abnormal	"Start 1C1D, then enter CLI to execute SQL.
When executing data query, the empty table is queried for the first time, and an exception of 500 is reported for the second and third times.
The SQL is:
{code:java}
create schema template t2 aligned (lat FLOAT encoding=Gorilla, lon FLOAT encoding=Gorilla);
create storage group root.sg1;
set schema template t2 to root.sg1.d2;
create timeseries of schema template on root.sg1.d2;
insert into root.sg1.d2(time, lat, lon) aligned values(1, 1, 1);
insert into root.sg1.d2(time, lat, lon) aligned values(2, 2, 0), (3, 3, 1);
select * from root.sg1.d2;

create timeseries root.sg1.d2.others.ver WITH DATATYPE=TEXT,ENCODING=PLAIN,COMPRESSOR=UNCOMPRESSED;
insert into root.sg1.d2.others(time, ver) values(1, 'v1');
insert into root.sg1.d2.others(time, ver) values(2, 'v2'), (3, 'v1'); 
select ** from root.sg1.d2; {code}

The log is:
!image-2022-11-07-17-12-42-896.png|width=554,height=229!"	IOTDB	Closed	3	1	6861	pull-request-available
13421586	Some contents in the user guide need to be updated	"Master  and V0.12.X need to update:

!image-2022-01-10-11-05-10-191.png!

!image-2022-01-10-11-04-47-099.png!

!image-2022-01-10-11-06-28-558.png!

 "	IOTDB	Closed	5	1	6861	0.13.0, pull-request-available
13511012	Optimize schema fetch in concurrent scenarios	When there're concurrent requests, tending to launch schema fetch due to schema cache miss, the tasks, fetching same schema, shall not be executed repeatedly, and only one of them need to be executed.	IOTDB	In Progress	3	4	6861	pull-request-available
13482144	[ MultiLeaderConsensus ] Very low write performance at 100k timeseries per dev	"m_0918_a28329e
multiLeader，{color:#DE350B}*在单个设备包含的序列较多时，写入性能变得非常慢*{color}。
1用户，1dev，10w序列，batch size=2，LOOP=10，即每序列写入20个点，相同数据库配置
与新单机的性能对比：
 !image-2022-09-19-18-08-27-560.png! 

测试环境
1. 192.168.10.71/72/73/74   48核384G
bm在ip71
confignode配置参数
MAX_HEAP_SIZE=""8G""
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
data_replication_factor=3
connection_timeout_ms=3600000

DataNode配置
MAX_HEAP_SIZE=""256G""
MAX_DIRECT_MEMORY_SIZE=""32G""
max_connection_for_internal_service=300
enable_timed_flush_seq_memtable=true
seq_memtable_flush_interval_in_ms=3600000
seq_memtable_flush_check_interval_in_ms=600000
enable_timed_flush_unseq_memtable=true
unseq_memtable_flush_interval_in_ms=3600000
unseq_memtable_flush_check_interval_in_ms=600000
max_deduplicated_path_num=1000000
max_waiting_time_when_insert_blocked=3600000
query_timeout_threshold=36000000

2. bm配置见附件"	IOTDB	Closed	1	1	6861	pull-request-available
13423914	[0.13.0][upgrade] upgrade to 0.13 fail to parse 0.12 schema snapshot 	"With 0.12 schema snapshot, 0.13 start fail which report snapshot corrupted.

0.13.0 log:

!image-2022-01-21-14-54-11-874.png!

 

0.12 log:

!image-2022-01-21-14-54-53-984.png!"	IOTDB	Closed	1	1	6861	0.13.0, pull-request-available, schema
13470024	show timeseries blocked and OOM	" !screenshot-1.png! 

一直没有返回，数据量多的时候，需要分页，不能直接抛出一个null或内存oom异常"	IOTDB	Closed	2	1	6861	pull-request-available
13439077	Schema Recover Test bug in non and partial memory mode 	"https://github.com/apache/iotdb/runs/5982679834?check_suite_focus=true

!image-2022-04-12-11-52-36-842.png!

https://github.com/apache/iotdb/runs/5970756756?check_suite_focus=true

!image-2022-04-12-11-53-12-378.png!"	IOTDB	Closed	3	1	6861	pull-request-available
13476290	Avoid predicting data type for each insertStringRecord request	"Currently, once we insert data by insertStringRecord api or insert sql statement, we need to predict the data type firstly and then do the schema validation. However we can combine these steps to avoid predicting data type for each insert.  

 

When validate the schema of an InsertString request, if the timeseries existed, we can fetch the schema directly without predicting data type. If not, we can do the type prediction for string value here and then auto create the timeseries."	IOTDB	Closed	3	4	6861	pull-request-available
13439375	Fail to get last time value during show latest timeseries	!image-2022-04-13-16-10-04-028.png!	IOTDB	Closed	3	1	6861	pull-request-available
13499135	[template]Delete timeseries using the template, and its msg needs to be optimized	"Create a metadata template, set and create it, then delete the timeseries. The message is not clear and needs to be optimized.
 The current prompt is as follows:

 
{code:java}
IoTDB> show nodes in schema template  t1;
+-----------+--------+--------+-----------+
|child nodes|dataType|encoding|compression|
+-----------+--------+--------+-----------+
|temperature|   FLOAT|     RLE|     SNAPPY|
|     status| BOOLEAN|   PLAIN|     SNAPPY|
+-----------+--------+--------+-----------+
Total line number = 2
It costs 0.002s
IoTDB> show paths using schema template t1;
+-----------+
|      paths|
+-----------+
|root.sg1.d1|
+-----------+
Total line number = 1
It costs 0.005s

IoTDB> show timeseries;
+-----------------------+-----+-------------+--------+--------+-----------+----+----------+
|             timeseries|alias|storage group|dataType|encoding|compression|tags|attributes|
+-----------------------+-----+-------------+--------+--------+-----------+----+----------+
|root.sg1.d1.temperature| null|     root.sg1|   FLOAT|     RLE|     SNAPPY|null|      null|
|     root.sg1.d1.status| null|     root.sg1| BOOLEAN|   PLAIN|     SNAPPY|null|      null|
+-----------------------+-----+-------------+--------+--------+-----------+----+----------+
Total line number = 2
It costs 0.006s
IoTDB> delete timeseries root.sg1.d1.temperature;
Msg: 304: Path [root.sg1.d1.temperature] does not exist {code}
 

 "	IOTDB	Closed	3	4	6861	pull-request-available
13432668	Separate CQ persistence from MLog	The plan of CQ should be managed individually rather than stored in mlog.	IOTDB	Closed	3	4	6861	pull-request-available
13451422	Support modify template	"# add measurement
 # delete measurement, notice that if the template is activated on some path, the data should be considered"	IOTDB	In Progress	4	4	6861	pull-request-available
13472360	In the case that the template is used, the latest query result is incorrect	"使用模板的情况下，最新点查询查询结果不正确，开启了最新点缓存.

场景如下:

创建存储组：root.clsu1

创建模板： t1(s1,int32),

该模板挂载到root.clsu1

插入一个实体和物理量

insert into root.clsu1.device1(time,s1) values (220,200)

插入另一个实体和物理量

insert into root.clsu1.device2(time,s1) values (320,200)

查询最新点数据，发现结果为device1的数据

select last s1 from root.clsu1.device2

 

结果示例：

IoTDB> select last s1 from root.clsu1.device1
{+}----------------------------{-}{-}{+}-------------------{-}++{-}---{-}{-}-------+
|                        Time|          timeseries|value|dataType|

{+}----------------------------{-}{-}{+}-------------------{-}++{-}---{-}{-}-------+
|1970-01-01T08:00:00.220+08:00|root.clsu1.device1.s1| 200|  INT32|

{+}----------------------------{-}{-}{+}-------------------{-}++{-}---{-}{-}-------+
Total line number = 1
It costs 0.007s
IoTDB> select s1 from root.clsu1.device2
{+}----------------------------{-}{-}{+}--------------------+
|                        Time|root.clsu1.device2.s1|

{+}----------------------------{-}{-}{+}--------------------+
|1970-01-01T08:00:00.320+08:00|                 200|

{+}----------------------------{-}{-}{+}--------------------+
Total line number = 1
It costs 0.012s
IoTDB> select last s1 from root.clsu1.device2
{+}----------------------------{-}{-}{+}-------------------{-}++{-}---{-}{-}-------+
|                        Time|          timeseries|value|dataType|

{+}----------------------------{-}{-}{+}-------------------{-}++{-}---{-}{-}-------+
|1970-01-01T08:00:00.220+08:00|root.clsu1.device2.s1| 200|  INT32|

{+}----------------------------{-}{-}{+}-------------------{-}++{-}---{-}{-}-------+"	IOTDB	Closed	3	1	6861	pull-request-available
13523695	Eliminate useless log during concurrent auto create schema	When there're concurrent schema auto creation, some logs, revealing the repeated creation of already existing timeseries, will be printed, which is useless and costs low performance.	IOTDB	Closed	3	4	6861	pull-request-available
13522034	[session]insert data failed to timeseries using aligned template when the data schema is more than template define, MetadataException: nullnull	"build:
master branch
commit 315d6526fae06b634e99a4dbe0e3a3127163c8f1
Date:   Sun Jan 29 23:58:36 2023 +0800
    Use gitbox to accelerate the picture loading (#8918)

reproduction:
1. start a standalone service with default config
2. execute the java test ng codes . notice this method of testUpdateAfterAddTS_alignedTemp 

notice:
the inserting data schema is more than template defied: appendFloat 

datanode error log:
{code}
2023-01-30 17:56:39,584 [1@group-0002000000C6-LeaderElection13] INFO  o.a.r.s.r.s.SegmentedRaftLogWorker:452 - 1@group-0002000000C6-SegmentedRaftLogWorker: Starting segment from index:0 
2023-01-30 17:56:39,587 [1@group-0002000000C6-LeaderElection13] INFO  o.a.r.s.impl.ServerState:430 - 1@group-0002000000C6: set configuration 0: peers:[1|rpc:127.0.0.1:10750|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null 
2023-01-30 17:56:39,588 [1@group-0002000000C6-SegmentedRaftLogWorker] INFO  o.a.r.s.r.s.SegmentedRaftLogWorker$StartLogSegment:656 - 1@group-0002000000C6-SegmentedRaftLogWorker: created new log segment /Users/changxue/work/soft/apache-iotdb-1.1.0-SNAPSHOT-all-bin/sbin/./../data/datanode/consensus/schema_region/47474747-4747-4747-4747-0002000000c6/current/log_inprogress_0 
2023-01-30 17:56:39,722 [pool-25-IoTDB-ClientRPC-Processor-8$20230130_080023_01035_1.1.0] INFO  o.a.i.d.e.s.TsFileProcessor:193 - create a new tsfile processor /Users/changxue/work/soft/apache-iotdb-1.1.0-SNAPSHOT-all-bin/sbin/./../data/datanode/data/sequence/root.blendAddTS/197/2759/1675072599720-1-0-0.tsfile 
2023-01-30 17:56:39,821 [Query-Worker-Thread-1$20230130_080023_01041_1.1.0.0] ERROR o.a.i.d.m.e.o.s.SchemaFetchScanOperator:92 - Error occurred during execute SchemaFetchOperator 3 
org.apache.iotdb.commons.exception.MetadataException: nullnull
	at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.traverse(Traverser.java:95)
	at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.fetchSchema(MTreeBelowSGMemoryImpl.java:648)
	at org.apache.iotdb.db.metadata.schemaregion.SchemaRegionMemoryImpl.fetchSchema(SchemaRegionMemoryImpl.java:841)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.fetchSchema(SchemaFetchScanOperator.java:116)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:90)
	at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchMergeOperator.next(SchemaFetchMergeOperator.java:76)
	at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.processInternal(Driver.java:204)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.lambda$processFor$1(Driver.java:136)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.tryWithLock(Driver.java:291)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.processFor(Driver.java:117)
	at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskThread.execute(DriverTaskThread.java:69)
	at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:73)
2023-01-30 17:56:39,826 [Query-Worker-Thread-1$20230130_080023_01041_1.1.0.0] WARN  o.a.i.d.m.e.s.AbstractDriverThread:79 - [ExecuteFailed] 
java.lang.RuntimeException: org.apache.iotdb.commons.exception.MetadataException: nullnull
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:93)
	at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchMergeOperator.next(SchemaFetchMergeOperator.java:76)
	at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.processInternal(Driver.java:204)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.lambda$processFor$1(Driver.java:136)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.tryWithLock(Driver.java:291)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.processFor(Driver.java:117)
	at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskThread.execute(DriverTaskThread.java:69)
	at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:73)
Caused by: org.apache.iotdb.commons.exception.MetadataException: nullnull
	at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.traverse(Traverser.java:95)
	at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.fetchSchema(MTreeBelowSGMemoryImpl.java:648)
	at org.apache.iotdb.db.metadata.schemaregion.SchemaRegionMemoryImpl.fetchSchema(SchemaRegionMemoryImpl.java:841)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.fetchSchema(SchemaFetchScanOperator.java:116)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:90)
	... 9 common frames omitted
2023-01-30 17:56:39,830 [pool-25-IoTDB-ClientRPC-Processor-8] WARN  o.a.i.d.u.ErrorHandlingUtils:89 - Status code: 507, Query Statement: ""select appendFloat from root.blendAddTS.alignedUsingTemp where time=1669109508000;"". executeStatement failed 
java.lang.RuntimeException: Fetch Schema failed. 
	at org.apache.iotdb.db.mpp.plan.analyze.schema.ClusterSchemaFetchExecutor.executeSchemaFetchQuery(ClusterSchemaFetchExecutor.java:173)
	at org.apache.iotdb.db.mpp.plan.analyze.schema.ClusterSchemaFetchExecutor.fetchSchemaOfPreciseMatch(ClusterSchemaFetchExecutor.java:98)
	at org.apache.iotdb.db.mpp.plan.analyze.schema.ClusterSchemaFetcher.fetchSchema(ClusterSchemaFetcher.java:142)
	at org.apache.iotdb.db.mpp.plan.analyze.schema.ClusterSchemaFetcher.fetchSchema(ClusterSchemaFetcher.java:51)
	at org.apache.iotdb.db.mpp.plan.analyze.AnalyzeVisitor.visitQuery(AnalyzeVisitor.java:225)
	at org.apache.iotdb.db.mpp.plan.analyze.AnalyzeVisitor.visitQuery(AnalyzeVisitor.java:177)
	at org.apache.iotdb.db.mpp.plan.statement.crud.QueryStatement.accept(QueryStatement.java:523)
	at org.apache.iotdb.db.mpp.plan.statement.StatementVisitor.process(StatementVisitor.java:113)
	at org.apache.iotdb.db.mpp.plan.analyze.Analyzer.analyze(Analyzer.java:48)
	at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.analyze(QueryExecution.java:263)
	at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.<init>(QueryExecution.java:147)
	at org.apache.iotdb.db.mpp.plan.Coordinator.createQueryExecution(Coordinator.java:107)
	at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:140)
	at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementInternal(ClientRPCServiceImpl.java:217)
	at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementV2(ClientRPCServiceImpl.java:411)
	at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeQueryStatementV2(ClientRPCServiceImpl.java:401)
	at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeQueryStatementV2.getResult(IClientRPCService.java:3459)
	at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeQueryStatementV2.getResult(IClientRPCService.java:3439)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
	at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.iotdb.commons.exception.IoTDBException: java.lang.RuntimeException: org.apache.iotdb.commons.exception.MetadataException: nullnull
	at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.dealWithException(QueryExecution.java:445)
	at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getResult(QueryExecution.java:428)
	at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getBatchResult(QueryExecution.java:454)
	at org.apache.iotdb.db.mpp.plan.analyze.schema.ClusterSchemaFetchExecutor.executeSchemaFetchQuery(ClusterSchemaFetchExecutor.java:171)
	... 23 common frames omitted
Caused by: java.lang.RuntimeException: org.apache.iotdb.commons.exception.MetadataException: nullnull
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:93)
	at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchMergeOperator.next(SchemaFetchMergeOperator.java:76)
	at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.processInternal(Driver.java:204)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.lambda$processFor$1(Driver.java:136)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.tryWithLock(Driver.java:291)
	at org.apache.iotdb.db.mpp.execution.driver.Driver.processFor(Driver.java:117)
	at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskThread.execute(DriverTaskThread.java:69)
	at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:73)
Caused by: org.apache.iotdb.commons.exception.MetadataException: nullnull
	at org.apache.iotdb.db.metadata.mtree.traverser.Traverser.traverse(Traverser.java:95)
	at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.fetchSchema(MTreeBelowSGMemoryImpl.java:648)
	at org.apache.iotdb.db.metadata.schemaregion.SchemaRegionMemoryImpl.fetchSchema(SchemaRegionMemoryImpl.java:841)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.fetchSchema(SchemaFetchScanOperator.java:116)
	at org.apache.iotdb.db.mpp.execution.operator.schema.SchemaFetchScanOperator.next(SchemaFetchScanOperator.java:90)
	... 9 common frames omitted
{code}"	IOTDB	Closed	3	1	6861	pull-request-available
13423036	root is not a legal path	!截屏2022-01-17 20.19.02.png!	IOTDB	Closed	3	1	6861	0.13.0
13446258	Avoid double-writing of the write ahead log for metadata under RatisConsensus	"Reprodece step:

1.set up a cluster with 3C3D ；

2.using below sql to test it:

3.stop a datanode

4.start the datanode again,then we got below error in log 

2022-05-23 07:17:24,419 [172.20.70.24-40010@group-000200000000-StateMachineUpdater] ERROR o.a.i.d.m.v.SchemaExecutionVisitor:59 - IoTDB: MetaData error:
org.apache.iotdb.db.exception.metadata.PathAlreadyExistException: Path [root.sg.d.s2] already exist

!image-2022-05-23-07-17-52-921.png!

 

Expect: there is no error msg"	IOTDB	Closed	3	4	6861	pull-request-available
13524411	[Template]The timeseries with the same device path as the template cannot be deleted	"master branch

 commit adf55e896ef55220785dcf72564cbb8a0e5a2280
First of all create a template, then mount the template, activate and insert data;
Then create other timeseries under the same device and insert the data
Finally, delete the newly created time series, which cannot be deleted.
{code:java}
create schema template t1 (temperature FLOAT encoding=RLE, status BOOLEAN encoding=PLAIN compression=SNAPPY);
show schema templates;
create database root.sg1;

set schema template t1 to root.sg1.d1;
show paths set schema template t1;

create timeseries of schema template on root.sg1.d1;
show paths using schema template t1;
show timeseries root.sg1.**;
show devices root.sg1.**;
show timeseries;

insert into root.sg1.d1(time, temperature, status) values(1, 1, 1);
insert into root.sg1.d1(time, temperature, status) values(2, 2, 0), (3, 3, 1);
select * from root.sg1.d1;

delete timeseries root.sg1.d1.temperature;

CREATE ALIGNED TIMESERIES root.sg1.d1.GPS(latitude FLOAT encoding=PLAIN compressor=SNAPPY, longitude FLOAT encoding=PLAIN compressor=SNAPPY);
show timeseries root.sg1.d1.**;

insert into root.sg1.d1.GPS(time, latitude, longitude) aligned values(1, 1, 1);
insert into root.sg1.d1.GPS(time, latitude, longitude) aligned values(2, 2, 2), (3, 3, 3);
select ** from root.sg1.d1;

delete timeseries root.sg1.d1.GPS.latitude;
show timeseries;
 {code}"	IOTDB	Closed	3	1	6861	pull-request-available
13506098	[schema] create timeseries occasionally return 906: There are no available SchemaRegionGroup RegionGroups currently	"[schema] create timeseries occasionally return 906: There are no available SchemaRegionGroup RegionGroups currently

3C3D cluster, Nov.27th source codes

The cluster just started up successfully and I'm sure all nodes are all right.

{code:sql}
IoTDB>create timeseries root.udf.exp1.s1 with datatype=int32
SQLSTATE : 906: There are no available SchemaRegionGroup RegionGroups currently, please use ""show cluster"" or ""show regions"" to check the cluster status
{code}

datanode log:
{code:java}
2022-11-27 09:54:31,464 [1@group-000200000000-StateMachineUpdater] INFO  o.a.i.d.m.s.SchemaRegionMemoryImpl:444 - Start create snapshot of schemaRegion SchemaRegion[0]
2022-11-27 09:54:31,464 [pool-26-IoTDB-ClientRPC-Processor-3] ERROR o.a.i.d.u.ErrorHandlingUtils:90 - Status code: 906, Query Statement: ""create timeseries root.udf.upload.d1.s1 with datatype=int32"". executeStatement failed
java.lang.RuntimeException: org.apache.iotdb.commons.exception.IoTDBException: There are no available SchemaRegionGroup RegionGroups currently, please use ""show cluster"" or ""show regions"" to check the cluster status
        at org.apache.iotdb.db.mpp.plan.analyze.ClusterPartitionFetcher.getOrCreateSchemaPartition(ClusterPartitionFetcher.java:146)
        at org.apache.iotdb.db.mpp.plan.analyze.AnalyzeVisitor.visitCreateTimeseries(AnalyzeVisitor.java:1363)
        at org.apache.iotdb.db.mpp.plan.analyze.AnalyzeVisitor.visitCreateTimeseries(AnalyzeVisitor.java:160)
        at org.apache.iotdb.db.mpp.plan.statement.metadata.CreateTimeSeriesStatement.accept(CreateTimeSeriesStatement.java:130)
        at org.apache.iotdb.db.mpp.plan.statement.StatementVisitor.process(StatementVisitor.java:107)
        at org.apache.iotdb.db.mpp.plan.analyze.Analyzer.analyze(Analyzer.java:43)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.analyze(QueryExecution.java:258)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.<init>(QueryExecution.java:142)
        at org.apache.iotdb.db.mpp.plan.Coordinator.createQueryExecution(Coordinator.java:104)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:137)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementInternal(ClientRPCServiceImpl.java:215)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementV2(ClientRPCServiceImpl.java:390)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeStatementV2.getResult(IClientRPCService.java:3453)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeStatementV2.getResult(IClientRPCService.java:3433)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.iotdb.commons.exception.IoTDBException: There are no available SchemaRegionGroup RegionGroups currently, please use ""show cluster"" or ""show regions"" to check the cluster status
        ... 20 common frames omitted
2022-11-27 09:54:31,466 [1@group-000200000000-StateMachineUpdater] INFO  o.a.i.d.m.s.SchemaRegionMemoryImpl:450 - MTree snapshot creation of schemaRegion SchemaRegion[0] costs 2ms.
{code}"	IOTDB	Closed	3	1	6861	pull-request-available
13486013	[ MetaData ] org.apache.iotdb.db.exception.metadata.MeasurementAlreadyExistException: Path [xx] already exist	"master_1010_9917053
schema region ： ratis
3副本
500dev，10万sensor/dev , enable_auto_create_schema=false
注册元数据，大量ERROR，部分dev没注册成功（ratis 超时？超时时间需可配置）
ERROR 信息：
2022-10-13 11:37:36,676 [192.168.10.62_50010@group-000200000001-StateMachineUpdater] ERROR o.a.i.d.m.v.SchemaExecutionVisitor:109 - IoTDB: MetaData error:
org.apache.iotdb.db.exception.metadata.MeasurementAlreadyExistException: Path [root.test.g_1.d_221.s_48893] already exist
        at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.createTimeseries(MTreeBelowSGMemoryImpl.java:218)
        at org.apache.iotdb.db.metadata.schemaregion.SchemaRegionMemoryImpl.createTimeseries(SchemaRegionMemoryImpl.java:622)
        at org.apache.iotdb.db.metadata.visitor.SchemaExecutionVisitor.visitCreateMultiTimeSeries(SchemaExecutionVisitor.java:106)
        at org.apache.iotdb.db.metadata.visitor.SchemaExecutionVisitor.visitCreateMultiTimeSeries(SchemaExecutionVisitor.java:63)
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.write.CreateMultiTimeSeriesNode.accept(CreateMultiTimeSeriesNode.java:141)
        at org.apache.iotdb.db.consensus.statemachine.SchemaRegionStateMachine.write(SchemaRegionStateMachine.java:74)
        at org.apache.iotdb.consensus.ratis.ApplicationStateMachineProxy.applyTransaction(ApplicationStateMachineProxy.java:135)
        at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1588)
        at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:239)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:182)
        at java.lang.Thread.run(Thread.java:748)

测试环境
1. 192.168.10.62/66/68  物理机  72CPU 256GB
bm在64
ConfigNode
MAX_HEAP_SIZE=""8G""
MAX_DIRECT_MEMORY_SIZE=""8G""
 schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
 data_replication_factor=3
connection_timeout_ms=1200000

DataNode
 MAX_HEAP_SIZE=""192G""
MAX_DIRECT_MEMORY_SIZE=""32G""

connection_timeout_ms=1200000
max_connection_for_internal_service=1100
 enable_timed_flush_seq_memtable=true
seq_memtable_flush_interval_in_ms=3600000
seq_memtable_flush_check_interval_in_ms=600000
enable_timed_flush_unseq_memtable=true
 unseq_memtable_flush_interval_in_ms=3600000
unseq_memtable_flush_check_interval_in_ms=600000
 max_waiting_time_when_insert_blocked=3600000
 query_timeout_threshold=36000000
 enable_auto_create_schema=false

2. bm配置见附件
"	IOTDB	Closed	3	1	6861	pull-request-available
13490515	[deleteStorageGroups]failed with 500 when contains empty string	"[deleteStorageGroups]failed with 500 when contains empty string

environment:

3C3D cluster

detail:

将一个空字符串 和 4个正常storage group  构成的 array list, 作为参数传入deleteStorageGroups, 执行后，如下：

 

!image-2022-10-24-10-42-56-262.png|width=866,height=498!"	IOTDB	Closed	4	1	6861	pull-request-available
13479194	"not expected result when do sql ""count timeseries root.**"""	"环境:集群版，1ConfigNode和1DataNode

!image-2022-08-30-08-08-27-150.png!

问题：
 # 执行count timeseries root.** 多次结果不一致
 # count没有统计出对应的数量

服务端日志见附件

iotdb_2022-08-30.0.log

操作示例如下图：

!image-2022-08-30-08-08-05-489.png!

 "	IOTDB	Resolved	3	1	6861	pull-request-available
13475708	[ raft log & MetaData ] raft log recovery should not redo MeasurementAlreadyExist operation 	"raft log恢复，会重做所有MetaData的写操作，对于失败的元数据写操作比如序列已存在：

 [192.168.10.62_50010@group-000200000000-StateMachineUpdater] ERROR o.a.i.d.m.v.SchemaExecutionVisitor:108 - IoTDB: MetaData error:
org.apache.iotdb.db.exception.metadata.MeasurementAlreadyExistException: Path [root.bm68_dn62.g_0.d_0.s_1] already exist

不需要重做。
完整的异常：
2022-08-08 17:46:29,045 [192.168.10.62_50010@group-000200000000-StateMachineUpdater] ERROR o.a.i.d.m.v.SchemaExecutionVisitor:108 - IoTDB: MetaData error:
org.apache.iotdb.db.exception.metadata.MeasurementAlreadyExistException: Path [root.bm68_dn62.g_0.d_0.s_2] already exist
        at org.apache.iotdb.db.metadata.mtree.MTreeBelowSGMemoryImpl.createTimeseries(MTreeBelowSGMemoryImpl.java:213)
        at org.apache.iotdb.db.metadata.schemaregion.SchemaRegionMemoryImpl.createTimeseries(SchemaRegionMemoryImpl.java:589)
        at org.apache.iotdb.db.metadata.visitor.SchemaExecutionVisitor.visitCreateMultiTimeSeries(SchemaExecutionVisitor.java:105)
        at org.apache.iotdb.db.metadata.visitor.SchemaExecutionVisitor.visitCreateMultiTimeSeries(SchemaExecutionVisitor.java:62)
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.metedata.write.CreateMultiTimeSeriesNode.accept(CreateMultiTimeSeriesNode.java:141)
        at org.apache.iotdb.db.consensus.statemachine.SchemaRegionStateMachine.write(SchemaRegionStateMachine.java:68)
        at org.apache.iotdb.consensus.ratis.ApplicationStateMachineProxy.applyTransaction(ApplicationStateMachineProxy.java:121)
        at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1588)
        at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:239)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:182)
        at java.lang.Thread.run(Thread.java:748)

复现流程
1. benchmark重复执行附件中的脚本
2.再重启恢复，datanode有error
"	IOTDB	Closed	3	4	6861	pull-request-available
13439582	MLog-parser-tool error while parsing CreateAlignedTimeseriesPlan	MLogTxtWriter will serialize wrong plan name and schema while paring  CreateAlignedTimeseriesPlan	IOTDB	Closed	3	4	6861	pull-request-available
13406313	TagIndex rebuild failure after upgrade MLog from mlog.txt to mlog.bin	When upgrade mlog from mlog.txt to mlog.bin, the data of tags and attributes are not read to the mlog.bin which results tagIndex rebuild failure. Thus add tagFile read operation while upgrade mlog.	IOTDB	Closed	3	1	6861	pull-request-available
13514656	[DOC] Add schema memory allocation upgrading guide	The schema memory allocation of 1.0 is different from that of 0.13x, and users should be reminded of this difference to avoid upgrade failure.	IOTDB	Closed	3	4	6861	pull-request-available
13517544	Accelerate auto create schema of multi devices	"The request of InsertRows and InsertTablets will launch auto create schema of multi devices.

Currently, the implementation is executing auto create schema of each device in sequence, which brings poor performance when there's many devices in one request.

Here are two improvements can be conducted:
1. implement schema batch auto creation as one mpp task
2. implement concurrent FI dispatch in mpp"	IOTDB	Closed	3	4	6861	pull-request-available
13449675	Optimize interface of SchemaFetcher for new Standalone	"We use PathPatternTree in SchemaFetcher to reduce the transmission in the new cluster, thus we need to convert a List<Path> to PathPatternTree.

 

However, this is not needed in the new Standalone version, we could change the SchemaFetcher interface to receive (String device, String[] measurements, TSDataType[] datatypes)."	IOTDB	Closed	3	4	6861	pull-request-available
13448505	[cluster]Support template in new cluster	"commit 565b14dbb104670b336bf928fdc9ebe3bf8608f0
Author: liuminghui233 <36565497+liuminghui233@users.noreply.github.com>
Date:   Mon Jun 6 09:22:32 2022 +0800

    [IOTDB-3383] Refactor calcInputLocationList() in LocalExecutionPlanner (#6152)

 

 

Reproduce steps：

1.Setup a cluster with 3C3D;

2.Using iotdb-cli to connect a data node;

3. Excute sql ""create schema template t1 (temperature FLOAT encoding=RLE, status BOOLEAN encoding=PLAIN compression=SNAPPY);""

got an error like below:

Msg: 400: [EXECUTE_STATEMENT_ERROR(400)] Exception occurred: checkAuthority failed. null

 

!image-2022-06-06-14-16-50-530.png!"	IOTDB	Closed	3	2	6861	pull-request-available
13526967	COUNT TIMESERIES <Path> GROUP BY LEVEL=<INTEGER> ：get NPE	"master 0303_0a4a84f
./sbin/start-cli.sh  -h 172.16.2.14 -e ""count timeseries root.test.g_0.** group by level=5""
2023-03-03 18:12:19,830 [pool-25-IoTDB-ClientRPC-Processor-171] WARN  o.a.i.d.u.ErrorHandlingUtils:89 - Status code: 301, Query Statement: ""count timeseries root.test.g_0.** group by level=5"". executeStatement failed
{color:red}*org.apache.iotdb.commons.exception.IoTDBException: java.lang.NullPointerException*{color}
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.dealWithException(QueryExecution.java:478)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getResult(QueryExecution.java:461)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.getByteBufferBatchResult(QueryExecution.java:496)
        at org.apache.iotdb.db.utils.QueryDataSetUtils.convertQueryResultByFetchSize(QueryDataSetUtils.java:254)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.lambda$static$0(ClientRPCServiceImpl.java:166)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementInternal(ClientRPCServiceImpl.java:239)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.executeStatementV2(ClientRPCServiceImpl.java:480)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeStatementV2.getResult(IClientRPCService.java:3629)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$executeStatementV2.getResult(IClientRPCService.java:3609)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.iotdb.db.service.thrift.ProcessorWithMetrics.process(ProcessorWithMetrics.java:64)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
{color:red}*Caused by: java.lang.NullPointerException: null*{color}
        at org.apache.iotdb.commons.path.PartialPath.hashCode(PartialPath.java:553)
        at java.base/java.util.HashMap.hash(HashMap.java:340)
        at java.base/java.util.HashMap.compute(HashMap.java:1204)
        at org.apache.iotdb.db.mpp.execution.operator.schema.CountGroupByLevelScanOperator.generateResult(CountGroupByLevelScanOperator.java:115)
        at org.apache.iotdb.db.mpp.execution.operator.schema.CountGroupByLevelScanOperator.next(CountGroupByLevelScanOperator.java:87)
        at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
        at org.apache.iotdb.db.mpp.execution.operator.schema.CountGroupByLevelMergeOperator.next(CountGroupByLevelMergeOperator.java:104)
        at org.apache.iotdb.db.mpp.execution.operator.sink.IdentitySinkOperator.next(IdentitySinkOperator.java:85)
        at org.apache.iotdb.db.mpp.execution.operator.Operator.nextWithTimer(Operator.java:46)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processInternal(Driver.java:212)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.lambda$processFor$1(Driver.java:144)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.tryWithLock(Driver.java:299)
        at org.apache.iotdb.db.mpp.execution.driver.Driver.processFor(Driver.java:125)
        at org.apache.iotdb.db.mpp.execution.schedule.DriverTaskThread.execute(DriverTaskThread.java:69)
        at org.apache.iotdb.db.mpp.execution.schedule.AbstractDriverThread.run(AbstractDriverThread.java:73)

元数据path层级是如下这样的：
 !image-2023-03-03-18-14-19-892.png! 

测试流程
1.启动1C1D集群
私有云1期  172.16.2.14 （bm 172.16.2.16）
ConfigNode ENV
MAX_HEAP_SIZE=""2G""
DataNode ENV
MAX_HEAP_SIZE=""20G""
MAX_DIRECT_MEMORY_SIZE=""6G""
COMMON PROP
query_timeout_threshold=3600000
avg_series_point_number_threshold=10000
2. benchmark运行
run_comp.sh
详细见附件
3. 统计时间序列数 指定的level 大于path 的层级数
[root@dn14 m_0303_0a4a84f_comp]# ./sbin/start-cli.sh  -h 172.16.2.14 -e ""count timeseries root.test.g_0.** group by level=5""
Msg: 301: null
"	IOTDB	Resolved	3	1	6861	pull-request-available
13393036	LastCache For Template And Vector	The origin lastCache storage on measurement doesn't support template and vector. A new storage mode should be designed and implemented.	IOTDB	Closed	3	2	6861	pull-request-available
13482993	partitialpath throw null	"insert data first, will throw this .

cannot dispatch FI for write operation

java.lang.negativeArraySizeException: null

at org.apache.iotdbcommons.path.PartialPath.deserialize(PartialPath.java.651)

at org.apache.iotdbcommons.path.MeasurementPath.deserialize(PartialPath.java.214)

at org.apache.iotdbcommons.path.PathDeserializeUtil.deserialize(PathDeseializeUtil.java.38)

.... ClusterSchemaFetcher.executeInternalCreateTimeseriesStatement(458)

....ClusterSchemaFetcher.internalCreateTimeseries(412)

...ClusterSchemaFetcher.checkAndAutoCreateMissingMeasurements(373)

ClusterSchemaFetcher.fetchSchemaWithAutoCreate(212)"	IOTDB	Closed	3	1	6861	pull-request-available
13442705	SchemaReader for schema query operator	"1. Refactor traverser in MTree to support iteratable schema read
2. Refactor operator to improve resuability "	IOTDB	Closed	3	4	6861	pull-request-available
13516557	Refactor device and timeseries count based on SchemaReader	"Currently, the DeviceCountOperator and TimeSeriesCountOperator are implemented by invoking SchemaRegion interfaces directly, which is unable to be interrupt and brings burden to SchemaRegion business logic. 

Since we want SchemaRegion to only provide the basic SchemaReader, we refactor the implementation of DeviceCountOperator and TimeSeriesCountOperator to extract the counting logic from SchemaRegion. Based on SchemaReader, it is more flexible to interrupt the process and release resources."	IOTDB	Closed	3	4	6861	pull-request-available
13505375	"[0.14][cluster] Result of ""count timeseries"" is an int type"	"The maximum value of int is _2147483647_
It is recommended to change to long type. 

!image-2022-11-22-23-06-47-805.png|width=240,height=344!"	IOTDB	Resolved	1	1	6861	pull-request-available
13422942	[Status Code]500Need to recheck	"复现步骤：

1.新启动的iotdb，未创建相关时间序列

2.cli登录发送语句：insert into root.test1(time, text) aligned values(1, 1.1), (2, 1),(3,hello),(4,false)  提示text是不可用的（可接受，{color:#de350b}但是用户手册需要更新{color}）

!image-2022-01-17-11-34-02-478.png|width=283,height=134!

3.更换SQL：insert into root.test1(time, text1) aligned values(1, 1.1), (2, 1),(3,hello),(4,false)

提示时间戳3的值hello输入不符合规范（可以接受，因为第一个值是1.1所以时间序列被创建为FLOAT）

4.添加单引号给hello，再次输入。报错500

{color:#de350b}该步骤应该提示非500错误，比如继续提示插入的值类型不匹配{color}

 

!image-2022-01-17-11-24-30-110.png|width=1261,height=583!

 "	IOTDB	Closed	3	1	6861	0.13.0, pull-request-available
13483950	ERROR o.a.i.d.m.p.s.FragmentInstanceDispatcherImpl:129 - [DispatchFailed] ： NPE	"m_0929_268de19
SchemaRegion ：RatisConsensus
DataRegion：MultiLeaderConsensus
均为3副本，3C5D , bm写入 3小时后，执行
delete timeseries root.**
datanode ERROR：
2022-09-30 14:22:20,228 [pool-24-IoTDB-ClientRPC-Processor-66$20220930_062216_45557_5] {color:#DE350B}*ERROR o.a.i.d.m.p.s.FragmentInstanceDispatcherImpl:129 - [DispatchFailed]
java.lang.NullPointerException: null*{color}
        at org.apache.iotdb.db.mpp.plan.planner.plan.node.write.InsertTabletNode.validateAndSetSchema(InsertTabletNode.java:180)
        at org.apache.iotdb.db.mpp.plan.analyze.SchemaValidator.validate(SchemaValidator.java:64)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchLocally(FragmentInstanceDispatcherImpl.java:265)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchOneInstance(FragmentInstanceDispatcherImpl.java:143)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatchWriteSync(FragmentInstanceDispatcherImpl.java:125)
        at org.apache.iotdb.db.mpp.plan.scheduler.FragmentInstanceDispatcherImpl.dispatch(FragmentInstanceDispatcherImpl.java:96)
        at org.apache.iotdb.db.mpp.plan.scheduler.ClusterScheduler.start(ClusterScheduler.java:102)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.schedule(QueryExecution.java:266)
        at org.apache.iotdb.db.mpp.plan.execution.QueryExecution.start(QueryExecution.java:184)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:146)
        at org.apache.iotdb.db.mpp.plan.Coordinator.execute(Coordinator.java:160)
        at org.apache.iotdb.db.service.thrift.impl.ClientRPCServiceImpl.insertTablet(ClientRPCServiceImpl.java:996)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:3512)
        at org.apache.iotdb.service.rpc.thrift.IClientRPCService$Processor$insertTablet.getResult(IClientRPCService.java:3492)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)

测试环境
1. 启动3C5D  48CPU 384GB
3C : 192.168.10.72 , 73, 74
5D : 192.168.10.72 , 73, 74 , 75,76

配置参数
ConfigNode
MAX_HEAP_SIZE=""8G""
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
data_replication_factor=3
connection_timeout_ms=120000

DataNode
MAX_HEAP_SIZE=""256G""
MAX_DIRECT_MEMORY_SIZE=""32G""

connection_timeout_ms=120000
max_connection_for_internal_service=200
max_waiting_time_when_insert_blocked=600000
query_timeout_threshold=36000000
enable_auto_create_schema=false

2. bm 运行配置见附件

3. 等待3小时后执行delete timeseries root.**
脚本在ip72 ${iotdb_dir}/datanode下
liuzhen@fit-72:/data/mpp_test/m_0929_2_268de19/datanode$ cat rm_dn.sh 
#!/bin/bash
sleep 3h
./sbin/start-cli.sh -h 192.168.10.74 -e ""show regions""
./sbin/start-cli.sh -h 192.168.10.74 -e ""show cluster""
./sbin/start-cli.sh -h 192.168.10.74 -e ""delete timeseries root.**"" &
sleep 120
./sbin/remove-datanode.sh  ""192.168.10.72:6667""  > 0930_remove_ip72.out
"	IOTDB	Closed	3	1	6861	pull-request-available
13420553	When the authorized user cancels the permission of a user, which still reports has no permission	"Grant the permission to cancel a user role to a specified user. When the specified user cancels the permission of a user, the user still reports that the user has no permission.

ROOT:
IoTDB> GRANT USER dddd PRIVILEGES REVOKE_ROLE_PRIVILEGE on root
Msg: The statement is executed successfully.

dddd:
IoTDB> REVOKE admin FROM xzh_02
Msg: 602: No permissions for this operation REVOKE_USER_ROLE"	IOTDB	Closed	3	1	6861	0.13.0
13470015	show child paths root.cisdi.im.**  command is dead	" !screenshot-1.png! 


0.13，这个命令没有返回，死了。 即使数据量有点多，也不应该死。另外，数据量并不多。

 !screenshot-2.png! 

建议：我觉得，这种语句，语法没错，但是可能由于数据量较大，应考虑分批返回，不能因为一个语句把CPU搞得很高。

"	IOTDB	Closed	2	1	6861	pull-request-available
13446283	[cluster]Got an error when delete root.** from 3 clients at the same time	"Reproduce steps:

1.set up a cluster with 3C3D

2.set sg like this:

set storage group to root.a;
set storage group to root.b.b1;
set storage group to root.c.c1.c2;
set storage group to root.x;

3.delete all sg at the same time in 3 cli(connect to 3 datanodes)

one of 3 got an error 

Msg: 500: [INTERNAL_SERVER_ERROR(500)] Exception occurred: ""delete storage group root.**"". executeStatement failed. error code: TSStatus(code:500, message:506: null)

 

!image-2022-05-23-12-36-25-860.png!"	IOTDB	Closed	3	1	6861	pull-request-available
13406347	Persistence Schema: refactor MNode Access interface	"Abstract the access of MNode.

Setup IMTreeStore interface and replace all mtree structure access methods provided directly by MNode with the methods provided by IMTreeStore. The concerned methods are those like getChild, getChildren, addChild, e.g."	IOTDB	Closed	3	3	6861	pull-request-available
13452447	Support core requirements of template	"The core requirements include:
 # deliver template from configNode to dataNode
 # mount template on specific path on involving dataNodes
 # activate template on specific path on involving dataNode"	IOTDB	Closed	3	4	6861	pull-request-available
13473664	Support drop template	Support drop template in cluster. A template set on no path can be dropped.	IOTDB	Closed	3	4	6861	pull-request-available
13523142	[Atmos] got a wrong result from COUNT NODES root.*.*.s1 level=2	"version 1.1.0-SNAPSHOT (Build: 52ddf63)

Reproduce Steps：

--1. 创建元数据
create database root.test.`001.002.003`;
create database root.test.g_0.s_0_b001;
create database root.sg;
create timeseries root.sg.d1.s1.t1 with datatype=float,encoding=RLE;
create timeseries root.sg.d1.s2 with datatype=double,encoding=RLE;
create timeseries root.sg.d2.s1 with datatype=INT32,encoding=RLE;
create timeseries root.sg.d3.s1 with datatype=INT64,encoding=RLE;
create timeseries root.sg.d3.s2 with datatype=boolean,encoding=RLE;
create timeseries root.sg.s1 with datatype=TEXT,encoding=PLAIN;
create database root.ln;
create timeseries root.ln.wf01.wt01.status with datatype=Boolean,encoding=PLAIN;
create timeseries root.ln.wf01.wt02.status with datatype=Boolean,encoding=PLAIN;
create timeseries root.ln.wf02.wt01.status with datatype=Boolean,encoding=PLAIN;

--2. 统计节点数
COUNT NODES root.*.*.s1 level=2;
COUNT NODES root.sg.*.s1 level=2;

 

!image-2023-02-06-12-11-10-038.png!"	IOTDB	Closed	3	1	6861	pull-request-available
13428450	Tag info sync error during metadata sync	!image-2022-02-15-09-13-37-672.png!	IOTDB	Resolved	3	1	6861	0.13.0, pull-request-available
13522376	Update Version in IoTDB to V_1_0	Update Version in IoTDB to V_1_0	IOTDB	Closed	3	4	9555	pull-request-available
13447866	NPE occurred while executing show latest timeseries on timeseries without any data	!image-2022-06-01-16-11-49-415.png!	IOTDB	Closed	3	1	9555	pull-request-available
13492380	[Metric] Update the write path of iotdb metric reporter	In IoTDB, now iotdb metric reporter will write metrics into `root.${database}.xxx`. In order to unify the management of build-in storage group, we need to write metrics into `root.__system.${database}.xxx`	IOTDB	Closed	3	4	9555	pull-request-available
13423163	"[start-cli] Execute start-cli.sh at [iotdb/conf], will create folder named ""IOTDB_HOME_IS_UNDEFINED"""	"# start iotdb-server.sh
 # cd iotdb/conf
 # ../sbin/start-cli.sh
 # exit

!image-2022-01-18-11-44-11-678.png!

 

!image-2022-01-18-11-44-25-472.png!"	IOTDB	Closed	5	1	9555	0.13.0, pull-request-available
13471189	[Multileader] Fix the problem of updating safelyDeletedIndex	When there are only one replica, there are no way to update safelyDeletedIndex in the implementation.	IOTDB	Closed	3	2	9555	pull-request-available
13435121	[Dependency] remove iotdb-session's dependency on iotdb-server(test)	"In iotdb-session's pom:

!image-2022-03-22-21-07-20-740.png|width=407,height=267!

There is no need for iotdb-session to dependent on iotdb-server, so I will remove these dependency and move related test to integration module."	IOTDB	Closed	3	4	9555	pull-request-available
13436144	[Metric] Optimize the implementation of IoTDB Reporter	Now, the implementation of IoTDB Reporter use session to write data, but I think use session pool can be a good idea.	IOTDB	Closed	3	4	9555	pull-request-available
13486272	[Metric] Add MultiLeader Metrics.	"# modify metric module to support to add metrics in consensus.
 # add metrics in multiLeader."	IOTDB	Closed	3	4	9555	pull-request-available
13445110	[Metric] Move metric-related parameter from iotdb-engine to iotdb-metric.	"1. Remove enable_performance_stat parameter in iotdb-engine.
2. Add enablePerformanceStat in iotdb-metric."	IOTDB	Closed	3	4	9555	pull-request-available
13403404	Metagroup snapshots support template	Snapshots of the meta group in the current cluster module support storage groups and TTL information, etc, but do not support templates. Templates need to be supported so that newly added nodes can register templates during cluster expansion.	IOTDB	Closed	3	4	9555	pull-request-available
13481805	[MultiLeader] Optimize the implementation of PendingBatch	Optimize the implementation of PendingBatch: calculate start index and end index in constructor method.	IOTDB	Closed	4	4	9555	pull-request-available
13476476	[LastCache] The concurrency problem when auto create schemaCacheEntry in updateLastCache method.	!image-2022-08-12-10-57-17-380.png|width=350,height=218!	IOTDB	Closed	3	1	9555	pull-request-available
13491025	detach  system storage group	"IoTDB have some built-in storage, such as: root._metric (monitoring), root.system.audit (system audit log).

When statistics the number of user storage group and user measuremen, must manually filter these non-service data. However, there is currently no unified prefix, whether it is possible to unify the prefix for the built-in system storage group, or specify certain rules 

 

目前会有一些系统内置的storage、比如：root._metric（监控）、root.system.audit（系统审计日志）等。

当做一些统计业务存储组、业务测点数量时，必须手动过滤这些非业务数据。但是目前并没有统一的前缀，是否可以为系统内置的storage统一前缀、或指定某些规则 "	IOTDB	Open	3	4	9555	pull-request-available
13502067	[Metric] Enable metric module in default config	"In order to implement the collection of some metrics, we need to enable metric in default config:
 # Start no reporter in default config which means metrics can only be collected by API.
 # Minimize and specific core level metric"	IOTDB	Closed	3	2	9555	pull-request-available
13524586	[Metric] Update IoT Consensus Metric Precision	Update IoT Consensus Metric Precision from ms to ns	IOTDB	Closed	3	4	9555	pull-request-available
13503744	Micrometer and DropWizard behave differently	"Micrometer: Value only appears in one window.

DropWizard: Value retains in every window.

Here is an example of Micrometer:
{code:java}
IoTDB> select DCP_SeriesScanOperator_hasNext_count.`name=DCP_A_GET_CHUNK_METADATAS`.value as cnt from root.__system.metric.`0.0.0.0:6667`
+-----------------------------+---+
|                         Time|cnt|
+-----------------------------+---+
|2022-11-19T20:12:09.306+08:00|2.0|
|2022-11-19T20:12:24.305+08:00|0.0|
|2022-11-19T20:12:39.306+08:00|0.0|
|2022-11-19T20:12:54.306+08:00|0.0|
+-----------------------------+---+
Total line number = 4
It costs 0.022s

IoTDB> select sum(DCP_SeriesScanOperator_hasNext_count.`name=DCP_A_GET_CHUNK_METADATAS`.value) as sum_cnt from root.__system.metric.`0.0.0.0:6667`
+-------+
|sum_cnt|
+-------+
|    2.0|
+-------+
Total line number = 1
It costs 0.320s {code}
Here is an example of DropWizard:
{code:java}
IoTDB> select `dropwizard:DCP_SeriesScanOperator_hasNext_count`.`name=DCP_A_GET_CHUNK_METADATAS`.value as cnt from root.__system.metric.`0.0.0.0:6667`
+-----------------------------+---+
|                         Time|cnt|
+-----------------------------+---+
|2022-11-19T20:09:17.090+08:00|  2|
|2022-11-19T20:09:31.881+08:00|  2|
|2022-11-19T20:09:46.847+08:00|  2|
|2022-11-19T20:10:01.867+08:00|  2|
|2022-11-19T20:10:16.868+08:00|  2|
|2022-11-19T20:10:31.852+08:00|  2|
+-----------------------------+---+
Total line number = 6
It costs 0.035s

IoTDB> select sum(`dropwizard:DCP_SeriesScanOperator_hasNext_count`.`name=DCP_A_GET_CHUNK_METADATAS`.value) as sum_cnt from root.__system.metric.`0.0.0.0:6667`
+-------+
|sum_cnt|
+-------+
|   18.0|
+-------+
Total line number = 1
It costs 0.010s {code}"	IOTDB	Closed	3	5	9555	pull-request-available
13439736	[Metric] remove useless metric related dependency.	Remove useless metric related dependency.	IOTDB	Closed	3	4	9555	pull-request-available
13421577	[metric] Modify some problems of metric	"# Fix some error in metric doc model.
 # Fix some description of iotdb-metric.yml
 # Modify some config of metric model into hot load way"	IOTDB	Closed	3	4	9555	0.13.0, pull-request-available
13482683	[cluster]After setting system to readonly, insert data returns Msg is null	"commit b10f6bcf8527f17fa7ef3dbcc19b4c38427b5591

Step1: Start 3C3D
Step2: Enter CLI, execute sql
{code:java}
IoTDB> SET SYSTEM TO READONLY
Msg: The statement is executed successfully. {code}
Step3: Execute insert SQL,then Msg is 904 null
{code:java}
IoTDB> insert into root.sg.d1(time, s1, s2) aligned values(1, 1, 1)
Msg: 904: null {code}
The log is:

!image-2022-09-22-11-46-42-497.png|width=553,height=208!

 "	IOTDB	Closed	3	1	9555	pull-request-available
13481259	[cluster]Monitoring can support any disk information	In the current cluster version, only the information of the system disk in the root directory can be supported, and other data disks or mounted disks cannot be monitored. It is recommended to monitor the information of any disk.	IOTDB	Closed	4	4	9555	pull-request-available
13473957	[cluster]There is already an existing meter named 'compaction_task_count' 	"commit d7d6c9e0ab844ff8e112029078062d9b5b8ee96b
Author: imquanke <39719966+imquanke@users.noreply.github.com>
Date:   Thu Jul 28 15:39:07 2022 +0800

    [IOTDB-3900] start-confignode ，Failed to execute system command (#6805)

!image-2022-07-28-17-30-09-435.png!

Prometheus requires that all meters with the same name have the same set of tag keys. There is already an existing meter named 'compaction_task_count' containing tag keys [name, type]. The meter you are attempting to register has keys [name].

 "	IOTDB	Closed	4	1	9555	pull-request-available
13514851	Metrics of the database's memory cost maybe wrong	"Through the dashboard, we can see that the database's memory cost only occupies 139MB, and the chunk metadata of the database occupies 0 MB.

However when we dump the process's memory, only the ChunkMetadata occupies about 3GB.  which does not match.

 

!image-2022-12-20-18-18-44-332.png!

!image-2022-12-20-18-19-08-489.png!"	IOTDB	Closed	3	1	9555	pull-request-available
13474121	[MultiLeader] Add the control of wal size used by multiLeader Consensus	Add the control of wal size used by multiLeader Consensus	IOTDB	Closed	3	4	9555	pull-request-available
13448485	[metric] add metrics of cache hit rate.	Add metrics of cache hit rate.	IOTDB	Closed	3	2	9555	pull-request-available
13505442	[Metric] Fix tag of metrics	There are some metrics in 0.14 version use same tag keys	IOTDB	Closed	3	4	9555	pull-request-available
13448902	[Config] update default config of confignode.	"To get better performance experience, I think default config of confignode can be updated as follows:
{code:java}
//代码占位符
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus schema_region_consensus_protocol_class=org.apache.iotdb.consensus.standalone.StandAloneConsensus
schema_replication_factor=1
data_replication_factor=1 {code}
 "	IOTDB	Closed	3	4	9555	pull-request-available
13471952	Add another interface for last update in SchemaCache	In current SchemaCache, if we don't do write before querying after restarting, SchemaCache will always be empty which will prevent last querying updating the last cache.	IOTDB	Closed	3	4	9555	pull-request-available
13481609	[MultiLeader] Move `cacheWindowTimeInMs` into config	Move `cacheWindowTimeInMs` into config.	IOTDB	Closed	3	2	9555	pull-request-available
13537930	[Metric] Add up time metric	Add up time metric to statistic the running time of IoTDB	IOTDB	In Progress	3	4	9555	pull-request-available
13482095	[grafana]Memory info not clear	"After starting 1C1D, when displaying monitoring information through Prometheus and grafana, the JVM information display in Memory is not very clear.

 

!image-2022-09-19-11-20-35-529.png|width=518,height=262!"	IOTDB	Closed	3	4	9555	pull-request-available
13469583	[Partition Cache] Support update region route map in datanode.	Support region route map in datanode.	IOTDB	Closed	3	2	9555	pull-request-available
13524970	[Metric] Fix potential concurrency problems when metric service start and stop	when starting, stopping, and restarting metric service, there are potential concurrency problems.	IOTDB	Closed	3	1	9555	pull-request-available
13473927	[ MultiLeaderConsensus ] When there is no new write operation, some data may be left without synchronization	"master_0727_975eaa5
问题描述：
MultiLeaderConsensus，写入停止，出现最后写入的部分数据没有被同步到follower。

复现流程：
1. 私有云
172.20.70.2/3/4/5/13/14/16
benchmark在15

ConfigNode
schema_region_consensus_protocol_class=org.apache.iotdb.consensus.ratis.RatisConsensus
data_region_consensus_protocol_class=org.apache.iotdb.consensus.multileader.MultiLeaderConsensus
schema_replication_factor=3
data_replication_factor=3

MAX_HEAP_SIZE=""4G""

DataNode
wal_buffer_size_in_byte=1048576
max_waiting_time_when_insert_blocked=3600000
enable_seq_space_compaction=false
enable_unseq_space_compaction=false
enable_cross_space_compaction=false
query_timeout_threshold=36000000

MAX_HEAP_SIZE=""16G""

2. ip15 启动2个bm，连2个datanode
配置文件见附件

3. 运行过程无ERROR
follower同步完成，执行flush。

4. 执行查询
连任一 datanode执行查询
select count(s_4) from root.** align by device
2000条记录的结果值，期望为100000。
实际上有部分序列的值（12条）为 99990（ bm的1个batch）

这些数据没有同步到follower节点（几个小时前写入已完成）"	IOTDB	Closed	3	1	9555	pull-request-available
13432311	[Metric] fix metric doc	"# Modify default level of metric to IMPORTANT.
 # Add metric standard into metric docs."	IOTDB	Closed	3	4	9555	pull-request-available
13483758	[Grafana]The Metrics template displayed in grafana does not match the actual	"Turn on the monitoring mode, perform read and write operations on IOTDB, and finally display in grafana inconsistent with the actual.
For example:
The number of file

!image-2022-09-29-12-36-24-267.png|width=548,height=76!

!image-2022-09-29-11-31-33-486.png|width=551,height=128!

 "	IOTDB	Closed	3	1	9555	pull-request-available
13515978	[metric] Collects the number of open file handles of  DataNode/ConfigNode	"metric 需支持 采集ConfigNode/DataNode 进程打开的文件句柄数，以监控是否有文件句柄泄露。
filehandle_num=`ssh ${u_name}@${node} ""lsof -p ${pid} | wc -l""`
"	IOTDB	Closed	3	4	9555	pull-request-available
13442192	[cache] fix PartitionCache when there are ** in device path.	Fix PartitionCache when there are ** in device path.	IOTDB	Closed	3	1	9555	pull-request-available
13445825	[Config] fix the place of dir in common config.	fix the user folder and role folder place in common config.	IOTDB	Closed	3	1	9555	pull-request-available
13447361	[TTL] optimize not set of ttl	Make unset of ttl in cluster mode same as 0.13.0 version.	IOTDB	Closed	3	1	9555	pull-request-available
13522688	Add Tracing Metric Dashboard	Add tracing dashboard, contains write and query main path	IOTDB	Closed	3	4	9555	pull-request-available
13439352	Handling user privileges for aligned timeseries related features	"step 1： start iotdb
step 2：root enter cli
{code:java}
create storage group root.line1;
create storage group root.line2;
create timeseries root.line1.wf01.wt01.temperatures with datatype=float,encoding=rle;{code}
{code:java}
create user user01 'pass1234'; 
grant user user01 privileges insert_timeseries on root.line1;{code}
step 3: user01 enter cli
{code:java}
insert into root.line1.wf01.wt01(time,temperatures) aligned values(1,1),(2,1),(3,1); {code}
Then reported no privileges:


!image-2022-04-13-13-58-07-329.png!"	IOTDB	Closed	1	1	9555	pull-request-available
13447629	[ insert failed ] “Import“ and “insert”  to sequence ，returned successfully , but query result is empty	"master_0530_95884ad
3confignode，3datanode . Other parameters are default .

benchmark连接node1，长时间写入数据，出现某些设备记录为空，
所以执行手动import和insert，执行结果均为成功，但是查询结果依然为空。

复现流程
1. 192.168.10.62/66/68   72C256G
记为node1,node2,node3.

2. benchmark运行附件中的配置

3. 约20小时后（可以试着不用等待这么长时间）
查询存储组root.test.g_0下的600个dev的count(s_4)，有74个dev的数据为空。
选择其中1个设备，手动import（从有数据的dev中先export），import成功，
但是查询此设备，结果集为空。
手动insert成功，查询还是为空。查看datanode，confignode无error。
bm位置192.168.10.68：
/data/benchmark/weekly_shell/bm_0527_e7b23aa

 !screenshot-1.png! 
 !screenshot-2.png! 
 !screenshot-3.png! 
"	IOTDB	Closed	3	1	9555	pull-request-available
13504850	[Metric] Fix NPE Problem When Load Configuration	When load configuration, there are NPE in metric module	IOTDB	Closed	3	4	9555	pull-request-available
13442191	[MPP] Implementation of count storage group.	Add implementation for counting storage group.	IOTDB	Closed	3	2	9555	pull-request-available
13516175	[Metric] Update Zh Metric Doc	Update Normal Level metrics details of Metric Doc in zh.	IOTDB	Closed	3	4	9555	pull-request-available
13504961	[metrics] Fix init failure when use iotdb reporter	Move the register of Metric Service after the set up of RPC service in order to avoid initial failure	IOTDB	Closed	3	4	9555	pull-request-available
13418619	[Cluster] Fix the config problems in cluster mode	Because of the development of iotdb server, there are some problems in cluster/src/resources(for metric framework and influxdb rpc service), it will fail when you use these config to start iotdb cluster.	IOTDB	Closed	3	1	9555	pull-request-available
13476262	Remove redundant RPC interfaces associated with Partition	"These Partition related interfaces are out of date:

 

!image-2022-08-11-08-49-23-731.png!

 

!image-2022-08-11-08-49-55-442.png!"	IOTDB	Closed	5	4	9555	pull-request-available
13431166	[Metric] Add Some feature into metric module.	"# Add metric level into metric framework: core, important, normal, all.
 # Remove the usage of pushgateway of dropwizard framework.
 # Fix docs of metric module."	IOTDB	Closed	3	2	9555	pull-request-available
13489346	[Metric] Fix the statistics of disk size in linux system in 0.13	The method for calculating the total number of disks on the linux server is incomplete. You should calculate the size of all disk mount points except the root directory (/)	IOTDB	Closed	3	1	9555	pull-request-available
13439070	Rename LocalConfigManager to LocalConfigNode and refactor the management of SchemaRegion	"# rename LocalConfigManager to LocalConfigNode
 # refactor the management of SchemaRegion, the LocalConfigNode only manage schemaRegionId rather than schemaRegion; SchemaRegion is managed by SchemaEngine
 # fix the CI problems occurred when clear schema  

!image-2022-04-12-11-03-47-251.png!

4. Finish createSchemaRegion in DataNodeManagementServiceImpl."	IOTDB	Closed	3	4	9555	pull-request-available
13450851	[metric] Fix some problem in grafana dashboard and add doc.	"# fix some problem in grafana dashboard
 # Add doc to explain the meaning of grafana dashboard."	IOTDB	Closed	3	1	9555	pull-request-available
13396243	JDBC connection failed: enable rpcThriftCompressionEnable	"Firstly, I set org.apache.iotdb.jdbc.Config.rpcThriftCompressionEnable = true; in code, then I use DriverManager.getConnection() to connect, but it failed.

Some logs:

org.apache.iotdb.rpc.IoTDBConnectionException: org.apache.thrift.transport.TTransportException: Socket is closed by peer.

java.sql.SQLException: Can not establish connection with jdbc:iotdb://127.0.0.1:6667/ : Socket is closed by peer..

cn.edu.tsinghua.iotdb.benchmark.tsdb.TsdbException: cn.edu.tsinghua.iotdb.benchmark.tsdb.TsdbException: java.sql.SQLException: Can not establish connection with jdbc:iotdb://127.0.0.1:6667/ : Socket is closed by peer.."	IOTDB	Closed	3	1	9555	easy-fix
13515678	[Metric]Only the leader confignode can show the number of datanode and confignode	"Only the leader confignode can show the number of datanode and confignode.

Please refer to below pictures:

Can we change it to ""can be show on both leader and follower""?

!image-2022-12-27-10-14-39-563.png|width=638,height=388!

!image-2022-12-27-10-14-27-396.png|width=637,height=335!"	IOTDB	Closed	4	4	9555	pull-request-available
13533142	PerformanceOverview monitoring consumes a significant amount of CPU resources.	PerformanceOverview monitoring accounts for approximately 11% of CPU overhead. For comparison, the Compaction module also only accounts for around 11% of CPU overhead. The results of CPU profiling are shown in the attached flame graph.	IOTDB	Open	3	4	9555	pull-request-available
13492622	[Metric] FileNotFoundException of confignode when start	!image-2022-10-28-10-27-45-904.png|width=635,height=187!	IOTDB	Closed	3	1	9555	pull-request-available
13443874	Add some optional parameters of storage group related sql.	"set storage group statement are extended:

 

setStorageGroup
: SET STORAGE GROUP TO prefixPath (WITH sgAttributeClause (COMMA sgAttributeClause)*)?
;

sgAttributeClause
: (TTL | SCHEMA_REPLICATION_FACTOR | DATA_REPLICATION_FACTOR | TIME_PARTITION_INTERVAL) '=' INTEGER_LITERAL
;

 

when you use show storage group, add ttl, dataReplicationFactor, schemaReplicationFactor and timePartitionInterval into result."	IOTDB	Closed	3	2	9555	pull-request-available
13504742	[Metric] Update config structure of IoTDB Metric Module	Update config structure of IoTDB Metric Module	IOTDB	Closed	3	4	9555	pull-request-available
13449098	[metric] add cluster info metrics	Add cluster info metrics, mainly about region, slot in region and device in slot.	IOTDB	Closed	3	4	9555	pull-request-available
13482847	[Optimize] Add some comment and fix potential memory leak.	Add some comment and fix potential memory leak.	IOTDB	Closed	3	4	9555	pull-request-available
13492499	[Metric] Fix remove metrics in Metric Module	Currently, Metrics has same tag keys will be removed at same time which leads to error.	IOTDB	Closed	3	4	9555	pull-request-available
13502684	[Metric] Support internal reporter in metric module	Support internal reporter in metric module	IOTDB	Closed	3	4	9555	pull-request-available
13423926	[Metric] When run CI about Dropwizard Timer, it will failed some time.	"When run CI, there are following problems:

[!https://user-images.githubusercontent.com/46039728/150483715-8c410cc4-a45a-492c-8155-bb3b1eb750b2.png!|https://user-images.githubusercontent.com/46039728/150483715-8c410cc4-a45a-492c-8155-bb3b1eb750b2.png]

It's because the implementation of snapshot for dropwizard use wighted one as default, so I change to uniform one now."	IOTDB	Closed	3	1	9555	pull-request-available
13446699	[TTL] Optimiza ttl related operation	"# Add precheck of ttl in set sg and set ttl: tll should greater than 0.
 # when use show ttl or show sg, and there are not set, it should show not set."	IOTDB	Closed	3	4	9555	pull-request-available
13482707	[0.14][cluster]close auto create schema, will auto create storage group and report NPE	"enable_auto_create_schema=false,
 # insert timeseries will create storage group
 # the current cli report ""can connect to node ***""
 # the other nodes report NPE

!image-2022-09-22-15-04-52-954.png|width=688,height=369!"	IOTDB	Closed	3	1	9555	pull-request-available
13485514	[Log] Remove meaningless log	There are some meaningless logs in server.	IOTDB	Closed	3	4	9555	pull-request-available
13447654	[Metric] Adapt Metric framework for new cluster.	"# Adapt Metric framework for confignode.
 # Modify confignode thrift processor with metrics.
 # Fix exception when data folder not exists."	IOTDB	Closed	3	2	9555	pull-request-available
13505443	[Metric] Fix tag of metric in 0.13	Fix same tags in metric of 0.13 and fix same category of metrics has different tag keys.	IOTDB	Closed	3	4	9555	pull-request-available
13480184	[master]After starting mqtt, after executing Trigger's SQL, there is an NPE error in the log	"commit 518dcfbfa461ffe4912fba18cb29299416301588

After enabling MQTT, execute the trigger use case, and find that there is an NPE authorization authentication error in the log.

Step 1: In the iotdb-datanode.properties ,modify parameter:
enable_mqtt_service=true
Step 2: Start server, Enter CLI

Step 3: Execute SQL:
{code:java}
set storage group to root.sg1;
CREATE TIMESERIES root.sg1.dev1.s_1 WITH DATATYPE=INT32, ENCODING=GORILLA;
CREATE TIMESERIES root.sg1.dev1.s_2 WITH DATATYPE=INT64, ENCODING=PLAIN;
CREATE TIMESERIES root.sg1.dev1.s_3 WITH DATATYPE=float, ENCODING=RLE;
CREATE TIMESERIES root.sg1.dev1.s_4 WITH DATATYPE=double, ENCODING=TS_2DIFF;
CREATE TIMESERIES root.sg1.dev1.s_5 WITH DATATYPE=text, ENCODING=DICTIONARY;
CREATE TIMESERIES root.sg1.dev1.s_6 WITH DATATYPE=boolean, ENCODING=PLAIN;

CREATE TRIGGER trig1
AFTER INSERT
ON root.sg1.dev1.s_1
AS 'TriggerTest'
WITH (
  'ts_type' = 'int32',
  'remote_ip' = '127.0.0.1',
  'trig_name'='trig1'
);

CREATE TRIGGER trig2
AFTER INSERT
ON root.sg1.dev1.s_2
AS 'TriggerTest'
WITH (
  'ts_type' = 'int64',
  'remote_ip' = '127.0.0.1',
  'trig_name'='trig2'
);

insert into root.sg1.dev1(time,s_1,s_2) values(1,100,200);
insert into root.sg1.dev1(time,s_1,s_2) values(2,300,400);

select s_1,s_2,s_3,s_4,s_5,s_6 from root.sg1.dev1;
select local_trig1,local_trig2,remotetrig1,remotetrig2 from root.target.alerting; 
  
delete timeseries root.sg1.dev1.s_1;
insert into root.sg1.dev1(time,s_2) values(100,1600);{code}"	IOTDB	Closed	3	1	9555	pull-request-available
13444121	Directory of Auth is put into sbin folder	" 

When start confignode, the data/system roles and users are put into sbin folder, which should be move to confignode/data/system.

 

!image-2022-05-10-15-35-56-080.png!"	IOTDB	Closed	3	4	9555	pull-request-available
13419471	Reconstruct the process of generating resultset header of query	The process of generating resultset header of query is confusing now, we'd better reconstruct it so that being easy to maintain.	IOTDB	Closed	3	3	9555	pull-request-available
13473926	[MultiLeader] Remove useless param in IndexedConsensusRequest and add some log.	Remove useless param in IndexedConsensusRequest and add some log.	IOTDB	Closed	3	4	9555	pull-request-available
13473146	can't to collect metrics when the config 'enable_auto_create_schema' is true	!16641658482554_.pic.jpg!	IOTDB	Closed	3	1	9555	pull-request-available
13446272	[cluster]Wrong Error message when create a sg which sub path has already be set as a sg	"Reproduce steps:

1.set up a cluster with 3C3D;

2.using iotdb-cli to set a sg ‘set storage group to root.test’

3..using iotdb-cli to set a sg ‘set storage group to root.test.sg’.Got an error like below:

Msg: 500: [INTERNAL_SERVER_ERROR(500)] Exception occurred: ""set storage group to root.test.sg"". executeStatement failed. error code: TSStatus({color:#de350b}code:500, message:902: StorageGroup root.test.sg is already set.{color})

!image-2022-05-23-09-54-38-346.png!"	IOTDB	Closed	3	1	9555	pull-request-available
13419181	[cluster] SHOW TIMESERIES will only display 2000 timeseries	"In current cluster version, if we have more than 2,000 timeseries, when we commend SHOW TIMESERIES, it will only display 2,000 timeseries, the same as the SHOW DEVICES...

This problem is fixed on master branch, but not cherry-pick to rel/0.12

 

Related PR: https://github.com/apache/iotdb/pull/2958"	IOTDB	Closed	3	1	9555	pull-request-available
13484532	Add javadoc and example in IPartitionFetcher.	Add javadoc and example in IPartitionFetcher.	IOTDB	Closed	3	4	9555	pull-request-available
13514840	[Metric] Unify the address and port in metric module	Modify the address and port of metric module in datanode to internal one	IOTDB	Closed	3	4	9555	pull-request-available
13444128	[cluster]  can not using ‘show verson' in iotdb-cli	"登录cli后使用show version命令报错

commit 847b8e34cc301a85bf904af85e649cdf1e51fca2
Author: 刘威 <51618159+LIU-WEI-git@users.noreply.github.com>
Date:   Tue May 10 13:03:58 2022 +0800

    IOTDB-2797 Use path pattern to check path in Auth (#5827)

*!image-2022-05-10-16-07-05-320.png!*

 "	IOTDB	Closed	3	1	9555	pull-request-available
13525692	[iotdb-metric] add a new metric of FULLGC	" 

add a new metric of FULLGC

 

性能逐渐下降，这里面有一个因素是可能相关的，而且对于用户应该是长时间使用IoTDB以后会可能出现的问题，就是FullGC的频度。

能不能：
 # grep日志拿出FullGC的记录；
 #  计算出FullGC的间隔时间；
 #  把#2中的间隔时间画出曲线，看看是不是有什么趋势（主要针对长测）；
 #  如果有明显趋势，和测试性能对比看有没有相关性"	IOTDB	Closed	4	2	9555	pull-request-available
13479012	[Metric] Update metric dashboard	Clear some duplicate panel and specific some metric panel.	IOTDB	Closed	3	4	9555	pull-request-available
13510853	[monitor]the file count of datanode become negative number	"[monitor]the file count of datanode become negative number

environment:
3C3D cluster, prometheus

datanode dashboard shows file count become negative number
  !screenshot-2.png! 

reproduction:
1. start iotdb cluster successfully
2. On each node, update dn_metric_reporter_list=PROMETHEUS 
3. On command window: load configuration
4. the iotdb become readonly


http://172.20.70.44:30000/d/TbEVYRw7A/apache-iotdb-datanode-dashboard?orgId=1&from=1670486400000&to=1670551199000
admin/Time10"	IOTDB	Closed	3	1	10039	pull-request-available
13483647	Start compaction only when IoTDB is fully recovered	Accelerate restarting	IOTDB	Closed	3	4	10039	pull-request-available
13442414	[ TTL ] Expired tsfiles with a TTL of more than 10 hours are not deleted	"master_0427_b633df5
问题描述：
长测配置，设置TTL为24小时，
grep ttl logs/log_all.log |tail -1

2022-04-29 08:57:07,404 [pool-11-IoTDB-TTL-Check-1] INFO  o.a.i.d.e.s.DataRegion:1658 - Removed a file /data/iotdb_data/data/sequence/root.test.g_4/0/0/1651092583331-4021-1-0.tsfile{color:#DE350B} before Sat Sep 01 19:33:53 CST 2018 by ttl{color} (115392194000ms)

./tools/tsfileToolSet/print-tsfile-resource-files.sh 解析sg中第1个resource文件，查看时间戳范围，最大时间戳：
device root.test.g_0.d_350, start time 1535749930200 (2018-09-01T05:12:10.200+08:00[Asia/Shanghai]), {color:#DE350B}*end time 1535749948000 (2018-09-01T05:12:28+08:00[Asia/Shanghai])*{color}

有14个小时的过期文件没有被删除，磁盘空间无法释放。

1. 机器信息
私有云172.20.70.14/13     8C32G

数据库，bm配置文件见附件。

长测运行起来之后，24小时，设置TTL：
#!/bin/bash
#ttl默认大小24小时
ttl=86400000
host=127.0.0.1
port=6667
function setTTLOfSg(){
  input_new_ttl=$1
  ./sbin/start-cli.sh -h $host -p $port -e ""set ttl to root.** $input_new_ttl""

}

#计算操作系统当前时间和iotdb 序列max_time的差值

function getDiffOfTime(){

   # 操作系统当前值 秒
   os_now=`date +%s`
   max_time=`./sbin/start-cli.sh -h $host -p $port -e ""show devices limit 1""|grep root|awk -F '|' '{print ""./sbin/start-cli.sh -h "" host "" -p "" port "" -e \""select max_time(s_0) from "" $2 "" \"" ""}' host=$host port=$port|sh|grep ""|    ""|awk -F '|' '{print $2}'`


   diffTime=$((((os_now*=1000))-max_time))

   echo $diffTime

}

# 2小时校正1次

#sleep 24h

for i in {1..40000}
do
   getDiffOfTime

   new_ttl=$((ttl+diffTime))
   echo $new_ttl

   setTTLOfSg $new_ttl
   sleep 2h
done
"	IOTDB	Closed	3	1	10039	pull-request-available
13534287	Optimize the implementation of some metric items in the metric module to prevent Prometheus pull timeouts	" !image-2023-04-27-17-03-29-978.png! 
!image-2023-04-27-17-01-37-144.png! 
Under high write pressure, even without Full GC, the elapsed time of individual monitor items in the monitoring framework will cause the Prometheus pull sampling timeout, resulting in missing monitor data, which ultimately affects performance problem troubleshooting.
The three main time points found by jprofile sampling are the number of file handles, the number of client concurrency, and the number of threads. The implementation needs to be optimized
"	IOTDB	Closed	3	4	10039	pull-request-available
13449407	Make data_region_num take effect in new standalone	"ConfigNode is a process used in the new cluster. To make the processing logic clear, we need to fake a ConfigNode in the new standalone version, called LocalConfigNode, which is a module but not a process.

 

Currently, in the new Standalone version, we just allocate one DataRegion and one SchemaRegion for each Storage Group. This means the data_region_num does not take effect.

 

We need to let data_region_num take effect."	IOTDB	Closed	3	4	10039	pull-request-available
13408088	some Unsequence files never be merged to higher level or Sequence folder	In rel/0.12, if we use level compaction management and enable level compaction for unsequence files, the system will not compact the unsequence files to the top level. Instead, it will merge the files that one level lower than the top to sequence file. During the merge process, the files will be set merging, so we don't need to set it before the merge. 	IOTDB	Closed	3	1	10039	pull-request-available
13510399	Add metrics for compaction deserializing pages or writing chunks	We want to trace the count of deserlializing chunk or pages during compaction.	IOTDB	Closed	3	4	10039	pull-request-available
13425300	Import a Status for TsFileResource	"Currently, we have isCompacting and isCandidate in TsFileResource for the compaction process.

 

It's better to add a status field in TsFileResource. This Status contains 4 types: Compacting, CompactionCandidate, Unclosed, Closed. This field is to replace the isClosed, compactionCandidate, Compacting.

 

When Compacting, we only select the Unclosed TsFile to compact."	IOTDB	Closed	4	4	10039	pull-request-available
13436058	[delete sg + compaction] compacting: false does not exist either, do nothing. Set system to read-only 	"rel/0.13 e10325f04e6d631799002dd76d12dd019cf72dc0
内部跑SQL功能测试：
 aligned_timeseries
 continuous_query
 issues
 maintenance_command
 privilege
 shell_scripts
 template
 trigger
 ttl
 udf
合并线程Set system to read-only（整个日志见附件） :
2022-03-25 17:01:47,597 [pool-7-IoTDB-Compaction-2] ERROR o.a.i.d.e.c.i.s.SizeTieredCompactionTask:196 - root.test.g_0-0 [Compaction] Throwable is caught during execution of SizeTieredCompaction, {} 
java.lang.NullPointerException: null
	at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:143)
	at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:123)
	at org.apache.iotdb.db.engine.compaction.inner.utils.MultiTsFileDeviceIterator.<init>(MultiTsFileDeviceIterator.java:63)
	at org.apache.iotdb.db.engine.compaction.inner.utils.InnerSpaceCompactionUtils.compact(InnerSpaceCompactionUtils.java:71)
	at org.apache.iotdb.db.engine.compaction.inner.sizetiered.SizeTieredCompactionTask.doCompaction(SizeTieredCompactionTask.java:119)
	at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:68)
	at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:45)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2022-03-25 17:01:47,597 [pool-7-IoTDB-Compaction-2] WARN  o.a.i.d.e.c.i.s.SizeTieredCompactionTask:200 - root.test.g_0-0 [Compaction] Start to handle exception 
2022-03-25 17:01:47,597 [pool-7-IoTDB-Compaction-2] INFO  o.a.i.d.e.c.i.InnerSpaceCompactionExceptionHandler:85 - root.test.g_0-0 [Compaction][ExceptionHandler] some source files [file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722177-3064-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722257-3067-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722295-3068-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722342-3070-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722378-3071-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722426-3073-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722472-3074-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722533-3076-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722585-3077-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722633-3079-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722718-3082-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722768-3083-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722815-3085-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722857-3086-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722916-3088-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723012-3091-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723059-3092-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723102-3093-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723148-3095-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723185-3096-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723269-3099-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723316-3101-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723350-3102-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723400-3104-0-1.tsfile, compactionCandidate: false, compacting: true, file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198723437-3105-0-1.tsfile, compactionCandidate: false, compacting: true] is lost 
2022-03-25 17:01:47,597 [pool-7-IoTDB-Compaction-2] WARN  o.a.i.d.e.c.i.InnerSpaceCompactionExceptionHandler:92 - root.test.g_0-0 [Compaction][ExceptionHandler] target file file is ./sbin/../data/data/sequence/root.test.g_0/0/0/1648198722177-3064-1-1.inner, compactionCandidate: false, compacting: false does not exist either, do nothing.{color:#DE350B}* Set system to read-only *{color}
2022-03-25 17:01:47,597 [pool-7-IoTDB-Compaction-2] ERROR o.a.i.d.e.c.i.InnerSpaceCompactionExceptionHandler:106 - root.test.g_0-0 [Compaction][ExceptionHandler] Failed to handle exception, set allowCompaction to false 
2022-03-25 17:01:47,600 [pool-7-IoTDB-Compaction-6] ERROR o.a.i.t.f.f.LocalFSFactory:118 - Failed to move file from /data/iotdb/iotdb-sql/iotdb-sql/iotdb/db1/./sbin/../data/data/sequence/root.test.g_0/0/0/1648198733100-3420-1-1.inner to /data/iotdb/iotdb-sql/iotdb-sql/iotdb/db1/./sbin/../data/data/sequence/root.test.g_0/0/0/1648198733100-3420-1-1.tsfile.  
ja "	IOTDB	Closed	3	1	10039	pull-request-available
13517519	[monitor]the file count of datanode become negative when load tsfile	"[monitor]the file count of datanode become negative when load tsfile
 !image-2023-01-11-09-58-31-593.png|width=600! 

reproduction:
1. start 3C3D cluster
2. run command:
{code}
./load-tsfile.sh -h iotdb-46 -u root -pw root -f /data2/IOTDB-5262/datanode/data/sequence --verify false --onSuccess none
{code}

{code}
---------------------
Starting IoTDB Cli
---------------------
 _____       _________  ______   ______    
|_   _|     |  _   _  ||_   _ `.|_   _ \   
  | |   .--.|_/ | | \_|  | | `. \ | |_) |  
  | | / .'`\ \  | |      | |  | | |  __'.  
 _| |_| \__. | _| |_    _| |_.' /_| |__) | 
|_____|'.__.' |_____|  |______.'|_______/  version 1.0.1-SNAPSHOT (Build: 4a90dbc)
                                           

Successfully login at iotdb-46:6667
IoTDB> show regions
+--------+------------+-------+---------+-------------+-----------+----------+------------+-------+--------+
|RegionId|        Type| Status| Database|SeriesSlotNum|TimeSlotNum|DataNodeId|  RpcAddress|RpcPort|    Role|
+--------+------------+-------+---------+-------------+-----------+----------+------------+-------+--------+
|       0|SchemaRegion|Running|root.test|           71|          0|         1|172.20.70.44|   6667|Follower|
|       0|SchemaRegion|Running|root.test|           71|          0|         3|172.20.70.45|   6667|  Leader|
|       1|  DataRegion|Running|root.test|           24|         24|         1|172.20.70.44|   6667|  Leader|
|       1|  DataRegion|Running|root.test|           24|         24|         3|172.20.70.45|   6667|Follower|
|       2|  DataRegion|Running|root.test|           19|         24|         3|172.20.70.45|   6667|  Leader|
|       2|  DataRegion|Running|root.test|           19|         24|         5|172.20.70.46|   6667|Follower|
|       3|  DataRegion|Running|root.test|           24|         24|         1|172.20.70.44|   6667|Follower|
|       3|  DataRegion|Running|root.test|           24|         24|         5|172.20.70.46|   6667|  Leader|
|       4|  DataRegion|Running|root.test|           24|         24|         1|172.20.70.44|   6667|  Leader|
|       4|  DataRegion|Running|root.test|           24|         24|         3|172.20.70.45|   6667|Follower|
|       5|  DataRegion|Running|root.test|           21|         24|         3|172.20.70.45|   6667|  Leader|
|       5|  DataRegion|Running|root.test|           21|         24|         5|172.20.70.46|   6667|Follower|
+--------+------------+-------+---------+-------------+-----------+----------+------------+-------+--------+
Total line number = 12
It costs 0.126s
{code}

expect:
the file count and file size should always be positive"	IOTDB	Closed	3	1	10039	pull-request-available
13412981	Compaction returning more records than expeced  after “Too many open files ”	"h1. rel/0.12 0.12.3 rc2
commit a8c5dc8a13a4b90303fec4b29e40a1b33e6b737d
Author: HTHou <hhaonan@outlook.com>
Date: Sun Nov 14 12:46:16 2021 +0800

[maven-release-plugin] prepare release v0.12.3

 ulimit -n
*65536*


h1. 1. test data
10 sg / 100dev /100 sensor （DOUBLE）
10 dev/sg
1 sensor/dev
200000000 points/sensor

h1. 2. execte 100  queries  :  Too many open files

select count(s_0)  from each device ：
./sbin/start-cli.sh -p 6667 -e ""select count() from root.ip5.ip58.d_88  ""
./sbin/start-cli.sh -p 6667 -e ""select count(s_0) from root.ip5.ip59.d_99  ""
...

log_error.log :  Too many open files

+----------------------------+
|count(root.ip5.ip54.d_4.s_0)|
+----------------------------+
|                   {color:#DE350B}*200117000*{color}|
+----------------------------+
Total line number = 1

expected : 200000000 

"	IOTDB	Closed	3	1	10039	pull-request-available
13493429	readChunkMetadataAndConstructIndexTree is too slow for sealing TsFile	" !image-2022-10-28-16-48-57-842.png! 

you can see this method spend almost half time on Path:<init>, is there any possible to speed up this method?"	IOTDB	Closed	3	4	10039	pull-request-available
13430849	 Cannot select compaction candidate after exception occurrence in previous compaction	After exception ocurrence in compaction, the system cannot find eligible files to be compacted.	IOTDB	Closed	3	1	10039	pull-request-available
13494275	Remove data in compaction when the datatype of one series is not consistent in different tsfile	In some scenario, the data type of one series is different in different tsfile for some reason(eg, the bug in auto data type inference). Compaction should remove the data whose type is different with the type in MManager.	IOTDB	Resolved	3	2	10039	pull-request-available
13446739	[0.12] Chunk size overflow in level compaction	"In level compaction, the total size of two chunk to be merged is too large, leading to overflow of integer type.

!image-2022-05-25-16-17-04-777.png!"	IOTDB	Closed	3	1	10039	pull-request-available
13454578	DataRegion cannot recover from snapshot	DataRegion cannot recover from snapshot when restarting IoTDB. It seems that the path of tsfiles is wrong.	IOTDB	Closed	3	1	10039	pull-request-available
13469901	Remove DELETED in TsFileResourceStatus	When we remove a tsfile, just removing it from the TsFileResourceList is ok. I do not see the need to maintain this status.	IOTDB	Closed	3	4	10039	pull-request-available
13445351	Apply visitor pattern to DataRegionStateMachine	Visitor pattern is used to process plan node in SchemaRegionStateMachine and DistributedPlanner, this issue aims to apply visitor pattern to DataRegionStateMachine.	IOTDB	Closed	3	2	10039	pull-request-available
13409974	Fix the modification of max_select_unseq_file_num_in_each_compaction parameter does not take effect	"# Description
No matter how much we modify the value of parameter max_select_unseq_file_num_in_each_unseq_compaction in the configuration file, its value is always 2000 (default value) when IoTDB starts.
# Cause
When IoTDBDescriptor parsing the configuration file, it uses a wrong key ""max_open_file_num_in_each_unseq_compaction"" to get the value of this parameter. But the key of this parameter should be ""max_select_unseq_file_num_in_each_unseq_compaction"""	IOTDB	Closed	3	1	10039	pull-request-available
13485379	It is recommended to add real-time monitoring data for tsfile merge	"Currently, only sequence tsfile, unsequence tsfile, and wal folder are monitored in the grafana monitoring interface, but file used by compaction are not monitored.

!image-2022-10-10-15-50-49-400.png|width=531,height=121!"	IOTDB	Closed	4	4	10039	pull-request-available
13482181	Print log when a tsfile is compacted too many times in cross space compaction	If a tsfile is compacted too many times, it hurts a lot to the performance. We should print log to show why this tsfile is chosen to be compacted, in order that we can figure out the reason and avoid it.	IOTDB	Closed	3	4	10039	pull-request-available
13425400	Remove tsfile in memory before delete it in compaction	Remove tsfile in memory before delete it in compaction, prevents compaction from waiting for the query to end	IOTDB	Closed	3	1	10039	pull-request-available
13447597	DataRegion snapshot may meet file not found due to compaction	"When do snapshot in data region, we just list files in each folder and then do hard link.

If the compaction delete some file just between list file and do hard link, an exception will occur.

 

We need to get tsfiles from TsFileManager."	IOTDB	Closed	3	1	10039	pull-request-available
13431644	" ""overlapped data should be consumed first"" occurs when executing query"	Same as [IOTDB-2624|https://issues.apache.org/jira/browse/IOTDB-2624]	IOTDB	Closed	3	1	10039	pull-request-available
13506171	CompactionSchedulerTest failed in ci	"[https://github.com/apache/iotdb/actions/runs/3561408220/jobs/5982296370]

 
[INFO] Running org.apache.iotdb.db.engine.compaction.CompactionSchedulerTest 
[2550|https://github.com/apache/iotdb/actions/runs/3561408220/jobs/5982296370#step:10:2551]Error: Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.094 s <<< FAILURE! - in org.apache.iotdb.db.engine.compaction.CompactionSchedulerTest 
[2551|https://github.com/apache/iotdb/actions/runs/3561408220/jobs/5982296370#step:10:2552]Error: test3(org.apache.iotdb.db.engine.compaction.CompactionSchedulerTest) Time elapsed: 2.189 s <<< FAILURE! 
[2552|https://github.com/apache/iotdb/actions/runs/3561408220/jobs/5982296370#step:10:2553]java.lang.AssertionError: expected:<0> but was:<24> 
[2553|https://github.com/apache/iotdb/actions/runs/3561408220/jobs/5982296370#step:10:2554] at org.apache.iotdb.db.engine.compaction.CompactionSchedulerTest.test3(CompactionSchedulerTest.java:432)
 

!image-2022-11-28-12-23-21-956.png|width=721,height=273!"	IOTDB	Closed	3	3	10039	pull-request-available
13412995	CompactionSchedulerTest is block in CI	"!image-2021-11-21-18-23-29-112.png!

!image-2021-11-21-18-23-39-554.png!

!image-2021-11-21-18-23-48-177.png!

!image-2021-11-21-18-23-56-895.png!

!image-2021-11-21-18-24-06-210.png!

 "	IOTDB	Closed	3	1	10039	0.13.0
13486063	IndexOutOfBoundsException when compacting aligned series	!image-2022-10-13-15-45-07-212.png!	IOTDB	Closed	3	1	10039	pull-request-available
13430932	"[0.12] ""overlapped data should be consumed first"" occurs when executing query"	" !image-2022-02-28-15-44-47-656.png! 
When selecting source files for cross space compaction, some sequence files are not selected while they are overlapped with the unsequence files. As a result, the compacted sequence files overlap with other sequence files."	IOTDB	Closed	3	1	10039	pull-request-available
13407705	Data increases abnormally after IoTDB restarts	"# Description
 If the IoTDB is shutdown when compacting, when IoTDB is restored, there will be an abnormal increase in data.
 # Cause
 When SGP perform recover process, it new a RestorableTsFileIOWriter for each tsfile. However, it doesn't close the writer after using it.  !screenshot-1.png! So the output stream to the file will be kept in process. When recovering compaction, if the system tries to delete a tsfile, exception will occur because the handler of the file isn't removed.  !screenshot-2.png! The new compaction process of same file will continue to write on this undeleted file.

Then, the undeleted data will be kept in the target file."	IOTDB	Closed	3	1	10039	pull-request-available
13528066	Support network monitor in metrics	Supports monitoring network bandwidth, pps, etc.	IOTDB	Closed	3	2	10039	pull-request-available
13408578	StreamClosed exception occurs while doing compacting	"master （new_compaction） ：

{color:#DE350B}*IOException during compact :*{color}
(合并时，文件流异常）
{color:#DE350B}*2021-10-20 20:58:48,333 [pool-4-IoTDB-Compaction-6] ERROR o.a.i.d.e.c.t.AbstractCompactionTask:57 - Stream Closed
java.io.IOException: Stream Closed*{color}
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:326)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at org.apache.iotdb.tsfile.write.writer.LocalTsFileOutput.getPosition(LocalTsFileOutput.java:65)
        at org.apache.iotdb.tsfile.write.writer.TsFileIOWriter.writeChunk(TsFileIOWriter.java:224)
        at org.apache.iotdb.db.engine.compaction.inner.utils.InnerSpaceCompactionUtils.writeByAppendPageMerge(InnerSpaceCompactionUtils.java:174)
        at org.apache.iotdb.db.engine.compaction.inner.utils.InnerSpaceCompactionUtils.compact(InnerSpaceCompactionUtils.java:435)
        at org.apache.iotdb.db.engine.compaction.inner.sizetiered.SizeTieredCompactionTask.doCompaction(SizeTieredCompactionTask.java:136)
        at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:55)
        at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)


{color:#DE350B}*2021-10-20 23:08:36,137 [pool-4-IoTDB-Compaction-7] ERROR o.a.i.d.e.c.t.AbstractCompactionTask:57 - null
java.io.IOException: null*{color}
        at org.apache.iotdb.db.engine.compaction.inner.utils.InnerSpaceCompactionUtils.compact(InnerSpaceCompactionUtils.java:298)
        at org.apache.iotdb.db.engine.compaction.inner.sizetiered.SizeTieredCompactionTask.doCompaction(SizeTieredCompactionTask.java:136)
        at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:55)
        at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

"	IOTDB	Closed	4	1	10039	pull-request-available
13477176	[Atmos][WeeklyTest]Increasing number of tsfiles	"WeeklyTest scenario (Required data volume) 
Recently, the number of tsfiles increases abnormally.

Please reser to below picture:

Time line: 

08/12 eable time partition (interval of one week);

08/15 interval of one day

!image-2022-08-17-15-19-35-455.png|width=567,height=481!"	IOTDB	Closed	3	4	10039	pull-request-available
13524178	Support disk io status monitor in metrics dashboard 	Monitoring disk IO helps us to get a clearer picture of the database state, but it is not currently supported by IoTDB. The purpose of this issue is to monitor disk IO under Linux.	IOTDB	Closed	3	2	10039	pull-request-available
13441037	DataRegion Snapshot	"When we use Raft, we need the DataRegionStateMachine to support the snapshot function.

 

The snapshot could be used for recovery."	IOTDB	Closed	3	3	10039	pull-request-available
13429659	Compaction recover in 0.13.0 is not compatible with 0.12.5	Compaction recover in 0.13.0 is not compatible with 0.12.5, which may cause data loss when recovering IoTDB.	IOTDB	Closed	3	1	10039	pull-request-available
13504893	DataRegion StateMachine support for Ratis Snapshot 	"Currently RatisConsensus cannot transfer snapshot to a newly joined follower. 

StateMachine should place all snapshot files under the given directory."	IOTDB	Closed	3	4	10039	pull-request-available
13528353	Inode cannot be released after deletion causing waste of disk space	"!image-2023-03-14-15-17-58-568.png!

 

The files are deleted after compaction, but the inode cannot be actually deleted because the file handlers is occupied by the process, which leads to a waste of disk space. "	IOTDB	Closed	3	1	10039	pull-request-available
13424810	Weekly test hanged when using benchmark for read and write mixing	"Due to the wrong release of the lock when the cross-space merge was released, the mixed execution of the weekly test read and write was stuck.
[commitid: 4e87d43]The weekly test uses the benchmark for reading and writing mixed progress to hang when the progress is about 15%.

 "	IOTDB	Closed	3	1	10039	pull-request-available
13533943	Network Metrics Failed to start	"Reporting NPE when booting IoTDB.

 

!image-2023-04-25-15-25-26-773.png!"	IOTDB	Open	3	1	10039	pull-request-available
13439314	[Conf]Got a warn when start iotdb instance	"复现步骤：

使用当前master分支代码0ec17d79eb7673fb23d4986e19b39ff1e1f7fe0c编译生成的版本，

启动iotdb，查看启动日志包含以下告警信息：

2022-04-13 09:34:54,419 [main] WARN  o.a.i.d.c.IoTDBDescriptor:908 - Incorrect format in config file, use default configuration
java.lang.RuntimeException: Illegal Cross Compaction Strategy REWRITE
        at org.apache.iotdb.db.engine.compaction.constant.CrossCompactionSelector.getCrossCompactionStrategy(CrossCompactionSelector.java:38)
        at org.apache.iotdb.db.conf.IoTDBDescriptor.loadProps(IoTDBDescriptor.java:367)
        at org.apache.iotdb.db.conf.IoTDBDescriptor.<init>(IoTDBDescriptor.java:69)
        at org.apache.iotdb.db.conf.IoTDBDescriptor$IoTDBDescriptorHolder.<clinit>(IoTDBDescriptor.java:1608)
        at org.apache.iotdb.db.conf.IoTDBDescriptor.getInstance(IoTDBDescriptor.java:73)
        at org.apache.iotdb.db.metadata.LocalSchemaProcessor.<clinit>(LocalSchemaProcessor.java:130)
        at org.apache.iotdb.db.service.IoTDB.<clinit>(IoTDB.java:65)

!image-2022-04-13-09-44-32-365.png!"	IOTDB	Closed	4	1	10039	pull-request-available
13436108	Clear Compaction Framework	!image-2022-03-28-16-25-31-132.png!	IOTDB	Closed	3	4	10039	pull-request-available
13417034	[0.12] Data loss after IoTDB's recovery	"# Description
After iotdb's recovery, some data cannot be queried, it seems that they are lost.
# Cause
When recovering sgp, the tsfiles with merge level greater than 0, is not recovered by `TsFileRecoverPerformer`, which results in uninitialization of these `TsFileResource` object ."	IOTDB	Closed	3	1	10039	pull-request-available
13482919	[Atmos]Cross_space and unseq_space compaction are slower at common timeseries and template timeseries	"[Atmos]Cross_space and unseq_space compaction are slower at common timeseries and template timeseries

[http://111.202.73.147:13000/d/ta84fXYnz/atm-biao-zhun-da-qi-ya-huan-jing-he-bing?orgId=1]

!image-2022-09-23-17-42-33-888.png|width=781,height=380!"	IOTDB	Closed	3	1	10039	pull-request-available
13450578	Apply Producer-Consumer pattern to compaction submission	Currently we use a timing thread to submit compaction task from task queue to execution thread pool. In fact we can use a simple producer-consumer pattern do this job.	IOTDB	Closed	3	4	10039	pull-request-available
13438597	Move PartialPath to node-commons module	"Currently, both the server and confignode modules need PartialPath class. We need to move PartialPath to the node-commons module.

However, PartialPath depends on many classes that only used in server. Need to remove these dependencies.

 

!image-2022-04-08-20-45-43-722.png!"	IOTDB	Closed	3	4	10039	pull-request-available
13425542	[0.12] NPE when merge recover	 !image-2022-01-29-17-42-10-932.png! 	IOTDB	Closed	3	1	10039	pull-request-available
13479005	Persist ChunkMetadata in TsFileIOWriter ahead of time to save memory	In the scenario of massive active sequences, the ChunkMetadata in TsFileIOWriter occupies a large part of the memory, which may lead to OOM.	IOTDB	Closed	3	4	10039	pull-request-available
13450029	Make python test stable	https://github.com/apache/iotdb/runs/6891994927?check_suite_focus=true	IOTDB	Closed	4	3	10039	pull-request-available
13535297	Compaction module seems to get stuck	"During cross-space compaction, sometimes due to the selection strategy of the selector, the system may execute compaction tasks that require a large amount of memory. The memory demand of a single compaction task may be larger than the total memory size allocated to the compaction module by the system. In this case, these compaction tasks will be stuck at the memory allocation step, waiting indefinitely. Subsequent compaction tasks cannot be executed either, making the compaction module appear as if it has ""stopped"".

 

!image-2023-05-07-15-21-43-672.png!

!image-2023-05-07-15-22-01-641.png!"	IOTDB	Open	3	1	10039	pull-request-available
13480118	Fix failed to query data after loading snapshot from other node	"!image-2022-09-05-19-21-21-367.png!

 

This node fails to query data after loading snapshot from other node."	IOTDB	Closed	3	1	10039	pull-request-available
13482922	There is .cmt file remain in disk	!image-2022-09-23-17-59-11-345.png!	IOTDB	Closed	3	1	10039	pull-request-available
13478389	QueryContext occupy too much memory	"In the case of a large number of active sequences, a single QueryContext instance takes up 1.6GB of memory.

!image-2022-08-24-19-06-44-287.png!"	IOTDB	Closed	3	1	10039	pull-request-available
13416211	[0.12] Simplify compaction recover	Current compaction recovery must run after  SGP recovery, which can be logistically complex and error-prone. In this issue, we refer the recovery of compaction before the recovery of SGP, and remove data structures such as recoverList from SGP to simplify the recovery process.	IOTDB	Closed	3	4	10039	pull-request-available
13444019	Parameter max_select_unseq_file_num_in_each_unseq_compaction doesn't work	In 0.12, the parameter max_select_unseq_file_num_in_each_unseq_compaction aims to limit the seleted files in one cross compaction, but it doesn't work.	IOTDB	Open	3	1	10039	pull-request-available
13436125	An exception occurs when the weekly test is mixed with read and write	"Sequence, when performing read-write mixing, two errors are encountered:
Negative Position and COMPACTION_CANDIDATE while its status is UNCLOSED.

Master Commit ID: 2022-03-28-12-47-46_5404730


Sequential read and write mixing by using benchmark.
Please refer to the attachment for the configuration of the benchmark."	IOTDB	Closed	3	1	10039	pull-request-available
13461526	Add log for snapshot taker and loader	IoTDB does not print logs when taking snapshot and loading snapshot, it is not convenience for us to monitor it and debug it.	IOTDB	Closed	3	4	10039	pull-request-available
13408896	 Modify the data dirs after shutting down IoTDB, then restarting, the data increases abnormally	"# Description
 If the IoTDB is shut down when compacting. If we change the data directory, and rerun IoTDB, there will be an abnormal increase in data.
 # Cause
 The compaction log (cross-space compaction and in-space compaction) record the absolute path of source file and target file, if the data diretory is changed, the recover of compaction cannot find the target file and source files."	IOTDB	Closed	3	1	10039	pull-request-available
13408897	The file disk space usage increases abnormally after IoTDB restarts	"# Description
 If the IoTDB is shut down when cross compaction when IoTDB is restarted, there will be abnormal redundant data in TsFile, but the abnormal data cannot be seen from the client.
 # Cause
 1. The target file of cross-space compaction is deleted when sgp recover, the cross-space compaction recover can never find the target file.
 2. The MergeLogAnalyzer searches for a log that is never written to the log file.
 3. The resume cross-space compaction process gets the schema from merge resources, but the schema list in merge resources is always empty when performing recovery.
 4. The recovery of cross-space compaction uses an InputStream to truncate the file, which is forbidden by JVM.
 5. The ChunkMetadata list in the last chunk group is lost when using a RestorableTsFileIOWriter to continue to write on the target file.

 

 "	IOTDB	Closed	3	1	10039	pull-request-available
13480209	NoSuchFileExeception when creating snapshot	"NoSuchFileException is threw when taking snapshot in DataRegion migration

!image-2022-09-06-14-22-05-045.png|width=1270,height=409!"	IOTDB	Closed	3	1	10039	pull-request-available
13502779	Add DELETED status back to TsFileResource	"!image-2022-11-16-09-08-46-841.png!

 

!image-2022-11-16-09-09-10-601.png!"	IOTDB	Closed	3	4	10039	pull-request-available
13449394	Change compaction execution thread pool to fix size thread pool 	Current compaction task execution thread pool and sub task execution thread pool are scheduled thread pool, but we don't use them to execute any timed task. It is better to change them to normal fix size thread pool.	IOTDB	Closed	3	1	10039	pull-request-available
13431393	[0.12] The new file has a higher compact priority than the old file in unseq compaction	"When recovering the TsFile list in the storage group, we traverse the list to recover and put the TsFile into TsFileManagement in descending order. Which causes the TsFile list in memory is in descending order of time.

Then the compaction selection selects the unsequence file from a larger timestamp to a lower timestamp, which is not correct. We should merge unsequence files from the old to the new."	IOTDB	Closed	3	1	10039	pull-request-available
13445780	Cannot insert data to data node	"Cannot insert data to data node, and npe is throw.

!image-2022-05-19-16-05-55-348.png!"	IOTDB	Closed	3	1	10039	pull-request-available
13470906	target file is not complete, and some source files is lost	"Meet error in compaction, then set system mode to read-only.
{code:java}
2022-07-09 17:45:20,943 [pool-39-IoTDB-RPC-Client-7] WARN  o.a.i.d.metadata.MManager:2009 - meet error when check root.DT.BJ.FW.KT_J0CRR11FA010_CL, message: Path [root.DT.BJ.FW.KT_J0CRR11FA010_CL] does not exist 
2022-07-09 17:45:20,944 [pool-39-IoTDB-RPC-Client-7] WARN  o.a.i.d.u.ErrorHandlingUtils:146 - Exception occurred while processing non-query.  
org.apache.iotdb.db.exception.BatchProcessException: Batch process failed:[TSStatus(code:304, message:org.apache.iotdb.db.exception.metadata.PathNotExistException: Path [KT_J0CRR11FA010_CL] does not exist)]
    at org.apache.iotdb.db.qp.executor.PlanExecutor.insert(PlanExecutor.java:1632)
    at org.apache.iotdb.db.qp.executor.PlanExecutor.processNonQuery(PlanExecutor.java:284)
    at org.apache.iotdb.db.service.basic.StandaloneServiceProvider.executeNonQuery(StandaloneServiceProvider.java:53)
    at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeOperationSync(TSServiceImpl.java:2165)
    at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeOperationSync.getResult(TSIService.java:3989)
    at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeOperationSync.getResult(TSIService.java:3969)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:830)
2022-07-09 17:45:21,653 [pool-7-IoTDB-Compaction-10] INFO  o.a.i.d.e.c.i.s.SizeTieredCompactionTask:95 - root.DT.HX-0 [Compaction] starting compaction task with 30 files 
2022-07-09 17:45:21,654 [pool-7-IoTDB-Compaction-10] INFO  o.a.i.d.e.c.i.s.SizeTieredCompactionTask:112 - root.DT.HX-0 [SizeTiredCompactionTask] Close the logger 
2022-07-09 17:45:21,654 [pool-7-IoTDB-Compaction-10] INFO  o.a.i.d.e.c.i.s.SizeTieredCompactionTask:114 - root.DT.HX-0 [Compaction] compaction with [file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657151026844-6-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657158229818-7-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657165428104-8-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657172631386-9-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657179827208-10-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657187027013-11-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657194231019-12-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657201430483-13-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657208626434-14-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657215828924-15-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657223028454-16-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657230224566-17-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657237428049-18-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657244627481-19-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657251827900-20-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657259027626-21-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657266227844-22-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657273423548-23-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657280623088-24-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657287827377-25-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657295023561-26-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657302227862-27-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657309423854-28-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657316627249-29-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657323823301-30-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657331023417-31-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657338227391-32-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657345427054-33-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657352623586-34-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657359823237-35-0-0.tsfile, status: ] 
2022-07-09 17:45:21,670 [pool-7-IoTDB-Compaction-10] ERROR o.a.i.d.e.c.i.s.SizeTieredCompactionTask:196 - root.DT.HX-0 [Compaction] Throwable is caught during execution of SizeTieredCompaction, {} 
java.lang.IllegalArgumentException: Negative position
    at java.base/sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:785)
    at org.apache.iotdb.tsfile.read.reader.LocalTsFileInput.read(LocalTsFileInput.java:90)
    at org.apache.iotdb.tsfile.read.TsFileSequenceReader.readTailMagic(TsFileSequenceReader.java:230)
    at org.apache.iotdb.tsfile.read.TsFileSequenceReader.loadMetadataSize(TsFileSequenceReader.java:203)
    at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:140)
    at org.apache.iotdb.tsfile.read.TsFileSequenceReader.<init>(TsFileSequenceReader.java:123)
    at org.apache.iotdb.db.query.control.FileReaderManager.get(FileReaderManager.java:129)
    at org.apache.iotdb.db.engine.compaction.inner.utils.MultiTsFileDeviceIterator.<init>(MultiTsFileDeviceIterator.java:88)
    at org.apache.iotdb.db.engine.compaction.CompactionUtils.compact(CompactionUtils.java:98)
    at org.apache.iotdb.db.engine.compaction.inner.sizetiered.SizeTieredCompactionTask.doCompaction(SizeTieredCompactionTask.java:121)
    at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:67)
    at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:38)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:830)
2022-07-09 17:45:21,670 [pool-7-IoTDB-Compaction-10] WARN  o.a.i.d.e.c.i.s.SizeTieredCompactionTask:200 - root.DT.HX-0 [Compaction] Start to handle exception 
2022-07-09 17:45:21,671 [pool-7-IoTDB-Compaction-10] INFO  o.a.i.d.e.c.t.CompactionExceptionHandler:63 - root.DT.HX-0 [Compaction][ExceptionHandler] inner space compaction start handling exception, source seqFiles is [], source unseqFiles is [file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657151026844-6-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657158229818-7-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657165428104-8-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657172631386-9-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657179827208-10-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657187027013-11-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657194231019-12-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657201430483-13-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657208626434-14-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657215828924-15-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657223028454-16-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657230224566-17-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657237428049-18-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657244627481-19-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657251827900-20-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657259027626-21-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657266227844-22-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657273423548-23-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657280623088-24-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657287827377-25-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657295023561-26-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657302227862-27-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657309423854-28-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657316627249-29-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657323823301-30-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657331023417-31-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657338227391-32-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657345427054-33-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657352623586-34-0-0.tsfile, status: , file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657359823237-35-0-0.tsfile, status: ]. 
2022-07-09 17:45:21,672 [pool-7-IoTDB-Compaction-10] ERROR o.a.i.d.e.c.t.CompactionExceptionHandler:237 - root.DT.HX-0 [Compaction][ExceptionHandler] target file file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657359823237-35-1-0.inner, status:  is not complete, and some source files [file is /home/xdb/xdb_home/apache-iotdb-0.13.1-all-bin/sbin/../data/data/unsequence/root.DT.HX/0/0/1657359823237-35-0-0.tsfile, status: ] is lost, do nothing. Set allowCompaction to false 
2022-07-09 17:45:21,672 [pool-7-IoTDB-Compaction-10] ERROR o.a.i.d.e.c.t.CompactionExceptionHandler:99 - [Compaction][ExceptionHandler] Fail to handle inner space compaction exception, set allowCompaction to false in root.DT.HX-0 
2022-07-09 17:45:21,674 [pool-39-IoTDB-RPC-Client-7] WARN  o.a.i.d.u.ErrorHandlingUtils:155 - Exception occurred while processing non-query.  
org.apache.iotdb.db.exception.StorageEngineReadonlyException: Database is read-only, and does not accept non-query operation now
    at org.apache.iotdb.db.service.basic.StandaloneServiceProvider.executeNonQuery(StandaloneServiceProvider.java:51)
    at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeOperationSync(TSServiceImpl.java:2165)
    at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeOperationSync.getResult(TSIService.java:3989)
    at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeOperationSync.getResult(TSIService.java:3969)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:830)
2022-07-09 17:45:21,674 [pool-39-IoTDB-RPC-Client-6] WARN  o.a.i.d.u.ErrorHandlingUtils:155 - Exception occurred while processing non-query.  
org.apache.iotdb.db.exception.StorageEngineReadonlyException: Database is read-only, and does not accept non-query operation now
    at org.apache.iotdb.db.service.basic.StandaloneServiceProvider.executeNonQuery(StandaloneServiceProvider.java:51)
    at org.apache.iotdb.db.service.thrift.impl.TSServiceImpl.executeOperationSync(TSServiceImpl.java:2165)
    at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeOperationSync.getResult(TSIService.java:3989)
    at org.apache.iotdb.service.rpc.thrift.TSIService$Processor$executeOperationSync.getResult(TSIService.java:3969)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:830) {code}
TODO:

(1) In the IoTDBConfig.setReadOnly() function, if it receive a true, we need to log the stack trace with the information ""change system mode to read-only"". To avoid missing the log outside.

if (readOnly) {

logger.error(""change system mode to read-only"", new RuntimeException());

}

(2) Check all places that set read-only, this is a dangerous operation in the deployment environment, and we should avoid using this if not necessary.

(3) Check why compaction lost file.

 "	IOTDB	Closed	2	1	10039	pull-request-available
13409972	Remove the operation of clearing the cache after the compaction is over	In origin compaction process, the ChunkCache and TimeseriesMetadataCache will be clear when the compaction finishes. This is because the key of Cache doesn't contain the times of compaction. If we don't clear the cache, the correctness of the query will be affected. But everytime we clear the cache, the efficiency of query will be decreased. If the cached key contains the number of compaction, we do not need to clear the cache at the end of the compaction, which can improve the efficiency of the query.	IOTDB	Closed	3	4	10039	pull-request-available
13494280	Inner space compaction may be blocked	"See the code below,

!image-2022-11-02-17-32-11-934.png|width=664,height=221!

Let's call the file which needn't be merged as an OK-file.

When selecting the candidate files, an OK-file will block the search to next level and lead to no compaction task be added to the queue.

Please check the logic here."	IOTDB	Resolved	3	1	10039	pull-request-available
13468387	Stop compaction schedule when all compaction is disable	Even if we close all compaction, the system will start threads periodically for compaction scheduling. We should stop it.	IOTDB	Closed	3	1	10039	pull-request-available
13505494	stop-datanode.sh takes snapshot	"master_1123_32e2f98
1. 正常停止datanode    stop-datanode.sh 会触发snapshot （单线程），为什么？
 !image-2022-11-23-17-15-27-900.png! "	IOTDB	Closed	4	4	10039	pull-request-available
13477970	[ snapshot.log ] Original tsfile paths and hard link paths should not be absolute paths	"master_0819_9e0512a
snapshot.log中记录的原始文件路径及硬链接路径不应该是绝对路径（考虑用户会修改dir导致丢数据）：
 !image-2022-08-22-18-01-04-825.png!"	IOTDB	Closed	3	1	10039	pull-request-available
13445041	Compaction is not well-distributed across sgs	!image-2022-05-16-09-36-56-978.png!	IOTDB	Closed	3	4	10039	pull-request-available
13487020	Support broken tsfile rewrite	TsFileRewriteTool only supports rewriting complete tsfile to IoTDB. If we want to load good chunks from a broken TsFile, RewriteTool does not support this feature. This feature can be used to restore as much data as possible from TsFiles that have been corrupted by accident (write errors, disk corruption, etc.).	IOTDB	Resolved	3	2	10039	pull-request-available
13518511	[Metric] Add mods file metrics	Maintain the file number in StorageEngine, and update these metrics each time. Then, we do not to calculate the file number in time.	IOTDB	Resolved	3	4	10039	pull-request-available
13424358	fix IoTDBRemovePartitionIT	The `IoTDBRemovePartitionIT` is not stable in master branch. This issue try to fix it.	IOTDB	Closed	3	1	10039	pull-request-available
13528057	BufferedUnderflowException occurs in inner space compaction	!image-2023-03-11-12-34-25-210.png!	IOTDB	In Progress	3	1	10039	pull-request-available
13470484	[ snapshot ] consensus_dir and data_dirs are not on the same disk , snapshot fails	"master_0707_bd0bab3
RatisConsensus ， consensus_dir和data_dirs不在同一块磁盘，快照失败
# consensus_dir=data/consensus   (/data3下）
data_dirs=/data2/mpp_data/data,/data1/mpp_data/data

2022-07-07 15:01:08,227 [192.168.130.3_40010@group-000100000005-StateMachineUpdater] ERROR o.a.i.d.e.s.SnapshotTaker:102 - Fail to create snapshot
java.nio.file.FileSystemException: /data3/cluster_test/master_0707_bd0bab3/datanode/sbin/../data/consensus/data_region/47474747-4747-4747-4747-000100000005/sm/1_399999/sequence/root.test.g_0/5/0/1657176628996-16-0-0.tsfile.resource -> /data1/mpp_data/data/sequence/root.test.g_0/5/0/1657176628996-16-0-0.tsfile.resource: Invalid cross-device link
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixFileSystemProvider.createLink(UnixFileSystemProvider.java:476)
        at java.nio.file.Files.createLink(Files.java:1086)
        at org.apache.iotdb.db.engine.snapshot.SnapshotTaker.createFileSnapshot(SnapshotTaker.java:184)
        at org.apache.iotdb.db.engine.snapshot.SnapshotTaker.takeFullSnapshot(SnapshotTaker.java:100)
        at org.apache.iotdb.db.consensus.statemachine.DataRegionStateMachine.takeSnapshot(DataRegionStateMachine.java:66)
        at org.apache.iotdb.consensus.ratis.ApplicationStateMachineProxy.takeSnapshot(ApplicationStateMachineProxy.java:163)
        at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:270)
        at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:262)
        at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:183)
        at java.lang.Thread.run(Thread.java:748)
2022-07-07 15:01:08,229 [192.168.130.3_40010@group-000100000005-StateMachineUpdater] ERROR o.a.i.d.e.s.SnapshotTaker:194 - Failed to delete link file /data3/cluster_test/master_0707_bd0bab3/datanode/sbin/../data/consensus/data_region/47474747-4747-4747-4747-000100000005/sm/1_399999/sequence after failing to create snapshot
"	IOTDB	Closed	3	1	10039	pull-request-available
13471318	[ stop-server.sh & compaction ] [pool-8-IoTDB-Compaction-6] ERROR o.a.i.t.r.TsFileSequenceReader:1089 - Exception happened while reading chunk of  xx ; java.nio.channels.ClosedByInterruptException: null	"rel/0.13 89d750ca2b2e9ece690e28cbb189be655c0a80e3
读写混合操作，写入顺序数据（对齐序列），正常停止iotdb 报错：

2022-07-13 11:24:52,009 [pool-8-IoTDB-Compaction-6] ERROR o.a.i.t.r.r.LocalTsFileInput:92 - Error happened while reading ./sbin/../data/data/sequence/root.test.g_7/0/0/1657682342332-63-0-0.tsfile from position 4420777
2022-07-13 11:24:52,011 [pool-8-IoTDB-Compaction-6] ERROR o.a.i.t.r.TsFileSequenceReader:1053 - Exception happened while reading chunk header of ./sbin/../data/data/sequence/root.test.g_7/0/0/1657682342332-63-0-0.tsfile
java.nio.channels.ClosedByInterruptException: null
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
        at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:746)
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
        at org.apache.iotdb.tsfile.read.reader.LocalTsFileInput.read(LocalTsFileInput.java:90)
        at org.apache.iotdb.tsfile.file.header.ChunkHeader.deserializeFrom(ChunkHeader.java:177)
        at org.apache.iotdb.tsfile.read.TsFileSequenceReader.readChunkHeader(TsFileSequenceReader.java:1051)
        at org.apache.iotdb.tsfile.read.TsFileSequenceReader.readMemChunk(TsFileSequenceReader.java:1083)
        at org.apache.iotdb.tsfile.read.TsFileAlignedSeriesReaderIterator.nextReader(TsFileAlignedSeriesReaderIterator.java:64)
        at org.apache.iotdb.db.engine.compaction.inner.utils.AlignedSeriesCompactionExecutor.execute(AlignedSeriesCompactionExecutor.java:122)
        at org.apache.iotdb.db.engine.compaction.inner.utils.InnerSpaceCompactionUtils.compactAlignedSeries(InnerSpaceCompactionUtils.java:147)
        at org.apache.iotdb.db.engine.compaction.inner.utils.InnerSpaceCompactionUtils.compact(InnerSpaceCompactionUtils.java:80)
        at org.apache.iotdb.db.engine.compaction.inner.sizetiered.SizeTieredCompactionTask.doCompaction(SizeTieredCompactionTask.java:122)
        at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:69)
        at org.apache.iotdb.db.engine.compaction.task.AbstractCompactionTask.call(AbstractCompactionTask.java:39)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

复现流程
1. 192.168.10.68 72C256G
数据库默认配置参数。

2. bm 运行附件中的脚本（对齐序列）

3. bm写入完成，正常停止数据库
./sbin/stop-server.sh
有error。"	IOTDB	Closed	3	1	10039	pull-request-available
13424958	[0.12] Division by zero when recovering merge	 !image-2022-01-26-23-38-25-630.png|thumbnail! 	IOTDB	Closed	3	1	10039	pull-request-available
13431458	[compaction level-2] The write performance deteriorates severely	"测试版本master-pre3
测试环境：私有云2台，8C32G
问题描述：
    长测配置，对齐序列，无乱序，运行5小时40分钟，发生合并2层文件，随之出现写入性能下降严重，同时GC上升。

测试结果：
1. 写入性能，绿色的线：
 !screenshot-2.png! 

2. master-pre3 printgc+开合并  Q7 GROUP BY语句按小时聚集求平均值-查询耗时
  !screenshot-1.png! 
3.磁盘IO
 !image-2022-03-02-17-51-49-608.png! 
4.GC
 !image-2022-03-02-17-52-06-148.png! 
5.log见附件
6.系统启动5小时40分钟，合并开始进行2层文件的合并，同时期显示gc次数上升，写入性能下降。因此推断合并2层文件导致了写入性能下降。"	IOTDB	Closed	3	1	10039	pull-request-available
13504790	[metrics] The seq file size in grafana is inconsistent with the actual query	"Start 3C3D, 1 replication, executed writing for 3 hours, and check that the size file of seq is incorrect.

!image-2022-11-21-18-19-28-703.png!

!image-2022-11-21-18-18-53-090.png!"	IOTDB	Closed	3	1	10039	pull-request-available
13539086	NullPointerException when updating Cpu metrics	!image-2023-06-07-21-03-45-246.png!	IOTDB	Open	3	1	10039	pull-request-available
