id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13415814	ResourceLimitCache leaks permits	{{ResourceLimitCache}} limits its size by using a group of permits.  {{put}} requires free permit before it can add data.  However, {{removeIf}} does not release permits.	HDDS	Resolved	2	1	1699	pull-request-available
13543455	Log more information about failed EC block allocation	We should include more information in the log if EC block allocation is rejected due to too many open pipelines.  ALLOCATE_BLOCK request's ExcludeList is not included in SCM audit log (maybe due to potentially large size?).  However, this piece of information may be important to understand why block allocation fails.	HDDS	Resolved	3	4	1699	pull-request-available
13517234	Simplify DatanodeDetails#toString to improve log messages	"{{DatanodeDetails#toString}} outputs too much detail for it to be usable in each and every log message related to datanodes.  Thus log statements currently have to build their own output (e.g. for list of hosts, etc.), leading to duplication.

The goal of this task is to replace {{toString}} with less verbose ({{getHostnameAndIP}} + UUID) implementation, and keep the current one for occasional usage as {{toDebugString}}.  Need to be verify if this causes and regressions."	HDDS	Resolved	3	3	1699	pull-request-available
13261237	Avoid buffer copying in GrpcReplicationService	In GrpcOutputStream, it writes data to a ByteArrayOutputStream and copies them to a ByteString.	HDDS	Resolved	3	4	1699	performance, pull-request-available
13316901	Ratis config key mismatch	"Some of the Ratis configurations in integration tests are not applied due to mismatch in config keys.
 # [Ratis|https://github.com/apache/incubator-ratis/blob/master/ratis-client/src/main/java/org/apache/ratis/client/RaftClientConfigKeys.java#L41-L53]: {{raft.client.rpc.watch.request.timeout}}
 [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCommitWatcher.java#L119-L122]: {{raft.client.watch.request.timeout}}
 # [Ratis|https://github.com/apache/incubator-ratis/blob/4db4f804aa90f9900cda08c79b54a45f80f4213b/ratis-server/src/main/java/org/apache/ratis/server/RaftServerConfigKeys.java#L470-L473]: {{raft.server.notification.no-leader.timeout}}
 [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/conf/DatanodeRatisServerConfig.java#L42]: {{raft.server.Notification.no-leader.timeout}}"	HDDS	Resolved	3	1	1699	pull-request-available
13437800	Bump Jackson Databind	Upgrade Jackson Databind to latest.	HDDS	Resolved	3	3	1699	pull-request-available
13562953	Clean up test dependencies	" * Provide the same set of basic test dependencies for all modules.
 * Remove leftover dependencies (mostly related to JUnit4).
"	HDDS	Resolved	3	7	1699	pull-request-available
13590128	Log for EC reconstruction command lists the missing indexes as ASCII control characters	"Logs for EC reconstruction command lists the missing indexes as ASCII control characters like ^A ^B.

 
{noformat}
2024-08-27 05:28:34,857 INFO [node1-UnderReplicatedProcessor]-org.apache.hadoop.hdds.scm.container.replication.ReplicationManager: Sending command [reconstructECContainersCommand: containerID: 15001, replicationConfig: EC{rs-3-2-1024k}, sources: [693753b9-cfb5-4bcc-9863-273cf3d32d05 replicaIndex: 3, 20213e45-1d8c-4224-96be-83d2c7f85c00 replicaIndex: 4, d6d0d2b5-dab9-48ec-8866-54b3d70f8b37 replicaIndex: 5], targets: [dd2d9383-a4c8-4aa9-9290-7d5494618a3a, 9b08f05b-2cbb-4bba-bcf5-1c1bd5d1ac01], missingIndexes: ^A^B] for container ContainerInfo{id=#15001, state=CLOSED, stateEnterTime=2024-08-27T05:26:28.228489Z, pipelineID=PipelineID=2b855dc6-d3fb-47a4-87d2-4b4b525bbae3, owner=ozone1724217699} to dd2d9383-a4c8-4aa9-9290-7d5494618a3a with datanode deadline 1724737384857 and scm deadline 1724737414857{noformat}
This is due to the change made as part of HDDS-10756"	HDDS	Resolved	3	1	1699	pull-request-available
13316718	Avoid HddsProtos.PipelineID#toString	"{{PipelineID}} was recently changed to have integer-based ID in addition to the string ID.  Now log messages including {{PipelineID}} span multiple lines:

{code:title=https://github.com/elek/ozone-build-results/blob/92d31c9b58065b37a371c71c97b346f99163318d/2020/07/11/1626/acceptance/docker-ozone-ozone-freon-scm.log#L218-L223}
datanode_1  | 2020-07-11 13:07:00,540 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: ""8101dcbf-1a28-4f20-863a-0616b4e4bc4b""
datanode_1  | uuid128 {
datanode_1  |   mostSigBits: -9150790254504423648
datanode_1  |   leastSigBits: -8774694229384053685
datanode_1  | }
datanode_1  | .
{code}"	HDDS	Resolved	3	1	1699	pull-request-available
13291773	Intermittent failure in TestReconWithOzoneManager due to BindException	"TestReconWithOzoneManager may fail with BindException:

{code:title=https://github.com/apache/hadoop-ozone/pull/677/checks?check_run_id=507376007}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.707 s <<< FAILURE! - in org.apache.hadoop.ozone.recon.TestReconWithOzoneManager
org.apache.hadoop.ozone.recon.TestReconWithOzoneManager  Time elapsed: 19.706 s  <<< ERROR!
picocli.CommandLine$ExecutionException: Error while calling command (org.apache.hadoop.ozone.recon.ReconServer@23f74a49): java.net.BindException: Port in use: 0.0.0.0:36263
	...
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:534)
	at org.apache.hadoop.ozone.recon.TestReconWithOzoneManager.init(TestReconWithOzoneManager.java:109)
	...
Caused by: java.net.BindException: Port in use: 0.0.0.0:36263
	at org.apache.hadoop.hdds.server.http.HttpServer2.constructBindException(HttpServer2.java:1200)
	at org.apache.hadoop.hdds.server.http.HttpServer2.bindForSinglePort(HttpServer2.java:1222)
	at org.apache.hadoop.hdds.server.http.HttpServer2.openListeners(HttpServer2.java:1281)
	at org.apache.hadoop.hdds.server.http.HttpServer2.start(HttpServer2.java:1136)
	at org.apache.hadoop.hdds.server.http.BaseHttpServer.start(BaseHttpServer.java:252)
	at org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:128)
	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:106)
	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50)
	at picocli.CommandLine.execute(CommandLine.java:1173)
	... 27 more
{code}

{code:title=test output}
2020-03-14 06:17:08,677 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(284)) - HTTP server of ozoneManager listening at http://0.0.0.0:36263
...
2020-03-14 06:17:11,589 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(170)) - Starting Web-server for recon at: http://0.0.0.0:36263
...
2020-03-14 06:17:12,756 [main] INFO  recon.ReconServer (ReconServer.java:start(125)) - Starting Recon server
2020-03-14 06:17:12,757 [main] INFO  http.HttpServer2 (HttpServer2.java:start(1139)) - HttpServer.start() threw a non Bind IOException
java.net.BindException: Port in use: 0.0.0.0:36263
...
{code}"	HDDS	Resolved	4	1	1699	pull-request-available
13566481	Refactor some constructors in SCM to avoid too many parameters	"{{SCMContext}} is created using builder.  To avoid too many parameters in the constructor, it should be changed to assign members directly from the {{Builder}} object.

Apply similar change to some other classes in SCM, too."	HDDS	Resolved	3	7	1699	pull-request-available
13471167	Update Contributing guide	The goal is to fix some outdated items in the contributors' guide, as well as make some minor improvements.	HDDS	Resolved	3	4	1699	pull-request-available
13290498	Unit check fails to execute insight and mini-chaos-tests modules	"This was observed in unit check run for 0.5.0 RC.

{code:title=https://github.com/apache/hadoop-ozone/runs/490978126?check_suite_focus=true}
2020-03-06T19:13:08.6122969Z [ERROR] Failed to execute goal on project hadoop-ozone-insight: Could not resolve dependencies for project org.apache.hadoop:hadoop-ozone-insight:jar:0.5.0-beta: Could not find artifact org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
2020-03-06T19:13:08.6180318Z [ERROR] Failed to execute goal on project mini-chaos-tests: Could not resolve dependencies for project org.apache.hadoop:mini-chaos-tests:jar:0.5.0-beta: Failure to find org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in https://repository.apache.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots.https has elapsed or updates are forced -> [Help 1]
{code}

Unit check skips {{integration-test}}, but these 2 modules depend on it."	HDDS	Resolved	3	1	1699	pull-request-available
13538788	Let reconfiguration handler update reconfigurable config objects	HDDS-8668 implements reconfigurable config objects.  HDDS-8702 creates a dedicated reconfiguration handler, which handles individual reconfigurable properties so far.  The goal of this task is to allow registering config objects with the reconfiguration handler.	HDDS	Resolved	3	7	1699	pull-request-available
13250198	StackOverflowError in OzoneClientInvocationHandler	"Happens if log level for {{org.apache.hadoop.ozone.client}} is set to TRACE.

{code}
SLF4J: Failed toString() invocation on an object of type [com.sun.proxy.$Proxy85]
Reported exception:
java.lang.StackOverflowError
...
	at org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)
	at com.sun.proxy.$Proxy85.toString(Unknown Source)
	at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)
	at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)
	at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)
	at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)
	at org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)
	at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)
	at com.sun.proxy.$Proxy85.toString(Unknown Source)
...
{code}"	HDDS	Resolved	5	1	1699	pull-request-available
13563740	Migrate FS contract tests to JUnit5	Hadoop FS contract tests are implemented using JUnit4, but hopefully they can be changed to support JUnit5, too (HADOOP-19028).  With that, we can also upgrade Ozone's contract test implementations to JUnit5.	HDDS	Resolved	2	7	1699	pull-request-available
13275694	Read to ByteBuffer uses wrong offset	"{{OzoneFSInputStream#read(ByteBuffer)}} uses the target buffer's position for offsetting into the temporary array:

{code:title=https://github.com/apache/hadoop-ozone/blob/b834fa48afef4ee4c73577c7af564e1e97cb9d5b/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSInputStream.java#L90-L97}
  public int read(ByteBuffer buf) throws IOException {

    int bufInitPos = buf.position();
    int readLen = Math.min(buf.remaining(), inputStream.available());

    byte[] readData = new byte[readLen];
    int bytesRead = inputStream.read(readData, bufInitPos, readLen);
    buf.put(readData);
{code}

Given a buffer with capacity=10 and position=8, this results in the following:

 * {{readLen}} = 2 => {{readData.length}} = 2
 * {{bufInitPos}} = 8

So {{inputStream}} reads 2 bytes and writes it into {{readData}} starting at offset 8, which results in an {{IndexOutOfBoundsException}}.

offset should always be 0, since the temporary array is sized exactly for the length to read, and it has no extra data at the start."	HDDS	Resolved	3	1	1699	pull-request-available
13577472	Integration check no longer needs Ozone repo	HDDS-9242 changed {{integration}} check to run JUnit tests from all modules.  Thus it no longer needs Ozone to be present in local Maven repo.	HDDS	Resolved	3	7	1699	pull-request-available
13550921	Speed up TestStorageContainerManagerHA	"{code}
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 419.934 s - in org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA
{code}"	HDDS	Resolved	3	7	1699	pull-request-available
13288109	Allow forced overwrite of local file	{{ozone sh key get}} refuses to overwrite existing local file.  I would like to add a {{--force}} flag (default: false) to allow overriding this behavior, to make it easier to repeatedly get a key without forcing me to delete it locally first.	HDDS	Resolved	4	4	1699	pull-request-available
13252711	Overlapping chunk region cannot be read concurrently	"Concurrent requests to datanode for the same chunk may result in the following exception in datanode:

{code}
java.nio.channels.OverlappingFileLockException
   at java.base/sun.nio.ch.FileLockTable.checkList(FileLockTable.java:229)
   at java.base/sun.nio.ch.FileLockTable.add(FileLockTable.java:123)
   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.addToFileLockTable(AsynchronousFileChannelImpl.java:178)
   at java.base/sun.nio.ch.SimpleAsynchronousFileChannelImpl.implLock(SimpleAsynchronousFileChannelImpl.java:185)
   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.lock(AsynchronousFileChannelImpl.java:118)
   at org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils.readData(ChunkUtils.java:175)
   at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerImpl.readChunk(ChunkManagerImpl.java:213)
   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleReadChunk(KeyValueHandler.java:574)
   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:195)
   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:271)
   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)
   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)
   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)
{code}

It seems this is covered by retry logic, as key read is eventually successful at client side.

The problem is that:

bq. File locks are held on behalf of the entire Java virtual machine. They are not suitable for controlling access to a file by multiple threads within the same virtual machine. ([source|https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileLock.html])

code ref: [{{ChunkUtils.readData}}|https://github.com/apache/hadoop/blob/c92de8209d1c7da9e7ce607abeecb777c4a52c6a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L175]"	HDDS	Resolved	2	1	1699	pull-request-available
13537881	Expose read-only interface of OzoneAdmins	"{{OzoneAdmins}} write interface ({{setAdminUsernames}}) should be accessible only to the ""owner"" of these objects, e.g. {{OzoneManager}}.  Other objects can be restricted to a read-only interface."	HDDS	Resolved	3	4	1699	pull-request-available
13533432	Disable LegacyReplicationManager by default	" * Change the default value of {{hdds.scm.replication.enable.legacy}} to false.
 * Fix any problems found by CI."	HDDS	Resolved	3	7	1699	pull-request-available
13414370	Intermittent failure due to IllegalStateException: zip file closed in HDDSLayoutVersionManager	"First seen here. The PR's changed are unrelated.

[https://github.com/apache/ozone/runs/4355766647]
{code:java}
[ERROR] testNodeWithOpenPipelineCanBeDecommissionedAndRecommissioned  Time elapsed: 44.016 s  <<< ERROR!
java.lang.IllegalStateException: zip file closed
    at java.util.zip.ZipFile.ensureOpen(ZipFile.java:686)
...
    at org.reflections.Reflections.<init>(Reflections.java:168)
    at org.apache.hadoop.hdds.upgrade.HDDSLayoutVersionManager.registerUpgradeActions(HDDSLayoutVersionManager.java:64)
    at org.apache.hadoop.hdds.upgrade.HDDSLayoutVersionManager.<init>(HDDSLayoutVersionManager.java:51)
    at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.<init>(DatanodeStateMachine.java:137)
    at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:275)
    at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:207)
    at org.apache.hadoop.ozone.MiniOzoneClusterImpl.restartHddsDatanode(MiniOzoneClusterImpl.java:423)
    at org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance.testNodeWithOpenPipelineCanBeDecommissionedAndRecommissioned(TestDecommissionAndMaintenance.java:197)
 {code}"	HDDS	Resolved	3	7	1699	pull-request-available
13270983	Let findbugs.sh skip frontend plugin for Recon	Findbugs/Spotbugs only checks Java code.  We can skip frontend plugin execution for Recon to save ~2 minutes.  Makes a difference mostly when running it locally.	HDDS	Resolved	4	4	1699	pull-request-available
13288114	Save each output of smoketest executed multiple times	Acceptance tests may invoke the same smoketest multiple times to verify behaviour in different states.  Currently output is saved to a file named based on _environment_, _test_ and _container_, so each execution's output overwrites the previous one.  We should check if the file already exists and add a suffix if necessary to avoid overwriting previous logs.	HDDS	Resolved	4	4	1699	pull-request-available
13263399	Fix TestKeyValueContainer#testRocksDBCreateUsesCachedOptions	"TestKeyValueContainer#testRocksDBCreateUsesCachedOptions, introduced in HDDS-2283, is failing:

{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2283-cnrrq/unit/hadoop-hdds/container-service/org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.txt}
testRocksDBCreateUsesCachedOptions(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.135 s  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<11>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testRocksDBCreateUsesCachedOptions(TestKeyValueContainer.java:406)
{noformat}"	HDDS	Resolved	3	1	1699	pull-request-available
13296046	Move Ozone Shell from ozone-manager to tools	"Ozone Shell is currently part of the {{ozone-manager}} module.  I think it would be more at home in the {{tools}} module.

Also rename the package name {{ozShell}} to {{shell}}, as package names should be all lowercase."	HDDS	Resolved	3	4	1699	pull-request-available
13434588	Intermittent failure in TestOzoneManagerHAWithData#testOMRestart	"{code}
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 162.569 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOzoneManagerHAWithData
[ERROR] org.apache.hadoop.ozone.om.TestOzoneManagerHAWithData.testOMRestart  Time elapsed: 75.918 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.apache.hadoop.ozone.om.TestOzoneManagerHAWithData.testOMRestart(TestOzoneManagerHAWithData.java:473)
{code}

{code:title=https://github.com/apache/ozone/blob/74d92c86be579e6d90535a13a276e1970ac644fc/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerHAWithData.java#L473-L474}
    Assert.assertTrue(
        followerOMLastAppliedIndex < leaderOMSnaphsotIndex);
{code}"	HDDS	Resolved	3	1	1699	pull-request-available
13538318	SCM HA transaction buffer may be closed without flush	"{code:title=https://github.com/apache/ozone/actions/runs/5136151298/jobs/9242526664?pr=4794#step:5:3753}
java.lang.AssertionError: Found 2 leaked objects, check logs
	at org.apache.hadoop.hdds.utils.db.CodecBuffer.assertNoLeaks(CodecBuffer.java:70)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:451)
	at org.apache.hadoop.hdds.upgrade.TestScmHAFinalization.shutdown(TestScmHAFinalization.java:117)
{code}

{code:title=org.apache.hadoop.hdds.upgrade.TestScmHAFinalization-output.txt}
2023-05-31 18:35:07,518 [Finalizer] WARN  db.CodecBuffer (CodecBuffer.java:finalize(94)) - LEAK 1: org.apache.hadoop.hdds.utils.db.CodecBuffer@617e237d, refCnt=1, capacity=1
2023-05-31 18:35:07,518 [Finalizer] WARN  db.CodecBuffer (CodecBuffer.java:finalize(94)) - LEAK 2: org.apache.hadoop.hdds.utils.db.CodecBuffer@435b14c4, refCnt=1, capacity=14
{code}"	HDDS	Resolved	3	1	1699	pull-request-available
13291618	Remove unused dependency version strings	"After the repo was split from hadoop, there are a few unused dependencies/version strings left in pom.xml. They can be removed.

Example: 

{code}
    <hbase.one.version>1.2.6</hbase.one.version>
    <hbase.two.version>2.0.0-beta-1</hbase.two.version>
{code}
There may be more."	HDDS	Resolved	4	3	1699	newbie
13591983	Internal error on S3 CompleteMultipartUpload if parts are not specified	"The following error can be received during multipart upload:
{code:java}
An error occurred (500) when calling the CompleteMultipartUpload operation (reached max retries: 4): Internal Server Error {code}
Reproduce steps:
 # Create bucket:  aws s3api --endpoint http://s3g:9878 create-bucket --bucket test
 # Create multipart upload: aws s3api --endpoint http://s3g:9878 create-multipart-upload --bucket test --key data
 # Upload Part 1: aws s3api --endpoint http://s3g:9878 upload-part --bucket test --part-number 1 --upload-id 5eee13a8-e326-4e3f-935b-b948e04b2ef2-108397230370979845 --key data --body /etc/hosts
 # Complete multipart upload without specifying parts: aws s3api --endpoint http://s3g:9878 complete-multipart-upload --bucket test --upload-id e901c545-81b0-4ccc-9d5b-2703243913a6-113126144083165185 --key data

The Amazon S3 has the following output in this case:
{code:java}
An error occured (InvalidRequest) when calling the CompleteMultipartUpload: You must specify at least one part{code}"	HDDS	Resolved	3	1	1699	pull-request-available
13536020	Ratis underreplication due to maintenance is not deprioritised	"According to the following javadoc, both decommission and maintenance replicas should be deprioritised:

{code:title=https://github.com/apache/ozone/blob/6d9002201e58dc995dc133941acaef2af03cb9d2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/replication/ContainerHealthResult.java#L145-L164}
    /**
     * The weightedRedundancy, is the remaining redundancy + the requeue count.
     * When this value is used for ordering in a priority queue it ensures the
     * priority is reduced each time it is requeued, to prevent it from blocking
     * other containers from being processed.
     * Additionally, so that decommission and maintenance replicas are not
     * ordered ahead of under-replicated replicas, a redundancy of
     * DECOMMISSION_REDUNDANCY is used for the decommission redundancy rather
     * than its real redundancy.
     * @return The weightedRedundancy of this result.
     */
    public int getWeightedRedundancy() {
      int result = requeueCount;
      if (dueToDecommission) {
        result += DECOMMISSION_REDUNDANCY;
      } else {
        result += getRemainingRedundancy();
      }
      return result;
    }
{code}

but {{dueToDecommission=true}} is set only based on decommission replicas, ignoring maintenance replicas ({{maintenanceCount}}):

{code:title=https://github.com/apache/ozone/blob/6d9002201e58dc995dc133941acaef2af03cb9d2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/replication/RatisContainerReplicaCount.java#L520-L533}
  /**
   * Checks whether insufficient replication is because of some replicas
   * being on datanodes that were decommissioned.
   * @param includePendingAdd if pending adds should be considered
   * @return true if there is insufficient replication and it's because of
   * decommissioning.
   */
  public boolean inSufficientDueToDecommission(boolean includePendingAdd) {
    if (isSufficientlyReplicated(includePendingAdd)) {
      return false;
    }
    int delta = redundancyDelta(true, includePendingAdd);
    return decommissionCount >= delta;
  }
{code}"	HDDS	Resolved	3	7	1699	pull-request-available
13505605	Eliminate duplicated config in LegacyReplicationManager	{{ReplicationManager}} and {{LegacyReplicationManager}} both define some of the same configuration keys.  In the long run, we are planning to get rid of {{LegacyReplicationManager}} completely.  In the meantime, we can eliminate the duplication by making {{LegacyReplicationManager}} get config values from {{ReplicationManager}}'s config where applicable.	HDDS	Resolved	3	7	1699	pull-request-available
13486570	Bump jackson-databind to 2.13.4.2	Bump {{jackson2-databind}} to 2.13.4.2 due to CVE-2022-42003.	HDDS	Resolved	3	3	1699	pull-request-available
13271512	Conditionally enable profiling at the kernel level	"Extend {{entrypoint.sh}} to set the kernel parameters required for profiling if the {{ASYNC_PROFILER_HOME}} environment variable (used by {{ProfileServlet}}) is set.

Ref:

{code:title=https://cwiki.apache.org/confluence/display/HADOOP/Java+Profiling+of+Ozone}
echo 1 > /proc/sys/kernel/perf_event_paranoid
echo 0 > /proc/sys/kernel/kptr_restrict
{code}"	HDDS	Resolved	4	4	1699	pull-request-available
13264785	Print out the ozone version during the startup instead of hadoop version	"Ozone components printing out the current version during the startup:

 
{code:java}
STARTUP_MSG: Starting StorageContainerManager
STARTUP_MSG:   host = om/10.8.0.145
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.2.0
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-{code}
But as it's visible the build / compiled information is about hadoop not about hadoop-ozone.

(And personally I prefer to use a github compatible url instead of the SVN style -r. Something like:
{code:java}
STARTUP_MSG: build =  https://github.com/apache/hadoop-ozone/commit/8541c5694efebb58f53cf4665d3e4e6e4a12845c ; compiled by '....' on ...{code}
 "	HDDS	Resolved	3	4	1699	Triaged, pull-request-available
13412292	Freon datanode chunk validator does not find pipeline from param	"Freon Datanode Chunk Validator does not find the pipeline provided via command-line option:

{code}
$ ozone freon dcg -n10 -t1 -p dcg1 --pipeline e92fa709-db01-433f-ae8b-8c42d328c819
...
Successful executions: 10

$ ozone freon dcv -n10 -t1 -p dcg1 --pipeline e92fa709-db01-433f-ae8b-8c42d328c819
...
Pipeline ID is defined, but there is no such pipeline: e92fa709-db01-433f-ae8b-8c42d328c819

$ ozone admin pipeline list | grep -c e92fa709-db01-433f-ae8b-8c42d328c819
1
{code}"	HDDS	Resolved	3	1	1699	pull-request-available
13260085	test-single.sh cannot copy results	"Previously {{result}} directory was created by simply {{source}}-ing {{testlib.sh}}, but HDDS-2185 changed it to avoid lost results.  {{test-single.sh}} needs to be adjusted accordingly.

{noformat}
$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone
$ docker-compose up -d --scale datanode=3
$ ../test-single.sh scm basic/basic.robot
...
invalid output path: directory ""hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone/result"" does not exist
{noformat}"	HDDS	Resolved	4	1	1699	pull-request-available
13543974	Create separate acceptance split for cert rotation	{{Test execution of ozonesecure-ha/test-root-ca-rotation.sh}} failed 23 times recently (on {{master}}, not counting PRs).  Also, {{acceptance (HA-secure)}} now takes ~1,5 hours.  Both problems could be less severe if we executed cert. rotation in its own split.	HDDS	Resolved	3	7	1699	pull-request-available
13288858	Add new Freon test for putBlock	The goal of this task is to introduce a new Freon test that issues putBlock commands.	HDDS	Resolved	3	4	1699	pull-request-available
13260747	Container Data Scrubber computes wrong checksum	"Chunk checksum verification fails for (almost) any file.  This is caused by computing checksum for the entire buffer, regardless of the actual size of the chunk.

{code:title=https://github.com/apache/hadoop/blob/55c5436f39120da0d7dabf43d7e5e6404307123b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainerCheck.java#L259-L273}
            byte[] buffer = new byte[cData.getBytesPerChecksum()];
...
                v = fs.read(buffer);
...
                bytesRead += v;
...
                ByteString actual = cal.computeChecksum(buffer)
                    .getChecksums().get(0);
{code}

This results in marking all closed containers as unhealthy."	HDDS	Resolved	2	7	1699	pull-request-available
13310385	Intermittent failure in TestDeleteWithSlowFollower	"TestDeleteWithSlowFollower failed soon after it was re-enabled in HDDS-3330.

{code:title=https://github.com/apache/hadoop-ozone/runs/753363338}
[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 28.647 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower
[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 0.163 s  <<< FAILURE!
java.lang.AssertionError
  ...
  at org.junit.Assert.assertNotNull(Assert.java:631)
  at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)
{code}

CC [~shashikant] [~elek]"	HDDS	Resolved	3	7	1699	pull-request-available
13559437	Avoid recreating typesafe config objects unnecessarily	"{{XceiverServerRatis}} creates {{DatanodeRatisServerConfig}} repeatedly for accessing individual properties.  It should create it only once and reuse.

There may be similar unnecessary creation in other classes, too."	HDDS	Resolved	4	4	1699	pull-request-available
13256838	OM Metric mismatch (MultipartUpload failures)	"{{incNumCommitMultipartUploadPartFails()}} increments {{numInitiateMultipartUploadFails}} instead of the counter for commit failures.

https://github.com/apache/hadoop/blob/85b1c728e4ed22f03db255f5ef34a2a79eb20d52/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java#L310-L312"	HDDS	Resolved	4	1	1699	pull-request-available
13253310	Partially started compose cluster left running	If any container in the sample cluster [fails to start|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L24], all successfully started containers are left running.  This [prevents|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L59] any further acceptance tests from normal completion.  This is only a minor inconvenience, since acceptance test as a whole fails either way.	HDDS	Resolved	4	1	1699	pull-request-available
13554501	Integration check should reuse Ozone jars from build check	Try to reuse Ozone jars created by _build_ check for running integration tests.	HDDS	Resolved	3	7	1699	pull-request-available
13248887	Cannot build hadoop-hdds-config from scratch in IDEA	"Building {{hadoop-hdds-config}} from scratch (eg. right after checkout or after {{mvn clean}}) in IDEA fails with the following error:

{code}
Error:java: Bad service configuration file, or exception thrown while constructing Processor object: javax.annotation.processing.Processor: Provider org.apache.hadoop.hdds.conf.ConfigFileGenerator not found
{code}"	HDDS	Resolved	4	1	1699	pull-request-available
13287717	Include output of timed out test in bundle	"Sometimes a unit/integration test does not complete, nor does it crash.  We should collect the output of such tests in the result bundle for analysis.

Example:

{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/469172863}
2020-02-26T08:15:58.2297584Z [INFO] Running org.apache.hadoop.ozone.freon.TestRandomKeyGenerator
2020-02-26T08:30:59.6189916Z [INFO] Running org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations
...
2020-02-26T08:32:47.6155975Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork
{code}

In this case TestRandomKeyGenerator had this problem.  It might be a bit tricky to find such tests, since these are not explicitly listed at the end, unlike failed or crashed tests."	HDDS	Resolved	3	4	1699	pull-request-available
13535046	Container DB open, but not found in DatanodeStoreCache	"Surefire fork Intermittently timeouts in {{TestDecommissionAndMaintenance}}.

Container DB added to cache:

{code}
2023-05-03 08:18:26,909 [EndpointStateMachine task thread for /0.0.0.0:43723 - 0 ] INFO  utils.DatanodeStoreCache (DatanodeStoreCache.java:addDB(58)) - Added db /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db to cache
{code}

but then not found and tried to open again:

{code}
2023-05-03 08:18:57,086 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1683101936 acquiring thread 139985634854656: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:182)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:212)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.quasiCloseContainer(KeyValueHandler.java:1121)
	at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.quasiCloseContainer(ContainerController.java:142)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.notifyGroupRemove(ContainerStateMachine.java:1052)
	at org.apache.ratis.server.impl.RaftServerImpl.groupRemove(RaftServerImpl.java:423)
	at org.apache.ratis.server.impl.RaftServerProxy.lambda$groupRemoveAsync$12(RaftServerProxy.java:530)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:529)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:479)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:459)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:822)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:644)
	at java.lang.Thread.run(Thread.java:750)
{code}

This continues until the fork is killed:

{code}
2023-05-03 08:33:24,505 [Command processor thread] ERROR utils.DatanodeStoreCache (DatanodeStoreCache.java:getDB(74)) - Failed to get DB store /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db
java.io.IOException: Failed init RocksDB, db path : /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db, exception :org.rocksdb.RocksDBException lock hold by current process, acquire time 1683101936 acquiring thread 139985634854656: /home/runner/work/ozone/ozone/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-ff176d5b-bea5-4cbe-a997-8236a6853a89/datanode-0/data-0/containers/hdds/ff176d5b-bea5-4cbe-a997-8236a6853a89/DS-4328e108-8c1a-4a6f-8bff-6f686dd50b24/container.db/LOCK: No locks available
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:182)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:212)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.start(AbstractDatanodeStore.java:147)
	at org.apache.hadoop.ozone.container.metadata.AbstractDatanodeStore.<init>(AbstractDatanodeStore.java:99)
	at org.apache.hadoop.ozone.container.metadata.DatanodeStoreSchemaThreeImpl.<init>(DatanodeStoreSchemaThreeImpl.java:66)
	at org.apache.hadoop.ozone.container.common.utils.DatanodeStoreCache.getDB(DatanodeStoreCache.java:69)
	at org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils.getDB(BlockUtils.java:132)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.flushAndSyncDB(KeyValueContainer.java:444)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.closeAndFlushIfNeeded(KeyValueContainer.java:385)
	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.quasiClose(KeyValueContainer.java:355)
{code}

* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/21/21757/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt
* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/24/21800/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt
* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/24/21805/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt
* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/27/21885/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt
* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/27/21895/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt
* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/04/28/21927/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt
* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/05/03/21994/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt
* https://github.com/adoroszlai/ozone-build-results/blob/master/2023/05/03/21995/it-flaky/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestDecommissionAndMaintenance-output.txt"	HDDS	Resolved	2	1	1699	pull-request-available
13357398	Skip coverage check for PRs and in forks	"Currently _coverage_ CI check:

# calculates combined test coverage
# uploads it to Sonar only for Apache Ozone repo and only for builds on push/schedule
# stores combined coverage in GitHub Actions artifact

Thus for PR in Apache Ozone and for all builds in forks, it only stores coverage in the artifact.  These expire in 30 days and I don't think anybody really checks them manually.

I propose to completely skip _coverage_ check for PRs and in forks, instead of only skipping upload to Sonar.  This would save ~12 minutes for such builds."	HDDS	Resolved	3	4	1699	pull-request-available
13391839	Ozone version mismatch in Kubernetes test lib	"Kubernetes test library allows running the tests from source dir, but looks for outdated version of Ozone in {{target}}.  This results in invalid path:

{code}
scm-statefulset.yaml
79:        hostPath:
80-          path: 'realpath: ../../../../../target/ozone-0.6.0-SNAPSHOT: No such file
{code}"	HDDS	Resolved	4	1	1699	pull-request-available
13318805	FLAKY-UT: TestWatchForCommit#testWatchForCommitForGroupMismatchException	"[INFO] Running org.apache.hadoop.ozone.client.rpc.TestWatchForCommit
[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 211.911 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestWatchForCommit
[ERROR] testWatchForCommitForGroupMismatchException(org.apache.hadoop.ozone.client.rpc.TestWatchForCommit)  Time elapsed: 38.862 s  <<< ERROR!
java.io.IOException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:740)
	at org.apache.hadoop.ozone.container.TestHelper.waitForPipelineClose(TestHelper.java:220)
	at org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForGroupMismatchException(TestWatchForCommit.java:344)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.ratis.protocol.GroupMismatchException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.
	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:414)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:372)
	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:355)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:738)
	... 29 more"	HDDS	Resolved	3	7	1699	pull-request-available
13579506	Check OM version before making rewrite key request	Integrate version framework to refuse client to request atomic calls if the server does not support it (new client against old server)	HDDS	Resolved	3	7	1699	pull-request-available
13317177	Use Duration for time in RatisClientConfig	Change parameter and return type of time-related config methods in {{RatisClientConfig}} to {{Duration}}.  This results in more readable parameter values and type safety.	HDDS	Resolved	3	4	1699	pull-request-available
13376514	Update commons-io to 2.8.0	"Similar to HADOOP-17683 we should update despite we don't use the vulnerable API.

https://nvd.nist.gov/vuln/detail/CVE-2021-29425

In Apache Commons IO before 2.7, When invoking the method FileNameUtils.normalize with an improper input string, like ""//../foo"", or ""\\..\foo"", the result would be the same value, thus possibly providing access to files in the parent directory, but not further above (thus ""limited"" path traversal), if the calling code would use the result to construct a path value."	HDDS	Resolved	3	3	1699	pull-request-available
13529971	ManagedWriteBatch is not closed properly in SCMHADBTransactionBuffer	"{{SCMHADBTransactionBufferStub}} may fail to close {{currentBatchOperation}} if {{dbStore == null}}.

{{DBTransactionBuffer}} is not closed by {{SCMHAManagerImpl}}."	HDDS	Resolved	3	1	1699	pull-request-available
13571472	Remove unused VolumeInfo#configuredCapacity	{{VolumeInfo#configuredCapacity}} is unused, can be removed.	HDDS	Resolved	5	7	1699	pull-request-available
13402158	Test cluster provider possibly returns null	"{{MiniOzoneClusterProvider}} may timeout (100 seconds) while waiting for a cluster to become available.  In this case it simply returns {{null}} without warning.  This results in NPE when trying to use the cluster in test.

{code}
testAllDataNodeFailuresAfterScmPostFinalizeUpgrade  Time elapsed: 100.076 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.hdds.upgrade.TestHDDSUpgrade.init(TestHDDSUpgrade.java:173)
	at org.apache.hadoop.hdds.upgrade.TestHDDSUpgrade.setUp(TestHDDSUpgrade.java:135)
{code}"	HDDS	Resolved	4	1	1699	pull-request-available
13324290	Update version number in upgrade tests	Ozone 0.6.0 release is renamed to Ozone 1.0.0, but there are a few leftover references to 0.6.0, mostly in {{upgrade}} acceptance test.	HDDS	Resolved	4	3	1699	pull-request-available
13581978	Let zero OzoneQuota use byte as unit	"Just found that {{valueOf(0)}} will use {{EB}}.  (Fortunately, we added {{EB}}; otherwise, it becomes an {{ArrayIndexOutOfBoundsException}}).  Not a big deal, although we probably should fix it.
{code}
  public static void main(String[] args) {
    final RawQuotaInBytes q = RawQuotaInBytes.valueOf(0);
    System.out.println(""q = "" + q);
  }
// q = 0 EB
{code}"	HDDS	Resolved	4	4	1699	pull-request-available
13580456	Remove unused org.glassfish:javax.servlet dependency declaration	Dependency declaration for org.glassfish:javax.servlet is unused, can be removed.	HDDS	Resolved	4	3	1699	pull-request-available
13492099	Bump Spring framework from 5.2.20 to 5.3.21	Bump Spring framework from 5.2.20 to 5.3.21	HDDS	Resolved	3	3	1699	pull-request-available
13274622	Unnecessary calls to isNoneEmpty and isAllEmpty	{{isNoneEmpty}} and {{isAllEmpty}} check variable number of strings.  For single string they can be replaced with {{isNotEmpty}} and {{isEmpty}}.	HDDS	Resolved	5	1	1699	pull-request-available
13315327	Maven warning due to deprecated expression pom.artifactId	"{code:title=mvn clean}
[INFO] Scanning for projects...
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-interface-client:jar:0.6.0-SNAPSHOT
[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-common:jar:0.6.0-SNAPSHOT
[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.
...
{code}

Same warning in {{hadoop-hdds/pom.xml}} was fixed during review of HDDS-3875, but the one in {{hadoop-ozone/pom.xml}} was left."	HDDS	Resolved	5	1	1699	pull-request-available
13282220	Unnecessary log messages in DBStoreBuilder	"DBStoreBuilder logs some table-related at INFO level.  This is fine for DBs that are created once per run, eg. OM or SCM, but Recon builds a new DB for each OM snapshot:

{code}
recon_1     | 2020-01-29 15:20:32,466 [pool-7-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/recon/om.snapshot.db_1580311232241
recon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: userTable
recon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: volumeTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: bucketTable
recon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTable
recon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: keyTable
recon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: deletedTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: openKeyTable
recon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTable
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3Table
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Table
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTable
recon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: dTokenTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTable
recon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTable
recon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: prefixTable
recon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTable
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: default
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:default
recon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default options. DBProfile.DISK
recon_1     | 2020-01-29 15:20:32,514 [pool-7-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB snapshot at /data/metadata/recon/om.snapshot.db_1580311232241.
{code}"	HDDS	Resolved	4	4	1699	pull-request-available
13435616	Spotbugs transitive dependencies may result in NoClassDefFound error	"The following transitive dependency allows using classes from {{commons-lang}} instead of {{commons-lang3}}.  Code compiles fine, but dependency is not available at runtime, resulting in {{NoClassDefFoundError}}.

{code}
\- com.github.spotbugs:spotbugs:jar:3.1.12:provided
   ...
   +- commons-lang:commons-lang:jar:2.6:provided
{code}"	HDDS	Resolved	3	1	1699	pull-request-available
13526744	Handle unchecked exception in KeyValueHandler more gracefully	"{{KeyValueHandler}} in general does not handle unchecked exceptions.  The exception reaches gRPC, which closes the connection abruptly.  As a result, client only gets some generic exception ({{StatusRuntimeException: UNKNOWN}}).

Steps to reproduce: same as HDDS-8019."	HDDS	Resolved	3	4	1699	pull-request-available
13431568	Selective checks: run rat for readme change	_rat_ check may fail if license is missing from non-root README files.  Thus it should be triggered for such changes in PRs.	HDDS	Resolved	3	1	1699	pull-request-available
13584901	Extract keywords for multipart upload tests	"Refactor {{MultipartUpload.robot}} by extracting reusable Robot keywords for test steps.

Also, reduce the number of temp files created for testing."	HDDS	Resolved	3	7	1699	pull-request-available
13270317	Avoid hostname lookup for invalid local IP addresses	"{{OzoneSecurityUtil#getValidInetsForCurrentHost}} performs hostname lookup for all local network interfaces, even for invalid addresses.  This significantly slows down some secure tests ({{TestHddsSecureDatanodeInit}}, {{TestSecureOzoneCluster}}) when run on a machine with special IPv6 network interfaces due to timeout reaching IPv6 DNS servers.

This issue proposes to disable the lookup for invalid addresses."	HDDS	Resolved	4	4	1699	pull-request-available
13298662	Extract test utilities to separate module	TimedOutTestsListener cannot be added globally because it is in hadoop-hdds-common, which is not accessible in hadoop-hdds-config (since the latter is a dependency of the former).  The listener and related classes (GenericTestUtils, etc.) should be extracted into a separate module to be used by all others.	HDDS	Resolved	3	3	1699	pull-request-available
13249930	Unused executor in SimpleContainerDownloader	{{SimpleContainerDownloader}} has an {{executor}} that's created and shut down, but never used.	HDDS	Resolved	4	1	1699	pull-request-available
13268397	Ensure streams are closed	"* ContainerDataYaml: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6IKcVY8lQ4ZsQU&open=AW5md-6IKcVY8lQ4ZsQU
* OmUtils: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-hdKcVY8lQ4Zr76&open=AW5md-hdKcVY8lQ4Zr76"	HDDS	Resolved	3	1	1699	pull-request-available, sonar
13250036	Missing or error-prone test cleanup	Some integration tests do not clean up after themselves.  Some only clean up if the test is successful.	HDDS	Resolved	3	1	1699	pull-request-available
13260334	Container Data Scrubber spams log in empty cluster	"In an empty cluster (without closed containers) logs are filled with messages from completed data scrubber iterations (~3600 per second for me), if Container Scanner is enabled ({{hdds.containerscrub.enabled=true}}), eg.:

{noformat}
datanode_1  | 2019-10-03 15:43:57 INFO  ContainerDataScanner:114 - Completed an iteration of container data scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 6763, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0
{noformat} 

Also CPU usage is quite high.

I think:

# there should be a small sleep between iterations
# it should log only if any containers were scanned"	HDDS	Resolved	3	7	1699	pull-request-available
13551061	Fail checks in Summary step instead of Test	"Ozone's CI check scripts ({{integration.sh}}, etc.) have 2 main conventions:
 * exit with non-zero code if there are failed tests
 * list failures in {{summary.txt}} or similar

Most CI jobs have a ""Summary of failures"" step, which provides a quick overview by showing the contents of {{summary.txt}}.

Github automatically expands the failed step, which according to these conventions is the main ""Execute tests"" step.  The problem is that this can have very long output.

It would be better to exit with failure in the ""summary"" step, thus focusing the developer's attention on the quick overview, keeping the long output collapsed by default."	HDDS	Resolved	4	7	1699	pull-request-available, usability
13376413	Acceptance test may exit with 0 in case of error	"If any acceptance test fails, {{test-all.sh}} (and in turn {{acceptance.sh}}) should exit with error code (1).  But if a successful test is run after a failing one, it will now wrongly exit with success (0).

{code}
$ export OZONE_TEST_SELECTOR='failing1\|ozone-csi'
$ ./hadoop-ozone/dev-support/checks/acceptance.sh
...
$ echo $?
0
{code}"	HDDS	Resolved	3	1	1699	pull-request-available
13562108	TokenRenewer should close OzoneClient after use	"Ozone's {{TokenRenewer}} implementations do not close {{OzoneClient}} after use.

Both O3FS and OFS define their own {{TokenRenewer}} implementation.

Hadoop finds implementations via {{ServiceLoader}}, and uses the first one that handles the kind of token being used:

{code:title=https://github.com/apache/hadoop/blob/7db9895000860605a66dd6403005b0c61a6ed744/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java#L454-L470}
  private static ServiceLoader<TokenRenewer> renewers =
      ServiceLoader.load(TokenRenewer.class);


  private synchronized TokenRenewer getRenewer() throws IOException {
    if (renewer != null) {
      return renewer;
    }
    renewer = TRIVIAL_RENEWER;
    synchronized (renewers) {
      Iterator<TokenRenewer> it = renewers.iterator();
      while (it.hasNext()) {
        try {
          TokenRenewer candidate = it.next();
          if (candidate.handleKind(this.kind)) {
            renewer = candidate;
            return renewer;
          }
{code}

The two implementations are the same (not FileSystem-specific), kind is the same for both, so we can remove the duplicate one."	HDDS	Resolved	3	1	1699	pull-request-available
13442590	Create compat acceptance split	Extract compatibility-related acceptance tests from {{misc}} into a separate suite.	HDDS	Resolved	3	7	1699	pull-request-available
13271103	Handle LeaderNot ready exception in OzoneManager StateMachine and upgrade ratis to latest version.	This Jira is to handle LeaderNotReadyException in OM and also update to latest ratis version.	HDDS	Resolved	3	1	2640	pull-request-available
13306022	Improve error message when GC parameters are not set	"Currently when GC parameters or any -XX are not set, it logs 

""No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS

It would be nice to improve this message with settings that are being set."	HDDS	Resolved	3	1	2640	pull-request-available
13316504	Unable to list intermediate paths on keys created using S3G.	"Keys created via the S3 Gateway currently use the createKey OM API to create the ozone key. Hence, when using a hdfs client to list intermediate directories in the key, OM returns key not found error. This was encountered while using fluentd to write Hive logs to Ozone via the s3 gateway.
cc [~bharat]"	HDDS	Resolved	1	2	2640	pull-request-available
13212335	Handle DeleteContainerCommand in the SCMDatanodeProtocolServer	"Right now, in the SCMDatanodeProtocolServer getCommandResponse() deleteContainerCommand is not handled, so deleteContainerCommand is not sent to Datanode.

 

The deletecontainercommand request is sent for over replicated containers, so this over replication is currently broken because of this.

 

Because of this we see below error:

 
{code:java}
java.lang.IllegalArgumentException: Not implemented
 at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.getCommandResponse(SCMDatanodeProtocolServer.java:345)
 at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.sendHeartbeat(SCMDatanodeProtocolServer.java:272)
 at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolServerSideTranslatorPB.sendHeartbeat(StorageContainerDatanodeProtocolServerSideTranslatorPB.java:88)
 at org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos$StorageContainerDatanodeProtocolService$2.callBlockingMethod(StorageContainerDatanodeProtocolProtos.java:27753)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}
 "	HDDS	Resolved	3	1	2640	SCM
13266942	Exclude webapps from hadoop-ozone-filesystem-lib-current uber jar	"This has caused issue for DN UI loading.

hadoop-ozone-filesystem-lib-current-xx.jar is in the classpath which accidentally loaded Ozone datanode web application instead of Hadoop datanode application. This leads to the reported error. "	HDDS	Resolved	3	1	2640	pull-request-available
13220659	In s3 when bucket already exists, it should just return location 	"In S3 for a create bucket request, when bucket already exists it should just return the location.

This was broken by HDDS-1068."	HDDS	Resolved	3	1	2640	pull-request-available
13221718	Implement actions need to be taken after chill mode exit wait time	"# Destroy and close the pipelines
 # Close all the containers on the pipeline.
 # trigger for pipeline creation"	HDDS	Resolved	3	4	2640	pull-request-available
13317055	Validate KeyNames created in FileSystem requests.	"This jira is to validate KeyNames which are created with OzoneFileSystem.
Similar to how hdfs handles using DFSUtil. isValidName()"	HDDS	Resolved	3	1	2640	pull-request-available
13305772	Fix JVMPause monitor start in OzoneManager	"Fix JVMPause monitor logic, right now it is started only in restart.

This should be started during OM start, and stopped during OM Stop. In restart() we can start this again."	HDDS	Resolved	3	1	2640	pull-request-available
13290187	OM RpcClient fail with java.lang.IllegalArgumentException	"In OM HA cluster, when one of the om service is down, during creation of RpcClient it will fail with below error.

 

 
{code:java}
java.lang.IllegalArgumentException: java.net.UnknownHostException: om1
 at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:447)
 at org.apache.hadoop.ozone.om.ha.OMProxyInfo.<init>(OMProxyInfo.java:40)
 at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:115)
 at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:207)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:198)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:124)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:249)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:274)
 at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:82)
 at picocli.CommandLine.execute(CommandLine.java:1173)
 at picocli.CommandLine.access$800(CommandLine.java:141)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)
 at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)
 at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)
 at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)
 at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)
 at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)
 at org.apache.hadoop.ozone.freon.Freon.execute(Freon.java:72)
 at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)
 at org.apache.hadoop.ozone.freon.Freon.main(Freon.java:98)
Caused by: java.net.UnknownHostException: om1
 ... 22 more
 
{code}
 "	HDDS	Resolved	1	7	2640	OMHATest, pull-request-available
13223462	In DatanodeStateMachine join check for not null	[https://builds.apache.org/job/PreCommit-HDDS-Build/2565/testReport/org.apache.hadoop.ozone.scm.node/TestSCMNodeMetrics/testNodeReportProcessingFailure/]	HDDS	Resolved	3	1	2640	pull-request-available
13417239	Update log4j version to 2.16	https://lists.apache.org/thread/d6v4r6nosxysyq9rvnr779336yf0woz4	HDDS	Resolved	3	1	2640	pull-request-available
13235852	Fix TestReplicationManager and checkstyle issues.	"When working on HDDS-1551, found some test failures which are not related to HDDS-1551.

This is caused by HDDS-700. 

 

This has not caught by Jenkins run because our Jenkins run does not run UT's for all the sub-modules. In this case, it should have run UT's for hadoop-hdds-server-scm, as there are some changes in src/test files in that module, but still, it has not run for it. I think Jenkins run for ozone project is not properly setup.

[https://ci.anzix.net/job/ozone/16895/testReport/]

 "	HDDS	Resolved	3	3	2640	pull-request-available
13392036	Support to upload/read keys from encrypted buckets through S3G	"When KMS is secured using hadoop.kms.authentication.type = KERBEROS. From S3 key put/get fails when decrypting the key due to missing Kerberos Credentials/KMS tokens.

*Proposal to fix this:*
1. Introduce keytab for s3g
2. Make s3g acts as proxy for end users while decrypt kms key during put/get/mpu.

The idea is similar to NFSgateway security model.
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html

"	HDDS	Resolved	3	3	2640	pull-request-available
13268112	Add partName, partNumber for CommitMultipartUpload	"Right now when complete Multipart Upload is not printing partName and  partNumber into the audit log. This will help in analyzing audit logs for MPU.

 

 

2019-11-13 15:14:10,191 | INFO  | OMAudit | user=root | ip=xx.xx.xx.xx | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s325d55ad283aa400af464c76d713c07ad, bucket=ozone-test, key=plc_1570850798896_2991, dataSize=5242880, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {

  containerBlockID

{     containerID: 2     localID: 103129366531867089   }

  blockCommitSequenceId: 4978

}

offset: 0

length: 5242880

createVersion: 0

pipeline {

  leaderID: """"

  members {

    uuid: ""5d03aed5-cfb3-4689-b168-0c9a94316551""

    ipAddress: ""xx.xx.xx.xx""

    hostName: ""xx.xx.xx.xx""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""5d03aed5-cfb3-4689-b168-0c9a94316551""

    networkLocation: ""/default-rack""

  }

  members {

    uuid: ""a71462ae-7865-4ed5-b84e-60616df60a0d""

    ipAddress: ""9.134.51.25""

    hostName: ""9.134.51.25""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""a71462ae-7865-4ed5-b84e-60616df60a0d""

    networkLocation: ""/default-rack""

  }

  members {

    uuid: ""79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03""

    ipAddress: ""9.134.51.215""

    hostName: ""9.134.51.215""

    ports

{       name: ""RATIS""       value: 9858     }

    ports

{       name: ""STANDALONE""       value: 9859     }

    networkName: ""79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03""

    networkLocation: ""/default-rack""

  }

  state: PIPELINE_OPEN

  type: RATIS

  factor: THREE

  id

{     id: ""ec6b06c5-193f-4c30-879b-5a12284dc4f8""   }

}

]} | ret=SUCCESS |"	HDDS	Resolved	3	1	2640	pull-request-available
13260495	Use new ReadWrite lock in OzoneManager	Use new ReadWriteLock added in HDDS-2223.	HDDS	Resolved	3	4	2640	pull-request-available
13279858	Use regex to match with ratis properties when creating ratis client	"This Jira is to use regex which are matching with ratis client and ratis grpc properties and set them when creating ratis client. 

Advantages:
 # We can use ratis properties directly, don't need to create corresponding ozone config.
 # When new properties are added in ratis client, we can set them and use them with out any ozone code changes.

In this Jira not removed the existing properties, if this looks fine, we can remove a clean up Jira to remove ozone config for ratis client or leave as it is for existing ones."	HDDS	Resolved	3	1	2640	pull-request-available, teragentest
13313517	Add resource core-site during loading of ozoneconfiguration	"If users add the properties of ozone to core-site, then during loading of OzoneConfiguration, it addsResource ozone-default.xml. This overrides the properties of ozone which are added to core-site.

To avoid this kind of override issue, we can addResource core-site.xml after ozone-default.xml"	HDDS	Resolved	3	1	2640	pull-request-available
13257236	MR job failing on secure Ozone cluster	"Failing with below error:
Caused by: Client cannot authenticate via:[TOKEN, KERBEROS]
org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)
at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)
at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
at org.apache.hadoop.ipc.Client.call(Client.java:1403)
at org.apache.hadoop.ipc.Client.call(Client.java:1367)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
at com.sun.proxy.$Proxy79.submitRequest(Unknown Source)
at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
at com.sun.proxy.$Proxy79.submitRequest(Unknown Source)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:332)
at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1163)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
at com.sun.proxy.$Proxy80.getServiceList(Unknown Source)
at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:248)
at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:167)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)
at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:161)
at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)
at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:102)
at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:155)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:268)
at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)"	HDDS	Resolved	1	1	2640	pull-request-available
13247180	Freon RandomKeyGenerator even if keySize is set to 0, it returns some random data to key	" 
{code:java}
***************************************************
Status: Success
Git Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf
Number of Volumes created: 1
Number of Buckets created: 1
Number of Keys added: 1
Ratis replication factor: THREE
Ratis replication type: STAND_ALONE
Average Time spent in volume creation: 00:00:00,002
Average Time spent in bucket creation: 00:00:00,000
Average Time spent in key creation: 00:00:00,002
Average Time spent in key write: 00:00:00,101
Total bytes written: 0
Total Execution time: 00:00:05,699
 
{code}
***************************************************

[root@ozoneha-2 ozone-0.5.0-SNAPSHOT]# bin/ozone sh key list /vol-0-28271/bucket-0-95211

[

{   ""version"" : 0,   ""md5hash"" : null,   ""createdOn"" : ""Fri, 26 Jul 2019 01:02:08 GMT"",   ""modifiedOn"" : ""Fri, 26 Jul 2019 01:02:09 GMT"",   ""size"" : 36,   ""keyName"" : ""key-0-98235"",   ""type"" : null }

]

 

This is because of the below code in RandomKeyGenerator:
{code:java}
for (long nrRemaining = keySize - randomValue.length;
 nrRemaining > 0; nrRemaining -= bufferSize) {
 int curSize = (int) Math.min(bufferSize, nrRemaining);
 os.write(keyValueBuffer, 0, curSize);
}
os.write(randomValue);
os.close();{code}
 "	HDDS	Resolved	3	1	2640	pull-request-available
13389978	Avoid refresh pipeline for S3 headObject	"S3 head uses OM API lookup key which refreshes pipeline info by contacting SCM.

For S3 head we donot require any pipeline info, we need very basic details like length, type, etag and last modification time. 

By removing pipeline info which is not required for HEAD object, HEAD API performance can be improved.

This is identified during looking up graphs from [~kerneltime] testing"	HDDS	Resolved	3	4	2640	pull-request-available
13225883	Convert all OM Volume related operations to HA model	"In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like volume not found or some conditions which i have not satisfied like when deleting volume should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also."	HDDS	Resolved	3	7	2640	pull-request-available
13284749	OM HA stability issues	"To conclude a little, _+{color:#ff0000}major issues{color}+_ that I find:
 # When I do a long running s3g writing to cluster with OM HA and I stop the Om leader to force a re-election, the writing will stop and can never recover.

--updates 2020-02-20:

https://issues.apache.org/jira/browse/HDDS-3031 {color:#ff0000}fixes{color} this issue.

 

2. If I force a OM re-election and do a scm restart after that, the cluster cannot see any leader datanode and no datanodes are able to send pipeline reports, which makes the cluster unavailable as well. I consider this a multi-failover case when the leader OM and SCM are on the same node and there is a short outage happen to the node.

 

--updates 2020-02-20:

 When you do a jar swap for a new version of Ozone and enable OM HA while keeping the same ozone-site.xml as last time, if you've written some data into the last Ozone cluster (and therefore there are existing versions and metadata for om and scm), SCM cannot be up after the jar swap.

{color:#ff0000}Error logs{color}: PipelineID=aae4f728-82ef-4bbb-a0a5-7b3f2af030cc not found in scm out logs when scm process cannot be started.

 

--updates 2020-02-24:

After I add some logs to SCM starter:
Assuming SCM is only bounced after the leader OM is stopped
1. If SCM is bounced {color:#de350b}after{color} former leader OM is restarted, meaning all OMs are up, SCM will be bootstrapped correctly but there will be missing pipeline report from the node who doesn't have OM process on it (it's always him tho). This would cause all pipelines stay at ALLOCATED state and cluster will be in safemode. At this point, if I {color:#de350b}restart the blacksheep datanode{color}, it will come back and send the pipeline report to SCM and all pipelines will be at OPEN state.
2. If SCM is bounced {color:#de350b}before{color} the former leader OM is restarted, meaning not all OMs in ratis ring are up, SCM {color:#de350b}cannot{color} be bootstrapped correctly and it shows Pipeline not found.

 

Original posting:

Use S3 gateway to keep writing data into a specific s3 gateway endpoint. After the writer starts to work, I kill the OM process on the OM leader host. After that, the s3 gateway can never allow writing data and keeps reporting InternalError for all new coming keys.

Process Process-488:
{noformat}
 S3UploadFailedError: Failed to upload ./20191204/file1056.dat to ozone-test-reproduce-123/./20191204/file1056.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-489:
 S3UploadFailedError: Failed to upload ./20191204/file9631.dat to ozone-test-reproduce-123/./20191204/file9631.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-490:
 S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-491:
 S3UploadFailedError: Failed to upload ./20191204/file4220.dat to ozone-test-reproduce-123/./20191204/file4220.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-492:
 S3UploadFailedError: Failed to upload ./20191204/file5523.dat to ozone-test-reproduce-123/./20191204/file5523.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
 Process Process-493:
 S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error
{noformat}

That's a partial list and note that all keys are different. I also tried re-enable the OM process on previous leader OM, but it doesn't help since the leader has changed. Also attach partial OM logs:
{noformat}
 2020-02-12 14:57:11,128 [IPC Server handler 72 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 72 on 9862, call Call#4859 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 2020-02-12 14:57:11,918 [IPC Server handler 159 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 159 on 9862, call Call#4864 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
 2020-02-12 14:57:15,395 [IPC Server handler 23 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 23 on 9862, call Call#4869 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561
 org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)
 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)
 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)
 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{noformat}
 

 

Also attach the ozone-site.xml config to enable OM HA:
{noformat}
<property>
 <name>ozone.om.service.ids</name>
 <value>OMHA</value>
 </property>
 <property>
 <name>ozone.om.nodes.OMHA</name>
 <value>om1,om2,om3</value>
 </property>
 <property>
 <name>ozone.om.node.id</name>
 <value>om1</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om1</name>
 <value>9.134.50.210:9862</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om2</name>
 <value>9.134.51.215:9862</value>
 </property>
 <property>
 <name>ozone.om.address.OMHA.om3</name>
 <value>9.134.51.25:9862</value>
 </property>
 <property>
 <name>ozone.om.ratis.enable</name>
 <value>true</value>
 </property>
 <property>
 <name>ozone.enabled</name>
 <value>true</value>
 <tag>OZONE, REQUIRED</tag>
 <description>
 Status of the Ozone Object Storage service is enabled.
 Set to true to enable Ozone.
 Set to false to disable Ozone.
 Unless this value is set to true, Ozone services will not be started in
 the cluster.

Please note: By default ozone is disabled on a hadoop cluster.
 </description>
 </property>
{noformat}"	HDDS	Resolved	1	1	2640	OMHATest
13184761	Datanode loops forever if it cannot create directories	"Datanode starts but runs in a tight loop forever if it cannot create the DataNode ID directory e.g. due to permissions issues. I encountered this by having a typo in my ozone-site.xml for {{ozone.scm.datanode.id}}.

In just a few minutes the DataNode had generated over 20GB of log+out files with the following exception:
{code:java}
2018-09-12 17:28:20,649 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 2
63:
java.io.IOException: Unable to create datanode ID directories.
at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
2018-09-12 17:28:20,648 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Execution exception when running task in Datanode State Mach
ine Thread - 160
2018-09-12 17:28:20,650 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 1
60:
java.io.IOException: Unable to create datanode ID directories.
at org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)
at org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){code}

We should just exit since this is a fatal issue."	HDDS	Resolved	1	1	2640	newbie
13236710	Merge code for HA and Non-HA OM requests for bucket	"In this Jira, we shall use the new code added in HDDS-1551 for Non-HA flow.

 

This Jira modifies the bucket requests only, further requests will be handled in further Jira's."	HDDS	Resolved	3	7	2640	pull-request-available
13318768	Generate encryption info for the bucket outside bucket lock	"This Jira is to generate FileEncryption for a key outside the bucket lock.
As right now, we hold the lock when making a network call to KMS to obtain encryption info."	HDDS	Resolved	3	4	2640	pull-request-available
13319822	Deprecate ozone.s3g.volume.name	"HDDS-3612 introduced bucket links.
After this feature now we don't need this parameter, any volume/bucket can be exposed to S3 via using bucket links.

ozone bucket link srcvol/srcbucket destvol/destbucket

So now to expose any ozone bucket to S3G

For example, the user wants to expose a bucket named bucket1 under volume1 to S3G, they can run below command

{code:java}
ozone bucket link volume1/bucket1 s3v/bucket2
{code}

Now, the user can access all the keys in volume/bucket1 using s3v/bucket2 and also ingest data to the volume/bucket1 using using s3v/bucket2

This Jira is opened to remove the config from ozone-default.xml
And also log a warning message to use bucket links, when it does not have default value s3v.
"	HDDS	Resolved	1	1	2640	pull-request-available
13313784	Fix typo in pom.xml	ratis.thirdpary.version -> ratis.thirdparty.version	HDDS	Resolved	4	1	2640	pull-request-available
13257232	Rename classes under package org.apache.hadoop.utils	"Rename classes under package org.apache.hadoop.utils -> org.apache.hadoop.hdds.utils in hadoop-hdds-common

 

Now, with current way, we might collide with hadoop classes."	HDDS	Resolved	3	1	2640	pull-request-available
13318363	Create volume required for S3G during OM startup	Create volume required for S3G operations during OM startup	HDDS	Resolved	3	1	2640	pull-request-available
13251698	Remove RatisClient in OM HA	In OM, we use ratis server api's to submit request. We can remove the RatisClient code from OM, which is no more used in submitting requests to ratis.	HDDS	Resolved	3	7	2640	pull-request-available
13421184	SCM StateMachine failing to reinitialize doesn't terminate the process	"When SCM state machine fails to reinitialize due to any error, it simply logs an error message saying ""Failed to reinitialize SCMStateMachine."" and continues with starting up other servers. In this case, it should not proceed if the state machine cannot be initialized. Also, the exception should be printed along with the error message to help with debugging the issue."	HDDS	Resolved	3	1	2640	pull-request-available
13377602	Use scm#checkLeader before processing client requests 	"Right now to check leader we use ScmContext#isLeader, this gets updated by notifyLeaderChanged.

But SCM server should start accepting requests when it is leader and isLeaderReady. 

We need isLeaderReady also because Statemachine should apply all the log committed transactions to start accepting requests."	HDDS	Resolved	3	7	2640	pull-request-available
13222489	Test ScmChillMode testChillModeOperations failed	"[https://ci.anzix.net/job/ozone-nightly/35/testReport/junit/org.apache.hadoop.ozone.om/TestScmChillMode/testChillModeOperations/]

 "	HDDS	Resolved	3	1	2640	pull-request-available
13212686	Allow option for force in DeleteContainerCommand	"Right now, we check container state if it is not open, and then we delete container.

We need a way to delete the containers which are open, so adding a force flag will allow deleting a container without any state checks. (This is required for delete replica's when SCM detects over-replicated, and that container to delete can be in open state)"	HDDS	Resolved	3	1	2640	pull-request-available
13322617	 Normalize Keypath for listKeys.	"When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.

When listKeys uses given keyName(not normalized key path) as prefix and Starkey the list-keys will return empty result.

Similar to HDDS-4102, we should normalize startKey and keyPrefix.


"	HDDS	Resolved	3	7	2640	pull-request-available
13297648	S3A failing complete multipart upload with Ozone S3	"
{code:java}
javax.xml.bind.UnmarshalException: unexpected element (uri:"""", local:""CompleteMultipartUpload""). Expected elements are <{http://s3.amazonaws.com/doc/2006-03-01/}CompleteMultipartUpload>,<{http://s3.amazonaws.com/doc/2006-03-01/}Part>
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleEvent(UnmarshallingContext.java:744)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:262)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:257)
        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportUnexpectedChildElement(Loader.java:124)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext$DefaultRootLoader.childElement(UnmarshallingContext.java:1149)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:574)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:556)
        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:168)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:509)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:374)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl$NSContentDriver.scanRootElementHook(XMLNSDocumentScannerImpl.java:613)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3132)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:852)

{code}


It seems http://s3.amazonaws.com/doc/2006-03-01/ is expected in the element.
But in class CompleteMultipartUploadRequest,  namespace http://s3.amazonaws.com/doc/2006-03-01/ is not defined here.

Reported by [~sammichen]





"	HDDS	Resolved	3	1	2640	pull-request-available
13286899	SCM crash during startup does not print any error message to log	"SCM start up failed due to a pipelineNotFoundException, there is no error message logged in to SCM log.

In the log file, we can see just below log message no reason for the crash is logged.

 

 
{code:java}
2020-02-20 15:37:56,079 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down StorageContainerManager at xx.xx.xx/10.65.51.49
{code}
In the out file, we can see below, but not complete exception message.
{code:java}
PipelineID=xxxxx not found{code}
 

The actual reason for failure is not clearly logged if an exception has occurred during SCM startup.

 "	HDDS	Resolved	3	4	2640	OMHATest, pull-request-available
13259144	Adding container related metrics in SCM	"This jira aims to add more container related metrics to SCM.
 Following metrics will be added as part of this jira:
 * Number of successful create container calls
 * Number of failed create container calls
 * Number of successful delete container calls
 * Number of failed delete container calls
 * Number of list container ops."	HDDS	Resolved	3	4	2640	pull-request-available
13246100	Implement S3 Abort MPU request to use Cache and DoubleBuffer	"Implement S3 Abort MPU request to use OM Cache, double buffer.

 

In this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path."	HDDS	Resolved	3	7	2640	pull-request-available
13237383	Implement Key Write Requests to use Cache and DoubleBuffer	"Implement Key write requests to use OM Cache, double buffer. 

In this Jira will add the changes to implement key operations, and HA/Non-HA will have a different code path, but once all requests are implemented will have a single code path."	HDDS	Resolved	3	7	2640	pull-request-available
13251345	Implement default acls for bucket/volume/key for OM HA code	This Jira is to implement default ACLs for Ozone volume/bucket/key.	HDDS	Resolved	3	7	2640	pull-request-available
13261233	Provide config for fair/non-fair for OM RW Lock	"Provide config in OzoneManager Lock for fair/non-fair for OM RW Lock.

Created based on review comments during HDDS-2244."	HDDS	Resolved	3	4	2640	pull-request-available
13246688	Fix TestOzoneManagerHA and TestOzoneManagerSnapShotProvider	"All tests in TestOzoneManagerHA are failing with below exception.

 

Broken by HDDS-1649. Not sure why this test is not running in CI. 

From PR HDDS-1845 run, not seeing this test run. 

[https://ci.anzix.net/job/ozone/17452/testReport/org.apache.hadoop.ozone.om/]

 
{code:java}
java.lang.Exception: test timed out after 300000 milliseconds
at java.lang.Object.wait(Native Method)
 at java.lang.Object.wait(Object.java:502)
 at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)
 at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)
 at org.apache.hadoop.ipc.Client.call(Client.java:1457)
 at org.apache.hadoop.ipc.Client.call(Client.java:1367)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
 at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:326)
 at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1155)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:234)
 at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:156)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)
 at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)
 at org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:126)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
 
{code}
 "	HDDS	Resolved	3	7	2640	pull-request-available
13218198	Pipeline Rule where atleast one datanode is reported in the pipeline	"h2. Pipeline Rule with configurable percentage of pipelines with at least one datanode reported:

In this rule we consider when at least  90% of pipelines have at least one datanode reported. 

 

This rule satisfies, when we exit chill mode, Ozone clients will have at least one open replica for reads to succeed. (We can increase this threshold default from 90%, if we want to see fewer failures during reads after exit chill mode."	HDDS	Resolved	3	4	2640	pull-request-available
13405707	parse and dump SCM ratis segment file to printable text	"With RATIS-755, a log dump utility for ratis logs has been added. however to parse SM data, a toString supplier is needed to dump the log to printable form. This can be in the form of JSON , XML.

This also will be very useful during debug."	HDDS	Resolved	3	2	2640	pull-request-available
13280873	Remove ozone ratis server specific config keys	Once HDDS-2903 went in, now we can use direct ratis server configurations in XceiverClientRatis. This Jira is to clean up the old configuration and add any new required configuration.	HDDS	Resolved	3	1	2640	pull-request-available, teragentest
13186250	Implement CopyObject REST endpoint	"The Copy object is a simple call to Ozone Manager.  This API can only be done after the PUT OBJECT Call.

This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing a GET and then a PUT. Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.

If the Put Object call has this header, then Put Object call will issue a rename. 

Work Items or JIRAs
Detect the presence of the extra header - x-amz-copy-source
Make sure that destination bucket exists.

The AWS reference is here:

https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html

(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody would be interested, I can be more specific, explain what we need or help)."	HDDS	Resolved	3	7	2640	newbie
13248162	Support Bucket ACL operations for OM HA.	-HDDS-15+40+- adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.	HDDS	Resolved	3	7	2640	pull-request-available
13407835	OM Validate S3 Auth for write requests	Based on logic introduced in HDDS-4440 on the write path, if the requests has S3 Auth information then extract and validate. All subsequent identity to be based on S3 Auth and not RPC thread local auth information.	HDDS	Resolved	3	7	2640	pull-request-available
13378748	SCM subsequent init failed when previous scm init failed	"The problem is SCM init because we use a new clusterID when the version writing failed.


{code:java}
Could not initialize SCM version file
java.io.IOException: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING
	at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)
	at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)
	at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:71)
	at org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:354)
	at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:371)
	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.initialize(SCMRatisServerImpl.java:115)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:925)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:173)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at picocli.CommandLine.executeUserObject(CommandLine.java:1952)
	at picocli.CommandLine.access$1100(CommandLine.java:145)
	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2332)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2326)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2291)
	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)
	at picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)
	at picocli.CommandLine.parseWithHandler(CommandLine.java:2465)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)
	at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:57)
Caused by: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING
{code}
"	HDDS	Resolved	2	7	2640	pull-request-available
13239152	Cleanup Volume Request 2 phase old code	This Jira is to clean up the old 2 phase HA code for Volume requests.	HDDS	Resolved	3	7	2640	pull-request-available
13292028	Datanode startup is slow due to iterating container DB 2-3 times	"During Datanode startup, for each container we iterate 2 times entire DB
1. For Setting block length
2. For finding delete Key count.

And for open containers, we do step 1 again.

*Code Snippet:*
*ContainerReader.java:*

*For setting Bytes Used:*
{code:java}
      List<Map.Entry<byte[], byte[]>> liveKeys = metadata.getStore()
          .getRangeKVs(null, Integer.MAX_VALUE,
              MetadataKeyFilters.getNormalKeyFilter());

      bytesUsed = liveKeys.parallelStream().mapToLong(e-> {
        BlockData blockData;
        try {
          blockData = BlockUtils.getBlockData(e.getValue());
          return blockData.getSize();
        } catch (IOException ex) {
          return 0L;
        }
      }).sum();
      kvContainerData.setBytesUsed(bytesUsed);
{code}

*For setting pending deleted Key count*

{code:java}
          MetadataKeyFilters.KeyPrefixFilter filter =
              new MetadataKeyFilters.KeyPrefixFilter()
                  .addFilter(OzoneConsts.DELETING_KEY_PREFIX);
          int numPendingDeletionBlocks =
              containerDB.getStore().getSequentialRangeKVs(null,
                  Integer.MAX_VALUE, filter)
                  .size();
          kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);
{code}

*For open Containers*

{code:java}
          if (kvContainer.getContainerState()
              == ContainerProtos.ContainerDataProto.State.OPEN) {
            // commitSpace for Open Containers relies on usedBytes
            initializeUsedBytes(kvContainer);
          }
{code}


*Jstack of DN during startup*
{code:java}
""Thread-8"" #34 prio=5 os_prio=0 tid=0x00007f5df5070000 nid=0x8ee runnable [0x00007f4d840f3000]
   java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksIterator.next0(Native Method)
        at org.rocksdb.AbstractRocksIterator.next(AbstractRocksIterator.java:70)
        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:195)
        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:155)
        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:158)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyAndFixupContainerData(ContainerReader.java:191)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:168)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:146)
        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:101)
        at java.lang.Thread.run(Thread.java:748)
{code}
"	HDDS	Resolved	1	4	2640	billiontest, pull-request-available
13376486	For AccessControlException do not perform failover	"For AccessControlException donot perform failOver, as there is no real need.
{code:java}
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1454)
        at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.recommissionNodes(SCMClientProtocolServer.java:459)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.recommissionNodes(StorageContainerLocationProtocolServerSideTranslatorPB.java:646)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:317)
        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:155)
        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:46954)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.8:9860 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
{code}
"	HDDS	Resolved	3	7	2640	pull-request-available
13286563	Fix Retry handling in ozone RPC Client	"Right now for all other exceptions other than serviceException we use FailOverOnNetworkException.

This Exception policy is created with 15 max fail overs and 15 retries. 

 
{code:java}
retryPolicyOnNetworkException.shouldRetry(
 exception, retries, failovers, isIdempotentOrAtMostOnce);{code}
*2 issues with this:*
 # When shouldRetry returns action FAILOVER_AND_RETRY, it will stuck with same OM, and does not perform failover to next OM.  As OMFailoverProxyProvider#performFailover() is a dummy call does not perform any failover.
 # When ozone.client.failover.max.attempts is set to 15, now with 2 policies with each set to 15, we will retry 15*2 times in worst scenario. 

 

 "	HDDS	Resolved	3	1	2640	OMHA, OMHATest, pull-request-available
13186256	Implement HeadBucket REST endpoint	"This operation is useful to determine if a bucket exists and you have permission to access it. The operation returns a 200 OK if the bucket exists and you have permission to access it. Otherwise, the operation might return responses such as 404 Not Found and 403 Forbidden.  

See the reference here:
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html"	HDDS	Resolved	3	7	2640	newbie
13327540	Get API not working from S3A filesystem with Ozone S3	"TroubleShooting S3A mentions S3 compatible servers that donot support Etags will see this server

Refer [link|https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html] and look for below section content.
Using a third-party S3 implementation that doesn’t support eTags might result in the following error.

org.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://my-bucket/test/file.txt':
 Change detection policy requires ETag
  at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:153)
  at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:200)
  at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:346)
  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$2(Invoker.java:195)
  at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
  at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:193)
  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:215)
  at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:339)
  at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:372)


{code:java}
org.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processNewRevision(ChangeTracker.java:275)
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processMetadata(ChangeTracker.java:261)
	at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:195)
	at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:208)
	at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:359)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$3(Invoker.java:223)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$5(Invoker.java:347)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:343)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:221)
	at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:265)
	at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:351)
	at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:464)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129)
	at org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:494)
	at org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:416)
	at org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:351)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:286)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:271)
	at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)
	at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)
	at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:266)
	at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)
	at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)
	at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:237)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
get: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag
{code}
"	HDDS	Resolved	3	1	2640	OzoneS3, S3A
13317592	Update proto.lock files	HDDS-3807 and HDDS-3612 introduced new additions to proto files but failed to update proto.lock files.  	HDDS	Resolved	3	3	2640	pull-request-available
13319052	Ozone s3 API return 400 Bad Request for head-bucket for non existing bucket	"Ozone s3 API returns 400 Bad Request for head-bucket for non-existing bucket.

hrt_qa$ aws s3api  --ca-bundle=/usr/local/share/ca-certificates/ca.crt --endpoint https://s3g:9879/  head-bucket --bucket fsdghj

An error occurred (400) when calling the HeadBucket operation: Bad Request

It should return 404 as per AWS documentation:
https://docs.aws.amazon.com/cli/latest/reference/s3api/head-bucket.html

A client error (404) occurred when calling the HeadBucket operation: Not Found "	HDDS	Resolved	1	1	2640	pull-request-available
13318795	Update S3 related documentation	"HDDS-3993 created volume required for S3G during the OM startup.
So, remove the step that s3v volume needs to be created."	HDDS	Resolved	3	1	2640	pull-request-available
13319173	Dir rename failed when sets 'ozone.om.enable.filesystem.paths' to true	"Sets ozone.om.enable.filesystem.paths=true, then starts the Ozone cluster.
{code:java}
[root~]$ ozone fs -mkdir o3fs://bucket2.vol2.ozone1/subdir2
[root~]$ ozone fs -mv o3fs://bucket2.vol2.ozone1/subdir2 o3fs://bucket2.vol2.ozone1/subdir2-renamedmv: Key not found /vol2/bucket2/subdir2
{code}
 "	HDDS	Resolved	1	1	2640	pull-request-available
13318794	S3G startup fails when multiple service ids are configured.	"This Jira is to fix this TODO.

OzoneServiceProvider.java L59:
{code:java}
      // HA cluster.
      //For now if multiple service id's are configured we throw exception.
      // As if multiple service id's are configured, S3Gateway will not be
      // knowing which one to talk to. In future, if OM federation is supported
      // we can resolve this by having another property like
      // ozone.om.internal.service.id.
      // TODO: Revisit this later.
      if (serviceIdList.size() > 1) {
        throw new IllegalArgumentException(""Multiple serviceIds are "" +
            ""configured. "" + Arrays.toString(serviceIdList.toArray()));
{code}

      "	HDDS	Resolved	2	1	2640	pull-request-available
13366252	[SCM HA Security] Make storeValidCertificate method idempotent	This Jira is to make storeValidCertificate idempotent so that during replay it does not cause any issues.	HDDS	Resolved	3	7	2640	pull-request-available
13345620	TableCache Refactor to fix issues in cleanup never policy	"Right now we have 2 clean up policies.
1. Never
2. Manual

Never = Full Table Cache
Manual = Partial Table Cache

In OM, the main purpose of Table cache is for correctness. (Because OM return response after adding to cache, does not wait for double buffer flush to complete)

The current implementation has few problems.
1. Cleanup Policy Never uses ConcurrentSkipListMap, and its computeIfPresent is not atomic, so there can be a race condition between cleanup and requests adding to cache. (This might cause cleaning up entries which are not flushed to DB, and this can cause correctness issue)
2. Cleanup for override entries for full cache, never removes epoch entries.

*Proposal:*
1. Make TableCache based on cache type and have separate implementation for full cache and partial cache.
2. Fix FullCache issue, using the lock.
3. Fix evict cache logic for full cache to cleanup epoch entries for override entries.

"	HDDS	Resolved	3	4	2640	pull-request-available
13386148	Return latest version of key location for client on createKey/createFile	"HDDS-5243 was a patch for omitting unnecessary key locations for clients on reading. But the same warning of large response size observed in our cluster for putting data. The patch can also be ported for putting data, as long as until object versioning is supported.

My hypothesis is: The large message was originally, and possibly maybe due to this warning and sudden connection close from client side on reading large message in Hadoop IPC layer, from Ozone Manager - which causes hopeless 15 retries from RetryInvocationHandler. The retries create another entry in OpenKeyTable but they never moved to KeyTable because the key never gets commited."	HDDS	Resolved	3	4	2640	pull-request-available
13222490	Fix asf license errors	"HDDS-1250 added a few new files. In few of them, it is missing adding asf license header.

[https://github.com/apache/hadoop/pull/591]

 

Yetus has not reported about them. I think Yetus is broken in warning asf license errors."	HDDS	Resolved	3	1	2640	pull-request-available
13245077	Add Eviction policy for table cache	"In this Jira we will add eviction policy for table cache.

In this Jira, we will add 2 eviction policies for the cache.

NEVER, // Cache will not be cleaned up. This mean's the table maintains full cache.
AFTERFLUSH // Cache will be cleaned up, once after flushing to DB.

 "	HDDS	Resolved	3	7	2640	pull-request-available
13217050	After allocating container, we are not adding to container DB.	"If we don't do that, we get an error when handling container report for open containers.

As they don't exist in container DB.

 
{code:java}
scm_1           | at java.lang.Thread.run(Thread.java:748)
scm_1           | 2019-02-21 00:00:32 ERROR ContainerReportHandler:173 - Received container report for an unknown container 1 from datanode e2733c00-162b-4993-a986-f6104f5008d8{ip: 172.18.0.2, host: 4f4e683d86c3} {}
scm_1           | org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #1
scm_1           | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:543)
scm_1           | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.updateContainerReplica(ContainerStateMap.java:230)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerStateManager.updateContainerReplica(ContainerStateManager.java:565)
scm_1           | at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerReplica(SCMContainerManager.java:393)
scm_1           | at org.apache.hadoop.hdds.scm.container.ReportHandlerHelper.processContainerReplica(ReportHandlerHelper.java:74)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:159)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:110)
scm_1           | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:51)
scm_1           | at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm_1           | at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
scm_1           | at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
{code}
 

 "	HDDS	Resolved	3	1	2640	pull-request-available
13231110	Use strongly typed codec implementations for the S3Table	"HDDS-864 added the implementation for Strongly typed codec implementation for the tables of OmMetadataManager.

 

Tables which are added as part of S3 Implementation are not using this. This Jira is address to this."	HDDS	Resolved	3	4	2640	pull-request-available
13265830	Remove usage of LogUtils class from ratis-common	"MiniOzoneChaoasCluster.java for setting log level it uses LogUtils from ratis-common. But this is removed from LogUtils as part of Ratis-508.

We can avoid depending on ratis for this, and use GenericTestUtils from hadoop-common test.

LogUtils.setLogLevel(GrpcClientProtocolClient.LOG, Level.WARN);"	HDDS	Resolved	3	1	2640	pull-request-available
13248984	Support Prefix ACL operations for OM HA.	+-HDDS-1608-+ adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.	HDDS	Resolved	3	7	2640	pull-request-available
13242320	Cleanup 2phase old HA code for Key requests.	"HDDS-1638 brought in HA code for Key operations like allocateBlock,createKey etc., 

Old code changes which are added as part of HDDS-1250 and HDDS-1262 for allocateBlock and openKey."	HDDS	Resolved	3	7	2640	pull-request-available
13361423	[SCM HA Security] Add failover proxy to SCM Security Server Protocol	This Jira is to add support for FailOverProxyProvider for SCMSecurityServer which is used by OM and Datanode. (In further jira's when security work is implemented, this API will be used by SCM also)	HDDS	Resolved	3	7	2640	pull-request-available
13381669	Handle SIGTERM to ensure clean shutdown of SCM	"Handle SIGTERM 15 with shutdown hook to properly/clean shutdown SCM.

In this way in SCM HA, the snapshot will be called and pending transactions will be flushed to DB.

"	HDDS	Resolved	3	4	2640	pull-request-available
13218453	Healthy pipeline Chill Mode rule to consider only pipelines with replication factor three	"Few offline comments from [~nandakumar131]
 # We should not process pipeline report from datanode again during calculations.
 # We should consider only replication factor 3 ratis pipelines."	HDDS	Resolved	3	4	2640	pull-request-available
13226268	Convert all OM Bucket related operations to HA model	"In this jira, we shall convert all OM Bucket related operations to OM HA model, which is a 2 step.
 # StartTransaction, where we validate request and check for any errors and return the response.
 # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.

In this way, all requests which are failed with like bucket not found or some conditions which i have not satisfied like when deleting bucket should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also."	HDDS	Resolved	3	7	2640	pull-request-available
13220954	In OM HA AllocateBlock call where connecting to SCM from OM should not happen on Ratis	"In OM HA, currently when allocateBlock is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB. The problem with this is, every OM allocateBlock and appends new BlockInfo into OMKeyInfom and also this a correctness issue. (As all OM's should have the same block information for a key, even though eventually this might be changed during key commit)

 

The proposed approach is:

1. Calling SCM for allocation of block will happen outside of ratis, and this block information is passed and writing to DB will happen via Ratis."	HDDS	Resolved	3	7	2640	pull-request-available
13287468	UpdateID check should be skipped for non-HA OzoneManager	"Delete key is failing . Here is the stack trace of the failure:

 

 
{noformat}
INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 26 which is not greater than the current value of 433 for OMKeyInfo{volume='vol-test-restartcomponentozonereaddata-1582093704', bucket='buck-test-restartcomponentozonereaddata-1582093704', key='ReadOzoneFile_1582093709', dataSize='10485760', creationTime='1582093712218', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862. Trying to failover immediately.
 
..
..
..
..
 
20/02/19 03:37:17 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 22 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862 after 15 failover attempts. Trying to failover immediately. E 2020-02-19 03:37:17,895 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(287)) - Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E 20/02/19 03:37:17 ERROR ha.OMFailoverProxyProvider: Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E Trying to set updateID to 23 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'}]
{noformat}
 

 "	HDDS	Resolved	1	1	2640	pull-request-available
13242825	Use ExecutorService in OzoneManagerStateMachine	"In the current code in applyTransaction we have 

CompletableFuture<Message> future = CompletableFuture
 .supplyAsync(() -> runCommand(request, trxLogIndex)); We are using ForkJoin#commonPool.

With the current approach we have 2 issues:
 # Thread exhausts when using this common pool.
 # Not a good practice of using common pool. Found some issues in our testing by using similarly in RatisPipelineUtils.
 # OM DB's across replica can be out of sync when the apply transactions are applied in out of order."	HDDS	Resolved	3	7	2640	pull-request-available
13408956	MPU getKey can fail, if completeMPU result is still in cache	"Failure was observed on this CI run: [https://github.com/apache/ozone/runs/4015387580?check_suite_focus=true]

The output bundles expired before I could add them to this Jira, but the failure can be reproduced on master by applying the patch attached to this Jira and running the test. The patch speeds up repeated execution of the test by repeatedly writing keys without having to spin up a new mini ozone cluster in between each write.  It usually takes about 4 minutes and 1200 iterations to reproduce.

The failing assertion is:
{code:java}
OzoneInputStream inputStream = bucket.readKey(keyName);
Assert.assertTrue(inputStream instanceof MultipartCryptoKeyInputStream);{code}
Indicating a plain OzoneInputStream is returned, since the class has no other sub classes.

*I have found the reason for this.*
1. If complete MPU is completed, it adds the entry to keyTable.
2. Now if getKey happens on this, if doublebuffer flush not completed flush and cleaned up cache if entry is still in keyTable, the key info returned as Not Mpu Key, this is due to a bug in OmKeyInfo#copyObject Which is not using isMultipartKey."	HDDS	Resolved	3	1	2640	pull-request-available
13249046	Remove hadoop script from ozone distribution	"/bin/hadoop script is included in the ozone distribution even if we a dedicated /bin/ozone

[~arp] reported that it can be confusing, for example ""hadoop classpath"" returns with a bad classpath (ozone classpath <projectname>) should be used instead.

To avoid such confusions I suggest to remove the hadoop script from distribution as ozone script already provides all the functionalities.

It also helps as to reduce the dependencies between hadoop 3.2-SNAPSHOT and ozone as we use the snapshot hadoop script as of now."	HDDS	Resolved	3	1	4052	pull-request-available
13254999	Failing acceptance test - smoketests.ozonesecure-s3.MultipartUpload	"{{""smoketests.ozonesecure-s3.MultipartUpload.Test Multipart Upload with the simplified aws s3 cp API""}} acceptance test is failing."	HDDS	Resolved	3	1	4052	TriagePending
13249697	Acceptance tests fail if scm webui shows invalid json	"Acceptance test of a nightly build is failed with the following error:

{code}
Creating ozonesecure_datanode_3 ... 
[7A[2K
Creating ozonesecure_kdc_1      ... [32mdone[0m
[7B[6A[2K
Creating ozonesecure_om_1       ... [32mdone[0m
[6B[8A[2K
Creating ozonesecure_scm_1      ... [32mdone[0m
[8B[1A[2K
Creating ozonesecure_datanode_3 ... [32mdone[0m
[1B[5A[2K
Creating ozonesecure_kms_1      ... [32mdone[0m
[5B[4A[2K
Creating ozonesecure_s3g_1      ... [32mdone[0m
[4B[2A[2K
Creating ozonesecure_datanode_2 ... [32mdone[0m
[2B[3A[2K
Creating ozonesecure_datanode_1 ... [32mdone[0m
[3Bparse error: Invalid numeric literal at line 2, column 0
{code}

https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-5b87q/acceptance/output.log

The problem is in the script which checks the number of available datanodes.

If the HTTP endpoint of the SCM is already started BUT not ready yet it may return with a simple HTML error message instead of json. Which can not be parsed by jq:

In testlib.sh:

{code}
  37   │   if [[ ""${SECURITY_ENABLED}"" == 'true' ]]; then
  38   │     docker-compose -f ""${compose_file}"" exec -T scm bash -c ""kinit -k HTTP/scm@EXAMPL
       │ E.COM -t /etc/security/keytabs/HTTP.keytab && curl --negotiate -u : -s '${jmx_url}'""
  39   │   else
  40   │     docker-compose -f ""${compose_file}"" exec -T scm curl -s ""${jmx_url}""
  41   │   fi \
  42   │     | jq -r '.beans[0].NodeCount[] | select(.key==""HEALTHY"") | .value'
{code}

One possible fix is to adjust the error handling (set +x / set -x) per method instead of using a generic set -x at the beginning. It would provide a more predictable behavior. In our case count_datanode should not fail evert (as the caller method: wait_for_datanodes can retry anyway)."	HDDS	Resolved	3	1	4052	pull-request-available
13235840	Remove hdds-server-scm dependency from ozone-common	"I noticed that the hadoop-ozone/common project depends on hadoop-hdds-server-scm project.

The common projects are designed to be a shared artifacts between client and server side. Adding additional dependency to the common pom means that the dependency will be available for all the clients as well.

(See the attached artifact about the current, desired structure).

We definitely don't need scm server dependency on the client side.

The code dependency is just one class (ScmUtils) and the shared code can be easily moved to the common."	HDDS	Resolved	3	1	4052	pull-request-available
13339955	findbugs.sh couldn't be executed after a full build	"./hadoop-ozone/dev-support/checks/findbugs.sh -- which is a short-cut to execute the CI findbugs check locally -- couldn't be executed locally after a full build:

{code}
./hadoop-ozone/dev-support/checks/findbugs.sh
....
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  3.451 s
[INFO] Finished at: 2020-11-11T11:42:40+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs (spotbugs) on project hadoop-hdds: Execution spotbugs of goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs failed: Java returned: 1 -> [Help 1]
[ERROR] 
{code}

The problem:

`target/classes` directory should be either empty/missing or it should contain java classes to make spotbugs work.

On github it works well as an empty checkout is tested. But locally it's possible that a dummy classpath file is created under `hadoop-hdds/target/classes` which breaks spotbug local execution.

The solution is easy: execute the classpath descriptor generation only if `src/main/java` dir exists."	HDDS	Resolved	3	1	4052	pull-request-available
13290503	Create isolated environment for OM to test it without SCM	"OmKeyGenerator class from Freon can generate keys (open key + commit key). But this test tests both OM and SCM performance. It seems to be useful to have a method to test only the OM performance with faking the response from SCM.  

Can be done easily with the same approach what we have in HDDS-3023: A simple utility class can be implemented and with byteman we can replace the client calls with the fake method."	HDDS	Resolved	3	4	4052	pull-request-available
13318220	Test Kubernetes examples with acceptance tests	"hadoop-ozone/dist/src/main/k8s/example directory contains example kubernetes resources to start Ozone in kubernetes environment. To make sure those resources are working and up-to-date I propose to test them during standard build.

K3s project provides a lightweight Kubernetes distribution which can be installed easily in Github Actions environment and kubernetes based clusters can be tested."	HDDS	Resolved	3	4	4052	pull-request-available
13170081	Remove hdfs command line from ozone distribution.	"As the ozone release artifact doesn't contain a stable namenode/datanode code the hdfs command should be removed from the ozone artifact.

ozone-dist-layout-stitching also could be simplified to copy only the required jar files (we don't need to copy the namenode/datanode server side jars, just the common artifacts"	HDDS	Resolved	3	7	4052	newbie
13338373	Update README.md after TLP separation	"README.md can be updated with the new mailing lists and references to ""Hadoop subproject"" can be removed."	HDDS	Resolved	3	4	4052	pull-request-available
13370653	Bump version of common-compress	Please see: https://github.com/apache/ozone/pull/2139	HDDS	Resolved	3	4	4052	pull-request-available
13355166	Adjust classpath of ozone version to include log4j	Please see: https://github.com/apache/ozone/pull/1850	HDDS	Resolved	3	4	4052	pull-request-available
13374638	Create github check to alert when dependency tree is changed	Please see: https://github.com/apache/ozone/pull/2177	HDDS	Resolved	3	4	4052	pull-request-available
13379260	EC: Create ECReplicationConfig on client side based on input string	"HDDS-5073 improves the existing ""ozone sh"" client to support ReplicationConfig. The input string is parsed to ReplicationConfig by the constructors of the ReplicationConfig classes with string parameters.

After merging this improvement to the EC branch we need to implement the same constructor for ECReplicationConfig.

There are multiple options here:

 1. Create an enum with ALL the possible ECReplicationConfig
 2. Use meaningful programmatic validation rules.

During the EC sync we agreed that 2nd option can be more flexible as we may have very huge configuration matrix with all the EC parameters.
 "	HDDS	Resolved	3	7	4052	pull-request-available
13266764	Ozoneperf docker cluster should use privileged containers	"The profiler [servlet|https://github.com/elek/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java] (which helps to run java profiler in the background and publishes the result on the web interface) requires privileged docker containers.

 

This flag is missing from the ozoneperf docker-compose cluster (which is designed to run performance tests).

 

 "	HDDS	Resolved	3	3	4052	pull-request-available
13237212	Introduce a new ozone specific runner image	"Ozone compose files use apache/hadoop-runner to provide a fixed environment to run any Ozone distribution.

 It can be better to use separated hadoop-runner and ozone-runner:

 1. To make it easier to include Ozone specific behaviour (For example goofys install, scm/om initialization)
 2. To make it clean which feature is required by all the subprojects of Hadoop and which one is Ozone specific (base on the comment from [~eyang] in HADOOP-16092)
 3. for hadoop-runner we maintain two tags (jdk11/jdk8/latest). And it seems to be hard to maintain all of them. jdk8 is required only for hadoop and with separating hadoop-runner/ozone-runner we can use only one simple branch for ozone-runner development (and we can create incremental fixed tags very easily)"	HDDS	Resolved	3	4	4052	pull-request-available
13256106	XSS fragments can be injected to the S3g landing page  	"VULNERABILITY DETAILS
There is a way to bypass anti-XSS filter for DOM XSS exploiting a ""window.location.href"".

Considering a typical URL:

scheme://domain:port/path?query_string#fragment_id

Browsers encode correctly both ""path"" and ""query_string"", but not the ""fragment_id"". 

So if used ""fragment_id"" the vector is also not logged on Web Server.

VERSION
Chrome Version: 10.0.648.134 (Official Build 77917) beta

REPRODUCTION CASE
This is an index.html page:


{code:java}
aws s3api --endpoint <script>document.write(window.location.href.replace(""static/"", """"))</script> create-bucket --bucket=wordcount</pre>
{code}


The attack vector is:
index.html?#<script>alert('XSS');</script>

* PoC:
For your convenience, a minimalist PoC is located on:
http://security.onofri.org/xss_location.html?#<script>alert('XSS');</script>

* References
- DOM Based Cross-Site Scripting or XSS of the Third Kind - http://www.webappsec.org/projects/articles/071105.shtml


reference:- 

https://bugs.chromium.org/p/chromium/issues/detail?id=76796"	HDDS	Resolved	3	1	4052	pull-request-available
13258339	Hadoop31-mr acceptance test is failing due to the shading	"From the daily build:

{code}
 	Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/ozone/shaded/org/apache/http/client/utils/URIBuilder
	at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:138)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
	at org.apache.hadoop.fs.shell.CommandWithDestination.getRemoteDestination(CommandWithDestination.java:195)
	at org.apache.hadoop.fs.shell.CopyCommands$Put.processOptions(CopyCommands.java:259)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:175)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.ozone.shaded.org.apache.http.client.utils.URIBuilder
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 15 more
{code}

It can be reproduced locally with executing the tests:

{code}
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31
./test.sh
{code}"	HDDS	Resolved	3	1	4052	pull-request-available
13286749	Use meaningful name for ChunkWriter threads	ChunkWriter threads acreated with a naming schema 'pool-[x]-thread-[y]'. We can use better naming (especially as we have 60 threads...)	HDDS	Resolved	3	4	4052	pull-request-available
13343531	Use fixed thread pool for closed container replication 	"Number of threads for closed container replications can be adjusted by the settings  {{hdds.datanode.replication.streams.limit}}. But this number is ignored today due to the misuse of {{ThreadPoolExecutor}}:

{code}
new ThreadPoolExecutor(
        0, poolSize, 60, TimeUnit.SECONDS,
        new LinkedBlockingQueue<>(),
        new ThreadFactoryBuilder().setDaemon(true)
            .setNameFormat(""ContainerReplicationThread-%d"")
            .build())
{code}

Here the minimal number of threads is 0 and the maximum number of the threads is the configured value.  Threads in the thread pool supposed to be scaled up, but it doesn't.

[From the JDK docs|https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#ThreadPoolExecutor(int,%20int,%20long,%20java.util.concurrent.TimeUnit,%20java.util.concurrent.BlockingQueue)]:

bq. A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize()) according to the bounds set by corePoolSize (see getCorePoolSize()) and maximumPoolSize (see getMaximumPoolSize()). When a new task is submitted in method execute(java.lang.Runnable), [...] [AND]  If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full.

So if queue is not full (and {{LinkedBlockgingQueue}} is unbounded by default) the threads will never be created.

For a quick fix we can switch to use static thread pool instead of dynamic and always keep the required number of threads."	HDDS	Resolved	3	7	4052	pull-request-available
13371977	Adjust download pages to use Apache Ozone (tlp) artifacts	Please see: https://github.com/apache/ozone-site/pull/4	HDDS	Resolved	3	4	4052	pull-request-available
13325776	Create a script to check AWS S3 compatibility	"Ozone S3G implements the REST interface of AWS S3 protocol. Our robot test based scripts check if it's possible to use Ozone S3 with the AWS client tool.

But occasionally we should check if our robot test definitions are valid: robot tests should be executed with using real AWS endpoint and bucket(s) and all the test cases should be passed.

This patch provides a simple shell script to make this cross-check easier.  "	HDDS	Resolved	3	4	4052	pull-request-available
13363276	Enhance SCMServerProtocol with using ReplicationConfig	"In HDDS-4882 a new ReplicationConfig is introduced. This patch shows how can it be used between OM and SCM on the protocol.

This patch is not a full refactor of SCM it focuses on the SCM protocol side only. Pipeline manager can be improved in follow-up patches..."	HDDS	Closed	3	7	4052	pull-request-available
13170075	add existing docker-compose files to the ozone release artifact	"Currently we use docker-compose files to run ozone pseudo cluster locally. After a full build, they can be found under hadoop-dist/target/compose.

As they are very useful, I propose to make them part of the ozone release to make it easier to try out ozone locally. 

I propose to create a new folder (docker/) in the ozone.tar.gz which contains all the docker-compose subdirectories + some basic README how they could be used.

We should explain in the README that the docker-compose files are not for production just for local experiments."	HDDS	Resolved	4	7	4052	newbie
13232697	Use /etc/ozone for configuration inside docker-compose 	"As [~eyang] reported the docker-compose clusters write the config files with uid=1000. In case of the build is created with different user (eg id=401) the hadoop user inside the container (id=100) can't work to the ozone/etc/hadoop directory.

I propose to generate the configuration file to /etc/hadoop (And add that directory to the classpath). In that case the volume mount of the ozone distribution folder can be read only."	HDDS	Resolved	3	7	4052	pull-request-available
13221705	Adjust default values of pipline recovery for more resilient service restart	"As of now we have a following algorithm to handle node failures:

1. In case of a missing node the leader of the pipline or the scm can detected the missing heartbeats.
2. SCM will start to close the pipeline (CLOSING state) and try to close the containers with the remaining nodes in the pipeline
3. After 5 minutes the pipeline will be destroyed (CLOSED) and a new pipeline can be created from the healthy nodes (one node can be part only one pipwline in the same time).

While this algorithm can work well with a big cluster it doesn't provide very good usability on small clusters:

Use case1:

Given 3 nodes, in case of a service restart, if the restart takes more than 90s, the pipline will be moved to the CLOSING state. For the next 5 minutes (ozone.scm.pipeline.destroy.timeout) the container will remain in the CLOSING state. As there are no more nodes and we can't assign the same node to two different pipeline, the cluster will be unavailable for 5 minutes.

Use case2:

Given 90 nodes and 30 pipelines where all the pipelines are spread across 3 racks. Let's stop one rack. As all the pipelines are affected, all the pipelines will be moved to the CLOSING state. We have no free nodes, therefore we need to wait for 5 minutes to write any data to the cluster.

These problems can be solved in multiple ways:

1.) Instead of waiting 5 minutes, destroy the pipeline when all the containers are reported to be closed. (Most of the time it's enough, but some container report can be missing)
2.) Support multi-raft and open a pipeline as soon as we have enough nodes (even if the nodes already have a CLOSING pipelines).

Both the options require more work on the pipeline management side. For 0.4.0 we can adjust the following parameters to get better user experience:

{code}
  <property>
    <name>ozone.scm.pipeline.destroy.timeout</name>
    <value>60s</value>
    <tag>OZONE, SCM, PIPELINE</tag>
    <description>
      Once a pipeline is closed, SCM should wait for the above configured time
      before destroying a pipeline.
    </description>

  <property>
    <name>ozone.scm.stale.node.interval</name>
    <value>90s</value>
    <tag>OZONE, MANAGEMENT</tag>
    <description>
      The interval for stale node flagging. Please
      see ozone.scm.heartbeat.thread.interval before changing this value.
    </description>
  </property>
 {code}

First of all, we can be more optimistic and mark node to stale only after 5 mins instead of 90s. 5 mins should be enough most of the time to recover the nodes.

Second: we can decrease the time of ozone.scm.pipeline.destroy.timeout. Ideally the close command is sent by the scm to the datanode with a HB. Between two HB we have enough time to close all the containers via ratis. With the next HB, datanode can report the successful datanode. (If the containers can be closed the scm can manage the QUASI_CLOSED containers)

We need to wait 29 seconds (worst case) for the next HB, and 29+30 seconds for the confirmation. --> 66 seconds seems to be a safe choice (assuming that 6 seconds is enough to process the report about the successful closing)"	HDDS	Resolved	2	1	4052	pull-request-available
13262135	Publish normalized Ratis metrics via the prometheus endpoint	"Latest Ratis contains very good metrics about the status of the ratis ring.

After RATIS-702 it will be possible to adjust the repoter of the Dropwizard based ratis metrics and export them directly to the /prom http endpoint (used by ozone insight and ratis).

Unfortunately Dropwizard is very simple, there is no tag support. All of the instance specific strings are part of the metric name. For example:
{code:java}
""ratis_grpc.log_appender.72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67@group""
 + ""-72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67""
 + "".grpc_log_appender_follower_75fa730a-59f0-4547""
 + ""-bd68-216162c263eb_latency"", {code}
In this patch I will use a simple method: during the export of the dropwizard metrics based on the well known format of the ratis metrics, they are converted to proper prometheus metrics where the instance information is included as tags:
{code:java}
ratis_grpc.log_appender.grpc_log_appender_follower_latency{instance=""72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67""}
 {code}
With this approach we can:

 1. monitor easily all the Ratis pipelines with one simple query

 2. Use the metrics for ozone insight which will show health state of the Ratis pipeline"	HDDS	Resolved	3	1	4052	pull-request-available
13378799	Add SSL support to the Ozone streaming API	"HDDS-5142 will introduce a new streaming API for closed container replication / snapshot download and other data movement.

For server2server communication we need to support mTLS. We should configure pure mTLS on the netty server "	HDDS	Resolved	3	4	4052	pull-request-available
13279555	Apache NiFi PutFile processor is failing with secure Ozone S3G	" 

(1) Create a simple PutS3Object processor in NiFi

(2) The request from NiFi to S3g will fail with HTTP 500

(3) The exception in the s3g log:

 
{code:java}
 s3g_1       | Caused by: java.io.IOException: Couldn't create RpcClient protocol
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:197)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:173)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:74)
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:114)
s3g_1       | 	at org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)
s3g_1       | 	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)
s3g_1       | 	... 92 more
s3g_1       | Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid S3 identifier:OzoneToken owner=testuser/scm@EXAMPLE.COM, renewer=, realUser=, issueDate=0, maxDate=0, sequenceNumber=0, masterKeyId=0, strToSign=AWS4-HMAC-SHA256
s3g_1       | 20200115T101329Z
s3g_1       | 20200115/us-east-1/s3/aws4_request
s3g_1       | (hash), signature=(sign), awsAccessKeyId=testuser/scm@EXAMPLE.COM{code}
 "	HDDS	Resolved	2	1	4052	pull-request-available
13221693	Fix the dynamic documentation of basic s3 client usage	"S3 gateway has a default web page to display a generic message if you open the endpoint in the browser:

http://localhost:9878/static/

It also contains a simple example to use the endpoint:

{code}
This is an endpoint of Apache Hadoop Ozone S3 gateway. Use it with any AWS S3 compatible tool with setting this url as an endpoint

For example with aws-cli:

aws s3api --endpoint http://localhost:9878/static/ create-bucket --bucket=wordcount

For more information, please check the documentation. 
{code}

Unfortunately the endpoint is wrong here, the static should be removed from the url.

The trivial fix is to move the ) in the js code>  

"	HDDS	Resolved	3	1	4052	pull-request-available
13223732	OzoneFileSystem can't work with spark/hadoop2.7 because incompatible security classes	"The current ozonefs compatibility layer is broken by: HDDS-1299.

The spark jobs (including hadoop 2.7) can't be executed any more:

{code}
2019-03-25 09:50:08 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProviderTokenIssuer
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)
        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:45)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
        at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:715)
        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:757)
        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:724)
        at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:45)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 43 more
{code}"	HDDS	Resolved	3	1	4052	pull-request-available
13237491	Csi server fails because transitive Netty dependencies	"CSI server can't be started because an ClassNotFound exception.

It turned out that with using the new configuration api we got old netty jar files as transitive dependencies. (hdds-configuration depends on hadoop-common, hadoop-commons depends on the word)

We should exclude all the old netty version from the classpath of the CSI server."	HDDS	Resolved	1	1	4052	pull-request-available
13247649	Remove anti-affinity rules from k8s minkube example	"HDDS-1646 introduced real persistence for k8s example deployment files which means that we need anti-affinity scheduling rules: Even if we use statefulset instead of daemonset we would like to start one datanode per real nodes.

With minikube we have only one node therefore the scheduling rule should be removed to enable at least 3 datanodes on the same physical nodes.

How to test:

{code}
 mvn clean install -DskipTests -f pom.ozone.xml
cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/kubernetes/examples/minikube
minikube start
kubectl apply -f .
kc get pod
{code}

You should see 3 datanode instances.
"	HDDS	Resolved	1	1	4052	pull-request-available
13338449	TestContainerMetrics is flaky	"TestContainerMetrics is flaky since HDDS-4359. Failed in following master builds:

{code}
2020/10/26/3569/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/27/3581/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/28/3591/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/29/3619/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/30/3628/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/30/3642/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/31/3650/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
2020/10/31/3654/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml
{code}

Some of the added assertions couldn't be guaranteed all the time:

{code}
      // ReadTime and WriteTime vary from run to run, only checking non-zero
      Assert.assertNotEquals(0L, getLongCounter(""ReadTime"", volumeIOMetrics));
      Assert.assertNotEquals(0L, getLongCounter(""WriteTime"", volumeIOMetrics));
{code}

In very lucky case the read/write time can be zero."	HDDS	Resolved	3	1	4052	pull-request-available
13215917	Remove default dependencies from hadoop-ozone project	"There are two ways to define common dependencies with maven:

  1.) put all the dependencies to the parent project and inherit them
  2.) get all the dependencies via transitive dependencies

TLDR; I would like to switch from 1 to 2 in hadoop-ozone

My main problem with the first approach that all the child project get a lot of dependencies independent if they need them or not. Let's imagine that I would like to create a new project (for example a java csi implementation) It doesn't need ozone-client, ozone-common etc, in fact it conflicts with ozone-client. But these jars are always added as of now.

Using transitive dependencies is more safe: we can add the dependencies where we need them and all of the other dependent projects will use them. "	HDDS	Resolved	3	4	4052	pull-request-available
13322745	Improve performance of the BufferPool management of Ozone client	"Teragen reported to be slow with low number of mappers compared to HDFS.

In my test (one pipeline, 3 yarn nodes) 10 g teragen with HDFS was ~3 mins but with Ozone it was 6 mins. It could be fixed with using more mappers, but when I investigated the execution I found a few problems reagrding to the BufferPool management.

 1. IncrementalChunkBuffer is slow and it might not be required as BufferPool itself is incremental
 2. For each write operation the bufferPool.allocateBufferIfNeeded is called which can be a slow operation (positions should be calculated).
 3. There is no explicit support for write(byte) operations

In the flamegraph it's clearly visible that with low number of mappers the client is busy with buffer operations. After the patch the rpc call and the checksum calculation give the majority of the time. "	HDDS	Resolved	1	4	4052	pull-request-available
13249939	Support copy during S3 multipart upload part creation	"Uploads a part by copying data from an existing object as data source

Documented here:

https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html"	HDDS	Resolved	1	7	4052	pull-request-available
13204046	Create isolated classloder to use ozonefs with any older hadoop versions	"As of now we create a shaded ozonefs artifact which includes all the required class files to use ozonefs (Hadoop compatible file system for Ozone)

But the shading process of this artifact is very easy, it includes all the class files but no relocation rules (package name renaming) are configured. With this approach ozonefs can be used from the compatible hadoop version (this is hadoop 3.1 only, I guess) but can't be used with any older hadoop version as it requires the newer version of hadoop-common.

I tried to configure a full shading (with relocation) but it's not a simple task. For example a pure (non-relocated) Configuration is required by the ozonefs itself, but an other, newer Configuration class is required by the ozone client code which is a dependency of OzoneFileSystem So we need a relocated and a non-relocated class in the same time.

I tried out a different approach: I moved out all of the ozone specific classes from the OzoneFileSystem to an adapter class (OzoneClientAdapter). In case of an older hadoop version the adapter class itself can be loaded with an isolated classloader. The isolated classloader can load all the required classes from the jar file from a specific path. It doesn't require any specific package relocation as the default class loader doesn't load these classes. 

The OzoneFileSystem (in case of older hadoop version) can load the adapter with the isolated classloader and only a few classes should be shared between the normal and isolated classloader (the interface of the adapter and the types in the method signatures). All of the other ozone classes and the newer hadoop dependencies will be hidden by the isolated classloader.

This patch is more like a proof of concept, I would like to start a discussion about this approach. I successfully used the generated artifact to use ozonefs from spark 2.4 default distribution (which includes hadoop 2.7). 

For a final patch I would add some check to use the ozonefs without any classpath separation by default. (could be configured or chosen by automatically)


For using spark (+ hadoop 2.7 + kubernetes scheduler) together with ozone, you can check this screencast: https://www.youtube.com/watch?v=cpRJcSHIEdM&t=8s
"	HDDS	Resolved	3	4	4052	pull-request-available
13241308	pv-test example to test csi is not working	"[~rmaruthiyodan] reported two problems regarding to the pv-test example in csi examples folder.

pv-test folder contains an example nginx deployment which can use an ozone PVC/PV to publish content of a folder via http.

Two problems are identified:
 * The label based matching filter of service doesn't point to the nginx deployment
 * The configmap mounting is missing from nginx deployment"	HDDS	Resolved	1	1	4052	pull-request-available
13295515	Use EventQueue for delayed/immediate safe mode rule notification	"SCM is built from loosely coupled components which communicate with async event with each other.

Using the same abstraction (EventQueue) has the benefit that we can use the same visibility / testing tools such as the 'ozone insight' definition (which makes visible all the messages) or the test handler (which can wait until all the event queue messages are processed) 

During the review of HDDS-3221 it was suggested (by me) to use the EventQueue instead of the new SafeModeNotification interface. 

There was only one counter argument against it:

bq. I personally find the event queue logic hard to follow due to its async nature (you cannot just follow method calls in the IDE). Its not bad, but more difficult when you don't yet understand it, while registering some instances to be notified is easy to follow in an IDE. This is of course a subjective opinion :)

I respect this opinion, but I think it's better to use one abstraction and a consistent architecture inside one component (together with all the existing limitations). The EventQueue is not the only one possible solution, but an existing one. We can either design and switch to a new one or use the existing one.

In this patch I would like to show how the previous listener interface can be replaced by the EventQueue.

It (hopefully) shows that this is not complex, and in fact can help us to decouple different component from each other    "	HDDS	Resolved	3	4	4052	pull-request-available
13363054	Rename Apache Hadoop Ozone to Apache Ozone in pom and markdown files	Please see: https://github.com/apache/ozone/pull/2005	HDDS	Resolved	3	4	4052	pull-request-available
13343305	Create freon test to measure closed container replication	Create new freon test for container download	HDDS	Resolved	3	7	4052	pull-request-available
13363282	Provide testkrb5 image for faster ozonesecure tests	Please see: https://github.com/apache/ozone-docker-testkrb5/pull/1	HDDS	Resolved	3	4	4052	pull-request-available
13226202	ConcurrentModificationException in TestMiniChaosOzoneCluster	"TestMiniChaosOzoneCluster is failing with the below exception
{noformat}
[ERROR] org.apache.hadoop.ozone.TestMiniChaosOzoneCluster  Time elapsed: 265.679 s  <<< ERROR!
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)
	at java.util.ArrayList$Itr.next(ArrayList.java:859)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.stop(MiniOzoneClusterImpl.java:350)
	at org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:325)
	at org.apache.hadoop.ozone.MiniOzoneChaosCluster.shutdown(MiniOzoneChaosCluster.java:130)
	at org.apache.hadoop.ozone.TestMiniChaosOzoneCluster.shutdown(TestMiniChaosOzoneCluster.java:92)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}"	HDDS	Resolved	3	1	4052	ozone-flaky-test, pull-request-available
13319704	Update documentation for the GA release	"HDDS-3413 is opened to add OM HA related documentation to the Ozone docs but it turned out that it contains additional out-of-date (and missing) information.

This issue is opened to track a big documentation update."	HDDS	Resolved	1	3	4052	pull-request-available
13390689	Multi-raft style placement with permutations for offline data generator	Please see: https://github.com/apache/ozone/pull/2434	HDDS	Resolved	3	4	4052	pull-request-available
13351790	Support scanning content of DN rocksdb instances with current scheme.	Please see: https://github.com/apache/ozone/pull/1786	HDDS	Resolved	3	4	4052	pull-request-available
13368671	Refactor Pipeline to use ReplicationConfig instead of factor/type	"HDDS-5011 introduces Java ReplicationConfig classes which can be used as a replacement of replicationType and replicationFactor.

First task is replacing type/factor with ReplicationConfig in Pipeline and related managers (PipelineManager BackgroundPipelineCreatorV2, PipelineStateManager...)

We can do it on the master without the EC related stuff... (later we will add the small part which is required for EC"	HDDS	Resolved	3	7	4052	pull-request-available
13370344	Bump Guava version	Please see: https://github.com/apache/ozone/pull/2131	HDDS	Resolved	3	4	4052	pull-request-available
13260331	KeyDeletingService throws NPE if it's started too early	"1. OzoneManager starts KeyManager

2. KeyManager starts KeyDeletingService

3. KeyDeletingService uses OzoneManager.isLeader()

4. OzoneManager.isLeader() uses omRatisServer

5. omRatisServer can be null (bumm)

 

Now the initialization order in OzoneManager:

 

new KeymanagerServer() *Includes start()!!!!*

omRatisServer initialization

start() (includes KeyManager.start())

 

The solution seems to be easy: start the key manager only from the OzoneManager.start() and not from the OzoneManager.instantiateServices()"	HDDS	Resolved	3	3	4052	pull-request-available
13280277	Remove default dependencies from hadoop-hdds/pom.xml	"There are two ways to add certain set of dependencies to all the maven projects.
 # You can add it to the parent project which will be inherited to all the children projects
 # You can add it only to the required project and will be used via transitive dependencies

I think the 2nd approach is safest as we might need to create a new child project *without* Hadoop dependencies which is not possible with the 1st approach."	HDDS	Resolved	3	4	4052	pull-request-available
13254192	Make StorageContainerDatanodeProtocolService message based	"We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).

StorageContainerDatanodeProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well."	HDDS	Resolved	3	7	4052	pull-request-available
13216977	Add optional web server to the Ozone freon test tool	"Recently we improved the default HttpServer to support prometheus monitoring and java profiling.

It would be very useful to enable the same options for freon testing:

 1. We need a simple way to profile freon and check the problems

 2. Long running freons should be monitored

We can create a new optional FreonHttpServer which includes all the required servlets by default."	HDDS	Resolved	3	4	4052	pull-request-available
13288225	Depend on lightweight ConfigurationSource interface instead of Hadoop Configuration	"To make it possible to create different client jars compiled with different version of Hadoop we need clear and Hadoop independent hdds-common (and hdds-client) projects.

(For more details about the motivation, check this design doc: https://lists.apache.org/thread.html/rd0ea00f958368e888db1947eb71e514fb977df0b7baaad928ac50e94%40%3Cozone-dev.hadoop.apache.org%3E)

Our current blocker is the usage of `org.apache.hadoop.conf.Configuration`. Configuration class is a heavyweight object from hadoop-common which introduce a lot of unnecessary dependencies. It also violates multiple [OOP principles|https://en.wikipedia.org/wiki/SOLID], for example the *Dependency inversion principle*.

To make our components more independent I propose to depend on a lightweight ConfigurationSource interface which includes all the required getXXX methods. OzoneConfiguration can implement that interface (and with older Hadoop we can create direct adapters).

"	HDDS	Resolved	3	7	4052	pull-request-available
13250037	S3 MPU part-list call fails if there are no parts	"If an S3 multipart upload is created but no part is upload the part list can't be called because it throws HTTP 500:

Create an MPU:

{code}
aws s3api --endpoint http://localhost:9999 create-multipart-upload --bucket=docker --key=testkeu                                         
{
    ""Bucket"": ""docker"",
    ""Key"": ""testkeu"",
    ""UploadId"": ""85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234""
}
{code}

List the parts:

{code}
aws s3api --endpoint http://localhost:9999 list-parts  --bucket=docker --key=testkeu --upload-id=85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234
{code}

It throws an exception on the server side, because in the KeyManagerImpl.listParts the  ReplicationType is retrieved from the first part:

{code}
        HddsProtos.ReplicationType replicationType =
            partKeyInfoMap.firstEntry().getValue().getPartKeyInfo().getType();
{code}

Which is not yet available in this use case."	HDDS	Resolved	3	1	4052	pull-request-available
13237155	Make the version of the used hadoop-runner configurable	"During an offline discussion with [~arp] and [~eyang] we agreed that it could be more safe to fix the tag of the used hadoop-runner images during the releases.

It also requires fix tags from hadoop-runner, but after that it's possible to use the fixed tags.

This patch makes it possible to define the required version/tag in pom.xml

 1. the default hadoop-runner.version is added to all .env files  during the build
 2. If a variable is added to the .env, it can be used from docker-compose files AND can be overridden by environment variables (it makes it possible to define custom version during a local run) "	HDDS	Resolved	3	4	4052	pull-request-available
13271173	TestTableCacheImpl is flaky	"Run(master): [https://github.com/apache/hadoop-ozone/runs/324342299]

 
{code:java}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl
-------------------------------------------------------------------------------
Tests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.955 s <<< FAILURE! - in org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl
testPartialTableCacheWithOverrideAndDelete[0](org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl)  Time elapsed: 0.039 s  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<6>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl.testPartialTableCacheWithOverrideAndDelete(TestTableCacheImpl.java:308)

 {code}
*How to reproduce it locally?*

Replace the last tableCache.evict call of testPartialTableCacheWithOverrideAndDelete to System.out.println(tableCache.size()).

You will see that the cache size is 2 even before the cleanup therefore the next GeneriTestUtils.waitFor is useless (it doesn't guarantee that the cleanup is finished).

*Fix:*

I propose to call the cleanup sync with using the Impl class instead of the interface. It simplifies the test but still validates the behavior."	HDDS	Resolved	3	1	4052	pull-request-available
13230702	Provide k8s resources files for prometheus and performance tests	"Similar to HDDS-1412 we can further improve the available k8s resources with providing example resources to:

1) install prometheus
2) execute freon test and check the results."	HDDS	Resolved	3	7	4052	pull-request-available
13286422	Support running full Ratis pipeline from IDE (IntelliJ) 	"HDDS-1522 introduced a method to run full cluster in IntelliJ. The runner configurations can be copied with a shell script and a basic ozone-site.xml and log configuration to make it easy to run ozone from IDE.

Unfortunately this setup supports only one Datanode and it's harder to debug full Ozone pipeline (3 datanodes) from IDE.

This patch provides 3 different configuration for 3 datanodes with different ports to make it possible to run them on the same host from the IDE."	HDDS	Resolved	3	4	4052	pull-request-available
13240844	Smoketest results are generated with an internal user	"[~eyang] reported the problem in HDDS-1609 that the smoketest results are generated a user (the user inside the docker container) which can be different from the host user.

There is a minimal risk that the test results can be deleted/corrupted by an other users if the current user is different from uid=1000

I opened this issue because [~eyang] said me during an offline discussion that HDDS-1609 is a more complex issue and not only about the ownership of the test results.

I suggest to handle the two problems in different way. With this patch, the permission of the test result files can be fixed easily.

In HDDS-1609 we can discuss about general security problems and try to find generic solution for them.

Steps to reproduce _this_ problem:
 # Use a user which is different from uid=1000
 # Create a new ozone build (mvn clean install -f pom.ozone.xml -DskipTests)
 # Go to a compose directory (cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/)
 # Execute tests (./test.sh)
 # check the ownership of the results (ls -lah ./results)

Current result: the owner of the result files are the user uid=1000

Expected result: the owner of the files should be always the current user (even if the current uid is different)

 "	HDDS	Resolved	4	1	4052	pull-request-available
13215973	Fix findbugs/checkstyle/accepteance errors in Ozone	"Unfortunately as the previous two big commits (error handling HDDS-1068, checkstyle HDDS-1103) are committed in the same time a few new errors are introduced during the rebase.

This patch will fix the remaining 5 issues (+ a type in the acceptance test executor) "	HDDS	Resolved	3	1	4052	pull-request-available
13240605	TestScmSafeNode is flaky	"org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode is failed at last night with the following error:
{code:java}
java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode(TestScmSafeMode.java:285) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}
Locally it can be tested but it's very easy to reproduce by adding an additional sleep DataNodeSafeModeRule:
{code:java}
+++ b/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/DataNodeSafeModeRule.java
@@ -63,7 +63,11 @@ protected boolean validate() {
 
   @Override
   protected void process(NodeRegistrationContainerReport reportsProto) {
-
+    try {
+      Thread.sleep(3000);
+    } catch (InterruptedException e) {
+      e.printStackTrace();
+    }{code}
This is a clear race condition:

DatanodeSafeModeRule and ContainerSafeModeRule are processing the same events but it can be possible (in case of an accidental sleep) that the container safe mode rule is done, but DatanodeSafeModeRule didn't process the new event (yet).

As a result the test execution will continue:
{code:java}
GenericTestUtils
    .waitFor(() -> scm.getCurrentContainerThreshold() == 1.0, 100, 20000);
{code}
(This line is waiting ONLY for the ContainerSafeModeRule).

The fix is easy, let's wait for the processing of all the async events:
{code:java}
EventQueue eventQueue =
    (EventQueue) cluster.getStorageContainerManager().getEventQueue();
eventQueue.processAll(5000L);{code}
As we are sure that the events are already sent to the EventQueue (because we have the previous waitFor), it should be enough."	HDDS	Resolved	2	1	4052	pull-request-available
13269847	Provide command to wait until SCM is out from the safe-mode	"The safe mode can be checked with ""ozone scmcli safemode status"". But for acceptance tests there is no easy way to check if the cluster is ready to execute the tests (See HDDS-2606 for example).

One easy solution is to create a polling version from ""safemode status"".

""safemode wait --timeout ..."" can be blocked until the scm is out from the safe mode.

Wit proper safe mode rules (min datanodes + min pipline numbers) it can help us to check if the acceptance tests are ready to test.

Same command can be used in k8s as well to test if the cluster is ready to start the freon commands..."	HDDS	Resolved	3	4	4052	pull-request-available
13239198	Auditparser robot test shold use a world writable working directory	"When I tried to reproduce a problem which is reported by [~eyang], I found that the auditparser robot test uses the /opt/hadoop directory as a working directory to generate the audit.db export.

/opt/hadoop is may or may not be writable, it's better to use /tmp instead."	HDDS	Resolved	3	1	4052	pull-request-available
13311991	Schedule daily 2 builds from master branch build	"Mukul suggested to schedule cron based build to have more frequent data points to identify flaky tests.

We can start with two additional daily build which can be independent from the commit frequency (today we build master only after the commits)."	HDDS	Resolved	3	4	4052	build
13229902	Fix content and format of Ozone documentation	"During the review of HDDS-1457 I realized that the current documentation contains many outdated information regarding the usage of docker, build commands or s3 usage.

The security information is also rendered in an incorrect way.

The png files for the prometheus page are missing (were included in the patch of HDDS-846 but missing from the commit)."	HDDS	Resolved	1	1	4052	pull-request-available
13375816	Use ReplicationConfig in OmKeyArgs	"During the implementation of HDDS-5145 I realized that OmKeyArgs also uses factor/type, it seems to be easier to convert it to replicationConfig as it's an in-memory class not a protobuf which is required to be persisted.

Having a half-baked patch planning to upload it soon."	HDDS	Resolved	3	7	4052	pull-request-available
13287203	Fix TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel	TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel is disabled due to intermittent issues. It should be fixed / rewritten or deleted.	HDDS	Resolved	3	7	4052	TriagePending
13239295	TestEventWatcher.testMetrics is flaky	"TestEventWatcher is intermittent. (Failed twice out of 44 executions).

Error is:

{code}
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.764 s <<< FAILURE! - in org.apache.hadoop.hdds.server.events.TestEventWatcher
testMetrics(org.apache.hadoop.hdds.server.events.TestEventWatcher)  Time elapsed: 2.384 s  <<< FAILURE!
java.lang.AssertionError: expected:<2> but was:<3>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdds.server.events.TestEventWatcher.testMetrics(TestEventWatcher.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

In the test we do the following:

 1. fire start-event1
 2. fire start-event2
 3. fire start-event3
 4. fire end-event1
 5. wait

Usually the event2 and event3 are timed out and event1 is completed but in case of an accidental time between 3 and 4 (in fact between 1 and 4) the event1 also can be timed out.

I improved the unit test and fixed the metrics calculation (completed message should be incremented only if it's not yet timed out)."	HDDS	Resolved	3	1	4052	pull-request-available
13329886	the icon of hadoop-ozone is bigger than ever	It could be a by-product of the introduction of the issue： https://issues.apache.org/jira/browse/HDDS-4166	HDDS	Resolved	5	1	4052	pull-request-available
13366713	Introduce EC ReplicationConfig and Java based ReplicationConfig implementation	"SCM proto file should be extended to use ECReplicationConfig which can be de-serialized to a specific ReplicationConfiguration.

Note: this is the bare minimum version of HDDS-4882 which doesn't include the rafactor of the existing proto/persistent fields but de-/serialize them to the new java pojos.
 "	HDDS	Resolved	3	7	4052	pull-request-available
13285828	README is missing from the source release tar	When we do a dist build with -Psrc the README.md of the root project is not packaged to the tar file which makes it impossible to do a build from the source package as the README.md is required by the dist script.	HDDS	Resolved	1	1	4052	pull-request-available
13291071	Disable index and filter block cache for RocksDB	"During preformance tests It was noticed that the OM performance is dropped after 10-20 million of keys. (see the screenshot).

By default cache_index_and_filter_blocks is enabled for all of our RocksDB instances (see DBProfile) which is not the best option. (For example see this thread: https://github.com/facebook/rocksdb/issues/3961#)

With turning on this cache the indexes and bloom filters are cached **inside the block cache** which makes slower the cache when we have significant data.

Without turning it on (based on my understanding) all the indexes will remain open without any cache. With our current settings we have only a few number of sst files (even with million of keys) therefore it seems to be safe to turn this option off.

With turning this option of I was able to write >100M keys with high throughput. "	HDDS	Resolved	4	4	4052	pull-request-available
13263050	Provide new Freon test to test Ratis pipeline with pure XceiverClientRatis	"[~xyao] suggested during an offline talk to implement one additional Freon test to test the ratis part only.

It can use XceiverClientManager which creates a pure XceiverClientRatis. The client can be used to generate chunks as the datanode accepts any container id / block id.

With this approach we can stress-test one selected ratis pipeline without having full end2end overhead of the key creation (OM, SCM, etc.)"	HDDS	Resolved	3	2	4052	pull-request-available
13324671	Increase default timeout in kubernetes tests	"Kubernetes tests are timing out sometimes. (eg. here: https://github.com/elek/ozone-build-results/tree/master/2020/08/26/2562/kubernetes)

Based on the log, SCM couldn't move out from safe mode. It's either a real issue or github environment is slow sometimes.

To make it clear what is the problem I propose to increase the default timeout from 90 sec to 300 sec (5 min)."	HDDS	Resolved	3	4	4052	pull-request-available
13219584	Remove TestContainerSQLCli unit test stub	"In HDDS-447 we removed the support the 'ozone noz' cli tool which was a rocksdb/leveldb to sql exporter.

But still we have the unit test for the tool (in fact only the skeleton of the unit test, as the main logic is removed). Even worse this unit test is failing as it calls System.exit:

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-tools: There are test failures.
[ERROR] 
[ERROR] Please refer to /testptch/hadoop/hadoop-ozone/tools/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
{code}

I think this test can be deleted."	HDDS	Resolved	3	7	4052	pull-request-available
13258303	Some RPC metrics are missing from SCM prometheus endpoint	"In Hadoop metrics it's possible to register multiple metrics with the same name but with different tags. For example each RpcServere has an own metrics instance in SCM.

{code}
    ""name"" : ""Hadoop:service=StorageContainerManager,name=RpcActivityForPort9860"",
    ""name"" : ""Hadoop:service=StorageContainerManager,name=RpcActivityForPort9863"",
{code}

They are converted by PrometheusSink to a prometheus metric line with proper name and tags. For example:

{code}
rpc_rpc_queue_time60s_num_ops{port=""9860"",servername=""StorageContainerLocationProtocolService"",context=""rpc"",hostname=""72736061cbc5""} 0
{code}

The PrometheusSink uses a Map to cache all the recent values but unfortunately the key contains only the name (rpc_rpc_queue_time60s_num_ops in our example) but not the tags (port=...)

For this reason if there are multiple metrics with the same name, only the first one will be displayed.

As a result in SCM only the metrics of the first RPC server can be exported to the prometheus endpoint. 
"	HDDS	Resolved	3	1	4052	pull-request-available
13226144	Make the ozonesecure-mr environment definition version independent	"The MapReduce example project on branch ozone-0.4 contains 0.5.0-SNAPSHOT references in the dir:

hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozonesecure-mr

After HDDS-1333 (which introduce filtering) it will be straightforward to always use the current version."	HDDS	Resolved	3	1	4052	pull-request-available
13312158	Hadoop3 artifact should depend on the ozonefs-shaded	"ozonefs-hadoop3 is an all-in-one ozonefs client which can be used as a single jar file.

Unfortunately it uses wrong dependency (ozonefs-common instead of ozonefs-common-shaded) which means that it downloads additional dependencies (netty-all, ...) if it's used from maven."	HDDS	Resolved	1	7	4052	pull-request-available
13181326	Remove dependencies between hdds/ozone and hdfs proto files	"It would be great to make the hdds/ozone proto files independent from hdfs proto files. It would help as to start ozone with multiple version of hadoop version.

Also helps to make artifacts from the hdds protos:  HDDS-220

 Currently we have a few unused ""hdfs.proto"" import in the proto files and we use the StorageTypeProto from hdfs:

{code}
cd hadoop-hdds
grep -r ""hdfs"" --include=""*.proto""
common/src/main/proto/ScmBlockLocationProtocol.proto:import ""hdfs.proto"";
common/src/main/proto/StorageContainerLocationProtocol.proto:import ""hdfs.proto"";

 cd ../hadoop-ozone
grep -r ""hdfs"" --include=""*.proto""
common/src/main/proto/OzoneManagerProtocol.proto:import ""hdfs.proto"";
common/src/main/proto/OzoneManagerProtocol.proto:    required hadoop.hdfs.StorageTypeProto storageType = 5 [default = DISK];
common/src/main/proto/OzoneManagerProtocol.proto:    optional hadoop.hdfs.StorageTypeProto storageType = 6;
{code}

I propose to 

1.) remove the hdfs import statements from the proto files
2.) Copy the StorageTypeProto and create a Hdds version from it (without PROVIDED)"	HDDS	Resolved	3	4	4052	newbie
13194160	Removing REST protocol support from OzoneClient	"Since we have functional {{S3Gateway}} for Ozone which works on REST protocol, having REST protocol support in OzoneClient feels redundant and it will take a lot of effort to maintain it up to date.
As S3Gateway is in a functional state now, I propose to remove REST protocol support from OzoneClient.

Once we remove REST support from OzoneClient, the following will be the interface to access Ozone cluster
 * OzoneClient (RPC Protocol)
 * OzoneFS (RPC Protocol)
 * S3Gateway (REST Protocol)"	HDDS	Resolved	3	4	4052	pull-request-available
13267254	Implement MiniOzoneHAClusterImpl#getOMLeader	Implement MiniOzoneHAClusterImpl#getOMLeader and use it.	HDDS	Resolved	3	4	9462	pull-request-available
13439248	[Multi-Tenant] Clean up unused tenantDefaultPolicyName field in CreateTenantRequest protobuf message	"There is this one place that hasn't been covered in the previous refactoring patch HDDS-6396.

{code:title=Current OmClientProtocol.proto on branh HDDS-4944}
message CreateTenantRequest {
    optional string tenantId = 1;  // Tenant name
    optional string tenantDefaultPolicyName = 2;  // TODO: REMOVE
    optional string volumeName = 3;
}
{code}

This {{tenantDefaultPolicyName}} field is no longer in-use. Remove it."	HDDS	Resolved	3	7	9462	pull-request-available
13424105	[Multi-Tenant] Fix KMS Encryption/Decryption	We need to pass the correct user principal from the client to KMS to get the correct DEK. Currently in multi-tenancy, accessId is passed rather than the actual user principal.	HDDS	Resolved	3	7	9462	pull-request-available
13564239	[hsync] Adopt RATIS-1994 to reduce hsync latency	RATIS-1994 proposes a new Ratis AsyncApi. It could potentially reduce hsync latency.	HDDS	Patch Available	3	7	9462	pull-request-available
13589003	[hsync] Revert config default ozone.fs.hsync.enabled to false	"ozone.fs.hsync.enabled was initially added in HDDS-8302 with default value of false.

But the default was later flipped to true in HDDS-10252. We need to:

1. Switch the default back to false
2. Ensure this doesn't break any existing tests, etc."	HDDS	Resolved	3	7	9462	pull-request-available
13359493	Update NodeStatus OperationalState for Datanodes in Recon	"Possibly due to Recon ignoring {{setNodeOperationalStateCommand}} (HDDS-4766), {{NodeStatus}} isn't being updated for its {{operationalState}} and {{opStateExpiryEpochSeconds}} fields (but {{DatanodeInfo}}'s {{persistedOpState}} and {{persistedOpStateExpiryEpochSec}} are correct).

See the attached screenshot.

Found this during development of HDDS-4832."	HDDS	Resolved	3	7	9462	pull-request-available
13273899	Maven property skipShade should not skip ozonefs compilation	"Currently, if {{-DskipShade}} is specified when running {{mvn}}, it will skip {{ozonefs}} module ({{hadoop-ozone-filesystem}} / Apache Hadoop Ozone FileSystem) compilation:
{code:xml|title=hadoop-ozone/pom.xml}
    <profile>
      <id>build-with-ozonefs</id>
      <activation>
        <property>
          <name>!skipShade</name>
        </property>
      </activation>
      <modules>
        <module>ozonefs</module>
        <module>ozonefs-lib-current</module>
        <module>ozonefs-lib-legacy</module>
      </modules>
    </profile>
{code}

As result of this, when I make code change under {{./hadoop-ozone/ozonefs/}} then run {{mvn clean install -Pdist -DskipTests -e -Dmaven.javadoc.skip=true -DskipShade}}, the change won't be reflected in the dist. Property {{skipShade}} should not be expected to do this.

We should compile {{ozonefs}} regardless of {{-DskipShade}}."	HDDS	Resolved	3	1	9462	pull-request-available
13277761	Implement ofs://: mkdir	A sub-task in HDDS-2665 to lay the foundation and make mkdir work in the new filesystem.	HDDS	Resolved	3	7	9462	pull-request-available
13286813	OzoneFileStatus#getModificationTime should return actual directory modification time when its OmKeyInfo is available	"As of current implementation, [{{getModificationTime()}}|https://github.com/apache/hadoop-ozone/blob/c9f26ccf9f93a052c5c0c042c57b6f87709597ae/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java#L90-L107] always returns ""fake"" modification time (current time) for directory due to the reason that a directory in Ozone might be faked from a file key.

But, there are cases where real directory key exists in OzoneBucket. For example when user calls {{fs.mkdirs(directory)}}. In this case, a reasonable thing to do would be getting the modification time from the OmInfoKey, rather than faking it.

CC [~xyao]


My POC for the fix:
{code:java|title=Diff}
diff --git a/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java b/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java
index 8717946512..708e62d692 100644
--- a/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java
+++ b/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java
@@ -93,7 +93,7 @@ public FileStatus makeQualified(URI defaultUri, Path parent,
    */
   @Override
   public long getModificationTime(){
-    if (isDirectory()) {
+    if (isDirectory() && super.getModificationTime() == 0) {
       return System.currentTimeMillis();
     } else {
       return super.getModificationTime();
diff --git a/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
index 1be5fb3f3c..cb8f647a41 100644
--- a/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
+++ b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java
@@ -2004,8 +2004,14 @@ public OmKeyInfo lookupFile(OmKeyArgs args, String clientAddress)
               } else {
                 // if entry is a directory
                 if (!deletedKeySet.contains(entryInDb)) {
-                  cacheKeyMap.put(entryInDb,
-                      new OzoneFileStatus(immediateChild));
+                  if (!entryKeyName.equals(immediateChild)) {
+                    cacheKeyMap.put(entryInDb,
+                        new OzoneFileStatus(immediateChild));
+                  } else {
+                    // If entryKeyName matches dir name, we have the info
+                    cacheKeyMap.put(entryInDb,
+                        new OzoneFileStatus(value, 0, true));
+                  }
                   countEntries++;
                 }
                 // skip the other descendants of this child directory.
{code}"	HDDS	Resolved	3	4	9462	pull-request-available
13248446	Ozone fs shell command should work with default port when port number is not specified	"{code:bash|title=Without port number -> Error}
$ ozone fs -ls o3fs://bucket.volume.localhost/
-ls: Ozone file system url should be either one of the two forms: o3fs://bucket.volume/key  OR o3fs://bucket.volume.om-host.example.com:5678/key
...
{code}
{code:bash|title=With port number -> Success}
$ ozone fs -ls o3fs://bucket.volume.localhost:9862/
Found 1 items
-rw-rw-rw-   1 hadoop hadoop       1485 2019-08-01 21:14 o3fs://bucket.volume.localhost:9862/README.txt
{code}

We expect the first command to attempt port 9862 by default."	HDDS	Resolved	3	4	9462	pull-request-available
13502748	[Snapshot] Fix SnapshotInfo#dbTxSequenceNumber (de)serialization	It turns out in HDDS-7281 I forgot to add the serialization and deserialization logic for dbTxSequenceNumber in SnapshotInfo. As a result, dbTxSequenceNumber is always zero when deserialized. This is a simple fix to address that issue.	HDDS	Resolved	3	7	9462	pull-request-available
13265038	Speed up TestOzoneManagerHA#testOMRetryProxy and #testTwoOMNodesDown	"Marton's comment:
https://github.com/apache/hadoop-ozone/pull/30#pullrequestreview-302465440

Out of curiosity, I ran entire TestOzoneManagerHA locally. The entire test class finished in 10m 30s. I discovered {{testOMRetryProxy}} and {{testTwoOMNodesDown}} are taking the most time (2m and 2m 30s respectively) to finish. Most time are wasted on retry and wait. We could reasonably reduce the amount of time on the wait.

As I tested, with the patch, {{testOMRetryProxy}} and {{testTwoOMNodesDown}} finish in 20 sec each, saving almost 4 min runtime on those two tests alone. The whole TestOzoneManagerHA test finishes in 5m 44s with the patch."	HDDS	Resolved	3	4	9462	pull-request-available
13401441	[Multi-Tenant] GetS3Secret should retrieve secret from new tables as well	Update existing GetS3Secret to support new multi-tenant tables as secrets generated by AssignUserToTenantRequest are stored in these new tables (i.e. TenantAccessIdTable).	HDDS	Resolved	3	7	9462	pull-request-available
13449127	[Snapshot] Implement Snapshot Delete CLI and API	"Snapshot delete through a Ratis transaction.

The Ratis tx moves the snapshot from ACTIVE state to DELETED.

This does not remove the snapshot's RocksDB checkpoint directory. That would be done by a new background service {{SnapshotDeletingTask}}, which would also move the snapshot to RECLAIMED state and eventually remove the snapshotInfo from the table and remove the snapshot DB checkpoint."	HDDS	Resolved	3	7	9462	pull-request-available
13449111	Bump kotlin-stdlib to 1.6.21 due to CVE-2022-24329	"[CVE-2022-24329|https://nvd.nist.gov/vuln/detail/CVE-2022-24329]

1.6.21 seems to be the latest stable version as of now: https://mvnrepository.com/artifact/org.jetbrains.kotlin/kotlin-stdlib"	HDDS	Resolved	3	3	9462	pull-request-available
13392201	[OFS] URI parser throws URISyntaxException when path contains space	"In docker-compose ozone:

{code:bash}
bash-4.2$ ozone fs -put ""compose/common/grafana/dashboards/Ozone - Object Metrics.json"" ofs://om/vol1/bucket2/dir3/
-put: Fatal internal error
java.lang.RuntimeException: java.net.URISyntaxException: Illegal character in path at index 51: user/hadoop/compose/common/grafana/dashboards/Ozone - Object Metrics.json
	at org.apache.hadoop.ozone.OFSPath.<init>(OFSPath.java:79)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileStatus(BasicRootedOzoneClientAdapterImpl.java:546)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileStatus(BasicRootedOzoneFileSystem.java:785)
	at org.apache.hadoop.fs.shell.PathData.lookupStat(PathData.java:173)
	at org.apache.hadoop.fs.shell.PathData.<init>(PathData.java:105)
	at org.apache.hadoop.fs.shell.PathData.<init>(PathData.java:82)
	at org.apache.hadoop.fs.shell.CopyCommands$Put.expandArgument(CopyCommands.java:287)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:233)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:105)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:327)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.ozone.OzoneFsShell.main(OzoneFsShell.java:81)
Caused by: java.net.URISyntaxException: Illegal character in path at index 51: user/hadoop/compose/common/grafana/dashboards/Ozone - Object Metrics.json
	at java.base/java.net.URI$Parser.fail(URI.java:2913)
	at java.base/java.net.URI$Parser.checkChars(URI.java:3084)
	at java.base/java.net.URI$Parser.parseHierarchical(URI.java:3166)
	at java.base/java.net.URI$Parser.parse(URI.java:3125)
	at java.base/java.net.URI.<init>(URI.java:600)
	at org.apache.hadoop.ozone.OFSPath.<init>(OFSPath.java:77)
	... 13 more
{code}

Looks like a parsing bug in OFSPath.

Easier repro:

{code}
$ ozone fs -ls ""ofs://ozone1/vo1/bucket2/ ""
-ls: Fatal internal error
java.lang.RuntimeException: java.net.URISyntaxException: Illegal character in path at index 12: vo1/bucket2/
	at org.apache.hadoop.ozone.OFSPath.<init>(OFSPath.java:79)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileStatus(BasicRootedOzoneClientAdapterImpl.java:538)
...
{code}"	HDDS	Resolved	3	1	9462	pull-request-available
13563756	Add hsync metadata to hsync'ed keys in OpenKeyTable as well	"Currently, only those keys in KeyTable/FileTable would have metadata {{HSYNC_CLIENT_ID}} when those keys have been hsync'ed (and not closed yet). The problem with this is that it makes {{getExpiredOpenKeys()}} and {{listOpenKeys}} (HDDS-8830) very *inefficient* by forcing them to look up KeyTable/FileTable while they could have just used OpenKeyTable/OpenFileTable solely to determine whether an open key is hsync'ed or not.

Proposal:
1. during an hsync(), persist metadata {{HSYNC_CLIENT_ID}} to {{OmKeyInfo}} in {{OpenKeyTable}} as well (in addition to {{KeyTable}}). Only write when the client ID changes so it doesn't cause write amplifications. Ideally only the first hsync() of a key would cause a write to {{OpenKeyTable}}.
2. during key close/commit, remove {{HSYNC_CLIENT_ID}} from {{OpenKeyTable}} if necessary so that {{HSYNC_CLIENT_ID}} isn't written to the final key."	HDDS	Resolved	3	7	9462	pull-request-available
13527056	Enforce new checkstyle: NewlineAtEndOfFile	"It is brought up by [~hemantkumar.dindi] a while back that we generally want a new line at the end of a file: https://github.com/apache/ozone/pull/4125#discussion_r1087167605

and it also makes sense in terms of diff output, from [doc|https://checkstyle.sourceforge.io/apidocs/com/puppycrawl/tools/checkstyle/checks/NewlineAtEndOfFileCheck.html]:

bq. Rationale: Any source files and text files in general should end with a line separator to let other easily add new content at the end of file and ""diff"" command does not show previous lines as changed.


Back in HDDS-2119, [~nanda] actually [added|https://github.com/apache/hadoop/commit/e78848fc3cb113733ea640f0aa3abbb271b16005#diff-bbd8da0f280e38951da50da904cf93b9915743c40f3424ccc24a9745f4c733c7R60] the NewlineAtEndOfFile check but it is commented out to this day for some reason. Though I didn't find any objections in the [PR|https://github.com/apache/hadoop/pull/1435].

In order to add the check we just need to uncomment that line, and fix whatever existing files that don't have a new line at EOF, similar to what I did in HDDS-6208."	HDDS	Resolved	3	5	9462	pull-request-available
13254015	Add tests for incorrect OM HA config when node ID or RPC address is not configured	"-OM will NPE and crash when `ozone.om.service.ids=id1,id2` is configured but `ozone.om.nodes.id1` doesn't exist; or `ozone.om.address.id1.omX` doesn't exist.-

-Root cause:-
-`OzoneManager#loadOMHAConfigs()` didn't check the case where `found == 0`. This happens when local OM doesn't match any `ozone.om.address.idX.omX` in the config.-

Due to the refactoring done in HDDS-2162. This fix has been included in that commit. I will repurpose the jira to add some tests for the HA config."	HDDS	Resolved	3	7	9462	pull-request-available
13352346	Add permission check in OMDBCheckpointServlet	TBA	HDDS	Resolved	3	1	9462	pull-request-available
13405087	`ozone sh volume/bucket/key list` should print valid JSON array	"Right now the output is just a bunch of separate JSONs (not a valid one as a whole).

This jira simply fixes it by grouping them into one JSON array. Should make it easier to be parsed by {{jq}} and similar utilies.

CC [~erose]"	HDDS	Resolved	3	4	9462	pull-request-available
13431374	[Multi-Tenant] Merge and cleanup tenant group/role/policy tables, refactor protobuf messages and `isTenantAdmin`	Goal: Clean up unnecessary tables before merging the feature branch to master.	HDDS	Resolved	3	7	9462	pull-request-available
13368407	Use getShortUserName in getTrashRoot(s)	"Inspired by HDDS-5019, we should use getShortUserName() instead of getUserName() as Kerberos principal can differ on different nodes while we trust admin will map those Kerberos principal to the same short user names.

https://github.com/apache/ozone/blob/4128813ab495dfd3941c0252e61e41ca7d1cf1ce/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OFSPath.java#L301

https://github.com/apache/ozone/blob/8585fba44a6ffe22fa2c65cc651acec6b6872e5e/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java#L581

cc [~xyao]"	HDDS	Resolved	3	1	9462	pull-request-available
13312024	OzoneManager#listVolumeByUser ignores userName parameter when ACL is enabled	"When {{ozone.acl.enabled}} is set to {{true}}, the [ACL check logic in OzoneManager#listVolumeByUser|https://github.com/apache/hadoop-ozone/blob/aa04ac0a894e15c98b05b1acef110c6e26bb01dc/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java#L1845-L1857] ignored the provided {{userName}}.

This bug is introduced by my commit HDDS-3056, unfortunately.

h3. Impact
e.g. {{userA}} won't be able to use {{ozone sh volume list --user userB}} to list {{userB}}'s volumes when ACL is enabled.

h3. Solution
Use {{userName}} rather than {{ProtobufRpcEngine.Server.getRemoteUser()}} for ACL check."	HDDS	Resolved	2	1	9462	pull-request-available
13359584	Improve Ozone admin shell decommission/recommission/maintenance commands user experience	"1. Currently, entering `ozone admin datanode decommission` command alone doesn't give any feedback.

w/ patch:
{code}
bash-4.2$ ozone admin datanode decommission
Incomplete command
Usage: ozone admin datanode decommission [-hV] [--scm=<scm>] [<hosts>...]
Decommission a datanode
      [<hosts>...]   List of fully qualified host names
  -h, --help         Show this help message and exit.
      --scm=<scm>    The destination scm (host:port)
  -V, --version      Print version information and exit.
{code}

2. When decommission command is executed successfully, it lacks feedback on the client (it does log on the server side though).

w/ patch:
{code}
bash-4.2$ ozone admin datanode decommission 172.18.0.7 172.18.0.2
Started decommissioning datanodes:
172.18.0.7
172.18.0.2
{code}

3. Improve decommission failure message due to host/port resolution.

w/ patch:
{code}
bash-4.2$ ozone admin datanode decommission 172.18.0.71
Host 172.18.0.71 (172.18.0.71) is not running any datanodes registered with SCM. Please check the host name.

bash-4.2$ ozone admin datanode decommission 172.18.0.7:9999
Host 172.18.0.7:9999 is running a datanode registered with SCM, but the port number doesn't match. Please check the port number.
{code}



Same for recommission and maintenance commands."	HDDS	Resolved	3	7	9462	pull-request-available
13540290	Remove redundant checkAcls() when caller is volume owner during key or prefix access	"It is unnecessary to call checkAcls() twice when caller is volume owner in {{OzoneAclUtils#checkAllAcls}}.

Because the reason we had to split that into two calls in HDDS-5903 is because Ranger only has one OWNER tag, and that we want OWNER tag on bucket/key level policies to be filled in with the *bucket* owner during ACL check if the caller is NOT the volume owner.

In the case where the caller is *volume* owner, this hierarchical check is already enforced by the authorizer (OzoneNativeAuthorizer or RangerOzoneAuthorizer) internally. Thus it is unnecessary."	HDDS	Resolved	3	4	9462	pull-request-available
13430080	Relax protolock rule to allow changing field names	"h2. Motivation

The motivation behind this is that currently in the master branch we use {{kerberosID}} as the protobuf message field name while it really means {{accessId}}. For example here: https://github.com/apache/ozone/blob/master/hadoop-ozone/interface-client/src/main/proto/OmClientProtocol.proto#L1323

It just so happens to be the case that before Multi-Tenancy (HDDS-4944) is implemented, kerberosID is equivalent to accessId in the context of Ozone S3 access. But with Multi-Tenancy feature this is no longer the case -- accessId does not equal to kerberosID any more.

And it could be confusing for other developers to work on S3 / Multi-Tenancy related stuff in the future. e.g. those {{getKerberosID}} calls really means {{getAccessId}}, and {{setKerberosID}} really is {{setAccessId}}. Filed HDDS-6339.


h2. Status Quo

Right now {{proto-backwards-compatibility}} uses default [protolock|https://github.com/nilslice/protolock#usage] command line argument, effectively enforcing strict mode for the [rules|https://github.com/nilslice/protolock#rules-enforced], which doesn't allow changing existing protobuf field names:

{code:xml|title=https://github.com/apache/ozone/blob/68c5ac5df4fbc0edd7114394112fa696a3dc9229/pom.xml#L1619-L1633}
          <groupId>com.salesforce.servicelibs</groupId>
          <artifactId>proto-backwards-compatibility</artifactId>
          <version>${proto-backwards-compatibility.version}</version>
          <configuration>
            <protoSourceRoot>${basedir}/target/classes</protoSourceRoot>
          </configuration>
{code}

Changing the field name itself does not break wire compatiblity (field type and field number are still the same), unless that protobuf message is decoded into JSON at some point (which could use the field name as key). Ref: https://stackoverflow.com/a/45431953


h2. Proposal

Add {{<options>--strict false</options>}} in {{pom.xml}} so that strict mode is off, which as a side affect also turns off two other rules currently enforced according to [protolock readme|https://github.com/nilslice/protolock#rules-enforced]. I'm not sure if protolock provides a way for more granular control of which exact rule to turn off.

{code}
No Removing Reserved Fields
Compares the current vs. updated Protolock definitions and will return a list of warnings if any reserved field has been removed.

Note: This rule is not enforced when strict mode is disabled.
{code}
{code}
No Changing Field Names
Compares the current vs. updated Protolock definitions and will return a list of warnings if any message's previous fields have been renamed.

Note: This rule is not enforced when strict mode is disabled.
{code}
{code}
No Removing RPCs
Compares the current vs. updated Protolock definitions and will return a list of warnings if any RPCs provided by a Service have been removed.

Note: This rule is not enforced when strict mode is disabled.
{code}
"	HDDS	Resolved	3	4	9462	pull-request-available
13293855	Rebase OFS branch	"Merge commits on master branch to OFS dev branch: {{git merge master}}

Also need to manually apply a couple of changes in master branch to OFS classes:
- HDDS-3049
- HDDS-2914 (HDDS-2188)
- HDDS-3065"	HDDS	Resolved	3	7	9462	pull-request-available
13561963	Deleted file reappears after HSync	"Scenario:
Open multiple FSO files in Ozone, write data and do Hsync. In middle Remove some file with -skipTrash.

Before deleting File_98.txt and File_99.txt
{code:java}
[root@ccycloud-1 ~]# ozone fs -du -s -h ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/* | grep File_9
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_9.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_90.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_91.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_92.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_93.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_94.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_95.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_96.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_97.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt
[root@ccycloud-1 ~]# {code}
Removing both 98 and 99th file
{code:java}
[root@ccycloud-1 ~]# ozone fs -rm -r -skipTrash ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt
Deleted ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt
[root@ccycloud-1 ~]# ozone fs -rm -r -skipTrash ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt
Deleted ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt
[root@ccycloud-1 ~]# {code}
File gets removed:
{code:java}
[root@ccycloud-1 ~]# ozone fs -du -s -h ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/* | grep File_9
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_9.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_90.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_91.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_92.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_93.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_94.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_95.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_96.txt
766 K  2.2 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_97.txt
[root@ccycloud-1 ~]#
[root@ccycloud-1 ~]# {code}
After the next Hsync call in test, Both file reappears with the exact length as if they were not deleted previously.
{code:java}
[root@ccycloud-1 ~]# ozone fs -du -s -h ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/* | grep File_9
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_9.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_90.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_91.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_92.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_93.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_94.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_95.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_96.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_97.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_98.txt
771 K  2.3 M  ofs://ozone1702444879/testsyncvol1702619029/testsyncbuck1702619029/File_99.txt {code}"	HDDS	Resolved	2	7	9462	pull-request-available
13304455	Implement ofs://: Override getTrashRoot	"[~pifta] found if we delete file with Hadoop shell, namely {{hadoop fs -rm}}, without {{-skipTrash}} option, the operation would fail in OFS due to the client is renaming the file to {{/user/<username>/.Trash/}} because renaming across different buckets is not allowed in Ozone. (Unless the file happens to be under {{/user/<username>/}}, apparently.)

We could override {{getTrashRoot()}} in {{BasicOzoneFileSystem}} to a dir under the same bucket to mitigate the problem. Thanks [~umamaheswararao] for the suggestion.

This raises one more problem though: We need to implement trash clean up on OM. Opened HDDS-3575 for this.

CC [~arp] [~bharat]"	HDDS	Resolved	3	7	9462	Triaged, pull-request-available
13428656	Upgrade acceptance test doesn't collect logs when the test fails	"Upgrade acceptance test is not collecting logs when the test fails:

https://github.com/smengcl/hadoop-ozone/runs/5205586631
https://github.com/smengcl/hadoop-ozone/runs/5207373946

The log bundle is supposed to have these logs regardless of whether the upgrade test suite fails or not:

{code}
docker-1.1.0-downgraded.log
docker-1.1.0.log
docker-1.2.0-finalized.log
docker-1.2.0-pre-finalized.log
{code}"	HDDS	Resolved	3	1	9462	pull-request-available
13517074	[Snapshot] Delete keys in snapshot scope from deleteTable during createSnapshot to accommodate snapshot garbage collection	"During snapshot creation, use deleteRange to remove the keys in the snapshot scope (bucket) from deletedTable so that they aren't pointlessly scanned by KeyDeletingService since optimization around KeyDeletingService scan of the keys ""trapped"" inside snapshots aren't yet in place."	HDDS	Resolved	3	7	9462	pull-request-available
13272307	Implement new Ozone Filesystem scheme ofs://	"Implement a new scheme for Ozone Filesystem where all volumes (and buckets) can be accessed from a single root.

Also known as Rooted Ozone Filesystem."	HDDS	Resolved	1	2	9462	Triaged, pull-request-available
13458622	[Multi-Tenant] Move Ranger plugin version to a separate tag	Goal: Move Ranger plugin version to a separate tag. Before this it was hard-coded.	HDDS	Resolved	3	7	9462	pull-request-available
13322214	Tests in TestOzoneFileSystem should use the existing MiniOzoneCluster	"In HDDS-2833, [~adoroszlai] made a change that greatly reduces the run time of the test suite {{TestOzoneFileSystem}} by sharing one {{MiniOzoneCluster}} among the tests.

But 4 new tests have been added since and are not sharing that {{MiniOzoneCluster}}.

I am able to cut down the run time of {{TestOzoneFileSystem}} from 3m18s to 1m2s on my Mac. It would only save more run time on GitHub Workflow."	HDDS	Resolved	5	4	9462	pull-request-available
13358924	Show Datanode OperationalState (IN_SERVICE/DECOMMISSION/MAINTENANCE) in Recon	"There are 5 NodeOperationalState s defined at the moment:

{code}
  /**
   * Protobuf enum {@code hadoop.hdds.NodeOperationalState}
   */
  public enum NodeOperationalState
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>IN_SERVICE = 1;</code>
     */
    IN_SERVICE(0, 1),
    /**
     * <code>DECOMMISSIONING = 2;</code>
     */
    DECOMMISSIONING(1, 2),
    /**
     * <code>DECOMMISSIONED = 3;</code>
     */
    DECOMMISSIONED(2, 3),
    /**
     * <code>ENTERING_MAINTENANCE = 4;</code>
     */
    ENTERING_MAINTENANCE(3, 4),
    /**
     * <code>IN_MAINTENANCE = 5;</code>
     */
    IN_MAINTENANCE(4, 5),
    ;
{code}

We should show the Datanode OperationalState in Recon Datanode page as well as it provides valuable information. See the attached screenshots for the result."	HDDS	Resolved	3	7	9462	pull-request-available
13450659	[Multi-Tenant] Use RangerClient for Ranger operations	"HDDS-5836 is merged. But we have yet to switch the actual logic to RangerClient.

1. Use {{RangerClientMultiTenantAccessController}} instead of {{RangerRestMultiTenantAccessController}}.
2. Get rid of {{MultiTenantAccessAuthorizer}} and {{MultiTenantAccessAuthorizerRangerPlugin}} -- use {{MultiTenantAccessController}} instead.
3. -work around RangerClient's missing getServiceVersion() API- Use {{rangerClient.getService(serviceName).getPolicyVersion()}} to implement {{RangerClientMultiTenantAccessController#getRangerServiceVersion()}}

{{RangerClient}} allows the use of Kerberos principal and ticket as login credential (preferred than username and password)."	HDDS	Resolved	3	7	9462	pull-request-available
13337718	Datanode can go OOM when a Recon or SCM Server is very slow in processing reports.	"From [~nanda619]'s analysis.

ContainerReportPublisher thread runs periodically (default interval 60s) in Datanode and adds ContainerReport to StateContext (Queue).
Heartbeat thread runs periodically (default interval 30s), picks up the ContainerReport (if any) from StateContext.
For short time, the ContainerReport will be held in Datanode StateContext.
For Recon, a change was made in datanode such that the ContainerReport will be cached in Datanode StateContext separately for each endpoint (i.e. SCM and Recon). As I see, if Recon is configured in the Datanode and all the reports that are to be sent to Recon will be pending in the StateContextQueue (LinkedList)"	HDDS	Resolved	1	3	9462	pull-request-available
13427506	Upgrade acceptance test log flooded with parse error	"These two lines are repeatedly printed during upgrade acceptance test:

{code:title=https://github.com/apache/ozone/runs/5122095480?check_suite_focus=true#step:5:1354}
Python-dotenv could not parse statement starting at line 27
Python-dotenv could not parse statement starting at line 28
{code}"	HDDS	Resolved	4	1	9462	pull-request-available
13487040	Remove Ozone 0.5.0-beta download link from the website	Remove Ozone 0.5.0-beta download link from the site, per discussion in the dev mailing list: https://lists.apache.org/thread/r3cdjk924o62yd9n5781vg7lqdgyd6gr	HDDS	Resolved	3	3	9462	pull-request-available
13589385	[hsync] Add a config as HBase-related features master switch	Simliar to what JVM does with {{-XX:+UnlockExperimentalVMOptions}}, we are adding a config to Ozone (client AND server) to lock hbase-related Ozone features (hsync, incremental chunklist, putBlock piggybacking, lease recovery) and enhancements behind an experimental flag.	HDDS	Resolved	3	7	9462	pull-request-available
13324571	Add OFS to FileSystem META-INF	"So that {{fs.ofs.impl}} won't have to be explicitly set in core-site.xml.

Derived from HDDS-3805."	HDDS	Resolved	3	4	9462	pull-request-available
13359495	Add line break when node has no pipelines for `ozone admin datanode list` command	"The line {{No related pipelines or the node is not in Healthy state.}} should have a new line after it.

Just a trivial formatting issue.

Before:
{code}
bash-4.2$ ozone admin datanode list
Datanode: e56040c8-5cfa-4558-966b-0851ccf5c6c5 (/default-rack/172.22.0.2/ozone_datanode_1.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
ece97f05-fbf4-49db-9959-58c49c479f9b/ONE/RATIS/OPEN/Leader
62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Follower

Datanode: f702a7cc-d887-4095-8f1a-7826669a5ddd (/default-rack/172.22.0.7/ozone_datanode_4.ozone_default/0 pipelines)
Operational State: DECOMMISSIONED
Related pipelines:
No related pipelines or the node is not in Healthy state.
Datanode: 1baca70e-69f5-48e0-ae31-7f6d8ed2fafc (/default-rack/172.22.0.3/ozone_datanode_2.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Follower
8a387971-315d-4fd2-b2a3-c993cd36e8bb/ONE/RATIS/OPEN/Leader

Datanode: 30815665-dcda-40e4-bce6-000a46ab6d3d (/default-rack/172.22.0.6/ozone_datanode_3.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
62973ede-afff-4edb-83e0-086ef84f31c8/THREE/RATIS/OPEN/Leader
a7ff21f5-6d91-4452-9532-0c8789b1d435/ONE/RATIS/OPEN/Leader
{code}

After:

{code}
bash-4.2$ ozone admin datanode list
Datanode: 8f572444-6134-4740-845f-12a8f454fad0 (/default-rack/172.22.0.2/ozone_datanode_4.ozone_default/0 pipelines)
Operational State: DECOMMISSIONING
Related pipelines:
No related pipelines or the node is not in Healthy state.

Datanode: ae9cab4e-d163-4983-b7bb-4d140fbd41b5 (/default-rack/172.22.0.7/ozone_datanode_1.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
7e7bd855-3cc4-4e95-b363-38740580915c/ONE/RATIS/OPEN/Leader
9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower

Datanode: d992655c-aa4f-44ec-8bca-631191a527ef (/default-rack/172.22.0.8/ozone_datanode_3.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower
6fa79344-0791-454f-8c4f-2edb0cb2ea34/ONE/RATIS/OPEN/Leader

Datanode: 327bc5f1-874b-42da-9951-f4cc1571bab9 (/default-rack/172.22.0.9/ozone_datanode_2.ozone_default/2 pipelines)
Operational State: IN_SERVICE
Related pipelines:
a28a6008-b3b8-45c8-a7f4-23ca680b3e46/ONE/RATIS/OPEN/Leader
9d4b291d-a086-4a8e-af8f-278ffd4769b0/THREE/RATIS/ALLOCATED/Follower
{code}"	HDDS	Resolved	5	7	9462	pull-request-available
13408007	OFS mkdir -p does not work as expected for bucket creation when volume exists due to volume create ACL check	"We discovered this problem during the implementation of HttpFS Gateway. I did an acceptance test for the HttpFS with Robot framework. In the ozonesecure docker environment when I tried to make a volume with the testuser principal it didn't work, because it doesn't have permission to do it. So we decided to make a volume with an admin, set the testuser as the owner of it and then create buckets with the testuser. Even after the owner change happened successfully it gave the same error:
User testuser/httpfs@EXAMPLE.COM doesn't have CREATE permission to access volume vol01 null null
After debugging we found why this happened. As the bucket is not existing first it goes to the getBucket() method in the [BasicRootedOzoneClientAdapterImpl|https://github.com/apache/ozone/blob/master/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java#L234] class.

The createIfNotExist is true and both in the VOLUME_NOT_FOUND and BUCKET_NOT_FOUND cases tries to create volume first, which the testuser does not have permission. So we got the error from there, despite of the fact that testuser is the owner of that volume, so it should be able to create buckets inside.

We were able to recreate this in terminal in the scm container (in that because it has both testuser and testuser2 as principals).
{code:java}
bash-4.2$ klist
Ticket cache: FILE:/tmp/krb5cc_1000
Default principal: testuser/scm@EXAMPLE.COM

Valid starting     Expires            Service principal
10/18/21 11:23:39  10/19/21 11:23:39  krbtgt/EXAMPLE.COM@EXAMPLE.COM
        renew until 10/25/21 11:23:39
{code}
In the scm testuser is an admin, testuser/scm@EXAMPLE.COM is added as an ozone administrator in the docker-config. I did the same with testuser/httpfs@EXAMPLE.COM but it is not an admin, as the username is mapped to short user principal name (with an auth-to-local rule), which is testuser. Because of this the equality check between testuser and testuser/httpfs@EXAMPLE.COM is false, so it is not taken as an admin user.
{code:java}
bash-4.2$ ozone sh volume update --user=testuser2 vol02
\{
  ""metadata"" : { },
  ""name"" : ""vol02"",
  ""admin"" : ""testuser"",
  ""owner"" : ""testuser2"",
  ""quotaInBytes"" : -1,
  ""quotaInNamespace"" : -1,
  ""usedNamespace"" : 0,
  ""creationTime"" : ""2021-10-18T11:19:59.777Z"",
  ""modificationTime"" : ""2021-10-18T11:24:04.183Z"",
  ""acls"" : [ \{
    ""type"" : ""USER"",
    ""name"" : ""testuser"",
    ""aclScope"" : ""ACCESS"",
    ""aclList"" : [ ""ALL"" ]
  } ]
}
bash-4.2$ kinit -kt /opt/hadoop/compose/_keytabs/testuser2.keytab testuser2/scm@EXAMPLE.COM
bash-4.2$ klist
Ticket cache: FILE:/tmp/krb5cc_1000
Default principal: testuser2/scm@EXAMPLE.COM

Valid starting     Expires            Service principal
10/18/21 11:24:17  10/19/21 11:24:17  krbtgt/EXAMPLE.COM@EXAMPLE.COM
        renew until 10/25/21 11:24:17
bash-4.2$ ozone fs -mkdir -p ofs://om/vol01/buck01
2021-10-18 11:24:47,729 [main] INFO rpc.RpcClient: Creating Volume: vol01, with testuser2 as owner and space quota set to -1 bytes, counts quota set to -1
mkdir: User testuser2/scm@EXAMPLE.COM doesn't have CREATE permission to access volume vol01 null null
{code}"	HDDS	Resolved	3	1	9462	pull-request-available
13283316	Implement ofs://: Fix getFileStatus for mkdir volume	"[~xyao] discovered that when running {{ozone fs -mkdir -p ofs://om/vol1/}} (only volume name is given, no bucket name or key path), the command would fail in {{getFileStatus}} (before reaching {{createDirectory()}}) in Hadoop common code:

{code:bash}
bash-4.2$ ozone fs -mkdir -p ofs://om/vol1/
-mkdir: Bucket or Volume length is illegal, valid length is 3-63 characters

# Same w/o -p
bash-4.2$ ozone fs -mkdir ofs://om/vol1/
-mkdir: Bucket or Volume length is illegal, valid length is 3-63 characters
{code}

And we discovered with debugger attached that the root cause is that {{getFileStatus()}} is not behaving as expected.

Solution: Patch existing OFS code, throw proper exception in {{getBucket()}} code to make Hadoop common happy."	HDDS	Resolved	3	7	9462	pull-request-available
13281073	Implement ofs://: temp directory mount	"Because of ofs:// filesystem hierarchy starts with volume then bucket, an application typically won't be able to write directly under a first-level folder, e.g. ofs://service-id1/tmp/. /tmp/ is a special case that we need to handle since that is the default location most legacy Hadoop applications write to for swap/temporary files. In order to address this, we would introduce /tmp/ as a client-side ""mount"" to another Ozone bucket.

Note that the preliminary implementation would only allow for /tmp/ to be a mount but not any user-defined path.

This depends on HDDS-2840 and HDDS-2928."	HDDS	Resolved	3	7	9462	pull-request-available
13286925	Allow users to list volumes they have access to, and optionally allow all users to list all volumes	"Current implementation of {{listVolumes}} only returns the volumes the user creates.
And there's no existing OM public API to return a list of users or return all volumes. Which means we must add new APIs to OM to either return user list or all volumes in order for this feature to work.

-We can open another jira on master branch to add those APIs, get back to this jira and add this function to OFS.-
-After a discussion with [~arpaga], Sanjay suggested we should allow *all* users to list *all* volumes for now. (Users still won't be able to access volumes which they don't have permission to.)-
Note: As HDDS-2610 is committed, it enabled clients to perform {{listAllVolumes}}, which allows admin users to list all volumes. This jira just need to make sure non-admin users can list all volumes as well.

In fact this jira tries to achieve two objectives:
1. Allow users to list volumes they have access to
2. Optionally allow all users to list all volumes"	HDDS	Resolved	3	4	9462	pull-request-available
13297603	OMVolumeSetOwnerRequest doesn't check if user is already the owner	"OMVolumeSetOwnerRequest doesn't seem to check if the user is already the owner.
If the user is already the owner, it shouldn't proceed to the update logic, otherwise the resulting volume list for that user in {{UserVolumeInfo}} would have duplicate volume entry. As demonstrated in the test case.

-It also doesn't seem to remove the volume from the UserVolumeInfo from the previous owner.- Checked [here|https://github.com/apache/hadoop-ozone/blob/80e9f0a7238953e41b06d22f0419f04ab31d4212/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeSetOwnerRequest.java#L152-L153].

[~bharat]"	HDDS	Resolved	3	1	9462	pull-request-available
13426635	Bump centos to 7.9.2009, dependencies and tools in ozone-runner	"This is a first step toward HDDS-6263.

arm64 docker base images are available in centos 8 official Docker Hub repo.

This should not affect any existing x86-64 / amd64 builds and tests."	HDDS	Resolved	3	7	9462	pull-request-available
13249870	Owner/group information for a file should be returned from OzoneFileStatus	"BasicOzoneFilesystem returns the file's user/group information as the current user/group. This should default to the information read from the acl's for the file.

cc [~xyao]"	HDDS	Resolved	2	1	9462	Triaged
13342255	[OFS] Listing volumes under root should return all volumes whenever possible	"Currently {{listStatusRoot()}} only lists volumes owned(ACL disabled) or accessible(ACL enabled) by current user.

This prevents HttpFS from listing all volumes under OFS root.

Since we probably can't provide an argument to allow passing in parameter {{--all}} due to conforming to HCFS, we should default to list ALL volumes whenever this is possible.

Note: {{ozone.om.volume.listall.allowed}} is an OM side argument that controls whether any user can list all volumes on an Ozone cluster, it defaults to {{true}}."	HDDS	Resolved	3	7	9462	pull-request-available
13442950	[MultiTenancy] User get-secret throws USER_MISMATCH	"User get-secret API fails with user mismatch even though its kinited with correct user.

 
{code:java}
bash-4.2$ kinit -kt /etc/security/keytabs/testuser.keytab testuser/om
bash-4.2$ ozone tenant user get-secret 'tenantone$testuser'
USER_MISMATCH Requested accessId 'tenantone$testuser' doesn't match current user 'testuser/om@EXAMPLE.COM', nor does current user has administrator privilege.{code}"	HDDS	Resolved	2	7	9462	ozone-multitenancy
13440847	[Multi-Tenant] Follow-up: Set owner of buckets created via S3 Gateway to actual user	This is a follow-up jira to HDDS-6574 in the multi-tenancy branch. See https://github.com/apache/ozone/pull/3298#discussion_r848659291	HDDS	Resolved	3	7	9462	pull-request-available
13299773	Refactor OFSPath to adapt to master branch	See PR description: https://github.com/apache/hadoop-ozone/pull/847	HDDS	Resolved	3	7	9462	pull-request-available
13326113	[OFS] Better owner and group display for listing Ozone volumes and buckets	"Improve volumes' and buckets' owner and group display when listing in OFS.

1. Display short name instead of full Kerberos principal.
2. For volumes, get actual group of the owner (currently it is the volume admin name which is incorrect)
3. For buckets, display the owner and group of its parent volume."	HDDS	Resolved	3	1	9462	pull-request-available
13589513	FS CLI gives incorrect recursive volume deletion prompt	"Symptom:

From CLI (with {{compose/ozone}} Docker dev cluster):
{code}
bash-4.2$ ozone fs -rm -skipTrash -r ofs://om/vol1/
rm: Recursive volume delete using ofs is not supported. Instead use 'ozone sh volume delete -r -skipTrash -id <OM_SERVICE_ID> <Volume_URI>' command
{code}

But when I follow the prompt, I get ""Unknown options"":

{code}
bash-4.2$ ozone sh volume delete -r -skipTrash -id om /vol1
Unknown options: '-skipTrash', '-id', '/vol1'
Usage: ozone sh volume delete [-hrVy] [-t=<threadNo>] <value>
deletes a volume
      <value>     URI of the volume.
                  Ozone URI could either be a full URI or short URI.
                  Full URI should start with o3://, in case of non-HA
                  clusters it should be followed by the host name and
                  optionally the port number. In case of HA clusters
                  the service id should be used. Service id provides a
                  logical name for multiple hosts and it is defined
                  in the property ozone.om.service.ids.
                  Example of a full URI with host name and port number
                  for a key:
                  o3://omhostname:9862/vol1/bucket1/key1
                  With a service id for a volume:
                  o3://omserviceid/vol1/
                  Short URI should start from the volume.
                  Example of a short URI for a bucket:
                  vol1/bucket1
                  Any unspecified information will be identified from
                  the config files.

  -h, --help      Show this help message and exit.
  -r              Delete volume recursively
  -t, --thread, --threads=<threadNo>
                  Number of threads used to execute recursive delete
  -V, --version   Print version information and exit.
  -y, --yes       Continue without interactive user confirmation
bash-4.2$
{code}

The correct command line to achieve the intended purpose (recursively delete the volume) should be this:

{code}
bash-4.2$ ozone sh volume delete -r o3://om/vol1
This command will delete volume recursively.
There is no recovery option after using this command, and no trash for FSO buckets.
Delay is expected running this command.
Enter 'yes' to proceed': yes
Volume vol1 is deleted
bash-4.2$
{code}"	HDDS	Resolved	3	1	9462	pull-request-available
13530490	[Snapshot] Reduce flakiness in testSkipTrackingWithZeroSnapshot	https://github.com/apache/ozone/actions/runs/4540058938/jobs/8001489891?pr=3980	HDDS	Resolved	3	7	9462	pull-request-available
13298356	Add response to SetVolumePropertyResponse proto	"https://github.com/apache/hadoop-ozone/pull/806#discussion_r408279098

1. Add {{optional bool response = 1;}} in the [message|https://github.com/apache/hadoop-ozone/blob/15db251f16236228c6596253dab6946494fc8f5b/hadoop-ozone/common/src/main/proto/OzoneManagerProtocol.proto#L425-L427].
2. Handle the response on the client. e.g. in setOwner, if the response is false, we can print message: The specified user is already the owner of the volume.

Will start the work after PR #821 is merged."	HDDS	Resolved	3	4	9462	pull-request-available
13393546	Restore adding ozone-site.xml to default resource in OzoneConfiguration#activate	"In rare cases, applications will not seemingly trigger OzoneConfiguration#loadDefaults, resulting in only default in ozone-default.xml being loaded (in activate()). We can *steadily repro* the bug with pysparkshell in the below mode:

{code}
pyspark --master yarn --deploy-mode client --name ""PySparkShell""
{code}

The line was removed in HDDS-1469 ([PR|https://github.com/apache/hadoop/pull/773#pullrequestreview-230833296]) with the refactoring as it was seemingly unnecessary to keep it."	HDDS	Resolved	3	1	9462	pull-request-available
13426526	OM crashes when trying to overwrite a key during upgrade downgrade testing	"While working on HDDS-6084 (related to upgrade acceptance testing), [~erose] and I found that if:
1) a key is created with a new OM version (1.3.0)
2) downgrade to OM 1.1.0
3) try to overwrite the key created in (1)

Step (3) will result in all 3 OMs crashing.

The issue seems to be introduced in the unreleased branch of 1.3.0. The same issue cannot be reproduced in 1.1.0 to 1.2.0 upgrade/downgrade testing (which should exclude HDDS-5243 or HDDS-5393 as a potential root cause). This could indicate some unreleased changes has broken the key versioning after the downgrade. (Could well be an incompatible change. Need further investigation.)

{code}
om2_1    | 2022-02-03 21:36:15,228 [OM StateMachine ApplyTransaction Thread - 0] ERROR ratis.OzoneManagerStateMachine: Terminating with exit status 1: Request cmdType: CreateKey
om2_1    | traceID: """"
om2_1    | clientId: ""client-72B024AF247D""
om2_1    | userInfo {
om2_1    |   userName: ""dlfknslnfslf""
om2_1    |   remoteAddress: ""10.9.0.19""
om2_1    |   hostName: ""ha_s3g_1.ha_net""
om2_1    | }
om2_1    | version: 1
om2_1    | createKeyRequest {
om2_1    |   keyArgs {
om2_1    |     volumeName: ""s3v""
om2_1    |     bucketName: ""old1-bucket""
om2_1    |     keyName: ""key2""
om2_1    |     dataSize: 17539
om2_1    |     type: RATIS
om2_1    |     factor: THREE
om2_1    |     keyLocations {
om2_1    |       blockID {
om2_1    |         containerBlockID {
om2_1    |           containerID: 1
om2_1    |           localID: 107736214721200128
om2_1    |         }
om2_1    |         blockCommitSequenceId: 0
om2_1    |       }
om2_1    |       offset: 0
om2_1    |       length: 268435456
om2_1    |       createVersion: 0
om2_1    |       pipeline {
om2_1    |         members {
om2_1    |           uuid: ""b92bf4c8-3b0c-40b0-bb2b-05b6d3594e13""
om2_1    |           ipAddress: ""10.9.0.16""
om2_1    |           hostName: ""ha_dn2_1.ha_net""
om2_1    |           ports {
om2_1    |             name: ""REPLICATION""
om2_1    |             value: 9886
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS""
om2_1    |             value: 9858
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS_ADMIN""
om2_1    |             value: 9857
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS_SERVER""
om2_1    |             value: 9856
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""STANDALONE""
om2_1    |             value: 9859
om2_1    |           }
om2_1    |           networkName: ""b92bf4c8-3b0c-40b0-bb2b-05b6d3594e13""
om2_1    |           networkLocation: ""/default-rack""
om2_1    |           persistedOpState: IN_SERVICE
om2_1    |           persistedOpStateExpiry: 0
om2_1    |           uuid128 {
om2_1    |             mostSigBits: -5103716611873029968
om2_1    |             leastSigBits: -4959864281830437357
om2_1    |           }
om2_1    |         }
om2_1    |         members {
om2_1    |           uuid: ""f0b7e615-d4ee-4ec4-a6b5-ec68b82c07e9""
om2_1    |           ipAddress: ""10.9.0.15""
om2_1    |           hostName: ""ha_dn1_1.ha_net""
om2_1    |           ports {
om2_1    |             name: ""REPLICATION""
om2_1    |             value: 9886
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS""
om2_1    |             value: 9858
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS_ADMIN""
om2_1    |             value: 9857
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS_SERVER""
om2_1    |             value: 9856
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""STANDALONE""
om2_1    |             value: 9859
om2_1    |           }
om2_1    |           networkName: ""f0b7e615-d4ee-4ec4-a6b5-ec68b82c07e9""
om2_1    |           networkLocation: ""/default-rack""
om2_1    |           persistedOpState: IN_SERVICE
om2_1    |           persistedOpStateExpiry: 0
om2_1    |           uuid128 {
om2_1    |             mostSigBits: -1101158602427707708
om2_1    |             leastSigBits: -6433976558118238231
om2_1    |           }
om2_1    |         }
om2_1    |         members {
om2_1    |           uuid: ""c7912312-811d-469d-8c40-c739cd2a1e62""
om2_1    |           ipAddress: ""10.9.0.17""
om2_1    |           hostName: ""ha_dn3_1.ha_net""
om2_1    |           ports {
om2_1    |             name: ""REPLICATION""
om2_1    |             value: 9886
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS""
om2_1    |             value: 9858
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS_ADMIN""
om2_1    |             value: 9857
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""RATIS_SERVER""
om2_1    |             value: 9856
om2_1    |           }
om2_1    |           ports {
om2_1    |             name: ""STANDALONE""
om2_1    |             value: 9859
om2_1    |           }
om2_1    |           networkName: ""c7912312-811d-469d-8c40-c739cd2a1e62""
om2_1    |           networkLocation: ""/default-rack""
om2_1    |           persistedOpState: IN_SERVICE
om2_1    |           persistedOpStateExpiry: 0
om2_1    |           uuid128 {
om2_1    |             mostSigBits: -4066430426156284259
om2_1    |             leastSigBits: -8340447458821005726
om2_1    |           }
om2_1    |         }
om2_1    |         state: PIPELINE_OPEN
om2_1    |         type: RATIS
om2_1    |         factor: THREE
om2_1    |         id {
om2_1    |           id: ""c0b6f272-9a39-4dc3-ada8-c3b833cc6e17""
om2_1    |           uuid128 {
om2_1    |             mostSigBits: -4560190998638408253
om2_1    |             leastSigBits: -5933277313150194153
om2_1    |           }
om2_1    |         }
om2_1    |         leaderID: ""f0b7e615-d4ee-4ec4-a6b5-ec68b82c07e9""
om2_1    |         creationTimeStamp: 1643924132125
om2_1    |         suggestedLeaderID {
om2_1    |           mostSigBits: -1101158602427707708
om2_1    |           leastSigBits: -6433976558118238231
om2_1    |         }
om2_1    |         leaderID128 {
om2_1    |           mostSigBits: -1101158602427707708
om2_1    |           leastSigBits: -6433976558118238231
om2_1    |         }
om2_1    |       }
om2_1    |       partNumber: 0
om2_1    |     }
om2_1    |     isMultipartKey: false
om2_1    |     acls {
om2_1    |       type: USER
om2_1    |       name: ""dlfknslnfslf""
om2_1    |       rights: ""\200""
om2_1    |       aclScope: ACCESS
om2_1    |     }
om2_1    |     modificationTime: 1643924174840
om2_1    |   }
om2_1    |   clientID: 107736214722445312
om2_1    | }
om2_1    | failed with exception
om2_1    | java.lang.IllegalArgumentException
om2_1    | 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:128)
om2_1    | 	at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.<init>(OmKeyInfo.java:81)
om2_1    | 	at org.apache.hadoop.ozone.om.helpers.OmKeyInfo$Builder.build(OmKeyInfo.java:378)
om2_1    | 	at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getFromProtobuf(OmKeyInfo.java:460)
om2_1    | 	at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.fromPersistedFormat(OmKeyInfoCodec.java:59)
om2_1    | 	at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.fromPersistedFormat(OmKeyInfoCodec.java:36)
om2_1    | 	at org.apache.hadoop.hdds.utils.db.CodecRegistry.asObject(CodecRegistry.java:55)
om2_1    | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getFromTableIfExist(TypedTable.java:261)
om2_1    | 	at org.apache.hadoop.hdds.utils.db.TypedTable.getIfExist(TypedTable.java:248)
om2_1    | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:236)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:415)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:240)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1    | 2022-02-03 21:36:15,253 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG:
om2_1    | /************************************************************
om2_1    | SHUTDOWN_MSG: Shutting down OzoneManager at a250845831a7/10.9.0.12
om2_1    | ************************************************************/
{code}

This is where OM is crashing in ozone 1.1.0 code:

https://github.com/apache/ozone/blob/ozone-1.1.0/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyInfo.java#L81-L82"	HDDS	Resolved	1	1	9462	pull-request-available
13311491	[OFS] Add User Guide	"Need to add a user guide markdown for OFS. Especially the usage for {{/tmp}}.

Thanks [~umamaheswararao] and [~xyao] for the reminder.

{{hadoop-hdds/docs/content/design/ofs.md}}"	HDDS	Resolved	1	7	9462	pull-request-available
13442951	[MultiTenancy] DBinfo message on console on missing accessId	"Assignadmin on an invalid accessId should throw better error message
{code:java}
bash-4.2$ ozone tenant user assignadmin none
Failed to assign admin to 'none': OmDBAccessIdInfo is missing for accessId 'none' in DB.{code}"	HDDS	Resolved	4	7	9462	ozone-multitenancy
13423595	New checkstyle: WhitespaceAround	"To enforce whitespace check around symbols.

Ref: https://checkstyle.sourceforge.io/config_whitespace.html#WhitespaceAround

Could optionally exclude some cases (allowEmptyMethods, allowEmptyConstructors, allowEmptyTypes, allowEmptyLoops, allowEmptyLambdas and allowEmptyCatches)."	HDDS	Resolved	3	4	9462	pull-request-available
13337674	Update the container replica history to the Recon DB lazily instead of for every report.	"Recon tracks the history for every container replica on the Ozone cluster in its SQL DB (By default, this is Derby). To track this, it keeps track of the last timestamp of a replica on a DN through reports. This becomes a SQL DB scan + write operation for every container report received.  Even though there is async hand off from the report to EventQueue, the event queue handler itself by default uses 1 thread per event type (report type). Hence, there is implicit blocking behavior here which is pushed down to DNs.

This has to be changed into a lazy update of DB to support better scalability. Details on how to achieve this will be added to the JIRA later."	HDDS	Resolved	2	3	9462	pull-request-available
13319640	[OFS] BasicRootedOzoneFileSystem to support batchDelete	"This Jira is to use deleteObjects in OFS delete now that HDDS-3286 is committed.

Currently when ozone.om.enable.filesystem.paths is enabled it normalizes the path, so using deleteKey for delete directory will fail.

According to [~bharat] this should be a blocker for 0.6.0.
"	HDDS	Resolved	1	1	9462	pull-request-available
13272337	Suppress loader constraint violation message in TestOzoneFileSystemWithMocks	"{{TestOzoneFileSystemWithMocks}} throws LinkageError error when run (but test succeeds):

{code}
ERROR StatusLogger Could not reconfigure JMX
 java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/MockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
	at org.powermock.core.classloader.MockClassLoader.loadUnmockedClass(MockClassLoader.java:250)
	at org.powermock.core.classloader.MockClassLoader.loadModifiedClass(MockClassLoader.java:194)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:71)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:335)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:259)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:164)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:558)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:619)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:636)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:231)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
	at org.apache.commons.logging.LogAdapter$Log4jLog.<clinit>(LogAdapter.java:135)
	at org.apache.commons.logging.LogAdapter$Log4jAdapter.createLog(LogAdapter.java:102)
	at org.apache.commons.logging.LogAdapter.createLog(LogAdapter.java:79)
	at org.apache.commons.logging.LogFactoryService.getInstance(LogFactoryService.java:46)
	at org.apache.commons.logging.LogFactoryService.getInstance(LogFactoryService.java:41)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:136)
	at org.apache.hadoop.fs.ozone.TestOzoneFileSystemWithMocks.testFSUriWithHostPortOverrides(TestOzoneFileSystemWithMocks.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:68)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:316)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:88)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:96)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:300)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:131)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.access$100(PowerMockJUnit47RunnerDelegateImpl.java:59)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner$TestExecutorStatement.evaluate(PowerMockJUnit47RunnerDelegateImpl.java:147)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.evaluateStatement(PowerMockJUnit47RunnerDelegateImpl.java:107)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:82)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:288)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:86)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:49)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:208)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:147)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:121)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:33)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:45)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:123)
	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:121)
	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:53)
	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)

Process finished with exit code 0
{code}

Goal: Suppress this error message."	HDDS	Resolved	3	4	9462	pull-request-available
13321348	[OFS] Implement AbstractFileSystem for RootedOzoneFileSystem	Extracted from HDDS-3805: introduce an implementation of {{AbstractFileSystem}}, similar to {{OzFs}}, for {{RootedOzoneFileSystem}}.	HDDS	Resolved	3	7	9462	pull-request-available
13588909	[hsync] Block ECKeyOutputStream from calling hsync and hflush	"ECKeyOutputStream extends KeyOutputStream, but EC keys does not support hsync() as there are known issues with that: HDDS-8932

(Even if we add EC hsync support in the future, there would be significant overhead with small writes due to striping and parity calc. But that is a different topic.)

This jira prevents hsync() and hflush() from being called on ECKeyOutputStream by throwing NotImplementedException."	HDDS	Resolved	3	7	9462	pull-request-available
13534881	[Snapshot] Custom SnapshotCache implementation to replace LoadingCache	"This is the continuation of HDDS-7935.

In HDDS-7935 we replaced LoadingCache's {{maximumSize()}} with {{softValues()}}. But there are concerns regarding JVM GC's undeterministic behavior of invalidating the cache entries with the soft references.

In this jira, we are going to implement a custom SnapshotCache to replace LoadingCache gain full control over snapshot cache entry eviction/invalidation.

PR: https://github.com/apache/ozone/pull/4567"	HDDS	Resolved	3	7	9462	pull-request-available
13567243	Remove unnecessary sleep in TestMiniOzoneCluster	See PR.	HDDS	Resolved	3	7	9462	pull-request-available
13440213	[MultiTenancy] No user validation on assignUser API	"No validation of user while running assignUser API under tenant.

Non-existent User
{code:java}
bash-4.2$ ozone tenant user assign user -t tenantone
Assigned 'user' to 'tenantone' with accessId 'tenantone$user'.
export AWS_ACCESS_KEY_ID='tenantone$user'
export AWS_SECRET_ACCESS_KEY='b58a64f66e6091cd22cdd1666e226c82e8138ba7a86804a3086d108ef6036961'{code}
Invalid user (tried regex)
{code:java}
bash-4.2$ ozone tenant user assign ""*"" -t tenantone
Assigned '*' to 'tenantone' with accessId 'tenantone$*'.
export AWS_ACCESS_KEY_ID='tenantone$*'
export AWS_SECRET_ACCESS_KEY='27f9420833b1433774660654a8cc054e76d630e0d5d2ee3d0e3a1c327ecc5ac8'
bash-4.2$ ozone tenant user assign ""user*"" -t tenantone
Assigned 'user*' to 'tenantone' with accessId 'tenantone$user*'.
export AWS_ACCESS_KEY_ID='tenantone$user*'
export AWS_SECRET_ACCESS_KEY='99c4652cc90a4f5b46396432b00c3422f0ba481528cdc968b91ee6cedaa2f649'{code}

User of length greater than 100
{code:java}
bash-4.2$ ozone tenant user assign --tenant=tenantone 'testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas'
Assigned 'testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas' to 'tenantone' with accessId 'tenantone$testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas'.
export AWS_ACCESS_KEY_ID='tenantone$testuser-f27b137a62cd8b021239527c725d6a9d56e0cdce8ca7db6a4b923c941452df00sfdadfdadfsddfaddsajjdakfisfiaidhikakdkjdkasjkdas'
export AWS_SECRET_ACCESS_KEY='b9e5ad69c39561446b571419dba3e39b0b90936040c63b2a70ba5b94a7fb9f85'
{code}
"	HDDS	Resolved	3	7	9462	ozone-multitenancy
13442382	Make Hugo markdown image syntax add responsive image class	"This should solve the image size issue (some are too large, like in PrefixFSO.md) *when using the Markdown image syntax* (not the shortcode).

See comment: https://github.com/apache/ozone/pull/2582/files#r861196807"	HDDS	Resolved	3	3	9462	pull-request-available
13298358	Rebase OFS branch - 2. Adapt OFS classes to HDDS-3101	"HDDS-3353 broke OFSPath since the latter uses {{org.apache.commons.codec.digest.DigestUtils}} but the dependency is removed.

HDDS-3359 broke OFSPath because the latter also uses {{org.apache.yetus.audience}}, but this is a simple fix - just replace it with {{org.apache.hadoop.hdds.annotation}}."	HDDS	Resolved	3	7	9462	pull-request-available
13310429	[OFS] Address merge conflicts after HDDS-3627	"HDDS-3627 removed isolated class loader and moved classes around.

I will address the merge conflicts after rebasing OFS feature branch to master branch which includes HDDS-3627 in this jira."	HDDS	Resolved	1	7	9462	pull-request-available
13550952	Instruct admins to use OZONE_MANAGER_CLASSPATH for RangerOzoneAuthorizer	Use {{OZONE_MANAGER_CLASSPATH}} instead of {{OZONE_CLASSPATH}} to narrow down the effective scope of the classpath to OMs only.	HDDS	Resolved	3	4	9462	pull-request-available
13428978	[Multi-Tenant] Add tenant CLI option to print results in JSON	"Applicable to all tenant subcommands:
{code}
ozone tenant create
ozone tenant user assign
ozone tenant user assignadmin
ozone tenant user info
ozone tenant list
...
{code}

And this would be especially helpful for TenantListHandler (`ozone tenant list`) which could print a lot of extra info ({{BucketNS, UserRole, AdminRole, BucketNSPolicy, BucketPolicy}})"	HDDS	Resolved	3	7	9462	pull-request-available
13419769	Update log4j version to 2.17.1	"Release notes: https://github.com/apache/logging-log4j2/blob/rel/2.17.1/RELEASE-NOTES.md

Looks like another RCE ([CVE-2021-44832|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44832]) in 2.17.0.

{code}
Apache Log4j2 versions 2.0-beta7 through 2.17.0 (excluding security fix releases 2.3.2 and 2.12.4) are vulnerable to a remote code execution (RCE) attack where an attacker with permission to modify the logging configuration file can construct a malicious configuration using a JDBC Appender with a data source referencing a JNDI URI which can execute remote code. This issue is fixed by limiting JNDI data source names to the java protocol in Log4j2 versions 2.17.1, 2.12.4, and 2.3.2.
{code}

https://www.bleepingcomputer.com/news/security/log4j-2171-out-now-fixes-new-remote-code-execution-bug/"	HDDS	Resolved	3	1	9462	pull-request-available
13543190	Pin maven-antrun-plugin version to 3.1.0	"Now sure what changed on my end but I suddenly hit this error during maven build today when I am building from the latest master branch:

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:3.1.0:run (default) on project hdds-interface-client: You are using 'tasks' which has been removed from the maven-antrun-plugin. Please use 'target' and refer to the >>Major Version Upgrade to version 3.0.0<< on the plugin site. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:3.1.0:run (default) on project hdds-interface-client: You are using 'tasks' which has been removed from the maven-antrun-plugin. Please use 'target' and refer to the >>Major Version Upgrade to version 3.0.0<< on the plugin site.
{code}

This issue can be solved by pinning maven-antrun-plugin version to the latest, and replacing the removed {{<tasks>}} tag, according to ""Major Version Upgrade to version 3.0.0"" section here:

https://maven.apache.org/plugins/maven-antrun-plugin/"	HDDS	Resolved	3	3	9462	pull-request-available
13249214	Place ozone.om.address config key default value in ozone-site.xml	"{code:xml}
   <property>
     <name>ozone.om.address</name>
-    <value/>
+    <value>0.0.0.0:9862</value>
     <tag>OM, REQUIRED</tag>
     <description>
       The address of the Ozone OM service. This allows clients to discover
{code}"	HDDS	Resolved	3	4	9462	pull-request-available
13281066	Implement ofs://: listStatus	"ofs:// should be able to handle list (recursive or not) under ""root"" and under each volume ""directory"", as if it is a flat filesystem (if the user has permissions to see the volumes).

This is dependent on HDDS-2840. Will post PR after HDDS-2840 is reviewed and committed."	HDDS	Resolved	3	7	9462	pull-request-available
13505361	EC: ReplicationManager - refactor logic to send datanode commands into a central place	The logic to send datanode commands, such as replicate, reconstruct, delete and close container happens in a few different places in the RM classes. It would make sense to centralise this in some common code, as it will make it more re-usable. There is some logic in the balancer which could re-use these new central methods to send replicate and delete commands too.	HDDS	Resolved	3	7	9499	pull-request-available
13529441	Let ReplicationManager decide the timeout for commands in Datanodes	"Right now, ""hdds.scm.replication.command.deadline.factor"" is a fraction. For long durations such as 60 minutes, the difference between SCM deadline and Datanode deadline will be 60 - 60 * 0.9, which is 6 minutes. This is a significant difference, so perhaps this configuration should be a duration instead, like 30 seconds.

Currently the APIs provided by RM expose the DN deadline as a parameter. We could remove this and just let the RM decide a deadline for commands in the DN."	HDDS	Resolved	3	7	9499	pull-request-available
13259239	Extend SCMCLI Topology command to print node Operational States	The scmcli topology command only consider the node health (healthy, stale or dead). With decommission and maintenance stages, we need to also consider the operational states and display them with this command.	HDDS	Resolved	3	7	9499	pull-request-available
13516730	 EC: Verify unrecoverable EC containers which are empty transition to deleting	"In EC, a container is considered ""missing"" if there are not enough replicas to reconstruct it. If a key is deleted from one of these containers, the blocks should still be deleted from the replicas that are present. When all the blocks are deleted, the remaining replicas will be empty so the container should be deleted. It looks like this already happens in the code, so this Jira is to add a unit test for this case as well."	HDDS	Resolved	3	7	9499	pull-request-available
13527630	ReplicationManager: Add RatisMisReplicationHandler into rm.processUnderReplicatedContainer	We missed adding the RatisMisReplicationHandler into the ReplicationManager.processUnderReplicatedContainer() method. This means that RatisMisReplication is never handled.	HDDS	Resolved	3	7	9499	pull-request-available
13401335	EC: ECBlockOutputstream commitKey should create one keyLocationInfo per logical block	"HDDS-5477 is pending due to a larger refactor effort. This change is therefore a temporary fix to the problem described in HDDS-5477, and will be superseded when HDDS-5477 is ready.

Currently at the time of commitKey, client creates the KeyLocationInfo for each individual instance of EC blocks. But when committing we should create one KeyLocationInfo per block as OM required to save one block info per block group in EC"	HDDS	Resolved	3	7	9499	pull-request-available
13558883	Incorrect sorting order for all unhealthy replicas in RatisOverReplicationHandler	"{code}
    if (allUnhealthy) {
      // prefer deleting replicas with lower sequence IDs
      return replicas.stream()
          .sorted(Comparator.comparingLong(ContainerReplica::getSequenceId)
              .reversed())
          .collect(Collectors.toList());
    }
{code}
This should actually be the opposite, allowing lower sequence IDs to be deleted first. Also need to consider what happens when two replicas have the same sequence ID - how are ties broken? Consistent ordering matters in case of SCM failover."	HDDS	Resolved	3	7	9499	pull-request-available
13267512	Refactor ReplicationManager to consider maintenance states	"In its current form the replication manager does not consider decommission or maintenance states when checking if replicas are sufficiently replicated. With the introduction of maintenance states, it needs to consider decommission and maintenance states when deciding if blocks are over or under replicated.

It also needs to provide an API to allow the decommission manager to check if blocks are over or under replicated, so the decommission manager can decide if a node has completed decommission and maintenance or not."	HDDS	Resolved	3	7	9499	pull-request-available
13579268	Rename rewriteGeneration to expectedDataGeneration in the protobuf and builders	"Currently in the code, the generation passed in the atomic overwrite API is named as overwriteGeneration in the proto definition.

Comments on the original PR suggest we should change that to one of:

generation
dataGeneration
expectedGeneration
expectedDataGeneration

This Jira is to decide on the name and then make the code change."	HDDS	Resolved	3	7	9499	pull-request-available
13392297	EC: Add missing break in switch statement when requesting EC blocks	We missed a break statement in the EC branch of the switch statement, so calls for EC blocks are falling through to the exception case.	HDDS	Resolved	3	7	9499	pull-request-available
13292082	Refactor SafeModeHandler to use a Notification Interface	"The SafeModeHandler currently accepts several objects which it notifies when the safe mode status changes.

Each of these object are notified using a different method (there is no ""notification interface"") and some of the logic which really belongs in those objects (ie what to do when safemode goes on or off) is in the safemode classes rather than in the receiving class.

As we may need to extend safemode somewhat to delay pipeline creation until sufficient nodes have registered, I think it is worthwhile to refactor this area to do the following:

1. Introduce a new Interface ""SafeModeTransition"" which must be implemented by any object which wants to listen for safemode starting or ending.

{code}
public interface SafeModeTransition {
  void handleSafeModeTransition(SCMSafeModeManager.SafeModeStatus status);
}
{code}

2. Pass the SafeModeStatus object over this new interface. That way, we can extend SafeModeStatus to include more states in the future than just safemode = true / false.

3. Change the constructor of SafeModeHandler to allow any number of objects to be registered to make it more flexible going forward.

4. Ensure the logic of what action to take on safemode transition lives within the notified objects rather than in the Safemode clases."	HDDS	Resolved	3	4	9499	pull-request-available
13450257	EC: ReplicationManager - Logic to process the over replicated queues and assign work to DNs	"We need some sort of thread which picks work from the under / over replicated queue and assigns the work to DNs with capacity. The DNs will pick the work up on each heartbeat.

This would use the class created in another Jira to create the command to fix the replication issue."	HDDS	Resolved	3	7	9499	pull-request-available
13521639	EC: Add normal and low priority to replication supervisor and commands	"In order to allow replication commands to fix under and mis-replication to run with higher priority that replication commands related to the balancer, we will change the replication supervisor to be a priority queue, ordered by priority, enqueue time.

Commands sent from the balancer will have a low priority, other commands will a normal priority. This means that balancer commands will not run while there are other replication commands present in the queue.

This means it is not important if the balancer adds a lot of replication commands to the queue and then some nodes go down in the cluster. The replication commands related to the down nodes will automatically get to the front of the queue."	HDDS	Resolved	3	7	9499	pull-request-available
13433178	EC: Avoid allocating buffers in EC Reconstruction Streams until first read	Due to the issue described in HDDS-6424, where KeyInputStream opens all its inputStreams for the entire key upfront, we should avoid allocating buffers in the EC Reconstruction InputStreams until they are actually needed (ie on first read).	HDDS	Resolved	3	7	9499	pull-request-available
13426222	EC: Fix todo items in TestECKeyOutputStream	"There are a couple of TODO comments in the test testWriteShouldSucceedWhenDNKilled - we should fix those. 

Additionally, we should look into making that test faster, as right now it can take over 1 minute to run."	HDDS	Resolved	3	7	9499	pull-request-available
13402299	Skip safemode check in TestOzoneManagerRocksDBLogging	"TestOzoneManagerRocksDBLogging can be made faster by skipping the SCM safemode check and setting the number of datanodes to 0 in the mini cluster.

Last runtime was:

{code}
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 81.735 s - in org.apache.hadoop.ozone.om.TestOzoneManagerRocksDBLogging
{code}"	HDDS	Resolved	3	4	9499	pull-request-available
13427563	Ensure immutable ContainerReplica set is returned from ContainerStateManagerImpl	"Inside ContainerStateMap, the replicas for a container are stored in a Set backed by a ConcurrentHashMap.

When you ask for the current replicas of a container, this method is used:
{code:java}
public Set<ContainerReplica> getContainerReplicas(
      final ContainerID containerID) {
    Preconditions.checkNotNull(containerID);
    final Set<ContainerReplica> replicas = replicaMap.get(containerID);
    return replicas == null ? null : Collections.unmodifiableSet(replicas);
} {code}
Note that it pulls out the Set, wraps it as unmodifiable and returns it.

There is a problem here, in that if the Set is updated by ICR / FCR at the same time as another part of the code has taken a reference to it, the other part of the code can make incorrect decisions. Eg:

 
{code:java}
Set<> replicas = getContainerReplicas(...)
replicaCount = replicas.size()
// continue to do something based on the size{code}
ReplicationManger has run into a race condition like this. We also use the Replicas to form pipelines for closed containers, so I worry there could be some strange issues if the set if mutated during the pipeline creation.

I see two possible solutions here. `GetContainerReplicas` should create a copy of the Set and return that, so the copy the other part of the code gets is its own copy and nothing can change it.

Or, we make the Set immutable, so that each new replica details are received, we create the new copy of the set and store that. Then any other parts of the code can get a reference to it, and know it will never change.

Mutations to the replicas for a closed container will only happen with FCR, which is relatively rare.

However we may ask for read pipelines very frequently, so it would be cheaper overall to use option 2.

It we go with option 2, I think we can move from a concurrentHashMap to a plain hashMap too, which may make the memory footprint slightly smaller.

Note access to the replicas is via ContainerStateManagerImpl, which already has a course RW lock protecting access to the container manager. Quite possibly FCR reporting could be improved by a finer grained or striped lock.

This problem was reported in HDDS-5643."	HDDS	Resolved	3	4	9499	pull-request-available
13428788	Remove toString in debug log parameters within SCMCommonPlacementPolicy	"The debug log has ""toString()"" called on datanode details, which means it must be evaluated before it gets passed into the debug logger. That means this string will always get created even when the log messages is not emitted.
{code:java}
LOG.debug(""Datanode {} is chosen. Required metadata size is {} and "" +
        ""required data size is {}"",
    datanodeDetails.toString(), metadataSizeRequired, dataSizeRequired); {code}
We can just drop the toString part to fix this."	HDDS	Resolved	3	4	9499	pull-request-available
13425225	EC: Bucket does not display correct EC replication details	"After creating a bucket using the shell:


{code:java}
ozone sh bucket create /vol1/testec -t EC -r rs-3-2-1024k {code}
And then listing the bucket info, the EC Replication details are not reflected in the output:
{code:java}
{
  ""metadata"" : { },
  ""volumeName"" : ""vol1"",
  ""name"" : ""testec"",
  ""storageType"" : ""DISK"",
  ""versioning"" : false,
  ""usedBytes"" : 0,
  ""usedNamespace"" : 0,
  ""creationTime"" : ""2022-01-27T17:29:54.289Z"",
  ""modificationTime"" : ""2022-01-27T17:29:54.289Z"",
  ""quotaInBytes"" : -1,
  ""quotaInNamespace"" : -1,
  ""bucketLayout"" : ""OBJECT_STORE"",
  ""owner"" : ""hadoop"",
  ""replicationConfig"" : {
    ""replicationFactor"" : ""THREE"",
    ""requiredNodes"" : 3,
    ""replicationType"" : ""RATIS""
  },
  ""link"" : false
} {code}"	HDDS	Resolved	3	7	9499	pull-request-available
13410811	EC: Create reusable buffer pool shared by all EC input and output streams	When reading and writing EC, we often have to allocate chunk sized Bytebuffers. It would be good if we could reuse them across multiple input / output instances to save freeing and re-allocating memory.	HDDS	Resolved	3	7	9499	pull-request-available
13505506	EC: ReplicationManager - remove calls to ECHealthCheck from under and over replication processing	"The under replication processing makes some calls which we no longer need into the ECContainerHealth check class. If the container ends up in the under replicated handler, it means it was under-replicated and we can just check that is still the case by checking the ECContainerReplicaCount object instead of going back to the ECContainerHealthCheck class.

"	HDDS	Resolved	3	7	9499	pull-request-available
13530576	ReplicationManager: Basic Throttling of EC Reconstruction commands	"EC Reconstruction commands should be throttled in a similar way to replicate container commands, so that a limited number of commands can be sent to any given node.

As reconstruction and replication share the same queue on the datanode, the datanode could be filled with a combination of replication and reconstruction commands, so the sendThrottledReplicationCommand method will also need adjusted to consider the number of reconstructions commands queue when checking the limit."	HDDS	Resolved	3	7	9499	pull-request-available
13473753	EC: ReplicationManager - Over replication handler should set repIndex on delete cmds	ContainerReplicaPendingOps needs to track the replica index of any pending replicas. In ECOverReplicationHandler we missed setting the repIndex of the replica in the delete command.	HDDS	Resolved	3	7	9499	pull-request-available
13541155	Move protobuf conversion out of the lock in PipelineStateManagerImpl	There are a few places where protobuf conversion is perform under a lock in PipelineStateManagerImpl. We should move these outside of the lock.	HDDS	Resolved	3	7	9499	pull-request-available
13530833	ReplicationManager: Add nodes to exclude list if they are overloaded	"When sending throttled replication / reconstruction commands, we know if the target node is at the replication task limit. If it is, we should add it to an exclude list in replicationManager.

When the DN heartbeats, it triggers a callback to RM, which lets it check if the node should be removed from the exclude list.

The exclude list can then be fed into the node selection for reconstruction tasks, so it can avoid picking nodes which are already overloaded as targets, but that will be another Jira."	HDDS	Resolved	3	7	9499	pull-request-available
13526983	ECReconstructionCoordinatorTask.runTask should catch Exception	"ECReconstructionCoordinatorTask.runTask() catches IOException, but any RuntimeExceptions fall threw the handler and are not logged correctly.

The handler should catch Exception to ensure any Runtime Exception caused by precondition checks are caught and handled too."	HDDS	Resolved	3	7	9499	pull-request-available
13334276	DatanodeAdminMonitor no longers needs maintenance end time to be passed	"An earlier change moved the maintenance endtime into the NodeStatus object. However when adding a node to the decommission monitor the end time must still be passed. This value is never used.

This Jira will remove the endInHours field from the interface:

{code}
public interface DatanodeAdminMonitor extends Runnable {

  void startMonitoring(DatanodeDetails dn, int endInHours);
  void stopMonitoring(DatanodeDetails dn);

}
{code}"	HDDS	Resolved	3	7	9499	pull-request-available
13502205	EC: ReplicationManager: Move Mis-Replicated into a separate unhealthy state	"While looking into mis-replicated handling for EC, we found the code is much simplified if we handle mis-replicated containers only when they are not also over or under replicated.

With that in mind, we should have a separate unhealthy state for mis-replication, rather than making it part of under-replication.

This change adds that new state, adds mis-replication logic to the ECReplicationCheckHandler and amends the RatisReplicationCheckHandler to match it.

For now, a mis-replicated queue has been added, but this may change later, as we need to look into the queues and see if they need more work to separate out EC and Ratis or not."	HDDS	Resolved	3	7	9499	pull-request-available
13386556	Avoid object creation in ReplicationManger debug log statements	"There are a few debug log entries in replication manager like this:

{code}
    LOG.debug(""Handling under-replicated container: {}"",
        container.getContainerID());

      LOG.debug(""Deleting empty container {} replicas,"", container.containerID());
{code}

Especially the latter one, will always allocate a new ContainerID object on each call, even if debug is not enabled.

If we just pass ""container"" then it will use the container.toString() method inside the logger, only if debug is enabled. The ContainerInfo toString looks like:

{code}
  @Override
  public String toString() {
    return ""ContainerInfo{""
        + ""id="" + containerID
        + "", state="" + state
        + "", pipelineID="" + pipelineID
        + "", stateEnterTime="" + stateEnterTime
        + "", owner="" + owner
        + '}';
  }
{code}

It contains some extra information, but some of that may be useful if debugging a problem."	HDDS	Resolved	3	4	9499	pull-request-available
13426220	EC: Container list command should allow filtering of EC containers	The container list command currently allows filtering by Factor, but with EC, it needs to be extended to filter by replication type and EC / Ratis replication schemes.	HDDS	Resolved	3	7	9499	pull-request-available
13287729	Replication manager should detect and correct containers which don't meet the replication policy	"In the current implementation, replication manager does not consider the container placement when checking if a container is healthy. Only the number of replicas are checked.

Now we have network topology available, we should consider whether replication manager should detect and correct mis-replicated containers.

In HDFS, the namenode will not automatically correct mis-replicated containers automatically, except at startup when all blocks are checked."	HDDS	Resolved	3	7	9499	pull-request-available
13256157	Update JMX metrics for node count in SCMNodeMetrics for Decommission and Maintenance	"Currently the class SCMNodeMetrics exposes JMX metrics for the number of HEALTHY, STALE and DEAD nodes.

It also exposes the disk capacity of the cluster and the amount of space used and available.

We need to decide how we want to display things in JMX when nodes are in and entering maintenance, decommissioning and decommissioned.

We now have 15 states rather than the previous 3, as we can have nodes in:
 * IN_SERVICE
 * ENTERING_MAINTENANCE
 * IN_MAINTENANCE
 * DECOMMISSIONING
 * DECOMMISSIONED

And in each of these states, nodes can be:
 * HEALTHY
 * STALE
 * DEAD

The simplest case would be to expose these 15 states directly in JMX, as it gives the complete picture, but I wonder if we need any summary JMX metrics too?

 

We also need to consider how to count disk capacity and usage. For example:
 # Do we count capacity and usage on a DECOMMISSIONING node? This is not a clear cut answer, as a decommissioning node does not provide any capacity for writers in the cluster, but it does use capacity.
 # For a DECOMMISSIONED node, we probably should not count capacity or usage
 # For an ENTERING_MAINTENANCE node, do we count capacity and usage? I suspect we should include the capacity and usage in the totals, however a node in this state will not be available for writes.
 # For an IN_MAINTENANCE node that is healthy?
 # For an IN_MAINTENANCE node that is dead?

I would welcome any thoughts on this before changing the code."	HDDS	Resolved	3	7	9499	pull-request-available
13260557	SortDatanodes does not return correct orders when many DNs on a given host	"In HDDS-2199 ScmNodeManager.getNodeByAddress() was changed to return a list of nodes rather than a single entry, to handle the case where many datanodes are running on the same host.

In SCMBlocKProtocol.sortDatanodes(), it uses the results returned from getNodesByAddress to determine if the client submitting the request is running on a cluster node, and if it is, it attempts to sort the datanodes by distance from the client machine.

To do this, the code currently takes the first DatanodeDetails object returned by getHostsByAddress and then compares it with the other passed in nodes. If any of the passed nodes are equal to the client node (based on the Java object ID) it returns a zero distance, otherwise the distance is calculated.

The sort is performed in NetworkTopologyImpl.sortByDistanceCost() which later calls NetworkTopologyImpl.getDistanceCost() which is where the object comparison is performed:

{code}
if ((node1 != null && node2 != null && node1.equals(node2)) ||
 (node1 == null && node2 == null)) {
 return 0;
}
{code}

This does not always work when there are many datanodes on the same host, as the first node returned from getNodesByAddress() is guarantted to be on the same host as the client, but the list of passed datanodes may not include that datanode instance.

To fix this, we should probably have getDistanceCost() compare hostnames or IP as a second check or instead of the object equality, however this is not trivial to implement.

The reason, is that getDistanceCost() takes Node objects (not DatanodeDetails) and a Node does not have a IP or Hostname field. It does have a getNetworkName method, which should return the hostname, but it is overwritten by the hosts UUID when it registed to the node manager, by this line in NodeManager.register():

datanodeDetails.setNetworkName(datanodeDetails.getUuidString());

 

Note this only affects test clusters where many DNs are on a single host, and it does not cause any failures. The DNs may be returned a less than ideal order."	HDDS	Resolved	3	1	9499	TriagePending
13442030	Use injected clocks in PipelineManagerImp and BackgroundPipelineScrubber to ease testing	"""There are a couple of places in the new scrubber code and the existing scrubber code in PipelineManagerImpl, where it uses {{Time.monotonicNow()}} to decide if the Safemode interval has passed, or if a pipeline has been Closed long enough etc. The unit tests do not correctly test these scenarios, as we just set the time to zero so there is no delay, otherwise the tests would need sleep calls, which will make them slow.

In ReplicationManager, we addressed this problem by injecting a Clock dependency. See {{MonotonicClock}} - if we inject this as a dependency to the scrubber code, then we can inject a {{MonotonicClock}} for runtime, but inject {{TestClock}} for tests. Then you can properly test the safemode delay by advancing the clock between check calls. Same for pipelines - we can check CLOSED ones are not removed before the delay, and then check they are scrubbed after the delay.

In general, we should try to avoid calls to {{Time.monotonicNow()}} across the codebase, and instead inject a clock as a dependency to make the code more testable without sleeps.""

-- Stephen"	HDDS	Resolved	4	4	9499	pull-request-available
13529466	ReplicationManager: Throttle delete container commands from over replication handlers	Similar to ReplicateContainerCommands, we should limit the number of delete commands queued on a given datanode at any time. This PR will enforce the limit with a static config variable with a view to making this more dynamic later.	HDDS	Resolved	3	7	9499	pull-request-available
13530837	ReplicationManager: EC Mis and Under replication handler should handle overloaded exceptions	"In RatisOverReplicationHandler and ECOverReplicationHandler, a container can be over replicated by several replicas, and the deletes are done in two stages:

1. First unhealthy replicas are removed.
2. Then healthy are removed.

While removing any replica, the handler could get a CommandTargetOverloadedException, but rather than throwing that exception immediately, it continues trying other replicas. At the end, if it has not deleted enough replicas, it re-throws the first CommandTargetOverloadedException so the over replication is re-queued on the over replication queue.

Other handlers also have multiple stages, but in the event of an error like CommandTargetOverloadedException, they give up immediately.

RatisOverReplicationHandler works as expected. So does ECOverReplicationHandler.

For RatisUnderReplicationHandler, as the command target is the source, and the RM.sentThrottleReplicationCommand() handles picking the lowest loaded source - it is possible to send one command, and then fail to send the second, but there is no point in retrying as it means all the sources are overloaded. As things stand, it will send what it can and then throw an exception, so that is fine.

For MisReplicationHandler, which is currently shared with EC and Ratis (HDDS-8109 may change this), I believe it could run into this problem with EC, where it may need to make a new copy of 2 EC indexes, and 1 of the nodes is overloaded and the other is not. It would be better to not fail completely if the first is overloaded.

For Ratis Mis Replication, as we can copy any replica after HDDS-8109 it should behave like the RatisUnderReplicationHandler after HDDS-8109.

For ECUnderReplicationHandler, there are multiple stages for processing and potential for partial success.

We should review both ECUnderReplicationHandler and EC MisReplication handling (after HDDS-8109) to handle overloaded exceptions and throw exceptions on partial success."	HDDS	Resolved	3	7	9499	pull-request-available
13399631	Reduce number of mini-clusters needed for decommission tests	There are a few scenarios in the decommission and maintenance integration tests that can be combined into the same test, reducing the number of mini-clusters needed. These changes reduce the number of tests by 3.	HDDS	Resolved	3	4	9499	pull-request-available
13360697	Ozone admin datanode list should report dead and stale nodes	"In ListInfoSubcommand, the logic explicitly only displays HEALTHY nodes:

{code}
  private List<DatanodeWithAttributes> getAllNodes(ScmClient scmClient)
      throws IOException {
    List<HddsProtos.Node> nodes = scmClient.queryNode(null,
        HddsProtos.NodeState.HEALTHY, HddsProtos.QueryScope.CLUSTER, """");
  ...
{code}

I believe we should include stale and dead nodes in the the output too."	HDDS	Resolved	3	4	9499	pull-request-available
13486622	EC: Close pipelines with unregistered nodes	"A datanode is stopped and before the stale node handler is triggered, SCM is restarted. When SCM restarts its loads all the only pipelines and nodes from RocksDB, and then all the nodes will register again.

In the case of EC pipelines, there is nothing to trigger the close of a pipeline (and the containers on it) except:

1. The Container getting full and the DN triggering the close
2. The stale / dead node handlers noticing a node on it has gone dead.

In the case above, the EC pipeline will sit forever in an Open state, but any attempt to write to it will likely result in errors on the client due to one of the nodes not being available. These errors still will not trigger it to close.

A solution to this problem, is to add logic to the pipeline scrubber to close any pipelines that have unregistered nodes. Stale / Dead nodes should be handled by the existing stale / dead node handlers."	HDDS	Resolved	3	7	9499	pull-request-available
13536856	ReplicationManager: Change default command timeout to 10 minutes	"In Replication Manager, a deadline is set on commands sent to a datanode. If the command has not completed within the timeout, RM assumes it is lost and will schedule a new command to another random node.

Right now the default is set to 30 minutes as the legacy RM scheduled a lot of work onto the DNs and it could take a long time to complete. The new RM throttles the work sent, so a large queue on the DNs should not be possible.

We should change the default event timeout to 10 minutes instead of 30."	HDDS	Resolved	3	7	9499	pull-request-available
13320904	Non rack aware pipelines should not be created if multiple racks are alive	"If we have a scenario where one rack has more nodes that others, it is possible for all hosts in the cluster to have reached their pipeline limit, while 3 nodes on the larger rack have not. 

The current fallback logic will then allow a pipeline to be created which uses only the 3 nodes on the same rack, violating the rack placement policy.

There may be other ways this could happen with cluster load too, were the pipeline capacity has reached its limit on some nodes but not others.

The proposal here, is that if the cluster has multiple racks AND there are healthy nodes covering at least 2 racks, where healthy is defined as a node which is registered and not stale or dead, then we should not allow ""fallback"" (pipelines which span only 1 rack) pipelines to be created.

This means if you have a badly configured cluster - eg Rack 1 = 10 nodes; Rack 2 = 1 node, the pipeline limit will be constrained by the capacity of that 1 node on rack 2. Even a setup like Rack 1 = 10 nodes, Rack 2 = 5 would be constrained by this.

This constraint is better than creating non rack aware pipelines, and the rule above will handle the case when the cluster degrades to 1 rack, as the healthy node definition will notice only 1 rack is alive."	HDDS	Resolved	3	4	9499	pull-request-available
13534432	ReplicationManager: Pass used and excluded node separately for Under and Mis-Replication	"With HDDS-7226 merged, the RackAwarePlacementPolicy now supports passing used nodes and excluded nodes separately. This allows it to select the racks correctly when there are one or two used nodes.

We need to change the Ratis under and mis-replication handlers to pass the used nodes and excluded nodes as two separate lists now."	HDDS	Resolved	3	7	9499	pull-request-available
13551254	Remove duplicate containers when loading volumes on a datanode	"Prior to HDDS-5032, if the same container is found on multiple volumes, then the second volume to load it fails the entire volume.

After HDDS-5032, the exception is caught so the volume doesn't fail, but depending on which container is loaded faster, either one of the replicas could win. Over several restarts, the container on either volume could be the one loaded, potentially resulting in inconsistencies.

This change catches the error, and then removes one of the duplicates based on the BCSID. The container with the largest BCSID is the one kept, while the other is removed.

This will free the disk space taken by the duplicate container, and also avoid the chance of a different copy being loaded on each restart."	HDDS	Resolved	3	1	9499	pull-request-available
13500747	EC: Notify ReplicationManager when a heartbeat updates datanode command counts	"When a datanode heartbeat is processed, it updates the queued command counts on the datanodes. Replication is going to use this information when assigning work to datanodes, and if a datanode exceeds the load limit it will be excluded until it has more capacity.

To allow RM to remove nodes from any exclude lists, we should fire a SCM event when the counts are updated, as this will avoid RM polling frequently.

For now, the notification in RM does not do anything - we will add code which uses it in future changes as we build out the load aware under / over replication processing."	HDDS	Resolved	3	7	9499	pull-request-available
13330341	Remove no longer needed class DatanodeAdminNodeDetails	"DatanodeAdminNodeDetails was added earlier in the decommission branch, to track metrics and, the decommission state and maintenance end time. 

After enhancing NodeStatus to old the Maintenance Expiry time, this class is no longer needed and it also duplicates information which is stored in other existing places.

This change removes it and then metrics etc can be added later in a different way."	HDDS	Resolved	3	7	9499	pull-request-available
13221390	Fix error propagation for SCM protocol	"HDDS-1068 fixed the error propagation between the OM client and OM server.

By default the Server.java transforms all the IOExceptions to one string (message + stack trace) and this is returned to the client.

But for business exception (eg. volume not found, chill mode is active, etc.) this is not what we need.

In the OM side we fixed this behaviour. In the ServerSideTranslator classes we catch (server) the business (OMException) exceptions and serialize them to the response object.

The exception (and the status code) is stored in message/status field of the OMResponse (hadoop-ozone/common/src/main/proto/OzoneManagerProtocol.proto)

Here I propose to do the same for the ScmBlockLocationProtocol.proto.

Unfortunately there is no common parent object (like OMRequest) in this protocol, but we can easily add one as only the Serverside/Clientside translator should be changed for that. "	HDDS	Resolved	2	4	9499	pull-request-available
13514763	EC: ReplicationManager - UnderRep maintenance handler should not request nodes if none needed	If an EC container is under-replicated somehow, and also has some maintenance indexes that do not need replicated, the logic calls the placement policy requesting zero nodes. The policy then throws an exception as it expects to have greater than zero nodes requested.	HDDS	Resolved	3	1	9499	pull-request-available
13426223	EC: Add replica index to the output in the container info command	HDDS-6177 added replica details to the container info command on the master branch. On the EC branch, we need to extend it to add the new replica index field.	HDDS	Resolved	3	7	9499	pull-request-available
13576050	Do not fail read of EC block if the last chunk is empty	"Due to HDDS-10682 some EC blocks in a cluster could have an empty final chunk. These blocks will fail to read and could cause data to become unavailable, even though it is still present on disk.

If the last chunk is empty, this should not stop the block from being empty."	HDDS	Resolved	3	1	9499	pull-request-available
13382535	ContainerInfo should use ReplicationConfig	"We introduced ReplicationConfig to most classes already, but we missed ContainerInfo.

This change will ensure that ContainerInfo uses ReplicationConfig rather than the legacy Type and Factor fields."	HDDS	Resolved	3	7	9499	pull-request-available
13519772	EC: GetChecksum for EC files can fail intermittently with IndexOutOfBounds exception	"When calculating a checksum for an EC file with Rack Topology enabled, you can get the following error intermittently:

{code}
ERROR : Failed with exception null
  java.lang.IndexOutOfBoundsException
        at java.nio.ByteBuffer.wrap(ByteBuffer.java:375)
        at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.computeCompositeCrc(ECBlockChecksumComputer.java:163)
        at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.compute(ECBlockChecksumComputer.java:65)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.getBlockChecksumFromChunkChecksums(ECFileChecksumHelper.java:148)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlock(ECFileChecksumHelper.java:106)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlocks(ECFileChecksumHelper.java:73)
        at org.apache.hadoop.ozone.client.checksum.BaseFileChecksumHelper.compute(BaseFileChecksumHelper.java:220)
        at org.apache.hadoop.fs.ozone.OzoneClientUtils.getFileChecksumWithCombineMode(OzoneClientUtils.java:223)
        at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileChecksum(BasicRootedOzoneClientAdapterImpl.java:1123)
        at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileChecksum(BasicRootedOzoneFileSystem.java:955)
        at org.apache.hadoop.fs.FileSystem.getFileChecksum(FileSystem.java:2831)
        at org.apache.hadoop.hive.ql.metadata.Hive.addInsertNonDirectoryInformation(Hive.java:3659)
        at org.apache.hadoop.hive.ql.metadata.Hive.addInsertFileInformation(Hive.java:3632)
        at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3578)
        at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3563)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:3224)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:418)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:769)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:504)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:498)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:88)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:327)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:345)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
ERROR : FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.MoveTask. java.lang.IndexOutOfBoundsException
        at java.nio.ByteBuffer.wrap(ByteBuffer.java:375)
        at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.computeCompositeCrc(ECBlockChecksumComputer.java:163)
        at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.compute(ECBlockChecksumComputer.java:65)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.getBlockChecksumFromChunkChecksums(ECFileChecksumHelper.java:148)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlock(ECFileChecksumHelper.java:106)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlocks(ECFileChecksumHelper.java:73)
        at org.apache.hadoop.ozone.client.checksum.BaseFileChecksumHelper.compute(BaseFileChecksumHelper.java:220)
        at org.apache.hadoop.fs.ozone.OzoneClientUtils.getFileChecksumWithCombineMode(OzoneClientUtils.java:223)
        at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileChecksum(BasicRootedOzoneClientAdapterImpl.java:1123)
        at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileChecksum(BasicRootedOzoneFileSystem.java:955)
        at org.apache.hadoop.fs.FileSystem.getFileChecksum(FileSystem.java:2831)
        at org.apache.hadoop.hive.ql.metadata.Hive.addInsertNonDirectoryInformation(Hive.java:3659)
        at org.apache.hadoop.hive.ql.metadata.Hive.addInsertFileInformation(Hive.java:3632)
        at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3578)
        at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3563)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:3224)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:418)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:769)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:504)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:498)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:88)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:327)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:345)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
INFO  : Completed executing command(queryId=hive_20221214035652_bc45477d-98df-408e-b945-a63b4ac6896a); Time taken: 22.167 seconds
  INFO  : OK
  Error: Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.MoveTask. java.lang.IndexOutOfBoundsException
        at java.nio.ByteBuffer.wrap(ByteBuffer.java:375)
        at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.computeCompositeCrc(ECBlockChecksumComputer.java:163)
        at org.apache.hadoop.ozone.client.checksum.ECBlockChecksumComputer.compute(ECBlockChecksumComputer.java:65)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.getBlockChecksumFromChunkChecksums(ECFileChecksumHelper.java:148)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlock(ECFileChecksumHelper.java:106)
        at org.apache.hadoop.ozone.client.checksum.ECFileChecksumHelper.checksumBlocks(ECFileChecksumHelper.java:73)
        at org.apache.hadoop.ozone.client.checksum.BaseFileChecksumHelper.compute(BaseFileChecksumHelper.java:220)
        at org.apache.hadoop.fs.ozone.OzoneClientUtils.getFileChecksumWithCombineMode(OzoneClientUtils.java:223)
        at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileChecksum(BasicRootedOzoneClientAdapterImpl.java:1123)
        at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileChecksum(BasicRootedOzoneFileSystem.java:955)
        at org.apache.hadoop.fs.FileSystem.getFileChecksum(FileSystem.java:2831)
        at org.apache.hadoop.hive.ql.metadata.Hive.addInsertNonDirectoryInformation(Hive.java:3659)
        at org.apache.hadoop.hive.ql.metadata.Hive.addInsertFileInformation(Hive.java:3632)
        at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3578)
        at org.apache.hadoop.hive.ql.metadata.Hive.addWriteNotificationLog(Hive.java:3563)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:3224)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:418)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:769)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:504)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:498)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:88)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:327)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:345)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748) (state=08S01,code=40000){noformat}
{code}

This is because the wrong nodes are used to obtain the stripe checksum sometimes as the node does not correctly use the replicaIndex in the pipeline to order the nodes."	HDDS	Resolved	3	7	9499	pull-request-available
13441195	Revert HDDS-6579	"HDDS-6579 added a flag to hide newly added information in the `ozone admin container info` command for compatibility reasons. However the flag is not necessary, as the changes are not incompatible. It is also not desirable to have a new command flag for every piece of new information that is to be added to every command. It will result in commands with many flags that have to be passed all the time to get the full set of information.

There is a community discussion about compatibility where the consensus is that we provide compatibility for these admin commands via the JSON output, and that the formatting and output of the ""human readable"" formats can change over time:

https://lists.apache.org/thread/5gqnbstv1pznwmcvx7txspj1qrksy7gl

For those reasons I am reverting HDDS-6579."	HDDS	Resolved	3	4	9499	pull-request-available
13528478	Replication Manager: Make all handlers send commands immediately instead of returning commands	"To allow better throttling control all the unhealthy handlers should send the command directly using the RM API, rather than gathering up a list of commands and returning them.

This change involves a change to the UnhealthyReplicationHandler interface, which previously returned the commands, but now returns just the count of commands sent instead."	HDDS	Resolved	3	7	9499	pull-request-available
13515116	EC metrics related to replication commands don't add up	"{code}
    ""EcReplicationCmdsSentTotal"" : 0,
    ""EcDeletionCmdsSentTotal"" : 259,
    ""EcReplicationCmdsCompletedTotal"" : 51,
    ""EcDeletionCmdsCompletedTotal"" : 51,
    ""EcReconstructionCmdsSentTotal"" : 571,
    ""EcReplicationCmdsTimeoutTotal"" : 765,
    ""EcDeletionCmdsTimeoutTotal"" : 204
{code}

Total replication commands sent are 0, while timed out are 765."	HDDS	Resolved	3	7	9499	pull-request-available
13365631	Decommission CLI should return details of nodes which fail	"With the current decommission / recommission / maintenance mode commands, you can pass a list of hosts to perform the operation on. If any of these hosts fail to enter the decommission / maintenance workflow, the command gives no feedback about the error. Some of the hosts can silently fail and the only way to know is to inspect the SCM log.

The most common way a host can fail, is if a node which is undergoing maintenance is instructed to go to decommission and vice versa as this is a transition which is not allowed.

This change will allow any failed nodes to feed back to the client. If the client detects that any of the nodes have failed, details will be written to stderr and the command exit code will be non-zero.

Note that even though the exit code is non-zero, the command may have partially worked.

Also note that the errors which are fed back are only around transitioning the node into the admin workflow - it is still possible for it to fail later for other reasons which will not be fed back to the client. This is because the client does not wait for the process to complete, but exits after confirmation the command has been processed by scm."	HDDS	Resolved	3	4	9499	pull-request-available
13396122	Speed up decommission tests using a background Mini Cluster provider	"The integration (ozone) test suit is the slowest part of the github actions build, taking over 2 hours usually. In a random PR I checked, 2hr16.

Often in integration tests, a large part of the test time is spent creating a new mini-Ozone cluster for each test, which can take 10 - 20 seconds to startup.

I also timed stopping a mini-cluster and found that can take up to 10 seconds.

Changing the tests to reuse the same cluster can be difficult and make the tests less standalone and more brittle, which is not a good thing. Changing the tests is also time consuming work.

Assuming a test runs for longer than the time taken to setup a mini-cluster and stop it, it would make the tests faster if we pre-created a mini-cluster in the background. Then when one test completes, the next cluster is already there, saving the startup time. Obviously this costs more concurrent cpu to reduce the wall clock time.

We could also queue the shutdown of the clusters in another background thread.

The slowest part of the Integration (Ozone) test suit are the decommission tests, taking 843 seconds on the last run I checked.

This PR adds a Mini-Cluster provider to the Decommission tests as an experiment to see if it makes the runtime significantly faster in practice. If it does, this may be something we can roll out across other integration tests.

As a baseline, I ran the decommission tests on my laptop, and it took 8min 37s.

After the changes in this PR, the test suit ran in 3min 53s.
"	HDDS	Resolved	3	4	9499	pull-request-available
13428338	Remove replicas from ContainerStateMap when a container is deleted	"In ContainerStateMap, there are several maps to hold various details, eg:
  private final Map<ContainerID, ContainerInfo> containerMap;
  private final Map<ContainerID, Set<ContainerReplica>> replicaMap;
When we add a new container, we add an entry to both of these sets. When a container is removed, we don’t see to remove from replicaMap (see below). There doesn’t seem to be any way to remove the replicas later once the containerMap entry is gone, so removing the container is leaking the replicas.
 
{code:java}
  public void removeContainer(final ContainerID id) {
    Preconditions.checkNotNull(id, ""ContainerID cannot be null"");
    if (contains(id)) {
      // Should we revert back to the original state if any of the below
      // remove operation fails?
      final ContainerInfo info = containerMap.remove(id);
      lifeCycleStateMap.remove(info.getState(), id);
      ownerMap.remove(info.getOwner(), id);
      repConfigMap.remove(info.getReplicationConfig(), id);
      typeMap.remove(info.getReplicationType(), id);
      // Flush the cache of this container type.
      flushCache(info);
      LOG.trace(""Container {} removed from ContainerStateMap."", id);
    }
  } {code}
You cannot remove the replicas anyway later, as the methods check if the container exists first, which it no longer will, eg:


{code:java}
public void removeContainerReplica(final ContainerID containerID,
    final ContainerReplica replica) {
  Preconditions.checkNotNull(containerID);
  Preconditions.checkNotNull(replica);
  if (contains(containerID)) {
    replicaMap.get(containerID).remove(replica);
  }
} {code}
Note that deleting a container seems to be a rare operation (eg delete it manually from the CLI). Empty containers are currently marked as deleted, but as far as I can tell, they are not actually removed from SCM."	HDDS	Resolved	3	1	9499	pull-request-available
13335011	ContainerInfo does not persist BCSID leading to failed replicas reports	"If you create a container, and then close it, the BCSID is synced on the datanodes and then the value is updated in SCM via setting the ""sequenceID"" field on the containerInfo object for the container.

If you later restart just SCM, the sequenceID becomes zero, and then container reports for the replica fail with a stack trace like:

{code}
Exception in thread ""EventQueue-ContainerReportForContainerReportHandler"" java.lang.AssertionError
	at org.apache.hadoop.hdds.scm.container.ContainerInfo.updateSequenceId(ContainerInfo.java:176)
	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerStats(AbstractContainerReportHandler.java:108)
	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:83)
	at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:162)
	at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:130)
	at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)
	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

The assertion here is failing, as it does not allow for the sequenceID to be changed on a CLOSED container:

{code}
  public void updateSequenceId(long sequenceID) {
    assert (isOpen() || state == HddsProtos.LifeCycleState.QUASI_CLOSED);
    sequenceId = max(sequenceID, sequenceId);
  }
{code}

The issue seems to be caused by the serialisation and deserialisation of the containerInfo object to protobuf, as sequenceId never persisted or restored.

However, I am also confused about how this ever worked, as this is a pretty significant problem.

"	HDDS	Resolved	3	1	9499	pull-request-available
13437926	Have the datanode heartbeat include queued command counts	"To allow SCM to make better decisions about scheduling commands on datanodes, the datanodes should report their current command queue size in the heartbeat. This change adds a section to the heartbeat protobuf message to allow the queued command count for each command type to be reported in each heartbeat.

There will be a count reported for each registered command, with a zero count if the queue is empty.

A followup change will be needed to use this information on SCM. With this Jira in place, the data will be sent but never used on SCM."	HDDS	Resolved	3	4	9499	pull-request-available
13521116	EC: Refactor ReplicationSupervisor to allow Replication and Reconstruction tasks	"A refactor or the existing ReplicationSupervisor so we can have the same supervisor (and threadpool) manage both Replication and EC Reconstruction tasks.

This change does not include moving the EC tasks into the ReplicationSupervisor - that will be done in another PR as this one is already large enough with this change."	HDDS	Resolved	3	7	9499	pull-request-available
13323585	Container report should update container key count and bytes used if they differ in SCM	"In HDDS-4037 it was noted that when blocks are deleted from closed containers, the bytesUsed and Key Count metrics on the SCM container are not updated correctly.

These stats should be updated via the container reports issued by the DNs to SCM periodically. However, in `AbstractContainerReportHandler#updateContainerStats`, the code assumes the values are always increasing and it will not update them if they are decreasing:

{code}
  private void updateContainerStats(final ContainerID containerId,
                                    final ContainerReplicaProto replicaProto)
      throws ContainerNotFoundException {
    if (isHealthy(replicaProto::getState)) {
      final ContainerInfo containerInfo = containerManager
          .getContainer(containerId);

      if (containerInfo.getSequenceId() <
          replicaProto.getBlockCommitSequenceId()) {
        containerInfo.updateSequenceId(
            replicaProto.getBlockCommitSequenceId());
      }
      if (containerInfo.getUsedBytes() < replicaProto.getUsed()) {
        containerInfo.setUsedBytes(replicaProto.getUsed());
      }
      if (containerInfo.getNumberOfKeys() < replicaProto.getKeyCount()) {
        containerInfo.setNumberOfKeys(replicaProto.getKeyCount());
      }
    }
  }
{code}

In HDDS-4037 a change was made to the Replication Manager, so it updates the stats. However I don't believe that is the correct place to perform this check, and the issue is caused by the logic shared above.

In this Jira, I have removed the changes to Replication Manager in HDDS-4037 (but retained the other changes), ensuring the problem statistics are only updated via the containers reports if they are different in SCM from what is reported."	HDDS	Resolved	3	4	9499	pull-request-available
13400807	EC: Resolve findbugs warnings after branch merge	After merging master into the EC branch, there are findbugs warnings on the branch.	HDDS	Resolved	3	7	9499	pull-request-available
13267080	ContainerReplica should contain DatanodeInfo rather than DatanodeDetails	"The ContainerReplica object is used by the SCM to track containers reported by the datanodes. The current fields stored in ContainerReplica are:

{code}
final private ContainerID containerID;
final private ContainerReplicaProto.State state;
final private DatanodeDetails datanodeDetails;
final private UUID placeOfBirth;
{code}

Now we have introduced decommission and maintenance mode, the replication manager (and potentially other parts of the code) need to know the status of the replica in terms of IN_SERVICE, DECOMMISSIONING, DECOMMISSIONED etc to make replication decisions.

The DatanodeDetails object does not carry this information, however the DatanodeInfo object extends DatanodeDetails and does carry the required information.

As DatanodeInfo extends DatanodeDetails, any place which needs a DatanodeDetails can accept a DatanodeInfo instead.

In this Jira I propose we change the DatanodeDetails stored in ContainerReplica to DatanodeInfo."	HDDS	Resolved	3	7	9499	pull-request-available
13541424	Allow EC PipelineChoosingPolicy to be defined separately from Ratis	"Cluster may have set the PipelineChoosingPolicy to the HealthyPipelineChoosePolicy for Ratis, but it adds overhead and is not necessary for EC, as the EC pipeline is always healthy:

{code}
  public boolean isHealthy() {
    // EC pipelines are not reported by the DN and do not have a leader. If a
    // node goes stale or dead, EC pipelines will by closed like RATIS pipelines
    // but at the current time there are not other health metrics for EC.
    if (replicationConfig.getReplicationType() == ReplicationType.EC) {
      return true;
    }
    ...
{code}

To allow for flexibility, add a new config for the ECPipelineChoosingPolicy so it can be different from the Ratis policy. "	HDDS	Resolved	3	7	9499	pull-request-available
13330591	Close Container event can fail if pipeline is removed	"If you call `pipelineManager.finalizeAndDestroyPipeline()` with onTimeout=false, then the finalizePipeline call will result in a closeContainer event to be fired for every container on the pipeline. These are handled asynchronously.

However, immediately after that, the `destroyPipeline(...)` call is made. This will remove the pipeline details from the various maps / stores.

Then the closeContainer events get processed, and they attempt to remove the container from the pipeline. However as the pipeline has already been destroyed, this throws an exception and the close container events never get sent to the DNs:

{code}
2020-10-01 15:44:18,838 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2
2020-10-01 15:44:18,842 [EventQueue-CloseContainerForCloseContainerEventHandler] ERROR container.CloseContainerEventHandler: Failed to close the container #2.
org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=59e5ae16-f1fe-45ff-9044-dd237b0e91c6 not found
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:372)
	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:111)
	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:413)
	at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:352)
	at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerState(SCMContainerManager.java:331)
	at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.onMessage(CloseContainerEventHandler.java:66)
	at org.apache.hadoop.hdds.scm.container.CloseContainerEventHandler.Onmessage(CloseContainerEventHandler.java:45)
	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor
{code}

The simple solution is to catch the exception and ignore it."	HDDS	Resolved	3	1	9499	pull-request-available
13480285	testContainerIsReplicatedWhenAllNodesGotoMaintenance is failing frequently	Since changes in HDDS-6975 the test testContainerIsReplicatedWhenAllNodesGotoMaintenance is failing frequently. This Jira is to fix the issues with that test.	HDDS	Resolved	3	1	9499	pull-request-available
13441448	datanode usageinfo CLI should provide JSON output option	It is good to have json output option so that it is easy parse the output when additonal information added to this command cli	HDDS	Resolved	3	7	9499	pull-request-available
13472317	EC: ReplicationManager - skip processing open containers	When processing containers, we should skip the open containers until they are closed.	HDDS	Resolved	3	7	9499	pull-request-available
13443409	EC: ReplicationManager - create version of ContainerReplicaCounts applicable to EC	"Currently, for Ratis containers a ContainerReplicaCounts object is used to calculate whether a container is over or under replicated, taking into account decommissioning, maintenance mode and inflight replicas.

We need a new version of this class to do something similar for EC, but it will need to return specific replicas with issues, rather than a simple delta which is returned by Ratis Containers."	HDDS	Resolved	3	7	9499	pull-request-available
13378098	EC: ReplicaIndex in Pipeline should be serialized and deserialized in the protobuf message	ReplicaIndex and ReplicationConfig were added to the Pipeline class, but we missed added ReplicaIndex to the serialized message. This Jira will add it in.	HDDS	Resolved	3	7	9499	pull-request-available
13444811	EC: ReplicationManager - create ContainerReplicaPendingOps class and integrate with ContainerManager	"The legacy replication manager internally keeps a list of all pending replications and deletes. Each time a container is checked, it check this list and removes any replications that have been completed or expired. Then it gets the list of remaining pending operations to help decide if container is healthy or not.

Rather than the ReplicationManager removing the completed and expired replications, we could have a standalone PendingContainerOps monitor, that works as follows:

1. Replication Manager adds pending replications and deletes to it.
2. Replication Manager queries it for anything pending for the current container and gets a list of PendingActions back.
3. The PendingReplicationMonitor has its own internal thread that checks for expired replications and removes them.
4. Completed replications and deletes are removed in ComtainerManagerImpl, which has add and removeContainer triggered via the container reports (ICR and FCR) from the datanodes as they are replicated.

This way, the ReplicationManager does not need to worry about expiring replications or removing completed entries. We also get the ability to have a more up-to-date view of the system, as the ICR / FCRs will keep the pending table up-to-date in real time, rather than having to wait for the container to be re-check inside replication manager.

We can have a fairly simple ""ContainerReplicaPendingOps"" class that is basically standalone and inject it into ReplicationManager and ContainerManagerImpl. This would allow for removing some complexity from RM and let the expiry and completion be tested in an isolated way."	HDDS	Resolved	3	7	9499	pull-request-available
13363315	EC: Implement ECBlockInputStream to read a single EC Block Group.	"Implement the happy-path read scenario, which does not support ""degraded read"" (on the fly EC recovery)."	HDDS	Resolved	3	7	9499	pull-request-available
13341627	Open RocksDB read only when loading containers	"When a datanode is started, it must read some metadata from all the Containers. Part of that metadata is stored in RocksDB, so the startup process involves opening each rocksDB and closing it again.

Testing on a dense node, with 45 high performance spinning disks and 200K containers, I saw about 75ms on average to open each RockDB. Further testing demonstrated that if we open RockDB read only, the average open time is about 35ms.

At startup time, the DBs are only read and never written, so opening read only is fine. HDDS-4427 already ensures these opened DBs are not cached."	HDDS	Resolved	3	4	9499	pull-request-available
13575517	EC Reconstruction does not issue put block to data index if it is unused	"Given a small EC block:

* <= 2MB for EC-3-2
* <= 5MB for EC-6-3
* <= 9MB for EC-10-4

So that it is less than a full stripe and does not use all the datanodes.

When reconstruction happens to replace a replica which is not used in the stripe, the unused containers are not issued with the put block to store the details of the empty block within the container. Note that the container replica will likely have other blocks, so it will still get reconstructed, but it will not be given a reference to this empty block.

All containers are checked for the presence of all blocks during reconstruction. If any of the containers do not have a reference to the block, it is considered an orphan block / abandoned stripe and will not be reconstructed.

Therefore if one replica has no entry for the block, then it is used it another reconstruction for another replica later, that block will not get reconstructed into a second replica. Over time this can result in the reference getting removed from all copies."	HDDS	Resolved	3	1	9499	pull-request-available
13539414	ReplicationManager: Add metric to count how often replication is throttled	"Adding a metric for the number of containers where we fail to send a delete, replication or reconstruction due to the throttling. This will give some visibility into how often throttling is occurring.

New metrics are:

ecReconstructionCmdsDeferredTotal
deleteContainerCmdsDeferredTotal
replicateContainerCmdsDeferredTotal"	HDDS	Resolved	3	7	9499	pull-request-available
13390559	Replication Manager should process containers synchronously for tests	"The method ReplicationManager.processContainersNow() only wakes up the thread, and returns before the containers have been processed.

This results in all RM tests having a sleep(100) after all calls to this method.

With a small refactor to RM, we can avoid this sleep. After this change all tests run about 100ms faster, and the code in the tests is slightly better."	HDDS	Resolved	3	4	9499	pull-request-available
13376016	EC: Allow EC blocks to be requests from OM	We need to change the allocateBlock calls from OM to SCM so the EC Replication Config is passed through the stack.	HDDS	Resolved	3	7	9499	pull-request-available
13515130	MisReplicationHandler does not consider QUASI_CLOSED replicas as sources	"MisReplicationHandler#filterSources gets a Set of replicas that can be used to fix mis replication. It selects CLOSED replicas:
{code}
  private Set<ContainerReplica> filterSources(Set<ContainerReplica> replicas) {
    return replicas.stream().filter(r -> r
                    .getState() == StorageContainerDatanodeProtocolProtos
                    .ContainerReplicaProto.State.CLOSED)
            .filter(r -> ReplicationManager
                    .getNodeStatus(r.getDatanodeDetails(), nodeManager)
                    .isHealthy())
            .filter(r -> r.getDatanodeDetails().getPersistedOpState()
                    == HddsProtos.NodeOperationalState.IN_SERVICE)
            .collect(Collectors.toSet());
  }
{code}

When thinking about Ratis Containers, QUASI_CLOSED replicas can also be mis replicated. They're also allowed to be replicated to datanodes. Should they also be considered as sources here?"	HDDS	Resolved	3	7	9499	pull-request-available
13580698	Enable Atomic Rewrite in FSO buckets	Extend the atomic commit support to FSO buckets. This will follow the same pattern for OBS buckets, ensuring that a key exists at the given path, and still existings upon commit with the same expected generation.	HDDS	Resolved	3	7	9499	pull-request-available
13357517	Add Genesis benchmark for various CRC implementations	As highlighted in HDDS-4138 Ozone appears to have a greater CRC overhead than Hadoop. In order to figure out where the problem is, we should add a benchmark to Genesis to test if one implementation is much better than the others.	HDDS	Resolved	3	2	9499	pull-request-available
