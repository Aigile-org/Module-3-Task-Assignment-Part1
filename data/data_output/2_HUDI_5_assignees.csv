id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13520131	Do not let users override internal metadata configs	"metadata cleaning, compaction configs in metadata table should not be overriden by user. we should make it internal configs. 

hoodie.metadata.clean.async
hoodie.metadata.cleaner.commits.retained
hoodie.metadata.enable.full.scan.log.files
hoodie.metadata.insert.parallelism
hoodie.metadata.keep.max.commits
hoodie.metadata.keep.min.commits
hoodie.metadata.populate.meta.fields"	HUDI	Closed	2	4	646	pull-request-available
13524702	Handle empty payloads for AbstractDebeziumAvroPayload	HoodieMergeHandle is deciding we no longer need to copy the old record using {{hoodieRecord.getData().combineAndGetUpdateValue}} and this was not being handled properly in {{AbstractDebeziumAvroPayload}}	HUDI	Closed	3	4	646	pull-request-available
13587583	Partition query for transformed value incorrectly prunes valid partitions	"With timestamp keygen you can have a partition column with timestamps, but then use the keygen so it will create partitions based on days so that all records that have a timestamp on 7-31-2024 will go to the same parititon even though the values in the partition column differ by hours and minutes etc.

This causes a problem with partition pruning. lets say you query ""select * from table where partition < 7-31-2024 at 7am and partition > 7-31-2024 at 6am "". Since the file structure has the partition of just 7-31-2024, that will be interpreted as 7-31-2024 at 12am. So the partition will be pruned from the search space."	HUDI	In Progress	2	1	646	pull-request-available
13540263	Spark CDC delete all records in a filegroup display fails	"If you delete the last record in a filegroup and do a cdc query it fails assert 

currentCDCFileSplit.getBeforeFileSlice.isPresent. "	HUDI	Closed	3	1	646	pull-request-available
13591080	Rename HoodieFilegroupReader to just fileslice reader	"It only reads file slices and we don't need ""hoodie"" because it is a hudi concept. Later on, we will create fg reader that wraps file slice reader and cdc reader"	HUDI	Closed	3	4	646	pull-request-available
13479118	Add table config change validation in deltastreamer	"looks like we are missing proper table config validation in deltastreamer. 
{code:java}
   if (fs.exists(new Path(cfg.targetBasePath))) {
        HoodieTableMetaClient meta =
            HoodieTableMetaClient.builder().setConf(new Configuration(fs.getConf())).setBasePath(cfg.targetBasePath).setLoadActiveTimelineOnLoad(false).build();
        tableType = meta.getTableType();
        // This will guarantee there is no surprise with table type
        ValidationUtils.checkArgument(tableType.equals(HoodieTableType.valueOf(cfg.tableType)),
            ""Hoodie table is of type "" + tableType + "" but passed in CLI argument is "" + cfg.tableType);

        // Load base file format
        // This will guarantee there is no surprise with base file type
        String baseFileFormat = meta.getTableConfig().getBaseFileFormat().toString();
        ValidationUtils.checkArgument(baseFileFormat.equals(cfg.baseFileFormat) || cfg.baseFileFormat == null,
            ""Hoodie table's base file format is of type "" + baseFileFormat + "" but passed in CLI argument is ""
                + cfg.baseFileFormat);
        cfg.baseFileFormat = baseFileFormat;
        this.cfg.baseFileFormat = baseFileFormat;
      } else {
        tableType = HoodieTableType.valueOf(cfg.tableType);
        if (cfg.baseFileFormat == null) {
          cfg.baseFileFormat = ""PARQUET""; // default for backward compatibility
        }
      } {code}"	HUDI	Closed	3	4	646	hudi-on-call, pull-request-available
13550763	Update SQL Pages for 0.14.0	update [https://hudi.apache.org/docs/table_management] and https://hudi.apache.org/docs/procedures	HUDI	Closed	3	4	646	pull-request-available
13507336	With multiple meta syncs, one meta sync failure should not impact other meta syncs.	For example, if you are using HMS and glue, if HMS sync fails, we should still sync with glue.	HUDI	Closed	2	4	646	pull-request-available
13563263	Fallback to key-based merging if there is no positions in log header	When turning on merging with record positions, if there is no position header in the log blocks, the reader should fall back to key-based merging.	HUDI	Closed	1	4	646	pull-request-available
13482502	Add validation to block COW table to use consistent hashing bucket index	"Consistent hashing bucket index's resizing relies on the log feature of MOR table. So with COW table, the consistent hashing bucket index can not achieve resizing currently. 

We should block the user from using it at the very beginning(i.e., table creation), and suggest them to use MOR table or Simple Bucket Index. "	HUDI	Closed	3	4	646	pull-request-available
13575634	Spark cannot write the hudi table containing array type created by flink	"When flink creates a Hudi table containing an array field, the elements of the default array field cannot be nullable. When using Spark SQL to read data from the Hive table to the Hudi table, a field verification exception will occur.
{code:java}
2024-03-27 12:47:51 INFO org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'decentral_level1
2024-03-27 12:47:51 INFO at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:138)
2024-03-27 12:47:51 INFO at org.apache.spark.sql.types.StructType$.$anonfun$fromAttributes$1(StructType.scala:549)
2024-03-27 12:47:51 INFO at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
2024-03-27 12:47:51 INFO at scala.collection.immutable.List.foreach(List.scala:392)
2024-03-27 12:47:51 INFO at scala.collection.TraversableLike.map(TraversableLike.scala:238)
2024-03-27 12:47:51 INFO at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
2024-03-27 12:47:51 INFO at scala.collection.immutable.List.map(List.scala:298)
2024-03-27 12:47:51 INFO at org.apache.spark.sql.types.StructType$.fromAttributes(StructType.scala:549)
2024-03-27 12:47:51 INFO at org.apache.spark.sql.catalyst.plans.QueryPlan.schema$lzycompute(QueryPlan.scala:281)
2024-03-27 12:47:51 INFO at org.apache.spark.sql.catalyst.plans.QueryPlan.schema(QueryPlan.scala:281)
2024-03-27 12:47:51 INFO at org.apache.spark.sql.hudi.command.InsertIntoHoodieTableCommand$.alignQueryOutput(InsertIntoHoodieTableCommand.scala:153)
2024-03-27 12:47:51 INFO at org.apache.spark.sql.hudi.command.InsertIntoHoodieTableCommand$.run(InsertIntoHoodieTableCommand.scala:105)
2024-03-27 12:47:51 INFO at org.apache.spark.sql.hudi.command.InsertIntoHoodieTableCommand.run(InsertIntoHoodieTableCommand.scala:60) {code}"	HUDI	Closed	2	1	646	pull-request-available
13562519	Ensure ClosableIterator is propagated all the way to FileScanRDD	CI tests are OOMing. One cause is that resources are not being freed from the new filegroup reader. After some code inspection, it was found that close is not being called in the HoodieFileGroupReaderIterator	HUDI	Closed	2	1	646	pull-request-available
13561462	Delete NewHoodieParquetFileFormat and all references	HoodieFileGroupReaderBasedParquetFileFormat now has feature parity with NewHoodieParquetFileFormat and no new work will be done on NewHoodieParquetFileFormat. 	HUDI	Closed	3	4	646	pull-request-available
13483531	Add a config to allow partition column type inference in bootstrap	"Currently, we assume that the partition column is always in String type during bootstrap operation.  TestDataSourceForBootstrap.testMetadataBootstrapCOWHiveStylePartitioned fails for date partition column if the type inference of partition column is turned on.

 

We need to add a config to allow partition column inference in bootstrap so that other types of partition columns are supported.

 

HoodieSparkBootstrapSchemaProvider
{code:java}
private static Schema getBootstrapSourceSchemaParquet(HoodieWriteConfig writeConfig, HoodieEngineContext context, Path filePath) {
  // NOTE: The type inference of partition column in the parquet table is turned off explicitly,
  // to be consistent with the existing bootstrap behavior, where the partition column is String
  // typed in Hudi table.
  ((HoodieSparkEngineContext) context).getSqlContext()
      .setConf(SQLConf.PARTITION_COLUMN_TYPE_INFERENCE(), false);
  StructType parquetSchema = ((HoodieSparkEngineContext) context).getSqlContext().read()
      .option(""basePath"", writeConfig.getBootstrapSourceBasePath())
      .parquet(filePath.toString())
      .schema(); {code}"	HUDI	Patch Available	3	4	646	pull-request-available
13574309	Break-up schema evolution: add schema evolution changes to ported spark	[https://github.com/apache/hudi/pull/10278] is too large to review and needs to be broken into smaller prs. Create a pr to add necessary schema evolution and spark minor version compatibility to the ported spark reader code.	HUDI	Closed	3	7	646	pull-request-available
13579153	Remove AvroWriteSupport and ParquetReaderIterator from hudi-common	2 classes with hadoop deps that can be moved to hadoop common and aren't covered by other prs.	HUDI	Closed	3	3	646	pull-request-available
13543790	HoodieMergeHelper merges bootstrap files twice	The merge helper uses bootstrap file reader which merges the skeleton and base files, but then merges again unnecessarily 	HUDI	Closed	3	1	646	pull-request-available
13588895	Get rid of separate reader instance for cdc reader	Use the existing spark reader instead of creating a separate cdc reader	HUDI	Closed	3	4	646	pull-request-available
13557235	Create config for choosing if you want to read using position based merging	"right now 

hoodie.write.record.positions controls this but it should be a separate read config"	HUDI	Closed	1	2	646	pull-request-available
13553989	sparksql query perfermance cost more in hudi 0.14-rc	"version: hudi-0.14.0-rc1，hudi-0.14.0-rc2

 

CREATE TABLE `hudi_test`.`tmp_hudi_test_1` (
  `id` string,
  `name` string,
  `dt` bigint,
  `day` STRING COMMENT '日期分区',
  `hour` INT COMMENT '小时分区'
)using hudi
OPTIONS ('hoodie.datasource.write.hive_style_partitioning' 'false', 'hoodie.datasource.meta.sync.enable' 'false', 'hoodie.datasource.hive_sync.enable' 'false')
tblproperties (
  'primaryKey' = 'id',
  'type' = 'mor',
  'preCombineField'='dt',
  'hoodie.index.type' = 'BUCKET',
  'hoodie.bucket.index.hash.field' = 'id',
  'hoodie.bucket.index.num.buckets'=512
)
PARTITIONED BY (`day`,`hour`);

insert into `hudi_test`.`tmp_hudi_test_1` select '1' as id, 'aa' as name, 123 as dt, '2023-10-12' as `day`, 10 as `hour`;
insert into `hudi_test`.`tmp_hudi_test_1` select '1' as id, 'aa' as name, 123 as dt, '2023-10-12' as `day`, 11 as `hour`;
insert into `hudi_test`.`tmp_hudi_test_1` select '1' as id, 'aa' as name, 123 as dt, '2023-10-12' as `day`, 12 as `hour`;

select * from `hudi_test`.`tmp_hudi_test_1` where day='2023-10-12' and hour=11;

 

 

right stage task number should be 1

if table files is much，would cause driver oom or fullgc for a long time"	HUDI	Open	3	1	646	pull-request-available
13571067	Classify exceptions as schema exceptions when converting from avro to spark row format	All issues related to schema should throw exceptions that are HoodieSchemaExceptions. Classify  exceptions when converting from avro to spark row format as schema compatibility exceptions because they are due to illegal schema, or the records are incompatible with the provided schema.	HUDI	Closed	3	4	646	pull-request-available
13502975	Address checkstyle warnings while building hudi	"As of now, we see lot of checkstyle warnings while building hudi. We need to take a look to see how exactly we can fix this. 
 * Take a stab at fixing as much as possible.
 * If we couldn't get it to a controllable number, suppress the checkstyle warnings. 

 

excerpt logs
{code:java}
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestGcsEventsHoodieIncrSource.java:46: 'org.apache.log4j.LogManager' should be separated from previous imports. [ImportOrder]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestGcsEventsHoodieIncrSource.java:74: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestGcsEventsHoodieIncrSource.java:271: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestInputBatch.java:33: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSourcePostProcessor.java:22: Import org.apache.hudi.common.config.TypedProperties appears after other imports that it should precede [ImportOrder]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSourcePostProcessor.java:61: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSourcePostProcessor.java:315: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestCheckpointUtils.java:24: Import org.apache.hudi.utilities.sources.helpers.KafkaOffsetGen.CheckpointUtils appears after other imports that it should precede [ImportOrder]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestS3EventsMetaSelector.java:56: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestProtoConversionUtil.java:47: Import com.google.protobuf.util.Timestamps appears after other imports that it should precede [ImportOrder]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestProtoConversionUtil.java:72: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestDFSPathSelectorCommonMethods.java:46: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestCloudObjectsSelector.java:57: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestDatePartitionPathSelector.java:49: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/debezium/TestMysqlDebeziumSource.java:31: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/debezium/TestPostgresDebeziumSource.java:31: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/debezium/TestAbstractDebeziumSource.java:58: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/checkpointing/TestKafkaConnectHdfsProvider.java:36: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHiveIncrementalPuller.java:54: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieRepairTool.java:69: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/transform/TestSqlFileBasedTransformer.java:45: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/transform/TestChainedTransformer.java:30: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/transform/TestSqlQueryBasedTransformer.java:35: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/transform/TestFlatteningTransformer.java:29: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotCopier.java:43: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java:68: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java:126: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java:159: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java:217: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java:233: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java:247: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestJdbcbasedSchemaProvider.java:42: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java:61: Missing a Javadoc comment. [JavadocType]
[INFO] /Users/nsb/Documents/personal/projects/nov26/hudi/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestAWSDatabaseMigrationServiceSource.java:40: Missing a Javadoc comment. [JavadocType] {code}
 "	HUDI	Closed	2	4	646	pull-request-available
13513504	Cleaning conflicts in occ mode	"{code:java}
configuration parameter: 
'hoodie.cleaner.policy.failed.writes' = 'LAZY'
'hoodie.write.concurrency.mode' = 'optimistic_concurrency_control' {code}
Because `getInstantsToRollback` is not locked, multiple writes get the same `instantsToRollback`, the same `instant` will be deleted multiple times and the same `rollback.inflight` will be created multiple times.

!image-2022-12-14-11-26-37-252.png!

!image-2022-12-14-11-26-21-995.png!"	HUDI	Open	3	1	646	pull-request-available
13582449	Ensure properties are copied when modifying schema	Properties are not always copied when we modify the schema, such as removing fields.	HUDI	Closed	3	1	646	pull-request-available
13571781	SchemaProvider not deduced for some deltastreamer scenarios	"There are a couple cases where the schema doesn't go through 

HoodieSchemaUtils.deduceWriterSchema which can cause stream failures or a bad table state."	HUDI	Closed	3	1	646	pull-request-available
13530625	Reading from bootstrap tables in spark takes around 2x the time compared to regular reads	Reading from bootstrap is slow. Even if it is a full bootstrap, it is pretty much just as slow.	HUDI	Closed	3	4	646	pull-request-available
13437339	Allow for offline compaction of MOR tables via spark streaming	"Currently there is no way to avoid compaction taking up a lot of resources when run inline or async for MOR tables via Spark Streaming. Delta Streamer has ways to assign resources between ingestion and async compaction but Spark Streaming does not have that option. 

Introducing a flag to turn off automatic compaction and allowing users to run compaction in a separate process will decouple both concerns.

This will also allow the users to size the cluster just for ingestion and deal with compaction separate without blocking.  We will need to look into documenting best practices for running offline compaction."	HUDI	Closed	2	4	646	easyfix, pull-request-available
13557253	HoodieBaseFileGroupRecordBuffer doesn't check if option is empty	"If the option is empty an exception will be thrown when get is called. This happens when the reader is enabled for the test testBaseFileAndLogFileUpdateMatchesDeleteBlock

 
{code:java}
Caused by: java.util.NoSuchElementException: No value present in Option
    at org.apache.hudi.common.util.Option.get(Option.java:89)
    at org.apache.hudi.common.table.read.HoodieBaseFileGroupRecordBuffer.doProcessNextDataRecord(HoodieBaseFileGroupRecordBuffer.java:143)
    at org.apache.hudi.common.table.read.HoodieKeyBasedFileGroupRecordBuffer.processNextDataRecord(HoodieKeyBasedFileGroupRecordBuffer.java:90)
    at org.apache.hudi.common.table.read.HoodieKeyBasedFileGroupRecordBuffer.processDataBlock(HoodieKeyBasedFileGroupRecordBuffer.java:81)
    at org.apache.hudi.common.table.log.BaseHoodieLogRecordReader.processQueuedBlocksForInstant(BaseHoodieLogRecordReader.java:751)
    at org.apache.hudi.common.table.log.BaseHoodieLogRecordReader.scanInternalV1(BaseHoodieLogRecordReader.java:393)
    ... 28 more {code}"	HUDI	Closed	3	1	646	pull-request-available
13561581	Enable completion time for File Group Reader	"For all query types, we should enable completion time semantic safely:
 # Snapshot/RO (MOR, COW)
 # Incremental (MOR, COW)
 # TimeTravel (MOR, COW),
 # CDC (MOR, COW)
 # Bootstrap 

Similar to HUDI-6802, when using file group reader for reading data, completion time should be used for getting the list of files in the file group reader.  The log file ordering will be tackled separately."	HUDI	Closed	1	3	646	pull-request-available
13484318	Update docker demo website page to explain how to run on m1 macs	Update the website to reflect the changes done in the HUDI-2786 fix	HUDI	Closed	3	4	646	pull-request-available
13505370	When creating table in spark-sql setting wrong keygenerator config does not warn	Setting `hoodie.datasource.write.keygenerator.class` when creating a table does nothing. `hoodie.table.keygenerator.class` needs to be set. We should warn when this is set on create table. Maybe we should warn about any configs that do nothing when set on table creation? The error will present on the first write if the keygenerator is not the default.	HUDI	Patch Available	4	4	646	pull-request-available
13527534	Clustering fails on uncompacted bootstrapped mor table	"If you create a bootstrapped table and then write upserts with the configs below, it will fail when clustering is attempted. On a non-bootstrap table, clustering will succeed.
{code:java}
betterdf.write.format(""hudi"")
            .option(HoodieWriteConfig.TABLE_NAME, ""hoodie_test"")
            .option(DataSourceWriteOptions.OPERATION_OPT_KEY, ""upsert"")
            .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, ""key"")
            .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, ""partition"")
            .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, ""ts"")
            .option(DataSourceWriteOptions.HIVE_STYLE_PARTITIONING.key, ""true"")
            .option(DataSourceWriteOptions.TABLE_TYPE.key, tt)
            .option(""hoodie.compact.inline"", ""false"")
.option(""hoodie.clustering.inline"", ""true"")             .option(""hoodie.clustering.plan.strategy.sort.columns"", ""partition,key"")
            .option(""hoodie.index.type"", ""MERGE_ON_READ"")
            .mode(SaveMode.Append)
            .save(basePath) {code}
When clustering is attempted I get the exception:
{code:java}
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: Partition path does not belong to base-path
  at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
  at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
  at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
  at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1596)
  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.IllegalArgumentException: Partition path does not belong to base-path
  at org.apache.hudi.common.fs.FSUtils.getRelativePartitionPath(FSUtils.java:228)
  at org.apache.hudi.BaseHoodieTableFileIndex.lambda$getAllQueryPartitionPaths$0(BaseHoodieTableFileIndex.java:203)
  at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
  at java.util.Iterator.forEachRemaining(Iterator.java:116)
  at scala.collection.convert.Wrappers$IteratorWrapper.forEachRemaining(Wrappers.scala:31)
  at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
  at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
  at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
  at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
  at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
  at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
  at org.apache.hudi.BaseHoodieTableFileIndex.getAllQueryPartitionPaths(BaseHoodieTableFileIndex.java:204)
  at org.apache.hudi.SparkHoodieTableFileIndex.listMatchingPartitionPaths(SparkHoodieTableFileIndex.scala:205)
  at org.apache.hudi.HoodieFileIndex.listFiles(HoodieFileIndex.scala:146)
  at org.apache.spark.sql.hudi.analysis.HoodiePruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(HoodiePruneFileSourcePartitions.scala:54)
  at org.apache.spark.sql.hudi.analysis.HoodiePruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(HoodiePruneFileSourcePartitions.scala:42)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)
  at org.apache.spark.sql.catalyst.plans.logical.Sort.mapChildren(basicLogicalOperators.scala:755)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)
  at org.apache.spark.sql.catalyst.plans.logical.RepartitionOperation.mapChildren(basicLogicalOperators.scala:1429)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
  at org.apache.spark.sql.hudi.analysis.HoodiePruneFileSourcePartitions.apply(HoodiePruneFileSourcePartitions.scala:42)
  at org.apache.spark.sql.hudi.analysis.HoodiePruneFileSourcePartitions.apply(HoodiePruneFileSourcePartitions.scala:40)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:91)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)
  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:173)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:172)
  at org.apache.hudi.HoodieDatasetBulkInsertHelper$.bulkInsert(HoodieDatasetBulkInsertHelper.scala:142)
  at org.apache.hudi.HoodieDatasetBulkInsertHelper.bulkInsert(HoodieDatasetBulkInsertHelper.scala)
  at org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy.performClusteringWithRecordsAsRow(SparkSortAndSizeExecutionStrategy.java:72)
  at org.apache.hudi.client.clustering.run.strategy.MultipleSparkJobExecutionStrategy.lambda$runClusteringForGroupAsyncAsRow$6(MultipleSparkJobExecutionStrategy.java:249)
  at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
  ... 5 more {code}"	HUDI	Closed	2	1	646	pull-request-available
13514762	Spark Sql Guide says that precombine field is only required for MOR but it is always required	Remove the line that says precombine field is required for MOR	HUDI	Closed	3	4	646	pull-request-available
13547762	Concurrent cleaner commit same instance conflict 	"Timeline 

 
{code:java}
-rw-r--r--   1 jon  wheel     0B Aug 16 19:58 20230816195843234.commit.requested
-rw-r--r--   1 jon  wheel     0B Aug 16 19:58 20230816195845557.commit.requested
-rw-r--r--   1 jon  wheel   2.2K Aug 16 19:58 20230816195843234.inflight
-rw-r--r--   1 jon  wheel   813B Aug 16 19:58 20230816195845557.inflight
-rw-r--r--   1 jon  wheel   2.6K Aug 16 19:58 20230816195845557.commit
-rw-r--r--   1 jon  wheel   2.6K Aug 16 19:58 20230816195843234.commit
-rw-r--r--   1 jon  wheel   1.7K Aug 16 19:58 20230816195855285.clean.requested
-rw-r--r--   1 jon  wheel   1.7K Aug 16 19:58 20230816195855285.clean.inflight
-rw-r--r--   1 jon  wheel   1.8K Aug 16 19:58 20230816195855389.clean.requested
-rw-r--r--   1 jon  wheel   1.7K Aug 16 19:58 20230816195855285.clean {code}
requests:
{code:java}
avrocat hudi/output/.hoodie/20230816195855285.clean.requested
{""earliestInstantToRetain"": {""HoodieActionInstant"": {""timestamp"": ""20230816195654386"", ""action"": ""commit"", ""state"": ""COMPLETED""}}, ""lastCompletedCommitTimestamp"": ""20230816195845557"", ""policy"": ""KEEP_LATEST_COMMITS"", ""filesToBeDeletedPerPartition"": {""map"": {}}, ""version"": {""int"": 2}, ""filePathsToBeDeletedPerPartition"": {""map"": {""1970/01/01"": [{""filePath"": {""string"": ""file:/tmp/hudi/output/1970/01/01/f66cf644-9e9f-477f-863c-eb62d1c6b14d-0_0-1391-2009_20230816195619275.parquet""}, ""isBootstrapBaseFile"": {""boolean"": false}}]}}, ""partitionsToBeDeleted"": {""array"": []}} {code}
{code:java}
avrocat hudi/output/.hoodie/20230816195855389.clean.requested {""earliestInstantToRetain"": {""HoodieActionInstant"": {""timestamp"": ""20230816195704584"", ""action"": ""commit"", ""state"": ""COMPLETED""}}, ""lastCompletedCommitTimestamp"": ""20230816195845557"", ""policy"": ""KEEP_LATEST_COMMITS"", ""filesToBeDeletedPerPartition"": {""map"": {}}, ""version"": {""int"": 2}, ""filePathsToBeDeletedPerPartition"": {""map"": {""1970/01/01"": [{""filePath"": {""string"": ""file:/tmp/hudi/output/1970/01/01/f66cf644-9e9f-477f-863c-eb62d1c6b14d-0_0-1391-2009_20230816195619275.parquet""}, ""isBootstrapBaseFile"": {""boolean"": false}}], ""1970/01/20"": [{""filePath"": {""string"": ""file:/tmp/hudi/output/1970/01/20/05942caf-2d53-4345-845c-5e42abaca797-0_0-1454-2121_20230816195635690.parquet""}, ""isBootstrapBaseFile"": {""boolean"": false}}]}}, ""partitionsToBeDeleted"": {""array"": []}}
{code}
Console output:

notice transaction starts twice for the same instance
{code:java}
424775 [pool-75-thread-1] INFO  org.apache.hudi.table.action.clean.CleanActionExecutor [] - Finishing previously unfinished cleaner instant=[==>20230816195855285__clean__INFLIGHT__20230816195855525]
424775 [pool-75-thread-1] INFO  org.apache.hudi.table.action.clean.CleanActionExecutor [] - Using cleanerParallelism: 1
424779 [pool-91-thread-1] INFO  org.apache.hudi.common.table.timeline.HoodieActiveTimeline [] - Loaded instants upto : Option{val=[==>20230816195855389__clean__REQUESTED__20230816195855634]}
424779 [pool-91-thread-1] INFO  org.apache.hudi.client.transaction.TransactionManager [] - Transaction starting for Option{val=[==>20230816195855285__clean__INFLIGHT]} with latest completed transaction instant Optional.empty
424779 [pool-91-thread-1] INFO  org.apache.hudi.client.transaction.lock.LockManager [] - LockProvider org.apache.hudi.client.transaction.lock.InProcessLockProvider
424779 [pool-91-thread-1] INFO  org.apache.hudi.client.transaction.lock.InProcessLockProvider [] - Base Path file:/tmp/hudi/output, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@78f60539[Write locks = 0, Read locks = 0], Thread pool-91-thread-1, In-process lock state ACQUIRING
424779 [pool-91-thread-1] INFO  org.apache.hudi.client.transaction.lock.InProcessLockProvider [] - Base Path file:/tmp/hudi/output, Lock Instance java.util.concurrent.locks.ReentrantReadWriteLock@78f60539[Write locks = 1, Read locks = 0], Thread pool-91-thread-1, In-process lock state ACQUIRED
424779 [pool-91-thread-1] INFO  org.apache.hudi.client.transaction.TransactionManager [] - Transaction started for Option{val=[==>20230816195855285__clean__INFLIGHT]} with latest completed transaction instant Optional.empty {code}
The following pr exposed the issue

[https://github.com/apache/hudi/pull/8602]

This does not cause data corruption. Writer needs to be restarted"	HUDI	Patch Available	3	1	646	pull-request-available
13567639	Add config for custom write support for parquet row writer	org.apache.hudi.avro.HoodieAvroWriteSupport allows for custom write support for avro writing, but this feature does not exist for spark row writer. Add new config to add custom write support.	HUDI	Closed	3	2	646	pull-request-available
13509313	Fix Bulk Insert ColumnSortPartitioners	"Currently, all of the Custom Bulk Insert ColumnSortPartitioner impls incorrectly return ""true"" from the ""arePartitionRecordsSorted"" method, even though records might not necessarily be sorted by the partition-path columns as is required by this method.

In case when such Partitioner is used and the data is NOT sorted by the list of columns that start w/ partition ones, this could lead to a Parquet writers being closed prematurely when writing files creating a LOT of small files."	HUDI	Closed	2	1	646	pull-request-available
13588647	Add hosts to storage info and pass from hive reader	Hive reader uses a map right now and that is kinda messy and breaks some assumptions about reader context. Instead we should pass it with the storage info	HUDI	Closed	3	2	646	pull-request-available
13524902	Add support for kafka offsets in various sources	Add support for kafka offsets in AvroKafkaSource and JsonKafkaSource	HUDI	Open	3	4	646	pull-request-available
13557524	Disable new file reader for metadata table	hfile base files are not yet implemented, so the metadata table should use the hoodie relations	HUDI	Open	3	4	646	pull-request-available
13590364	build is broken if no spark profile provided on m1 mac	m1-mac profile activation blocks active by default profiles	HUDI	Closed	3	1	646	pull-request-available
13596390	Spark can't read enum from avro log block	Spark reads enum as a string. In the avro log block this will fail. 	HUDI	Closed	2	1	646	pull-request-available
13505322	Use proper parallelism for engine context APIs	"do a global search of these APIs
- org.apache.hudi.common.engine.HoodieEngineContext#flatMap
- org.apache.hudi.common.engine.HoodieEngineContext#map

and similar ones take in parallelism.

A lot of occurrences are using number of items as parallelism, which affect performance. Parallelism should be based on num cores available in the cluster and set by user via parallelism configs."	HUDI	Patch Available	2	4	646	pull-request-available
13534353	Retry Pending Compactions before ingestion in Deltastreamer	"Currently, the timeline looks like this: The compaction is done after the deltacommit
{code:java}
drwxr-xr-x  2 jon  staff    64 Apr 27 15:15 archived
-rw-r--r--  1 jon  staff     0 Apr 27 15:15 20230427151550027.deltacommit.requested
drwxr-xr-x  4 jon  staff   128 Apr 27 15:15 metadata
-rw-r--r--  1 jon  staff   898 Apr 27 15:15 hoodie.properties
-rw-r--r--  1 jon  staff  2199 Apr 27 15:15 20230427151550027.deltacommit.inflight
-rw-r--r--  1 jon  staff  5269 Apr 27 15:15 20230427151550027.deltacommit
-rw-r--r--  1 jon  staff     0 Apr 27 15:15 20230427151558008.deltacommit.requested
-rw-r--r--  1 jon  staff  4365 Apr 27 15:15 20230427151558008.deltacommit.inflight
-rw-r--r--  1 jon  staff  6177 Apr 27 15:16 20230427151558008.deltacommit
-rw-r--r--  1 jon  staff  3211 Apr 27 15:16 20230427151602293.compaction.requested
-rw-r--r--  1 jon  staff     0 Apr 27 15:16 20230427151604955.deltacommit.requested
-rw-r--r--  1 jon  staff  4365 Apr 27 15:16 20230427151604955.deltacommit.inflight
-rw-r--r--  1 jon  staff  5977 Apr 27 15:16 20230427151604955.deltacommit
-rw-r--r--  1 jon  staff  1229 Apr 27 15:16 20230427151609434.rollback.requested
-rw-r--r--  1 jon  staff     0 Apr 27 15:16 20230427151609434.rollback.inflight
-rw-r--r--  1 jon  staff  1413 Apr 27 15:16 20230427151609434.rollback
-rw-r--r--  1 jon  staff     0 Apr 27 15:16 20230427151602293.compaction.inflight
-rw-r--r--  1 jon  staff  5271 Apr 27 15:16 20230427151602293.commit {code}"	HUDI	Closed	3	4	646	pull-request-available
13554327	HoodieAvroParquetReader sets configs wrong (and prevents schema evolution with non row writer)	"tryOverrideDefaultConfigs sets the wrong config object. Inside the reader there are 2 different config objects. 

Setting the configs like 
{code:java}
ParquetReader<IndexedRecord> reader = new HoodieAvroParquetReaderBuilder<IndexedRecord>(path).withConf(conf)
.set(ParquetInputFormat.STRICT_TYPE_CHECKING, ""false"")
.set(AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS, ""false"")
.build(); {code}
will set the correct object I believe"	HUDI	Closed	2	1	646	pull-request-available
13575548	DataSourceWriteOptions.TABLE_NAME() does not work	DataSourceWriteOptions.TABLE_NAME(). If you set that the write will fail saying the table name is missing.	HUDI	Closed	2	1	646	pull-request-available, starter
13565847	hoodie.write.handle.missing.cols.with.lossless.type.promotion does not work with HoodieIncrSource unless meta cols are dropped	The incoming meta cols are treated as new columns which is not allowed by internalschema so it fails	HUDI	Closed	3	1	646	pull-request-available
13586178	Disable ci testing for spark versions less than 3.3	We will be removing support for spark 2.4, 3.0, 3.1, 3.2. Before we remove them we will transition our ci to run all the tests on higher versions of spark. To prevent ci failures during this transition, we will stop the ci from running tests on the versions to be discontinued.	HUDI	Closed	1	6	646	pull-request-available
13479511	Add compliance check in GH actions	"Investigate GH actions and add a job before running the tests (the job with matrix)
 - check if Hudi repo's PR template is been used in PR description
 - make sure Hudi repo's PRs have JIRA titles [HUDI-XXXX] or [MINOR] 

 

Checklist:
 * Clean up committer's guide
 * Protect master branch
 * always squash and merge."	HUDI	Closed	3	4	646	pull-request-available
13530139	Clustering on bootstrap table fails when row writer is disabled	As was pointed out in [https://github.com/apache/hudi/pull/8206#pullrequestreview-1345104330,] clustering on bootstrap table fails when row writer is disabled. The non-row writer path does not handle bootstrap file paths. An attemp to fix this was made in [https://github.com/apache/hudi/pull/8289] but it only succeeds for Spark 3.2+ versions.	HUDI	Closed	3	1	646	pull-request-available
13579400	DefaultHoodieRecordPayload should be projection compatible	DefaultHoodieRecordPayload is not listed as projection compatible. Therefore, with relation reader we end up reading all the columns for mor reads.	HUDI	Closed	1	1	646	pull-request-available
13500958	GH actions and azure ci tests run even for trivial fixes	PR's such as [https://github.com/apache/hudi/pull/7178] do not need to run gh actions/azure ci. It is not necessary and takes resources from important fixes.	HUDI	Closed	4	4	646	pull-request-available
13532356	Auto capitalize all config values	Auto capitalize all config values as soon as they are input into Hudi so that we minimize having to deal with capitalization internally. This is blocked until all config values are capitalized	HUDI	In Progress	4	4	646	pull-request-available
13579434	Bootstrap read tries to parse partition from the bootstrap base path	Bootstrap gets the partition path values from the bootstrap base path when reading the base file but from the hudi table in all other cases. Just use the hudi path in all cases to keep partition parsing more simple	HUDI	Closed	3	1	646	pull-request-available
13593603	Fix MOR queries on Hive in integration tests	"MOR queries in integration tests (ITTestHoodieDemo#testParquetDemo) on Hive fail to run.
"	HUDI	Closed	1	1	646	pull-request-available
13484264	Some unit tests cannot run on mac m1	org.xerial.snappy is not compatible with m1 macs before version 1.1.8.2. Additionally Spark is not compatible with m1 macs before version. 2.4.8 and rocksdb-jini is not compatible before version 6.29.4.1	HUDI	Closed	3	4	646	pull-request-available
13552496	Handle schema evolution across base and log files in HoodieFileGroupReader	Goal: When the schema evolves from base to log files, the new HoodieFileGroupReader should handle the schema evolution within the file group properly.	HUDI	Closed	1	3	646	pull-request-available
13557365	Temp view different reads cause issues with support batch	"The same reader is used so the support batch true from the first query is cached for the second even though the second shouldn't support batch
{code:java}
   res.registerTempTable(""bootstrapped"");
    assertEquals(1950, sqlContext.sql(""select distinct _hoodie_record_key from bootstrapped"").count());
    // NOTE: To fetch record's count Spark will optimize the query fetching minimal possible amount
    //       of data, which might not provide adequate amount of test coverage
    sqlContext.sql(""select * from bootstrapped"").show(); {code}"	HUDI	Open	3	1	646	pull-request-available
13479537	INSERT_OVERWRITE(/TABLE) in spark sql should not fail time travel queries for older timestamps	"when INSERT_OVERWRITE or INSERT_OVERWRITE_TABLE is used in spark-sql, we should still support time travel queries for older timestamps. 

 

Ref issue: https://github.com/apache/hudi/issues/6452"	HUDI	Open	2	4	646	hudi-on-call
13575250	Add API to create HoodieStorage in HoodieIOFactory	We should use the HoodieIOFactory to create HoodieStorage instance, to replace the hardcoded reflection logic.	HUDI	Closed	3	3	646	hoodie-storage, pull-request-available
13484458	Make bundle combination testing covered in CI	"this is to cover 
- spark-bundle 
- utilities-bundle
- utilities-slim-bundle"	HUDI	Closed	3	6	646	pull-request-available
13582603	Use TypedProperties to store the spillable map configs for the FG reader	This takes up 4 params for the fg reader that can just be stored in the TypedProperties that is already passed in.	HUDI	Closed	3	4	646	pull-request-available
13524734	Use only single connection per push gateway	With the addition of a metrics instance per table, we now end up creating a pushgateway per table. If they are all connecting to the same hostname and port, this can lead to issues. 	HUDI	Open	3	1	646	pull-request-available
13562047	MIT fails when attempting to change partition	"With the following 
{code:java}
DROP TABLE IF EXISTS datalake_person_spark;DROP TABLE IF EXISTS datalake_partial_update_partition_change_hudi;
CREATE TABLE datalake_person_sparkUSING PARQUETPARTITIONED BY (inc_day)asSELECT1 as id, 2 as version, 'str_2' as name, CAST('2023-01-01 12:12:12' as TIMESTAMP) as birthDate, '2023-10-02' as inc_day;
CREATE TABLE datalake_partial_update_partition_change_hudi (    id INT,    version INT,    name STRING,    birthDate TIMESTAMP,    inc_day STRING)    USING hudi    PARTITIONED BY (inc_day)    TBLPROPERTIES (        'primaryKey' = 'id',        'type' = 'cow'    );
INSERT INTO datalake_partial_update_partition_change_hudiSELECT 1 as id, 1 as version, 'str_1' as name,CAST('2023-01-01 11:11:11' AS TIMESTAMP) as birthDate,CAST('2023-10-01' AS DATE) as inc_day;

select * from datalake_partial_update_partition_change_hudi;
set hoodie.index.type=GLOBAL_SIMPLE;set hoodie.simple.index.update.partition.path=true;
MERGE INTO datalake_partial_update_partition_change_hudi t using (select * from  datalake_person_spark) as s  on t.id=s.id when matched THEN UPDATE SET * ; {code}
We get exception 
{code:java}
Failed to upsert for commit time 20231215212435126org.apache.hudi.exception.HoodieUpsertException: Failed to upsert for commit time 20231215212435126	at org.apache.hudi.table.action.commit.BaseWriteHelper.write(BaseWriteHelper.java:70)	at org.apache.hudi.table.action.commit.SparkUpsertCommitActionExecutor.execute(SparkUpsertCommitActionExecutor.java:44)	at org.apache.hudi.table.HoodieSparkCopyOnWriteTable.upsert(HoodieSparkCopyOnWriteTable.java:114)	at org.apache.hudi.table.HoodieSparkCopyOnWriteTable.upsert(HoodieSparkCopyOnWriteTable.java:103)	at org.apache.hudi.client.SparkRDDWriteClient.upsert(SparkRDDWriteClient.java:142)	at org.apache.hudi.DataSourceUtils.doWriteOperation(DataSourceUtils.java:224)	at org.apache.hudi.HoodieSparkSqlWriterInternal.liftedTree1$1(HoodieSparkSqlWriter.scala:514)	at org.apache.hudi.HoodieSparkSqlWriterInternal.writeInternal(HoodieSparkSqlWriter.scala:512)	at org.apache.hudi.HoodieSparkSqlWriterInternal.write(HoodieSparkSqlWriter.scala:203)	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:120)	at org.apache.spark.sql.hudi.command.MergeIntoHoodieTableCommand.executeUpsert(MergeIntoHoodieTableCommand.scala:469)	at org.apache.spark.sql.hudi.command.MergeIntoHoodieTableCommand.run(MergeIntoHoodieTableCommand.scala:283)	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)	at org.apache.spark.sql.hudi.TestMergeIntoTable.$anonfun$new$13(TestMergeIntoTable.scala:321)	at org.apache.spark.sql.hudi.TestMergeIntoTable.$anonfun$new$13$adapted(TestMergeIntoTable.scala:265)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.withTempDir(HoodieSparkSqlTestBase.scala:77)	at org.apache.spark.sql.hudi.TestMergeIntoTable.$anonfun$new$12(TestMergeIntoTable.scala:265)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.$anonfun$withRecordType$3(HoodieSparkSqlTestBase.scala:216)	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.withSQLConf(HoodieSparkSqlTestBase.scala:188)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.$anonfun$withRecordType$1(HoodieSparkSqlTestBase.scala:215)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.$anonfun$withRecordType$1$adapted(HoodieSparkSqlTestBase.scala:207)	at scala.collection.immutable.List.foreach(List.scala:431)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.withRecordType(HoodieSparkSqlTestBase.scala:207)	at org.apache.spark.sql.hudi.TestMergeIntoTable.$anonfun$new$11(TestMergeIntoTable.scala:265)	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.$anonfun$test$1(HoodieSparkSqlTestBase.scala:85)	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)	at org.scalatest.Transformer.apply(Transformer.scala:22)	at org.scalatest.Transformer.apply(Transformer.scala:20)	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)	at scala.collection.immutable.List.foreach(List.scala:431)	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)	at org.scalatest.Suite.run(Suite.scala:1112)	at org.scalatest.Suite.run$(Suite.scala:1094)	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.org$scalatest$BeforeAndAfterAll$$super$run(HoodieSparkSqlTestBase.scala:44)	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)	at org.apache.spark.sql.hudi.HoodieSparkSqlTestBase.run(HoodieSparkSqlTestBase.scala:44)	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1314)	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1308)	at scala.collection.immutable.List.foreach(List.scala:431)	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1308)	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:993)	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:971)	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1474)	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:971)	at org.scalatest.tools.Runner$.run(Runner.scala:798)	at org.scalatest.tools.Runner.run(Runner.scala)	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:43)	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:26)Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 1 times, most recent failure: Lost task 1.0 in stage 37.0 (TID 168) (jonathans-mbp executor driver): java.lang.NullPointerException	at org.apache.hudi.avro.HoodieAvroUtils.getActualSchemaFromUnion(HoodieAvroUtils.java:1182)	at org.apache.hudi.avro.HoodieAvroUtils.rewriteRecordWithNewSchema(HoodieAvroUtils.java:874)	at org.apache.hudi.avro.HoodieAvroUtils.rewriteRecordWithNewSchema(HoodieAvroUtils.java:845)	at org.apache.hudi.avro.HoodieAvroUtils.rewriteRecordWithNewSchema(HoodieAvroUtils.java:829)	at org.apache.hudi.common.model.HoodieAvroRecord.prependMetaFields(HoodieAvroRecord.java:132)	at org.apache.hudi.index.HoodieIndexUtils.mergeIncomingWithExistingRecord(HoodieIndexUtils.java:259)	at org.apache.hudi.index.HoodieIndexUtils.lambda$mergeForPartitionUpdatesIfNeeded$8d2a44d8$1(HoodieIndexUtils.java:316)	at org.apache.hudi.data.HoodieJavaRDD.lambda$flatMap$a6598fcb$1(HoodieJavaRDD.java:137)	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125)	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)	at org.apache.spark.scheduler.Task.run(Task.scala:136)	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:750)
Driver stacktrace:	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)	at scala.Option.foreach(Option.scala:407)	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$countByKey$1(PairRDDFunctions.scala:367)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)	at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:367)	at org.apache.spark.api.java.JavaPairRDD.countByKey(JavaPairRDD.scala:314)	at org.apache.hudi.data.HoodieJavaPairRDD.countByKey(HoodieJavaPairRDD.java:105)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.buildProfile(BaseSparkCommitActionExecutor.java:196)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.execute(BaseSparkCommitActionExecutor.java:170)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.execute(BaseSparkCommitActionExecutor.java:82)	at org.apache.hudi.table.action.commit.BaseWriteHelper.write(BaseWriteHelper.java:63)	... 104 moreCaused by: java.lang.NullPointerException	at org.apache.hudi.avro.HoodieAvroUtils.getActualSchemaFromUnion(HoodieAvroUtils.java:1182)	at org.apache.hudi.avro.HoodieAvroUtils.rewriteRecordWithNewSchema(HoodieAvroUtils.java:874)	at org.apache.hudi.avro.HoodieAvroUtils.rewriteRecordWithNewSchema(HoodieAvroUtils.java:845)	at org.apache.hudi.avro.HoodieAvroUtils.rewriteRecordWithNewSchema(HoodieAvroUtils.java:829)	at org.apache.hudi.common.model.HoodieAvroRecord.prependMetaFields(HoodieAvroRecord.java:132)	at org.apache.hudi.index.HoodieIndexUtils.mergeIncomingWithExistingRecord(HoodieIndexUtils.java:259)	at org.apache.hudi.index.HoodieIndexUtils.lambda$mergeForPartitionUpdatesIfNeeded$8d2a44d8$1(HoodieIndexUtils.java:316)	at org.apache.hudi.data.HoodieJavaRDD.lambda$flatMap$a6598fcb$1(HoodieJavaRDD.java:137)	at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125)	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)	at org.apache.spark.scheduler.Task.run(Task.scala:136)	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:750) {code}"	HUDI	Closed	1	1	646	pull-request-available
13421651	Allow original partition column value to be retrieved when using TimestampBasedKeyGen	"{color:#172b4d}Currently, b/c Spark by default omits partition values from the data files (instead encoding them into partition paths for partitioned tables), using `TimestampBasedKeyGenerator` w/ original timestamp based-column makes it impossible to retrieve the original value (reading from Spark) even though it's persisted in the data file as well.{color}

 
{code:java}
import org.apache.hudi.DataSourceWriteOptions
import org.apache.hudi.config.HoodieWriteConfig
import org.apache.hudi.keygen.constant.KeyGeneratorOptions._
import org.apache.hudi.hive.MultiPartKeysValueExtractor

val df = Seq((1, ""z3"", 30, ""v1"", ""2018-09-23""), (2, ""z3"", 35, ""v1"", ""2018-09-24"")).toDF(""id"", ""name"", ""age"", ""ts"", ""data_date"")

// mor
df.write.format(""hudi"").
option(HoodieWriteConfig.TABLE_NAME, ""issue_4417_mor"").
option(""hoodie.datasource.write.table.type"", ""MERGE_ON_READ"").
option(""hoodie.datasource.write.recordkey.field"", ""id"").
option(""hoodie.datasource.write.partitionpath.field"", ""data_date"").
option(""hoodie.datasource.write.precombine.field"", ""ts"").
option(""hoodie.datasource.write.keygenerator.class"", ""org.apache.hudi.keygen.TimestampBasedKeyGenerator"").
option(""hoodie.deltastreamer.keygen.timebased.timestamp.type"", ""DATE_STRING"").
option(""hoodie.deltastreamer.keygen.timebased.output.dateformat"", ""yyyy/MM/dd"").
option(""hoodie.deltastreamer.keygen.timebased.timezone"", ""GMT+8:00"").
option(""hoodie.deltastreamer.keygen.timebased.input.dateformat"", ""yyyy-MM-dd"").
mode(org.apache.spark.sql.SaveMode.Append).
save(""file:///tmp/hudi/issue_4417_mor"")

+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+---+----------+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|name|age| ts| data_date|
+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+---+----------+
|  20220110172709324|20220110172709324...|                 2|            2018/09/24|703e56d3-badb-40b...|  2|  z3| 35| v1|2018-09-24|
|  20220110172709324|20220110172709324...|                 1|            2018/09/23|58fde2b3-db0e-464...|  1|  z3| 30| v1|2018-09-23|
+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+---+----------+

// can not query any data
spark.read.format(""hudi"").load(""file:///tmp/hudi/issue_4417_mor"").where(""data_date = '2018-09-24'"")
// still can not query any data
spark.read.format(""hudi"").load(""file:///tmp/hudi/issue_4417_mor"").where(""data_date = '2018/09/24'"").show 

// cow
df.write.format(""hudi"").
option(HoodieWriteConfig.TABLE_NAME, ""issue_4417_cow"").
option(""hoodie.datasource.write.table.type"", ""COPY_ON_WRITE"").
option(""hoodie.datasource.write.recordkey.field"", ""id"").
option(""hoodie.datasource.write.partitionpath.field"", ""data_date"").
option(""hoodie.datasource.write.precombine.field"", ""ts"").
option(""hoodie.datasource.write.keygenerator.class"", ""org.apache.hudi.keygen.TimestampBasedKeyGenerator"").
option(""hoodie.deltastreamer.keygen.timebased.timestamp.type"", ""DATE_STRING"").
option(""hoodie.deltastreamer.keygen.timebased.output.dateformat"", ""yyyy/MM/dd"").
option(""hoodie.deltastreamer.keygen.timebased.timezone"", ""GMT+8:00"").
option(""hoodie.deltastreamer.keygen.timebased.input.dateformat"", ""yyyy-MM-dd"").
mode(org.apache.spark.sql.SaveMode.Append).
save(""file:///tmp/hudi/issue_4417_cow"") 

+-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+---+----------+
 |_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|name|age| ts| data_date|
 +-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+---+----------+
 |  20220110172721896|20220110172721896...|                 2|            2018/09/24|81cc7819-a0d1-4e6...|  2|  z3| 35| v1|2018/09/24|
 |  20220110172721896|20220110172721896...|                 1|            2018/09/23|d428019b-a829-41a...|  1|  z3| 30| v1|2018/09/23|
 +-------------------+--------------------+------------------+----------------------+--------------------+---+----+---+---+----------+ 
// can not query any data
spark.read.format(""hudi"").load(""file:///tmp/hudi/issue_4417_cow"").where(""data_date = '2018-09-24'"").show

// but 2018/09/24 works
spark.read.format(""hudi"").load(""file:///tmp/hudi/issue_4417_cow"").where(""data_date = '2018/09/24'"").show  {code}
 

 "	HUDI	Open	2	1	646	hudi-on-call, pull-request-available, sev:critical
13518366	Set class loader for parquet data block	"inlineConf.setClassLoader(InLineFileSystem.class.getClassLoader()); 

is set in HoodieHFileDataBlock but it is not set in HoodieParquetDataBlock

This causes 
{code:java}
java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hudi.common.fs.inline.InLineFileSystem not found  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)  at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)  at org.apache.parquet.hadoop.ParquetReader$Builder.build(ParquetReader.java:336)  at org.apache.hudi.io.storage.HoodieAvroParquetReader.getIndexedRecordIteratorInternal(HoodieAvroParquetReader.java:168)  at org.apache.hudi.io.storage.HoodieAvroParquetReader.getIndexedRecordIterator(HoodieAvroParquetReader.java:99)  at org.apache.hudi.io.storage.HoodieAvroFileReaderBase.getRecordIterator(HoodieAvroFileReaderBase.java:39)  at org.apache.hudi.io.storage.HoodieAvroParquetReader.getRecordIterator(HoodieAvroParquetReader.java:53)  at org.apache.hudi.common.table.log.block.HoodieParquetDataBlock.readRecordsFromBlockPayload(HoodieParquetDataBlock.java:162)  at org.apache.hudi.common.table.log.block.HoodieDataBlock.getRecordIterator(HoodieDataBlock.java:128)  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.getRecordsIterator(AbstractHoodieLogRecordReader.java:779)  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processDataBlock(AbstractHoodieLogRecordReader.java:641)  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processQueuedBlocksForInstant(AbstractHoodieLogRecordReader.java:691)  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:379)  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:231)  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scan(AbstractHoodieLogRecordReader.java:220)  at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.performScan(HoodieMergedLogRecordScanner.java:114)  at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:107)  at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader.<init>(HoodieMetadataMergedLogRecordReader.java:61)  at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader.<init>(HoodieMetadataMergedLogRecordReader.java:49)  at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader$Builder.build(HoodieMetadataMergedLogRecordReader.java:232)  at org.apache.hudi.metadata.HoodieBackedTableMetadata.getLogRecordScanner(HoodieBackedTableMetadata.java:528)  at org.apache.hudi.metadata.HoodieBackedTableMetadata.openReaders(HoodieBackedTableMetadata.java:438)  at org.apache.hudi.metadata.HoodieBackedTableMetadata.lambda$getOrCreateReaders$12(HoodieBackedTableMetadata.java:421)  at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)  at org.apache.hudi.metadata.HoodieBackedTableMetadata.getOrCreateReaders(HoodieBackedTableMetadata.java:421)  at org.apache.hudi.metadata.HoodieBackedTableMetadata.lambda$getRecordsByKeys$2(HoodieBackedTableMetadata.java:227)  at java.util.HashMap.forEach(HashMap.java:1290)  at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordsByKeys(HoodieBackedTableMetadata.java:225)  at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordByKey(HoodieBackedTableMetadata.java:148)  at org.apache.hudi.metadata.BaseTableMetadata.fetchAllFilesInPartition(BaseTableMetadata.java:327)  at org.apache.hudi.metadata.BaseTableMetadata.getAllFilesInPartition(BaseTableMetadata.java:145)  at org.apache.hudi.metadata.HoodieMetadataFileSystemView.listPartition(HoodieMetadataFileSystemView.java:65)  at org.apache.hudi.common.table.view.AbstractTableFileSystemView.lambda$ensurePartitionLoadedCorrectly$10(AbstractTableFileSystemView.java:311)  at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)  at org.apache.hudi.common.table.view.AbstractTableFileSystemView.ensurePartitionLoadedCorrectly(AbstractTableFileSystemView.java:302)  at org.apache.hudi.common.table.view.AbstractTableFileSystemView.getLatestBaseFiles(AbstractTableFileSystemView.java:515)  at org.apache.hudi.hadoop.HoodieROTablePathFilter.accept(HoodieROTablePathFilter.java:200)  at org.apache.spark.sql.execution.datasources.PathFilterWrapper.accept(InMemoryFileIndex.scala:165)  at org.apache.spark.util.HadoopFSUtils$.$anonfun$listLeafFiles$8(HadoopFSUtils.scala:285)  at org.apache.spark.util.HadoopFSUtils$.$anonfun$listLeafFiles$8$adapted(HadoopFSUtils.scala:285)  at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:304)  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)  at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)  at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)  at scala.collection.mutable.ArrayOps$ofRef.filterImpl(ArrayOps.scala:198)  at scala.collection.TraversableLike.filter(TraversableLike.scala:395)  at scala.collection.TraversableLike.filter$(TraversableLike.scala:395)  at scala.collection.mutable.ArrayOps$ofRef.filter(ArrayOps.scala:198)  at org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:285)  at org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)  at scala.collection.TraversableLike.map(TraversableLike.scala:286)  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)  at scala.collection.AbstractTraversable.map(Traversable.scala:108)  at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)  at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)  at org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:565)  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)  at org.apache.hudi.BaseFileOnlyRelation.toHadoopFsRelation(BaseFileOnlyRelation.scala:203)  at org.apache.hudi.DefaultSource$.resolveBaseFileOnlyRelation(DefaultSource.scala:277)  at org.apache.hudi.DefaultSource$.createRelation(DefaultSource.scala:241)  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:115)  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:72)  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)  at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)  at scala.Option.getOrElse(Option.scala:189)  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)  at org.apache.hudi.integ.testsuite.dag.nodes.ValidateDatasetNode.getDatasetToValidate(ValidateDatasetNode.java:56)  at org.apache.hudi.integ.testsuite.dag.nodes.BaseValidateDatasetNode.execute(BaseValidateDatasetNode.java:116)  at org.apache.hudi.integ.testsuite.dag.scheduler.DagScheduler.executeNode(DagScheduler.java:135)  at org.apache.hudi.integ.testsuite.dag.scheduler.DagScheduler.lambda$execute$0(DagScheduler.java:104)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)  at java.util.concurrent.FutureTask.run(FutureTask.java:266)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)  at java.lang.Thread.run(Thread.java:750)Caused by: java.lang.ClassNotFoundException: Class org.apache.hudi.common.fs.inline.InLineFileSystem not found  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665) {code}"	HUDI	Closed	1	1	646	pull-request-available
13545430	New file format does not work with in memory index	".option(HoodieIndexConfig.INDEX_TYPE.key, IndexType.INMEMORY.toString)

then the file index is empty so we need to get the files from glob paths"	HUDI	In Progress	3	4	646	pull-request-available
13510679	Clean up partially failed restore if any	"If a table was attempted w/ ""restore"" operation and if it failed mid-way, restore could still be lying around. when re-attempted, a new instant time will be allotted and re-attempted from scratch. but this may thwart compaction progression in MDT. so we need to ensure for a given savepoint, we always re-use restore instant if any. "	HUDI	Closed	1	1	646	pull-request-available
13583560	Remove support of Spark 2, 3.0, 3.1, and 3.2	"For Hudi 0.16.0 (bridge release) and 1.0, we should remove Hudi support on old Spark versions including Spark 2, 3.0, 3.1, and 3.2 at least.  Dev work include:

(1) Removes build profiles and GitHub actions that are no longer relevant

(2) Cleans up unused code in Spark integration

(3) Upgrades the default dependency version to be consistent with Spark 3.3

(4) Updates README.md"	HUDI	Closed	1	4	646	pull-request-available
13591874	Get rid of disable vectorized reader in sql config in spark fg reader implementation	Modifying the sql conf side effects and is not necessary	HUDI	Closed	1	1	646	pull-request-available
13578657	Fix usage of new Configuration() in production code	"new Configuration() is used in non-test code in several places:

HoodieParquetDataBlock.java

Metrics.java

 "	HUDI	Closed	3	4	646	pull-request-available
13568625	Make Issues with schema easier to understand for users	Provide exceptions that classify issues with schema. Additionally, provide users with a clear explanation of what is wrong.	HUDI	Closed	3	4	646	pull-request-available
13486988	Add support to DELETE_PARTITIONS w/ wild card	"as of now, DELETE_PARTITIONS expected comma separated list of partitions to delete. But would like to support wild card with that. 

For eg,

year=2022/month=10/day=05/*

assuming its hour based partitioning

 

Ref: https://github.com/apache/hudi/issues/6866"	HUDI	Closed	3	4	646	pull-request-available
13385749	Support Reading Bootstrap MOR RT Table  In Spark DataSource Table	"Currently spark datasource table use the HoodieBootstrapRelation to read bootstrap table.

However, for bootstrap mor rt table, we have not support yet."	HUDI	Patch Available	2	3	646	pull-request-available
13546010	Implement core mor and bootstrap functionality	Implement basic functionality for mor and bootstrap	HUDI	Closed	1	2	646	pull-request-available
13593895	Use filegroup reader for clustering - row writer	org.apache.hudi.client.clustering.run.strategy.MultipleSparkJobExecutionStrategy#readRecordsForGroupAsRow. Can just use the datasource reader api. However we specify the files to read. So probably need to implement HUDI-6613. I have done that already here: [https://github.com/apache/hudi/pull/10062] but pr has not been updated in a while	HUDI	Patch Available	3	4	646	pull-request-available
13448657	Add tooling to delete empty non-completed instants from timeline	If there are empty instants in timeline, older versions of hudi can run into issues. We have put in a fix [here|https://github.com/apache/hudi/pull/5261] for it. But would like to provider users in older versions w/ some tool to assist deleting such empty instants if incase they are not completed. 	HUDI	Closed	3	4	646	pull-request-available
13344758	HoodieAvroUtils - rewrite() is not handling evolution of a nested record field.	"When a schema has nested record field and one of the fields of the nested record evolves, then rewrite() results in SchemaCompatibilityException (or ArrayIndexOutOfBoundsException).

{{/*
   *  OldRecord:                     NewRecord:
   *      field1 : String                field1 : String
   *      field2 : record                field2 : record
   *         field_21 : string              field_21 : string
   *         field_22 : Integer             field_22 : Integer
   *      field3: Integer                   field_23 : String
   *                                       field_24 : Integer
   *                                     field3: Integer
   *
   *  When a nested record has changed/evolved, newRecord.put(field2, oldRecord.get(field2)), is not sufficient.
   *  Requires a deep-copy/rewrite of the evolved field.
   */}}

Note 1:  When reading the parquet file using the writer schema, this should not be a problem, as new fields are substituted with null.  When reading the parquet using reader schema and writing to a new file using the writer schema, this issue is manifested.

Note 2:  Hudi test suite - upsertNode exercies this path.  (fixed as a work around in a separate task).
"	HUDI	Closed	2	1	646	pull-request-available, sev:critical
13593598	Fix incremental queries in integration tests on Hive	Incremental queries in integration tests (ITTestHoodieDemo#testParquetDemo) on Hive do not return expected results.	HUDI	Closed	1	1	646	pull-request-available
13547308	If table with recordkey doesn't have recordkey in spark ds write, it will bulk insert by default	If an existing table has a recordkey, if you write with spark ds and don't include a recordkey, it will think it is pkless and should default to bulk insert	HUDI	Open	1	1	646	pull-request-available
13538616	SQL Insert into should default to bulk insert	Insert into is mainly used for append only workflows. For a better out of the box experience, it should default to bulk insert. This is already the case with CTAS. Merge into is usually used when updating 	HUDI	Closed	3	4	646	pull-request-available
13409695	Upgrade Java toolset/runtime to JDK11	"We should upgrade to at least JDK11, or preferably current latest LTS JDK17

 

Plan for migration:

*Compilation*

JDK8 will still be used to *compile* source code (both source/target will stay `1.8`): this is required to make sure as we migrate to JDK11, we don't add dependencies on features not compatible w/ JDK8. Migrating off JDK8 in toolset, will happen at a later point, when we would stop providing any assurances about the code being able to be run on 1.8.

*Runtime*

JDK11 will be used to *run* the code: due to JVM b/w compatibility there should be no issues of running the code compiled for 1.8 on JDK11+, other than dependencies compatibility. For that we would make sure that all our test-suites do run against JDK11+.

 

 

 "	HUDI	Closed	1	4	1904	performance
13448009	TableSchemaResolver fetches/parses HoodieCommitMetadata multiple times while extracting Schema	"We've recently discovered that TableSchemaResolver does a lot of throw-away work during initialization and basic schema reading performed by Spark Datasource (see screenshot).

This poses a problem for large tables where HoodieCommitMetadata is of non-trivial size (100s of Mbs).

We'd minimize amount of throw-away work done by `TableSchemaResolver` and try to re-use read/parsed commits' metadata as much as possible."	HUDI	Closed	1	1	1904	pull-request-available
13503357	"Hudi throwing ""PipeBroken"" exception during Merging on GCS"	"Originally reported at [https://github.com/apache/hudi/issues/7234]

-------

 

Root-cause:
Basically, the reason it’s failing is following: # GCS uses PipeInputStream/PipeOutputStream comprising reading/writing ends of the “pipe” it’s using for unidirectional comm b/w Threads
 # PipeInputStream (for whatever reason) remembers the thread that actually wrote into the pipe
 # In BoundedInMemoryQueue we’re bootstrapping new executors (read, threads) for reading and _writing_ (it’s only used in HoodieMergeHandle, and in bulk-insert)
 # When we’re done writing in HoodieMergeHelper, we’re shutting down *first* BIMQ, then the HoodieMergeHandle, and that’s exactly the reason why it’s failing

 

Issue has been introduced at [https://github.com/apache/hudi/pull/4264/files]"	HUDI	Patch Available	2	1	1904	pull-request-available
13473375	Optimizing file-listing path in MT	We should review file-listing path and try to optimize the file-listing path as much as possible.	HUDI	Closed	1	1	1904	pull-request-available
13256299	Apache Pulsar data source for Hudi DeltaStreamer	[Apache Pulsar|https://pulsar.apache.org/en/] is a pub/sub messaging system like Kafka, with a lot of new features like multiple subscription modes, out of the box service discovery etc. The goal here is to add Pulsar as a data source to DeltaStreamer. To get started please follow [Pulsar adaptor for Apache Spark|https://pulsar.apache.org/docs/en/adaptors-spark/]	HUDI	Closed	3	2	1904	pull-request-available
13523394	Fix Partitioners to avoid assuming that parallelism is always present	"Currently, `Partitioner` impls assume that there's always going to be some parallelism level.

This has not been issue previously for the following reasons:
 * RDDs always have inherent ""parallelism"" level defined as the # of partitions they operating upon. However for Dataset (SparkPlan) that's not necessarily the case (som SparkPlans might not be reporting the output partitioning)
 * Additionally, we have had the default parallelism level set in our configs before which meant that we'd prefer that over the actual incoming dataset.

However, since we've recently removed default parallelism value from our configs we now need to fix Partitioners to make sure these are not assuming that parallelism is always going to be present."	HUDI	Open	1	1	1904	pull-request-available
13444277	Evaluate Spark SQL vs DS performance	"In our internal benchmarks we've detected a regression in Spark SQL relative to Spark DataSource integration.

We need to investigate and subsequently address that."	HUDI	Closed	1	3	1904	pull-request-available
13413458	Configuration from CLI and properties files is not interoperable	"Currently, specifying commands t/h CLI or properties file does not seem to inter-operate seamlessly: configs are gonna be siloed w/in the original format (either config parsed from CLI, or properties object) and are NOT in sync.

 

This leads to the problems like the one reported below
{noformat}
Setting hoodie.async.clustering.enable=true  inside a properties file, throws error, since it only checks against --hoodie-conf /deltastreamer{noformat}"	HUDI	Closed	1	1	1904	pull-request-available
13413503	Archived Metadata Timeline is crashing if timeline contains REPLACE_COMMIT	"When doing `commits showarchived` in CLI it fails if REPLACE_COMMIT is present in the archived Timeline:
{code:java}
hudi:hoodie_benchmark->commits showarchived 5993625 [Spring Shell] ERROR org.springframework.shell.core.SimpleExecutionStrategy - Command failed org.apache.hudi.exception.HoodieIOException: Unknown action in metadata replacecommit 5993625 [Spring Shell] WARN org.springframework.shell.core.JLineShellComponent.exceptions - Unknown action in metadata replacecommit org.apache.hudi.exception.HoodieIOException: Unknown action in metadata replacecommit
{code}
 "	HUDI	Closed	1	1	1904	pull-request-available
13418925	Unify Hive `FileInputFormat` implementations	Currently there's substantial overlap b/w `HoodieParquetInputFormat` and `HoodieHFileInputFormat` (they are practically identical).	HUDI	Closed	1	4	1904	pull-request-available
13431528	Investigate spark3 read issues w/ hudi spark bundle 3.2 with S3 dataset	"{code:java}
scala> df.write.format(""hudi"").
     |   options(getQuickstartWriteConfigs).
     |   option(PRECOMBINE_FIELD_OPT_KEY, ""ts"").
     |   option(RECORDKEY_FIELD_OPT_KEY, ""uuid"").
     |   option(PARTITIONPATH_FIELD_OPT_KEY, ""partitionpath"").
     |   option(TABLE_NAME, tableName).
     |   mode(Overwrite).
     |   save(basePath)
warning: one deprecation; for details, enable `:setting -deprecation' or `:replay -deprecation'
2022-03-02 14:57:00,922 WARN config.DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
2022-03-02 14:57:00,930 WARN config.DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
2022-03-02 14:57:00,947 WARN hudi.HoodieSparkSqlWriter$: hoodie table at /tmp/hudi_trips_cow already exists. Deleting existing data & overwriting with new data.
2022-03-02 14:57:01,523 WARN metadata.HoodieBackedTableMetadata: Metadata table was not found at path /tmp/hudi_trips_cow/.hoodie/metadata
2022-03-02 14:57:10,929 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 15.0 (TID 15) (ip-172-31-47-53.us-east-2.compute.internal executor 2): java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.hudi.spark.org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.hudi.spark.org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.hudi.spark.org.apache.spark.sql.avro.HoodieAvroSerializer.<init>(HoodieAvroSerializer.scala:26)
	at org.apache.spark.sql.adapter.Spark3Adapter.createAvroSerializer(Spark3Adapter.scala:47)
	at org.apache.hudi.AvroConversionUtils$.$anonfun$createInternalRowToAvroConverter$1(AvroConversionUtils.scala:79)
	at org.apache.hudi.HoodieSparkUtils$.$anonfun$createRdd$5(HoodieSparkUtils.scala:166)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)


2022-03-02 14:57:12,923 ERROR scheduler.TaskSetManager: Task 1 in stage 15.0 failed 4 times; aborting job
org.apache.hudi.exception.HoodieUpsertException: Failed to upsert for commit time 20220302145700945
  at org.apache.hudi.table.action.commit.BaseWriteHelper.write(BaseWriteHelper.java:64)
  at org.apache.hudi.table.action.commit.SparkUpsertCommitActionExecutor.execute(SparkUpsertCommitActionExecutor.java:46)
  at org.apache.hudi.table.HoodieSparkCopyOnWriteTable.upsert(HoodieSparkCopyOnWriteTable.java:121)
  at org.apache.hudi.table.HoodieSparkCopyOnWriteTable.upsert(HoodieSparkCopyOnWriteTable.java:105)
  at org.apache.hudi.client.SparkRDDWriteClient.upsert(SparkRDDWriteClient.java:159)
  at org.apache.hudi.DataSourceUtils.doWriteOperation(DataSourceUtils.java:218)
  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:289)
  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:162)
  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)
  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:303)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
  ... 66 elided {code}"	HUDI	Closed	1	1	1904	pull-request-available
13507199	NPE in collumn stats for null values	"[https://github.com/apache/hudi/issues/6936]



[This code|https://github.com/apache/hudi/blob/0d70df89fe6b7049d576e2b9bf75afb29c75c46d/hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java#L147] can throw NPE from avro utils while processing null values. "	HUDI	Closed	1	1	1904	pull-request-available
13513483	Make sure predicates are appropriately pushed down to HoodieFileIndex when lazy listing	"After introduction of lazy-listing capability in HUDI-4812, it exposed an issue in Spark's design, where predicates are pushed-down into generic FileIndex implementations only during the execution phase.

This poses following issues:
 # HoodieFileIndex isn't listing the table until `listFiles` method is invoked
 # Listing would actually be performed only during actual execution in `FileSourceScanExac` node
 # Since listing isn't performed until the actual execution, table statistics are initialized w/ bogus values (of 1 byte) and Cost-based Optimizations (CBO) will be taking incorrect decisions based on that"	HUDI	Closed	1	1	1904	pull-request-available
13476039	Address S3 timeouts in Bloom Index with metadata table	"For partitioned table, there are significant number of S3 requests timeout causing the upserts to fail when using Bloom Index with metadata table.
{code:java}
Load meta index key ranges for file slices: hudi
collect at HoodieSparkEngineContext.java:137+details
org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
org.apache.hudi.client.common.HoodieSparkEngineContext.flatMap(HoodieSparkEngineContext.java:137)
org.apache.hudi.index.bloom.HoodieBloomIndex.loadColumnRangesFromMetaIndex(HoodieBloomIndex.java:213)
org.apache.hudi.index.bloom.HoodieBloomIndex.getBloomIndexFileInfoForPartitions(HoodieBloomIndex.java:145)
org.apache.hudi.index.bloom.HoodieBloomIndex.lookupIndex(HoodieBloomIndex.java:123)
org.apache.hudi.index.bloom.HoodieBloomIndex.tagLocation(HoodieBloomIndex.java:89)
org.apache.hudi.table.action.commit.HoodieWriteHelper.tag(HoodieWriteHelper.java:49)
org.apache.hudi.table.action.commit.HoodieWriteHelper.tag(HoodieWriteHelper.java:32)
org.apache.hudi.table.action.commit.BaseWriteHelper.write(BaseWriteHelper.java:53)
org.apache.hudi.table.action.commit.SparkUpsertCommitActionExecutor.execute(SparkUpsertCommitActionExecutor.java:45)
org.apache.hudi.table.HoodieSparkCopyOnWriteTable.upsert(HoodieSparkCopyOnWriteTable.java:113)
org.apache.hudi.table.HoodieSparkCopyOnWriteTable.upsert(HoodieSparkCopyOnWriteTable.java:97)
org.apache.hudi.client.SparkRDDWriteClient.upsert(SparkRDDWriteClient.java:155)
org.apache.hudi.DataSourceUtils.doWriteOperation(DataSourceUtils.java:206)
org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:329)
org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:183)
org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84) {code}
{code:java}
org.apache.hudi.exception.HoodieException: Exception when reading log file 
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:352)
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scan(AbstractHoodieLogRecordReader.java:196)
    at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader.getRecordsByKeys(HoodieMetadataMergedLogRecordReader.java:124)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.readLogRecords(HoodieBackedTableMetadata.java:266)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.lambda$getRecordsByKeys$1(HoodieBackedTableMetadata.java:222)
    at java.util.HashMap.forEach(HashMap.java:1290)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordsByKeys(HoodieBackedTableMetadata.java:209)
    at org.apache.hudi.metadata.BaseTableMetadata.getColumnStats(BaseTableMetadata.java:253)
    at org.apache.hudi.index.bloom.HoodieBloomIndex.lambda$loadColumnRangesFromMetaIndex$cc8e7ca2$1(HoodieBloomIndex.java:224)
    at org.apache.hudi.client.common.HoodieSparkEngineContext.lambda$flatMap$7d470b86$1(HoodieSparkEngineContext.java:137)
    at org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125)
    at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
    at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
    at scala.collection.AbstractIterator.to(Iterator.scala:1431)
    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
    at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.hudi.exception.HoodieIOException: IOException when reading logblock from log file HoodieLogFile{pathStr='s3a://<>/.hoodie/metadata/column_stats/.col-stats-0000_00000000000000.log.4_5-116-20141', fileLen=-1}
    at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:389)
    at org.apache.hudi.common.table.log.HoodieLogFormatReader.next(HoodieLogFormatReader.java:123)
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:229)
    ... 38 more
Caused by: org.apache.hadoop.fs.s3a.AWSClientIOException: re-open s3a://<>/.hoodie/metadata/column_stats/.col-stats-0000_00000000000000.log.4_5-116-20141 at 475916 on s3a://<>/.hoodie/metadata/column_stats/.col-stats-0000_00000000000000.log.4_5-116-20141: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:208)
    at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:117)
    at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:226)
    at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:392)
    at org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$3(Invoker.java:228)
    at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:115)
    at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:354)
    at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:226)
    at org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:270)
    at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:384)
    at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:418)
    at java.io.FilterInputStream.read(FilterInputStream.java:83)
    at java.io.DataInputStream.readInt(DataInputStream.java:387)
    at org.apache.hudi.common.table.log.block.HoodieLogBlock.getLogMetadata(HoodieLogBlock.java:228)
    at org.apache.hudi.common.table.log.HoodieLogFileReader.readBlock(HoodieLogFileReader.java:193)
    at org.apache.hudi.common.table.log.HoodieLogFileReader.next(HoodieLogFileReader.java:387)
    ... 40 more
Caused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1216)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1162)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:811)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5437)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5384)
    at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1519)
    at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$reopen$0(S3AInputStream.java:227)
    at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:115)
    ... 54 more
Caused by: com.amazonaws.thirdparty.apache.http.NoHttpResponseException: The target server failed to respond
    at com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)
    at com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)
    at com.amazonaws.thirdparty.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)
    at com.amazonaws.thirdparty.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)
    at com.amazonaws.thirdparty.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)
    at com.amazonaws.thirdparty.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)
    at com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)
    at com.amazonaws.thirdparty.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)
    at com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)
    at com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
    at com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
    at com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
    at com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
    at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1343)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1154)
    ... 66 more {code}"	HUDI	Closed	1	4	1904	pull-request-available
13484057	Serializing objects using Kryo fails to deserialize data back w/o prior registration	"Originally reported in:

[https://github.com/apache/hudi/issues/6621]

 

Kryo (used in SerializationUtils) by default allows class objects to be serialized w/o prior registration w/ Kryo: in that case Kryo will encode the first occurrence of the object of a particular class with full class-name, but subsequent occurrences will be using class-id associated with it (on the fly).

This poses issues for durable serialization (when we persist such serialized layout) in this case we're trying to deserialize file that doesn't have the class-name encoded and since user is running a different Spark job to read there's no association preserved in-memory either.

*NOTE: We should be using custom serialization sequences for every object we serialize for durable persistence, and avoid using frameworks like Kryo for that.*

 
----
*EDIT*

I'm taking back my hypothesis that the issue is in the class encoding, after writing a small test to validate the issue i confirmed that Kryo actually writes out full class-name for all classes registered implicitly (as it should).

It seems that the problem is actually indeed in misalignment of the Avro versions as reported by [@KnightChess|https://github.com/KnightChess]: quick-checking i see that b/w Avro 1.8.2 and 1.10.2, {{Utf8}} actually had one more field added:
{code:java}
  // 1.8.2 
  private byte[] bytes = EMPTY;
  private int length;
  private String string;

  // 1.10.2
  private byte[] bytes;
  private int hash;
  private int length;
  private String string; {code}
 
{{  }}Provided that we're relying on Kryo to generate serializer for {{orderingVal}} that could be {{Utf8}} (based on {{{}FieldSerializer{}}}) it would actually explain why it couldn't deserialize it back (since they will have different serializers)."	HUDI	Closed	1	1	1904	pull-request-available
13438669	Evaluate MT Column Stats Performance 	"h3. *UPDATE*

--------

*TL;DR* After identifying the bottlenecks as Avro 1.10 regression in generated Builder classes, relying on `SpecificData.getForSchema` call that loads corresponding model's class using reflection, we're able to bring down in a similar setting the runtime of reading for Column Stats Index from MT containing 800k records from *60s* to *3s* (20x)

 

Follow the comments for updates on that.

--------

Previously, while evaluating Data Skipping runtime in EMR setting, it was measured that reading of Column Stats Index of about ~800k records takes about *60s* (~10ms / record).

 

Given that the total size of the log-files involved is ~60Mb, seems like there are some performance bottlenecks that we should investigate before 0.11 release"	HUDI	Closed	1	1	1904	pull-request-available
13439914	Investigate Hudi vs Raw Parquet table discrepancy	"While benchmarking querying raw Parquet tables against Hudi tables, i've run the test against the same (Hudi) table:
 # In one query path i'm reading it as just a raw Parquet table
 # In another, i'm reading it as Hudi RO (read_optimized) table

Surprisingly enough, those 2 diverge in the # of files being read:

 
_Raw Parquet_
!https://t18029943.p.clickup-attachments.com/t18029943/f700a129-35bc-4aaa-948c-9495392653f2/Screen%20Shot%202022-04-15%20at%205.20.41%20PM.png|width=1691,height=149!
 
_Hudi_
!https://t18029943.p.clickup-attachments.com/t18029943/d063c689-a254-45cf-8ba5-07fc88b354b6/Screen%20Shot%202022-04-15%20at%205.21.33%20PM.png|width=1673,height=142!"	HUDI	Closed	1	3	1904	pull-request-available
13412414	Z-ordering Layout Optimization Strategy fails w/ Data Skipping enabled	"During testing of Z-ordering in test environment i've discovered following issues:
 # Queries failing for tables w/ enabled Clustering w/ Z-ordering Layout and data-skipping enabled, being unable to read `_SUCCESS` file (automatically created by Spark)
 # Some of the translations of the original query predicates into Z-index table predicates are translated incorrectly (`!=`, `not like`, etc)
 # Join merging indexes across commits incorrectly always checks for null first column (instead of Nth column) when picking the result of the merge"	HUDI	Closed	1	3	1904	pull-request-available
13522546	deduceShuffleParallelism Returns 0 when that should never happen	"This test 
{code:java}
  forAll(BulkInsertSortMode.values().toList) { (sortMode: BulkInsertSortMode) =>    val sortModeName = sortMode.name()    test(s""Test Bulk Insert with BulkInsertSortMode: '$sortModeName'"") {      withTempDir { basePath =>        testBulkInsertPartitioner(basePath, sortModeName)      }    }  }
  def testBulkInsertPartitioner(basePath: File, sortModeName: String): Unit = {    val tableName = generateTableName    //Remove these with [HUDI-5419]    spark.sessionState.conf.unsetConf(""hoodie.datasource.write.operation"")    spark.sessionState.conf.unsetConf(""hoodie.datasource.write.insert.drop.duplicates"")    spark.sessionState.conf.unsetConf(""hoodie.merge.allow.duplicate.on.inserts"")    spark.sessionState.conf.unsetConf(""hoodie.datasource.write.keygenerator.consistent.logical.timestamp.enabled"")    //Default parallelism is 200 which means in global sort, each record will end up in a different spark partition so    //9 files would be created. Setting parallelism to 3 so that each spark partition will contain a hudi partition.    val parallelism = if (sortModeName.equals(BulkInsertSortMode.GLOBAL_SORT.name())) {      ""hoodie.bulkinsert.shuffle.parallelism = 3,""    } else {      """"    }    spark.sql(      s""""""         |create table $tableName (         |  id int,         |  name string,         |  price double,         |  dt string         |) using hudi         | tblproperties (         |  primaryKey = 'id',         |  preCombineField = 'name',         |  type = 'cow',         |  $parallelism         |  hoodie.bulkinsert.sort.mode = '$sortModeName'         | )         | partitioned by (dt)         | location '${basePath.getCanonicalPath}/$tableName'        """""".stripMargin)    spark.sql(""set hoodie.sql.bulk.insert.enable = true"")    spark.sql(""set hoodie.sql.insert.mode = non-strict"")    spark.sql(      s""""""insert into $tableName  values         |(5, 'a', 35, '2021-05-21'),         |(1, 'a', 31, '2021-01-21'),         |(3, 'a', 33, '2021-03-21'),         |(4, 'b', 16, '2021-05-21'),         |(2, 'b', 18, '2021-01-21'),         |(6, 'b', 17, '2021-03-21'),         |(8, 'a', 21, '2021-05-21'),         |(9, 'a', 22, '2021-01-21'),         |(7, 'a', 23, '2021-03-21')         |"""""".stripMargin)    assertResult(3)(spark.sql(s""select distinct _hoodie_file_name from $tableName"").count())  } {code}
Fails due to 
{code:java}
requirement failed: Number of partitions (0) must be positive.
java.lang.IllegalArgumentException: requirement failed: Number of partitions (0) must be positive.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.Repartition.<init>(basicLogicalOperators.scala:951)
	at org.apache.spark.sql.Dataset.coalesce(Dataset.scala:2946)
	at org.apache.hudi.execution.bulkinsert.PartitionSortPartitionerWithRows.repartitionRecords(PartitionSortPartitionerWithRows.java:48)
	at org.apache.hudi.execution.bulkinsert.PartitionSortPartitionerWithRows.repartitionRecords(PartitionSortPartitionerWithRows.java:34)
	at org.apache.hudi.HoodieDatasetBulkInsertHelper$.prepareForBulkInsert(HoodieDatasetBulkInsertHelper.scala:124)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:763)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:239)
	at org.apache.spark.sql.hudi.command.InsertIntoHoodieTableCommand$.run(InsertIntoHoodieTableCommand.scala:107)
	at org.apache.spark.sql.hudi.command.InsertIntoHoodieTableCommand.run(InsertIntoHoodieTableCommand.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)
	at org.apache.spark.sql.hudi.TestInsertTable.testBulkInsertPartitioner(TestInsertTable.scala:1204) {code}
!image (1).png!"	HUDI	Closed	1	1	1904	pull-request-available
13521829	Fixing HoodieSparkRecord performance bottlenecks	"There currently following issues w/ the current HoodieSparkRecord implementation:
 # It rewrites records using `rewriteRecord` and `rewriteRecordWithNewSchema` which do Schema traversals for every record. Instead we should do schema traversal only once and produce a transformer that will directly create new record from the old one.
 # Records are currently copied for every Executor even for Simple one which actually is not buffering any records and therefore doesn't require records to be copied."	HUDI	Closed	1	1	1904	pull-request-available
13524122	MOR table w/ delete block in 0.12.2 not readable in 0.13 and also not compactable	"If we have a Delete block in MOR log blocks in 0.12.2 hudi version, read from 0.13.0 fails due to Kryo serialization/deser. In similar sense compaction also does not work. 

 

Set of users who might be impacted w/ this:

Those who are using MOR table and has 

uncompacted file groups which has Delete blocks. 

Delete blocks are possible only in following scenarios:

a. Delete operation

b. GLOBAL_INDEX + update partition path = true. Chances that it could result in delete blocks. 

 

Root cause:
HoodieKey was made KryoSerializable as part of RFC46, but guess missed to register.

 
{code:java}
 spark.sql(""select * from hudi_trips_snapshot "").show(100, false)
23/02/09 16:53:43 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19:02  WARN: [kryo] Unable to load class 7e51db6-6033-4794-ac59-44a930424b2b with kryo's ClassLoader. Retrying with current..
23/02/09 16:53:44 ERROR AbstractHoodieLogRecordReader: Got exception when reading log file
com.esotericsoftware.kryo.KryoException: Unable to find class: 7e51db6-6033-4794-ac59-44a930424b2b
Serialization trace:
orderingVal (org.apache.hudi.common.model.DeleteRecord)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:160)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:133)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:731)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:391)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
	at org.apache.hudi.common.util.SerializationUtils$KryoSerializerInstance.deserialize(SerializationUtils.java:100)
	at org.apache.hudi.common.util.SerializationUtils.deserialize(SerializationUtils.java:74)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.deserialize(HoodieDeleteBlock.java:106)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.getRecordsToDelete(HoodieDeleteBlock.java:91)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processQueuedBlocksForInstant(AbstractHoodieLogRecordReader.java:675)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:367)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:223)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.performScan(HoodieMergedLogRecordScanner.java:198)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:114)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:73)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner$Builder.build(HoodieMergedLogRecordScanner.java:464)
	at org.apache.hudi.LogFileIterator$.scanLog(Iterators.scala:326)
	at org.apache.hudi.LogFileIterator.<init>(Iterators.scala:91)
	at org.apache.hudi.RecordMergingFileIterator.<init>(Iterators.scala:172)
	at org.apache.hudi.HoodieMergeOnReadRDD.compute(HoodieMergeOnReadRDD.scala:100)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:111)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:154)
	... 42 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at java.lang.ClassLoader.findClass(ClassLoader.java:530)
	at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:106)
	... 47 more
23/02/09 16:53:44 ERROR Executor: Exception in task 0.0 in stage 40.0 (TID 78)
org.apache.hudi.exception.HoodieException: Exception when reading log file 
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:376)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:223)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.performScan(HoodieMergedLogRecordScanner.java:198)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:114)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:73)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner$Builder.build(HoodieMergedLogRecordScanner.java:464)
	at org.apache.hudi.LogFileIterator$.scanLog(Iterators.scala:326)
	at org.apache.hudi.LogFileIterator.<init>(Iterators.scala:91)
	at org.apache.hudi.RecordMergingFileIterator.<init>(Iterators.scala:172)
	at org.apache.hudi.HoodieMergeOnReadRDD.compute(HoodieMergeOnReadRDD.scala:100)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: 7e51db6-6033-4794-ac59-44a930424b2b
Serialization trace:
orderingVal (org.apache.hudi.common.model.DeleteRecord)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:160)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:133)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:731)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:391)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
	at org.apache.hudi.common.util.SerializationUtils$KryoSerializerInstance.deserialize(SerializationUtils.java:100)
	at org.apache.hudi.common.util.SerializationUtils.deserialize(SerializationUtils.java:74)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.deserialize(HoodieDeleteBlock.java:106)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.getRecordsToDelete(HoodieDeleteBlock.java:91)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processQueuedBlocksForInstant(AbstractHoodieLogRecordReader.java:675)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:367)
	... 28 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:111)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:154)
	... 42 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at java.lang.ClassLoader.findClass(ClassLoader.java:530)
	at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:106)
	... 47 more
23/02/09 16:53:44 WARN TaskSetManager: Lost task 0.0 in stage 40.0 (TID 78, localhost, executor driver): org.apache.hudi.exception.HoodieException: Exception when reading log file 
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:376)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:223)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.performScan(HoodieMergedLogRecordScanner.java:198)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:114)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:73)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner$Builder.build(HoodieMergedLogRecordScanner.java:464)
	at org.apache.hudi.LogFileIterator$.scanLog(Iterators.scala:326)
	at org.apache.hudi.LogFileIterator.<init>(Iterators.scala:91)
	at org.apache.hudi.RecordMergingFileIterator.<init>(Iterators.scala:172)
	at org.apache.hudi.HoodieMergeOnReadRDD.compute(HoodieMergeOnReadRDD.scala:100)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: 7e51db6-6033-4794-ac59-44a930424b2b
Serialization trace:
orderingVal (org.apache.hudi.common.model.DeleteRecord)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:160)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:133)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:731)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:391)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
	at org.apache.hudi.common.util.SerializationUtils$KryoSerializerInstance.deserialize(SerializationUtils.java:100)
	at org.apache.hudi.common.util.SerializationUtils.deserialize(SerializationUtils.java:74)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.deserialize(HoodieDeleteBlock.java:106)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.getRecordsToDelete(HoodieDeleteBlock.java:91)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processQueuedBlocksForInstant(AbstractHoodieLogRecordReader.java:675)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:367)
	... 28 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:111)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:154)
	... 42 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at java.lang.ClassLoader.findClass(ClassLoader.java:530)
	at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:106)
	... 47 more


23/02/09 16:53:44 ERROR TaskSetManager: Task 0 in stage 40.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 78, localhost, executor driver): org.apache.hudi.exception.HoodieException: Exception when reading log file
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:376)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:223)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.performScan(HoodieMergedLogRecordScanner.java:198)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:114)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:73)
	at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner$Builder.build(HoodieMergedLogRecordScanner.java:464)
	at org.apache.hudi.LogFileIterator$.scanLog(Iterators.scala:326)
	at org.apache.hudi.LogFileIterator.<init>(Iterators.scala:91)
	at org.apache.hudi.RecordMergingFileIterator.<init>(Iterators.scala:172)
	at org.apache.hudi.HoodieMergeOnReadRDD.compute(HoodieMergeOnReadRDD.scala:100)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: 7e51db6-6033-4794-ac59-44a930424b2b
Serialization trace:
orderingVal (org.apache.hudi.common.model.DeleteRecord)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:160)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:133)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:731)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:391)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
	at org.apache.hudi.common.util.SerializationUtils$KryoSerializerInstance.deserialize(SerializationUtils.java:100)
	at org.apache.hudi.common.util.SerializationUtils.deserialize(SerializationUtils.java:74)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.deserialize(HoodieDeleteBlock.java:106)
	at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.getRecordsToDelete(HoodieDeleteBlock.java:91)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processQueuedBlocksForInstant(AbstractHoodieLogRecordReader.java:675)
	at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:367)
	... 28 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:111)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:154)
	... 42 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
	at java.lang.ClassLoader.findClass(ClassLoader.java:530)
	at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:106)
	... 47 more


Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:753)
  ... 61 elided
Caused by: org.apache.hudi.exception.HoodieException: Exception when reading log file
  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:376)
  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternal(AbstractHoodieLogRecordReader.java:223)
  at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.performScan(HoodieMergedLogRecordScanner.java:198)
  at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:114)
  at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:73)
  at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner$Builder.build(HoodieMergedLogRecordScanner.java:464)
  at org.apache.hudi.LogFileIterator$.scanLog(Iterators.scala:326)
  at org.apache.hudi.LogFileIterator.<init>(Iterators.scala:91)
  at org.apache.hudi.RecordMergingFileIterator.<init>(Iterators.scala:172)
  at org.apache.hudi.HoodieMergeOnReadRDD.compute(HoodieMergeOnReadRDD.scala:100)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:123)
  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: 7e51db6-6033-4794-ac59-44a930424b2b
Serialization trace:
orderingVal (org.apache.hudi.common.model.DeleteRecord)
  at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:160)
  at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:133)
  at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
  at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:118)
  at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
  at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:731)
  at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:391)
  at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:302)
  at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
  at org.apache.hudi.common.util.SerializationUtils$KryoSerializerInstance.deserialize(SerializationUtils.java:100)
  at org.apache.hudi.common.util.SerializationUtils.deserialize(SerializationUtils.java:74)
  at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.deserialize(HoodieDeleteBlock.java:106)
  at org.apache.hudi.common.table.log.block.HoodieDeleteBlock.getRecordsToDelete(HoodieDeleteBlock.java:91)
  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processQueuedBlocksForInstant(AbstractHoodieLogRecordReader.java:675)
  at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scanInternalV1(AbstractHoodieLogRecordReader.java:367)
  ... 28 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
  at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:111)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:154)
  ... 42 more
Caused by: java.lang.ClassNotFoundException: 7e51db6-6033-4794-ac59-44a930424b2b
  at java.lang.ClassLoader.findClass(ClassLoader.java:530)
  at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:106)
  ... 47 more {code}
Run book to reproduce: 

[https://gist.github.com/nsivabalan/b45ebc6cb64ac1d1b45cf4e6ef6d6482]

 "	HUDI	Closed	1	1	1904	pull-request-available
13472184	Make sure Partition/Data schema are properly delineated in Spark Relations	After recent changes supporting preservation of the persistence of Partition Columns w/in the Data files, we need to make sure that Partittion/Data schemas are handled correctly and are consistent across Spark Relation implementation's stack (Relation, RDD, etc)	HUDI	Closed	1	1	1904	pull-request-available
13482833	Fix incompatibility w/ Spark 3.2.2	"As reported by the user:
??In spark 3.2.1, ProjectionOverSchema has a argument. But in spark 3.2.2 and spark 3.3.0, ProjectionOverSchema has 2 arguments. Spark32NestedSchemaPruning.scala is not compatible with spark 3.2.2.??????

 

More context in:

[https://github.com/apache/hudi/issues/6635]

 "	HUDI	Closed	1	1	1904	pull-request-available
13480572	Fix Hudi bundles requiring log4j2 on the classpath	"As part of addressing HUDI-4441, we've erroneously rebased Hudi onto ""log4j-1.2-api"" module under impression that it's an API module (as advertised) which turned out not to be the case: it's actual bridge implementation, requiring Log4j2 be provided on the classpath as required dependency.

For version of Spark < 3.3 this triggers exceptions like the following one (reported by [~akmodi])

 
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/LogManager    at org.apache.hudi.metrics.datadog.DatadogReporter.<clinit>(DatadogReporter.java:55)    at org.apache.hudi.metrics.datadog.DatadogMetricsReporter.<init>(DatadogMetricsReporter.java:62)    at org.apache.hudi.metrics.MetricsReporterFactory.createReporter(MetricsReporterFactory.java:70)    at org.apache.hudi.metrics.Metrics.<init>(Metrics.java:50)    at org.apache.hudi.metrics.Metrics.init(Metrics.java:96)    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamerMetrics.<init>(HoodieDeltaStreamerMetrics.java:44)    at org.apache.hudi.utilities.deltastreamer.DeltaSync.<init>(DeltaSync.java:243)    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer$DeltaSyncService.<init>(HoodieDeltaStreamer.java:663)    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.<init>(HoodieDeltaStreamer.java:143)    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.<init>(HoodieDeltaStreamer.java:116)    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.main(HoodieDeltaStreamer.java:562)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1000)    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1089)    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1098)    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.LogManager    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)    ... 23 more {code}"	HUDI	Closed	1	1	1904	pull-request-available
13476055	Ingestion failing if source column is dropped	"Ingestion using Deltastreamer fails if columns are dropped from source. I had reproduced using docker-demo setup. Below are the steps for reproducing it.
 # I had created data file `stage_1.json`(attached), ingested it to kafka and ingested to hudi-table from kafka using Deltastreamer job(using FileschemaProvider with `schema_stage1.avsc`)
 # Simulating column dropping from source in the next step.
 #  Repeat steps in step1 with stage2 files. Stage2 files doesn't have `day` column, Ingestion job failed. Below is detailed stacktrace.
{code:java}
Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
    at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
    at org.apache.spark.rdd.RDD.fold(RDD.scala:1092)
    at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply$mcD$sp(DoubleRDDFunctions.scala:35)
    at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:35)
    at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:35)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
    at org.apache.spark.rdd.DoubleRDDFunctions.sum(DoubleRDDFunctions.scala:34)
    at org.apache.spark.api.java.JavaDoubleRDD.sum(JavaDoubleRDD.scala:165)
    at org.apache.hudi.utilities.deltastreamer.DeltaSync.writeToSink(DeltaSync.java:607)
    at org.apache.hudi.utilities.deltastreamer.DeltaSync.syncOnce(DeltaSync.java:335)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.lambda$sync$2(HoodieDeltaStreamer.java:201)
    at org.apache.hudi.common.util.Option.ifPresent(Option.java:97)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.sync(HoodieDeltaStreamer.java:199)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.main(HoodieDeltaStreamer.java:557)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:329)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$mapPartitionsAsRDD$a3ab3c4$1(BaseSparkCommitActionExecutor.java:244)
    at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
    at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)
    at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)
    at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)
    at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
    at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed
    at org.apache.hudi.table.action.commit.HoodieMergeHelper.runMerge(HoodieMergeHelper.java:149)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdateInternal(BaseSparkCommitActionExecutor.java:358)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:349)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:322)
    ... 30 more
Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed
    at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:161)
    at org.apache.hudi.table.action.commit.HoodieMergeHelper.runMerge(HoodieMergeHelper.java:147)
    ... 33 more
Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed
    at java.util.concurrent.FutureTask.report(FutureTask.java:122)
    at java.util.concurrent.FutureTask.get(FutureTask.java:192)
    at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:155)
    ... 34 more
Caused by: org.apache.hudi.exception.HoodieException: operation has failed
    at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:248)
    at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:226)
    at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:52)
    at org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:278)
    at org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:36)
    at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:135)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    ... 3 more
Caused by: org.apache.hudi.exception.HoodieException: unable to read next record from parquet file
    at org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:53)
    at org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:45)
    at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:106)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    ... 4 more
Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'day' not found
    at org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)
    at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)
    at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)
    at org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)
    at org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)
    at org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)
    at org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)
    at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)
    at org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:48)
    ... 8 more {code}
 

 "	HUDI	Closed	1	1	1904	pull-request-available, schema, schema-evolution
13438745	Data Skipping is not working correctly in the presence of Schema Evolution	"Currently, Data Skipping is not handling correctly the case when column-stats are not aligned and, for ex, some of the (column, file) combinations are missing from the CSI.

 

This could occur in different scenarios (schema evolution, CSI config changes)"	HUDI	Closed	1	1	1904	pull-request-available
13482987	Make sure LogRecordReader doesn't flush the cache before each lookup	"Currently {{HoodieMetadataMergedLogRecordReader }}will flush internal record cache before each lookup which makes every lookup essentially do re-processing of the whole log-blocks stack again.

We should avoid that and only do the re-parsing incrementally (for the keys that ain't already cached)"	HUDI	Closed	1	1	1904	pull-request-available
13510843	Troubleshoot `testMetadataColumnStatsIndexPartialProjection` flakiness	"Test is periodically failing in CI, trying it repro locally was able to make it fail only once (w/ valueCount flip-flopping b/w 3 and 4 for a single file) out of a dozen runs.

Examples of failing runs:

[https://dev.azure.com/apache-hudi-ci-org/apache-hudi-ci/_build/results?buildId=13428&view=logs&j=3272dbb2-0925-5f35-bae7-04e75ae62175&t=fb428e45-27ff-524a-7e12-db1cb49c418a&s=ee3800fd-6e81-525f-e564-94108585217d]

 "	HUDI	Open	4	1	1904	pull-request-available
13306030	Partition Columns missing in files upserted after Metadata Bootstrap	"This issue happens in when the source data is partitioned using _*hive-style partitioning*_ which is also the default behavior of spark when it writes the data. With this partitioning, the partition column/schema is never stored in the files but instead retrieved on the fly from the file paths which have partition folder in the form *_partition_key=partition_value_*.

Now, during metadata bootstrap we store only the metadata columns in the hudi table folder. Also the *bootstrap schema* we are computing directly reads schema from the source data file which does not have the *partition column schema* in it. Thus it is not complete.

All this manifests into issues when we ultimately do *upserts* on these bootstrapped files and they are fully bootstrapped. During upsert time the schema evolves because the upsert dataframe needs to have partition column in it for performing upserts. Thus ultimately the *upserted rows* have the correct partition column value stored, while the other records which are simply copied over from the metadata bootstrap file have missing partition column in them. Thus, we observe a different behavior here with *bootstrapped* vs *non-bootstrapped* tables.

While this is not at the moment creating issues with *Hive* because it is able to determine the partition columns becuase of all the metadata it stores, however it creates a problem with other engines like *Spark* where the partition columns will show up as *null* when the upserted files are read.

Thus, the proposal is to fix the following issues:
 * When performing bootstrap, figure out the partition schema and store it in the *bootstrap schema* in the commit metadata file. This would provide the following benefits:
 ** From a completeness perspective this is good so that there is no behavioral changes between bootstrapped vs non-bootstrapped tables.
 ** In spark bootstrap relation and incremental query relation where we need to figure out the latest schema, once can simply get the accurate schema from the commit metadata file instead of having to determine whether or not partition column is present in the schema obtained from the metadata file and if not figure out the partition schema everytime and merge (which can be expensive).
 * When doing upsert on files that are metadata bootstrapped, the partition column values should be correctly determined and copied to the upserted file to avoid missing and null values.
 ** Again this is consistent behavior with non-bootstrapped tables and even though Hive seems to somehow handle this, we should consider other engines like *Spark* where it cannot be automatically handled.
 ** Without this it will be significantly more complicated to be able to provide the partition value on read side in spark, to be able to determine everytime whether partition value is null and somehow filling it in.
 ** Once the table is fully bootstrapped at some point in future, and the bootstrap commit is say cleaned up and spark querying happens through *parquet* datasource instead of *new bootstrapped datasource*, the *parquet datasource* will return null values wherever it find the missing partition values. In that case, we have no control over the *parquet* datasource as it is simply reading from the file. "	HUDI	Closed	1	1	1904	pull-request-available
13413244	Make sure usages of any `DateFormat`-based components are thread-safe	"Any `DateFormat` based class is inherently NOT thread-safe.

Therefore we need to make sure we do concurrency-control (either through ThreadLocals or locks) when accessing these.


This has already bitten us w/ HUDI-2812. "	HUDI	Closed	3	1	1904	pull-request-available
13441931	Make sure Hudi Relations are only fetching strictly required columns	"In 0.11 we were able to considerably optimize data throughput for MOR tables in XXX: previously MOR table would read the whole table (full row with all columns) even for a simple `df.count()` query.

 

This has been optimized to just fetch the _required_ columns for each table-type (COW/MOR). However, we can optimize this even further:
 # COW tables do not require _any columns at all_
 # MOR tables do required primary-key and pre-combine-key columns only when actual merging w/ updated records (from delta-log files) is performed. Otherwise, they could avoid reading even these cols."	HUDI	Closed	1	1	1904	pull-request-available
13476605	Support ingesting from Apache Pulsar in DeltaStreamer	"Currently, you can ingest from Pulsar by using their native ""pulsar-spark-connector"" to ingest from it into Spark, then subsequently use Hudi's Spark DS integration to write it out.

Would be great to also to match support for Pulsar as a DeltaStreamer's source."	HUDI	Closed	1	2	1904	pull-request-available
13481771	Fix CSI not supporting InSet operator	"Currently Column Stats Index only supports `In` operator but doesn't support optimized `InSet` version.

As reported by the user:
https://github.com/apache/hudi/issues/6655"	HUDI	Closed	3	1	1904	pull-request-available
13475822	Add more test coverage for Spark SQL,  Spark Quickstart guide	"We should more test coverage, and in particular in these areas:


 # Add tests for ""DELETE FROM"" clauses
 # Make sure Spark Quickstart guide matches the one on the website"	HUDI	Closed	2	1	1904	pull-request-available
13406270	Add support to configure no of small files to consider with MOR	"Looks like in MOR, when an index is used which cannot index log files (which is the case for all out of box indexes in hudi), we just choose the smallest parquet file for every commit. So, over time, every file will grow to become fullest is the idea here. In other words, only one small file will be bin backed per commit even though there could be more. 

source [link|https://github.com/apache/hudi/blob/3354fac42f9a2c4dbc8ac73ca4749160e9b9459b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitPartitioner.java#L66]

 

We can add a config which can control the total number of files considered as small files for MOR table when index which cannot index log files are used. 

We can leave the default value to 1 (current behavior). But for interested users, this should be flexible. 

 

Original issue

https://github.com/apache/hudi/issues/3676 

 

 "	HUDI	Resolved	1	4	1904	pull-request-available, sev:critical, user-support-issues
13436007	MOR MergeOnRead FitleringIterator stackoverflow error	"We run integration tests against hudi at regular cadence and recently we are seeing stackoverflow error w/ MOR table for spark long running yaml. 

 
{code:java}
22/03/26 14:27:04 INFO ValidateDatasetNode: Validate data in target hudi path basaePath/*/*/*
22/03/26 14:28:51 ERROR Executor: Exception in task 2.0 in stage 975.0 (TID 17933)
java.lang.StackOverflowError
        at java.util.HashMap.removeNode(HashMap.java:821)
        at java.util.HashMap.remove(HashMap.java:800)
        at org.apache.hudi.common.util.collection.ExternalSpillableMap.remove(ExternalSpillableMap.java:238)
        at org.apache.hudi.common.util.collection.ExternalSpillableMap.remove(ExternalSpillableMap.java:55)
        at scala.collection.convert.Wrappers$JMapWrapperLike.remove(Wrappers.scala:296)
        at scala.collection.convert.Wrappers$JMapWrapperLike.remove$(Wrappers.scala:296)
        at scala.collection.convert.Wrappers$JMapWrapper.remove(Wrappers.scala:317)
        at org.apache.hudi.HoodieMergeOnReadRDD$LogFileIterator.removeLogRecord(HoodieMergeOnReadRDD.scala:187)
        at org.apache.hudi.HoodieMergeOnReadRDD$RecordMergingFileIterator.hasNext(HoodieMergeOnReadRDD.scala:262)
        at org.apache.hudi.HoodieMergeOnReadRDD$RecordMergingFileIterator.hasNext(HoodieMergeOnReadRDD.scala:271)
        at org.apache.hudi.HoodieMergeOnReadRDD$RecordMergingFileIterator.hasNext(HoodieMergeOnReadRDD.scala:271)
        at org.apache.hudi.HoodieMergeOnReadRDD$RecordMergingFileIterator.hasNext(HoodieMergeOnReadRDD.scala:271)
        at org.apache.hudi.HoodieMergeOnReadRDD$RecordMergingFileIterator.hasNext(HoodieMergeOnReadRDD.scala:271)
        at org.apache.hudi.HoodieMergeOnReadRDD$RecordMergingFileIterator.hasNext(HoodieMergeOnReadRDD.scala:271)
        at org.apache.hudi.HoodieMergeOnReadRDD$RecordMergingFileIterator.hasNext(HoodieMergeOnReadRDD.scala:271) 
.
.
.
.{code}
this repeats for some time and the jobs fails eventually. 

 

Likely the root cause is, in our iterator, if we encounter a delete record, we call hasNext() so that we skip current one and go to next. But this creates a call function in stack and so if this repeats for 8k or more times and if stack size in lesser in the corresponding jvm, our test will fail. In reality, there could be million delete records too. so, we need to find a way to fix this. For now, we are experimenting around ""-Xss100m"" java option temporarily to increase the stack size in the jvm. 

 

Code snippet from HoodieMORRDD

especially the line 

```

if (mergedAvroRecordOpt.isEmpty) { // Record has been deleted, skipping this.hasNext

``` 

in below snippet. 

 
{code:java}
override def hasNext: Boolean = {
  if (baseFileIterator.hasNext) {
    val curRowRecord = baseFileIterator.next()
    val curKey = curRowRecord.getString(recordKeyOrdinal)
    val updatedRecordOpt = removeLogRecord(curKey)
    if (updatedRecordOpt.isEmpty) {
      // No merge needed, load current row with required projected schema
      recordToLoad = unsafeProjection(projectRowUnsafe(curRowRecord, requiredSchema.structTypeSchema, requiredSchemaFieldOrdinals))
      true
    } else {
      val mergedAvroRecordOpt = merge(serialize(curRowRecord), updatedRecordOpt.get)
      if (mergedAvroRecordOpt.isEmpty) {
        // Record has been deleted, skipping
        this.hasNext
      } else {
        // NOTE: In occurrence of a merge we can't know the schema of the record being returned, b/c
        //       record from the Delta Log will bear (full) Table schema, while record from the Base file
        //       might already be read in projected one (as an optimization).
        //       As such we can't use more performant [[projectAvroUnsafe]], and instead have to fallback
        //       to [[projectAvro]]
        val projectedAvroRecord = projectAvro(mergedAvroRecordOpt.get, requiredAvroSchema, recordBuilder)
        recordToLoad = unsafeProjection(deserialize(projectedAvroRecord))
        true
      }
    }
  } else {
    super[LogFileIterator].hasNext
  } {code}"	HUDI	Closed	1	1	1904	pull-request-available
13418743	[Phase 1] Unify MOR table access across Spark, Hive	"This is Phase 1 of what outlined in HUDI-3081

 

The goal is 
 * Unify Hive’s RecordReaders (`RealtimeCompactedRecordReader`, {{{}RealtimeUnmergedRecordReader{}}})
 ** _These Readers should only differ in the way they handle the payload, everything else should remain constant_
 * Abstract w/in common component (name TBD)
 ** Listing current file-slices at the requested instant (handling the timeline)
 ** Creating Record Iterator for the provided file-slice"	HUDI	Closed	1	4	1904	pull-request-available
13440699	Add guide page for data-skipping 	Maybe under https://hudi.apache.org/docs/next/performance	HUDI	Closed	2	3	1904	pull-request-available
13328900	Support Metadata Table in Spark DataSource	"MT exposed as Spark DataSource should provide for following interface:
{code:java}
Columnar interface to MT:
             - Filename
               - Already available in the payload
             - Partition Path (?)
                - We can decode on the fly (we have list of partitions, so we can match it with the key)
             - Col A stats
             - Col B stats
             - ... {code}"	HUDI	Closed	1	3	1904	pull-request-available
13429410	Refactor Spark Relations to avoid code duplication	Currently, there's a great deal of duplication b/w COW/MOR implementations of Spark Relations leading to necessity to replicate same fixes multiple times (as in case of HUDI-3396)	HUDI	Closed	1	1	1904	pull-request-available
13430581	Leverage MT Column-stats Index in HoodieFileIndex	After HUDI-1296 is implemented, we can now leverage MT from Spark's HoodieFileIndex for the purposes of enabling DataSkipping in lieu of bespoke Column-stats index implementation	HUDI	Closed	1	4	1904	pull-request-available
13522141	Metadata Bootstrap flow resulting in NPE	"After adding a simple statement forcing the test to read whole bootstrapped table:
{code:java}
sqlContext.sql(""select * from bootstrapped"").show(); {code}
 

Following NPE have been observed on master (testBulkInsertsAndUpsertsWithBootstrap):
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 183.0 failed 1 times, most recent failure: Lost task 0.0 in stage 183.0 (TID 971, localhost, executor driver): java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:109)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_1$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:256)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:836)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:836)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)Driver stacktrace:    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:52)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3389)
    at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2550)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)
    at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:751)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:710)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:719)
    at org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer.testBulkInsertsAndUpsertsWithBootstrap(TestHoodieDeltaStreamer.java:669)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
    at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:440)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.Iterator.forEachRemaining(Iterator.java:116)
    at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:109)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_1$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:448)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:256)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:836)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:836)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748) {code}"	HUDI	Closed	1	1	1904	pull-request-available
13430555	Clustering fails with updating col stats when disabling metadata table	[https://github.com/apache/hudi/issues/4889]	HUDI	Closed	1	1	1904	hudi-on-call, pull-request-available
13440234	Fallback to HadoopFsRelation for non-sophisticated COW use-cases	While we figure out best way forward on HUDI-3896, we should fallback to `HadoopFsRelation` to make sure that all of the Spark optimizations are applicable to Hudi COW/Read Optimized tables as well	HUDI	Closed	1	1	1904	pull-request-available
13443235	Avoid invoking `getDataSize` in the hot-path	`getDataSize` has non-trivial overhead of traversing already encoded Column Groups stored in memory. We should sample its invocations to amortize its costs.	HUDI	Closed	1	4	1904	pull-request-available
13303109	parquet schema conflict: optional binary <some-field> (UTF8) is not a group	"When dealing with struct types like this

{code:json}
{
  ""type"": ""struct"",
  ""fields"": [
    {
      ""name"": ""categoryResults"",
      ""type"": {
        ""type"": ""array"",
        ""elementType"": {
          ""type"": ""struct"",
          ""fields"": [
            {
              ""name"": ""categoryId"",
              ""type"": ""string"",
              ""nullable"": true,
              ""metadata"": {}
            }
          ]
        },
        ""containsNull"": true
      },
      ""nullable"": true,
      ""metadata"": {}
    }
  ]
}
{code}

The second ingest batch throws that exception:


{code}
ERROR [Executor task launch worker for task 15] commit.BaseCommitActionExecutor (BaseCommitActionExecutor.java:264) - Error upserting bucketType UPDATE for partition :0
org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed
	at org.apache.hudi.table.action.commit.CommitActionExecutor.handleUpdateInternal(CommitActionExecutor.java:100)
	at org.apache.hudi.table.action.commit.CommitActionExecutor.handleUpdate(CommitActionExecutor.java:76)
	at org.apache.hudi.table.action.deltacommit.DeltaCommitActionExecutor.handleUpdate(DeltaCommitActionExecutor.java:73)
	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.handleUpsertPartition(BaseCommitActionExecutor.java:258)
	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.handleInsertPartition(BaseCommitActionExecutor.java:271)
	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.lambda$execute$caffe4c4$1(BaseCommitActionExecutor.java:104)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed
	at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:143)
	at org.apache.hudi.table.action.commit.CommitActionExecutor.handleUpdateInternal(CommitActionExecutor.java:98)
	... 34 more
Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:141)
	... 35 more
Caused by: org.apache.hudi.exception.HoodieException: operation has failed
	at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:227)
	at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:206)
	at org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:52)
	at org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:257)
	at org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:36)
	at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:121)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.lang.ClassCastException: optional binary categoryId (UTF8) is not a group
	at org.apache.parquet.schema.Type.asGroupType(Type.java:207)
	at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:279)
	at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:232)
	at org.apache.parquet.avro.AvroRecordConverter.access$100(AvroRecordConverter.java:78)
	at org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter$ElementConverter.<init>(AvroRecordConverter.java:536)
	at org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter.<init>(AvroRecordConverter.java:486)
	at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:289)
	at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:141)
	at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:279)
	at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:141)
	at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)
	at org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)
	at org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)
	at org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)
	at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)
	at org.apache.hudi.client.utils.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:49)
	at org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:45)
	at org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:92)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	... 4 more
{code}

Parquet schema of the failing struct
{code}
optional group categoryResults (LIST) {
  repeated group array {
    optional binary categoryId (UTF8);
  }
}
{code}

When the leaf record has multiple fields the issue has gone. I assume that this issue relates to either parquet/avro. Following array of struct definition is handled fine withtout exception:

{code}
    optional group productResult (LIST) {
      repeated group array {
        optional binary productId (UTF8);
        optional boolean productImages;
        optional binary productShortDescription (UTF8);
      }
    }
{code}"	HUDI	Closed	2	1	1904	hudi-on-call, sev:critical, user-support-issues
13439976	Support SchemaPruning optimization for Hudi's own relations	"After migrating to Hudi's own Relation impls, we unfortunately broke off some of the optimizations that Spark apply exclusively for `HadoopFsRelation`.

 

While these optimizations could be perfectly implemented for any `FileRelation`, Spark is unfortunately predicating them on usage of HadoopFsRelation, therefore making them non-applicable to any of the Hudi's relations.

Proper longterm solutions would be fixing this in Spark and could be either of:
 # Generalizing such optimizations to any `FileRelation`
 # Making `HadoopFsRelation` extensible (making it non-case class)

 

One example of this is Spark's `SchemaPrunning` optimization rule (HUDI-3891): Spark 3.2.x is able to effectively reduce amount of data read via schema pruning (projecting read data) even for nested structs, however this optimization is predicated on the usage of `HadoopFsRelation`:

!Screen Shot 2022-04-16 at 1.46.50 PM.png|width=739,height=143!"	HUDI	Closed	1	1	1904	performance, pull-request-available
13447108	Fix incorrect partition schema being passed to HadoopFsRelation	"Currently incorrect partition schema could be passed to HadoopFsRelation, which affects Spark's optimizations for partitioned tables.

 "	HUDI	Closed	1	1	1904	pull-request-available
13510670	Fix Merge Into performance traps	Merge Into currently relies on SparkSqlTypedRecord abstraction doing for *every* field of *every* record lookup in the cache keyed by Avro's Schema incurring Schema.equals (bearing non-trivial overhead), leading to 95% of the compute being wasted on it.	HUDI	Closed	1	1	1904	performance, pull-request-available
13516536	Make sure CTAS always uses Bulk Insert	There's been a [regression|https://github.com/apache/hudi/pull/5178/files#diff-560283e494c8ba8da102fc217a2201220dd4db731ec23d80884e0f001a7cc0bcR117] where we're not propagating configuration properly b/w {{CreateHoodiTableAsSelectCommand}} and {{InsertIntoHoodieTableCommand}} resulting in CTAS essentially doing an insert when instead it can just do a Bulk Insert.	HUDI	Closed	1	1	1904	pull-request-available
13425273	ParquetUtils fails extracting Parquet Column Range Metadata	"[~manojpec] discovered following issue while testing MT flows, with {{TestHoodieBackedMetadata#testTableOperationsWithMetadataIndex}} failing with:

 
{code:java}
17400 [Executor task launch worker for task 240] ERROR org.apache.hudi.metadata.HoodieTableMetadataUtil  - Failed to read column stats for /var/folders/t7/kr69rlvx5rdd824m61zjqkjr0000gn/T/junit2402861080324269156/dataset/2016/03/15/44396fda-48db-4d10-9f47-275c39317115-0_0-101-234_0000003.parquet
java.lang.ClassCastException: org.apache.parquet.io.api.Binary$ByteArrayBackedBinary cannot be cast to java.lang.Integer
	at org.apache.hudi.common.util.ParquetUtils.convertToNativeJavaType(ParquetUtils.java:369)
	at org.apache.hudi.common.util.ParquetUtils.lambda$null$2(ParquetUtils.java:305)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.hudi.common.util.ParquetUtils.readRangeFromParquetMetadata(ParquetUtils.java:313)
	at org.apache.hudi.metadata.HoodieTableMetadataUtil.getColumnStats(HoodieTableMetadataUtil.java:878)
	at org.apache.hudi.metadata.HoodieTableMetadataUtil.translateWriteStatToColumnStats(HoodieTableMetadataUtil.java:858)
	at org.apache.hudi.metadata.HoodieTableMetadataUtil.lambda$createColumnStatsFromWriteStats$7e2376a$1(HoodieTableMetadataUtil.java:819)
	at org.apache.hudi.client.common.HoodieSparkEngineContext.lambda$flatMap$7d470b86$1(HoodieSparkEngineContext.java:134)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748) {code}"	HUDI	Closed	1	1	1904	pull-request-available
13480370	Fix ScalaTest not respecting Log4j2 configs	After recently addressing issues w/ Log4j2 configuration not being properly respected in Hudi's tests (XXX), we still however have an issue w/ configs not being picked up by ScalaTest tests.	HUDI	Closed	2	1	1904	pull-request-available
13510669	Fixing performance traps in CTAS	"We can improve on following aspects
 # Avoid column re-ordering in CTAS (not necessary)
 # Avoid dereferencing Dataset to RDD when meta-fields are disabled (instead we can simply add Projection adding these)"	HUDI	Closed	1	1	1904	performance, pull-request-available
13478073	Remove code duplicated over from Spark	"At present, a lot of code in `HoodieAnalysis` is unnecessarily duplicating the resolution logic from Spark that leads to interference w/ normal operations of Spark's Analyzer and leading to non-trivial issues (like HUDI-4503) when dealing w/ Spark or Spark SQL

 

We should minimize the amount of logic and code that is localized from Spark to Hudi to strictly necessary to either 
 # Address issues (alternative to upstreaming in Spark)
 # Back-port features (from newer Spark versions to older ones)

 "	HUDI	Closed	1	1	1904	pull-request-available
13484891	Spark Row-writing Bulk Insert produces incorrect Bloom Filter metadata	Troubleshooting duplicates issue w/ Abhishek Modi from Notion, we've found that the min/max record key stats are being currently persisted incorrectly into Parquet metadata, leading to duplicate records being produced in their pipeline after initial bulk-insert.	HUDI	Closed	1	1	1904	pull-request-available
13375691	Investigate if hive-sync works as expected in a quickstart environment	"Hive-Sync seems to be failing for few users as reported on slack, see an example here -> [https://apache-hudi.slack.com/archives/C4D716NPQ/p1619509938030000]

 

We need to investigate if this is a real issue"	HUDI	Open	3	1	1904	sev:critical
13415706	Address high small objects churn in Bulk Insert/Layout Optimization	"Based on findings in HUDI-2949, following needs to be addressed to reduce pressure on GC, and improve performance: 
 * Remove unnecessary `ArrayList` resizing (during Hilbert Curve mapping)
 * Avoid unnecessary boxing (during Hilbert Curve mapping)
 * (In Parquet) Avoid allocating `ByteBuffer`s in `compareTo` method invoked from `BinaryStatistics.updateStats` method (on every write to Parquet's `ColumnWriterBase`)
 * Avoid {{bytesToAvro}} / {{avroToBytes}} ser-de loop (due to use of {{{}OverwriteWithLatestAvroPayload{}}}, to be replaced w/ {{{}RewriteAvroPayload{}}})
 * Avoid re-allocating substrings (caching them) when fetching {{Path.getName}} (from{{ }}{{HoodieWrapperFileSystem.getBytesWritten)}}
 * Avoid allocating large deques by {{DefaultSizeEstimator.sizeEstimate}} (currently allocates 16 x 1024 default internal `ArrayDeque`) {{ }}{{}}"	HUDI	Resolved	1	3	1904	pull-request-available
13520122	Fixing Kryo classes registration for Spark	"Due to RFC-46 the profile of the data being serialized by Hudi had changed considerably: previously we're mostly passing around Avro payloads, while now we hold our own internal {{HoodieRecord}} implementations.

When classes are not explicitly registered w/ Kryo, it would have to serialize class fully qualified name (FQN) as id every time an object is serialized, which carries a lot of [unnecessary overhead|https://github.com/apache/hudi/pull/7026/files#diff-81f9b48f7f7e71b46ea8764c7d63e310c871895d03640ae93c81b09f38306acb].

To work this around in [#7026|https://github.com/apache/hudi/pull/7026] added {{HoodieSparkKryoRegistrar}} registering some of the commonly serialized Hudi classes. However, during rebasing/merging of the RFC-46 feature branch this changes have been partially reverted and so we need to restore that."	HUDI	Closed	1	1	1904	pull-request-available
13421844	Unify Hive's MOR `InputFormat` implementations (Parquet, HFile)	"Essentially, HIve's different MOR implementations should only differ in the file-format of the actual base-files written. 

 

Today, that's not the case: currently Hive's MOR `InputFormat` implementations inherit from their respective COW file-format counterparts (

`HoodieParquetInputFormat`, `HoodieHFileInputFormat`).

 

Instead we should unify both MOR impls to have common base class, which separate file-format specific impls would extend (Parquet, HFile), only overriding the `getRecordReader` method"	HUDI	Closed	1	4	1904	pull-request-available
13522991	Spark SQL re-lists Hudi table after every SQL operations	"Currently, after most DML operations in Spark SQL, Hudi invokes `Catalog.refreshTable`

Prior to Spark 3.2, this was essentially doing the following:
 # Invalidating relation cache (forcing next time for relation to be re-resolved, creating new FileIndex, listing files, etc)
 # Trigger cascading invalidation (re-caching) of the cached data (in CacheManager)

As of Spark 3.2 it now additionally does `LogicalRelation.refresh` for ALL tables (previously this was only done for Temporary Views), therefore entailing whole table to be re-listed again by triggering `FileIndex.refresh` which might be costly operation.

 

We should revert back to preceding behavior from Spark 3.1"	HUDI	Open	1	1	1904	pull-request-available
13415338	Experiment running Hudi services using jemalloc	"[Jemalloc|http://jemalloc.net/] is general purpose glibc `malloc` implementation replacing native version, aimed at battling memory fragmentation and providing richer profiling capabilities.

 

Given the nature of Hudi (it's memory intensive) we could try experimenting running it w/ jemalloc evaluating whether it could bring any performance benefits."	HUDI	Open	4	3	1904	performance
13436512	CI ignored test failure in TestDataSkippingUtils	"failure in 

TestDataSkippingUtils

was ignored. something to do with Junit in Scala maybe?

See the attached CI logs and search for `TestDataSkippingUtils`"	HUDI	Closed	1	1	1904	pull-request-available
13412984	Address issues w/ Z-order Layout Optimization	"During extensive testing following issues have been discovered, which we're planning to addres in the upcoming PR:
 * Data-skipping seq incorrectly handles cases when columns that are not Z-sorted are present in the query (it simply ignores this fact, while it should abandon pruning altogether[1])
 * Exception w/in file-pruning seq should not be affecting overall query (it should in the worst case fallback to full-scan)
 * Merging seq prefers records from the old Z-index table, while should prefer those from the new one.
 * After clustering columns change, Z-index should simply overwrite index (currently it actually does the opposite – it skips updating the index in case old and new tables diverge in schemas)
 * Incorrect type conversions (for ex, Decimal is converted to Double)

Additionally we're planning to beef up current Z-index implementation test-suite making sure that all critical flows of the Z-indexing have appropriate coverage.

[1] Actually, with more advanced analysis we could still prune the search space, but this requires substantial sophistication of the analysis conducted, which is beyond our current focus"	HUDI	Closed	1	3	1904	pull-request-available
13418299	Make sure that Compression Codec configuration is respected across the board	"Currently there are quite a few places where we assume GZip as the compression codec which is incorrect, given that this is configurable and users might actually prefer to use different compression codec.

Examples:

[HoodieParquetDataBlock|https://github.com/apache/hudi/pull/4333/files#diff-798a773c6eef4011aef2da2b2fb71c25f753500548167b610021336ef6f14807]"	HUDI	Open	3	1	1904	new-to-hudi
13420926	Extract common Hudi Table File Index implementation 	Extract common Hudi Table File Index implementation from Spark's `HoodieFileIndex`, to leverage common file indexing functionality across Spark/Hive	HUDI	Closed	1	4	1904	pull-request-available
13522854	Fix HoodiePruneFileSourcePartition missing to list non-partitioned tables	This results in this tables having incorrectly interpreted by Spark as empty (0 bytes) in its CBO analysis	HUDI	Closed	1	1	1904	pull-request-available
13434455	Clean up Column Stats Index introduced along with Spatial Curves Clustering	Bespoke implementation introduced along with Spatial Curves is not required anymore after HUDI-3514	HUDI	Closed	1	1	1904	pull-request-available
13415343	Evaluate rebasing Hudi's default compression from Gzip to Zstd	"Currently, having Gzip as a default we prioritize Compression/Storage cost at the expense of
 * Compute (on the {+}write-path{+}): about *30%* of Compute burned during bulk-insert in local benchmarks on Amazon Reviews dataset is Gzip (see below) 
 * Compute (on the {+}read-path{+}), as well as queries Latencies: queries scanning large datasets are likely to be compression-/CPU-bound (Gzip t/put is *3-4x* less than Snappy, Zstd, [EX|https://stackoverflow.com/a/56410326/3520840])

P.S Spark switched its default compression algorithm to Snappy [a while ago|https://github.com/apache/spark/pull/12256].

 

*EDIT*

We should actually evaluate putting in [zstd|https://engineering.fb.com/2016/08/31/core-data/smaller-and-faster-data-compression-with-zstandard/] instead of Snappy. It has compression ratios comparable to Gzip, while bringing in much better performance:

!image-2021-12-03-13-13-02-892.png!

[https://engineering.fb.com/2016/08/31/core-data/smaller-and-faster-data-compression-with-zstandard/]

 

 

 "	HUDI	Open	2	4	1904	pull-request-available
13522703	Fix performance gap in Bulk Insert row-writing path with enabled de-duplication	"Currently, in case flag {{hoodie.combine.before.insert}} is set to true and {{hoodie.bulkinsert.sort.mode}} is set to {{{}NONE{}}}, Bulk Insert Row Writing performance will considerably degrade due to the following circumstances
 * During de-duplication (w/in {{{}dedupRows{}}}) records in the incoming RDD would be reshuffled (by Spark's default {{{}HashPartitioner{}}}) based on {{(partition-path, record-key)}} into N partitions
 * In case {{BulkInsertSortMode.NONE}} is used as partitioner, no re-partitioning will be performed and therefore each Spark task might be writing into M table partitions
 * This in turn entails explosion in the number of created (small) files, killing performance and table's layout"	HUDI	Open	1	1	1904	pull-request-available
13440800	Add config to fallback to extracting Partition Values from Partition Path	"In the aftermath of HUDI-3204 fix, we want to add a fallback config to allow users to fallback to existing behavior to avoid pipelines breakages if they are already compensating this issue in some way in their setup.

This config would be false by default."	HUDI	Closed	1	1	1904	pull-request-available
13432084	Task Failed to Serialize due to ConcurrentModificationException	"Occasionally tests are observed failing with ConcurrentModificationException while iterating over some Map that is being serialized as part of the Spark closure serialization.

 

In this particular case ""Test Call run_clustering Procedure By Table"" (TestCallProcedure) was failing:

 
{code:java}
- Test Call run_clustering Procedure By Table *** FAILED ***
  java.util.concurrent.CompletionException: org.apache.spark.SparkException: Task not serializable
  at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
  at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
  at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
  at java.lang.Thread.run(Thread.java:750)
  ...
  Cause: org.apache.spark.SparkException: Task not serializable
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:403)
  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:393)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)
  at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)
  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)
  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)
  ...
  Cause: java.util.ConcurrentModificationException:
  at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)
  at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:742)
  at java.util.HashSet.writeObject(HashSet.java:287)
  at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  ...
 {code}
 

 "	HUDI	Closed	1	1	1904	pull-request-available
13436303	Fix translation of isNotNull predicates in Data Skipping	"Right now isNotNull predicates are translated incorrectly 
{code:java}
// Filter ""colA is not null""
// Translates to ""colA_num_nulls = 0"" for index lookup
case IsNotNull(attribute: AttributeReference) =>
  getTargetIndexedColumnName(attribute, indexSchema)
    .map(colName => EqualTo(genColNumNullsExpr(colName), Literal(0))) {code}
Instead of ""colA_num_nulls = 0"" the condition should be ""colA_num_nulls != colA_numRecords"""	HUDI	Closed	1	1	1904	pull-request-available
13476035	SQLConf is not propagated correctly into RDDs	"There were a few reports and slack as well as in GI, related to Spark SQL configs not being respected by DeltaStreamer while working perfectly fine when leveraging DataSource API:

[https://github.com/apache/hudi/issues/6278]

 

I was able to trace these down to
 # `HoodieSparkUtils.createRDD` instantiating `AvroSerializer` which uses SQLConf that isn't propagated by Spark properly."	HUDI	Closed	1	1	1904	pull-request-available
13470202	Bulk Insert not URL encoding Partition Path properly	"Currently when using partition paths with slashes in it, Hudi lays out partitioned table incorrectly (see below):
!Screen Shot 2022-07-05 at 1.07.19 PM.png|width=623,height=206!"	HUDI	Closed	1	1	1904	pull-request-available
13386479	[UMBRELLA] Support space-filling curves in Hudi	supoort space curve to optimize the cluster of hudi file to improve query performance.	HUDI	Closed	1	15	1904	hudi-umbrellas
13449886	Fix in-memory HoodieData implementations to operate lazily	"Currently both `HoodieListData` and `HoodieMapPairData` operate eagerly on their payloads meaning that each transformation is immediately applied. 

This has following performance drawbacks:
 # It always executes full transformation regardless of whether the whole sequence will be required, potentially wasting quite a bit of compute.
 # It also might be the cause of OOMs if the sequence potentially could be larger than available memory (where caller might be relying on assumption that it would be performing stream processing)

 

Instead it should be rebased to hold `Stream`s internally and provide semantic close to Spark's RDD container."	HUDI	Closed	1	1	1904	pull-request-available
13468374	Make sure HoodieStorageConfig.PARQUET_WRITE_LEGACY_FORMAT_ENABLED could be specified by the writer	"Currently after [#4253|https://github.com/apache/hudi/pull/4253] no matter whether the user specified it explicitly or was relying on a default value this config value will be overridden (ie potentially reverting what have been specified by the user). As such, there's no way presently to enforce this config on the write side if your schema contains {{DecimalType}} fitting into the range.

Instead the behavior should be to only override _default_ config value in cases when there's a {{DecimalType}} of expected range, and if this config have been specified by the user -- it should not be overridden."	HUDI	Closed	1	1	1904	pull-request-available
13421199	Rebase Hive's FileInputFormat onto AbstractHoodieTableFileIndex	"There are multiple control flows that would require accurate re-mapping to start leveraging `AbstractHoodieTableFileIndex`
 # Snapshot Query mode
 # Incremental Query mode

This task would focus mostly on rebasing Snapshot Mode"	HUDI	Closed	1	4	1904	pull-request-available
13439718	"Bulk-insert w/ sort-mode ""NONE"" leads to file-sizing issues"	"Even after HUDI-3709, i still see that when writing partitioned-table file-sizing doesn't seem to be properly respected: in that case i was running ingestion job with following configs which was supposed to yield me ~100Mb files
{code:java}
Map(
  ""hoodie.parquet.small.file.limit"" -> String.valueOf(100 * 1024 * 1024),  // 100Mb
  ""hoodie.parquet.max.file.size""    -> String.valueOf(120 * 1024 * 1024)   // 120Mb
) {code}
 

Instead, my table contains a lot of very small (~1Mb) files: 

!Screen Shot 2022-04-14 at 1.08.19 PM.png|width=742,height=422!"	HUDI	Closed	1	1	1904	pull-request-available
13482923	Fix flaky  TestHoodieBackedTableMetadata.testMultiReaderForHoodieBackedTableMetadata	https://dev.azure.com/apache-hudi-ci-org/apache-hudi-ci/_build/results?buildId=11592&view=logs&j=600e7de6-e133-5e69-e615-50ee129b3c08&t=bbbd7bcc-ae73-56b8-887a-cd2d6deaafc7	HUDI	Closed	2	4	1904	pull-request-available
13393118	Refactor Spark datasource functional tests	Changing and running a test like HoodieSparkSQLWriterSuite is a huge pain. Modularize to reuse code as much as possible. common setup and tear down methods. For HoodieSparkSqlWriter, TestCOWDatasource, TestMORDataSource.	HUDI	Closed	3	3	3280	pull-request-available
13407322	Ability to clean up dangling data files using hudi-cli	"See https://github.com/apache/hudi/issues/3739

Scenario: commits archived but data files not cleaned up because cleaning frequency is lesser than tha of archival."	HUDI	Open	2	3	3280	sev:normal, user-support-issues
13521845	Hive sync using run_sync_tool fails due to NoClassDefFoundError	"run_sync_tool.sh \
>   --jdbc-url jdbc:hive2://hiveserver:10000 \
>   --user hive \
>   --pass hive \
>   --partitioned-by dt \
>   --base-path /user/hive/warehouse/stock_ticks_cow \
>   --database default \
>   --table stock_ticks_cow \
>   --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor

 

throws following exception
{code:java}
2023-01-28 06:31:18,143 INFO  [main] hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 0
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/KryoSerializable
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at org.apache.hudi.common.table.TableSchemaResolver.hasOperationField(TableSchemaResolver.java:481)
    at org.apache.hudi.util.Lazy.get(Lazy.java:54)
    at org.apache.hudi.common.table.TableSchemaResolver.getTableSchemaFromLatestCommitMetadata(TableSchemaResolver.java:231)
    at org.apache.hudi.common.table.TableSchemaResolver.getTableAvroSchemaInternal(TableSchemaResolver.java:199)
    at org.apache.hudi.common.table.TableSchemaResolver.getTableAvroSchema(TableSchemaResolver.java:139)
    at org.apache.hudi.common.table.TableSchemaResolver.getTableParquetSchema(TableSchemaResolver.java:179)
    at org.apache.hudi.sync.common.HoodieSyncClient.getStorageSchema(HoodieSyncClient.java:109)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:234)
    at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:173)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:161)
    at org.apache.hudi.hive.HiveSyncTool.main(HiveSyncTool.java:420)
Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.kryo.KryoSerializable
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 23 more {code}
*Works with version 0.12.2.*"	HUDI	Closed	1	1	3280	pull-request-available
13421094	HoodieConfig getBoolean method returns null instead of default value	If a config has default value then that should be returned instead of null.	HUDI	Closed	1	1	3280	pull-request-available
13401711	KVComparator in HFile for metadata table is tied to HBase version and shading	"There is no guarantee of compatibility between HFile reading/writing from different versions of ** HBase. For instance, HBase version 2.2.6 modified the comparator class stored in HFile metadata to {{org.apache.hadoop.hbase.KeyValue.KVComparator}} and {{org.apache.hadoop.hbase.KeyValue$KeyComparator}} no longer exists which may create issues when reading HFiles created by earlier versions of Hudi. And this comparator class is already marked as deprecated for {{CellComparatorImpl.}}

{{This also creates issues when shading HBase dependencies under Hudi namespace}}

{{Can we refactor HoodieKVComparator which is used in bootstrap index and have it applied for all use of HFile in Hudi. }}

{{[https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java#L580-L586]}}

 

There will need to be some logic as well for backward compatibility when reading metadata which was written from earlier version of Hudi as well. We can maybe enforce that user must perform an upgrade step beforehand which re-writes metadata base file

 

 "	HUDI	Resolved	1	4	3280	pull-request-available
13429357	Fix partition name in all code paths for LogRecordScanner	"partition name is required when metadata table is read (since we enable virtual keys for metadata). Chase all code paths for LogRecordScanner usages and ensure partition name is set in the builder. 

 

check out [this|https://github.com/apache/hudi/pull/4849] patch for example fix.

 "	HUDI	Closed	1	3	3280	pull-request-available
13568601	Meta sync does not consider clean commits while syncing partitions	Cleaner could not delete partitions but meta sync fails to drop partition in that case. This could cause query using engines that depend on catalog to fail.	HUDI	Closed	3	3	3280	pull-request-available
13411277	Validate metadata config for all readers	"Validate metadata config for all readers

 

Ensure default is false. and metadata is not enabled. 

When enabled, ensure metadata table is used for file listing. 

 

 "	HUDI	Resolved	1	3	3280	pull-request-available
13479761	Option read.streaming.skip_compaction skips delta commit	"Option read.streaming.skip_compaction was introduced to avoid consuming duplicate data from delta-commits and compactions in MOR table.

But the option may cause delta-commits, here the case:

Support we have a timeline (d for delta-commit, C for compaction/commit):

d1 --> d2 --> C3 --> d4 --> d5 -->

t1.......................................................t2..........

Let's say scans for streaming read happen at time t1 and t2, when d1 and d5 is the latest instant seperately. 

When we scan at t2 with read.streaming.skip_compaction=true, we get a latest merged fileslice with only log files containing d4+d5.  So d2 is skipped."	HUDI	Closed	2	1	3280	pull-request-available
13377097	clustering support for external index	We want to support records staying in same fileId after clustering (For example, sort each file OR remove a column from each file) 	HUDI	Closed	1	3	3280	pull-request-available
13380698	When clustering fail, generating unfinished replacecommit timeline.	"When clustering fail, generating unfinished replacecommit.
 Restart job will generate delta commit. if the commit contain clustering group file, the task will fail.
 ""Not allowed to update the clustering file group %s
 For pending clustering operations, we are not going to support update for now.""
 Need to ensure that the unfinished replacecommit file is deleted, or perform clustering first, and then generate delta commit."	HUDI	Closed	1	3	3280	pull-request-available
13521900	Enable schema reconciliation by default	Turn on schema reconciliation to allow wider/superset schema to be selected as write schema.	HUDI	Patch Available	1	3	3280	pull-request-available
13591865	Support pruning based on partition stats index in Hudi Flink	Hudi Flink should be able to use partition stats index from metadata table to do partition pruning.	HUDI	Closed	3	2	3280	pull-request-available
13510826	Close file reader wherever missing	"If not closed, open file handles could lead to 
{code:java}
java.io.InterruptedIOException: getFileStatus on s3a://bucket/base/path/274df949-03a5-4837-840f-a0b558b82827-0_0-9095-234238_20221206220929477.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool {code}"	HUDI	Closed	3	1	3280	pull-request-available
13420790	Support INDEX action for async metadata indexing	Add a new WriteOperationType and handle conflicts with concurrent writer or any other async table service. Implement the protocol in HUDI-2488	HUDI	Closed	1	2	3280	metadata, pull-request-available
13596563	Data skipping not working with RLI if record key is composite	"Data skipping with RLI is working when using one column as primary key but not working when primary key have multiple columns.

 

Code to Reproduce.

```
from pyspark.sql.functions import *

columns = [""ts"",""uuid"",""rider"",""driver"",""fare"",""city""]
data =[(1695159649087,""334e26e9-8355-45cc-97c6-c31daf0df330"",""rider-A"",""driver-K"",19.10,""san_francisco""),
(1695091554788,""e96c4396-3fad-413a-a942-4cb36106d721"",""rider-B"",""driver-L"",27.70 ,""san_francisco""),
(1695046462179,""9909a8b1-2d15-4d3d-8ec9-efc48c536a00"",""rider-C"",""driver-M"",33.90 ,""san_francisco""),
(1695516137016,""e3cf430c-889d-4015-bc98-59bdce1e530c"",""rider-C"",""driver-N"",34.15,""sao_paulo"")]


inserts = spark.createDataFrame(data).toDF(*columns)

hudi_options = {
'hoodie.table.name': TABLE_NAME,
'hoodie.datasource.write.recordkey.field' : 'uuid,rider',
'hoodie.datasource.write.precombine.field' : 'ts',
'hoodie.datasource.write.partitionpath.field': 'city',
'hoodie.index.type' : 'RECORD_INDEX',
'hoodie.metadata.record.index.enable' : 'true',
'hoodie.metadata.enable':'true',
'hoodie.enable.data.skipping':'true'
}

# Insert data
inserts.write.format(""hudi""). \
options(**hudi_options). \
mode(""overwrite""). \
save(PATH)

inserts.write.format(""hudi""). \
options(**hudi_options). \
mode(""append""). \
save(PATH)

spark.read.options(**hudi_options).format(""hudi"").load(PATH).where(""uuid = 'e3cf430c-889d-4015-bc98-59bdce1e530c' and rider = 'rider-C'"").show()
```

The read query above scanned both the files.

!image-2024-10-24-13-44-06-097.png!

But if we use below code - 

 

```
hudi_options = {
'hoodie.table.name': TABLE_NAME,
'hoodie.datasource.write.recordkey.field' : 'uuid',
'hoodie.datasource.write.precombine.field' : 'ts',
'hoodie.datasource.write.partitionpath.field': 'city',
'hoodie.index.type' : 'RECORD_INDEX',
'hoodie.metadata.record.index.enable' : 'true',
'hoodie.metadata.enable':'true',
'hoodie.enable.data.skipping':'true'
}

# Insert data
inserts.write.format(""hudi""). \
options(**hudi_options). \
mode(""overwrite""). \
save(PATH)

inserts.write.format(""hudi""). \
options(**hudi_options). \
mode(""append""). \
save(PATH)

spark.read.options(**hudi_options).format(""hudi"").load(PATH).where(""uuid = 'e3cf430c-889d-4015-bc98-59bdce1e530c'"").show()
```
with one key, read works as expected. 

!image-2024-10-24-13-45-03-581.png!"	HUDI	Patch Available	1	1	3280	pull-request-available
13375224	Upstream changes made in PrestoDB to eliminate file listing to Trino	"inputFormat.getSplits() code was optimized for PrestoDB code base. This change is not implemented / upstreamed in Trino.

 

Additionally, there are other changes that need to be upstreamed in Trino. "	HUDI	Closed	1	3	3280	sev:high, sev:triage
13594733	Improve functional index tests	"[https://github.com/apache/hudi/pull/12069#issuecomment-2400846952]

`TestFunctionalIndex` currently covers the following:
 # Index initialization and drop for cow/mor.
 # Index enable and disable for cow.
 # Index non-partitioned table and hive sync for mor.
 # Upsert after initialization for cow/mor.

We need to ensure that we cover the following cases too:
 # insert few records validate. update the same and validate updates are reflected. repeat the updates and validate stats.
for MOR, trigger compaction and validate.
 # trigger clustering on top of 1 and validate stats. a. for MOR, lets trigger clustering before compaction and also after compaction. ensure that no stats are available for the replaced file groups.
 # insert few records, update. and delete subset of records which should impact the min and max values. validate.
 # lets add a test for async compaction and validate. i.e. some log files are added to new phantom file slice and stats are intact.
 # lets have a test for non partitioned table.
 # lets trigger rollbacks and validate. i.e. insert, update (partially failed). validate that only stats pertianing to inserts are reflected. trigger a rollback and validate its still the same. retry the updates. stats should reflect stats w/ updated records.
 # lets add one long running tests. i.e with 20+ commits and aggressive cleaner and archival. just for sanity. or if we can enable all kinds of index in an existing sanity tests, I am good.
 # lets test all write operations. bulk_insert, insert, upsert, delete, insert_overwrite, insert_overwrite_table, delete_partition.
 # add a test for non partitioned dataset as well (for the unmerged log record reading flow)"	HUDI	In Progress	1	3	3280	pull-request-available
13399275	Make metadata tests lean and consistent	Make metadata tests lean and consistent using HoodieTestTable.	HUDI	Resolved	3	3	3280	pull-request-available
13470685	Support BULK_INSERT row-writing on streaming Dataset/DataFrame 	"With structured streaming setup, when Hudi table is written from a streaming source, then HoodieStreamingSink calls HoodieSparkSqlWriter.write(). If BULK_INSERT operation type is set, then HoodieSparkSqlWriter.write() internally calls HoodieSparkSqlWriter.bulkInsertAsRow() which does a simple df.write.format(""hudi"").options(...).save(). The 'write' call does not work on streaming Dataset/DataFrame.
{code:java}
org.apache.spark.sql.AnalysisException: 'write' can not be called on streaming Dataset/DataFrame
    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    at org.apache.spark.sql.Dataset.write(Dataset.scala:3377)
    at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:557)
    at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:178)
    at org.apache.hudi.HoodieStreamingSink.$anonfun$addBatch$2(HoodieStreamingSink.scala:91)
    at scala.util.Try$.apply(Try.scala:213)
    at org.apache.hudi.HoodieStreamingSink.$anonfun$addBatch$1(HoodieStreamingSink.scala:90)
    at org.apache.hudi.HoodieStreamingSink.retry(HoodieStreamingSink.scala:166)
    at org.apache.hudi.HoodieStreamingSink.addBatch(HoodieStreamingSink.scala:89) {code}
Bulk insert can still be done by not going via the row-writing path. But, we need to fix the HoodieStreamingSink to support bulk insert via row-writing."	HUDI	Closed	3	3	3280	pull-request-available, streaming
13579013	Upgrade Spark patch version to include a fix related to data correctness	https://issues.apache.org/jira/browse/SPARK-44805 shows data correctness issue with Spark 3.3.1 and 3.4.1. We have already upgraded to Spark 3.4.3 in [https://github.com/apache/hudi/commit/cdd146b2c73d50a28bee9f712b689df4fc923222.] We should upgrade to 3.3.4. The issue does not affect 3.2.x.	HUDI	Closed	3	4	3280	pull-request-available
13400355	Using HBase shaded jars in Hudi presto bundle 	"Execute {{hbase-server}} and {{hbase-client}} dependency in Hudi-presto-bundle.
Add {{hbase-shaded-client}} and {{hbase-shaded-server}} in Hudi-presto-bundle."	HUDI	Closed	1	1	3280	pull-request-available
13357670	Corrupted Avro schema extracted from parquet file	"we are running a HUDI deltastreamer on a very complex stream. Schema is deeply nested, with several levels of hierarchy (avro schema is around 6600 LOC).

 

The version of HUDI that writes the dataset if 0.5-SNAPTHOT and we recently started attempts to upgrade to the latest. Hovewer, latest HUDI can't read the provided dataset. Exception I get: 

 

 
{code:java}
Got exception while parsing the arguments:Got exception while parsing the arguments:Found recursive reference in Avro schema, which can not be processed by Spark:{  ""type"" : ""record"",  ""name"" : ""array"",  ""fields"" : [ {    ""name"" : ""id"",    ""type"" : [ ""null"", ""string"" ],    ""default"" : null  }, {    ""name"" : ""type"",    ""type"" : [ ""null"", ""string"" ],    ""default"" : null  }, {    ""name"" : ""exist"",    ""type"" : [ ""null"", ""boolean"" ],    ""default"" : null  } ]}          Stack trace:org.apache.spark.sql.avro.IncompatibleSchemaException:Found recursive reference in Avro schema, which can not be processed by Spark:{  ""type"" : ""record"",  ""name"" : ""array"",  ""fields"" : [ {    ""name"" : ""id"",    ""type"" : [ ""null"", ""string"" ],    ""default"" : null  }, {    ""name"" : ""type"",    ""type"" : [ ""null"", ""string"" ],    ""default"" : null  }, {    ""name"" : ""exist"",    ""type"" : [ ""null"", ""boolean"" ],    ""default"" : null  } ]}
 at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:75) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:89) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:82) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:81) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:81) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:82) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:81) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:81) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:82) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:81) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:81) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:89) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:82) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:81) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:81) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:82) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:81) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:81) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:105) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:82) at org.apache.spark.sql.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:81) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.spark.sql.avro.SchemaConverters$.toSqlTypeHelper(SchemaConverters.scala:81) at org.apache.spark.sql.avro.SchemaConverters$.toSqlType(SchemaConverters.scala:46) at org.apache.hudi.AvroConversionUtils$.convertAvroSchemaToStructType(AvroConversionUtils.scala:56) at org.apache.hudi.MergeOnReadSnapshotRelation.<init>(MergeOnReadSnapshotRelation.scala:67) at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:89) at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:53) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318) at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178) at com.css.dw.spark.SQLHudiOutputJob.run(SQLHudiOutputJob.java:118) at com.css.dw.spark.SQLHudiOutputJob.main(SQLHudiOutputJob.java:164) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845) at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161) at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184) at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86) at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
 

I wrote a simple test that opens parquet file, loads schema, and attempts to convert it into avro and it does fail with the same error. It appears that Avro schema that looked like:

 
{noformat}
{
          ""name"": ""entity_path"",
          ""type"": [
            ""null"",
            {
              ""type"": ""record"",
              ""name"": ""MenuEntityPath"",
              ""fields"": [
                {
                  ""name"": ""path_nodes"",
                  ""type"": [
                    ""null"",
                    {
                      ""type"": ""array"",
                      ""items"": {
                        ""type"": ""record"",
                        ""name"": ""PathNode"",
                        ""namespace"": ""Menue_pathPath$"",
                        ""fields"": [
                          {
                            ""name"": ""id"",
                            ""type"": [
                              ""null"",
                              {
                                ""type"": ""string"",
                                ""avro.java.string"": ""String""
                              }
                            ],
                            ""default"": null
                          },
                          {
                            ""name"": ""type"",
                            ""type"": [
                              ""null"",
                              {
                                ""type"": ""enum"",
                                ""name"": ""MenuEntityType"",
                                ""namespace"": ""shared"",
                                ""symbols"": [
                                  ""UNKNOWN""
                                ]
                              }
                            ],
                            ""default"": null
                          }
                        ]
                      }
                    }
                  ],
                  ""default"": null
                }
              ]
            }
          ],
          ""default"": null
        }
      ]
    }
  ],
  ""default"": null
},{noformat}
Is converted into:
{noformat}
[
  ""null"",
  {
    ""type"": ""record"",
    ""name"": ""entity_path"",
    ""fields"": [
      {
        ""name"": ""path_nodes"",
        ""type"": [
          ""null"",
          {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""record"",
              ""name"": ""array"",
              ""fields"": [
                {
                  ""name"": ""id"",
                  ""type"": [
                    ""null"",
                    ""string""
                  ],
                  ""default"": null
                },
                {
                  ""name"": ""type"",
                  ""type"": [
                    ""null"",
                    ""string""
                  ],
                  ""default"": null
                },
                {
                  ""name"": ""exist"",
                  ""type"": [
                    ""null"",
                    ""boolean""
                  ],
                  ""default"": null
                }
              ]
            }
          }
        ],
        ""default"": null
      },
      {
        ""name"": ""exist"",
        ""type"": [
          ""null"",
          ""boolean""
        ],
        ""default"": null
      }
    ]
  }
]{noformat}
A couple of questions: did anyone have similar issues and what is the best way forward?

 

Edit: 

I converted the dataset into pure parquet by using presto as an intermediary (create table as select). The result fails with a similar error, but in the different place:

 
{noformat}
Found recursive reference in Avro schema, which can not be processed by Spark:
{
  ""type"" : ""record"",
  ""name"" : ""bag"",
  ""fields"" : [ {
    ""name"" : ""array_element"",
    ""type"" : [ ""null"", {
      ""type"" : ""record"",
      ""name"" : ""array_element"",
      ""fields"" : [ {
        ""name"" : ""id"",{noformat}
it looks like the parquet writer replaces arrays with some synthetic records and gives them the same name.  

 

Also, Spark reader works. I can open the parquet file directly by using:
{noformat}
Dataset dataset = spark.read().parquet() {noformat}"	HUDI	Closed	1	1	3280	core-flow-ds, pull-request-available, sev:critical
13442338	Provide bundle jar options in each e2e test pipeline	Make integ test bundle slim and run tests w/ actual bundles	HUDI	Closed	3	3	3280	pull-request-available
13348541	Support incrementally reading clustering  commit via Spark Datasource/DeltaStreamer	now in DeltaSync.readFromSource() can  not read last instant as replace commit, such as clustering. 	HUDI	Resolved	1	3	3280	pull-request-available
13474421	Restore bundle name for spark3 profile	Run `mvn clean install -DskipTests -Dspark3 - Dscala-2.12` and it will generate hudi-spark3.3-bundle instead of hudi-spark3-bundle. Bundle name should not change as users depend on consistent naming.	HUDI	Closed	1	1	3280	pull-request-available
13559593	Support query for tables written as partitionBy but synced as non-partitioned	"In HUDI-7023, we added support to sync any table as non-partitioned table and yet be able to query via Spark with the same performance benefits of partitioned table.
This ticket extends the functionality end-to-end. If a user executes  `spark.write.format(""hudi"").options(options).partitionBy(partCol).save(basePath)`, then do logical partitioning and sync as non-partitioned table to the catalog. Yet be able to query efficiently,."	HUDI	Closed	3	3	3280	hudi-1.0.0-beta2, pull-request-available
13427440	Ensure immutable hudi configurations are set properly and not changed later	https://github.com/apache/hudi/pull/4714#discussion_r798474157	HUDI	Closed	1	4	3280	pull-request-available
13256787	Add JDBC Source support for Hudi DeltaStreamer	"Mirroring RDBMS to HUDI is one of the most basic use cases of HUDI. Hence, for such use cases, DeltaStreamer should provide inbuilt support.

DeltaSteamer should accept something like jdbc-source.properties where users can define the RDBMS connection properties along with a timestamp column and an interval which allows users to express how frequently HUDI should check with RDBMS data source for new inserts or updates.

Details are documented in RFC-14
https://cwiki.apache.org/confluence/display/HUDI/RFC+-+14+%3A+JDBC+incremental+puller"	HUDI	Closed	5	3	3280	pull-request-available, sev:normal
13425901	Conversion of write stats to metadata index records should use HoodieData throughout	HoodieMetadataTableUtil convertMetadataToRecords() converts all write stats to metadata index records to List of HoodieRecords before passing on them to engine specific commit() to prep records. This can OOM driver. We need to use HoodieData<HoodieRecords> throughout. 	HUDI	Closed	1	3	3280	pull-request-available
13436292	Hudi 0.10.1 raises exception if hoodie.write.lock.dynamodb.endpoint_url not provided	"[https://github.com/apache/hudi/issues/4904]

Issue in parsing a ddb config."	HUDI	Closed	3	1	3280	pull-request-available
13397067	Add tests for _hoodie_is_deleted functionality	https://github.com/apache/hudi/issues/3321	HUDI	Resolved	4	6	3280	pull-request-available
13446364	RFC for new Table APIs proposal for query engine integrations	Document all APIs.	HUDI	Patch Available	1	3	3280	pull-request-available
13547563	Support Record Index with the Async Indexer	Record index can be created using the async indexer if there are no inflight commits. With inflight commits, the catch-up task will [throw the UnsupportedException|https://github.com/apache/hudi/blob/616b663c19eb059438dc3a7f80c5c84b39a6ef6f/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java#L380-L382] as we don't have WriteStatus (which is needed to build record index) at that time. Hudi should be able to build the complete record index concurrently with ingestion (except for the limitations that currently exist for concurrent indexing).	HUDI	Closed	3	3	3280	pull-request-available
13577755	Initialize all indexes in parallel instead of computing type by type.	https://github.com/apache/hudi/pull/10352#discussion_r1584141789	HUDI	Open	2	3	3280	hudi-1.0.0-beta2
13577757	Move MDT partition type related logic in HoodieBackedTableMetadataWriter to MetadataPartitionType	https://github.com/apache/hudi/pull/10352#discussion_r1584129779	HUDI	Open	2	3	3280	hudi-1.0.0-beta2
13442034	ClassNotFoundException when using hudi-spark-bundle to write table with hbase index	"I ran a spark job and encountered several ClassNotFoundExceptions. spark version is 3.1 and scala version is 2.12.

1. 
{code:java}
java.lang.NoClassDefFoundError: org/apache/hudi/org/apache/hadoop/hbase/protobuf/generated/AuthenticationProtos$TokenIdentifier$Kind
     at org.apache.hudi.org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.translateException(RpcRetryingCallerImpl.java:222)
     at org.apache.hudi.org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:195)
     at org.apache.hudi.org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:395)
     at org.apache.hudi.org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:369)
     at org.apache.hudi.org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:108) {code}
including org.apache.hbase:hbase-protocol in packaging/hudi-spark-bundle/pom.xml can solve this error.

2.
{code:java}
 java.lang.ClassNotFoundException: org.apache.hudi.org.apache.hbase.thirdparty.com.google.gson.GsonBuilder
     at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
     at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
     at java.lang.ClassLoader.loadClass(ClassLoader.java:357) {code}
including org.apache.hbase.thirdparty:hbase-shaded-gson n packaging/hudi-spark-bundle/pom.xml can solve this error.

3.
{code:java}
 java.lang.ClassNotFoundException: Class org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener not found {code}
There is a configuration in hbase-site.xml 
{code:java}
<property>
  <name>hbase.status.listener.class&amp;amp;amp;amp;amp;lt;/name>
  <value>org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener</value>
  <description>
    Implementation of the status listener with a multicast message.
  </description>
</property> {code}
I set _*hbase.status.listener.class*_ to _*org.apache.hudi.org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener*_ in hbase configureation, the ClassNotFoundException has resolved, but get another exception
{code:java}
org.apache.hudi.org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=1, exceptions:
2022-08-26T07:12:57.603Z, RpcRetryingCaller{globalStartTime=2022-08-26T07:12:56.651Z, pause=100, maxAttempts=1}, org.apache.hudi.org.apache.hadoop.hbase.exceptions.ConnectionClosedException: Call to address=x.x.x.x/x.x.x.x:16020 failed on local exception: org.apache.hudi.org.apache.hadoop.hbase.exceptions.ConnectionClosedException: Connection closed    at org.apache.hudi.org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:146)
    at org.apache.hudi.org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hudi.org.apache.hadoop.hbase.exceptions.ConnectionClosedException: Call to address=x.x.x.x/x.x.x.x:16020 failed on local exception: org.apache.hudi.org.apache.hadoop.hbase.exceptions.ConnectionClosedException: Connection closed
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.IPCUtil.wrapException(IPCUtil.java:214)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:384)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:89)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:415)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:411)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:118)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.Call.setException(Call.java:133)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.cleanupCalls(NettyRpcDuplexHandler.java:203)
    at org.apache.hudi.org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelInactive(NettyRpcDuplexHandler.java:211)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:831)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at org.apache.hudi.org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    ... 1 more
Caused by: org.apache.hudi.org.apache.hadoop.hbase.exceptions.ConnectionClosedException: Connection closed
    ... 26 more{code}
I remove the relocations related to hbase in packaging/hudi-spark-bundle/pom.xml , the job succeed.

I checked the debug logs, but have no idea of the reason why ConnectionClosedException occurs when we use relocation."	HUDI	Closed	2	1	3280	pull-request-available
13532048	Avoid loading TableSchemaResolver until needed in reading bootstrap tables	While reading bootstrap tables, unnecessary cyles are spent instantiating table schema resolver to setup schema evolution context. 	HUDI	Open	3	3	3280	pull-request-available
13410160	[UMBRELLA] A new Trino connector for Hudi	This JIRA tracks all the tasks related to building a new Hudi connector in Trino.	HUDI	In Progress	2	15	3280	hudi-umbrellas
13357971	Issues w/ using hive metastore by disabling jdbc	Ref: https://github.com/apache/hudi/issues/1679	HUDI	Closed	3	3	3280	pull-request-available, sev:critical, user-support-issues
13481792	Bootstrap table from Deltastreamer cannot be read in Spark	" 
{code:java}
scala> val df = spark.read.format(""hudi"").load(""<bootstrap_table>"")
org.apache.hudi.exception.HoodieException: No files found for reading in user provided path.
  at org.apache.hudi.HoodieBootstrapRelation.buildFileIndex(HoodieBootstrapRelation.scala:167)
  at org.apache.hudi.HoodieBootstrapRelation.<init>(HoodieBootstrapRelation.scala:65)
  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:144)
  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:68)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
  at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
  ... 47 elided

{code}"	HUDI	Closed	1	1	3280	pull-request-available
13436063	Add metrics for async indexer	Add metrics for async metadata indexer, e.g. time for base file initialization, time for catch up etc.	HUDI	Closed	3	3	3280	pull-request-available
13475674	Cloudwatch reporter not being created with hudi-spark-bundle and hudi-aws-bundle in classpath	"spark-shell --jars hudi-spark3-bundle_2.12-0.13.0-SNAPSHOT.jar,hudi-aws-bundle-0.13.0-SNAPSHOT.jar --conf ""spark.serializer=org.apache.spark.serializer.KryoSerializer""   --conf ""spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog"" --conf ""spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension""


/** 
* DO df.write.format(""hudi"").options(**common_opts).option(""hoodie.metrics.on"", ""true"").option(""hoodie.metrics.reporter.type"", ""CLOUDWATCH"").save()
**/

java.lang.NoSuchMethodError: org.apache.hudi.aws.cloudwatch.CloudWatchReporter.forRegistry(Lorg/apache/hudi/com/codahale/metrics/MetricRegistry;)Lorg/apache/hudi/aws/cloudwatch/CloudWatchReporter$Builder;
  at org.apache.hudi.metrics.cloudwatch.CloudWatchMetricsReporter.createCloudWatchReporter(CloudWatchMetricsReporter.java:57)
  at org.apache.hudi.metrics.cloudwatch.CloudWatchMetricsReporter.<init>(CloudWatchMetricsReporter.java:47)
  at org.apache.hudi.metrics.MetricsReporterFactory.createReporter(MetricsReporterFactory.java:82)
  at org.apache.hudi.metrics.Metrics.<init>(Metrics.java:50)
  at org.apache.hudi.metrics.Metrics.init(Metrics.java:96)
  at org.apache.hudi.metrics.HoodieMetrics.<init>(HoodieMetrics.java:61)
  at org.apache.hudi.client.BaseHoodieWriteClient.<init>(BaseHoodieWriteClient.java:178)
  at org.apache.hudi.client.SparkRDDWriteClient.<init>(SparkRDDWriteClient.java:95)
  at org.apache.hudi.client.SparkRDDWriteClient.<init>(SparkRDDWriteClient.java:79)
  at org.apache.hudi.DataSourceUtils.createHoodieClient(DataSourceUtils.java:193)
  at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$write$13(HoodieSparkSqlWriter.scala:311)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:310)
  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:183)"	HUDI	Closed	1	1	3280	pull-request-available
13596499	Push down partition filters to functional index	Partition pruning key encodings we did for col stats, and only querying for latest file slices fix ([https://github.com/apache/hudi/pull/10493]), need to be applied for functional index as well.	HUDI	In Progress	1	3	3280	pull-request-available
13582722	Ensure 1.x commit instants in table version 6 are readable w/ 0.16.0 	"Ensure 1.x commit instants are readable w/ 0.16.0 reader.

 

May be we need to migrate HoodieInstant parsing logic to 0.16.0 in a backwards compatible manner. or its already ported. we just need to write tests and validate. 

[https://github.com/apache/hudi/pull/9617] - contains some portion (HoodieInstant changes and some method renames)"	HUDI	Closed	3	4	3280	pull-request-available
13593000	Fix bootstrap index type setting in metaClient	In HoodieTableMetaClient the payload class is wrongly set instead of bootstrap index class name here - https://github.com/apache/hudi/blob/5a171e9ef7cebf9000242905eb3a9e51fd04f911/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java#L1173	HUDI	Closed	3	1	3280	pull-request-available
13474802	Diff tool to compare metadata across snapshots in a given time range	A tool that diffs two snapshots at table and partition level and can give info about what new file ids got created, deleted, updated and track other changes that are captured in write stats. 	HUDI	Closed	1	3	3280	pull-request-available
13595156	Fix partition stats update with async comapction	In case of compaction we set isTightBound to true. Now, with async compction, it's possible that some deltacommits completed during compaction. And then compaction completed and we recomputed stats based on compaction metadata, which does not have any info about the commits that happened during compaction. In that case, the stats due to files added in those deltacommits while async compaction was in progress could be ignored.	HUDI	Closed	1	3	3280	pull-request-available
13437360	Fix BloomIndex incorrectly using ColStats to lookup records locations	"Currently, BloomIndex tries to rely solely on Column Stats to lookup records locations. This is however incorrect, since CS state might not be complete at any given moment; instead we should use it on the basis of best effort (not assuming that it would have any record at all), and for those files that are not found in ColStats we should list from them directly.

You can search in code for ""HUDI-3776"" to see exact code location this is related to"	HUDI	Closed	1	1	3280	pull-request-available
13551880	Integration tests using docker demo do not test with latest hudi-trino-bundle	Integration tests using docker demo do not test with latest hudi-trino-bundle. Check [https://github.com/apache/hudi/pull/9617#issuecomment-1732227136] for more details.	HUDI	Closed	3	6	3280	pull-request-available
13418036	spark.read fails when drop partitions columns is used w/o glob path	"With 0.9.0, we added support for dropping partition columns after generating the partition path for hoodie records. but looks like we have some gaps in end to end flow. 

Main issue is with not giving glob path. if proper glob path is given, it works. 

 

hudi serializes partition path fields to table properties and re-uses when in need. So, while querying the field may not be part of table schema since hudi would have removed the field (along with trimming the schema) completely. 

Stacktrace for Issue (1)
{code:java}
scala> val tripsSnapshotDF = spark.read.format(""hudi"").load(basePath)
java.lang.IllegalArgumentException: Cannot find column: 'partitionId' in the schema[StructField(_hoodie_commit_time,StringType,true),StructField(_hoodie_commit_seqno,StringType,true),StructField(_hoodie_record_key,StringType,true),StructField(_hoodie_partition_path,StringType,true),StructField(_hoodie_file_name,StringType,true),StructField(rowId,StringType,true),StructField(preComb,LongType,true),StructField(name,StringType,true),StructField(versionId,StringType,true),StructField(toBeDeletedStr,StringType,true),StructField(intToLong,IntegerType,true),StructField(longToInt,LongType,true)]
  at org.apache.hudi.HoodieFileIndex$$anonfun$5$$anonfun$apply$1.apply(HoodieFileIndex.scala:106)
  at org.apache.hudi.HoodieFileIndex$$anonfun$5$$anonfun$apply$1.apply(HoodieFileIndex.scala:106)
  at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
  at scala.collection.AbstractMap.getOrElse(Map.scala:59)
  at org.apache.hudi.HoodieFileIndex$$anonfun$5.apply(HoodieFileIndex.scala:106)
  at org.apache.hudi.HoodieFileIndex$$anonfun$5.apply(HoodieFileIndex.scala:105)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.hudi.HoodieFileIndex._partitionSchemaFromProperties$lzycompute(HoodieFileIndex.scala:105)
  at org.apache.hudi.HoodieFileIndex._partitionSchemaFromProperties(HoodieFileIndex.scala:99)
  at org.apache.hudi.HoodieFileIndex.getAllQueryPartitionPaths(HoodieFileIndex.scala:348)
  at org.apache.hudi.HoodieFileIndex.loadPartitionPathFiles(HoodieFileIndex.scala:420)
  at org.apache.hudi.HoodieFileIndex.refresh0(HoodieFileIndex.scala:214)
  at org.apache.hudi.HoodieFileIndex.<init>(HoodieFileIndex.scala:149)
  at org.apache.hudi.DefaultSource.getBaseFileOnlyView(DefaultSource.scala:199)
  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:116)
  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:67)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)
  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  ... 63 elided {code}
Steps to reproduce: 

[https://gist.github.com/nsivabalan/570a96004e41f84565c99d8994b12d57]

 "	HUDI	Open	3	1	3280	sev:normal, user-support-issues
13514350	Make sure Trino does not re-instantiates Hive's InputFormat for every partition during file listing	"To unblock 0.12.2, we've implemented a stop-gap falling back to FileSystemView-based listing (HUDI-5409).

This is not an appropriate long-term solution though, and we need to make sure we fix it properly by avoiding re-instantiating InputFormats w/in Trino itself (so that we can properly use the FileIndex and MT) "	HUDI	Closed	2	1	3280	pull-request-available
13592385	Fix partition stats with compaction or clustering	"Consider a partition with 10 file slices. If compaction triggered for 1 file slice fs1_1, the partition stats are updated for that file slice with the same key (partition path). The older partition stat record for that partition path would account for the other 9 file slices (fs2_0 - fs10_0) + the older stat (fs1_0). The final read value would be merging of all versions of file slices (fs2_0 - fs10_0, fs1_0, fs1_1). It should only account for the latest version of fs1.

Upon compaction or clustering, the partition stat should be recomputed and the older records for that partition should be invalidated.

Also add a validation test in org.apache.hudi.utilities.TestHoodieMetadataTableValidator#testPartitionStatsValidation"	HUDI	Closed	1	1	3280	pull-request-available
13591530	Fix colstats collection when record type is SPARK	For record type SPARK (introduced in RFC-46), nested fields, map and bytes type fields are not handled properly while building colstats metadata.	HUDI	Closed	1	3	3280	pull-request-available
13412095	Async Clustering via deltstreamer fails with IllegalStateException: Duplicate key [==>20211116123724586__replacecommit__INFLIGHT]	"Setup:

Started deltastreamer with parquet dfs source. source folder did not have any data as such. Enabled async clustering with below props

```

hoodie.clustering.async.max.commits=2

hoodie.clustering.plan.strategy.sort.columns=type,id

```

Added 1 file to the source folder. and deltastreamer failed during this. commit went through fine. looks like 1st replace commit also went through fine. but deltastreamer failed. I need to understand why deltastreamer tries to schedule a 2nd replace commit as well.  It runs in continuous mode and goes into next round immediately and there is no more data to sync. 

Note: there is only one partition and one file group in the entire dataset. 

 

clustering plan seems to be same in both replace commit requested meta files
{code:java}
^@&<93>c%^Z<F1><81>9%<E6>-^K<EF><AC><FC>A^B<BA>^G^B^NCLUSTER^B^B^B^B^B^B<EC>^Afile:/tmp/hudi-deltastreamer-gh-mw/PushEvent/2542ddef-0169-4978-9b1b-84977d6141cf-0_0-49-161_20211116130523827.parquet^B^@^BL2542ddef-0169-4978-9b1b-84977d6141cf-0^B^RPushEvent^B^@^@^B^@^B
^^TOTAL_LOG_FILES^@^@^@^@^@^@^@^@^VTOTAL_IO_MB^@^@^@^@^@^@^@^@ TOTAL_IO_READ_MB^@^@^@^@^@^@^@^@(TOTAL_LOG_FILES_SIZE^@^@^@^@^@^@^@^@""TOTAL_IO_WRITE_MB^@^@^@^@^@^@^@^@^@^@^B^@^B^@^B^B<A0>^Aorg.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy^B^BXhoodie.clustering.plan.strategy.sort.columns^Ntype,id^@^@^B^B^@^@^B^B^A^B^@^@^B&<93>c%^Z<F1><81>9%<E6>-^K<EF><AC><FC>A{code}
 
{code:java}
^@<FB>^L%b<C3>3<85><D7><<BB><A3><B1><BA>
<A9><89>^B<BA>^G^B^NCLUSTER^B^B^B^B^B^B<EC>^Afile:/tmp/hudi-deltastreamer-gh-mw/PushEvent/2542ddef-0169-4978-9b1b-84977d6141cf-0_0-49-161_20211116130523827.parquet^B^@^BL2542ddef-0169-4978-9b1b-84977d6141cf-0^B^RPushEvent^B^@^@^B^@^B
^^TOTAL_LOG_FILES^@^@^@^@^@^@^@^@^VTOTAL_IO_MB^@^@^@^@^@^@^@^@ TOTAL_IO_READ_MB^@^@^@^@^@^@^@^@(TOTAL_LOG_FILES_SIZE^@^@^@^@^@^@^@^@""TOTAL_IO_WRITE_MB^@^@^@^@^@^@^@^@^@^@^B^@^B^@^B^B<A0>^Aorg.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy^B^BXhoodie.clustering.plan.strategy.sort.columns^Ntype,id^@^@^B^B^@^@^B^B^A^B^@^@^B<FB>^L%b<C3>3<85><D7><<BB><A3><B1><BA>
<A9><89> {code}
 

timeline

!Screen Shot 2021-11-16 at 12.42.20 PM.png!

 

stacktrace:
{code:java}
21/11/16 13:05:20 WARN HoodieDeltaStreamer: Next round 
21/11/16 13:05:20 WARN DeltaSync: Extra metadata :: 20211116130512915, 20211116130512915.commit, = [schema, deltastreamer.checkpoint.key]
21/11/16 13:05:23 WARN HoodieDeltaStreamer: Starting async clustering service if required 111 
21/11/16 13:05:27 WARN HoodieDeltaStreamer: Scheduled async clustering for instant: 20211116130526895
21/11/16 13:05:27 WARN HoodieDeltaStreamer: Next round 
21/11/16 13:05:27 WARN DeltaSync: Extra metadata :: 20211116130523827, 20211116130523827.commit, = [schema, deltastreamer.checkpoint.key]
21/11/16 13:05:27 WARN HoodieDeltaStreamer: Scheduled async clustering for instant: 20211116130527394
21/11/16 13:05:27 WARN HoodieDeltaStreamer: Next round 
21/11/16 13:05:27 WARN DeltaSync: Extra metadata :: 20211116130523827, 20211116130523827.commit, = [schema, deltastreamer.checkpoint.key]
21/11/16 13:05:28 ERROR Executor: Exception in task 0.0 in stage 74.0 (TID 176)
java.lang.IllegalStateException: Duplicate key [==>20211116130526895__replacecommit__INFLIGHT]
	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133)
	at java.util.HashMap.merge(HashMap.java:1254)
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320)
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.hudi.common.util.ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(ClusteringUtils.java:127)
	at org.apache.hudi.common.table.view.AbstractTableFileSystemView.init(AbstractTableFileSystemView.java:113)
	at org.apache.hudi.common.table.view.HoodieTableFileSystemView.init(HoodieTableFileSystemView.java:106)
	at org.apache.hudi.common.table.view.HoodieTableFileSystemView.<init>(HoodieTableFileSystemView.java:100)
	at org.apache.hudi.common.table.view.FileSystemViewManager.createInMemoryFileSystemView(FileSystemViewManager.java:168)
	at org.apache.hudi.common.table.view.FileSystemViewManager.lambda$createViewManager$5fcdabfe$1(FileSystemViewManager.java:259)
	at org.apache.hudi.common.table.view.FileSystemViewManager.lambda$getFileSystemView$1(FileSystemViewManager.java:111)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at org.apache.hudi.common.table.view.FileSystemViewManager.getFileSystemView(FileSystemViewManager.java:110)
	at org.apache.hudi.table.HoodieTable.getSliceView(HoodieTable.java:277)
	at org.apache.hudi.table.action.cluster.strategy.ClusteringPlanStrategy.getFileSlicesEligibleForClustering(ClusteringPlanStrategy.java:77)
	at org.apache.hudi.client.clustering.plan.strategy.SparkSizeBasedClusteringPlanStrategy.getFileSlicesEligibleForClustering(SparkSizeBasedClusteringPlanStrategy.java:118)
	at org.apache.hudi.table.action.cluster.strategy.PartitionAwareClusteringPlanStrategy.lambda$generateClusteringPlan$4e6aac78$1(PartitionAwareClusteringPlanStrategy.java:79)
	at org.apache.hudi.client.common.HoodieSparkEngineContext.lambda$flatMap$7d470b86$1(HoodieSparkEngineContext.java:134)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/11/16 13:05:28 WARN TaskSetManager: Lost task 0.0 in stage 74.0 (TID 176, localhost, executor driver): java.lang.IllegalStateException: Duplicate key [==>20211116130526895__replacecommit__INFLIGHT] {code}
 

I tried adding a 10 sec delay in continuous mode and things were fine. Within the 10 sec delay, clustering completes and so next round does do trigger any scheduling. 

When I tried w/ 3 sec delay, ran into the same exception as above. I see 2nd time scheduling happens. 

 

 

 

 

 

 

 "	HUDI	Closed	1	4	3280	core-flow-ds, pull-request-available, sev:high
13595146	Avoid glob paths and use the log record reader to build functonal index	"To buil functional index in Spark, the spark-sql functions are applied over Dataset<Row> which are loaded using glob paths here - [https://github.com/apache/hudi/blob/7530e4fa48fb6c32e9cafb587914521bbbb4bc23/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkMetadataWriterUtils.java#L164]

There might some inefficiencies with glob path due to listing. Hence, use the usual HoodieUnMergedLogRecordReader to load the dataset as HoodieRecord, and fetch explicit col values of interest (rquired for functional index) and attache file name and create a Row directly (RowFactory may be). And then create a dataset out of it and apply the functional index over it."	HUDI	Closed	1	3	3280	pull-request-available
13584881	Partitions not created correctly with SQL when multiple partitions specified out of order	"When multiple partitions are specified out of order (as compared to the order of fields in the create table command), the partitioning on storage is incorrect. Test script (notice that create table or insert into command has city and then state, while the partitioned by clause has state first and then city):
{code:java}
DROP TABLE IF EXISTS hudi_table_mlp;

CREATE TABLE hudi_table_mlp (    
  ts BIGINT,    
  id STRING,    
  rider STRING,    
  driver STRING,    
  fare DOUBLE,    
  city STRING,    
  state STRING) 
USING HUDI options(    
  primaryKey ='id',    
  preCombineField = 'ts')
PARTITIONED BY (state, city)location 'file:///tmp/hudi_table_mlp';

INSERT INTO hudi_table_mlp VALUES (1695159649,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-A','driver-K',19.10,'san_francisco','california');
INSERT INTO hudi_table_mlp VALUES (1695091554,'e96c4396-3fad-413a-a942-4cb36106d721','rider-C','driver-M',27.70,'sunnyvale','california');
INSERT INTO hudi_table_mlp VALUES (1695332066,'1dced545-862b-4ceb-8b43-d2a568f6616b','rider-E','driver-O',93.50,'austin','texas');
INSERT INTO hudi_table_mlp VALUES (1695516137,'e3cf430c-889d-4015-bc98-59bdce1e530c','rider-F','driver-P',34.15,'houston','texas'); {code}
This creates partition as follows (note that city and state values are swapped):

!Screenshot 2024-07-11 at 5.43.41 PM.png|width=737,height=335!

Now, if i query with state='texas' filter, there are no results:
{code:java}
spark-sql> select * from hudi_table_mlp where state='texas'; -- no results --
Time taken: 0.356 seconds {code}
I have tested this with master, 0.15.0 and 0.14.1, so it's not a recent regression.

 "	HUDI	Closed	3	1	3280	pull-request-available, spark-sql
13552984	Fix some integration test failures	"{code:java}
2023-10-04T12:20:50.9713320Z [ERROR] Failures: 2023-10-04T12:20:50.9752255Z [ERROR]   ITTestHoodieDemo.testParquetDemo:138->testHiveAfterSecondBatchAfterCompaction:417->ITTestBase.assertStdOutContains:374 Did not find output the expected number of times. ==> expected: <4> but was: <2>2023-10-04T12:20:50.9756538Z [ERROR]   ITTestHoodieSanity.testRunHoodieJavaAppOnMultiPartitionKeysMORTable:110->testRunHoodieJavaApp:178->ITTestBase.executeCommandStringInDocker:263->ITTestBase.executeCommandStringInDocker:274->ITTestBase.executeCommandInDocker:197->ITTestBase.executeCommandInDocker:246 Command ([/var/hoodie/ws/hudi-spark-datasource/hudi-spark/run_hoodie_app.sh, --hive-sync, --table-path, hdfs://namenode/docker_hoodie_multi_partition_key_mor_test_20231004120229187, --hive-url, jdbc:hive2://hiveserver:10000, --table-type, MERGE_ON_READ, --hive-table, docker_hoodie_multi_partition_key_mor_test_20231004120229187, --use-multi-partition-keys]) expected to succeed. Exit (1) ==> expected: <0> but was: <1>2023-10-04T12:20:50.9762292Z [ERROR]   ITTestHoodieSanity.testRunHoodieJavaAppOnMultiPartitionKeysMORTable:110->testRunHoodieJavaApp:178->ITTestBase.executeCommandStringInDocker:263->ITTestBase.executeCommandStringInDocker:274->ITTestBase.executeCommandInDocker:197->ITTestBase.executeCommandInDocker:246 Command ([/var/hoodie/ws/hudi-spark-datasource/hudi-spark/run_hoodie_streaming_app.sh, --hive-sync, --table-path, hdfs://namenode/docker_hoodie_multi_partition_key_mor_test_20231004120323761, --hive-url, jdbc:hive2://hiveserver:10000, --table-type, MERGE_ON_READ, --hive-table, docker_hoodie_multi_partition_key_mor_test_20231004120323761, --use-multi-partition-keys, --streaming-source-path, hdfs://namenode/streaming/source/docker_hoodie_multi_partition_key_mor_test_20231004120323761, --streaming-checkpointing-path, hdfs://namenode/streaming/ckpt/docker_hoodie_multi_partition_key_mor_test_20231004120323761]) expected to succeed. Exit (255) ==> expected: <0> but was: <255>2023-10-04T12:20:50.9766642Z [ERROR]   ITTestHoodieSanity.testRunHoodieJavaAppOnSinglePartitionKeyMORTable:96->testRunHoodieJavaApp:238->testRunHoodieJavaApp:178->ITTestBase.executeCommandStringInDocker:263->ITTestBase.executeCommandStringInDocker:274->ITTestBase.executeCommandInDocker:197->ITTestBase.executeCommandInDocker:246 Command ([/var/hoodie/ws/hudi-spark-datasource/hudi-spark/run_hoodie_app.sh, --hive-sync, --table-path, hdfs://namenode/docker_hoodie_single_partition_key_mor_test_20231004115759538, --hive-url, jdbc:hive2://hiveserver:10000, --table-type, MERGE_ON_READ, --hive-table, docker_hoodie_single_partition_key_mor_test_20231004115759538]) expected to succeed. Exit (1) ==> expected: <0> but was: <1>2023-10-04T12:20:50.9768468Z [INFO] 2023-10-04T12:20:50.9768820Z [ERROR] Tests run: 11, Failures: 4, Errors: 0, Skipped: 1 {code}"	HUDI	Closed	1	6	3280	pull-request-available
13549760	Make multiple base file formats within each file group.	Ability to mix different types of base files within a single table or even a single file group (e.g images, json, vectors ...)	HUDI	Closed	3	3	3280	pull-request-available
13404801	[UMBRELLA] Seamless meta sync	"Hudi to Hive sync is a common use case which enables querying Hudi tables through other query engines that support hive connector such as Presto and Trino. Currently, Hudi supports syncing to Hive asynchronously using run_sync_tool or synchronously through deltastreamer.

The goal of this umbrella JIRA is to imrpove the current sync mechanism and support Hive3. Additionally, we need to improve the documentation around different configs and sync modes. "	HUDI	Open	3	15	3280	hive, hive3, hudi-umbrellas
13556532	Support querying without syncing partition metadata to catalog	Show how we can now reduce load on the HMS/Glue Catalog by registering just non-partitioned tables.	HUDI	Closed	3	3	3280	pull-request-available
13568147	Align MDT cleaner configs with the data table	Metadata table should retain at least as much history as data table. Follow similar policy as data table and set retention to 1.2x for metadata table.	HUDI	Closed	3	3	3280	pull-request-available
13428434	Fix out of sync Clustering config properties 	"Hoodie Clustering config describes the config property as [https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java#L191|https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java#L191].]. However, the HoodieSparkSqlWriter.scala looks at [https://github.com/apache/hudi/blob/master/hudi-spark-datasource/hudi-spark-common/src/main/scala/org/apache/hudi/DataSourceOptions.scala#L595] which differs by Key. 

 

The configs needs to be unified under single source file to prevent this mismatch.

For now work around is to set both configs to true to enable async clustering.

 

 "	HUDI	Closed	2	1	3280	pull-request-available
13438392	upgrade spring cve-2022-22965	"We should upgrade the Spring Framework version at Hudi CLI because of cve-2022-22965. The Qualys Scanner finds these packages and raises a warning because of the existence of these files on the system. 

The found files are:
/usr/lib/hudi/cli/lib/spring-beans-4.2.4.RELEASE.jar /usr/lib/hudi/cli/lib/spring-core-4.2.4.RELEASE.jar

More Information: 
Spring Framework: https://spring.io/projects/spring-framework
Spring project spring-framework release notes: https://github.com/spring-projects/spring-framework/releases
CVE-2022-22965: https://tanzu.vmware.com/security/cve-2022-22965"	HUDI	Patch Available	2	1	3280	pull-request-available
13577885	Support query hint to inject indexes in query plans	[Hints|https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html] give users a way to suggest how SQL to use specific approaches to generate its execution plan. Simply creating the index, such as functional index or secondary index, does not necessarily ensure its usage in the query planning. While we have hierarchy of index to use in `HoodieFileIndex`, we want a way for users to provide explicitly to use some specific index (for instantance during index join) to use while planning.	HUDI	Open	3	3	3280	hudi-1.0.0-beta2
13396161	Only include meta fields to reorder while preparing dataset for bulk insert	"Below filter in `HoodieDatasetBulkInsertHelper` will result in `_hoodie_is_deleted` to be reordered as well even though it is not part of meta columns. 

{code:java}
List<Column> originalFields =
        Arrays.stream(rowsWithMetaCols.schema().fields()).filter(field -> !field.name().contains(""_hoodie_"")).map(f -> new Column(f.name())).collect(Collectors.toList());

    List<Column> metaFields =
        Arrays.stream(rowsWithMetaCols.schema().fields()).filter(field -> field.name().contains(""_hoodie_"")).map(f -> new Column(f.name())).collect(Collectors.toList());
{code}

The fix is to check only for `HoodieRecord.HOODIE_META_COLUMNS_WITH_OPERATION`."	HUDI	Resolved	3	1	3280	pull-request-available
13417265	Enable metadata file listing for Presto directory lister	"lets the PR landed. with numbers showing that queries stabilize after a while (w/o metadata) 
https://github.com/prestodb/presto/pull/17084"	HUDI	Closed	1	3	3280	pull-request-available
13454447	Hive connector in Presto returns duplicate records after clustering	"When querying the Hudi table using Hive connector in Presto after a cluster action is complete in the table, the query result contains duplicate records.

Environment: Presto 0.274-SNAPSHOT (latest), Hudi 0.11

Steps to reproduce:

Write Hudi table with clustering
{code:java}
./bin/spark-shell  \
     --master yarn \
     --deploy-mode client \
     --driver-memory 8g \
     --executor-memory 8g \
     --num-executors 20 \
     --executor-cores 4 \
     --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.11.1 \
     --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
     --conf spark.kryoserializer.buffer=256m \
     --conf spark.kryoserializer.buffer.max=1024m \
     --conf ""spark.driver.defaultJavaOptions=-XX:+UseG1GC"" \
     --conf ""spark.executor.defaultJavaOptions=-XX:+UseG1GC"" \
     --conf spark.ui.proxyBase="""" \
     --conf 'spark.eventLog.enabled=true' --conf 'spark.eventLog.dir=hdfs:///var/log/spark/apps' \
     --conf ""spark.sql.hive.convertMetastoreParquet=false"" \
     --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \
     --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog'  {code}
 
{code:java}
import org.apache.hudi.DataSourceWriteOptions
import org.apache.hudi.config.HoodieWriteConfig
import org.apache.hudi.config.HoodieClusteringConfig
import org.apache.hudi.HoodieDataSourceHelpers
import org.apache.hudi.config.HoodieWriteConfig._
import org.apache.spark.sql.SaveMode

val srcPath = ""s3a://amazon-reviews-pds/parquet/""
val tableName = ""amazon_reviews_clustered""
val tablePath = <>

val inputDF = spark.read.format(""parquet"").load(srcPath)

inputDF.write.
  format(""hudi"").
  option(HoodieWriteConfig.TABLE_NAME, tableName).
  option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL).
  option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY, DataSourceWriteOptions.COW_TABLE_TYPE_OPT_VAL).
  option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, ""review_id"").
  option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, ""product_category"").
  option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, ""review_date"").
  option(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, ""true"").
  option(HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP, ""0"").
  option(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS, ""43"").
  option(HoodieClusteringConfig.CLUSTERING_MAX_NUM_GROUPS, ""100"").
  option(HoodieClusteringConfig.CLUSTERING_SORT_COLUMNS_PROPERTY, ""star_rating,total_votes"").
  option(""hoodie.metadata.index.column.stats.enable"", ""true"").
  option(BULK_INSERT_SORT_MODE.key(), ""NONE"").
  mode(SaveMode.Overwrite).
  save(tablePath) {code}
Query the table using Hive connector in Presto:
{code:java}
/presto-cli --catalog hudi --server localhost:9090

select count(review_id) from <table_name> where star_rating > 4 and total_votes > 10;{code}
The result is different from a Hudi table without clustering like below:
{code:java}
import org.apache.hudi.DataSourceWriteOptions
import org.apache.hudi.config.HoodieWriteConfig
import org.apache.hudi.HoodieDataSourceHelpers
import org.apache.spark.sql.SaveMode
import org.apache.hudi.config.HoodieWriteConfig._

val srcPath = ""s3a://amazon-reviews-pds/parquet/""
val tableName = ""amazon_reviews_no_clustering""
val tablePath = <>
val inputDF = spark.read.format(""parquet"").load(srcPath)inputDF.write.format(""hudi"").
  option(HoodieWriteConfig.TABLE_NAME, tableName).
  option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL).
  option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY, DataSourceWriteOptions.COW_TABLE_TYPE_OPT_VAL).
  option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, ""review_id"").
  option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, ""product_category"").
  option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, ""review_date"").
  option(""hoodie.metadata.index.column.stats.enable"", ""true"").
  option(BULK_INSERT_SORT_MODE.key(), ""NONE"").
  mode(SaveMode.Overwrite).
  save(tablePath) {code}
 

 

 "	HUDI	Closed	1	1	3280	Presto, pull-request-available
13551697	Read Optimized Queries should not use RLI	"Read optimized query on a MOR table with RLI present don't produce correct results as RLI lookup doesn't have the ability to distinguish between records that are in base table vs records that are in log files.

To reproduce the issue:
 # Create a MOR table and add three records to the table.
 # Delete one of the records (identified by uuid=5 for example) from the table (this delete will go into a log file).
 # Run read optimized query ""select * from mytable"", you will see all three records as base table does not have the knowledge that record with id 5 was deleted.
 # Run read optimized query ""select * from mytable where id = 5"". This query should return one record since the record is present in the base file. However, if RLI is enabled this query will get evaluated against RLI and will not return any records. This appears to be inconsistent with the results returned in step 3.

spark-shell script to reproduce the issue attached below:
{code:java}
  //
  // Scala script for creating a table with RLI
  //
  import org.apache.hudi.QuickstartUtils._
  import scala.collection.JavaConversions._
  import org.apache.spark.sql.SaveMode._
  import org.apache.hudi.DataSourceReadOptions._
  import org.apache.hudi.DataSourceWriteOptions._
  import org.apache.hudi.config.HoodieWriteConfig._
  import org.apache.hudi.common.model.HoodieRecord


  val tableName = ""hudi_trips_cow""
  val basePath = ""file:///Users/amrish/tables/morereadoptimized""
  val dataGen = new DataGenerator


  // Generate inserts
  val inserts = convertToStringList(dataGen.generateInserts(3))
  val df = spark.read.json(spark.sparkContext.parallelize(inserts, 1))


  df.write.format(""hudi"").
       |   options(getQuickstartWriteConfigs).
       |   option(PRECOMBINE_FIELD_OPT_KEY, ""ts"").
       |   option(RECORDKEY_FIELD_OPT_KEY, ""uuid"").
       |   option(PARTITIONPATH_FIELD_OPT_KEY, ""partitionpath"").
       |   option(TABLE_NAME, tableName).
       |   option(""hoodie.metadata.enable"", ""true"").
       |   option(""hoodie.metadata.record.index.enable"", ""true"").
       |   option(""hoodie.enable.data.skipping"", ""true"").
       |   option(""hoodie.index.type"", ""RECORD_INDEX"").
       |   option(""hoodie.metadata.secondary.record.index.enable"", ""true"").
       |   option(""hoodie.datasource.write.table.type"", ""MERGE_ON_READ"").
       |   option(""hoodie.parquet.small.file.limit"", ""0"").
       |   option(""hoodie.compact.inline.max.delta.commits"", ""3"").
       |   mode(Append).
       |   save(basePath)




//
// Select query
//
val readOpts = Map(
    ""hoodie.metadata.enable"" -> ""true"",
    ""hoodie.metadata.record.index.enable"" -> ""true"",
    ""hoodie.enable.data.skipping"" -> ""true"",
    ""hoodie.index.type"" -> ""RECORD_INDEX""
)


val tripsSnapshotDF = spark.
  read.
  format(""hudi"").
  options(readOpts).
  load(basePath)


tripsSnapshotDF.createOrReplaceTempView(""myrli"")
spark.sql(""select * from myrli order by _hoodie_commit_time asc"").show(false)
//spark.sql(""select count(*) from myrli"").show(false)




//
// Delete one records (Hard Delete)
//
val dataset = spark.sql(""select * from myrli where uuid='7cb4080c-05ff-475c-94d8-ebe369ff4c2d'"").limit(1)
val deletes = dataGen.generateDeletes(dataset.collectAsList())
val deleteDf = spark.read.json(spark.sparkContext.parallelize(deletes, 2))


deleteDf.write.format(""hudi"").
  option(OPERATION_OPT_KEY, ""delete"").
  option(PRECOMBINE_FIELD_OPT_KEY, ""ts"").
  option(RECORDKEY_FIELD_OPT_KEY, ""uuid"").
  option(PARTITIONPATH_FIELD_OPT_KEY, ""partitionpath"").
  option(TABLE_NAME, tableName).
  mode(Append).
  save(basePath)


spark.sql(""select count(*) from myrli"").show(false)


spark.sql(""select driver, rider, uuid from myrli where uuid='49d34c1a-1558-4c0d-996c-1c05851f384a'"").show(truncate=false)




//
// Read optimzied query
//
val readOpts = Map(
    ""hoodie.metadata.enable"" -> ""true"",
    ""hoodie.metadata.record.index.enable"" -> ""false"",
    ""hoodie.enable.data.skipping"" -> ""true"",
    ""hoodie.index.type"" -> ""RECORD_INDEX"",
    ""hoodie.datasource.query.type"" -> ""read_optimized"",
)


val tripsSnapshotDF = spark.
  read.
  format(""hudi"").
  options(readOpts).
  load(basePath)


tripsSnapshotDF.createOrReplaceTempView(""myrli"")
spark.sql(""select * from myrli order by _hoodie_commit_time asc"").show(false)
spark.sql(""select * from myrli WHERE uuid='7cb4080c-05ff-475c-94d8-ebe369ff4c2d'"").show(false)
//spark.sql(""select count(*) from myrli"").show(false) {code}"	HUDI	Closed	1	1	3280	pull-request-available
13484030	Shade JOL in every bundle	"Bundles where it is not shaded

hudi-aws-bundle

hudi-datahub-sync-bundle

hudi-gcp-bundle

hudi-timeline-server-bundle

hudi-utilities-slim-bundle"	HUDI	Closed	1	1	3280	pull-request-available
13454521	Test TestCleanPlanExecutor.testKeepLatestFileVersions is flaky	"[https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/9418/logs/33]

 

 

https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/9413/logs/36"	HUDI	Closed	3	1	3280	pull-request-available
13407884	[Performance] Lower parallelism with snapshot query on COW tables in Presto	"After crossing a certain number of partitions, we observe performance degradation due to lower parallelism for snapshot query on COW tables in Presto. This does not happen for non-partitioned or tables with partioins in lower hundreds (~ 200-300).

The issue is independent of the data size. It is more about partitions."	HUDI	Closed	1	3	3280	pull-request-available
13559597	Implement secondary index	"# Secondary index schema should be flexible enough to accommodate various kinds of secondary index. 
 # Reuse as much as possible the existing framework for indexing.
 # Merge with existing index config and introduce as less configs as possible."	HUDI	Closed	3	3	3280	hudi-1.0.0-beta2, pull-request-available
13422270	Unit tests requiring hive service in setup failing	"This is only happening for some tests in hudi-utilities module, e.g. TestHoodieDeltaStreamer, TestHiveIncrementalPuller. The error is:

NoClassDefFoundError: org/apache/logging/log4j/core/Appender

We should log4j-core in hudi-utilities at least in test scope."	HUDI	Closed	1	1	3280	pull-request-available
13475287	hive sync bundle causes class loader issue	"A weird classpath issue i found: when testing deltastreamer using hudi-utilities-slim-bundle, if i put --jars hudi-hive-sync-bundle.jar,hudi-spark-bundle.jar then i’ll get this error when writing

{code:java}
Caused by: java.lang.NoSuchMethodError: org.apache.hudi.avro.MercifulJsonConverter.convert(Ljava/lang/String;Lorg/apache/avro/Schema;)Lorg/apache/avro/generic/GenericRecord;
	at org.apache.hudi.utilities.sources.helpers.AvroConvertor.fromJson(AvroConvertor.java:86)
	at org.apache.spark.api.java.JavaPairRDD$.$anonfun$toScalaFunction$1(JavaPairRDD.scala:1070)
{code}

if i put the spark bundle before the hive sync bundle, then no issue. Without hive-sync-bundle, also no issue. So hive-sync-bundle somehow messes up with classpath? not sure why it reports a hudi-common API not found… caused by shading avro?


the same behavior i observed with aws-bundle, which makes sense, as it’s a superset of hive-sync-bundle"	HUDI	Closed	1	1	3280	pull-request-available
13313144	[Umbrella] Support clustering on filegroups	please see [https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+speed+and+query+performance]	HUDI	Closed	3	15	3280	hudi-umbrellas
13477964	Avoid all illegal reflective access in the code	"Since Java 16, certain kinds of reflective access is no longer allowed. Check JEP 396 on strong encapsulation for more details: [https://openjdk.org/jeps/396]

One such example in our code is the usage of Field.setAccessible(true) in ObjectSizeCalculator. This code will throw InaccessibleObjectException when running on jdk16 or higher. We should avoid such illegal reflective access."	HUDI	Closed	2	3	3280	jdk, pull-request-available, reflection, writer
13412299	Upgrade HBase dependencies in Hudi	Bootstrap and metadata depend on hbase-server and hfile reader/writer for IO. Currently, Hudi is on 1.2.3 hbase version. We should upgrade to 2.4.x. There have been some significant changes since 1.2.3, especially the changes related to comparator. We need to fully certify the upgrade and also determine how it will affect users with readers and writers on different version.	HUDI	Closed	1	4	3280	pull-request-available
13477121	Enhance retries for failed writes w/ write conflicts in a multi writer scenarios	"lets say there are two writers from t0 to t5. so hudi fails w2 and succeeds w1. and user restarts w2 and for next 5 mins, lets say there are no other overlapping writers. So the same write from w2 will now succeed. so, whenever there is a write conflict and pipeline fails, all user needs to do is, just restart the pipeline or retry to ingest the same batch.

 

Ask: can we add retries within hudi during such failures. Anyways, in most cases, users just restart the pipeline in such cases. 

 "	HUDI	Closed	1	4	3280	pull-request-available
13550813	Fix InternalSchema schemaId when column is dropped	In case of dropping a column via spark-sql, schema id is incorrectly set to max column id instead of commit timestamp.	HUDI	Closed	3	3	3280	pull-request-available
13274542	Use ColumnIndex in parquet to speed up scans	[https://github.com/apache/parquet-format/blob/master/PageIndex.md]	HUDI	Closed	3	4	4726	help-requested
13539934	Automatically decide archival boundary based on cleaning configs	We throw an error if the archival is more aggressive than the hour-based cleaning after [https://github.com/apache/hudi/pull/8422/files.]  We can improve this by automatically deciding archival boundary for hour-based cleaning.	HUDI	Closed	3	4	4726	pull-request-available
13526627	Table service client overrides the timeline service and write config of regular write client	"In 0.13.0 and latest master, the table service client `BaseHoodieTableServiceClient` is instantiated without any timeline server instance, even if the regular write client has one.  This causes the table service client to start a new embedded timeline server and overwrite the write config passed in from the constructor so that the write config points to the newly started timeline server.

As the regular write client such as `SparkRDDWriteClient` directly passes in the same writeConfig instance, the regular write client's write config is also affected, causing the regular write client to use the newly started embedded timeline server always, instead of the timeline server instance passed in from the constructor or the one instantiated by the regular write client itself.

This means that the Deltastreamer's long-lived timeline server is never going to be used because of this issue.
{code:java}
BaseHoodieTableServiceClient:

protected BaseHoodieTableServiceClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {
  super(context, clientConfig, Option.empty());
}
->
BaseHoodieClient:

protected BaseHoodieClient(HoodieEngineContext context, HoodieWriteConfig clientConfig,
    Option<EmbeddedTimelineService> timelineServer) {
  ...
  startEmbeddedServerView();
}

private synchronized void startEmbeddedServerView() {
  if (config.isEmbeddedTimelineServerEnabled()) {
    if (!timelineServer.isPresent()) {
      // Run Embedded Timeline Server
      try {
        timelineServer = EmbeddedTimelineServerHelper.createEmbeddedTimelineService(context, config);
      } catch (IOException e) {
        LOG.warn(""Unable to start timeline service. Proceeding as if embedded server is disabled"", e);
        stopEmbeddedServerView(false);
      }
    } else {
      LOG.info(""Timeline Server already running. Not restarting the service"");
    }
  } else {
    LOG.info(""Embedded Timeline Server is disabled. Not starting timeline service"");
  }
}

public static synchronized Option<EmbeddedTimelineService> createEmbeddedTimelineService(
    HoodieEngineContext context, HoodieWriteConfig config) throws IOException {
  if (config.isEmbeddedTimelineServerReuseEnabled()) {
    if (!TIMELINE_SERVER.isPresent() || !TIMELINE_SERVER.get().canReuseFor(config.getBasePath())) {
      TIMELINE_SERVER = Option.of(startTimelineService(context, config));
    } else {
      updateWriteConfigWithTimelineServer(TIMELINE_SERVER.get(), config);
    }
    return TIMELINE_SERVER;
  }
  if (config.isEmbeddedTimelineServerEnabled()) {
    return Option.of(startTimelineService(context, config));
  } else {
    return Option.empty();
  }
}

public static void updateWriteConfigWithTimelineServer(EmbeddedTimelineService timelineServer,
    HoodieWriteConfig config) {
  // Allow executor to find this newly instantiated timeline service
  if (config.isEmbeddedTimelineServerEnabled()) {
    config.setViewStorageConfig(timelineServer.getRemoteFileSystemViewConfig());
  }
}{code}
SparkRDDWriteClient:
{code:java}
public SparkRDDWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig,
                           Option<EmbeddedTimelineService> timelineService) {
  super(context, writeConfig, timelineService, SparkUpgradeDowngradeHelper.getInstance());
  this.tableServiceClient = new SparkRDDTableServiceClient<>(context, writeConfig);
} {code}
 "	HUDI	Closed	1	1	4726	pull-request-available
13468956	Add example configuration for HoodieCleaner in docs	"[https://github.com/apache/hudi/issues/5975]

We should add example configs in properties file to illustrate how to configure HoodieCleaner with different policies."	HUDI	Closed	4	3	4726	pull-request-available
13480922	Commit metadata in Json contains redundant information	"The commit metadata in Json (*.commit, *.deltacommit) written to the Hudi timeline under .hoodie contains redundant fields that can be trimmed.  As shown below, the same set of write stats is written to both ""partitionToWriteStats"" and ""writeStats"", doubling the size and increasing the serde overhead.  Other fields like ""totalRecordsDeleted"", ""writePartitionPaths"", ""fileIdAndRelativePaths"", etc., can be removed as well as they are derived from ""partitionToWriteStats"" and not directly used by HoodieCommitMetadata class.

Example commit metadata:

 
{code:java}
{
  ""partitionToWriteStats"" : {
    ""2022/1/31"" : [ {
      ""fileId"" : ""0cb6ac8a-ee31-4f00-a359-ba6ebfb80463-0"",
      ""path"" : ""2022/1/31/0cb6ac8a-ee31-4f00-a359-ba6ebfb80463-0_0-9-38_20220410134618909.parquet"",
      ""prevCommit"" : ""20220410134320333"",
      ""numWrites"" : 250175,
      ""numDeletes"" : 0,
      ""numUpdateWrites"" : 0,
      ""numInserts"" : 50035,
      ""totalWriteBytes"" : 90720802,
      ""totalWriteErrors"" : 0,
      ""tempPath"" : null,
      ""partitionPath"" : ""2022/1/31"",
      ""totalLogRecords"" : 0,
      ""totalLogFilesCompacted"" : 0,
      ""totalLogSizeCompacted"" : 0,
      ""totalUpdatedRecordsCompacted"" : 0,
      ""totalLogBlocks"" : 0,
      ""totalCorruptLogBlock"" : 0,
      ""totalRollbackBlocks"" : 0,
      ""fileSizeInBytes"" : 90720802,
      ""minEventTime"" : null,
      ""maxEventTime"" : null
    } ],
    ...
  },
  ""compacted"" : false,
  ""extraMetadata"" : {
    ""schema"" : ""{\""type\"":\""record\"",\""name\"":\""hoodie_source\"",\""namespace\"":\""hoodie.source\"",\""fields\"":[{\""name\"":\""key\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""partition\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""ts\"",\""type\"":[\""null\"",\""long\""],\""default\"":null},{\""name\"":\""textField\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""decimalField\"",\""type\"":[\""null\"",\""float\""],\""default\"":null},{\""name\"":\""longField\"",\""type\"":[\""null\"",\""long\""],\""default\"":null},{\""name\"":\""arrayField\"",\""type\"":[\""null\"",{\""type\"":\""array\"",\""items\"":[\""int\"",\""null\""]}],\""default\"":null},{\""name\"":\""mapField\"",\""type\"":[\""null\"",{\""type\"":\""map\"",\""values\"":[\""int\"",\""null\""]}],\""default\"":null},{\""name\"":\""round\"",\""type\"":[\""null\"",\""int\""],\""default\"":null}]}"",
    ""deltastreamer.checkpoint.key"" : ""17""
  },
  ""operationType"" : ""INSERT"",
  ""writeStats"" : [ {
    ""fileId"" : ""0cb6ac8a-ee31-4f00-a359-ba6ebfb80463-0"",
    ""path"" : ""2022/1/31/0cb6ac8a-ee31-4f00-a359-ba6ebfb80463-0_0-9-38_20220410134618909.parquet"",
    ""prevCommit"" : ""20220410134320333"",
    ""numWrites"" : 250175,
    ""numDeletes"" : 0,
    ""numUpdateWrites"" : 0,
    ""numInserts"" : 50035,
    ""totalWriteBytes"" : 90720802,
    ""totalWriteErrors"" : 0,
    ""tempPath"" : null,
    ""partitionPath"" : ""2022/1/31"",
    ""totalLogRecords"" : 0,
    ""totalLogFilesCompacted"" : 0,
    ""totalLogSizeCompacted"" : 0,
    ""totalUpdatedRecordsCompacted"" : 0,
    ""totalLogBlocks"" : 0,
    ""totalCorruptLogBlock"" : 0,
    ""totalRollbackBlocks"" : 0,
    ""fileSizeInBytes"" : 90720802,
    ""minEventTime"" : null,
    ""maxEventTime"" : null
  }, 
  ... 
  ],
  ""totalRecordsDeleted"" : 0,
  ""totalLogFilesSize"" : 0,
  ""totalScanTime"" : 0,
  ""totalCreateTime"" : 0,
  ""totalUpsertTime"" : 309120,
  ""minAndMaxEventTime"" : {
    ""Optional.empty"" : {
      ""val"" : null,
      ""present"" : false
    }
  },
  ""writePartitionPaths"" : [ ""2022/1/31"", ""2022/1/30"", ""2022/1/28"", ""2022/1/27"", ""2022/2/2"", ""2022/1/29"", ""2022/1/24"", ""2022/2/1"", ""2022/1/26"", ""2022/1/25"" ],
  ""fileIdAndRelativePaths"" : {
    ""3e31414c-fb4c-4ce9-aa27-a43640d94430-0"" : ""2022/1/25/3e31414c-fb4c-4ce9-aa27-a43640d94430-0_9-9-47_20220410134618909.parquet"",
    ...
  },
  ""totalLogRecordsCompacted"" : 0,
  ""totalLogFilesCompacted"" : 0,
  ""totalCompactedRecordsUpdated"" : 0
} {code}
 

 "	HUDI	Closed	2	1	4726	pull-request-available
13430370	MOR compaction and archive settings may prevent metadata table compaction	"For an MOR table, we have a setting to specify the number of delta commits after which compaction takes place. Similarly we have a setting which controls archiving of instants. 

If the hoodie.compact.inline.max.delta.commits setting is > hoodie.keep.max.commits, there will never be enough deltacommits accumulated in the .hoodie folder for compaction to kick in. This may prevent metadata table compaction.
"	HUDI	Closed	1	1	4726	pull-request-available
13486466	Increase default num_instants to fetch for incremental source	"By default, the default maximum number of instants to fetch in incremental source (""hoodie.deltastreamer.source.hoodieincr.num_instants"") is 1.  The checkpoint of the target Hudi table from the incremental ETL lags behind the source if the ingestion runs at a lower frequency than the source or the ingestion job stalls due to some issues, causing the data freshness issue.  We should make the default value larger."	HUDI	Closed	3	4	4726	pull-request-available
13590660	Use SerializableConfiguration with Spark broadcast	There are still places that broadcast StorageConf in Spark which can cause NPE.	HUDI	Closed	1	1	4726	pull-request-available
13440523	Fix partition path construction in metadata table validator	"The metadata table validator throws exception for non-partitioned table due to partition path construction.
{code:java}
org.apache.hudi.exception.HoodieException: Unable to do hoodie metadata table validation in file:/Users/ethan/Work/scripts/mt_reliability_testing/b5_ds_cont_mor_np_async_inp_noerr/test_table
    at org.apache.hudi.utilities.HoodieMetadataTableValidator.run(HoodieMetadataTableValidator.java:364)
    at org.apache.hudi.utilities.HoodieMetadataTableValidator.main(HoodieMetadataTableValidator.java:345)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string
    at org.apache.hadoop.fs.Path.checkPathArg(Path.java:172)
    at org.apache.hadoop.fs.Path.<init>(Path.java:184)
    at org.apache.hadoop.fs.Path.<init>(Path.java:119)
    at org.apache.hudi.utilities.HoodieMetadataTableValidator.lambda$validatePartitions$5(HoodieMetadataTableValidator.java:493)
    at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
    at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747)
    at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721)
    at java.util.stream.AbstractTask.compute(AbstractTask.java:327)
    at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)
    at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
    at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.hudi.utilities.HoodieMetadataTableValidator.validatePartitions(HoodieMetadataTableValidator.java:502)
    at org.apache.hudi.utilities.HoodieMetadataTableValidator.doMetadataTableValidation(HoodieMetadataTableValidator.java:430)
    at org.apache.hudi.utilities.HoodieMetadataTableValidator.doHoodieMetadataTableValidationOnce(HoodieMetadataTableValidator.java:375)
    at org.apache.hudi.utilities.HoodieMetadataTableValidator.run(HoodieMetadataTableValidator.java:361)
    ... 13 more {code}"	HUDI	Closed	1	3	4726	pull-request-available
13583712	Remove shared HFile reader in HoodieNativeAvroHFileReader	The shared HFile reader in HoodieNativeAvroHFileReader uses significant memory for reading meta info from the HFile.  We should avoid keeping the reference to the shared HFile reader and cache the meta info only.	HUDI	Closed	1	4	4726	pull-request-available
13438473	Enabling MT by default on Read path makes HiveSync fail	"{code:java}
#################################################################################################
49273 [main] INFO  org.apache.hudi.integ.ITTestBase  - Container : /adhoc-1, Running command :spark-submit --class org.apache.hudi.hive.HiveSyncTool /var/hoodie/ws/docker/hoodie/hadoop/hive_base/target/hoodie-hive-sync-bundle.jar --database default --table stock_ticks_cow --base-path /user/hive/warehouse/stock_ticks_cow --base-file-format PARQUET --user hive --pass hive --jdbc-url jdbc:hive2://hiveserver:10000 --partitioned-by dt
49273 [main] INFO  org.apache.hudi.integ.ITTestBase  - 
#################################################################################################
59728 [dockerjava-jaxrs-async-8] INFO  org.apache.hudi.integ.ITTestBase  - onComplete called
59732 [main] INFO  org.apache.hudi.integ.ITTestBase  - Exit code for command : 1
59732 [main] ERROR org.apache.hudi.integ.ITTestBase  -  ###### Stdout #######59733 [main] ERROR org.apache.hudi.integ.ITTestBase  -  ###### Stderr #######
22/04/07 21:32:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/util/Bytes
    at org.apache.hudi.common.bootstrap.index.HFileBootstrapIndex.<clinit>(HFileBootstrapIndex.java:94)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at org.apache.hudi.common.util.ReflectionUtils.getClass(ReflectionUtils.java:54)
    at org.apache.hudi.common.util.ReflectionUtils.loadClass(ReflectionUtils.java:89)
    at org.apache.hudi.common.bootstrap.index.BootstrapIndex.getBootstrapIndex(BootstrapIndex.java:163)
    at org.apache.hudi.common.table.view.AbstractTableFileSystemView.init(AbstractTableFileSystemView.java:108)
    at org.apache.hudi.common.table.view.HoodieTableFileSystemView.init(HoodieTableFileSystemView.java:108)
    at org.apache.hudi.common.table.view.HoodieTableFileSystemView.<init>(HoodieTableFileSystemView.java:102)
    at org.apache.hudi.common.table.view.HoodieTableFileSystemView.<init>(HoodieTableFileSystemView.java:93)
    at org.apache.hudi.metadata.HoodieTableMetadataUtil.getFileSystemView(HoodieTableMetadataUtil.java:1014)
    at org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartitionFileSlices(HoodieTableMetadataUtil.java:1032)
    at org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartitionLatestMergedFileSlices(HoodieTableMetadataUtil.java:980)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.getPartitionFileSliceToKeysMapping(HoodieBackedTableMetadata.java:370)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordsByKeys(HoodieBackedTableMetadata.java:197)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordByKey(HoodieBackedTableMetadata.java:140)
    at org.apache.hudi.metadata.BaseTableMetadata.fetchAllPartitionPaths(BaseTableMetadata.java:281)
    at org.apache.hudi.metadata.BaseTableMetadata.getAllPartitionPaths(BaseTableMetadata.java:111)
    at org.apache.hudi.common.fs.FSUtils.getAllPartitionPaths(FSUtils.java:299)
    at org.apache.hudi.sync.common.AbstractSyncHoodieClient.getPartitionsWrittenToSince(AbstractSyncHoodieClient.java:189)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:226)
    at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:150)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:138)
    at org.apache.hudi.hive.HiveSyncTool.main(HiveSyncTool.java:433)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.util.Bytes
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 36 more59742 [main] INFO  org.apache.hudi.integ.ITTestBase  -  {code}"	HUDI	Closed	1	1	4726	pull-request-available
13479179	Fix inflight clean action preventing clean service to continue when multiple cleans are not allowed	"For Hudi Deltastreamer async cleaning, when the Spark job fails in the middle of the cleaning, leaving the clean instant inflight, the Spark job retried next time may not resume the inflight clean action if `hoodie.clean.allow.multiple` is `false`, i.e., multiple clean schedules are disabled.  This is due to a bug in the code below.

 

Relevant logic in BaseHoodieWriteClient:
{code:java}
public HoodieCleanMetadata clean(String cleanInstantTime, boolean scheduleInline, boolean skipLocking) throws HoodieIOException {
  if (!tableServicesEnabled(config)) {
    return null;
  }
  final Timer.Context timerContext = metrics.getCleanCtx();
  CleanerUtils.rollbackFailedWrites(config.getFailedWritesCleanPolicy(),
      HoodieTimeline.CLEAN_ACTION, () -> rollbackFailedWrites(skipLocking));

  HoodieCleanMetadata metadata = null;
  HoodieTable table = createTable(config, hadoopConf);
  if (config.allowMultipleCleans() || !table.getActiveTimeline().getCleanerTimeline().filterInflightsAndRequested().firstInstant().isPresent()) {
    LOG.info(""Cleaner started"");
    // proceed only if multiple clean schedules are enabled or if there are no pending cleans.
    if (scheduleInline) {
      scheduleTableServiceInternal(cleanInstantTime, Option.empty(), TableServiceType.CLEAN);
      table.getMetaClient().reloadActiveTimeline();
    }

    metadata = table.clean(context, cleanInstantTime, skipLocking);
    if (timerContext != null && metadata != null) {
      long durationMs = metrics.getDurationInMs(timerContext.stop());
      metrics.updateCleanMetrics(durationMs, metadata.getTotalFilesDeleted());
      LOG.info(""Cleaned "" + metadata.getTotalFilesDeleted() + "" files""
          + "" Earliest Retained Instant :"" + metadata.getEarliestCommitToRetain()
          + "" cleanerElapsedMs"" + durationMs);
    }
  }
  return metadata;
} {code}"	HUDI	Closed	1	1	4726	pull-request-available
13582176	Bump AWS SDK v2 version to 2.25.69	The current version of AWS SDK v2 used is 2.18.40 which is 1.5 years old.	HUDI	Closed	3	4	4726	pull-request-available
13475795	Failed to create timeline-server marker due to HoodieRemoteException	https://github.com/apache/hudi/issues/4230	HUDI	Open	3	1	4726	pull-request-available
13438013	Empty requested rollback plan can stay on the timeline forever	When the instanttime.rollback.requested is empty or corrupted, it cannot be parsed.  When running getPendingRollbackInfos(), it's going to skip that empty/corrupted requested rollback instant and the rollback instant is going to stay on the timeline forever, preventing metadata table archival.	HUDI	Closed	1	1	4726	pull-request-available
13253583	Hudi Website - Provide links to documentation corresponding to older release versions	"While this may be too difficult to do it retroactively for previous versions, we need to support this for apache releases. 

See flink website (e:g - [https://flink.apache.org/] you will see a link 1.9 version  [https://ci.apache.org/projects/flink/flink-docs-release-1.9/]

For older releases, 0.4.6 and 0.4.7, we have created git tags *hoodie-site-0.4.6 and*  *hoodie-site-0.4.7* 

*You can checkout the tags and read README.md to access and run website locally.*

 "	HUDI	Closed	3	2	4726	new-to-hudi, user-support-issues
13435481	Revisit hudi-utilities-bundle build wrt Spark versions	"When we build hudi-utilities-bundle, the Spark profile can affect the bundle jar.  This causes incompatibility between hudi-utilities-bundle and some Spark versions.  When the hudi-utilities-bundle is built with the Spark version that is going to be used for the ingestion, there is no error.

 

For example:

When running deltastreamer with hudi-utilities-bundle_2.12-0.10.1.jar using Spark 3.1.2, the ingestion job throws java.lang.ClassNotFoundException: org.apache.spark.sql.adapter.Spark3Adapter.
{code:java}
/Users/ethan/Work/lib/spark-3.1.2-bin-hadoop3.2/bin/spark-submit \
        --master local[6] \
        --driver-memory 6g --executor-memory 2g --num-executors 6 --executor-cores 1 \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.sql.catalogImplementation=hive \
        --conf spark.driver.maxResultSize=1g \
        --conf spark.speculation=true \
        --conf spark.speculation.multiplier=1.0 \
        --conf spark.speculation.quantile=0.5 \
        --conf spark.ui.port=6679 \
        --conf spark.eventLog.enabled=true \
        --conf spark.eventLog.dir=/Users/ethan/Work/data/hudi/spark-logs \
        --packages org.apache.spark:spark-avro_2.12:3.1.2 \
        --jars /Users/ethan/Work/repo/hudi-benchmarks/target/hudi-benchmarks-0.1-SNAPSHOT.jar \
        --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \
        /Users/ethan/Work/lib/hudi_releases/0.10.1/hudi-utilities-bundle_2.12-0.10.1.jar \
        --props /Users/ethan/Work/scripts/hbase-upgrade-testing/hudi_0_10_1_cow/ds_cow_before.properties \
        --source-class BenchmarkDataSource \
        --source-ordering-field ts \
        --target-base-path file:/Users/ethan/Work/scripts/hbase-upgrade-testing/hudi_0_10_1_cow/test_table \
        --target-table test_table \
        --table-type COPY_ON_WRITE \
        --op INSERT >> ds_before.log 2>&1 {code}
 
{code:java}
Exception in thread ""main"" java.lang.ClassNotFoundException: org.apache.spark.sql.adapter.Spark3Adapter
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    at org.apache.hudi.SparkAdapterSupport.sparkAdapter(SparkAdapterSupport.scala:35)
    at org.apache.hudi.SparkAdapterSupport.sparkAdapter$(SparkAdapterSupport.scala:29)
    at org.apache.hudi.HoodieSparkUtils$.sparkAdapter$lzycompute(HoodieSparkUtils.scala:48)
    at org.apache.hudi.HoodieSparkUtils$.sparkAdapter(HoodieSparkUtils.scala:48)
    at org.apache.hudi.HoodieSparkUtils$.createRddInternal(HoodieSparkUtils.scala:144)
    at org.apache.hudi.HoodieSparkUtils$.createRdd(HoodieSparkUtils.scala:136)
    at org.apache.hudi.HoodieSparkUtils.createRdd(HoodieSparkUtils.scala)
    at org.apache.hudi.utilities.deltastreamer.SourceFormatAdapter.lambda$fetchNewDataInAvroFormat$1(SourceFormatAdapter.java:79)
    at org.apache.hudi.common.util.Option.map(Option.java:107)
    at org.apache.hudi.utilities.deltastreamer.SourceFormatAdapter.fetchNewDataInAvroFormat(SourceFormatAdapter.java:70)
    at org.apache.hudi.utilities.deltastreamer.DeltaSync.readFromSource(DeltaSync.java:425)
    at org.apache.hudi.utilities.deltastreamer.DeltaSync.syncOnce(DeltaSync.java:290)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.lambda$sync$2(HoodieDeltaStreamer.java:193)
    at org.apache.hudi.common.util.Option.ifPresent(Option.java:96)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.sync(HoodieDeltaStreamer.java:191)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.main(HoodieDeltaStreamer.java:514)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) {code}
For latest master:

When running deltastreamer using Spark 3.1.2, the ingestion job throws different exceptions using different Spark profiles.
{code:java}
java.lang.ClassNotFoundException: org.apache.spark.sql.adapter.Spark3_2Adapter
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    at org.apache.hudi.SparkAdapterSupport.sparkAdapter(SparkAdapterSupport.scala:37)
    at org.apache.hudi.SparkAdapterSupport.sparkAdapter$(SparkAdapterSupport.scala:29)
    at org.apache.hudi.HoodieSparkUtils$.sparkAdapter$lzycompute(HoodieSparkUtils.scala:44)
    at org.apache.hudi.HoodieSparkUtils$.sparkAdapter(HoodieSparkUtils.scala:44)
    at org.apache.hudi.AvroConversionUtils$.$anonfun$createInternalRowToAvroConverter$1(AvroConversionUtils.scala:79)
    at org.apache.hudi.HoodieSparkUtils$.$anonfun$createRdd$5(HoodieSparkUtils.scala:161)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
    at scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
    at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
    at scala.collection.AbstractIterator.to(Iterator.scala:1431)
    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)
    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
    at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1449)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748) {code}
{code:java}
java.lang.RuntimeException: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: java.lang.NoClassDefFoundError: org/apache/parquet/schema/LogicalTypeAnnotation$LogicalTypeAnnotationVisitor     at org.apache.hudi.client.utils.LazyIterableIterator.next(LazyIterableIterator.java:121)     at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:44)     at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)     at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)     at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)     at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)     at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1440)     at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)     at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)     at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)     at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)     at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)     at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)     at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)     at org.apache.spark.scheduler.Task.run(Task.scala:131)     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)     at java.lang.Thread.run(Thread.java:748)
 
{code}
 "	HUDI	Closed	1	3	4726	pull-request-available
13521798	Improve the usability of Hudi CLI bundle	"# ability to specify existing hudi-cli-bundle and hudi-spark-bundle
 # automatically download aux jars (Jakarta) for Hudi CLI"	HUDI	Closed	1	4	4726	pull-request-available
13524905	Keep RFC-56 early conflict detection update to date	"* Config and class naming
 * New implementation details"	HUDI	Closed	3	4	4726	pull-request-available
13470042	Fix HoodieDropPartitionsTool based on refactored meta sync	[https://github.com/apache/hudi/pull/4459] causes master to fail to due to refactoring of the meta sync.	HUDI	Closed	3	4	4726	pull-request-available
13581212	Resolve the conflicts between mixed hdfs and local path in Flink tests	Bumps the depdency to mitigate vulnerability.	HUDI	Open	3	1	4726	pull-request-available
13580768	Use Spark SerializableConfiguration to avoid NPE in Kryo serde	"With Hudi 0.14.1, without ""spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar"", Hudi query in Spark quick start guide succeeds.  In Hudi 0.15.0-rc2, without the Kryo registratrar, the Hudi read throws NPE due to HadoopStorageConfiguration.
{code:java}
Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:492)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2728)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2935)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:326)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:806)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:765)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:774)
  ... 47 elided
Caused by: java.lang.NullPointerException
  at org.apache.spark.sql.execution.datasources.parquet.Spark32LegacyHoodieParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(Spark32LegacyHoodieParquetFileFormat.scala:152)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
  at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:131)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:750) {code}"	HUDI	Closed	3	4	4726	hoodie-storage, pull-request-available
13569897	Move PR size labeling to GitHub scheduled workflow	The schedule workflow has write permission to PRs while the PR event triggers do not.	HUDI	Closed	3	4	4726	pull-request-available
13537187	Add bundle validation based on release candidates	We should check in the code for validation bundles in release candidates to make the release process easier.	HUDI	Open	2	4	4726	pull-request-available
13448884	Reading metadata table on S3 using Spark throws NullPointerException during createHFileReader	"Environment: EMR 6.6.0, OSS Spark 3.2.1, Hudi master

Storage: S3

When loading the metadata table in Spark shell using the following code, it throws NullPointerException.  In this case, the metadata table has the base files in HFile format.

This also happens for the following combinations: (1) Spark 3.1.3, Hudi 0.11.0 (1) Spark 3.2.1, Hudi 0.11.0 
{code:java}
spark.read.format(""hudi"").load(""s3a://<base_path>/.hoodie/metadata/"").show {code}
 
{code:java}
Caused by: java.lang.NullPointerException
  at org.apache.hudi.org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:178)
  at org.apache.hudi.org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:167)
  at org.apache.hudi.org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:163)
  at org.apache.hudi.HoodieBaseRelation$.$anonfun$createHFileReader$1(HoodieBaseRelation.scala:531)
  at org.apache.hudi.HoodieBaseRelation.$anonfun$createBaseFileReader$1(HoodieBaseRelation.scala:482)
  at org.apache.hudi.HoodieMergeOnReadRDD.readBaseFile(HoodieMergeOnReadRDD.scala:130)
  at org.apache.hudi.HoodieMergeOnReadRDD.compute(HoodieMergeOnReadRDD.scala:100)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:131)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:750) {code}
Spark shell:
{code:java}
./bin/spark-shell  \
     --master yarn \
     --deploy-mode client \
     --driver-memory 20g \
     --executor-memory 20g \
     --num-executors 2 \
     --executor-cores 8 \
     --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
     --conf spark.kryoserializer.buffer=256m \
     --conf spark.kryoserializer.buffer.max=1024m \
     --jars /home/hadoop/hudi-spark3.2-bundle_2.12-0.12.0-SNAPSHOT.jar \
     --conf 'spark.eventLog.enabled=true' --conf 'spark.eventLog.dir=hdfs:///var/log/spark/apps' \
     --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \
     --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' {code}
 

 "	HUDI	Closed	1	1	4726	pull-request-available
13258679	Translate Documentation -> Performance page	"Translate this page into Chinese:

 

[http://hudi.apache.org/performance.html]"	HUDI	Closed	3	7	4726	pull-request-available
13377566	Fix NPE in `RowKeyGenertorHelper#getNestedFieldVal` when row writer is enabled 	"When row writer is enabled, NullPointerException is thrown when inserting records with partition path in a nested field.

 

To reproduce:
{code:java}
df.write.format(""hudi"")
  .option(OPERATION_OPT_KEY, ""bulk_insert"")
  .option(PRECOMBINE_FIELD_OPT_KEY, ""timestamp"")
  .option(RECORDKEY_FIELD_OPT_KEY, ""_row_key"")
  .option(PARTITIONPATH_FIELD_OPT_KEY, ""fare.currency"")
  .option(HoodieWriteConfig.TABLE_NAME, ""hoodie_test"")
  .option(""hoodie.metadata.enable"", ""true"")
  .option(""hoodie.datasource.write.row.writer.enable"", ""true"")
  .option(""hoodie.bulkinsert.shuffle.parallelism"", ""2"")
  .mode(SaveMode.Overwrite)
  .save(basePath){code}
 

Stacktrace:
{code:java}
Caused by: java.lang.NullPointerException
	at org.apache.hudi.keygen.RowKeyGeneratorHelper.lambda$getPartitionPathFromRow$1(RowKeyGeneratorHelper.java:117)
	at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250)
	at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110)
	at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.hudi.keygen.RowKeyGeneratorHelper.getPartitionPathFromRow(RowKeyGeneratorHelper.java:124)
	at org.apache.hudi.keygen.SimpleKeyGenerator.getPartitionPath(SimpleKeyGenerator.java:72)
	at org.apache.spark.sql.UDFRegistration$$anonfun$259.apply(UDFRegistration.scala:759)
	... 22 more
{code}
 

 This happens when the value in the nested field of the partition path is null.  The method above does not handle this properly."	HUDI	Resolved	3	1	4726	pull-request-available
13318766	MOR appends slow due to file listing in executor side for finding the log file	"Another place where we do listing in executor. 

(Source : [https://github.com/apache/hudi/issues/1852])

: sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:352)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation.processResponse(AbfsHttpOperation.java:259)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:167)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:124)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:180)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listFiles(AzureBlobFileSystemStore.java:549)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:628)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:532)
shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:344)
org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1517)
org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1557)
org.apache.hudi.common.fs.HoodieWrapperFileSystem.listStatus(HoodieWrapperFileSystem.java:487)
org.apache.hudi.common.fs.FSUtils.getAllLogFiles(FSUtils.java:409)
org.apache.hudi.common.fs.FSUtils.getLatestLogVersion(FSUtils.java:420)
org.apache.hudi.common.fs.FSUtils.computeNextLogVersion(FSUtils.java:434)
org.apache.hudi.common.model.HoodieLogFile.rollOver(HoodieLogFile.java:115)
org.apache.hudi.common.table.log.HoodieLogFormatWriter.(HoodieLogFormatWriter.java:101)
org.apache.hudi.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:249)
org.apache.hudi.io.HoodieAppendHandle.createLogWriter(HoodieAppendHandle.java:291)
org.apache.hudi.io.HoodieAppendHandle.init(HoodieAppendHandle.java:141)
org.apache.hudi.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:197)
org.apache.hudi.table.action.deltacommit.DeltaCommitActionExecutor.handleUpdate(DeltaCommitActionExecutor.java:77)
org.apache.hudi.table.action.commit.BaseCommitActionExecutor.handleUpsertPartition(BaseCommitActionExecutor.java:246)
org.apache.hudi.table.action.commit.BaseCommitActionExecutor.lambda$execute$caffe4c4$1(BaseCommitActionExecutor.java:102)
org.apache.hudi.table.action.commit.BaseCommitActionExecutor$$Lambda$192/1449069739.call(Unknown Source)
org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:105)"	HUDI	Open	3	3	4726	perf
13505695	Hudi getAllQueryPartitionPaths use regular match caused Invalid input path add 	" 

When we query sql in hive like:

select mainwaybillno,
zonecode,
accountantcode,
baroprcode,
opcode,
row_number() over(PARTITION BY mainwaybillno, zonecode, opcode ORDER BY barscantm) sn from dm_kafka_rdmp_dw.fvp_core_fact_route_op_hudi_op_new_rt WHERE opcode IN ('50') and inc_day='20221120' limit 10;

In MapReduce Job the config mapreduce.input.fileinputformat.inputdir=hdfs://dw/hive/warehouse/dm/dm_kafka_rdmp_dw/fvp_core_fact_route_op_hudi_op_new/inc_day=20221120/opcode=50

But this file split hdfs://dw/hive/warehouse/dm/dm_kafka_rdmp_dw/fvp_core_fact_route_op_hudi_op_new/inc_day=20221120/opcode=5000 was added to the job.

This job was failed and throw exception :
2022-11-21 18:11:33,895 INFO [IPC Server handler 1 on 45077] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1668750926041_1011874_m_000110_0: Error: java.lang.RuntimeException: java.lang.IllegalStateException: Invalid input path hdfs://dw/hive/warehouse/dm/dm_kafka_rdmp_dw/fvp_core_fact_route_op_hudi_op_new/inc_day=20221120/opcode=501/.00000006-2d6e-4d26-93ea-1026632abb67_20221119235956333.log.1_44-150-2
at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:169)
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.IllegalStateException: Invalid input path hdfs://dw/hive/warehouse/dm/dm_kafka_rdmp_dw/fvp_core_fact_route_op_hudi_op_new/inc_day=20221120/opcode=501/.00000006-2d6e-4d26-93ea-1026632abb67_20221119235956333.log.1_44-150-2
at org.apache.hadoop.hive.ql.exec.AbstractMapOperator.getNominalPath(AbstractMapOperator.java:119)
at org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:452)
at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1106)
at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:482)
at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:160)
... 8 more

2022-11-21 18:11:33,897 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1668750926041_1011874_m_000110_0: Error: java.lang.RuntimeException: java.lang.IllegalStateException: Invalid input path hdfs://dw/hive/warehouse/dm/dm_kafka_rdmp_dw/fvp_core_fact_route_op_hudi_op_new/inc_day=20221120/opcode=501/.00000006-2d6e-4d26-93ea-1026632abb67_20221119235956333.log.1_44-150-2
at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:169)
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.IllegalStateException: Invalid input path hdfs://dw/hive/warehouse/dm/dm_kafka_rdmp_dw/fvp_core_fact_route_op_hudi_op_new/inc_day=20221120/opcode=501/.00000006-2d6e-4d26-93ea-1026632abb67_20221119235956333.log.1_44-150-2
at org.apache.hadoop.hive.ql.exec.AbstractMapOperator.getNominalPath(AbstractMapOperator.java:119)
at org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:452)
at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1106)
at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:482)
at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:160)
... 8 more"	HUDI	Closed	1	1	4726	pull-request-available
13441639	Fix upgrade step wrt precombine field	"This is related to HUDI-3972

Details about the bug: if someone is not explicitly setting preCombine field, hudi still falls backs to the default value of ""ts"" and this goes into hoodie.properties as well. So, when reading MOR table, since we are projecting just the required columns, we also add preCombine field to it and if the original table does not have ""ts"", basic read fails (even if there are no log files, but just base files in MOR).

For the upgrade step, we need to update the preCombine field in the hoodie.properties.  If the user does not set the preCombine field, it should be removed from the hoodie.properties; if the users sets the field, it should be updated in the hoodie.properties."	HUDI	Closed	1	1	4726	pull-request-available
13396222	Implement scheduling of compaction/ clustering for Kafka Connect	"* Implement compaction/ clustering etc. from Java client
 * Schedule from Coordinator"	HUDI	Closed	1	3	4726	pull-request-available
13535244	Improve the test on incremental queries	The test `TestIncrementalReadWithFullTableScan#testFailEarlyForIncrViewQueryForNonExistingFiles` can fail due to changes in archival behavior, because of hard-coded parameters.	HUDI	Closed	2	4	4726	pull-request-available
13433731	Re-attempt of failed cleaning from DT to MDT fails 	"C5.clean.requested in DT 

C5.clean.inflight in DT

and then we try to apply this to MDT. 

C5.deltacommit.requested and C5.deltacommit.inflight

and crashed. 

 

If pipeline is restarted, cleaner will just go ahead retry the pending clean. It will not trigger rollback like compaction or clustering. So, This fails in MDT, since we only check for completed operation and avoid calling writeClient.startCommit(newInstantTime). 

 

SparkHoodieMetadataTableWriter 
{code:java}
if (!metadataMetaClient.getActiveTimeline().filterCompletedInstants().containsInstant(instantTime)) {
  // if this is a new commit being applied to metadata for the first time
  writeClient.startCommitWithTime(instantTime);
} else {
  // this code path refers to a re-attempted commit that got committed to metadata table, but failed in datatable.
  // for eg, lets say compaction c1 on 1st attempt succeeded in metadata table and failed before committing to datatable.
  // when retried again, data table will first rollback pending compaction. these will be applied to metadata table, but all changes
  // are upserts to metadata table and so only a new delta commit will be created.
  // once rollback is complete, compaction will be retried again, which will eventually hit this code block where the respective commit is
  // already part of completed commit. So, we have to manually remove the completed instant and proceed.
  // and it is for the same reason we enabled withAllowMultiWriteOnSameInstant for metadata table.
  HoodieInstant alreadyCompletedInstant = metadataMetaClient.getActiveTimeline().filterCompletedInstants().filter(entry -> entry.getTimestamp().equals(instantTime)).lastInstant().get();
  HoodieActiveTimeline.deleteInstantFile(metadataMetaClient.getFs(), metadataMetaClient.getMetaPath(), alreadyCompletedInstant);
  metadataMetaClient.reloadActiveTimeline();
}
List<WriteStatus> statuses = writeClient.upsertPreppedRecords(preppedRecordRDD, instantTime).collect();
 {code}
In above code snippet, the if condition should be fixed to check for any instant in timeline and not just completed. 

And within else block, if there is a completed instant, we should delete. if not, its a no-op. 

 

 

 "	HUDI	Closed	1	1	4726	pull-request-available
13423359	Spark datasource cannot read MOR table written by Kafka Connect Sink	"I'm hitting this on master with spark datasource reading MOR table from Kafka Connect Sink:
{code:java}
scala> val basePath = ""/tmp/hoodie/hudi-test-topic""
basePath: String = /tmp/hoodie/hudi-test-topic

scala> val df = spark.read.format(""hudi"").load(basePath)
org.apache.hudi.spark.org.apache.spark.sql.avro.IncompatibleSchemaException: Unexpected type null.
  at org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:199)
  at org.apache.hudi.MergeOnReadSnapshotRelation.liftedTree1$1(MergeOnReadSnapshotRelation.scala:74)
  at org.apache.hudi.MergeOnReadSnapshotRelation.tableAvroSchema$lzycompute(MergeOnReadSnapshotRelation.scala:69)
  at org.apache.hudi.MergeOnReadSnapshotRelation.tableAvroSchema(MergeOnReadSnapshotRelation.scala:68)
  at org.apache.hudi.MergeOnReadSnapshotRelation.tableStructSchema$lzycompute(MergeOnReadSnapshotRelation.scala:78)
  at org.apache.hudi.MergeOnReadSnapshotRelation.tableStructSchema(MergeOnReadSnapshotRelation.scala:78)
  at org.apache.hudi.MergeOnReadSnapshotRelation.schema(MergeOnReadSnapshotRelation.scala:97)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:440)
  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
  at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
  ... 47 elided {code}
I hit the same exception on Spark 2.4.4, 3.1.2, 3.2.0"	HUDI	Closed	1	1	4726	pull-request-available
13535693	CDC payload with op field for deletes do not work	"Delete operation in custom payload after RFC-46: while looking into a 0.13.1 release [blocker|https://github.com/apache/hudi/pull/8573], I found that custom payload implementation like AWS DMS payload and Debezium payload are not properly migrated to the new APIs introduced by RFC-46, causing the delete operation to fail.  Our tests did not catch this.  
 
It is currently assumed that delete records are marked by ""_hoodie_is_deleted""; however, custom CDC payloads use op field to mark deletes.
 
Impact:
OverwriteWithLatest payload(also OverwriteNonDefaultsWithLatestAvroPayload) are not affected.

for any other custom payloads: (AWSDMSAvropayload, All debezium payloads) deletes are broken. 
If someone is using ""_is_hoodie_deleted"" to enforce deletes, there are no issues w/ custome payloads.

COW: 
deleting a non-existant will break if not using ""_is_hoodie_deleted"" way.

MOR: 
any deletes will break if not using ""_is_hoodie_deleted"" way.

Writer:
all writers(spark, flink) except spark-sql.

DefaultHoodieRecordPayload delete marker support in 0.14.0 is also affected."	HUDI	Closed	1	1	4726	pull-request-available
13583571	Add Hudi CLI bundle for Scala 2.13	Build of Hudi CLI bundle should succeed on Scala 2.13 and work on Spark 3.5 and Scala 2.13.	HUDI	Closed	3	2	4726	pull-request-available
13557364	Use existing relation logic for queries reading base files only in Spark	`org.apache.hudi.functional.TestCOWDataSource#testHoodieIsDeletedCOW` fails with new file group reader based parquet file format.  To fix that, we temporarily fallback to existing logic for queries reading base files only.	HUDI	Closed	1	1	4726	pull-request-available
13528049	Fix the validation of partition listing in metadata table validator	"In HoodieMetadataTableValidator, we compare the partition listing between MDT and file system:
{code:java}
// ignore partitions created by uncommitted ingestion.
allPartitionPathsFromFS = allPartitionPathsFromFS.stream().parallel().filter(part -> {
  HoodiePartitionMetadata hoodiePartitionMetadata =
      new HoodiePartitionMetadata(metaClient.getFs(), FSUtils.getPartitionPath(basePath, part));

  Option<String> instantOption = hoodiePartitionMetadata.readPartitionCreatedCommitTime();
  if (instantOption.isPresent()) {
    String instantTime = instantOption.get();
    return completedTimeline.containsOrBeforeTimelineStarts(instantTime);
  } else {
    return false;
  }
}).collect(Collectors.toList());

List<String> allPartitionPathsMeta = FSUtils.getAllPartitionPaths(engineContext, basePath, true, cfg.assumeDatePartitioning);

Collections.sort(allPartitionPathsFromFS);
Collections.sort(allPartitionPathsMeta);

if (allPartitionPathsFromFS.size() != allPartitionPathsMeta.size()
    || !allPartitionPathsFromFS.equals(allPartitionPathsMeta)) {
  String message = ""Compare Partitions Failed! "" + ""AllPartitionPathsFromFS : "" + allPartitionPathsFromFS + "" and allPartitionPathsMeta : "" + allPartitionPathsMeta;
  LOG.error(message);
  throw new HoodieValidationException(message);
} {code}
When deciding the partitions from the file system to consider for comparison, we look at the commit time that creates the partition.
{code:java}
if (instantOption.isPresent()) { String instantTime = instantOption.get(); return completedTimeline.containsOrBeforeTimelineStarts(instantTime); } else { return false; } {code}
In the following scenario, the validation job fires a false alarm complaining that the partition list returned by the file system and the metadata table because of this check:
- Commit C1 creates the partition, the partition metadata is written, and C1 fails during writing data files.  Next time, C2 adds new data to the same partition after C1 is rolled back. In this case, the partition metadata still has C1 as the created commit time, since Hudi does not rewrite the partition metadata in C2.

 "	HUDI	Closed	1	1	4726	pull-request-available
13528692	"Revert ""[HUDI-4675] add unittest for RebalancedSparkHoodieHBaseIndex (#6458)"""	"HUDI-4675 causes the Azure CI to fail:
{code:java}
2023-03-15T00:57:04.0841798Z [INFO] Results:
2023-03-15T00:57:04.0841931Z [INFO] 
2023-03-15T00:57:04.0842053Z [ERROR] Failures: 
2023-03-15T00:57:04.0842236Z [ERROR] TestRebalancedSparkHoodieHBaseIndex.testGetHBaseKeyWithPrefix()(TestRebalancedSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0842451Z [ERROR]   Run 1: expected: <true> but was: <false>
2023-03-15T00:57:04.0842637Z [ERROR]   Run 2: expected: <true> but was: <false>
2023-03-15T00:57:04.0842783Z [ERROR]   Run 3: expected: <true> but was: <false>
2023-03-15T00:57:04.0842942Z [ERROR]   Run 4: expected: <true> but was: <false>
2023-03-15T00:57:04.0843086Z [INFO] 
2023-03-15T00:57:04.0843228Z [ERROR] TestSparkHoodieHBaseIndex.testDelete()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0843416Z [ERROR]   Run 1: expected: <10> but was: <0>
2023-03-15T00:57:04.0843568Z [ERROR]   Run 2: expected: <10> but was: <0>
2023-03-15T00:57:04.0843705Z [ERROR]   Run 3: expected: <10> but was: <0>
2023-03-15T00:57:04.0843855Z [ERROR]   Run 4: expected: <10> but was: <0>
2023-03-15T00:57:04.0843969Z [INFO] 
2023-03-15T00:57:04.0844149Z [ERROR] TestSparkHoodieHBaseIndex.testEnsureTagLocationUsesCommitTimeline()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0844337Z [ERROR]   Run 1: null
2023-03-15T00:57:04.0844451Z [ERROR]   Run 2: null
2023-03-15T00:57:04.0844579Z [ERROR]   Run 3: null
2023-03-15T00:57:04.0844704Z [ERROR]   Run 4: null
2023-03-15T00:57:04.0845192Z [INFO] 
2023-03-15T00:57:04.0845375Z [ERROR] TestSparkHoodieHBaseIndex.testHbaseTagLocationForArchivedCommits()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0845573Z [ERROR]   Run 1: expected: <20> but was: <0>
2023-03-15T00:57:04.0845712Z [ERROR]   Run 2: expected: <20> but was: <0>
2023-03-15T00:57:04.0845863Z [ERROR]   Run 3: expected: <20> but was: <0>
2023-03-15T00:57:04.0845998Z [ERROR]   Run 4: expected: <20> but was: <0>
2023-03-15T00:57:04.0846128Z [INFO] 
2023-03-15T00:57:04.0846311Z [ERROR] TestSparkHoodieHBaseIndex.testSimpleTagLocationAndUpdateWithRollback()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0848613Z [ERROR]   Run 1: expected: <10> but was: <0>
2023-03-15T00:57:04.0852965Z [ERROR]   Run 2: expected: <10> but was: <0>
2023-03-15T00:57:04.0856734Z [ERROR]   Run 3: expected: <10> but was: <0>
2023-03-15T00:57:04.0869490Z [ERROR]   Run 4: expected: <10> but was: <0>
2023-03-15T00:57:04.0869894Z [INFO] 
2023-03-15T00:57:04.0870118Z [ERROR] TestSparkHoodieHBaseIndex.testSimpleTagLocationWithInvalidCommit()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0870359Z [ERROR]   Run 1: null
2023-03-15T00:57:04.0870499Z [ERROR]   Run 2: null
2023-03-15T00:57:04.0870653Z [ERROR]   Run 3: null
2023-03-15T00:57:04.0871135Z [ERROR]   Run 4: null
2023-03-15T00:57:04.0871421Z [INFO] 
2023-03-15T00:57:04.0871770Z [ERROR] TestSparkHoodieHBaseIndex.testSmallBatchSize()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0871960Z [ERROR]   Run 1: expected: <10> but was: <0>
2023-03-15T00:57:04.0872127Z [ERROR]   Run 2: expected: <10> but was: <0>
2023-03-15T00:57:04.0872294Z [ERROR]   Run 3: expected: <10> but was: <0>
2023-03-15T00:57:04.0872756Z [ERROR]   Run 4: expected: <10> but was: <0>
2023-03-15T00:57:04.0873060Z [INFO] 
2023-03-15T00:57:04.0873240Z [ERROR] TestSparkHoodieHBaseIndex.testTagLocationAndDuplicateUpdate()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0873427Z [ERROR]   Run 1: expected: <10> but was: <0>
2023-03-15T00:57:04.0873581Z [ERROR]   Run 2: expected: <10> but was: <0>
2023-03-15T00:57:04.0873744Z [ERROR]   Run 3: expected: <10> but was: <0>
2023-03-15T00:57:04.0873882Z [ERROR]   Run 4: expected: <10> but was: <0>
2023-03-15T00:57:04.0874016Z [INFO] 
2023-03-15T00:57:04.0874186Z [ERROR] TestSparkHoodieHBaseIndex.testTagLocationAndPartitionPathUpdate()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0874391Z [ERROR]   Run 1: expected: <20> but was: <0>
2023-03-15T00:57:04.0874544Z [ERROR]   Run 2: expected: <20> but was: <0>
2023-03-15T00:57:04.0874681Z [ERROR]   Run 3: expected: <20> but was: <0>
2023-03-15T00:57:04.0874838Z [ERROR]   Run 4: expected: <20> but was: <0>
2023-03-15T00:57:04.0874971Z [INFO] 
2023-03-15T00:57:04.0875075Z [ERROR] Errors: 
2023-03-15T00:57:04.0875462Z [ERROR] TestSparkHoodieHBaseIndex.testTagLocationAndPartitionPathUpdateWithExplicitRollback()(TestSparkHoodieHBaseIndex)
2023-03-15T00:57:04.0876209Z [ERROR]   Run 1: Failed to rollback hdfs://localhost:43515/user/vsts/test-data/e29a14fc-7783-2be0-a4f5-ca8aad1139d1/test_table commits 20230315005319759
2023-03-15T00:57:04.0876704Z [ERROR]   Run 2: Failed to rollback hdfs://localhost:40419/user/vsts/test-data/d216b5d2-345e-af82-0474-4cb9844d468f/test_table commits 20230315005425235
2023-03-15T00:57:04.0877203Z [ERROR]   Run 3: Failed to rollback hdfs://localhost:44885/user/vsts/test-data/6e944ee9-3a54-17a4-8362-5451a35a429b/test_table commits 20230315005535359
2023-03-15T00:57:04.0877848Z [ERROR]   Run 4: Failed to rollback hdfs://localhost:34187/user/vsts/test-data/549190ee-26be-6b70-9c57-dfaa74be8764/test_table commits 20230315005645264
2023-03-15T00:57:04.0878074Z [INFO] 
2023-03-15T00:57:04.0878183Z [WARNING] Flakes: 
2023-03-15T00:57:04.0878705Z [WARNING] HoodieTableType).[1] COPY_ON_WRITE(testSimpleTagLocationAndUpdate(HoodieTableType))
2023-03-15T00:57:04.0879053Z [INFO]   Run 1: PASS
2023-03-15T00:57:04.0879386Z [ERROR]   Run 2: expected: <10> but was: <0>
2023-03-15T00:57:04.0879769Z [INFO] 
2023-03-15T00:57:04.0880120Z [WARNING] HoodieTableType).[2] MERGE_ON_READ(testSimpleTagLocationAndUpdate(HoodieTableType))
2023-03-15T00:57:04.0880489Z [INFO]   Run 1: PASS
2023-03-15T00:57:04.0880654Z [ERROR]   Run 2: expected: <10> but was: <0>
2023-03-15T00:57:04.0880788Z [INFO] 
2023-03-15T00:57:04.0880924Z [INFO] 
2023-03-15T00:57:04.0881100Z [ERROR] Tests run: 338, Failures: 9, Errors: 1, Skipped: 0, Flakes: 2
2023-03-15T00:57:04.0881261Z [INFO] 
2023-03-15T00:57:04.1004705Z [INFO] ------------------------------------------------------------------------
2023-03-15T00:57:04.1004973Z [INFO] BUILD FAILURE
2023-03-15T00:57:04.1005226Z [INFO] ------------------------------------------------------------------------
2023-03-15T00:57:04.1005429Z [INFO] Total time:  01:19 h
2023-03-15T00:57:04.1005671Z [INFO] Finished at: 2023-03-15T00:57:04Z
2023-03-15T00:57:04.1006092Z [INFO] ------------------------------------------------------------------------ {code}"	HUDI	Open	1	1	4726	pull-request-available
13515862	Improve performance of savepoint with MDT	"[https://github.com/apache/hudi/issues/7541]

When metadata table is enabled, the savepoint operation is slow for a large number of partitions (e.g., 75k).  The root cause is that for each partition, the metadata table is scanned, which is unnecessary."	HUDI	Closed	1	4	4726	pull-request-available
13435194	Fix areAnyTableServicesInline() in HoodieWriteConfig	"{code:java}
public Boolean areAnyTableServicesInline() {
    return inlineClusteringEnabled() || inlineCompactionEnabled() || isAutoClean();
  } {code}
Member  vinothchandar yesterday

should this check for asyncClean instead of just isAutoClean() 

Member  yihua 1 minute ago

Yes, both asyncClean and isAutoClean should be checked here. But this is not used for clean-related logic. "	HUDI	Closed	1	1	4726	pull-request-available
13557219	Support reading only log files in file group reader-based Spark parquet file format	"In HoodieFileGroupReaderBasedParquetFileFormat, reading log file-only slice is not supported. This should be fixed:
{code:java}
if (FSUtils.isLogFile(filePath)) {
  // TODO: Use FileGroupReader here: HUDI-6942.
  throw new NotImplementedError(""Not support reading with only log files"")
} {code}"	HUDI	Closed	1	4	4726	pull-request-available
13557101	Fix partial merging logic based on projected schema	When querying the table with multiple round of partial updates generating multiple log files, the partial merging logic may fail or give wrong results due to schema handling and merging logic.	HUDI	Closed	1	1	4726	pull-request-available
13435649	Upsert to metadata table fails due to schema change	"Scenario: Deltastreamer continuous mode, COW table, single writer with async clustering and cleaning.  Only files partition is enabled in metadata table.  The table is written before the metadata schema change (adding ""columnName"").  When using the new writer with the new schema, the upsert to metadata table fails with schema compatibility check. 
{code:java}
22/03/23 23:11:38 WARN CleanActionExecutor: Failed to perform previous clean operation, instant: [==>20220314172020474__clean__INFLIGHT]
org.apache.hudi.exception.HoodieUpsertException: Failed upsert schema compatibility check.
    at org.apache.hudi.table.HoodieTable.validateUpsertSchema(HoodieTable.java:729)
    at org.apache.hudi.client.SparkRDDWriteClient.upsertPreppedRecords(SparkRDDWriteClient.java:169)
    at org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter.commit(SparkHoodieBackedTableMetadataWriter.java:154)
    at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.processAndCommit(HoodieBackedTableMetadataWriter.java:670)
    at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.update(HoodieBackedTableMetadataWriter.java:694)
    at org.apache.hudi.table.action.BaseActionExecutor.lambda$writeTableMetadata$1(BaseActionExecutor.java:69)
    at org.apache.hudi.common.util.Option.ifPresent(Option.java:97)
    at org.apache.hudi.table.action.BaseActionExecutor.writeTableMetadata(BaseActionExecutor.java:69)
    at org.apache.hudi.table.action.clean.CleanActionExecutor.runClean(CleanActionExecutor.java:211)
    at org.apache.hudi.table.action.clean.CleanActionExecutor.runPendingClean(CleanActionExecutor.java:176)
    at org.apache.hudi.table.action.clean.CleanActionExecutor.lambda$execute$6(CleanActionExecutor.java:238)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.apache.hudi.table.action.clean.CleanActionExecutor.execute(CleanActionExecutor.java:232)
    at org.apache.hudi.table.HoodieSparkCopyOnWriteTable.clean(HoodieSparkCopyOnWriteTable.java:339)
    at org.apache.hudi.client.BaseHoodieWriteClient.clean(BaseHoodieWriteClient.java:781)
    at org.apache.hudi.client.BaseHoodieWriteClient.clean(BaseHoodieWriteClient.java:738)
    at org.apache.hudi.async.AsyncCleanerService.lambda$startService$0(AsyncCleanerService.java:55)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hudi.exception.HoodieException: Failed schema compatibility check for writerSchema :{""type"":""record"",""name"":""HoodieMetadataRecord"",""namespace"":""org.apache.hudi.avro.model"",""doc"":""A record saved within the Metadata Table"",""fields"":[{""name"":""_hoodie_commit_time"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_commit_seqno"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_record_key"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_partition_path"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_file_name"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""key"",""type"":{""type"":""string"",""avro.java.string"":""String""}},{""name"":""type"",""type"":""int"",""doc"":""Type of the metadata record""},{""name"":""filesystemMetadata"",""type"":[""null"",{""type"":""map"",""values"":{""type"":""record"",""name"":""HoodieMetadataFileInfo"",""fields"":[{""name"":""size"",""type"":""long"",""doc"":""Size of the file""},{""name"":""isDeleted"",""type"":""boolean"",""doc"":""True if this file has been deleted""}]},""avro.java.string"":""String""}],""doc"":""Contains information about partitions and files within the dataset""},{""name"":""BloomFilterMetadata"",""type"":[""null"",{""type"":""record"",""name"":""HoodieMetadataBloomFilter"",""doc"":""Data file bloom filter details"",""fields"":[{""name"":""type"",""type"":{""type"":""string"",""avro.java.string"":""String""},""doc"":""Bloom filter type code""},{""name"":""timestamp"",""type"":{""type"":""string"",""avro.java.string"":""String""},""doc"":""Instant timestamp when this metadata was created/updated""},{""name"":""bloomFilter"",""type"":""bytes"",""doc"":""Bloom filter binary byte array""},{""name"":""isDeleted"",""type"":""boolean"",""doc"":""Bloom filter entry valid/deleted flag""}]}],""doc"":""Metadata Index of bloom filters for all data files in the user table"",""default"":null},{""name"":""ColumnStatsMetadata"",""type"":[""null"",{""type"":""record"",""name"":""HoodieMetadataColumnStats"",""doc"":""Data file column statistics"",""fields"":[{""name"":""fileName"",""type"":[""null"",{""type"":""string"",""avro.java.string"":""String""}],""doc"":""File name for which this column statistics applies""},{""name"":""columnName"",""type"":[""null"",{""type"":""string"",""avro.java.string"":""String""}],""doc"":""Column name for which this column statistics applies""},{""name"":""minValue"",""type"":[""null"",{""type"":""string"",""avro.java.string"":""String""}],""doc"":""Minimum value in the range. Based on user data table schema, we can convert this to appropriate type""},{""name"":""maxValue"",""type"":[""null"",{""type"":""string"",""avro.java.string"":""String""}],""doc"":""Maximum value in the range. Based on user data table schema, we can convert it to appropriate type""},{""name"":""valueCount"",""type"":[""null"",""long""],""doc"":""Total count of values""},{""name"":""nullCount"",""type"":[""null"",""long""],""doc"":""Total count of null values""},{""name"":""totalSize"",""type"":[""null"",""long""],""doc"":""Total storage size on disk""},{""name"":""totalUncompressedSize"",""type"":[""null"",""long""],""doc"":""Total uncompressed storage size on disk""},{""name"":""isDeleted"",""type"":""boolean"",""doc"":""Column range entry valid/deleted flag""}]}],""doc"":""Metadata Index of column statistics for all data files in the user table"",""default"":null}]}, table schema :{""type"":""record"",""name"":""HoodieMetadataRecord"",""namespace"":""org.apache.hudi.avro.model"",""doc"":""A record saved within the Metadata Table"",""fields"":[{""name"":""_hoodie_commit_time"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_commit_seqno"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_record_key"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_partition_path"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""_hoodie_file_name"",""type"":[""null"",""string""],""doc"":"""",""default"":null},{""name"":""key"",""type"":{""type"":""string"",""avro.java.string"":""String""}},{""name"":""type"",""type"":""int"",""doc"":""Type of the metadata record""},{""name"":""filesystemMetadata"",""type"":[""null"",{""type"":""map"",""values"":{""type"":""record"",""name"":""HoodieMetadataFileInfo"",""fields"":[{""name"":""size"",""type"":""long"",""doc"":""Size of the file""},{""name"":""isDeleted"",""type"":""boolean"",""doc"":""True if this file has been deleted""}]},""avro.java.string"":""String""}],""doc"":""Contains information about partitions and files within the dataset""},{""name"":""BloomFilterMetadata"",""type"":[""null"",{""type"":""record"",""name"":""HoodieMetadataBloomFilter"",""doc"":""Data file bloom filter details"",""fields"":[{""name"":""type"",""type"":{""type"":""string"",""avro.java.string"":""String""},""doc"":""Bloom filter type code""},{""name"":""timestamp"",""type"":{""type"":""string"",""avro.java.string"":""String""},""doc"":""Instant timestamp when this metadata was created/updated""},{""name"":""bloomFilter"",""type"":""bytes"",""doc"":""Bloom filter binary byte array""},{""name"":""isDeleted"",""type"":""boolean"",""doc"":""Bloom filter entry valid/deleted flag""}]}],""doc"":""Metadata Index of bloom filters for all data files in the user table"",""default"":null},{""name"":""ColumnStatsMetadata"",""type"":[""null"",{""type"":""record"",""name"":""HoodieMetadataColumnStats"",""doc"":""Data file column statistics"",""fields"":[{""name"":""fileName"",""type"":[""null"",{""type"":""string"",""avro.java.string"":""String""}],""doc"":""File name for which this column statistics applies""},{""name"":""minValue"",""type"":[""null"",{""type"":""string"",""avro.java.string"":""String""}],""doc"":""Minimum value in the range. Based on user data table schema, we can convert this to appropriate type""},{""name"":""maxValue"",""type"":[""null"",{""type"":""string"",""avro.java.string"":""String""}],""doc"":""Maximum value in the range. Based on user data table schema, we can convert it to appropriate type""},{""name"":""valueCount"",""type"":[""null"",""long""],""doc"":""Total count of values""},{""name"":""nullCount"",""type"":[""null"",""long""],""doc"":""Total count of null values""},{""name"":""totalSize"",""type"":[""null"",""long""],""doc"":""Total storage size on disk""},{""name"":""totalUncompressedSize"",""type"":[""null"",""long""],""doc"":""Total uncompressed storage size on disk""},{""name"":""isDeleted"",""type"":""boolean"",""doc"":""Column range entry valid/deleted flag""}]}],""doc"":""Metadata Index of column statistics for all data files in the user table"",""default"":null}]}, base path :file:/Users/ethan/Work/scripts/mt_rollout_testing/deploy_b_single_writer_async_services/b3_ds_cow_010mt_011mt_conf_fix2/test_table/.hoodie/metadata
    at org.apache.hudi.table.HoodieTable.validateSchema(HoodieTable.java:721)
    at org.apache.hudi.table.HoodieTable.validateUpsertSchema(HoodieTable.java:727)
    ... 20 more {code}
 

 "	HUDI	Closed	1	1	4726	pull-request-available
13559416	Fix operation type for bulk insert with row writer in Hudi Streamer	"{code:java}
""operationType"" : null {code}
The operationType is null in the commit metadata of bulk insert operation with row writer enabled in Hudi Streamer (hoodie.streamer.write.row.writer.enable=true)."	HUDI	Closed	1	1	4726	pull-request-available
13524275	Allow lazy rollback for async indexer commit	"This is to fix HUDI-5733, where async indexer may fail due to eager rollback in metadata table.

Temporary solution for 0.13.0: Little more invovled and not so clean fix. Apply eager rollbacks only for regular delta commits. Deduce delta commits from HoodieIndexer and employ lazy clean policy(based on heartbeat). "	HUDI	Closed	1	1	4726	pull-request-available
13469038	Update docs for building Hudi for the docker demo	"[https://github.com/apache/hudi/pull/5787]

`-Pintegration-tests` argument is required now for the mvn build of Hudi.  Otherwise, the docker demo does not work."	HUDI	Closed	3	4	4726	pull-request-available
13411168	Use earliest instant by default for compaction and clustering job	Currently, HoodieCompactor (compaction) and HoodieClusteringJob (clustering) require a command-line argument of the instant time for async executions.  To improve the usability of these jobs, by default the jobs can search for the earliest instant of the corresponding action for execution, to save one step of searching the instant time from the user.	HUDI	Closed	1	4	4726	pull-request-available
13480198	Cannot find partition column when querying bootstrapped table in Spark	"Bootstrap table:

{code:java}
val srcPath = ""<>/bootstrap-testing/partitioned-parquet-table-date""
val basePath = ""<>/bootstrap-testing/bootstrap-hudi-table-2""
val bootstrapDF = spark.emptyDataFrame
bootstrapDF.write
      .format(""hudi"")
      .option(HoodieWriteConfig.TABLE_NAME, ""hoodie_test"")
      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL)
      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, ""key"")
      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, ""partition"")
      .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, ""ts"")
      .option(HoodieBootstrapConfig.BOOTSTRAP_BASE_PATH_PROP, srcPath)
      .option(HoodieBootstrapConfig.BOOTSTRAP_KEYGEN_CLASS, classOf[SimpleKeyGenerator].getName)
      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR, classOf[BootstrapRegexModeSelector].getName)
      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX, ""2022/1/2[4-8]"")
      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX_MODE, ""METADATA_ONLY"")
      .option(HoodieBootstrapConfig.FULL_BOOTSTRAP_INPUT_PROVIDER, classOf[SparkParquetBootstrapDataProvider].getName)
      .mode(SaveMode.Overwrite)
      .save(basePath) {code}
 
{code:java}
scala> spark.sql(""select partition, _hoodie_partition_path, count(*) from test_table group by partition, _hoodie_partition_path "")
org.apache.spark.sql.AnalysisException: cannot resolve 'partition' given input columns: [test_table._hoodie_commit_seqno, test_table._hoodie_commit_time, test_table._hoodie_file_name, test_table._hoodie_partition_path, test_table._hoodie_record_key, test_table.arrayField, test_table.decimalField, test_table.key, test_table.longField, test_table.mapField, test_table.round, test_table.textField, test_table.ts]; line 1 pos 76;
'Aggregate ['partition, _hoodie_partition_path#912], ['partition, _hoodie_partition_path#912, count(1) AS count(1)#956L]
+- SubqueryAlias test_table
   +- View (`test_table`, [_hoodie_commit_time#909,_hoodie_commit_seqno#910,_hoodie_record_key#911,_hoodie_partition_path#912,_hoodie_file_name#913,key#914,ts#915L,textField#916,decimalField#917,longField#918L,arrayField#919,mapField#920,round#921])
      +- Relation [_hoodie_commit_time#909,_hoodie_commit_seqno#910,_hoodie_record_key#911,_hoodie_partition_path#912,_hoodie_file_name#913,key#914,ts#915L,textField#916,decimalField#917,longField#918L,arrayField#919,mapField#920,round#921] org.apache.hudi.HoodieBootstrapRelation@2daee5f


  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:179)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:175)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:535)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:535)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:181)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:286)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:181)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:161)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:175)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  ... 47 elided {code}
 "	HUDI	Closed	3	1	4726	pull-request-available
13475809	Fix schema evolution docs	Fix any incorrect information on the schema evolution docs page.	HUDI	Closed	1	4	4726	pull-request-available
13438055	Add separate configs to control multi-modal index usage in write-side indexing 	"We'll have three sets of configs regarding MDT/multi-modal indexing to control:

(1) whether the write-side should update metadata table partitions,

(2) whether query engine should use multi-modal indexing for data skipping,

(3) whether indexing/tagging in the write pipeline should use MDT index."	HUDI	Closed	1	3	4726	pull-request-available
13572467	Fix HoodieMetadataPayload merging logic around repeated deletes	"When there are repeated duplicate deletes to the partition file list in files partition of the MDT, the current HoodieMetadataPayload merging logic drops such ""deletion"", causing the file that is deleted from the file system and supposed to be deleted from MDT file listing still left in MDT, because of the following logic:
{code:java}
private Map<String, HoodieMetadataFileInfo> combineFileSystemMetadata(HoodieMetadataPayload previousRecord) {
    Map<String, HoodieMetadataFileInfo> combinedFileInfo = new HashMap<>();

    // First, add all files listed in the previous record
    if (previousRecord.filesystemMetadata != null) {
      combinedFileInfo.putAll(previousRecord.filesystemMetadata);
    }

    // Second, merge in the files listed in the new record
    if (filesystemMetadata != null) {
      validatePayload(type, filesystemMetadata);

      filesystemMetadata.forEach((key, fileInfo) -> {
        combinedFileInfo.merge(key, fileInfo,
            (oldFileInfo, newFileInfo) ->
                newFileInfo.getIsDeleted()
                    ? null
                    : new HoodieMetadataFileInfo(Math.max(newFileInfo.getSize(), oldFileInfo.getSize()), false));
      });
    } {code}
Here's a concrete example of how this bug causes the ingestion to fail:

(1) A data file and file group are replaced by clustering.  The data file is still on the file system and in MDT file listing.

(2) A cleaner plan is generated to delete the data file.

(3) The cleaner plan is executed the first time, and fails before commit due to Spark job shutdown.

(4) The ingestion continues and succeeds, and another cleaner plan is generated containing the same data file/file group to delete.

(5) The first cleaner plan is successfully executed, incurring deletion to the file list with a metadata payload, and this is added to one log file in MDT, e.g.,
{code:java}
HoodieMetadataPayload {key=partition, type=2, Files: {creations=[], deletions=[7f6b146e-cd43-4fd3-9ce0-118232562569-0_63-29223-5579389_20240303214408245.parquet], }}{code}
(6) The second cleaner plan is also successfully executed, incurring deletion to the file list with a metadata payload containing the same data file to delete, and this is added to a subsequent log file in the same file slice in MDT, e.g.,
{code:java}
HoodieMetadataPayload {key=partition, type=2, Files: {creations=[], deletions=[7f6b146e-cd43-4fd3-9ce0-118232562569-0_63-29223-5579389_20240303214408245.parquet], }} {code}
(7) The replacecommit corresponds to the clustering is archived as the cleaner has deleted the replaced file groups.

(8) When reading MDT or MDT compaction happens, the merging of these two metadata payloads with identical deletes leads to empty deletion, so the data file is not deleted from the partition file list in MDT.  The expected behavior is to keep the data file in the ""deletions"" field.
{code:java}
HoodieMetadataPayload {key=partition, type=2, Files: {creations=[], deletions=[], }}{code}
(9) Next time, when doing upsert and indexing, the deleted data file is served by the file system view based on MDT (e.g., ""7f6b146e-cd43-4fd3-9ce0-118232562569-0_63-29223-5579389_20240303214408245.parquet""), and the data file cannot be found on the file system, causing the ingestion to fail."	HUDI	Closed	1	1	4726	pull-request-available
13510330	"Adjust coalesce behavior within ""NONE"" sort mode for bulk insert"	The NONE sort mode for bulk insert does coalesce for the input records or rows based on the shuffle parallelism of bulk insert ({{{}hoodie.bulkinsert.shuffle.parallelism{}}}) to reduce the parallelism. This could affect write latency if the cluster workers are not fully utilized due to reduced parallelism.	HUDI	Closed	1	4	4726	pull-request-available
13582058	Bump apache-rat-plugin to 0.16.1 to eliminate thread-safe warning in maven parallel build	"The following warning is thrown when doing maven parallel build with `mvn -T 1C ...`
{code:java}
[WARNING] Enable debug to see precisely which goals are not marked as thread-safe.
[WARNING] *****************************************************************
[WARNING] * Your build is requesting parallel execution, but this         *
[WARNING] * project contains the following plugin(s) that have goals not  *
[WARNING] * marked as thread-safe to support parallel execution.          *
[WARNING] * While this /may/ work fine, please look for plugin updates    *
[WARNING] * and/or request plugins be made thread-safe.                   *
[WARNING] * If reporting an issue, report it against the plugin in        *
[WARNING] * question, not against Apache Maven.                           *
[WARNING] *****************************************************************
[WARNING] The following plugins are not marked as thread-safe in hudi-hadoop-mr:
[WARNING]   org.apache.rat:apache-rat-plugin:0.13 {code}"	HUDI	Closed	3	4	4726	pull-request-available
13479365	Add examples of soft deletes in docs	We need to add code examples of soft deletes to quick start guide.	HUDI	Closed	3	4	4726	pull-request-available
13535294	Fix lock identity in InProcessLockProvider	"#6847 extends the `InProcessLockProvider` to support multiple tables in the same process, by having an in-memory static final map storing the mapping of the table base path to the read-write reentrant lock, so that the writer uses the corresponding lock based on the base path.  When closing the lock provider, `close()` removes the lock entry.  Since `close()` is called when closing the write client, the lock is removed and subsequent concurrent writers will get a different lock instance on the same table, causing the locking mechanism on the same table to be useless.  Take the following example where three writers write to the same table concurrently and need to acquire the in-process lock:

```
Writer 1:   lock |----------------| unlock and close
Writer 2:   try lock   |      ...       lock |------| unlock and close
Writer 3:                                    try lock  | ...  lock |------| unlock and close
```

after Writer 1 releases the lock and closes the lock provider, the lock instance is removed from the map, and Writer 3 will get a different lock instance compared to Writer 2."	HUDI	Closed	1	1	4726	pull-request-available
13524396	Improve deploy script of release artifacts	Current script is inefficient as some artifacts are repeatedly uploaded which wastes time.	HUDI	Closed	1	4	4726	pull-request-available
13525107	Fix async indexer metadata writer to avoid eager rollback / cleaning	"Even though the async indexer metadata writer is configured to use LAZY failed write cleaning policy, there is other logic that can potentially roll back the delta commits from regular metadata writer, e.g., since heartbeats are disabled for regular MDT writes, failed write cleaning is going to rollback commits regardless.  We need to fix this so that the async indexer metadata writer does not touch other delta commits in the MDT.

 

This can cause the following test to be flaky:
{code:java}
2023-02-16T13:46:06.1573775Z [ERROR] Tests run: 113, Failures: 0, Errors: 1, Skipped: 2, Time elapsed: 3,518.191 s <<< FAILURE! - in org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer
2023-02-16T13:46:06.1576031Z [ERROR] testHoodieIndexer{HoodieRecordType}[2]  Time elapsed: 79.838 s  <<< ERROR!
2023-02-16T13:46:06.1576937Z java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.hudi.exception.HoodieException
2023-02-16T13:46:06.1577820Z 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2023-02-16T13:46:06.1578597Z 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2023-02-16T13:46:06.1579536Z 	at org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer.deltaStreamerTestRunner(TestHoodieDeltaStreamer.java:901)
2023-02-16T13:46:06.1580628Z 	at org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer.deltaStreamerTestRunner(TestHoodieDeltaStreamer.java:884)
2023-02-16T13:46:06.1581740Z 	at org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer.deltaStreamerTestRunner(TestHoodieDeltaStreamer.java:929)
2023-02-16T13:46:06.1582838Z 	at org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer.testHoodieIndexer(TestHoodieDeltaStreamer.java:1163)
2023-02-16T13:46:06.1583757Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-02-16T13:46:06.1584522Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-02-16T13:46:06.1585420Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-02-16T13:46:06.1586228Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2023-02-16T13:46:06.1587044Z 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
2023-02-16T13:46:06.1587939Z 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-02-16T13:46:06.1594100Z 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-02-16T13:46:06.1595968Z 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2023-02-16T13:46:06.1597065Z 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2023-02-16T13:46:06.1598245Z 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2023-02-16T13:46:06.1599109Z 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2023-02-16T13:46:06.1599961Z 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2023-02-16T13:46:06.1600807Z 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-02-16T13:46:06.1601654Z 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-02-16T13:46:06.1602480Z 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-02-16T13:46:06.1603309Z 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-02-16T13:46:06.1604064Z 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2023-02-16T13:46:06.1604757Z 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2023-02-16T13:46:06.1605552Z 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
2023-02-16T13:46:06.1606381Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1607176Z 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
2023-02-16T13:46:06.1607988Z 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
2023-02-16T13:46:06.1608891Z 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
2023-02-16T13:46:06.1610205Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2023-02-16T13:46:06.1611066Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1611875Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2023-02-16T13:46:06.1612589Z 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-16T13:46:06.1613320Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2023-02-16T13:46:06.1614132Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1614898Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2023-02-16T13:46:06.1615656Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2023-02-16T13:46:06.1616533Z 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2023-02-16T13:46:06.1617479Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
2023-02-16T13:46:06.1618327Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
2023-02-16T13:46:06.1619168Z 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2023-02-16T13:46:06.1620011Z 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2023-02-16T13:46:06.1620740Z 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2023-02-16T13:46:06.1621378Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1623243Z 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2023-02-16T13:46:06.1623930Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1624573Z 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2023-02-16T13:46:06.1625208Z 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:440)
2023-02-16T13:46:06.1625833Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1626464Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1627095Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1627708Z 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2023-02-16T13:46:06.1628338Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1629056Z 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2023-02-16T13:46:06.1629666Z 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2023-02-16T13:46:06.1630323Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2023-02-16T13:46:06.1630960Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2023-02-16T13:46:06.1631599Z 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2023-02-16T13:46:06.1632269Z 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2023-02-16T13:46:06.1632922Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2023-02-16T13:46:06.1633525Z 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2023-02-16T13:46:06.1634150Z 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2023-02-16T13:46:06.1635825Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1636458Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1637102Z 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2023-02-16T13:46:06.1637751Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2023-02-16T13:46:06.1638378Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2023-02-16T13:46:06.1639012Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2023-02-16T13:46:06.1639664Z 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2023-02-16T13:46:06.1640320Z 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2023-02-16T13:46:06.1640972Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2023-02-16T13:46:06.1641589Z 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2023-02-16T13:46:06.1642206Z 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2023-02-16T13:46:06.1642854Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2023-02-16T13:46:06.1643488Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2023-02-16T13:46:06.1644133Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2023-02-16T13:46:06.1644786Z 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2023-02-16T13:46:06.1645462Z 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2023-02-16T13:46:06.1646097Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2023-02-16T13:46:06.1646717Z 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2023-02-16T13:46:06.1647434Z 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2023-02-16T13:46:06.1648237Z 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2023-02-16T13:46:06.1649207Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2023-02-16T13:46:06.1650019Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1650811Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2023-02-16T13:46:06.1651547Z 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-16T13:46:06.1652275Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2023-02-16T13:46:06.1653086Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1653857Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2023-02-16T13:46:06.1654614Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2023-02-16T13:46:06.1655234Z 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2023-02-16T13:46:06.1655994Z 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2023-02-16T13:46:06.1656929Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2023-02-16T13:46:06.1657733Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1658520Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2023-02-16T13:46:06.1659252Z 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-16T13:46:06.1660059Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2023-02-16T13:46:06.1660849Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1661623Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2023-02-16T13:46:06.1662367Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2023-02-16T13:46:06.1662967Z 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2023-02-16T13:46:06.1663737Z 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2023-02-16T13:46:06.1664670Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2023-02-16T13:46:06.1665480Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1666268Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2023-02-16T13:46:06.1667003Z 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-02-16T13:46:06.1667736Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2023-02-16T13:46:06.1668522Z 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-02-16T13:46:06.1670214Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2023-02-16T13:46:06.1670985Z 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2023-02-16T13:46:06.1671842Z 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2023-02-16T13:46:06.1672789Z 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2023-02-16T13:46:06.1674085Z 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
2023-02-16T13:46:06.1674890Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)
2023-02-16T13:46:06.1675698Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2023-02-16T13:46:06.1676783Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2023-02-16T13:46:06.1677657Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2023-02-16T13:46:06.1678489Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2023-02-16T13:46:06.1679238Z 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)
2023-02-16T13:46:06.1679917Z 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
2023-02-16T13:46:06.1681491Z 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2023-02-16T13:46:06.1682401Z 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2023-02-16T13:46:06.1684238Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2023-02-16T13:46:06.1684986Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2023-02-16T13:46:06.1686438Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2023-02-16T13:46:06.1687115Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2023-02-16T13:46:06.1687704Z Caused by: java.lang.RuntimeException: org.apache.hudi.exception.HoodieException
2023-02-16T13:46:06.1688684Z 	at org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer.lambda$deltaStreamerTestRunner$9(TestHoodieDeltaStreamer.java:893)
2023-02-16T13:46:06.1689478Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2023-02-16T13:46:06.1690064Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2023-02-16T13:46:06.1690677Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2023-02-16T13:46:06.1691351Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2023-02-16T13:46:06.1691893Z 	at java.lang.Thread.run(Thread.java:750)
2023-02-16T13:46:06.1692434Z Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException
2023-02-16T13:46:06.1694361Z 	at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.lambda$sync$1(HoodieDeltaStreamer.java:197)
2023-02-16T13:46:06.1695072Z 	at org.apache.hudi.common.util.Option.ifPresent(Option.java:97)
2023-02-16T13:46:06.1696111Z 	at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.sync(HoodieDeltaStreamer.java:192)
2023-02-16T13:46:06.1696972Z 	at org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer.lambda$deltaStreamerTestRunner$9(TestHoodieDeltaStreamer.java:890)
2023-02-16T13:46:06.1698335Z 	... 5 more
2023-02-16T13:46:06.1698799Z Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException
2023-02-16T13:46:06.1699436Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2023-02-16T13:46:06.1700053Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2023-02-16T13:46:06.1700715Z 	at org.apache.hudi.async.HoodieAsyncService.waitForShutdown(HoodieAsyncService.java:103)
2023-02-16T13:46:06.1701465Z 	at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.lambda$sync$1(HoodieDeltaStreamer.java:195)
2023-02-16T13:46:06.1701987Z 	... 8 more
2023-02-16T13:46:06.1702354Z Caused by: org.apache.hudi.exception.HoodieException
2023-02-16T13:46:06.1703051Z 	at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer$DeltaSyncService.lambda$startService$1(HoodieDeltaStreamer.java:758)
2023-02-16T13:46:06.1704867Z 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2023-02-16T13:46:06.1705362Z 	... 3 more
2023-02-16T13:46:06.1705711Z Caused by: java.lang.IllegalArgumentException
2023-02-16T13:46:06.1706251Z 	at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:31)
2023-02-16T13:46:06.1706995Z 	at org.apache.hudi.common.table.timeline.HoodieActiveTimeline.transitionState(HoodieActiveTimeline.java:633)
2023-02-16T13:46:06.1707847Z 	at org.apache.hudi.common.table.timeline.HoodieActiveTimeline.transitionRequestedToInflight(HoodieActiveTimeline.java:698)
2023-02-16T13:46:06.1708751Z 	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.saveWorkloadProfileMetadataToInflight(BaseCommitActionExecutor.java:147)
2023-02-16T13:46:06.1709792Z 	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.execute(BaseSparkCommitActionExecutor.java:172)
2023-02-16T13:46:06.1710733Z 	at org.apache.hudi.table.action.deltacommit.SparkUpsertPreppedDeltaCommitActionExecutor.execute(SparkUpsertPreppedDeltaCommitActionExecutor.java:44)
2023-02-16T13:46:06.1712815Z 	at org.apache.hudi.table.HoodieSparkMergeOnReadTable.upsertPrepped(HoodieSparkMergeOnReadTable.java:111)
2023-02-16T13:46:06.1713593Z 	at org.apache.hudi.table.HoodieSparkMergeOnReadTable.upsertPrepped(HoodieSparkMergeOnReadTable.java:80)
2023-02-16T13:46:06.1714353Z 	at org.apache.hudi.client.SparkRDDWriteClient.upsertPreppedRecords(SparkRDDWriteClient.java:154)
2023-02-16T13:46:06.1715155Z 	at org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter.commit(SparkHoodieBackedTableMetadataWriter.java:186)
2023-02-16T13:46:06.1716395Z 	at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.processAndCommit(HoodieBackedTableMetadataWriter.java:830)
2023-02-16T13:46:06.1718036Z 	at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.update(HoodieBackedTableMetadataWriter.java:897)
2023-02-16T13:46:06.1718885Z 	at org.apache.hudi.client.BaseHoodieWriteClient.lambda$writeTableMetadata$1(BaseHoodieWriteClient.java:355)
2023-02-16T13:46:06.1719843Z 	at org.apache.hudi.common.util.Option.ifPresent(Option.java:97)
2023-02-16T13:46:06.1720514Z 	at org.apache.hudi.client.BaseHoodieWriteClient.writeTableMetadata(BaseHoodieWriteClient.java:355)
2023-02-16T13:46:06.1721234Z 	at org.apache.hudi.client.BaseHoodieWriteClient.commit(BaseHoodieWriteClient.java:282)
2023-02-16T13:46:06.1722195Z 	at org.apache.hudi.client.BaseHoodieWriteClient.commitStats(BaseHoodieWriteClient.java:233)
2023-02-16T13:46:06.1722912Z 	at org.apache.hudi.client.SparkRDDWriteClient.commit(SparkRDDWriteClient.java:102)
2023-02-16T13:46:06.1723971Z 	at org.apache.hudi.client.SparkRDDWriteClient.commit(SparkRDDWriteClient.java:61)
2023-02-16T13:46:06.1724682Z 	at org.apache.hudi.client.BaseHoodieWriteClient.commit(BaseHoodieWriteClient.java:199)
2023-02-16T13:46:06.1725385Z 	at org.apache.hudi.utilities.deltastreamer.DeltaSync.writeToSink(DeltaSync.java:713)
2023-02-16T13:46:06.1726061Z 	at org.apache.hudi.utilities.deltastreamer.DeltaSync.syncOnce(DeltaSync.java:395)
2023-02-16T13:46:06.1726846Z 	at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer$DeltaSyncService.lambda$startService$1(HoodieDeltaStreamer.java:716)
2023-02-16T13:46:06.1727456Z 	... 4 more {code}"	HUDI	Closed	2	1	4726	pull-request-available
13473389	Improve metadata table based file listing for Presto Hive connector	This is improve the latency of improve metadata table based file listing when metadata table is enabled in Hive connector in Presto.	HUDI	Closed	1	4	4726	pull-request-available
13529097	Infer clean policy based on clean configs	"CLEANER_COMMITS_RETAINED: when either ""hoodie.cleaner.commits.retained"", ""hoodie.cleaner.hours.retained"", or ""hoodie.cleaner.fileversions.retained"" is set, should we automatically use the corresponding clean policy?"	HUDI	Closed	3	2	4726	pull-request-available
13545617	Add a downgrade step from 6 to 5 to detect new delete blocks	"In table version 6, we introduce a new delete block format (v3) with Avro serde (HUDI-5760).  For downgrading a table from v6 to v5, we need to perform compaction to handle v3 delete blocks created using the new format.
Also with the addition of record index field in Metadata table schema, the downgrade needs to delete the metadata table to avoid column drop errors after downgrade."	HUDI	Open	3	4	4726	pull-request-available
13575231	Read log block header only for the schema and instant time	"The TableSchemaResolver reads the schema from the log file header.  The current way of instantiating log reader does not lazily read the content, causing the whole block content to be read, which is unnecessary.  This causes the OOM on the Spark driver during clustering when clustering rewrites a file group that contains log files, which requires deriving the schema from the file group in the current logic.
{code:java}
  public static MessageType readSchemaFromLogFile(FileSystem fs, Path path) throws IOException {
    try (Reader reader = HoodieLogFormat.newReader(fs, new HoodieLogFile(path), null)) {
      HoodieDataBlock lastBlock = null;
      while (reader.hasNext()) {
        HoodieLogBlock block = reader.next();
        if (block instanceof HoodieDataBlock) {
          lastBlock = (HoodieDataBlock) block;
        }
      }
      return lastBlock != null ? new AvroSchemaConverter().convert(lastBlock.getSchema()) : null;
    }
  } {code}"	HUDI	Closed	1	1	4726	pull-request-available
13521552	Add since version for new configs added for 0.13.0	"Some new configs miss to add "".sinceVersion(""0.13.0"")"".  We need to add this for documentation."	HUDI	Closed	1	4	4726	pull-request-available
13422027	RFC-46: Optimize Record Payload handling	"h2. These are the gaps that we need to fill for the new record merging API

* [P0][HUDI-6702] Extend merge API to support all merging operations (inserts, updates and deletes, including customized getInsertValue)
 ** Option<Pair<HoodieRecord, Schema>> merge(Option<HoodieRecord> older, Schema oldSchema, Option<HoodieRecord> newer, Schema newSchema, TypedProperties props)
* [P0][HUDI-6765] Add merge mode to allow differentiation of dedup logic
 ** Add a new argument of merge mode (pre-combine, or update) to the merge API for customized dedup (or merging of log records?), instead of using OperationModeAwareness 
* [P0?][HUDI-6767] Simplify compatibility of HoodieRecord conversion
 ** HoodieRecordCompatibilityInterface provides adaption among any representation type (Avro, Row, etc.)
 ** Guarantee one type end-to-end: Avro, Row for Spark (RowData for Flink). For Avro log block, needs conversion from Avro to Row for Spark
* [P0][HUDI-6768] Revisit HoodieRecord design and how it affects e2e row writing
 ** HoodieRecord does not merely wrap engine-specific data structure; it also contains Java objects to store record key, location, etc.
 ** For end-to-end row writing, could we just use engine-specific type InternalRow instead of HoodieRecord<InternalRow> by appending key, location, etc. as row fields, to better leverage Spark's optimization on DataFrame with InternalRow? 
* [P0] Bug fixes
 ** HUDI-5807 HoodieSparkParquetReader is not appending partition-path values

h2. These are nice-to-haves but not on the critical path

* [P1] Make merge logic engine-agnostic
 ** Different engines need to implement the merging logic based in the engine-specific data structure (Spark's InternalRow, Flink's RowData, etc.) different HoodieRecordMerger implementation class. Providing getField API from the HoodieRecord could allow engine-agnostic merge logic.
* [P1][HUDI-5249][HUDI-5282] Implement MDT payload using new merge API
 ** Only necessary if we use parquet as the base and log file format in MDT
* [P1][HUDI-3354] Existing engine-specific readers to use HoodieRecord
 ** As we will implement a new file-group readers and writers, we do not need to fix existing readers now

— OLD PLAN —

Currently Hudi is biased t/w assumption of particular payload representation (Avro), long-term we would like to steer away from this to keep the record payload be completely opaque, so that
 # We can keep record payload representation engine-specific
 # Avoid unnecessary serde loops (Engine-specific > Avro > Engine-specific > Binary)

h2. *Proposal*

 
*Phase 2: Revisiting Record Handling*
{_}T-shirt{_}: 2-2.5 weeks
{_}Goal{_}: Avoid tight coupling with particular record representation on the Read Path (currently Avro) and enable
  * Revisit RecordPayload APIs
 * 
 ** Deprecate {{getInsertValue}} and {{combineAndGetUpdateValue}} APIs replacing w/ new “opaque” APIs (not returning Avro payloads)
 ** Rebase RecordPayload hierarchy to be engine-specific:
 *** Common engine-specific base abstracting common functionality (Spark, Flink, Java)
 *** Each feature-specific semantic will have to implement for all engines
 ** Introduce new APIs
 *** To access keys (record, partition)
 *** To convert record to Avro (for BWC)
 * Revisit RecordPayload handling
 ** In WriteHandles 
 *** API will be accepting opaque RecordPayload (no Avro conversion)
 *** Can do (opaque) record merging if necessary
 *** Passes RP as is to FileWriter
 ** In FileWriters
 *** Will accept RecordPayload interface
 *** Should be engine-specific (to handle internal record representation
 ** In RecordReaders
 *** API will be providing opaque RecordPayload (no Avro conversion)

 

 "	HUDI	In Progress	2	15	4726	hudi-umbrellas, pull-request-available
13511201	Fix flaky tests in TestCleanerInsertAndCleanByCommits	"In the tests, the {{KEEP_LATEST_COMMITS}} cleaner policy is used. This policy first figures out the earliest commit to retain based on the config of the number of retained commits ({{{}hoodie.cleaner.commits.retained{}}}). Then, for each file group, one more version before the earliest commit to retain is also kept from cleaning. The commit for the version can be different among file groups. 

However, the current validation logic only statically picks the one commit before the earliest commit to retain in the Hudi timeline for all file groups, which does not match the {{KEEP_LATEST_COMMITS}} cleaner policy."	HUDI	Closed	1	4	4726	pull-request-available
13515672	Optimize timeline loading in Hudi sync client	The Hudi archived timeline is always loaded during the metastore sync process if the last sync time is given. Besides, the archived timeline is not cached inside the meta client if the start instant time is given. These cause performance issues and read timeout on cloud storage due to rate limiting on requests because of loading archived timeline from the storage, when the archived timeline is huge, e.g., hundreds of log files in {{.hoodie/archived}} folder.	HUDI	Closed	1	4	4726	pull-request-available
13241363	Replace --key-generator-class CLI arg in HoodieDeltaStreamer with corresponding datasource property 	"Hudi DataSource use an option ""hoodie.datasource.write.keygenerator.class""  to generate KeyGenerator class whereas DeltaStreamer uses the CLI argument ""--key-generator-class"" to configure this. 

We need to remove the CLI argument and instead use datasource option"	HUDI	Closed	4	3	4726	new-to-hudi, pull-request-available
13539310	Add docs for Spark 3.4.0 support	Docs update for HUDI-6198	HUDI	Closed	3	4	4726	pull-request-available
13411947	Enable timeline server based marker type as default	Enable timeline server based marker type as default	HUDI	Closed	1	4	4726	pull-request-available
13425062	Unable to merge HoodieMetadataPayload: java.lang.IllegalArgumentException: Cannot combine 2 with 1	"When running the integration test with `mvn -Pintegration-tests verify`, the test failed due to retrieving list of partition from metadata table.

Stacktrace:
{code:java}
Caused by: org.apache.hudi.exception.HoodieException: Error fetching partition paths from metadata table
    at org.apache.hudi.common.fs.FSUtils.getAllPartitionPaths(FSUtils.java:299)
    at org.apache.hudi.HoodieTableFileIndexBase.getAllQueryPartitionPaths(HoodieTableFileIndexBase.scala:233)
    at org.apache.hudi.HoodieTableFileIndexBase.loadPartitionPathFiles(HoodieTableFileIndexBase.scala:195)
    at org.apache.hudi.HoodieTableFileIndexBase.refresh0(HoodieTableFileIndexBase.scala:108)
    at org.apache.hudi.HoodieTableFileIndexBase.<init>(HoodieTableFileIndexBase.scala:88)
    at org.apache.hudi.hadoop.HiveHoodieTableFileIndex.<init>(HiveHoodieTableFileIndex.java:52)
    at org.apache.hudi.hadoop.HoodieFileInputFormatBase.listStatusForSnapshotMode(HoodieFileInputFormatBase.java:170)
    at org.apache.hudi.hadoop.HoodieFileInputFormatBase.listStatus(HoodieFileInputFormatBase.java:141)
    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
    at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:442)
    at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:561)
    at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:330)
    at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:322)
    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:198)
    at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)
    at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1338)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1338)
    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:575)
    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:570)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:570)
    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:561)
    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:411)
    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:151)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
    at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
    ... 11 more
Caused by: org.apache.hudi.exception.HoodieMetadataException: Failed to retrieve list of partition from metadata
    at org.apache.hudi.metadata.BaseTableMetadata.getAllPartitionPaths(BaseTableMetadata.java:100)
    at org.apache.hudi.common.fs.FSUtils.getAllPartitionPaths(FSUtils.java:297)
    ... 47 more
Caused by: org.apache.hudi.exception.HoodieException: Exception when reading log file 
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scan(AbstractHoodieLogRecordReader.java:333)
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scan(AbstractHoodieLogRecordReader.java:179)
    at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.performScan(HoodieMergedLogRecordScanner.java:103)
    at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader.<init>(HoodieMetadataMergedLogRecordReader.java:71)
    at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader.<init>(HoodieMetadataMergedLogRecordReader.java:51)
    at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader$Builder.build(HoodieMetadataMergedLogRecordReader.java:246)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.getLogRecordScanner(HoodieBackedTableMetadata.java:346)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.lambda$openReadersIfNeeded$2(HoodieBackedTableMetadata.java:262)
    at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.openReadersIfNeeded(HoodieBackedTableMetadata.java:239)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordsByKeys(HoodieBackedTableMetadata.java:129)
    at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordByKey(HoodieBackedTableMetadata.java:124)
    at org.apache.hudi.metadata.BaseTableMetadata.fetchAllPartitionPaths(BaseTableMetadata.java:154)
    at org.apache.hudi.metadata.BaseTableMetadata.getAllPartitionPaths(BaseTableMetadata.java:98)
    ... 48 more
Caused by: java.lang.IllegalArgumentException: Cannot combine 2 with 1
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.metadata.HoodieMetadataPayload.preCombine(HoodieMetadataPayload.java:141)
    at org.apache.hudi.metadata.HoodieMetadataPayload.preCombine(HoodieMetadataPayload.java:63)
    at org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.processNextRecord(HoodieMergedLogRecordScanner.java:144)
    at org.apache.hudi.metadata.HoodieMetadataMergedLogRecordReader.processNextRecord(HoodieMetadataMergedLogRecordReader.java:78)
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processDataBlock(AbstractHoodieLogRecordReader.java:369)
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.processQueuedBlocksForInstant(AbstractHoodieLogRecordReader.java:431)
    at org.apache.hudi.common.table.log.AbstractHoodieLogRecordReader.scan(AbstractHoodieLogRecordReader.java:239)
    ... 61 more    at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:385)
    at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:254)
    at org.apache.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:476)
    at org.apache.hudi.cli.utils.HiveUtil.countRecords(HiveUtil.java:58)
    at org.apache.hudi.cli.commands.HoodieSyncCommand.validateSync(HoodieSyncCommand.java:69)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:216)
    ... 5 more{code}
[https://gist.github.com/yihua/2c7481fbbd509ca7a81e5d0ea7efc2fd]"	HUDI	Closed	1	1	4726	HUDI-bug
13320088	Re-implement marker files via timeline server	"Even as you can argue that RFC-15/consolidated metadata, removes the need for deleting partial files written due to spark task failures/stage retries. It will still leave extra files inside the table (and users will pay for it every month) and we need the marker mechanism to be able to delete these partial files. 

Here we explore if we can improve the current marker file mechanism, that creates one marker file per data file written, by 

Delegating the createMarker() call to the driver/timeline server, and have it create marker metadata into a single file handle, that is flushed for durability guarantees

 

P.S: I was tempted to think Spark listener mechanism can help us deal with failed tasks, but it has no guarantees. the writer job could die without deleting a partial file. i.e it can improve things, but cant provide guarantees "	HUDI	Resolved	1	4	4726	pull-request-available
13221944	CSV Source support for Hudi Delta Streamer	DeltaStreamer does not have support to pull CSV data from sources (hdfs log files/kafka). THis ticket is to provide support for csv sources.	HUDI	Resolved	3	4	4726	pull-request-available
13328894	[Umbrella] RFC-15 : Metadata Table for File Listing and other table metadata	This is the umbrella ticket that tracks the overall implementation of RFC-15	HUDI	Closed	1	15	4726	hudi-umbrellas, pull-request-available
13580491	Fix bundle validation scripts	"Issues:
 * Bundle validation with packaging/bundle-validation/ci_run.sh fails for release-0.15.0 branch due to script issue
 * scripts/release/validate_staged_bundles.sh needs to include additional bundles.
 * Add release candidate validation on scala 2.13 bundles.
 * Disable release candidate validation by default."	HUDI	Closed	3	1	4726	pull-request-available
13547056	Fix partition validation to only consider commits in metadata table validator	"The completed rollback in data table's timeline interferes with the partition validation in the metadata table validator.  Only commits should be considered in the validation.  See the following example.
Timeline of DT and MDT:

{code:java}
╔═════╤═══════════════════╤═════════════╤═══════════╤═══════════════════╤═════════════╤═════════════╤═════════════╤═════════════╤═══════════╤═════════════╤═════════════╤═════════════╗
║ No. │ Instant           │ Action      │ State     │ Rollback Info     │ Requested   │ Inflight    │ Completed   │ MT          │ MT        │ MT          │ MT          │ MT          ║
║     │                   │             │           │                   │ Time        │ Time        │ Time        │ Action      │ State     │ Requested   │ Inflight    │ Completed   ║
║     │                   │             │           │                   │             │             │             │             │           │ Time        │ Time        │ Time        ║
╠═════╪═══════════════════╪═════════════╪═══════════╪═══════════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═══════════╪═════════════╪═════════════╪═════════════╣
║ 0   │ 00000000000000010 │ -           │ -         │ -                 │ -           │ -           │ -           │ deltacommit │ COMPLETED │ 08-11 23:59 │ 08-11 23:59 │ 08-11 23:59 ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 1   │ 00000000000000011 │ -           │ -         │ -                 │ -           │ -           │ -           │ deltacommit │ COMPLETED │ 08-11 23:59 │ 08-11 23:59 │ 08-11 23:59 ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 2   │ 20230812065907463 │ deltacommit │ INFLIGHT  │ Rolled back by    │ 08-11 23:59 │ 08-12 00:00 │ -           │ -           │ -         │ -           │ -           │ -           ║
║     │                   │             │           │ 20230812070238150 │             │             │             │             │           │             │             │             ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 3   │ 20230812070238150 │ rollback    │ INFLIGHT  │ Rolls back        │ 08-12 00:02 │ 08-12 00:02 │ -           │ -           │ -         │ -           │ -           │ -           ║
║     │                   │             │           │ 20230812065907463 │             │             │             │             │           │             │             │             ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 4   │ 20230812070241429 │ -           │ -         │ -                 │ -           │ -           │ -           │ rollback    │ COMPLETED │ 08-12 00:02 │ 08-12 00:02 │ 08-12 00:02 ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 5   │ 20230812070351902 │ deltacommit │ REQUESTED │ -                 │ 08-12 00:04 │ -           │ -           │ -           │ -         │ -           │ -           │ -           ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 6   │ 20230812070532879 │ deltacommit │ REQUESTED │ -                 │ 08-12 00:06 │ -           │ -           │ -           │ -         │ -           │ -           │ -           ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 7   │ 20230812070605364 │ rollback    │ COMPLETED │ Rolls back        │ 08-12 00:06 │ 08-12 00:06 │ 08-12 00:06 │ deltacommit │ COMPLETED │ 08-12 00:06 │ 08-12 00:06 │ 08-12 00:06 ║
║     │                   │             │           │ 20230812070205857 │             │             │             │             │           │             │             │             ║
╟─────┼───────────────────┼─────────────┼───────────┼───────────────────┼─────────────┼─────────────┼─────────────┼─────────────┼───────────┼─────────────┼─────────────┼─────────────╢
║ 8   │ 20230812070606670 │ -           │ -         │ -                 │ -           │ -           │ -           │ rollback    │ COMPLETED │ 08-12 00:06 │ 08-12 00:06 │ 08-12 00:06 ║
╚═════╧═══════════════════╧═════════════╧═══════════╧═══════════════════╧═════════════╧═════════════╧═════════════╧═════════════╧═══════════╧═════════════╧═════════════╧═════════════╝
{code}

The partition metadata:

{code:java}
2023/06/24/.hoodie_partition_metadata
#partition metadata
#Sat Aug 12 07:00:21 UTC 2023
commitTime=20230812065907463
partitionDepth=3
{code}

Validator throws the exception:


{code:java}
org.apache.hudi.exception.HoodieValidationException: Compare Partitions Failed! AllPartitionPathsFromFS : [2023/06/24, 2023/06/25, 2023/06/26, 2023/06/27, 2023/06/28, 2023/06/29, 2023/06/30, 2023/07/01, 2023/07/02, 2023/07/03] and allPartitionPathsMeta : []
	at org.apache.hudi.utilities.HoodieMetadataTableValidator.validatePartitions(HoodieMetadataTableValidator.java:558)
	at org.apache.hudi.utilities.HoodieMetadataTableValidator.doMetadataTableValidation(HoodieMetadataTableValidator.java:435)
	at org.apache.hudi.utilities.HoodieMetadataTableValidator.doHoodieMetadataTableValidationOnce(HoodieMetadataTableValidator.java:377)
	at org.apache.hudi.utilities.HoodieMetadataTableValidator.run(HoodieMetadataTableValidator.java:362)
	at org.apache.hudi.utilities.HoodieMetadataTableValidator.main(HoodieMetadataTableValidator.java:342)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

{code}
"	HUDI	Open	1	1	4726	pull-request-available
13537841	Fix maven build	"Maven build is broken on master.
{code:java}
mvn clean package -DskipTests{code}"	HUDI	Closed	1	1	4726	pull-request-available
13510673	Cache file slices within MDT reader	As of now, we only cache the log file reader. We should also cache the latest file slices at partition level since we keep calling getLatestFileSlices often.  This causes a new file system view to be instantiated each time and thus lists the {{files}} partition. The FS listing call can be avoided by caching the file system view inside {{HoodieBackedTableMetadata}} .	HUDI	Closed	1	4	4726	pull-request-available
13542850	Java 11 compile time support	Certify Hudi with Java 11 runtime support	HUDI	Closed	3	1	4726	pull-request-available
13558469	Improve CI scripts	Remove unnecessary bundle validation tasks and rebalance Azure CI tasks.	HUDI	Closed	3	4	4726	pull-request-available
13579726	 Allow HoodieTableMetaClient to take HoodieStorage instance directly	We need to functionality for the meta client to 	HUDI	Closed	3	4	4726	hoodie-storage, pull-request-available
13420457	add support for local dynamo db lock provider	"Ref issue:

[https://github.com/apache/hudi/issues/4499]

 "	HUDI	Closed	3	3	9267	pull-request-available, user-support-issues
13393379	Bring down the total test run time with CI	Bring down the total test run time with CI. As of now, utilities, spark-client and rest are taking > 50 mins. 	HUDI	Resolved	3	4	9267	pull-request-available
13519949	Files written by first commit/delta commit if it failed is detected as valid data files	"We have an method in HoodieFileGroup which detects whether a file group is committed or not. If timeline is such that, 

c1.inflight

c2.complete

c3.complete

 

when we check for c1, it will return true. 

HoodieFileGroup.java
{code:java}
/**
 * A FileSlice is considered committed, if one of the following is true - There is a committed data file - There are
 * some log files, that are based off a commit or delta commit.
 */
private boolean isFileSliceCommitted(FileSlice slice) {
  if (!compareTimestamps(slice.getBaseInstantTime(), LESSER_THAN_OR_EQUALS, lastInstant.get().getTimestamp())) {
    return false;
  }

  return timeline.containsOrBeforeTimelineStarts(slice.getBaseInstantTime());
} {code}
HoodieDefaultTimeline : 
{code:java}
@Override
public boolean containsOrBeforeTimelineStarts(String instant) {
  return getInstantsAsStream().anyMatch(s -> s.getTimestamp().equals(instant)) || isBeforeTimelineStarts(instant);
} {code}
 

This needs to be fixed. "	HUDI	Closed	2	1	9267	pull-request-available
13519998	Support auto record key generation with Spark SQL	"HUDI-2681 adds support for auto record key generation with spark dataframes. This Jira aims to add support for the same with spark sql.

One of the changes required here as pointed out by [~kazdy] is that SQL_INSERT_MODE would need to be handled here. In this case if SQL_INSERT_MODE mode is set to strict, the insert should fail.

cc [~shivnarayan] 

Essentially, based on this patch ([https://github.com/apache/hudi/pull/7681),|https://github.com/apache/hudi/pull/7681,]
we want to ensure spark-sql writes also supports auto generation of record keys. "	HUDI	Closed	2	1	9267	release-0.14.0-blocker
13398387	Fix spark quick start guide for minor issues	Fix spark quick start guide for minor issues	HUDI	Resolved	3	4	9267	pull-request-available
13573875	Fix MDT validator to account for additional partitions in MDT	"There is a chance that MDT could list additional partitions when compared to FS based listing. 

reason is: 

We load active timeline from metaclient and poll FS based listing for completed commits. And then we poll MDT for list of all partitions. in between these two, there could be a commit that could have been completed and hence MDT could be serving that as well. So, lets account for that in our validation tool "	HUDI	Closed	3	1	9267	pull-request-available
13418379	savepoint rollback leaves trail of rollback meta files 	"savepoint rollback triggers a restore operation. But once successfully completed, I see there are rollback.requested and rollback.inflight meta files in timeline. 

 
{code:java}
-rw-r--r--  1 nsb  wheel     0 Dec 17 19:57 20211217195708258.savepoint.inflight
-rw-r--r--  1 nsb  wheel  1168 Dec 17 19:57 20211217195708258.savepoint
-rw-r--r--  1 nsb  wheel     0 Dec 17 20:00 20211217200028051.restore.inflight
-rw-r--r--  1 nsb  wheel  1703 Dec 17 20:00 20211217200028099.rollback.requested
-rw-r--r--  1 nsb  wheel  1703 Dec 17 20:00 20211217200028099.rollback.inflight
-rw-r--r--  1 nsb  wheel  2770 Dec 17 20:00 20211217200028051.restore {code}"	HUDI	Closed	3	1	9267	sev:critical
13489160	Non serializable path used with engineContext with metadata table initialization	"issue reported by use in glue env. we could not reproduce in EMR w/ S3 by ourselves. 

 
{code:java}
py4j.protocol.Py4JJavaError: An error occurred while calling o1011.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Failed to serialize task 3847, not attempting to retry it. Exception during serialization: java.io.NotSerializableException: org.apache.hadoop.fs.Path
Serialization stack:
    - object not serializable (class: org.apache.hadoop.fs.Path, value: s3://somebucket/someprefix)
    - element of array (index: 0)
    - array (class [Ljava.lang.Object;, size 1)
    - field (class: scala.collection.mutable.WrappedArray$ofRef, name: array, type: class [Ljava.lang.Object;)
    - object (class scala.collection.mutable.WrappedArray$ofRef, WrappedArray(s3://somebucket/someprefix))
    - writeObject data (class: org.apache.spark.rdd.ParallelCollectionPartition)
    - object (class org.apache.spark.rdd.ParallelCollectionPartition, org.apache.spark.rdd.ParallelCollectionPartition@3488)
    - field (class: org.apache.spark.scheduler.ResultTask, name: partition, type: interface org.apache.spark.Partition)
    - object (class org.apache.spark.scheduler.ResultTask, ResultTask(114, 0))
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
    at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)
    at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
    at org.apache.hudi.client.common.HoodieSparkEngineContext.map(HoodieSparkEngineContext.java:103)
    at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.listAllPartitions(HoodieBackedTableMetadataWriter.java:631)
    at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.initialCommit(HoodieBackedTableMetadataWriter.java:1064)
    at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.initializeFromFilesystem(HoodieBackedTableMetadataWriter.java:557)
    at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.initializeIfNeeded(HoodieBackedTableMetadataWriter.java:390)
    at org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter.initialize(SparkHoodieBackedTableMetadataWriter.java:120)
    at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.<init>(HoodieBackedTableMetadataWriter.java:171)
    at org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter.<init>(SparkHoodieBackedTableMetadataWriter.java:89)
    at org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter.create(SparkHoodieBackedTableMetadataWriter.java:75)
    at org.apache.hudi.client.SparkRDDWriteClient.initializeMetadataTable(SparkRDDWriteClient.java:446)
    at org.apache.hudi.client.SparkRDDWriteClient.doInitTable(SparkRDDWriteClient.java:431)
    at org.apache.hudi.client.BaseHoodieWriteClient.initTable(BaseHoodieWriteClient.java:1459)
    at org.apache.hudi.client.BaseHoodieWriteClient.initTable(BaseHoodieWriteClient.java:1491)
    at org.apache.hudi.client.SparkRDDWriteClient.upsert(SparkRDDWriteClient.java:152)
    at org.apache.hudi.DataSourceUtils.doWriteOperation(DataSourceUtils.java:206)
    at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:329)
    at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:183)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
    at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
    at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:750) {code}
 "	HUDI	Closed	3	1	9267	pull-request-available
13405638	Late arriving records and global index with partition path update set to true	"incase of a global index, we have a config to update partition path. if this is set to true, if there is an incoming record to a newer partition compared to whats in storage, older record will be deleted and new incoming record will be routed to new partition. 

But it could run into issues if new incoming is a late arriving record. Expected behavior is, old record is retained and new one is discarded it it has lower preCombine value. But in this case, we may not honor that. 

 "	HUDI	Open	3	4	9267	sev:normal, user-support-issues
13402509	Fix refreshing timeline for every operation	with metadata enabled, we need to refresh timeline for the table before very operation. if not, some states might be missed out. Atleast for deltastreamer continuous mode, we need this to be fixed. also some tests are failing due to this. 	HUDI	Resolved	3	3	9267	pull-request-available
13449349	GetAllPartitions w/o metadata has regressed in perf from 0.9.0 to 0.11.0	"Looks like getAllPartitonPaths have regressed in performance from 0.9.0 to 0.11.0. 

Ref issue: https://github.com/apache/hudi/issues/5776"	HUDI	Closed	1	4	9267	pull-request-available
13411424	To avoid the duplicates for streaming read MOR table	"Imagine there are commits on the timeline:

{noformat}
                         -----delta-99 ----- commit 100(include 99 delta data set) ----- delta-101 ----- delta-102 -----
                          first read ->| second read ->
                         – range 1 ---| ----------------------range 2 -------------------|

{noformat}

instant 99, 101, 102 are successful non-compaction delta commits;
instant 100 is successful compaction instant.

The first inc read consumes to instant 99 and the second read consumes from instant 100 to instant 102, the second read would consumes the commit files of instant 100 which has already been consumed before.

The duplicate reading happens when this condition triggers: a compaction instant schedules then completes in *one* consume range."	HUDI	Closed	2	4	9267	release-0.14.0-blocker
13483551	Cleaner cleans up files touched by clustering	"I have some integration long running tests w/ cleaner and clustering. from 21st or 22nd of sep, my tests have started to fail.

 

Reason is, when clustering kicks in, it could not find the data files to be clustered. Looks like cleaner has cleaned it up. 

 

 "	HUDI	Closed	1	1	9267	pull-request-available
13425761	Ignore non existant temp marker dir for a commit while downgrading from 2 to 1 table version	"While downgrading from 2 to 1, if for a commit, there is no marker directory, an exception is thrown. 

We should silently ignore it. 

 

Code of interest in TwoToOneDowngradeHandler
{code:java}
private void convertToDirectMarkers(final String commitInstantTime,
                                    HoodieTable table,
                                    HoodieEngineContext context,
                                    int parallelism) throws IOException {
  String markerDir = table.getMetaClient().getMarkerFolderPath(commitInstantTime);
  FileSystem fileSystem = FSUtils.getFs(markerDir, context.getHadoopConf().newCopy());
  Option<MarkerType> markerTypeOption = MarkerUtils.readMarkerType(fileSystem, markerDir);
  if (markerTypeOption.isPresent()) {
    switch (markerTypeOption.get()) {
      case TIMELINE_SERVER_BASED:
        // Reads all markers written by the timeline server
        Map<String, Set<String>> markersMap =
            MarkerUtils.readTimelineServerBasedMarkersFromFileSystem(
                markerDir, fileSystem, context, parallelism);
        DirectWriteMarkers directWriteMarkers = new DirectWriteMarkers(table, commitInstantTime);
        // Recreates the markers in the direct format
        markersMap.values().stream().flatMap(Collection::stream)
            .forEach(directWriteMarkers::create);
        // Deletes marker type file
        MarkerUtils.deleteMarkerTypeFile(fileSystem, markerDir);
        // Deletes timeline server based markers
        deleteTimelineBasedMarkerFiles(context, markerDir, fileSystem, parallelism);
        break;
      default:
        throw new HoodieException(""The marker type \"""" + markerTypeOption.get().name()
            + ""\"" is not supported for rollback."");
    }
  } else {
    // In case of partial failures during downgrade, there is a chance that marker type file was deleted,
    // but timeline server based marker files are left.  So deletes them if any
    deleteTimelineBasedMarkerFiles(context, markerDir, fileSystem, parallelism);
  }
} {code}
else block in above.

 
{code:java}
private void deleteTimelineBasedMarkerFiles(HoodieEngineContext context, String markerDir,
                                            FileSystem fileSystem, int parallelism) throws IOException {
  // Deletes timeline based marker files if any.
  Predicate<FileStatus> prefixFilter = fileStatus ->
      fileStatus.getPath().getName().startsWith(MARKERS_FILENAME_PREFIX);
  FSUtils.parallelizeSubPathProcess(context, fileSystem, new Path(markerDir), parallelism,
          prefixFilter, pairOfSubPathAndConf ->
                  FSUtils.deleteSubPath(pairOfSubPathAndConf.getKey(), pairOfSubPathAndConf.getValue(), false));
} {code}
fix: in the else block in first snippet, we should call deleteTimelineBasedMarkerFiles only if marker dir for the commit exists. 

 "	HUDI	Closed	3	3	9267	pull-request-available
13403590	Fix usage of different key generators with metadata enabled	"With [sync metadata patch|https://github.com/apache/hudi/pull/3590/], when metadata is enabled by default, some spark datasource tests failed which were using timestamp based key gen and custom key gen. Metadata table's records are getting picked up when we do 

 
{code:java}
spark.read.format(hudi).load(basePath + ""/*/*"")
{code}
 

For now, I have disabled metadata for these tests. 

testSparkPartitonByWithTimestampBasedKeyGenerator

testSparkPartitonByWithCustomKeyGenerator

 

I was looking at [options|https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html] to ignore certain path, but looks like there is none. 

 

 "	HUDI	Resolved	3	3	9267	oct18_2021, pull-request-available, sev:critical
13489167	Optimize rdd.isEmpty within DeltaSync	We are calling rdd.isEmpty for source rdd twice in DeltaSync. we should try and optimize/reuse. 	HUDI	Patch Available	2	4	9267	pull-request-available
13269361	Make cleaner retention based on time period to account for higher deviations in ingestion runs	Cleaner by commits is based on number of commits to be retained.  Ingestion time could vary across runs due to various factors. For providing a bound on the maximum running time for a query and for providing consistent retention period, it is better to use a retention config based on time (e:g 12h) 	HUDI	Closed	3	3	9267	core-flow-ds, new-to-hudi, pull-request-available, sev:high
13478712	Fix thread safety w/ RemoteTableFileSystemView 	"After retry mechanism was added to RemoteTableFileSystemView, looks like the code is not thread safe. 

 

[https://github.com/apache/hudi/pull/5884/files#diff-0d301525ef388eb460372ea300c827728c954fdda799adfce7040158ec8b1d84R183|https://github.com/apache/hudi/pull/5884/files#r955363946]

 

This might impact regular flows as well even if no retries are enabled. 

 

 

 "	HUDI	Closed	3	6	9267	pull-request-available
13273237	Updates sent to diff partition for a given key with Global Index 	"Updates sent to diff partition for a given key with Global Index should succeed by updating the record under original partition. As of now, it throws exception. 

[https://github.com/apache/incubator-hudi/issues/1021] 

 

 

error log:
{code:java}


 14738 [Executor task launch worker-0] INFO com.uber.hoodie.common.table.timeline.HoodieActiveTimeline - Loaded instants java.util.stream.ReferencePipeline$Head@d02b1c7
 14738 [Executor task launch worker-0] INFO com.uber.hoodie.common.table.view.AbstractTableFileSystemView - Building file system view for partition (2016/04/15)
 14738 [Executor task launch worker-0] INFO com.uber.hoodie.common.table.view.AbstractTableFileSystemView - #files found in partition (2016/04/15) =0, Time taken =0
 14738 [Executor task launch worker-0] INFO com.uber.hoodie.common.table.view.AbstractTableFileSystemView - addFilesToView: NumFiles=0, FileGroupsCreationTime=0, StoreTimeTaken=0
 14738 [Executor task launch worker-0] INFO com.uber.hoodie.common.table.view.HoodieTableFileSystemView - Adding file-groups for partition :2016/04/15, #FileGroups=0
 14738 [Executor task launch worker-0] INFO com.uber.hoodie.common.table.view.AbstractTableFileSystemView - Time to load partition (2016/04/15) =0
 14754 [Executor task launch worker-0] ERROR com.uber.hoodie.table.HoodieCopyOnWriteTable - Error upserting bucketType UPDATE for partition :0
 java.util.NoSuchElementException: No value present
 at com.uber.hoodie.common.util.Option.get(Option.java:112)
 at com.uber.hoodie.io.HoodieMergeHandle.(HoodieMergeHandle.java:71)
 at com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:226)
 at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:180)
 at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:263)
 at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:442)
 at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
 at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
 at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
 at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
 at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
 at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
 at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
 at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
 at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
 at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
 at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
 at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
 at org.apache.spark.scheduler.Task.run(Task.scala:99)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)

 {code}

Refer [here|[https://github.com/apache/incubator-hudi/issues/1021]] for some context. 

 

 

 "	HUDI	Closed	3	1	9267	pull-request-available
13489158	Add support to rollback residual clustering after disabling clustering	"if a user enabled clustering and after sometime disabled it due to whatever reason, there is a chance that there is a pending clustering left in the timeline. But once clustering is disabled, this could just be lying around. but this could affect metadata table compaction whcih in turn might affect the data table archival. 

so, we need a way to fix this. 

 "	HUDI	Patch Available	1	1	9267	pull-request-available
13392455	[SQL] Changing index type fails	"I tried to set a different index type and it failed. 

 

```

set hoodie.index.type = SIMPLE

 

spark-sql> create table hudi_17Gb_ext1 using hudi location 's3a://siva-test-bucket-june-16/hudi_testing/gh_arch_dump/hudi_5/' options ( 

         >   type = 'cow', 

         >   primaryKey = 'randomId', 

         >   preCombineField = 'date_col' 

         >  ) 

         > partitioned by (type) as select * from gh_17Gb_date_col;

21/07/29 04:24:23 ERROR SparkSQLDriver: Failed in [create table hudi_17Gb_ext1 using hudi location 's3a://siva-test-bucket-june-16/hudi_testing/gh_arch_dump/hudi_5/' options ( 

  type = 'cow', 

  primaryKey = 'randomId', 

  preCombineField = 'date_col' 

 ) 

partitioned by (type) as select * from gh_17Gb_date_col]

java.lang.IllegalArgumentException: No enum constant org.apache.hudi.index.HoodieIndex.IndexType.SIMPLE

 

 

describe hudi_17Gb_ext

 at java.lang.Enum.valueOf(Enum.java:238)

 at org.apache.hudi.index.HoodieIndex$IndexType.valueOf(HoodieIndex.java:106)

 at org.apache.hudi.config.HoodieIndexConfig$Builder.build(HoodieIndexConfig.java:333)

 at org.apache.hudi.config.HoodieWriteConfig$Builder.setDefaults(HoodieWriteConfig.java:1608)

 at org.apache.hudi.config.HoodieWriteConfig$Builder.build(HoodieWriteConfig.java:1650)

 at org.apache.hudi.DataSourceUtils.createHoodieConfig(DataSourceUtils.java:196)

 at org.apache.hudi.DataSourceUtils.createHoodieClient(DataSourceUtils.java:201)

 at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$write$5(HoodieSparkSqlWriter.scala:183)

 at scala.Option.getOrElse(Option.scala:189)

 at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:182)

 at org.apache.spark.sql.hudi.command.InsertIntoHoodieTableCommand$.run(InsertIntoHoodieTableCommand.scala:97)

 at org.apache.spark.sql.hudi.command.CreateHoodieTableAsSelectCommand.run(CreateHoodieTableAsSelectCommand.scala:86)

 at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)

 at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)

 at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)

 at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)

 at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)

 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)

 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)

 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)

 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)

 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)

 at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)

 at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)

 at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)

 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)

 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)

 at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)

 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)

 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)

 at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)

 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)

 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:377)

 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:496)

```

 "	HUDI	Resolved	1	3	9267	release-blocker
13526593	Update release notes regarding the HoodieMetadataFileSystemView regression	Relevant bug and fix: HUDI-5863	HUDI	Closed	1	1	9267	pull-request-available
13333874	introduce a builder pattern init the table properties	"introduce a builder pattern init the table properties. those overloaded {{initXX}} are hard to read.

such as

/**
 * Used primarily by tests, examples.
 */
public static HoodieTableMetaClient initTableType(Configuration hadoopConf, String basePath, HoodieTableType tableType,
 String tableName, String payloadClassName, String indexType) throws IOException {
 return initTableType(hadoopConf, basePath, tableType, tableName, null, payloadClassName,
 null, null, null, null, indexType);
}"	HUDI	Closed	3	4	9267	pull-request-available
13408289	Fix test failures in TestHoodieLogFormat	"CI is broken. [https://dev.azure.com/apache-hudi-ci-org/web/build.aspx?pcguid=7db69d7d-9e2b-477d-bd89-311206ac8d2d&builduri=vstfs%3a%2f%2f%2fBuild%2fBuild%2f2832]

 

 "	HUDI	Resolved	1	4	9267	pull-request-available
13327324	Update Apache Hudi website docs to clarify the property of record_keys	"Context: [https://github.com/apache/hudi/issues/1955]

 

The following section needs update  in [https://hudi.apache.org/docs/writing_data.html]

""RECORDKEY_FIELD_OPT_KEY (Required): *Primary key* field(s). Nested fields can be specified using the dot notation eg: a.b.c. When using multiple columns as primary key use comma separated notation, eg: ""col1,col2,col3,etc"". Single or multiple columns as primary key specified by KEYGENERATOR_CLASS_OPT_KEY property.
Default value: ""uuid""

 "	HUDI	Closed	3	4	9267	user-support-issues
13514290	Rollbacks in MDT is not effective	"On rare conditions, rollbacks in MDT is not effective. Apparenlty, we have set cleaning policy to be lazy. hence rollbacks happens only when cleaner kicks in and not when we start a new commit. Given MDT is a single writer table, rollback blocks are effective only when the commit to rollback is just prior to the rollback block. 

 

Scenarios where this could fail w/ inline compaction. 

 
{code:java}
Data table timeline
t1.dc   t2.comp.req.     |Crash  t3.dc     t2.comp.inflight    t2.commit

MDT timeline
t1.dc.  t2.comp.inflight |Crash  t3.dc  t4.rb(t2)           t2.dc

{code}
 

The first attempt of t2 in MDT should be rolled back since it crashed mid-way. in other words, if there are any log blocks written by t2 in MDT, it should be deemed invalid. 

 

But what happens is, here is how the log blocks are laid out. 

log1(t1).  log2(t2 first attempt) crash.... log3 (t3) log4(t4.rb rolling back t2) ... log5 (t2)

 

So, when we read the log blocks via AbstractLogRecordReader, ideally we want to ignore log2. but when we encounter log4 for a rollback block, we only check the previous log block for matching commit to rollback. since it does not match w/ t2, we assume log4 is a duplicate rollback and hence still deem log2 as a valid log block. 

hence MDT could serve more data files which are not valid from a FS based listing standpoint. 

 

Impact:

log blocks to be ignored are considered valid if not for this fix. 

 

 

 "	HUDI	Closed	1	1	9267	pull-request-available
13489022	Fix Flaky TestCleaner test : testInsertAndCleanByCommits	"We are seeing CI flakiness with 

testInsertAndCleanByCommits test in TestCleaner. 

 

[https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/12431/logs/43]

 

[https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/12430/logs/33]

 

 "	HUDI	Closed	1	1	9267	pull-request-available
13425982	Investigate and fix hive query validation in integ test suite	hive query validations are not working w/ integ test suite framework. Investigate and fix the same. 	HUDI	Closed	2	3	9267	user-support-issues
13550555	[DOCS] Add info about partially failed writes handling w/ hudi	"We dont' have a page that discussed how partially failed writes are handled in hudi. 

would be good to have one"	HUDI	Closed	3	4	9267	pull-request-available
13555928	Add support for non-partitioned dataset w/ RLI	"We need to support RLI w/ non-partitioned datasets as well. 

both initialization of RLI for an existing table and for new tables"	HUDI	Closed	3	4	9267	pull-request-available
13483927	Reading from metadata table could fail when there are no completed commits	"When metadata table is just getting initialized, but first commit is not yet fully complete, reading from metadata table could fail w/ below stacktrace. 

 
{code:java}
22/08/20 02:56:58 ERROR client.RemoteDriver: Failed to run client job 39d720db-b15d-4823-b8b1-54398b143d6e
org.apache.hudi.exception.HoodieException: Error fetching partition paths from metadata table
at org.apache.hudi.common.fs.FSUtils.getAllPartitionPaths(FSUtils.java:315)
at org.apache.hudi.BaseHoodieTableFileIndex.getAllQueryPartitionPaths(BaseHoodieTableFileIndex.java:176)
at org.apache.hudi.BaseHoodieTableFileIndex.loadPartitionPathFiles(BaseHoodieTableFileIndex.java:219)
at org.apache.hudi.BaseHoodieTableFileIndex.doRefresh(BaseHoodieTableFileIndex.java:264)
at org.apache.hudi.BaseHoodieTableFileIndex.(BaseHoodieTableFileIndex.java:139)
at org.apache.hudi.hadoop.HiveHoodieTableFileIndex.(HiveHoodieTableFileIndex.java:49)
at org.apache.hudi.hadoop.HoodieCopyOnWriteTableInputFormat.listStatusForSnapshotMode(HoodieCopyOnWriteTableInputFormat.java:234)
at org.apache.hudi.hadoop.HoodieCopyOnWriteTableInputFormat.listStatus(HoodieCopyOnWriteTableInputFormat.java:141)
at org.apache.hudi.hadoop.HoodieParquetInputFormatBase.listStatus(HoodieParquetInputFormatBase.java:90)
at org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat$HoodieCombineFileInputFormatShim.listStatus(HoodieCombineHiveInputFormat.java:889)
at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:217)
at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:76)
at org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat$HoodieCombineFileInputFormatShim.getSplits(HoodieCombineHiveInputFormat.java:942)
at org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat.getCombineSplits(HoodieCombineHiveInputFormat.java:241)
at org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat.getSplits(HoodieCombineHiveInputFormat.java:363)
at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
at org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:267)
at org.apache.spark.api.java.JavaRDDLike$class.getNumPartitions(JavaRDDLike.scala:65)
at org.apache.spark.api.java.AbstractJavaRDDLike.getNumPartitions(JavaRDDLike.scala:45)
at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateMapInput(SparkPlanGenerator.java:252)
at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateParentTran(SparkPlanGenerator.java:179)
at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:130)
at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient$JobStatusJob.call(RemoteHiveSparkClient.java:355)
at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:400)
at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:365)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hudi.exception.HoodieMetadataException: Failed to retrieve list of partition from metadata
at org.apache.hudi.metadata.BaseTableMetadata.getAllPartitionPaths(BaseTableMetadata.java:113)
at org.apache.hudi.common.fs.FSUtils.getAllPartitionPaths(FSUtils.java:313)
... 32 more
Caused by: java.util.NoSuchElementException: No value present in Option
at org.apache.hudi.common.util.Option.get(Option.java:89)
at org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartitionFileSlices(HoodieTableMetadataUtil.java:1057)
at org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartitionLatestMergedFileSlices(HoodieTableMetadataUtil.java:1001)
at org.apache.hudi.metadata.HoodieBackedTableMetadata.getPartitionFileSliceToKeysMapping(HoodieBackedTableMetadata.java:377)
at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordsByKeys(HoodieBackedTableMetadata.java:204)
at org.apache.hudi.metadata.HoodieBackedTableMetadata.getRecordByKey(HoodieBackedTableMetadata.java:140)
at org.apache.hudi.metadata.BaseTableMetadata.fetchAllPartitionPaths(BaseTableMetadata.java:281)
at org.apache.hudi.metadata.BaseTableMetadata.getAllPartitionPaths(BaseTableMetadata.java:111)
... 33 more
22/08/20 02:56:59 INFO client.RemoteDriver: Shutting down Spark Remote Driver.
22/08/20 02:56:59 INFO server.AbstractConnector: Stopped Spark@ce7a81b{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
22/08/20 02:56:59 INFO ui.SparkUI: Stopped Spark web UI at http://scsp04097:34219
22/08/20 02:56:59 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
22/08/20 02:56:59 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
22/08/20 02:56:59 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
22/08/20 02:56:59 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
services=List(),
started=false)
22/08/20 02:56:59 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/08/20 02:56:59 INFO memory.MemoryStore: MemoryStore cleared {code}"	HUDI	Closed	1	1	9267	pull-request-available
13411818	IllegalArgException from timeline server when serving getLastestBaseFiles with multi-writer	"When concurrent writes try to ingest to hudi, occasionally, we run into IllegalArgumentException as below. Even though exception is seen, the actual write succeeds though. 

Here is what is happening from my understanding. 

 

Lets say table's latest commit is C3. 

Writer1 tries to commit C4, writer2 tries to do C5 and writer3 tries to do C6 (all 3 are non-overlapping and so expected to succeed) 

I started C4 from writer1 and then switched to writer 2 and triggered C5 and then did the same for writer3. 

C4 went through fine for writer1 and succeeded. 

for writer2, when timeline got instantiated, it's latest snapshot was C3, but when it received the getLatestBaseFiles() request, latest commit was C4 and so it throws an exception. Similar issue happend w/ writer3 as well. 

 
{code:java}
scala> df.write.format(""hudi"").
     |   options(getQuickstartWriteConfigs).
     |   option(PRECOMBINE_FIELD.key(), ""created_at"").
     |   option(RECORDKEY_FIELD.key(), ""other"").
     |   option(PARTITIONPATH_FIELD.key(), ""type"").
     |   option(""hoodie.cleaner.policy.failed.writes"",""LAZY"").
     |   option(""hoodie.write.concurrency.mode"",""OPTIMISTIC_CONCURRENCY_CONTROL"").
     |   option(""hoodie.write.lock.provider"",""org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider"").
     |   option(""hoodie.write.lock.zookeeper.url"",""localhost"").
     |   option(""hoodie.write.lock.zookeeper.port"",""2181"").
     |   option(""hoodie.write.lock.zookeeper.lock_key"",""locks"").
     |   option(""hoodie.write.lock.zookeeper.base_path"",""/tmp/mw_testing/.locks"").
     |   option(TBL_NAME.key(), tableName).
     |   mode(Append).
     |   save(basePath)
21/11/15 07:47:33 WARN HoodieSparkSqlWriter$: Commit time 20211115074733457
21/11/15 07:47:35 WARN EmbeddedTimelineService: Started embedded timeline server at 10.0.0.202:57644
[Stage 2:>                                                        (0                                                                    21/11/15 07:47:39 ERROR RequestHandler: Got runtime exception servicing request partition=CreateEvent&maxinstant=20211115074301094&basepath=file%3A%2Ftmp%2Fmw_testing%2Ftrial2&lastinstantts=20211115074301094&timelinehash=ce963fe977a9d2176fadecf16c223cb3b98d7f6f7aaaf41cd7855eb098aee47d
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:132)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:39 WARN ExceptionMapper: Uncaught exception
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:132)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:39 ERROR PriorityBasedFileSystemView: Got error running preferred function. Trying secondary
org.apache.hudi.exception.HoodieRemoteException: Server Error
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFilesFromParams(RemoteHoodieTableFileSystemView.java:241)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFilesBeforeOrOn(RemoteHoodieTableFileSystemView.java:248)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.execute(PriorityBasedFileSystemView.java:97)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.getLatestBaseFilesBeforeOrOn(PriorityBasedFileSystemView.java:134)
    at org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForPartition(HoodieIndexUtils.java:54)
    at org.apache.hudi.index.HoodieIndexUtils.lambda$getLatestBaseFilesForAllPartitions$ff6885d8$1(HoodieIndexUtils.java:74)
    at org.apache.hudi.client.common.HoodieSparkEngineContext.lambda$flatMap$7d470b86$1(HoodieSparkEngineContext.java:134)
    at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125)
    at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125)
    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
    at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
    at scala.collection.AbstractIterator.to(Iterator.scala:1334)
    at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
    at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.http.client.HttpResponseException: Server Error
    at org.apache.http.impl.client.AbstractResponseHandler.handleResponse(AbstractResponseHandler.java:70)
    at org.apache.http.client.fluent.Response.handleResponse(Response.java:90)
    at org.apache.http.client.fluent.Response.returnContent(Response.java:97)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.executeRequest(RemoteHoodieTableFileSystemView.java:179)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFilesFromParams(RemoteHoodieTableFileSystemView.java:237)
    ... 33 more
21/11/15 07:47:39 ERROR RequestHandler: Got runtime exception servicing request partition=CreateEvent&basepath=file%3A%2Ftmp%2Fmw_testing%2Ftrial2&fileid=3ed17c94-b793-4224-b7b3-1c5766549df4-1&lastinstantts=20211115074301094&timelinehash=ce963fe977a9d2176fadecf16c223cb3b98d7f6f7aaaf41cd7855eb098aee47d
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
    at org.apache.hudi.org.apache.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:39 WARN ExceptionMapper: Uncaught exception
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
    at org.apache.hudi.org.apache.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:39 ERROR PriorityBasedFileSystemView: Got error running preferred function. Trying secondary
org.apache.hudi.exception.HoodieRemoteException: Server Error
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFile(RemoteHoodieTableFileSystemView.java:493)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.execute(PriorityBasedFileSystemView.java:97)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.getLatestBaseFile(PriorityBasedFileSystemView.java:140)
    at org.apache.hudi.io.HoodieReadHandle.getLatestDataFile(HoodieReadHandle.java:62)
    at org.apache.hudi.io.HoodieReadHandle.createNewFileReader(HoodieReadHandle.java:67)
    at org.apache.hudi.io.HoodieRangeInfoHandle.getMinMaxKeys(HoodieRangeInfoHandle.java:39)
    at org.apache.hudi.index.bloom.HoodieBloomIndex.lambda$loadInvolvedFiles$4cbadf07$1(HoodieBloomIndex.java:149)
    at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
    at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
    at scala.collection.AbstractIterator.to(Iterator.scala:1334)
    at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
    at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.http.client.HttpResponseException: Server Error
    at org.apache.http.impl.client.AbstractResponseHandler.handleResponse(AbstractResponseHandler.java:70)
    at org.apache.http.client.fluent.Response.handleResponse(Response.java:90)
    at org.apache.http.client.fluent.Response.returnContent(Response.java:97)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.executeRequest(RemoteHoodieTableFileSystemView.java:179)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFile(RemoteHoodieTableFileSystemView.java:489)
    ... 31 more
21/11/15 07:47:40 ERROR RequestHandler: Got runtime exception servicing request partition=CreateEvent&maxinstant=20211115074301094&basepath=file%3A%2Ftmp%2Fmw_testing%2Ftrial2&lastinstantts=20211115074301094&timelinehash=ce963fe977a9d2176fadecf16c223cb3b98d7f6f7aaaf41cd7855eb098aee47d
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
    at org.apache.hudi.org.apache.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:40 WARN ExceptionMapper: Uncaught exception
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
    at org.apache.hudi.org.apache.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:40 ERROR PriorityBasedFileSystemView: Got error running preferred function. Trying secondary
org.apache.hudi.exception.HoodieRemoteException: Server Error
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFilesFromParams(RemoteHoodieTableFileSystemView.java:241)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFilesBeforeOrOn(RemoteHoodieTableFileSystemView.java:248)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.execute(PriorityBasedFileSystemView.java:97)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.getLatestBaseFilesBeforeOrOn(PriorityBasedFileSystemView.java:134)
    at org.apache.hudi.table.action.commit.UpsertPartitioner.getSmallFiles(UpsertPartitioner.java:275)
    at org.apache.hudi.table.action.commit.UpsertPartitioner.lambda$getSmallFilesForPartitions$f1d92f9e$1(UpsertPartitioner.java:256)
    at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043)
    at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
    at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
    at scala.collection.AbstractIterator.to(Iterator.scala:1334)
    at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
    at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.http.client.HttpResponseException: Server Error
    at org.apache.http.impl.client.AbstractResponseHandler.handleResponse(AbstractResponseHandler.java:70)
    at org.apache.http.client.fluent.Response.handleResponse(Response.java:90)
    at org.apache.http.client.fluent.Response.returnContent(Response.java:97)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.executeRequest(RemoteHoodieTableFileSystemView.java:179)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFilesFromParams(RemoteHoodieTableFileSystemView.java:237)
    ... 31 more
21/11/15 07:47:41 ERROR RequestHandler: Got runtime exception servicing request partition=CreateEvent&basepath=file%3A%2Ftmp%2Fmw_testing%2Ftrial2&fileid=3ed17c94-b793-4224-b7b3-1c5766549df4-1&lastinstantts=20211115074301094&timelinehash=ce963fe977a9d2176fadecf16c223cb3b98d7f6f7aaaf41cd7855eb098aee47d
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
    at org.apache.hudi.org.apache.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:41 WARN ExceptionMapper: Uncaught exception
java.lang.IllegalArgumentException: Last known instant from client was 20211115074301094 but server has the following timeline [[20211115074301094__commit__COMPLETED], [20211115074731908__commit__COMPLETED]]
    at org.apache.hudi.common.util.ValidationUtils.checkArgument(ValidationUtils.java:40)
    at org.apache.hudi.timeline.service.RequestHandler$ViewHandler.handle(RequestHandler.java:510)
    at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)
    at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)
    at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)
    at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)
    at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)
    at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)
    at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
    at org.apache.hudi.org.apache.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
    at org.apache.hudi.org.apache.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
    at org.apache.hudi.org.apache.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
    at org.apache.hudi.org.apache.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerList.handle(HandlerList.java:61)
    at org.apache.hudi.org.apache.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)
    at org.apache.hudi.org.apache.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
    at org.apache.hudi.org.apache.jetty.server.Server.handle(Server.java:502)
    at org.apache.hudi.org.apache.jetty.server.HttpChannel.handle(HttpChannel.java:370)
    at org.apache.hudi.org.apache.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
    at org.apache.hudi.org.apache.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
    at org.apache.hudi.org.apache.jetty.io.FillInterest.fillable(FillInterest.java:103)
    at org.apache.hudi.org.apache.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
    at org.apache.hudi.org.apache.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
    at org.apache.hudi.org.apache.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
    at org.apache.hudi.org.apache.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
    at java.lang.Thread.run(Thread.java:748)
21/11/15 07:47:41 ERROR PriorityBasedFileSystemView: Got error running preferred function. Trying secondary
org.apache.hudi.exception.HoodieRemoteException: Server Error
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFile(RemoteHoodieTableFileSystemView.java:493)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.execute(PriorityBasedFileSystemView.java:97)
    at org.apache.hudi.common.table.view.PriorityBasedFileSystemView.getLatestBaseFile(PriorityBasedFileSystemView.java:140)
    at org.apache.hudi.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:115)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.getUpdateHandle(BaseSparkCommitActionExecutor.java:350)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:321)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:295)
    at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$execute$ecf5068c$1(BaseSparkCommitActionExecutor.java:154)
    at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
    at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:875)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:875)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
    at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
    at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)
    at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
    at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.http.client.HttpResponseException: Server Error
    at org.apache.http.impl.client.AbstractResponseHandler.handleResponse(AbstractResponseHandler.java:70)
    at org.apache.http.client.fluent.Response.handleResponse(Response.java:90)
    at org.apache.http.client.fluent.Response.returnContent(Response.java:97)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.executeRequest(RemoteHoodieTableFileSystemView.java:179)
    at org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView.getLatestBaseFile(RemoteHoodieTableFileSystemView.java:489)
    ... 36 more
[Stage 22:> {code}
 

 

writer1 : 20211115074731908

!Screen Shot 2021-11-15 at 8.27.11 AM.png!

 

writer2 : 20211115074733457

!Screen Shot 2021-11-15 at 8.27.33 AM.png!

writer3: 20211115074735131

!Screen Shot 2021-11-15 at 8.28.03 AM.png!

 

!Screen Shot 2021-11-15 at 8.28.25 AM.png!

 "	HUDI	Closed	2	3	9267	pull-request-available
13521824	Flaky ParquetProjection tests w/ CI	"some of the parquet projection tests are flaky in CI

[https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/14673/logs/57]

 
8 tests in TestParquetColumnProjection
is failing.
testMergeOnReadSnapshotRelationWithNoDeltaLogs
testMergeOnReadSnapshotRelationWithDeltaLogsFallback
etc"	HUDI	Closed	3	6	9267	pull-request-available
13426030	Hudi 0.8.0 cannot restore CoW table	"Hi Guys,

 

Environment: AWS EMR 6.4 / Hudi v0.8.0

Problem: I have a CoW table wich is ingested by DeltaStremer (batch style: every 5 minutes from Kafka), and after a certain time, DeltaStremer stops working with a message like this:

 

{{diagnostics: User class threw exception: org.apache.hudi.exception.HoodieRollbackException: Found commits after time :20220131215051, please rollback greater commits first}}

 

hudi:iot_raw.ingress_pkg_decoded_rep->desc
╔════════════════════════════════╤═════════════════════════════════════════════════════════════╗
║ Property                       │ Value                                                       ║
╠════════════════════════════════╪═════════════════════════════════════════════════════════════╣
║ basePath                       │ s3://scgps-datalake/iot_raw/ingress_pkg_decoded_rep         ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ metaPath                       │ s3://scgps-datalake/iot_raw/ingress_pkg_decoded_rep/.hoodie ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ fileSystem                     │ s3                                                          ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ hoodie.table.name              │ iot_raw.ingress_pkg_decoded_rep                             ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ hoodie.table.type              │ COPY_ON_WRITE                                               ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ hoodie.archivelog.folder       │ archived                                                    ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ hoodie.table.base.file.format  │ PARQUET                                                     ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ hoodie.timeline.layout.version │ 1                                                           ║
╟────────────────────────────────┼─────────────────────────────────────────────────────────────╢
║ hoodie.table.version           │ 1                                                           ║
╚════════════════════════════════╧═════════════════════════════════════════════════════════════╝

 

It is usually a replace commit, I would say I am pretty sure in this.

I have commits in the timeline:

 

20220131214354<-before
20220131215051<-error message
20220131215514<-after

 

So as it was told to me, I try to rollback with the following steps in hudi-cli:

1.) connect --path s3://scgps-datalake/iot_raw/ingress_pkg_decoded_rep / SUCCESS

2.) savepoint create --commit 20220131214354 --sparkMaster local[2] / SUCCESS

3.) savepoint rollback --savepoint 20220131214354 --sparkMaster local[2] / FAILED

4.) savepoint create --commit 20220131215514 --sparkMaster local[2] / SUCCESS

5.) savepoint rollback --savepoint 20220131215514 --sparkMaster local[2] / FAILED

 

Long story short, if I run a situation like this I am not able to solve it with the known methods ;) - My use-case is working progress, but I cannot go prod with an issue like this.

 

My question, what would be the right steps / commands to solve an issue like this, and be able to restart deltastremer again.

 

This table, does not have dimension data, so I am happy to share the whole table if someone curiuous (if that is needed or would be helpful, lets talk in a private mail / slack about the sharing). ~15GB  ;) it was stoped after a few run, actually after the 1st clustering.

 

I use this clustering config in the DeltaStremer:

hoodie.clustering.inline=true
hoodie.clustering.inline.enabled=true
hoodie.clustering.inline.max.commits=36
hoodie.clustering.plan.strategy.sort.columns=correlation_id
hoodie.clustering.plan.strategy.daybased.lookback.partitions=7
hoodie.clustering.plan.strategy.target.file.max.bytes=268435456
hoodie.clustering.plan.strategy.small.file.limit=134217728
hoodie.clustering.plan.strategy.max.bytes.per.group=671088640

 

I hope there is someone who can help me to tackle with this, becase if I able to solve this manually, I would be confident to go prod.

So thanks in advance,

Darvi

Slack Hudi: istvan darvas / U02NTACPHPU"	HUDI	Closed	2	1	9267	pull-request-available
13520008	Support any record key generation along w/ any partition path generation for row writer	"HUDI-5535 adds support for record key generation along w/ any partition path generation. It also separates the record key generation and partition path generation into separate interfaces.

This jira aims to add similar support for the row writer path in spark.

cc [~shivnarayan] "	HUDI	Reopened	2	1	9267	pull-request-available
13481671	Fix tooling for deprecated partition 	"hudi cli has support to fix deprecated partition. but it assume ""string"" datatype for the partitioning col. We might have to fix that assumption. 

 "	HUDI	Closed	1	1	9267	pull-request-available
13403349	Tuning HoodieROTablePathFilter by caching, aiming to reduce unnecessary list/get requests	"Cache HoodieTableFileSystemView at baseDir level

The same as HoodieTableMetaClient"	HUDI	Resolved	1	3	9267	pull-request-available
13533768	Fix multiple streaming writers w/ streaming sink	"if we have multiple streaming writers w/ out of order commits, chances that checkpoint management can miss things and can cause dups. we need to fix it properly. 

 "	HUDI	Closed	3	1	9267	pull-request-available
13397868	Fix tabs in spark quick start page esply around spark-sql	Fix tabs in spark quick start page esply around spark-sql	HUDI	Resolved	3	4	9267	pull-request-available
13418288	Fix flaky TestHoodieClientMultiWriter. testHoodieClientBasicMultiWriter	"Ref: [https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/4428/logs/21]

 
{code:java}
2021-12-17T11:39:57.1645757Z [INFO] Running org.apache.hudi.client.TestHoodieClientMultiWriter
2021-12-17T11:39:57.3453991Z 339506 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit8865530218583640556/dataset/.hoodie/metadata
2021-12-17T11:39:57.3984328Z 339559 [dispatcher-event-loop-5] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:39:57.5278608Z 339689 [dispatcher-event-loop-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 1 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:39:57.9783107Z 340139 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit8865530218583640556/dataset/.hoodie/metadata
2021-12-17T11:39:57.9927490Z 340154 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit8865530218583640556/dataset/.hoodie/metadata
2021-12-17T11:40:10.1428665Z 352304 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:40:10.9930023Z 353149 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:40:11.4294603Z 353590 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit3262960667280061850/dataset/.hoodie/metadata
2021-12-17T11:40:11.4763085Z 353637 [dispatcher-event-loop-5] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:40:11.6014876Z 353762 [dispatcher-event-loop-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 1 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:40:12.0892513Z 354250 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit3262960667280061850/dataset/.hoodie/metadata
2021-12-17T11:40:12.1061317Z 354267 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit3262960667280061850/dataset/.hoodie/metadata
2021-12-17T11:40:23.1499732Z 365311 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:40:24.1626167Z 366323 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit294667857867877904/dataset/.hoodie/metadata
2021-12-17T11:40:24.1945944Z 366355 [dispatcher-event-loop-5] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:40:24.3084730Z 366469 [dispatcher-event-loop-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 1 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:40:24.7350862Z 366896 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit294667857867877904/dataset/.hoodie/metadata
2021-12-17T11:40:24.7482727Z 366909 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit294667857867877904/dataset/.hoodie/metadata
2021-12-17T11:40:43.1530857Z 385314 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:40:44.0641298Z 386225 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:40:44.5065216Z 386667 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit4425942033067835021/dataset/.hoodie/metadata
2021-12-17T11:40:44.5408012Z 386702 [dispatcher-event-loop-5] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:40:44.6779567Z 386839 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 1 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:40:45.1410815Z 387301 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit4425942033067835021/dataset/.hoodie/metadata
2021-12-17T11:40:45.1543539Z 387315 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit4425942033067835021/dataset/.hoodie/metadata
2021-12-17T11:41:03.1384862Z 405299 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:41:04.0150577Z 406176 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit5137952068799441319/dataset/.hoodie/metadata
2021-12-17T11:41:04.0514934Z 406212 [dispatcher-event-loop-5] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:41:04.1677826Z 406329 [dispatcher-event-loop-2] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 1 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:41:04.6164436Z 406777 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit5137952068799441319/dataset/.hoodie/metadata
2021-12-17T11:41:04.6343307Z 406791 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit5137952068799441319/dataset/.hoodie/metadata
2021-12-17T11:41:13.0144546Z 415165 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:41:13.8545883Z 416010 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:41:14.2624248Z 416423 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit1733672335771853967/dataset/.hoodie/metadata
2021-12-17T11:41:14.2957800Z 416457 [dispatcher-event-loop-5] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:41:14.3989543Z 416560 [dispatcher-event-loop-3] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 1 contains a task of very large size (101 KB). The maximum recommended task size is 100 KB.
2021-12-17T11:41:14.7647136Z 416926 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit1733672335771853967/dataset/.hoodie/metadata
2021-12-17T11:41:14.7772128Z 416938 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit1733672335771853967/dataset/.hoodie/metadata
2021-12-17T11:41:22.6703021Z 424828 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:41:23.5771003Z 425738 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit4023714709478834450/dataset/.hoodie/metadata
2021-12-17T11:41:24.3341519Z 426495 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit4023714709478834450/dataset/.hoodie/metadata
2021-12-17T11:41:24.3488610Z 426510 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit4023714709478834450/dataset/.hoodie/metadata
2021-12-17T11:41:45.2669117Z 447428 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:41:46.1117672Z 448273 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:41:46.5285885Z 448689 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit6437731807948505371/dataset/.hoodie/metadata
2021-12-17T11:41:47.2040819Z 449365 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit6437731807948505371/dataset/.hoodie/metadata
2021-12-17T11:41:47.2190197Z 449380 [main] WARN  org.apache.hudi.metadata.HoodieBackedTableMetadata  - Metadata table was not found at path /tmp/junit6437731807948505371/dataset/.hoodie/metadata
2021-12-17T11:42:12.0807715Z 474241 [main] WARN  org.apache.hudi.testutils.HoodieClientTestHarness  - Closing file-system instance used in previous test-run
2021-12-17T11:42:12.8434589Z [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 135.656 s <<< FAILURE! - in org.apache.hudi.client.TestHoodieClientMultiWriter
2021-12-17T11:42:12.8435532Z [ERROR] org.apache.hudi.client.TestHoodieClientMultiWriter.testHoodieClientBasicMultiWriter(HoodieTableType)[1]  Time elapsed: 13.652 s  <<< ERROR!
2021-12-17T11:42:12.8436481Z java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.hudi.exception.HoodieWriteConflictException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
2021-12-17T11:42:12.8437368Z 	at org.apache.hudi.client.TestHoodieClientMultiWriter.testHoodieClientBasicMultiWriter(TestHoodieClientMultiWriter.java:129)
2021-12-17T11:42:12.8438208Z Caused by: java.lang.RuntimeException: org.apache.hudi.exception.HoodieWriteConflictException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
2021-12-17T11:42:12.8439052Z 	at org.apache.hudi.client.TestHoodieClientMultiWriter.lambda$testHoodieClientBasicMultiWriter$1(TestHoodieClientMultiWriter.java:125)
2021-12-17T11:42:12.8439840Z Caused by: org.apache.hudi.exception.HoodieWriteConflictException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
2021-12-17T11:42:12.8440785Z 	at org.apache.hudi.client.TestHoodieClientMultiWriter.createCommitWithUpserts(TestHoodieClientMultiWriter.java:432)
2021-12-17T11:42:12.8441570Z 	at org.apache.hudi.client.TestHoodieClientMultiWriter.lambda$testHoodieClientBasicMultiWriter$1(TestHoodieClientMultiWriter.java:122)
2021-12-17T11:42:12.8442342Z Caused by: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
2021-12-17T11:42:12.8443007Z 	at org.apache.hudi.client.TestHoodieClientMultiWriter.createCommitWithUpserts(TestHoodieClientMultiWriter.java:432)
2021-12-17T11:42:12.8443792Z 	at org.apache.hudi.client.TestHoodieClientMultiWriter.lambda$testHoodieClientBasicMultiWriter$1(TestHoodieClientMultiWriter.java:122)
2021-12-17T11:42:12.8444188Z  {code}"	HUDI	Closed	1	6	9267	pull-request-available
13412674	Ensure within a single lock we commit to both metadata table and data table for table services	Ensure within a single lock we commit to both metadata table and data table for table services	HUDI	Resolved	1	4	9267	pull-request-available
13482414	docker demo fails w/ ClassNotFound w/ LogicalType in latest master	"docker demo fails during hive-sync w/ latest master. 

also, some of the env variables are not applied. for eg, HUDI_UTILITIES_BUNDLE was not set. after setting it explicitly, I was able to get the 1st ingest working. and then hive sync failed.

 

command used:
{code:java}
/var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh   --jdbc-url jdbc:hive2://hiveserver:10000   --user hive   --pass hive   --partitioned-by dt   --base-path /user/hive/warehouse/stock_ticks_cow   --database default   --table stock_ticks_cow {code}
 

output:
{code:java}
2022-09-20 14:24:39,122 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(179)) - Trying to sync hoodie table stock_ticks_cow with base path /user/hive/warehouse/stock_ticks_cow of type COPY_ON_WRITE
2022-09-20 14:24:39,758 INFO  [main] table.TableSchemaResolver (TableSchemaResolver.java:readSchemaFromParquetBaseFile(439)) - Reading schema from /user/hive/warehouse/stock_ticks_cow/2018/08/31/b4a7076c-30e6-4320-bb04-be47246b6646-0_0-29-29_20220920142351042.parquet
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2022-09-20 14:24:40,432 INFO  [main] hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 0
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/avro/LogicalType
	at org.apache.hudi.common.table.TableSchemaResolver.convertParquetSchemaToAvro(TableSchemaResolver.java:288)
	at org.apache.hudi.common.table.TableSchemaResolver.getTableAvroSchemaFromDataFile(TableSchemaResolver.java:121)
	at org.apache.hudi.common.table.TableSchemaResolver.hasOperationField(TableSchemaResolver.java:566)
	at org.apache.hudi.util.Lazy.get(Lazy.java:53)
	at org.apache.hudi.common.table.TableSchemaResolver.getTableSchemaFromLatestCommitMetadata(TableSchemaResolver.java:225)
	at org.apache.hudi.common.table.TableSchemaResolver.getTableAvroSchemaInternal(TableSchemaResolver.java:193)
	at org.apache.hudi.common.table.TableSchemaResolver.getTableAvroSchema(TableSchemaResolver.java:142)
	at org.apache.hudi.common.table.TableSchemaResolver.getTableParquetSchema(TableSchemaResolver.java:173)
	at org.apache.hudi.sync.common.HoodieSyncClient.getStorageSchema(HoodieSyncClient.java:103)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:206)
	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:153)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:141)
	at org.apache.hudi.hive.HiveSyncTool.main(HiveSyncTool.java:358)
Caused by: java.lang.ClassNotFoundException: org.apache.avro.LogicalType
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 13 more {code}
 

 

tried the spark-submit command directly and it succeeded. 
{code:java}
spark-submit   --class org.apache.hudi.hive.HiveSyncTool /var/hoodie/ws/packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.13.0-SNAPSHOT.jar    --database default   --table stock_ticks_cow   --base-path /user/hive/warehouse/stock_ticks_cow    --base-file-format PARQUET   --user hive --pass hive   --jdbc-url jdbc:hive2://hiveserver:10000 --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor   --partitioned-by dt {code}
 

output:
{code:java}
spark-submit   --class org.apache.hudi.hive.HiveSyncTool /var/hoodie/ws/packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.13.0-SNAPSHOT.jar    --database default   --table stock_ticks_cow   --base-path /user/hive/warehouse/stock_ticks_cow    --base-file-format PARQUET   --user hive --pass hive   --jdbc-url jdbc:hive2://hiveserver:10000 --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor   --partitioned-by dt
22/09/20 15:23:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/09/20 15:23:12 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_cow
22/09/20 15:23:12 INFO table.HoodieTableConfig: Loading table properties from /user/hive/warehouse/stock_ticks_cow/.hoodie/hoodie.properties
22/09/20 15:23:12 INFO table.HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from /user/hive/warehouse/stock_ticks_cow
22/09/20 15:23:12 INFO table.HoodieTableMetaClient: Loading Active commit timeline for /user/hive/warehouse/stock_ticks_cow
22/09/20 15:23:12 INFO timeline.HoodieActiveTimeline: Loaded instants upto : Option{val=[20220920142351042__commit__COMPLETED]}
22/09/20 15:23:12 INFO jdbc.Utils: Supplied authorities: hiveserver:10000
22/09/20 15:23:12 INFO jdbc.Utils: Resolved authority: hiveserver:10000
22/09/20 15:23:12 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hiveserver:10000
22/09/20 15:23:13 INFO ddl.QueryBasedDDLExecutor: Successfully established Hive connection to  jdbc:hive2://hiveserver:10000
22/09/20 15:23:13 INFO hive.metastore: Trying to connect to metastore with URI thrift://hivemetastore:9083
22/09/20 15:23:13 INFO hive.metastore: Connected to metastore.
22/09/20 15:23:13 INFO hive.HiveSyncTool: Syncing target hoodie table with hive table(default.stock_ticks_cow). Hive metastore URL :null, basePath :/user/hive/warehouse/stock_ticks_cow
22/09/20 15:23:13 INFO hive.HiveSyncTool: Trying to sync hoodie table stock_ticks_cow with base path /user/hive/warehouse/stock_ticks_cow of type COPY_ON_WRITE
22/09/20 15:23:14 INFO table.TableSchemaResolver: Reading schema from /user/hive/warehouse/stock_ticks_cow/2018/08/31/b4a7076c-30e6-4320-bb04-be47246b6646-0_0-29-29_20220920142351042.parquet
22/09/20 15:23:14 INFO hive.HiveSyncTool: Hive table stock_ticks_cow is not found. Creating it
22/09/20 15:23:15 INFO ddl.QueryBasedDDLExecutor: Creating table with CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_cow`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` String) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_cow') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_cow' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{""type"":""struct"",""fields"":[{""name"":""_hoodie_commit_time"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_commit_seqno"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_record_key"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_partition_path"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_file_name"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""volume"",""type"":""long"",""nullable"":false,""metadata"":{}},{""name"":""ts"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""symbol"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""year"",""type"":""integer"",""nullable"":false,""metadata"":{}},{""name"":""month"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""high"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""low"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""key"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""date"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""close"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""open"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""day"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""dt"",""type"":""string"",""nullable"":false,""metadata"":{}}]}')
22/09/20 15:23:15 INFO ddl.QueryBasedDDLExecutor: Executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`stock_ticks_cow`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `volume` bigint, `ts` string, `symbol` string, `year` int, `month` string, `high` double, `low` double, `key` string, `date` string, `close` double, `open` double, `day` string) PARTITIONED BY (`dt` String) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='/user/hive/warehouse/stock_ticks_cow') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION '/user/hive/warehouse/stock_ticks_cow' TBLPROPERTIES('spark.sql.sources.schema.partCol.0'='dt','spark.sql.sources.schema.numParts'='1','spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.provider'='hudi','spark.sql.sources.schema.part.0'='{""type"":""struct"",""fields"":[{""name"":""_hoodie_commit_time"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_commit_seqno"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_record_key"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_partition_path"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""_hoodie_file_name"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""volume"",""type"":""long"",""nullable"":false,""metadata"":{}},{""name"":""ts"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""symbol"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""year"",""type"":""integer"",""nullable"":false,""metadata"":{}},{""name"":""month"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""high"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""low"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""key"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""date"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""close"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""open"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""day"",""type"":""string"",""nullable"":false,""metadata"":{}},{""name"":""dt"",""type"":""string"",""nullable"":false,""metadata"":{}}]}')
22/09/20 15:23:17 INFO hive.HiveSyncTool: Schema sync complete. Syncing partitions for stock_ticks_cow
22/09/20 15:23:17 INFO hive.HiveSyncTool: Last commit time synced was found to be null
22/09/20 15:23:17 INFO common.HoodieSyncClient: Last commit time synced is not known, listing all partitions in /user/hive/warehouse/stock_ticks_cow,FS :DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1898513899_1, ugi=root (auth:SIMPLE)]]
22/09/20 15:23:17 INFO hive.HiveSyncTool: Storage partitions scan complete. Found 1
22/09/20 15:23:17 INFO hive.HiveSyncTool: New Partitions [2018/08/31]
22/09/20 15:23:17 INFO ddl.QueryBasedDDLExecutor: Adding partitions 1 to table stock_ticks_cow
22/09/20 15:23:17 INFO ddl.QueryBasedDDLExecutor: Executing SQL ALTER TABLE `default`.`stock_ticks_cow` ADD IF NOT EXISTS   PARTITION (`dt`='2018-08-31') LOCATION '/user/hive/warehouse/stock_ticks_cow/2018/08/31' 
22/09/20 15:23:18 INFO hive.HiveSyncTool: Sync complete for stock_ticks_cow
22/09/20 15:23:18 INFO util.ShutdownHookManager: Shutdown hook called
22/09/20 15:23:18 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-59275d30-b1bb-4f7d-af85-bce24962ca1e {code}
 "	HUDI	Closed	1	1	9267	pull-request-available
13420928	Include only files belonging to completed commits while bootstrapping metadata table	metadata table bootstrap does not filter for completed commit files, but all files based on fs.list call. We should filter out files that are part of an ongoing commit while doing bootstrap. 	HUDI	Closed	1	3	9267	pull-request-available, sev:critical
13420201	Fix Show Partitions Command's Result after drop partition	"# add two partitions  dt='2021-10-01',  dt='2021-10-02'
 # drop one partition dt='2021-10-01'
 # show partitions ,The query result: dt='2021-10-01',  dt='2021-10-02' The expected result is: dt='2021-10-02'"	HUDI	Closed	1	1	9267	pull-request-available, user-support-issues
13515120	Add support to write record level index to MDT	Add support to write our record level index partition to MDT	HUDI	Closed	1	4	9267	pull-request-available
13357740	Fix archival max log size and potentially a bug in archival	"Gist of the issue from Udit

 

I took a deeper look at this. For you this seems to be happening in the archival code path:

 

{{ at org.apache.hudi.table.HoodieTimelineArchiveLog.writeToFile(HoodieTimelineArchiveLog.java:309)
 at org.apache.hudi.table.HoodieTimelineArchiveLog.archive(HoodieTimelineArchiveLog.java:282)
 at org.apache.hudi.table.HoodieTimelineArchiveLog.archiveIfRequired(HoodieTimelineArchiveLog.java:133)
 at org.apache.hudi.client.HoodieWriteClient.postCommit(HoodieWriteClient.java:381)}}

In {{HoodieTimelineArchiveLog}} where it needs to write log files with commit record, similar to how log files are written for MOR tables. However, in this code I notice a couple of issues:
 * The default maximum log block size of 256 MB defined [here|https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieStorageConfig.java#L51], is not utilized for this class and is only used for the MOR log blocks writing case. As a result, there is no real control over the block size that it can end up writing which can potentially overflow {{ByteArrayOutputStream}} whose maximum size is {{Integer.MAX_VALE - 8}}. That is what seems to be happening in this scenario here because of an integer overflow following that code path inside {{ByteArrayOutputStream}}. So we need to use the maximum block size concept here as well.
 * In addition I see a bug in code [here|https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java#L302] where even after flushing out the records into a file after a batch size of 10 (default) it is not clearing the list and just goes on accumulating the records. This seems logically wrong as well (duplication), apart from the fact that it would keep increasing the log file blocks size it is writing.

Reference: https://github.com/apache/hudi/issues/2408#issuecomment-758320870"	HUDI	Open	3	1	9267	core-flow-ds, sev:high, user-support-issues
13437912	Ensure heart beat client health check for rollbacks are threadsafe with multiple writers	If multiple writers are checking for partially failed commits to rollback, ensure its thread safe. ie. no overstepping happens wrt two writer rolling back the same commit 	HUDI	Closed	1	1	9267	pull-request-available
13595148	Avoid filesystem view and use commit metadata to get the partition file slices pairs for which functional index is to be updated	The logic [here|https://github.com/apache/hudi/blob/7530e4fa48fb6c32e9cafb587914521bbbb4bc23/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java#L1084] could be problematic for COW and we might end up recomputing index for some files that are not touched by the commit. This should be fixed. We should solely rely on commit metadata.	HUDI	Closed	1	3	9267	pull-request-available
13479382	PartitionsForFullCleaning in CleanPlanner is using FileSystemBasedListing	"[https://github.com/apache/hudi/pull/5169/files#diff-f3f9696ca040bd9f581a1122b1c5cf0409b3ba4525672fbf3328e9f2c40eec66]

 

Looks like we are doing file system based full listing when partitionsForFullCleaning is being invoked. When the patch was landed, DELETE_PARTITION was synchronous and hence the fix. but later we fixed DELETE_PARTITION to be lazy. and so, we can use metadata table if enabled. 

 "	HUDI	In Progress	3	1	9267	pull-request-available
13348387	Support configuration to let user override record-size estimate  	"Context: [https://github.com/apache/hudi/issues/2393]

 

It would be helpful if for some reason the user needs to ingest a batch of records which has a very different record sizes compared to existing records. "	HUDI	Resolved	3	4	9267	newbie
13410662	rollback of a partially failed commit which has new partitions fails with metadata table	"When a commit is being rolledback, and the commit has new partitions which was not present in the table before, files pertaining to this new partition may not be part of rollback plan. and so these files will be end up dangling w/o being cleaned up. 

 

Eg:

commit 1: p1 (5 files) p2(5 files)

commit2: p1(3 files) p2(3 files) p3(2 files) partial failed write. 

 

when commit3 is triggered, it will rollback commit2 

when generating rollback plan, we first fetch all partitions from TableFileSystemView which will hit metadata table when enabled. 

This may return only p1 and p2 and not p3(since commit2 is not completed)

and then we do fs.list and filter out files that matches the commit2. 

So, in this case, we might miss to rollback the files added to p3. 

 

 

 "	HUDI	Resolved	1	3	9267	pull-request-available
13353100	Hudi spark datasource fails w/ NoClassDefFoundError: org/apache/hudi/client/common/HoodieEngineContext	"I tried Quick Start Guide w/ latest master.

 
{code:java}
// first insert
scala> df.write.format(""hudi"").
     |   options(getQuickstartWriteConfigs).
     |   option(PRECOMBINE_FIELD_OPT_KEY, ""ts"").
     |   option(RECORDKEY_FIELD_OPT_KEY, ""uuid"").
     |   option(PARTITIONPATH_FIELD_OPT_KEY, ""partitionpath"").
     |   option(TABLE_NAME, tableName).
     |   mode(Overwrite).
     |   save(basePath)
java.lang.NoClassDefFoundError: org/apache/hudi/client/common/HoodieEngineContext
  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:120)
  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:134)
  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)
  ... 68 elided
Caused by: java.lang.ClassNotFoundException: org.apache.hudi.client.common.HoodieEngineContext
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 91 more
{code}
Command used for spark-submit:
{code:java}
./bin/spark-shell   --packages org.apache.spark:spark-avro_2.11:2.4.4   --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'   --jars hudi-utilities-bundle_2.11-0.7.0-SNAPSHOT.jar
{code}
Steps to repro:

Just follow the quick start guide w/ above command for spark submit. 

 

 "	HUDI	Resolved	3	1	9267	pull-request-available, release-blocker
13417266	Add support to start incremental consumption from begin time rather than latest commit time with S3EventsHoodieIncrSource	with S3EventsHoodieIncrSource, if begin time is not provided to consume from 1st meta table, we fallback to latest commit time. we should support starting from earliest instant. 	HUDI	Resolved	3	4	9267	core-flow-ds, pull-request-available, sev:normal
13383657	Fix extra commit metadata in row writer path	In regular path (write client), users can pass in extra commit metadata with help of a commit key prefix config. In row writer path, this was not addressed. 	HUDI	Resolved	3	1	9267	pull-request-available
13389518	Virtual keys support for Compaction	Virtual keys support for Compaction	HUDI	Resolved	3	3	9267	pull-request-available
13478227	Primary key-less data model	Hudi requires users to specify a primary key field. Can we do away with this requirement? This epic tracks the work to support use cases which does not require primary key based data modelling.	HUDI	Closed	3	15	9267	pull-request-available
13411946	Enable marker based rollback by default	Enable marker based rollback by default	HUDI	Resolved	1	4	9267	pull-request-available
13413893	Fix handling of cluster update reject exception by Deltastreamer	"When a commit from deltastreamer clashes w/ a pending clustering, detlastreamer shuts it down and expects user to restart. 

 

We need to fix this behavior to keep retrying until clustering completes and eventually deltastreamer gets going."	HUDI	Closed	1	1	9267	pull-request-available
13473916	Fix handling of corrupt avro files properly	"We fixed handling of corupt files sometime back, but feedback was given that we should catch only the required exception and stacktrace and not entire ""Exception"" as it could mean other things as well. 

 

[https://github.com/apache/hudi/pull/5245/files#r927097078]

 

 

 "	HUDI	Closed	2	4	9267	pull-request-available
13593001	Fix LogRecord reader to account for rollback blocks with higher timestamps	"With LogRecordReader, we also configure maxIntant time to read. Sometimes rollback blocks could have higher timestamps compared to the maxInstant set, which might lead to some data inconsistencies.  

 

Lets go through an illustration:

Say, we have t1.dc, t2.dc and t2.dc crashed mid way.
Current layout is,
{{base file(t1), lf1(partially committed data w/ t2 as instant time)}}
 
Then we start t5.dc say. just when we start t5.dc, hudi detects pending commit and triggers a rollback. And this rollback will get an instant time of t6 (t6.rb). Note that rollback's commit time is greater than t5 or current ongoing delta commit.
So, once rollback completes, this is the layout.
{{base file, lf1(from t2.dc partially failed), lf3 (rollback command block with t6).}}
 
And once t5.dc completes, this is how the layout looks like
{{base file, lf1(from t2.dc partially failed), lf3 (rollback command block with t6). lf4 (from t5)}}
 
At this point in time, when we trigger snapshot read or try to trigger tagLocation w/ global index, maxInstant is set to last entry among commits timeline which is t5. So, while LogRecordReader while processing all log blocks, when it reaches lf3, it detects the timestamp of t6 > t5 (i.e max instant time) and bails out of for loop. So, in essence it may not even read lf4 in above scenario.

 

If lf1 and lf4 is referring to a delete block, it could lead to data consistency issues w/ global indexes when record moves from one partition to another. 

 "	HUDI	Closed	1	4	9267	pull-request-available
13524718	Fail any new commits if there is any inflight restore in timeline	"if restore failed mid-way, users should not be allowed to start new commits. lets add a guard rail around that. 

 "	HUDI	Closed	2	4	9267	pull-request-available
13579732	Duplicate Key exception with RLI 	"We are occasionally hitting an exception as below meaning, two records are ingested to RLI for the same record key from data table. This is not expected to happen. 

 
{code:java}
Caused by: org.apache.hudi.exception.HoodieAppendException: Failed while appending records to file:/var/folders/ym/8yjkm3n90kq8tk4gfmvk7y140000gn/T/junit2792173348364470678/.hoodie/metadata/record_index/.record-index-0009-0_00000000000000011.log.3_3-275-476	at org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:475)	at org.apache.hudi.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:439)	at org.apache.hudi.table.action.deltacommit.BaseSparkDeltaCommitActionExecutor.handleUpdate(BaseSparkDeltaCommitActionExecutor.java:90)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:355)	... 28 moreCaused by: org.apache.hudi.exception.HoodieException: Writing multiple records with same key 1 not supported for org.apache.hudi.common.table.log.block.HoodieHFileDataBlock	at org.apache.hudi.common.table.log.block.HoodieHFileDataBlock.serializeRecords(HoodieHFileDataBlock.java:146)	at org.apache.hudi.common.table.log.block.HoodieDataBlock.getContentBytes(HoodieDataBlock.java:121)	at org.apache.hudi.common.table.log.HoodieLogFormatWriter.appendBlocks(HoodieLogFormatWriter.java:166)	at org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:467)	... 31 more
Driver stacktrace:51301 [main] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 78 failed: collect at HoodieJavaRDD.java:177, took 0.245313 s51303 [main] INFO  org.apache.hudi.client.BaseHoodieClient [] - Stopping Timeline service !!51303 [main] INFO  org.apache.hudi.client.embedded.EmbeddedTimelineService [] - Closing Timeline server51303 [main] INFO  org.apache.hudi.timeline.service.TimelineService [] - Closing Timeline Service51321 [main] INFO  org.apache.hudi.timeline.service.TimelineService [] - Closed Timeline Service51321 [main] INFO  org.apache.hudi.client.embedded.EmbeddedTimelineService [] - Closed Timeline server
org.apache.hudi.exception.HoodieUpsertException: Failed to upsert for commit time 19700101000020000
	at org.apache.hudi.table.action.commit.BaseWriteHelper.write(BaseWriteHelper.java:80)	at org.apache.hudi.table.action.deltacommit.SparkUpsertDeltaCommitActionExecutor.execute(SparkUpsertDeltaCommitActionExecutor.java:47)	at org.apache.hudi.table.HoodieSparkMergeOnReadTable.upsert(HoodieSparkMergeOnReadTable.java:98)	at org.apache.hudi.table.HoodieSparkMergeOnReadTable.upsert(HoodieSparkMergeOnReadTable.java:88)	at org.apache.hudi.client.SparkRDDWriteClient.upsert(SparkRDDWriteClient.java:156)	at org.apache.hudi.functional.TestGlobalIndexEnableUpdatePartitions.testUdpateSubsetOfRecUpdates(TestGlobalIndexEnableUpdatePartitions.java:225)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:373)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:419)	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)	at java.util.ArrayList.forEach(ArrayList.java:1257)	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)	at java.util.ArrayList.forEach(ArrayList.java:1257)	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96)	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:71)	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)Caused by: org.apache.hudi.exception.HoodieException: Failed to update metadata	at org.apache.hudi.table.action.BaseActionExecutor.writeTableMetadata(BaseActionExecutor.java:72)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.commit(BaseSparkCommitActionExecutor.java:326)	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.autoCommit(BaseCommitActionExecutor.java:206)	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.commitOnAutoCommit(BaseCommitActionExecutor.java:188)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.updateIndexAndCommitIfNeeded(BaseSparkCommitActionExecutor.java:294)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.execute(BaseSparkCommitActionExecutor.java:196)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.execute(BaseSparkCommitActionExecutor.java:87)	at org.apache.hudi.table.action.commit.BaseWriteHelper.write(BaseWriteHelper.java:74)	... 129 moreCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 275.0 failed 1 times, most recent failure: Lost task 3.0 in stage 275.0 (TID 476) (10.0.0.202 executor driver): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :3	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:362)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$mapPartitionsAsRDD$a3ab3c4$1(BaseSparkCommitActionExecutor.java:272)	at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)	at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)	at org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1418)	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1482)	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1305)	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)	at org.apache.spark.scheduler.Task.run(Task.scala:131)	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.hudi.exception.HoodieAppendException: Failed while appending records to file:/var/folders/ym/8yjkm3n90kq8tk4gfmvk7y140000gn/T/junit2792173348364470678/.hoodie/metadata/record_index/.record-index-0009-0_00000000000000011.log.3_3-275-476	at org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:475)	at org.apache.hudi.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:439)	at org.apache.hudi.table.action.deltacommit.BaseSparkDeltaCommitActionExecutor.handleUpdate(BaseSparkDeltaCommitActionExecutor.java:90)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:355)	... 28 moreCaused by: org.apache.hudi.exception.HoodieException: Writing multiple records with same key 1 not supported for org.apache.hudi.common.table.log.block.HoodieHFileDataBlock	at org.apache.hudi.common.table.log.block.HoodieHFileDataBlock.serializeRecords(HoodieHFileDataBlock.java:146)	at org.apache.hudi.common.table.log.block.HoodieDataBlock.getContentBytes(HoodieDataBlock.java:121)	at org.apache.hudi.common.table.log.HoodieLogFormatWriter.appendBlocks(HoodieLogFormatWriter.java:166)	at org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:467)	... 31 more
Driver stacktrace:	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)	at scala.Option.foreach(Option.scala:407)	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)	at org.apache.spark.api.java.JavaRDDLike.collect(JavaRDDLike.scala:362)	at org.apache.spark.api.java.JavaRDDLike.collect$(JavaRDDLike.scala:361)	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)	at org.apache.hudi.data.HoodieJavaRDD.collectAsList(HoodieJavaRDD.java:177)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.setCommitMetadata(BaseSparkCommitActionExecutor.java:304)	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.autoCommit(BaseCommitActionExecutor.java:202)	at org.apache.hudi.table.action.commit.BaseCommitActionExecutor.commitOnAutoCommit(BaseCommitActionExecutor.java:188)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.updateIndexAndCommitIfNeeded(BaseSparkCommitActionExecutor.java:294)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.execute(BaseSparkCommitActionExecutor.java:196)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.execute(BaseSparkCommitActionExecutor.java:161)	at org.apache.hudi.table.action.deltacommit.SparkUpsertPreppedDeltaCommitActionExecutor.execute(SparkUpsertPreppedDeltaCommitActionExecutor.java:44)	at org.apache.hudi.table.HoodieSparkMergeOnReadTable.upsertPrepped(HoodieSparkMergeOnReadTable.java:126)	at org.apache.hudi.table.HoodieSparkMergeOnReadTable.upsertPrepped(HoodieSparkMergeOnReadTable.java:88)	at org.apache.hudi.client.SparkRDDWriteClient.upsertPreppedRecords(SparkRDDWriteClient.java:170)	at org.apache.hudi.client.SparkRDDWriteClient.upsertPreppedRecords(SparkRDDWriteClient.java:66)	at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.commitInternal(HoodieBackedTableMetadataWriter.java:1101)	at org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter.commit(SparkHoodieBackedTableMetadataWriter.java:117)	at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.processAndCommit(HoodieBackedTableMetadataWriter.java:813)	at org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.updateFromWriteStatuses(HoodieBackedTableMetadataWriter.java:868)	at org.apache.hudi.table.action.BaseActionExecutor.writeTableMetadata(BaseActionExecutor.java:67)	... 136 moreCaused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :3	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:362)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$mapPartitionsAsRDD$a3ab3c4$1(BaseSparkCommitActionExecutor.java:272)	at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)	at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)	at org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1418)	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1482)	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1305)	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)	at org.apache.spark.scheduler.Task.run(Task.scala:131)	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.hudi.exception.HoodieAppendException: Failed while appending records to file:/var/folders/ym/8yjkm3n90kq8tk4gfmvk7y140000gn/T/junit2792173348364470678/.hoodie/metadata/record_index/.record-index-0009-0_00000000000000011.log.3_3-275-476	at org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:475)	at org.apache.hudi.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:439)	at org.apache.hudi.table.action.deltacommit.BaseSparkDeltaCommitActionExecutor.handleUpdate(BaseSparkDeltaCommitActionExecutor.java:90)	at org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:355)	... 28 moreCaused by: org.apache.hudi.exception.HoodieException: Writing multiple records with same key 1 not supported for org.apache.hudi.common.table.log.block.HoodieHFileDataBlock	at org.apache.hudi.common.table.log.block.HoodieHFileDataBlock.serializeRecords(HoodieHFileDataBlock.java:146)	at org.apache.hudi.common.table.log.block.HoodieDataBlock.getContentBytes(HoodieDataBlock.java:121)	at org.apache.hudi.common.table.log.HoodieLogFormatWriter.appendBlocks(HoodieLogFormatWriter.java:166)	at org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:467)	... 31 more
 {code}"	HUDI	Closed	3	1	9267	pull-request-available
13421269	Fix invisible writes(commits) during compaction (HoodieParquetRealtimeInputFormat)	"Suppose a compaction (with instant A) is going on, all writes related with the compaction (i.e., touch the file groups that are under compaction) will end up with timestamp A.

For current `HoodieParquetRealtimeInputFormat` implementation, even the writes complete, the records are invisible until the compaction complete.

The following pseudocode could reproduce the case

```
write 200 records and complete
scheduleCompaction
write 200 records and complete
read the table and only get 200 records
```

Note, the Spark read path is correct and will cover the corner cases during compaction. But the hive path (also presto) is wrong."	HUDI	Closed	2	1	9267	hudi-on-call, pull-request-available
13404496	Multi-writer w/ DeltaStreamer and Spark datasource does not work	"Multi-writer w/ DeltaStreamer and Spark datasource does not work

 

Subsequent commits in deltastreamer will fail.

 

Related issue: https://github.com/apache/hudi/issues/3598"	HUDI	Closed	1	4	9267	pull-request-available, sev:critical, user-support-issues
13334872	Adding Delete support to test suite framework	Add delete support to test suite framework	HUDI	Closed	3	4	9267	pull-request-available
13275111	Design Inline FileSystem which supports embedding any file format (parquet/avro/etc) 	Basically the log file should be capable of embedding any file format. In other words, if parquet is embedded, direct parquet reader should work on reading the content directly. 	HUDI	Closed	3	2	9267	pull-request-available
13437271	We automatically enable InProcessLockProvider and lazy rollbacks in spark datasoruce write if compaction configs are not set for MOR	"Sometime back, we added a fix to hudi, where in we automatically detect if any async table services are enabled and if no lock providers are configured, we automatically enable InProcessLockProvider, OCC and lazy rollbacks. This is a pre-requisite for enabling metadata table and hence we had put in this fix. 

 

This worked out well for COW, clustering. But for MOR, it was tricky, and we had to have explicit checks for below condition and auto enable it

if table type = MOR and if compaction is async -> enable InProcessLockProvider. 

bcoz, for COW there is no compaction, but for MOR, compaction has to be enabled. its a question of whether its inline or async. 

 

This all works out well, if user explicitly sets the compaction config as below
{code:java}
df.write.format(""hudi"").
     |   options(getQuickstartWriteConfigs).
     |   option(PRECOMBINE_FIELD_OPT_KEY, ""ts"").
     |   option(RECORDKEY_FIELD_OPT_KEY, ""uuid"").
     |   option(PARTITIONPATH_FIELD_OPT_KEY, ""partitionpath"").
     |   option(""hoodie.datasource.write.table.type"",""MERGE_ON_READ"").
     |   option(""hoodie.compact.inline"",""true"").
     |   option(""hoodie.compact.inline.max.delta.commits"",""2"").
     |   option(TABLE_NAME, tableName).
     |   mode(Append).
     |   save(basePath) {code}
 

So, we clearly detect its inline and do not enable InProcessLockProvier. 

Auto detection also works well w/ Deltastreamer code path, since we can clearly detect whether compaction is inline or async. for inline, Deltastreamer will explicitly set ""hoodie.compact.inline"" to ""true"".

 

But the tricky part is, with spark datasource, if user skips the compaction config altogether, we auto detect that its inline and go ahead and enable inProcessLockProvider. In addition, OCC and lazy rollbacks as well. So, this is a behavior change for a simple single writer coming from 0.10.0. 

 
{code:java}
df2.write.format(""hudi"").
     |   options(getQuickstartWriteConfigs).
     |   option(PRECOMBINE_FIELD_OPT_KEY, ""ts"").
     |   option(RECORDKEY_FIELD_OPT_KEY, ""uuid"").
     |   option(PARTITIONPATH_FIELD_OPT_KEY, ""partitionpath"").
     |   option(""hoodie.datasource.write.table.type"",""MERGE_ON_READ"").
     |   option(TABLE_NAME, tableName).
     |   mode(Append).
     |   save(basePath) {code}
 

Reason is that, as per code, default value for ""hoodie.compact.inline"" is ""false"". And so we deduce that, compaction is async if user does not explicitly set it.

 

We have to find a way to fix this. 

May be, in a production pipeline, its likely every write will have compaction configs set. I don't see why someone will have compaction configs set for few writes and not for others. But lets try to see if we can maintain the same behavior. 

 

 

 

 

 

 

 

 "	HUDI	Closed	1	1	9267	pull-request-available
13310250	Bulk Insert w/o converting to RDD	"Our bulk insert(not just bulk insert, all operations infact) does dataset to rdd conversion in HoodieSparkSqlWriter and our HoodieClient deals with JavaRDD<HoodieRecord>s. We are trying to see if we can improve our performance by avoiding the rdd conversion.  We will first start off w/ bulk insert and get end to end working before we decide if we wanna do this for other operations too after doing some perf analysis. 

 

On a high level, this is the idea

1. Dataset<Row> will be passed in all the way from spark sql writer to the storage writer. We do not convert to HoodieRecord at any point in time. 

2. We need to use [ParquetWriteSupport|[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala]] to write to Parquet as InternalRows.

3. So, gist of what we wanna do is, with the Dataset<Rows>s, sort by partition path and record keys, repartition by parallelism config, and do mapPartitions. Within MapPartitions, we will iterate through the Rows, encode to InternalRows and write to Parquet using the write support linked above. 

We first wanted to check if our strategy will actually improve the perf. So, I did a quick hack of just the mapPartition func in HoodieSparkSqlWriter just to see how the numbers look like. Check for operation ""bulk_insert_direct_parquet_write_support"" [here|#diff-5317f4121df875e406876f9f0f012fac]]. 

These are the numbers I got. (1) is existing hoodie bulk insert which does the rdd conversion to JavaRdd<HoodieRecords>. (2) is writing directly to parquet in spark. Code given below. (3) is the modified hoodie code i.e. operation bulk_insert_direct_parquet_write_support)

 
| |5M records 100 parallelism input size 2.5 GB|
|(1) Orig hoodie(unmodified)|169 secs. output size 2.7 GB|
|(2) Parquet |62 secs. output size 2.5 GB|
|(3) Modified hudi code. Direct Parquet Write |73 secs. output size 2.5 GB|

 

So, essentially our existing code for bulk insert is > 2x that of parquet. Our modified hudi code (i.e. operation bulk_insert_direct_parquet_write_support) is close to direct Parquet write in spark, which shows that our strategy should work. 

// This is the Parquet write in spark. (2) above. 

transformedDF.sort(*""partition""*, *""key""*)

.coalesce(parallelism)

 .write.format(*""parquet""*)

 .partitionBy(*""partition""*)

 .mode(saveMode)

 .save(*s""**$*outputPath*/**$*format*""*)

 "	HUDI	Closed	1	4	9267	pull-request-available
13442331	Allow arbitrary hoodie config overwrite for e2e pipeline	"we should be able to override any arbitrary hoodie configs for any of existing yaml and test property combination with integ test suite.

 

mostly it should work w/ --hoodie-config. but lets try it out and add support if not available. "	HUDI	Closed	3	3	9267	pull-request-available
13334843	Improving Hudi test suite framework to support proper validation and long running tests	Improve hudi test suite framework to support proper validation and long running tests. 	HUDI	Closed	3	4	9267	pull-request-available
13392640	Reduce CI run time for deltastreamer and bulk insert row writer tests	"Reduce CI run time for deltastreamer and bulk insert row writer tests

 

org.apache.hudi.utilities.functional.TestHoodieMultiTableDeltaStreamer
org.apache.hudi.spark3.internal.TestHoodieDataSourceInternalBatchWrite
org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer
org.apache.hudi.spark3.internal.TestHoodieBulkInsertDataInternalWriter"	HUDI	Resolved	3	6	9267	pull-request-available
13528292	PostWriteTermination Strategy does not shutdown the spark job completely	"When post write termination strategy is used, spark job is not fully shutdown. 
{code:java}
23/03/13 11:16:06 INFO DFSPathSelector: Root path => /tmp/input_dataset/ source limit => 30000000
23/03/13 11:16:06 INFO DeltaSync: No new data, source checkpoint has not changed. Nothing to commit. Old checkpoint=(Option{val=1678730775000}). New Checkpoint=(1678730775000)
23/03/13 11:16:06 INFO NoNewDataTerminationStrategy: Shutting down on continuous mode as there is no new data for 3
23/03/13 11:16:12 INFO HoodieAsyncService: Waiting for next instant up to 10 seconds
23/03/13 11:16:22 INFO HoodieAsyncService: Waiting for next instant up to 10 seconds
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 410
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 638
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 567
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 618
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 591
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 577
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 620
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 539
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 424
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 429
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 516
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 622
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 503
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 446
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 460
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 547
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 606
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 435
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 625
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 601
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 630
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 405
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 495
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 500
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 559
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 520
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 621
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 588
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 431
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 423
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 534
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 603
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 631
23/03/13 11:16:31 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.1.7:60400 in memory (size: 94.3 KB, free: 4.1 GB)
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 537
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 568
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 597
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 562
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 535
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 481
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 611
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 585
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 640
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 515
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 645
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 609
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 581
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 530
23/03/13 11:16:31 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 192.168.1.7:60400 in memory (size: 25.7 KB, free: 4.1 GB)
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 602
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 650
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 469
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 517
23/03/13 11:16:31 INFO ContextCleaner: Cleaned accumulator 542 {code}
Job is just stuck there. "	HUDI	Closed	2	1	9267	pull-request-available
13420306	CI broken with test failure in TestHiveSyncTool.dropPartitions	"CI broken with test failure in TestHiveSyncTool.dropPartitions

[https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/4856/logs/33]

[https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/4837/logs/33]

stacktace:
{code:java}
8312 [pool-7-thread-3] ERROR org.apache.hadoop.hive.metastore.HiveAlterHandler  - Failed to alter table testdb.test1
8315 [pool-7-thread-3] ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler  - InvalidOperationException(message:The following columns have types incompatible with the existing columns in their respective positions :
favorite_number)
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(MetaStoreUtils.java:633)
    at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:148)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3994)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3965)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
    at com.sun.proxy.$Proxy23.alter_table_with_environment_context(Unknown Source)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:11565)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:11549)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
    at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
    at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
    at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)8317 [main] ERROR org.apache.hudi.hive.ddl.HMSDDLExecutor  - Failed to update table for test1
InvalidOperationException(message:The following columns have types incompatible with the existing columns in their respective positions :
favorite_number)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59589)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:360)
    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:309)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
    at com.sun.proxy.$Proxy27.alter_table_with_environmentContext(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2303)
    at com.sun.proxy.$Proxy27.alter_table_with_environmentContext(Unknown Source)
    at org.apache.hudi.hive.ddl.HMSDDLExecutor.updateTableDefinition(HMSDDLExecutor.java:147)
    at org.apache.hudi.hive.HoodieHiveClient.updateTableDefinition(HoodieHiveClient.java:205)
    at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:252)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:184)
    at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:129)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:115)
    at org.apache.hudi.hive.TestHiveSyncTool.testDropPartition(TestHiveSyncTool.java:824)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:212)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:208)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:137)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:71)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
    at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:373)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
    at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1257)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1257)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:87)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:53)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:66)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:51)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:87)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:66)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:71)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
8382 [pool-7-thread-4] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.metastore.local does not exist
8383 [pool-7-thread-4] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
OK
OKorg.apache.hudi.exception.HoodieException: Got runtime exception when hive syncing test1    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:118)
    at org.apache.hudi.hive.TestHiveSyncTool.testDropPartition(TestHiveSyncTool.java:824)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:212)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:208)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:137)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:71)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
    at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:373)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
    at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1257)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1257)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:87)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:53)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:66)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:51)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:87)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:66)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:71)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.hudi.hive.HoodieHiveSyncException: Failed to update table for test1
    at org.apache.hudi.hive.ddl.HMSDDLExecutor.updateTableDefinition(HMSDDLExecutor.java:150)
    at org.apache.hudi.hive.HoodieHiveClient.updateTableDefinition(HoodieHiveClient.java:205)
    at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:252)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:184)
    at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:129)
    at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:115)
    ... 125 more
Caused by: InvalidOperationException(message:The following columns have types incompatible with the existing columns in their respective positions :
favorite_number)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59589)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result$alter_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:59575)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_with_environment_context_result.read(ThriftHiveMetastore.java:59517)
    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1689)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table_with_environment_context(ThriftHiveMetastore.java:1673)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:360)
    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:309)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
    at com.sun.proxy.$Proxy27.alter_table_with_environmentContext(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2303)
    at com.sun.proxy.$Proxy27.alter_table_with_environmentContext(Unknown Source)
    at org.apache.hudi.hive.ddl.HMSDDLExecutor.updateTableDefinition(HMSDDLExecutor.java:147)
    ... 130 more
 {code}"	HUDI	Closed	3	3	9267	pull-request-available
13377494	Make schema post processor's default as disabled	With default value [fix|https://github.com/apache/hudi/pull/2765], schema post processor is not required as mandatory. 	HUDI	Patch Available	3	4	9267	core-flow-ds, pull-request-available, sev:high, triaged
13435681	Fix testHoodieAsyncClusteringJob in TestHoodieDeltaStreamer	"{code:java}
[ERROR] Tests run: 106, Failures: 1, Errors: 1, Skipped: 16, Time elapsed: 2,239.169 s <<< FAILURE! - in JUnit Vintage
[ERROR] boolean).[2] false(testHoodieAsyncClusteringJob  Time elapsed: 15.362 s  <<< ERROR!
java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.hudi.exception.HoodieException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
    at java.util.concurrent.FutureTask.report(FutureTask.java:122)
    at java.util.concurrent.FutureTask.get(FutureTask.java:192)
    at org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer.deltaStreamerTestRunner(TestHoodieDeltaStreamer.java:763)
    at org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer.deltaStreamerTestRunner(TestHoodieDeltaStreamer.java:749)
    at org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer.deltaStreamerTestRunner(TestHoodieDeltaStreamer.java:767)
    at org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer.testHoodieAsyncClusteringJob(TestHoodieDeltaStreamer.java:971)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:212)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:208)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:137)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:71)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
    at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:440)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
    at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
    at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:87)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:53)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:66)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:51)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:87)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75)
    at org.junit.platform.runner.JUnitPlatform.run(JUnitPlatform.java:139)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
    at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
    at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
    at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
    at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
    at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
    at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.lang.RuntimeException: org.apache.hudi.exception.HoodieException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
    at org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer.lambda$deltaStreamerTestRunner$6(TestHoodieDeltaStreamer.java:758)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.lambda$sync$1(HoodieDeltaStreamer.java:184)
    at org.apache.hudi.common.util.Option.ifPresent(Option.java:97)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.sync(HoodieDeltaStreamer.java:179)
    at org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer.lambda$deltaStreamerTestRunner$6(TestHoodieDeltaStreamer.java:755)
    ... 5 more
Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
    at org.apache.hudi.async.HoodieAsyncService.waitForShutdown(HoodieAsyncService.java:103)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.lambda$sync$1(HoodieDeltaStreamer.java:182)
    ... 8 more
Caused by: org.apache.hudi.exception.HoodieException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer$DeltaSyncService.lambda$startService$0(HoodieDeltaStreamer.java:690)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
    ... 3 more
Caused by: org.apache.hudi.exception.HoodieWriteConflictException: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
    at org.apache.hudi.client.transaction.SimpleConcurrentFileWritesConflictResolutionStrategy.resolveConflict(SimpleConcurrentFileWritesConflictResolutionStrategy.java:102)
    at org.apache.hudi.client.utils.TransactionUtils.lambda$resolveWriteConflictIfAny$0(TransactionUtils.java:95)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:743)
    at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
    at org.apache.hudi.client.utils.TransactionUtils.resolveWriteConflictIfAny(TransactionUtils.java:89)
    at org.apache.hudi.client.utils.TransactionUtils.resolveWriteConflictIfAny(TransactionUtils.java:63)
    at org.apache.hudi.client.SparkRDDWriteClient.preCommit(SparkRDDWriteClient.java:476)
    at org.apache.hudi.client.BaseHoodieWriteClient.commitStats(BaseHoodieWriteClient.java:211)
    at org.apache.hudi.client.SparkRDDWriteClient.commit(SparkRDDWriteClient.java:122)
    at org.apache.hudi.utilities.deltastreamer.DeltaSync.writeToSink(DeltaSync.java:613)
    at org.apache.hudi.utilities.deltastreamer.DeltaSync.syncOnce(DeltaSync.java:327)
    at org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer$DeltaSyncService.lambda$startService$0(HoodieDeltaStreamer.java:656)
    ... 4 more
Caused by: java.util.ConcurrentModificationException: Cannot resolve conflicts for overlapping writes
    ... 17 more
{code}"	HUDI	Closed	1	1	9267	pull-request-available
13397051	Fix spark-sql docs with spark quick start guide	Fix spark-sql docs with spark quick start guide	HUDI	Resolved	3	4	9267	pull-request-available
13441277	Don't keep the last commit before the earliest commit to retain	"Don't keep the last commit before the earliest commit to retain

According to the document of {{{}hoodie.cleaner.commits.retained{}}}:

Number of commits to retain, without cleaning. This will be retained for num_of_commits * time_between_commits (scheduled). This also directly translates into how much data retention the table supports for incremental queries.
 
We only need to keep the number of commit configured through parameters {{{}hoodie.cleaner.commits.retained{}}}.
And the commit retained by clean is completed.This ensures that “This will be retained for num_of_commits * time_between_commits” in the document.

So we don't need to keep the last commit before the earliest commit to retain,If we want to keep more versions, we can increase the parameters {{hoodie.cleaner.commits.retained}}"	HUDI	Patch Available	2	4	9267	pull-request-available
13523090	Col drop config is not honored when schema validation is disabled	"[https://dev.azure.com/apache-hudi-ci-org/785b6ef4-2f42-4a89-8f0e-5f0d7039a0cc/_apis/build/builds/14920/logs/38]

 

looks like col drop flag is tightly coupled w/ schema validation flag. only if schema validation is enabled, col drop flag is honored. if not, it may succeed. 

 

for eg, 

if table schema is col1, col2, col3

and new incoming schema is col1, col2

col drop config is set to false (which means col drop should not be supported), and schema validation is set to false, commit will succeed. Expectation is, commit should fail for this new batch. 

 

 

 "	HUDI	Closed	1	1	9267	pull-request-available
13424542	Investigate integ test failures with spark yamls	"We are having issues around integ test test failures w/ spark data source yamls. We want to get that fixed. 

 "	HUDI	Closed	3	3	9267	pull-request-available
13360957	Implement Spark Datasource option to read hudi configs from properties file	"Provide config option like ""hoodie.datasource.props.file"" to load all the options from a file.

 

GH: https://github.com/apache/hudi/issues/2605"	HUDI	Resolved	3	4	9267	sev:high, user-support-issues
