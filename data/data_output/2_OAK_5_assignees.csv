id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13038418	Test failure: standalone.RepositoryBootIT.repositoryLogin	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1386 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1386|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1386/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1386/console]"	OAK	Closed	3	1	3114	test-failure
13017105	Support bundling of nodes present in version store	"Bundling logic would not work for node structures which are present in versionstore i.e. nodes stored under /jcr:system/jcr:versionStorage as the nodes there always have type {{nt:frozenNode}}. So any node structure which gets version would not get benefit of bundling

Currently bundling logic looks for {{jcr:primaryType}} and {{jcr:mixinTypes}} for determining type information. To support bundling for nodes stored in version store we should also look for 

* jcr:frozenPrimaryType
* jcr:frozenMixinTypes

These properties contains the type information of original node stored which got versioned"	OAK	Closed	3	4	3114	bundling
13025009	OakDirectory should not save dir listing if no change is done	"[~alex.parvulescu] noted that OakDirectory saves the directory listing even if no actual change happened in the directory. Only change that happens is the order of entries in set. 

In normal cases LuceneIndexEditor avoids initializing the IndexWriter if there is no change. However it can happen that when any node gets deleted the editor performs a delete operation. It can happen that tree being deleted is not indexed but still editor would do this as it cannot determine that easily. This would lead to OakDirectory being closed without any change and thus can lead save of dir listing with just change in order of entries"	OAK	Closed	4	4	3114	performance
13014980	Persistent cache should not cache those paths which are covered by DocumentNodeStateCache	With OAK-4180 its possible to use a SegmentNodeStore as secondary store and thus like a cache for certain set of path. In such kind of setup persistent cache should not cache those NodeStates which are covered by DocumentNodeStateCache	OAK	Closed	3	4	3114	secondary-nodestore
13057878	Document Metrics related classes and interfaces	"The Metrics related classes and interfaces in {{org.apache.jackrabbit.oak.stats}} and {{org.apache.jackrabbit.oak.plugins.metric}} are largely undocumented. Specifically it is not immediately how they should be used, how a new {{Stats}} instance should be added, what the effect this would have and how it would (or would) not be exposed (e.g. via JMX). 

"	OAK	Open	3	4	3114	documentation, technical_debt
12834183	Remove code related to directmemory for off heap caching	"DocumentNodeStore has some code related to off heap which makes use of Apache Directmemory (OAK-891). This feature was not much used and PersistentCache made this feature obsolete.

Recently it was mentioned on Directmemory that there is not much activity going on [1] in that project and it might be referred to attic. In light of that we should remove this feature from Oak

[1] http://markmail.org/thread/atia2ecaa2mugmjx"	OAK	Closed	3	3	3114	technical_debt
13014175	Enable configuring QueryEngineSettings via OSGi config	"Oak QueryEngine exposes few settings options via {{QueryEngineSettings}}. Currently they can be configured via

# System properties
# JMX - The settings are not persistent 

We should have a way to configure them via OSGi also. A simple option can be to have a OSGi component which obtains a reference to {{QueryEngineSettingsMBean}} and then modifies the config upon activation"	OAK	Closed	4	4	3114	docs-impacting
12833421	ReferenceEditor newIds consuming lots of memory during migration	"In a large migration its seen that {{ReferenceEditor}} {{newIds}} can consume lots of memory as it records all the uuid property. This system has 33 million uuid index and the set was consuming ~1.5G of memory

We should look into ways such that it does not have to maintain such a big in memory state"	OAK	Closed	4	4	3114	resilience
12830964	SegmentBlob does not return blobId for contentIdentity	{{SegmentBlob}} currently returns recordId for {{contentIdentity}} even when an external DataStore is configured. Given that recordId is not stable it would be better to return the blobId as part of  {{contentIdentity}} if external DataStore is configured	OAK	Closed	4	1	3114	resilience
12922590	SessionMBean not getting registered due to MalformedObjectNameException	"Due to changes done in OAK-3477 SessionMBean is not getting registered as it contains ',' in the ObjectName. Unfortunately the exception thrown gets lost and this did not got detected so far

{noformat}
javax.management.MalformedObjectNameException: Invalid character in value: `,'
	at javax.management.ObjectName.checkValue(ObjectName.java:1009)
	at javax.management.ObjectName.construct(ObjectName.java:725)
	at javax.management.ObjectName.<init>(ObjectName.java:1425)
	at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardUtils.registerMBean(WhiteboardUtils.java:79)
	at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardUtils.registerMBean(WhiteboardUtils.java:68)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$RegistrationTask.run(RepositoryImpl.java:523)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
{noformat}

The name passed for ObjectName is 
{code}
{name=admin@session-11@Dec 17, 2015 9:57:11 AM, type=SessionStatistics}
{code}

"	OAK	Closed	4	1	3114	regresion
12844356	Cache recently extracted text to avoid duplicate extraction	"It can happen that text can be extracted from same binary multiple times in a given indexing cycle. This can happen due to 2 reasons

# Multiple Lucene indexes indexing same node - A system might have multiple Lucene indexes e.g. a global Lucene index and an index for specific nodeType. In a given indexing cycle same file would be picked up by both index definition and both would extract same text
# Aggregation - With Index time aggregation same file get picked up multiple times due to aggregation rules

To avoid the wasted effort for duplicate text extraction from same file in a given indexing cycle it would be better to have an expiring cache which can hold on to extracted text content for some time. The cache should have following features
# Limit on total size
# Way to expire the content using [Timed Evicition|https://code.google.com/p/guava-libraries/wiki/CachesExplained#Timed_Eviction] - As chances of same file getting picked up are high only for a given indexing cycle it would be better to expire the cache entries after some time to avoid hogging memory unnecessarily 

Such a cache would provide following benefit
# Avoid duplicate text extraction - Text extraction is costly and has to be minimized on critical path of {{indexEditor}}
# Avoid expensive IO specially if binary content are to be fetched from a remote {{BlobStore}}"	OAK	Closed	3	4	3114	performance
12613758	OSGi related dependencies should be set to provided scoped and not marked as optional	"Currently the OSGi related dependencies org.osgi.core and org.osgi.compendium are marked as optional. Due to this packages under org.osgi.* are marked as optional which is not correct. 

As such dependencies are already marked as provided scope they would not be included as part of transient dependencies. For more details refer to [1]

Fix: The optional flag should be removed

[1] http://markmail.org/thread/njukyten6fdipts3"	OAK	Closed	4	1	3114	osgi
13014649	Config option to disable specific bundling config	"With OAK-4975 Oak would be shipping some default bundling config. An application might want to disable such bundling and for those cases we need to support some config option to disable bundling for specific nodetypes.

*Proposal*

Have a boolean property {{disabled}} on bundling config for specific nodetype to indication that this bundling config is not to be used
"	OAK	Closed	4	4	3114	bundling, docs-impacting
13058163	Support path exclusion in secondary nodestore	"Secondary NodeStore feature (OAK-4180) for now currently supports path inclusion. It would be useful to have support for path exclusion also.

Using this a user can can include all content  under / but exclude /oak:index/uuid/:index entries."	OAK	Open	3	4	3114	secondary-nodestore
12840750	DocumentRootBuilder: revisit update.limit default	"update.limit decides whether a commit is persisted using a branch or not. The default is 10000 (and can be overridden using the system property).

A typical call pattern in JCR is to persist batches of ~1024 nodes. These translate to more than 10000 changes (see PackageImportIT), due to JCR properties, and also indexing commit hooks.

"	OAK	Closed	1	4	3114	performance
13033612	Test failure: BasicServerTest.testServerOk() Address already in use	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1363 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1363|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1363/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1363/console]"	OAK	Closed	3	1	3114	test-failure
13108896	Implement support for disabling indexes which are replaced with newer index	"For upgrade case in many applications older index type is set to {{disabled}} when new index is provisioned. If the new index is async then it would take some time for reindex and till then any query which used to make use of old index would end up traversing the repository

To avoid such a scenario we should only mark older index as ""disabled"" only if the newer index is reindex. "	OAK	Closed	3	2	3114	docs-impacting
12824211	Refactor the optimize logic regarding path include and exclude to avoid duplication	"{{ObservationManagerImpl}} has a optimize method which process the list of includes and excludes and removes redundant clauses. That logic is now also being used in index filtering (OAK-2599) and is getting duplicated.

Going forward we need to refactor this logic so that both places can use it without copying. Possibly making it part of PathUtils

[~mduerig] Also suggested to further optimize
bq. Also PathFilter#optimise could be further optimised by removing entries that subsume each other (e.g. including /a/b, /a is the same as including (/a. "	OAK	Closed	4	4	3114	technical_debt
12862325	Boosting fields not working as expected	"When the boost support was added the intention was to support a usecase like 

{quote}
For the fulltext search on a node where the fulltext content is derived from multiple field it should be possible to boost specific text contributed by individual field. Meaning that if a title field is boosted more than description, the title (part) in the fulltext field will mean more than the description (part) in the fulltext field.
{quote}

This would enable a user to perform a search like _/jcr:root/content/geometrixx-outdoors/en//element(*, cq:Page)\[jcr:contains(., 'Keyword')\]_ and get a result where pages having 'Keyword' in title come above in search result compared to those where Keyword is found in description.

Current implementation just sets the boost while add the field value to fulltext field with the intention that Lucene would use the boost as explained above. However it does not work like that and boost value gets multiplies with other field and hence boosting does not work as expected"	OAK	Closed	3	1	3114	doc-impacting
13099592	Provide list of all bundled nodes within a given DocumentNodeState	"For OAK-6353 we need to know all bundled nodestate in a given parent. For this purpose we should provide following method in DocumentNodeState

{code}
public Iterable<DocumentNodeState> getBundledNodesStates() {
{code}"	OAK	Closed	4	4	3114	bundling
12729658	Add command to dump Lucene index in Oak Console	"Add a command in Oak Run Console to dump lucene index and also provide stats related to Lucene index

"	OAK	Closed	4	2	3114	console
13035392	Test failure: RepositoryBootIT.repositoryLogin	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1369 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1369|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1369/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1369/console]"	OAK	Closed	3	1	3114	test-failure, ubuntu
12845482	AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logic	"At times the CopyOnWrite reports following exception

{noformat}
15.07.2015 14:20:35.930 *WARN* [pool-58-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate The async index update failed
org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:204)
	at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:219)
	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:366)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:311)
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105)
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: _2s7.fdt
	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:261)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory$COWLocalFileReference.fileLength(IndexCopier.java:837)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory.fileLength(IndexCopier.java:607)
	at org.apache.lucene.index.SegmentCommitInfo.sizeInBytes(SegmentCommitInfo.java:141)
	at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:529)
	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502)
	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:508)
	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:618)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3147)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3123)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:988)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:932)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:894)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.closeWriter(LuceneIndexEditorContext.java:192)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:202)
	... 10 common frames omitted
{noformat}"	OAK	Closed	3	1	3114	resilience
13013458	Review the support for wildcards in bundling pattern	"Bundling pattern currently supports wild card pattern. This makes it powerful but at same time can cause issue if it misconfigured. 

We should review this aspect before 1.6 release to determine if this feature needs to be exposed or not. "	OAK	Open	3	3	3114	bundling
12818550	Misleading warn message about local copy size different than remote copy in oak-lucene with copyOnRead enabled	"At times following warning is seen in logs

{noformat}
31.03.2015 14:04:57.610 *WARN* [pool-6-thread-7] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _0.cfs in NIOFSDirectory@/path/to/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/path/to/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 1040384 differs from remote 1958385. Content would be read from remote file only
{noformat}

The file length check provides a weak check around index file consistency. In some cases this warning is misleading. For e.g. 

# Index version Rev1 - Task submitted to copy index file F1 
# Index updated to Rev2 - Directory bound to Rev1 is closed
# Read is performed with Rev2 for F1 - Here as the file would be locally created the size would be different as the copying is in progress

In such a case the logic should ensure that once copy is done the local file gets used"	OAK	Closed	4	4	3114	resilience
12819638	Oak instance does not close the executors created upon ContentRepository creation	"Oak.createContentRepository does not closes the executors it creates upon close. It should close the executor if that is created by itself and not passed by outside

Also see recent [thread|http://markmail.org/thread/rryydj7vpua5qbub]."	OAK	Closed	4	1	3114	CI, Jenkins
12829452	Log stats around time spent in extracting text from binaries	"For issues like OAK-2787 it would helpful if we collect some stats around how much time is spent in extracting text from binaries.

For that purpose I would like add some logging around text extraction"	OAK	Closed	4	4	3114	tooling
12827624	Log NodePropBundle id for which no bundle is found	"At times in migration following exception is seen

{noformat}
Caused by: java.lang.NullPointerException
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.createProperties(JackrabbitNodeState.java:311)
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.<init>(JackrabbitNodeState.java:149)
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.getChildNodeEntries(JackrabbitNodeState.java:255)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1014)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1015)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1015)
{noformat}

This would happen if the NodePropBundle is null for a given id. It would be good to add a NPE check in loader itself"	OAK	Closed	4	4	3114	resilience
12783611	Repository Upgrade could shut down the source repository early	"I noticed that during the upgrade we can distinguish 2 phases: first copying the data from the source, then applying all the Editors (indexes and co.).
After phase 1 is done the repository upgrader could shut down the old repo to allow clearing some memory resources which might be used for the second phase."	OAK	Closed	3	4	3114	resilience
12948663	Cached lucene index gets corrupted in case of unclean shutdown and journal rollback in SegmentNodeStore	"Currently Oak Lucene support would copy index files to local file system as part of CopyOnRead feature. In one of the setup it has been observed that index logic was failing with following error

{noformat}
04.02.2016 17:47:52.391 *WARN* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier [/oak:index/lucene] Found local copy for _2ala.cfs in MMapDirectory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 9320 differs from remote 3714150. Content would be read from remote file only
04.02.2016 17:47:52.399 *WARN* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier [/oak:index/lucene] Found local copy for segments_28je in MMapDirectory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 1214 differs from remote 1175. Content would be read from remote file only
04.02.2016 17:47:52.491 *ERROR* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker Failed to open Lucene index at /oak:index/lucene
org.apache.lucene.index.CorruptIndexException: codec header mismatch: actual header=1953790076 vs expected header=1071082519 (resource: SlicedIndexInput(SlicedIndexInput(_2ala.fnm in _2ala.cfs) in _2ala.cfs slice=8810:9320))
	at org.apache.lucene.codecs.CodecUtil.checkHeader(CodecUtil.java:128)
	at org.apache.lucene.codecs.lucene46.Lucene46FieldInfosReader.read(Lucene46FieldInfosReader.java:56)
	at org.apache.lucene.index.SegmentReader.readFieldInfos(SegmentReader.java:215)
{noformat}

Here size of __2ala.cfs_ differed from remote copy and possible other index file may have same size but different content. Comparing the modified time of the files with those in Oak it can be seen that one of file system was older than one in Oak

{noformat}

_2alr.cfs={name=_2alr.cfs, size=1152402, sizeStr=1.2 MB, modified=Thu Feb 04 17:52:31 GMT 2016, osModified=Feb 4 17:52, osSize=1152402, mismatch=false}
_2ala.cfe={name=_2ala.cfe, size=224, sizeStr=224 B, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=224, mismatch=false}
_2ala.si={name=_2ala.si, size=252, sizeStr=252 B, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=252, mismatch=false}
_2ala.cfs={name=_2ala.cfs, size=3714150, sizeStr=3.7 MB, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=9320, mismatch=true}
_14u3_29.del={name=_14u3_29.del, size=1244036, sizeStr=1.2 MB, modified=Thu Feb 04 16:37:35 GMT 2016, osModified=Feb 4 16:37, osSize=1244036, mismatch=false}
_2akw.si={name=_2akw.si, size=252, sizeStr=252 B, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=252, mismatch=false}
_2akw.cfe={name=_2akw.cfe, size=224, sizeStr=224 B, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=224, mismatch=false}
_2akw.cfs={name=_2akw.cfs, size=4952761, sizeStr=5.0 MB, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=4952761, mismatch=false}
{noformat}

And on same setup the system did saw a rollback in segment node store 
{noformat}

-rw-rw-r--. 1 crx crx  25961984 Feb  4 16:47 data01357a.tar
-rw-rw-r--. 1 crx crx  24385536 Feb  4 16:41 data01357a.tar.bak
-rw-rw-r--. 1 crx crx    359936 Feb  4 17:18 data01358a.tar
-rw-rw-r--. 1 crx crx    345088 Feb  4 17:17 data01358a.tar.bak
-rw-rw-r--. 1 crx crx  70582272 Feb  4 18:35 data01359a.tar
-rw-rw-r--. 1 crx crx  66359296 Feb  4 18:33 data01359a.tar.bak
-rw-rw-r--. 1 crx crx    282112 Feb  4 18:46 data01360a.tar
-rw-rw-r--. 1 crx crx    236544 Feb  4 18:45 data01360a.tar.bak
-rw-rw-r--. 1 crx crx    138240 Feb  4 18:56 data01361a.tar
{noformat}

So one possible cause is that 
# At some time earlier to 17:17 lucene index got updated and __2ala.cfs_ got created. 
# Post update the head revision in Segment store was updated but the revision yet to made it to journal log
# Lucene CopyOnRead logic got event for the change and copied the file
# System crashed and hence journal did not got updated
# System restarted and per last entry in journal system suffered with some ""data loss"" and hence index checkpoint also moved back
# As checkpoint got reverted index started at earlier state and hence created a file with same name __2ala.cfs_ 
# CopyOnRead detected file length change and logged a warning routing call to remote
# However other files like _2ala.si, _2ala.cfe which were created in same commit had same size but likely different content which later cause lucene query to start failing

In such a case a restart after cleaning the existing index content would have brought back the system to normal state.

So as a fix we would need to come up with some sanity check at time of system startup"	OAK	Closed	2	1	3114	resilience
12831661	DataStoreBlobStore should expose a buffer input stream for getInputStream call	"DataStoreBlobStore directly exposes the InputStream from the wrapped DataStore. In most cases underlying DataStore exposes a LazyFileInputStream [0] which is not buffered.

For performance reason the stream finally exposed at the BlobStore layer should be buffered one. See [1] for the discussion

[1] http://markmail.org/thread/xi4isnzw57vphcsq
[0]
https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-data/src/main/java/org/apache/jackrabbit/core/data/LazyFileInputStream.java#L102 
"	OAK	Closed	4	4	3114	performance
12821795	Remove Nullable annotation in Predicates of BackgroundObserver	"{code}
@Override
            public int getLocalEventCount() {
                return size(filter(queue, new Predicate<ContentChange>() {
                    @Override
                    public boolean apply(@Nullable ContentChange input) {
                        return input.info != null;
                    }
                }));
            }

            @Override
            public int getExternalEventCount() {
                return size(filter(queue, new Predicate<ContentChange>() {
                    @Override
                    public boolean apply(@Nullable ContentChange input) {
                        return input.info == null;
                    }
                }));
            }
{code}

both methods should probably check for {{input}} being null before accessing {{input.info}}"	OAK	Closed	3	1	3114	technical_debt
12706142	FileDataStore inUse map causes contention in concurrent env	"JR2 FileDataStore#inUseMap [1] is currently a synchronized map and that at times causes contention concurrent env. This map is used for supporting the Blob GC logic for JR2. 

With Oak this map content is not used. As a fix we can either

# Set inUseMap to a Guava Cache Map which has weak keys and value
# Set inUseMap to a no op map where all put calls are ignored
# Modify FDS to disable use of inUseMap or make {{usesIdentifier}} protected

#3 would be a proper fix and #2 can be used as temp workaround untill FDS gets fixed

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-data/src/main/java/org/apache/jackrabbit/core/data/FileDataStore.java#L118"	OAK	Closed	4	4	3114	concurrency
12787023	Troublesome AbstractTree.toString	"the default {{toString}} for all tree implementations calculates a string containing the path, the toString of all properties as well as the names of all child tree... this is prone to cause troubles in case for trees that have plenty of properties and children.

i would strongly recommend to review this and make the toString of trees both meaningful and cheap."	OAK	Closed	3	4	3114	technical_debt
12958002	Use another NodeStore as a local cache for a remote Document store	"DocumentNodeStore makes use of persistent cache to speed up its processing and save on making remote calls for data already present in cache

In addition to that we can look into make use of Segment NodeStore as kind of ""local copy"" for certain paths in repository and route calls to it if possible. As part of this task I would like to prototype such an approach. At high level it would work as below

# At start bootstrap the setup and shutdown it down
# Use a modified ""sidegrade"" and copy over the NodeStats from Document store to Segment store. In such a copy we also store some Document specific properties like {{readRevision}} and {{lastRevision}} as hidden property in Segment NodeStates
# In DocumentNodeStore we refactor the current code to extract a 
## {{AbstractDocumentNodeState}} - Abase class which has some logic move out from {{DocumentNodeState}}
## {{SegmentDocumentNodeState}} extends above and delegate calls to a wrapped {{SegmentNodeState}}
## {{DocumentNodeState}} would also extend {{AbstractDocumentNodeState}} and hence delegate to some calls to parent. In this when a call comes for {{getChildNode}} it can check if that can be served by a local copy of {{SegmentNodeStore}} for given {{rootRevision}} then it delegates to that
# For update plan is to make use of {{Observer}} which listens to changes and updates the local copy for certain configured paths. 
## Key aspect to address here is handle the restart case where in a cluster a specific node restarts after some time then how it refreshes itself there

*Usage*
Following 2 OSGi configs would need to be seed

* org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.config
{noformat}
secondary=B""true""
{noformat}
* org.apache.jackrabbit.oak.plugins.document.secondary.SecondaryStoreCacheService.config
{noformat}
includedPaths=[ \
  ""/"",
  ]
{noformat}

With these settings if DocumentNodeStoreService gets started it would pickup the cache and use it. Change {{includedPaths}} depending on paths in repository which you want to include in secondary store.

*Feature Docs*
http://jackrabbit.apache.org/oak/docs/nodestore/document/secondary-store.html"	OAK	Closed	3	2	3114	secondary-nodestore
12737509	MBean to provide consolidated cache stats 	Currently DocumentNodeStore has 5 different types of caches and each register there own MBean. To get a better understanding of the overall cache usage it would be good to have a {{ConsolidatedCacheStatsMBean}} which depicts all the stats in tabular form	OAK	Closed	4	4	3114	monitoring
12773538	Flag Document having many children	"Current DocumentMK logic while performing a diff for child nodes works as below

# Get children for _before_ revision upto MANY_CHILDREN_THRESHOLD (which defaults to 50). Further note that current logic of fetching children nodes also add children {{NodeDocument}} to {{Document}} cache and also reads the complete Document for those children
# Get children for _after_ revision with limits as above
# If the child list is complete then it does a direct diff on the fetched children
# if the list is not complete i.e. number of children are more than the threshold then it for a query based diff (also see OAK-1970)

So in those cases where number of children are large then all work done in #1 above is wasted and should be avoided. To do that we can mark those parent nodes which have many children via special flag like {{_manyChildren}}. One such nodes are marked the diff logic can check for the flag and skip the work done in #1

This is kind of similar to way we mark nodes which have at least one child (OAK-1117)

"	OAK	Resolved	3	4	3114	performance
12819945	Change default cache distribution ratio if persistent cache is enabled	"By default the cache memory in DocumentNodeStore is distributed in following ratio

* nodeCache - 25%
* childrenCache - 10%
* docChildrenCache - 3%
* diffCache - 5%
* documentCache - Is given the rest i.e. 57%

However off late we have found that with persistent cache enabled we can lower the cache allocated to Document cache. That would reduce the time spent in invalidating cache entries in periodic reads. So far we are using following ration in few setup and that is turning out well

* nodeCachePercentage=35
* childrenCachePercentage=20
* diffCachePercentage=30
* docChildrenCachePercentage=10
* documentCache - Is given the rest i.e. 5%

We should use the above distribution by default if the persistent cache is found to be enabled
"	OAK	Resolved	3	4	3114	performance
12979401	CI failing on branches due to unknown fixture SEGMENT_TAR	"These failures are caused by adding the SEGMENT_TAR fixture to the matrix. That one doesn't exit in the branches thus the {{IllegalArgumentException}} ""No enum constant"".

See discussion http://markmail.org/message/oaptnvco5y2a4rjk"	OAK	Closed	3	1	3114	CI, build, jenkins
12820275	Use non unique PathCursor in LucenePropertyIndex	"{{LucenePropertyIndex}} currently uses unique PathCursor [1] due to which the cursor would maintain an in memory set of visited path. This might grow big if result size is big and cursor is traversed completely.

As with current impl the path would not be duplicated we can avoid using unique cursor

[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java#L1153-1154"	OAK	Closed	4	4	3114	resilience
12916181	Partial re-index from last known good state	"ATM indexes break (by whatever circumstances) users need to perform a full re-index. Depending on the size off the repository this can take a long time.
If the user knows that the indexes were in a good state at a certain revision in the past then it would be very useful, if the user could trigger a ""partial"" re-index where only the content added after a certain revision was updated in the index."	OAK	Open	3	2	3114	resilience
12989714	Specify thread pool name which should be used by Async Indexing task	"While running Oak in Sling we rely on Sling Scheduler to ensure that async indexing task are run on leader (OAK-1246) with specified frequency. 

Be default Sling Scheduler uses a default pool for managing all tasks. It can happen that number of task can be quite hight which can then lead to default thread pool getting exhausted and that causes async indexing to get delayed.

To ensure that async indexing is not affected by such scenarios we should make use of a dedicated thread pool. This is now supported by SLING-5831"	OAK	Closed	4	4	3114	docs-impacting
12975788	Optimize RevisionVector methods	"{{RevisionVector}} is used in very critical paths and we should look into optimzing some of its critical method

"	OAK	Closed	4	4	3114	performance
12822289	Improve system resilience in case of index corruption	"I have had issues in cases when the async Lucene index had gotten corrupted. With this index unusable the only option was to re-index.  The problem however was that there were ongoing queries relying on this index even during re-indexing. Because the original index was corrupted these queries led to further load on the system (traversals afair).

I wonder if we could improve the system resilience in such situations.
One thing I could think of: could we maybe fallback to the last known non-corrupted index state while the re-index is running? This would at least take off the load due to new incoming queries."	OAK	Open	3	4	3114	resilience
13013457	Review the security aspect of bundling configuration	"The config for node bundling feature in DocumentNodeStore is currently stored under {{jcr:system/rep:documentStore/bundlor}}. This task is meant to 

* Review the access control aspect - This config should be only updatetable by system admin
* Config under here should be writeable via JCR api"	OAK	Open	3	3	3114	bundling
12960635	Provide a way to abort an async indexing run	"In some cases where a user is tweaking the indexing config it can happen that he saves the config mid way which triggers a long indexing run. Currently there is no easy way to abort such a run and only way to avoid wasting time in the long indexing cycle is to shut down the system.

For such cases it would be good to provide an ""abort"" operation as part of {{IndexStatsMBean}} which user can invoke to abort any run safely and cleanly"	OAK	Closed	3	4	3114	docs-impacting
12990274	Define oak:Resource nodetype as non referenceable alternative to nt:resource	"In most cases where code uses JcrUtils.putFile [1] it leads to
creation of below content structure

{noformat}
+ foo.jpg (nt:file)
   + jcr:content (nt:resource)
       - jcr:data
{noformat}

Due to usage of nt:resource each nt:file node creates a entry in uuid
index as nt:resource is referenceable. So if a system has 1M
nt:file nodes then we would have 1M entries in /oak:index/uuid as in
most cases the files are created via [1] and hence all such files are
referenceable

The nodetype defn for nt:file does not mandate that the
requirement for jcr:content being nt:resource. To support such non referenceable files we would define a new nodeType similar to nt:resource but which is non referenceable.

See [2] for related discussion

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-jcr-commons/src/main/java/org/apache/jackrabbit/commons/JcrUtils.java#L1062
[2] http://jackrabbit-oak.markmail.org/thread/qicpzm5ltnzfsd42"	OAK	Closed	3	4	3114	docs-impacting
12751610	CopyOnWriteDirectory implementation for Lucene for use in indexing	"Currently a Lucene index when is written directly to OakDirectory. For reindex case it might happen that Lucene merge policy read the written index files again and then perform a sgement merge. This might have lower performance when OakDirectroy is writing to remote storage.

Instead of that we can implement a CopyOnWriteDirectory on similar lines to  OAK-1724 where CopyOnReadDirectory support copies the  index locally for faster access. 

At high level flow would be

# While writing index the index file is first written to local directory
# Any write is done locally and once a file is written its written asynchronously to OakDirectory
# When IndexWriter is closed it would wait untill all the write is completed

This needs to be benchmarked with existing reindex timings to see it its actually beneficial"	OAK	Closed	3	2	3114	docs-impacting, performance
12818886	IndexCopier fails to delete older index directory upon reindex	"{{IndexCopier}} tries to remove the older index directory incase of reindex. This might fails on platform like Windows if the files are still memory mapped or are locked.

For deleting directories we would need to take similar approach like being done with deleting old index files i.e. do retries later.

Due to this following test fails on Windows (Per [~julian.reschke@gmx.de] )

{noformat}
Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.07 sec <<< FAILURE!
deleteOldPostReindex(org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest)  Time elapsed: 0.02 sec  <<< FAILURE!
java.lang.AssertionError: Old index directory should have been removed
        at org.junit.Assert.fail(Assert.java:93)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertFalse(Assert.java:68)
        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest.deleteOldPostReindex(IndexCopierTest.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
{noformat}"	OAK	Closed	4	1	3114	resilience
13033355	Skip processing of queued changes if async index update is detected in ExternalIndexObserver	"ExternalIndexObserver is currently backed by a queue (its wrapped in BackgroundObserver). Currently it processed the changes one by one as received from the queue. If this processing takes long time then its possible that it would lag behind the async indexing cycle.

So ExternalIndexObserver may be busy indexing changes from [r1-r2] but async indexing is already done indexing changes upto r3 (r3 > r2) and IndexTracker would move to newer index version. In such case work done by ExternalIndexObserver is wasted. 

This can be optimized by ensuring that ExternalIndexObserver can see the lastIndexTo of :async as per latest entry in queue. If that is newer than one its processing then it can skip processing the queue entry and thus free up space in queue"	OAK	Open	3	4	3114	performance
13032769	Test failure: TomcatIT.testTomcat()	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1357 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1357|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1357/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1357/console]"	OAK	Closed	3	6	3114	test-failure, ubuntu
12652187	Enable stats for various caches used in Oak by default	"To get a better picture around the usage of cache it would be helpful to enable the [statistics|http://code.google.com/p/guava-libraries/wiki/CachesExplained#Statistics] for various caches used in Oak

{code:java}
nodeCache = CacheBuilder.newBuilder()
                        .weigher(...)
                        .maximumWeight(...)
                        .recordStats()
                        .build();
{code}

Once enabled it allows to get stats like below
{noformat}
CacheStats{hitCount=763322, missCount=51333, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=3496}
{noformat}

As stats collection adds a very minor overhead we can look into making this setting configurable. 

Untill we expose the stats via JMX one can extract the value in Sling env via approach mentioned in [this gist|https://gist.github.com/chetanmeh/5748650]"	OAK	Closed	4	7	3114	monitoring
13096992	Lucene index: include/exclude key pattern list	"Similar to OAK-4637 but for lucene indexes

In some cases, property indexes contain many nodes, and updating them can be slow. Right now we have filters for node and mixin types, path (include and exclude). 

An include and exclude list of values (patterns) would be useful. For example the property ""status"", if we only ever run queries with the condition ""status = 'ACTIVE'"", then nodes with status INACTIVE and DONE don't need to be indexed."	OAK	Closed	3	4	3114	docs-impacting
12715494	Document Oak Clustering	"the 'clustering' page in our oak documentation is currently an empty placeholder and it would be great if there would be some initial pointers.

[~chetanm], [~mreutegg], what do you think?"	OAK	Open	3	3	3114	documentation
13018193	Journal diff not working for changes in bundled node	For changes in bundled nodes diff is not reporting change in properties of bundled node	OAK	Closed	3	1	3114	bundling
12823779	Save Lucene directory listing as array property	"OakDirectory has to at times perform directory listing specially at the time of opening of index. With DocumentNodeStore such listing of child nodes ""might"" be slow if there are lots more deleted nodes and GC has not cleared them so far (due to OAK-1557). 

As seen in OAK-2808 Lucene might be creating and deleting lot more files. To speed up such lookup one OakDirectory can save the listing of child nodes as an array property once the writer is closed. "	OAK	Closed	3	4	3114	performance
12832953	oak-jcr bundle should be usable as a standalone bundle	Currently oak-jcr bundle needs to be embedded within some other bundle if the Oak needs to be properly configured in OSGi env. Need to revisit this aspect and see what needs to be done to enable Oak to be properly configured without requiring the oak-jcr bundle to be embedded in the repo	OAK	Closed	3	4	3114	modularization, osgi, technical_debt
12828314	CopyOnReadDirectory mode might delete a valid local file upon close	"{{CopyOnReadDirectory}} currently deletes local files which are not found in remote upon close. The list of remote file is fixed for a given revision however list of local files may vary. 

{{IndexTracker}} opens a new {{IndexNode}} upon update before closing the older one. When CopyOnRead is enabled it can happen that same local directory might be in use by two wrapper directories at the same time. 

This introduces a race condition in {{removeDeletedFiles}} method as by the time it is invoked a newer wrapped directory might have started adding new files so those files would get included in the listing done for local directory and hence cause them to be deleted as they would not be found in remote directory which is pinned to older revision. Leading to following exception

{noformat}
Caused by: java.io.FileNotFoundException: /path/to/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/4/_1r.cfe (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:241)
	at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:193)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnReadDirectory$FileReference.openLocalInput(IndexCopier.java:393)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnReadDirectory.openInput(IndexCopier.java:221)
	at org.apache.lucene.store.Directory.copy(Directory.java:185)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexMBeanImpl.dumpIndexContent(LuceneIndexMBeanImpl.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

As a fix the list of local file should be maintained as progress is made once the CopyOnRead instance gets created to ensure it does not pick up files which are added once the directory is closed"	OAK	Closed	3	1	3114	resilience
13038747	Improve indexing resilience	grouping the improvements for indexer resilience in this issue for easier tracking	OAK	Open	2	15	3114	resilience
12831411	Avoid accessing binary content if the mimeType is excluded from indexing	"Currently the recommended way to exclude certain types of files from getting indexed is to add them to {{EmptyParser}} in Tika Config. However looking at how Tika works even if mimetype is provided as part metadata. 

Tika Detector try to determine the mimetype by actually reading some bytes from InputStream [1] before looking up from passed MetaData. This would cause unnecessary IO in case large number of binaries are excluded.

We would need to look for way where any access to binary content which is not being indexed can be avoided. One option can to expose a multi value config property which takes a list of mimetypes to be excluded from indexing. If the mimeType provided as part of JCR data is part of that excluded list then call to Tika should be avoided

[1] https://github.com/apache/tika/blob/trunk/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java#L446"	OAK	Closed	4	4	3114	performance
12767690	SegmentNodeStoreService prone to deadlocks	"The SegmentNodeStoreService is prone to deadlocks because of the way in which is synchronizes access to the _SegmentNodeStore_ delegate.

The issue can now be seen on #deactivate, when the deregistration is being synchronously broadcast and if a referring service calls #getNodeStore the deadlock happens.

{code}
Found one Java-level deadlock:
=============================
""qtp844483043-936"":
  waiting to lock monitor 0x000001d1aacc7208 (object 0x000001d231f52698, a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService),
  which is held by ""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)""
""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)"":
  waiting to lock monitor 0x000001d4d0907c88 (object 0x000001d2334be930, a java.lang.Object),
  which is held by ""pool-5-thread-4""
""pool-5-thread-4"":
  waiting to lock monitor 0x000001d1aacc7208 (object 0x000001d231f52698, a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService),
  which is held by ""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)""

Java stack information for the threads listed above:
===================================================
""qtp844483043-936"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:144)
	- waiting to lock <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:73)
	at org.apache.jackrabbit.oak.spi.state.ProxyNodeStore.getRoot(ProxyNodeStore.java:35)
	at org.apache.jackrabbit.oak.core.MutableRoot.<init>(MutableRoot.java:160)
	at org.apache.jackrabbit.oak.core.ContentSessionImpl.getLatestRoot(ContentSessionImpl.java:110)
	at org.apache.jackrabbit.oak.spi.security.authentication.AbstractLoginModule.getRoot(AbstractLoginModule.java:403)
	at org.apache.jackrabbit.oak.security.authentication.token.TokenLoginModule.getTokenProvider(TokenLoginModule.java:215)
	at org.apache.jackrabbit.oak.security.authentication.token.TokenLoginModule.login(TokenLoginModule.java:128)
	at org.apache.felix.jaas.boot.ProxyLoginModule.login(ProxyLoginModule.java:52)
	at sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:595)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:161)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:256)
	at com.adobe.granite.repository.impl.CRX3RepositoryImpl.login(CRX3RepositoryImpl.java:92)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:197)
	at org.apache.sling.jcr.base.AbstractSlingRepository2.login(AbstractSlingRepository2.java:297)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProviderInternal(JcrResourceProviderFactory.java:289)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProvider(JcrResourceProviderFactory.java:201)
	at org.apache.sling.resourceresolver.impl.tree.ResourceProviderFactoryHandler.login(ResourceProviderFactoryHandler.java:164)
	at org.apache.sling.resourceresolver.impl.tree.RootResourceProviderEntry.loginToRequiredFactories(RootResourceProviderEntry.java:95)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolverInternal(CommonResourceResolverFactoryImpl.java:109)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolver(CommonResourceResolverFactoryImpl.java:90)
	at org.apache.sling.resourceresolver.impl.ResourceResolverFactoryImpl.getResourceResolver(ResourceResolverFactoryImpl.java:93)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.getAnonymousResolver(SlingAuthenticator.java:839)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.doHandleSecurity(SlingAuthenticator.java:478)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.handleSecurity(SlingAuthenticator.java:438)
	at org.apache.sling.engine.impl.SlingHttpContext.handleSecurity(SlingHttpContext.java:121)
	at org.apache.felix.http.base.internal.context.ServletContextImpl.handleSecurity(ServletContextImpl.java:335)
	at org.apache.felix.http.base.internal.handler.ServletHandler.doHandle(ServletHandler.java:337)
	at org.apache.felix.http.base.internal.handler.ServletHandler.handle(ServletHandler.java:300)
	at org.apache.felix.http.base.internal.dispatch.ServletPipeline.handle(ServletPipeline.java:93)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:50)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.i18n.impl.I18NFilter.doFilter(I18NFilter.java:128)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.sslfilter.internal.SslFilter.doFilter(SslFilter.java:55)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.sslfilter.internal.SslFilter.doFilter(SslFilter.java:89)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at com.adobe.granite.license.impl.LicenseCheckFilter.doFilter(LicenseCheckFilter.java:298)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.security.impl.ReferrerFilter.doFilter(ReferrerFilter.java:290)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.featureflags.impl.FeatureManager.doFilter(FeatureManager.java:115)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.engine.impl.log.RequestLoggerFilter.doFilter(RequestLoggerFilter.java:75)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.base.internal.dispatch.FilterPipeline.dispatch(FilterPipeline.java:76)
	at org.apache.felix.http.base.internal.dispatch.Dispatcher.dispatch(Dispatcher.java:49)
	at org.apache.felix.http.base.internal.DispatcherServlet.service(DispatcherServlet.java:67)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:722)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:229)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)"":
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.unbindTopologyEventListener(DiscoveryServiceImpl.java:242)
	- waiting to lock <0x000001d2334be930> (a java.lang.Object)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.BindMethod.invoke(BindMethod.java:37)
	at org.apache.felix.scr.impl.manager.DependencyManager.invokeUnbindMethod(DependencyManager.java:1717)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.invokeUnbindMethod(SingleComponentManager.java:404)
	at org.apache.felix.scr.impl.manager.DependencyManager$MultipleDynamicCustomizer.removedService(DependencyManager.java:376)
	at org.apache.felix.scr.impl.manager.DependencyManager$MultipleDynamicCustomizer.removedService(DependencyManager.java:304)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.unregister(AbstractComponentManager.java:1011)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.unregister(AbstractComponentManager.java:992)
	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:141)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.unregisterService(AbstractComponentManager.java:1054)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:900)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:974)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:895)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.unregisterService(AbstractSlingRepositoryManager.java:258)
	at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.stop(AbstractSlingRepositoryManager.java:345)
	at com.adobe.granite.repository.impl.SlingRepositoryManager.deactivate(SlingRepositoryManager.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:149)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.disposeImplementationObject(SingleComponentManager.java:355)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.deleteComponent(SingleComponentManager.java:170)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:908)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:974)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:895)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.unregisterNodeStore(SegmentNodeStoreService.java:320)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.deactivate(SegmentNodeStoreService.java:295)
	- locked <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:149)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.disposeImplementationObject(SingleComponentManager.java:355)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.deleteComponent(SingleComponentManager.java:170)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:908)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.reconfigure(SingleComponentManager.java:638)
	at org.apache.felix.scr.impl.config.ConfigurableComponentHolder.configurationUpdated(ConfigurableComponentHolder.java:328)
	at org.apache.felix.scr.impl.config.ConfigurationSupport.configurationEvent(ConfigurationSupport.java:290)
	at org.apache.felix.cm.impl.ConfigurationManager$FireConfigurationEvent.sendEvent(ConfigurationManager.java:2032)
	at org.apache.felix.cm.impl.ConfigurationManager$FireConfigurationEvent.run(ConfigurationManager.java:2002)
	at org.apache.felix.cm.impl.UpdateThread.run(UpdateThread.java:103)
	at java.lang.Thread.run(Thread.java:745)
""pool-5-thread-4"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:144)
	- waiting to lock <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:73)
	at org.apache.jackrabbit.oak.spi.state.ProxyNodeStore.getRoot(ProxyNodeStore.java:35)
	at org.apache.jackrabbit.oak.core.MutableRoot.<init>(MutableRoot.java:160)
	at org.apache.jackrabbit.oak.core.ContentSessionImpl.getLatestRoot(ContentSessionImpl.java:110)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.<init>(SessionDelegate.java:160)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$1.<init>(RepositoryImpl.java:273)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.createSessionDelegate(RepositoryImpl.java:271)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:257)
	at com.adobe.granite.repository.impl.CRX3RepositoryImpl.login(CRX3RepositoryImpl.java:92)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl$2.run(SlingRepositoryImpl.java:108)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl$2.run(SlingRepositoryImpl.java:100)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAsPrivileged(Subject.java:536)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl.createAdministrativeSession(SlingRepositoryImpl.java:100)
	at org.apache.sling.jcr.base.AbstractSlingRepository2.loginAdministrative(AbstractSlingRepository2.java:362)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProviderInternal(JcrResourceProviderFactory.java:246)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getAdministrativeResourceProvider(JcrResourceProviderFactory.java:209)
	at org.apache.sling.resourceresolver.impl.tree.ResourceProviderFactoryHandler.login(ResourceProviderFactoryHandler.java:162)
	at org.apache.sling.resourceresolver.impl.tree.RootResourceProviderEntry.loginToRequiredFactories(RootResourceProviderEntry.java:95)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolverInternal(CommonResourceResolverFactoryImpl.java:109)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getAdministrativeResourceResolver(CommonResourceResolverFactoryImpl.java:76)
	at org.apache.sling.resourceresolver.impl.ResourceResolverFactoryImpl.getAdministrativeResourceResolver(ResourceResolverFactoryImpl.java:98)
	at org.apache.sling.discovery.impl.cluster.ClusterViewServiceImpl.getClusterView(ClusterViewServiceImpl.java:132)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.getTopology(DiscoveryServiceImpl.java:418)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.handlePotentialTopologyChange(DiscoveryServiceImpl.java:466)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.handleTopologyChanged(DiscoveryServiceImpl.java:650)
	- locked <0x000001d2334be930> (a java.lang.Object)
	at org.apache.sling.discovery.impl.topology.TopologyChangeHandler.handleTopologyChanged(TopologyChangeHandler.java:134)
	at org.apache.sling.discovery.impl.topology.TopologyChangeHandler.handleEvent(TopologyChangeHandler.java:124)
	at org.apache.felix.eventadmin.impl.handler.EventHandlerProxy.sendEvent(EventHandlerProxy.java:412)
	at org.apache.felix.eventadmin.impl.tasks.SyncDeliverTasks.execute(SyncDeliverTasks.java:118)
	at org.apache.felix.eventadmin.impl.handler.EventAdminImpl.sendEvent(EventAdminImpl.java:114)
	at org.apache.felix.eventadmin.impl.security.EventAdminSecurityDecorator.sendEvent(EventAdminSecurityDecorator.java:96)
	at org.apache.sling.jcr.resource.internal.OakResourceListener.sendOsgiEvent(OakResourceListener.java:243)
	at org.apache.sling.jcr.resource.internal.OakResourceListener.changed(OakResourceListener.java:133)
	at org.apache.jackrabbit.oak.plugins.observation.NodeObserver$NodeEventHandler.leave(NodeObserver.java:208)
	at org.apache.jackrabbit.oak.plugins.observation.FilteredHandler.leave(FilteredHandler.java:51)
	at org.apache.jackrabbit.oak.plugins.observation.EventGenerator$Continuation.run(EventGenerator.java:175)
	at org.apache.jackrabbit.oak.plugins.observation.EventGenerator.generate(EventGenerator.java:118)
	at org.apache.jackrabbit.oak.plugins.observation.NodeObserver.contentChanged(NodeObserver.java:156)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:117)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:111)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Found 1 deadlock.
{code}"	OAK	Closed	1	1	3114	resilience
13107951	Provide a way to for persistent cache to determine which all nodes can be cached	"Currently persistent cache if enabled for nodes caches all nodes accessed on the system. It would be better if it can be configured to only cache those nodes which are not volatile so that caching can be effective

Purpose of this issue is to
* Provide an extension point in PersistentCache logic to check if a node is to be cached
* Provide an impl which relies on some static OSGi config to determine that

Later we can make this impl dynamic i.e. rely on access pattern to cache imp stuff"	OAK	Closed	4	4	3114	doc-impacting
12827602	Test failure: OSGiIT	"The OSGiIT tests are failing silently, so not failing the build when the tests don't pass.

{code}
Running org.apache.jackrabbit.oak.osgi.OSGiIT
[main] INFO org.ops4j.pax.exam.spi.DefaultExamSystem - Pax Exam System (Version: 3.4.0) created.
[main] INFO org.ops4j.pax.exam.junit.impl.ProbeRunner - creating PaxExam runner for class org.apache.jackrabbit.oak.osgi.OSGiIT
[main] INFO org.ops4j.pax.exam.junit.impl.ProbeRunner - running test class org.apache.jackrabbit.oak.osgi.OSGiIT
ERROR: org.apache.jackrabbit.oak-lucene (23): [org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService(1)] The activate method has thrown an exception
java.lang.NullPointerException: Index directory cannot be determined as neither index directory path [localIndexDir] nor repository home [repository.home] defined
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:236)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService.createTracker(LuceneIndexProviderService.java:197)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService.activate(LuceneIndexProviderService.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}"	OAK	Closed	2	1	3114	CI, Jenkins
12930078	Lucene index / compatVersion 2: search for 'abc!' does not work	"When using a Lucene fulltext index with compatVersion 2, then the following query does not return any results. When using compatVersion 1, the correct result is returned.

{noformat}
SELECT * FROM [nt:unstructured] AS c 
WHERE CONTAINS(c.[jcr:description], 'abc!') 
AND ISDESCENDANTNODE(c, '/content')
{noformat}

With compatVersion 1 and 2, searching for just 'abc' works. Also, searching with '=' instead of 'contains' works."	OAK	Closed	3	1	3114	docs-impacting
12757647	Switch default IndexFormatVersion to V2 	"OAK-2276 added support for {{IndexFormatVersion}} where {{V1}} is compatible with existing {{LuceneIndex}} while {{V2}} is compatible with newer index implemention being worked on OAK-2278.

Once implementation in OAK-2278 is stable enough we should switch the default version to be used for fresh index (unless overrided with {{compatMode}} ) from V1 to V2"	OAK	Closed	3	3	3114	performance
12944433	Allow use of pre extrcated text cache for incremental indexing	"Pre Extraction support was implemented with an assumption that such big indexing would happen as part of reindex so it was used in reindex phase only. Reason to avoid using it in incremental indexing (non reindex case) were
# Incremental index would does not have text for newly added files. So checking with pre extracted cache would not be useful
# PreExtraction logic keeps in memory state (blobs_empty.txt,blobs_error.txt) which would then unnecessary hog memory.

However in some cases people make use of new incremental migration feature in upgrade. Which would lead to one big incremental indexing step once next migration is done and that would then not able to make use of pre extraction support.

So as a fix we should provide a policy option to ignore the reindex clause per admin setting"	OAK	Closed	4	4	3114	docs-impacting
12975125	Optimize Revison fromString and toString implementation	"Current implementation of Revision {{fromString}} and {{toString}} make use of std JDK API to perform string manipulation. While running some performance test it was seen that these 2 methods are called quite frequently and that adds up to some decent times. Further they also generate quite a bit of short lived objects.

!hot-methods.png!

It would be worthwhile to perform a micro benchmark of these method and optimize them further such that they perform better and also generate less garbage. The micro optimized code would be bit more complex but if performance numbers are better we can look into changing the current implementation"	OAK	Closed	3	4	3114	performance
12830925	Add support for generating mongo export command to oak-mongo	"At time to analyse a issue with {{DocumentNodeStore}} running on Mongo we need a dump of various documents so as to recreate the scenario locally. In most case if issue is being observed for a specific path like /a/b then its sufficient to get Mongo documents for /, /a, /a/b and all the split documents for those paths.

It would be useful to have a function in oak-mongo which generates the required export command. For e.g. for path like /a/b following export command would dump all required info

{noformat}
mongoexport -h <mongo server> --port 27017 --db <db name> --collection nodes --out all-required-nodes.json --query '{$or:[{_id : /^4:p\/a\/b\//},{_id : /^3:p\/a\//},{_id : /^2:p\//},{_id:{$in:[""2:/a/b"",""1:/a"",""0:/""]}}]}'
{noformat}"	OAK	Closed	4	4	3114	tooling
12742399	Killing a node may stop async index update to to 30 minutes (Tar storage)	"When killing a node that is running the sync index update, then this async index update will not run for up to 15 minutes, because the lease time is set to 15 minutes.

I think the lease time should be much smaller, for example 1 minute, or maybe even 10 seconds.

Also, we might need to better document this issue (in addition to the warning in the log file). For non cluster case we can do away with lease time out and this for such cases indexing would not get paused upon restart post abrupt shutdown"	OAK	Closed	3	4	3114	resilience
12616917	Remove exported packages from Mongo MK Bundle	The oak-mongomk bundle currently exports couple of packages which are not required to be exported. These exports should be removed	OAK	Resolved	4	3	3114	osgi
12782058	Use buffered variants for IndexInput and IndexOutput	"Lucene provides a buffered variants for {{IndexInput}} and {{IndexOutput}}. Currently Oak extends these classes directly. For better performance itshould extend the buffered variants.

As discussed [here|https://issues.apache.org/jira/browse/OAK-2222?focusedCommentId=14178265#comment-14178265]"	OAK	Resolved	3	4	3114	performance
12830649	Support migration without access to DataStore	"Migration currently involves access to DataStore as its configured as part of repository.xml. However in complete migration actual binary content in DataStore is not accessed and migration logic only makes use of

* Dataidentifier = id of the files
* Length = As it gets encoded as part of blobId (OAK-1667)

It would be faster and beneficial to allow migration without actual access to the DataStore. It would serve two benefits

# Allows one to test out migration on local setup by just copying the TarPM files. For e.g. one can only zip following files to get going with repository startup if we can somehow avoid having direct access to DataStore
{noformat}
>crx-quickstart# tar -zcvf repo-2.tar.gz repository --exclude=repository/repository/datastore --exclude=repository/repository/index --exclude=repository/workspaces/crx.default/index --exclude=repository/tarJournal
{noformat}
# Provides faster (repeatable) migration as access to DataStore can be avoided which in cases like S3 might be slow.  Given we solve how to get length

*Proposal*
Have a DataStore implementation which can be provided a mapping file having entries for blobId and length. This file would be used to answer queries regarding length and existing of blob and thus would avoid actual access to DataStore.

Going further this DataStore can be configured with a delegate which can be used as a fallback in case the required details is not present in pre computed data set (may be due to change in content after that data was computed)

"	OAK	Closed	3	2	3114	docs-impacting, performance
13005064	Remove usage of Tree in LuceneIndexEditor	"{{LuceneIndexEditor}} currently creates 2 tree instances for determining IndexRule. [~ianeboston] highlighted this on list [1] and this is something which we should avoid and remove usage of Tree api

This was earlier done so as to simplify future support for conditional rules (OAK-2281) which might need access to ancestor which is not possible with NodeState api.  As that is not going to be done so we can get rid of Tree construction in the editor.

[1] https://lists.apache.org/thread.html/7d51b45296f5801c3b510a30a4847ce297707fb4e0d4c2cefe19be62@%3Coak-dev.jackrabbit.apache.org%3E
"	OAK	Closed	4	4	3114	performance
12696503	Offline tool to repair MongoMK documents	"For cases where the document semantics in Mongo that are created by Oak get corrupted to a point that Oak does not come up anymore (but MongoDB is still available), we should have a mechanism to fix those inconsistencies.

Of course, one could use Mongo tools like cmdline or MongoHub to manually go in, but an automated approach would be preferable in the medium term."	OAK	Closed	4	4	3114	production, resilience, tools
12849611	Lucene Version should be based on IndexFormatVersion	"Currently in oak-lucene where ever call is made to Lucene it passes Version.LUCENE_47 as hardcoded version. To enable easier upgrade of Lucene and hence change of defaults for fresh setup this version should be instead based on {{IndexFormatVersion}}.

Say
* For IndexFormatVersion set to V2 (current default) - Lucene version used is LUCENE_47
* For IndexFormatVersion set to V3 (proposed) - Lucene version used would be per Lucene library version

If the index is reindexed then it would automatically be updated to the latest revision"	OAK	Open	3	7	3114	technical_debt
12603584	MicroKernelService should set metatype to true to easier configuration	MicroKernelService currently uses @Component annotation without enabling metatype. If metatype is enabled it would simply the configuration of home directory. 	OAK	Closed	4	4	3114	osgi
13037570	Reduce lookup for oak:index node under each changed node	"Currently {{IndexUpdate}} does a lookup for {{oak:index}} node under each changed node. This is done to pickup index definitions and create IndexEditor based on those so as to index content under that subtree. 

This lookup results in extra remote calls on DocumentNodeStore based setup as for non leaf nodes NodeStore has to check from remote storage to determine if {{oak:index}} node is present or not.

This lookup can be avoided by
# Having an {{Editor}} which adds a hidden property {{:oak-index-present}} in parent node upon addition of {{oak:index}} as a child node
# IndexUpdate would then does a lookup for {{oak:index}} node only if such a hidden property is found

For upgrade we would have some logic which would make use of Nodetype index to identify all such nodes and mark them

Discussion [thread|https://lists.apache.org/thread.html/70d5ffff0f950d7fc25bc1bbb41527f5672825f8cf2b238f54df2966@%3Coak-dev.jackrabbit.apache.org%3E] on oak-dev"	OAK	Open	3	4	3114	performance
12841567	RemoteServerIT test are failing on the CI server	"Most of the test in {{RemoteServerIT}} at times fail on the CI server [1] with following exception

{noformat}
Error Message

/home/jenkins/jenkins-slave/workspace/Apache%20Jackrabbit%20Oak%20matrix/jdk/latest1.7/label/Ubuntu/nsfixtures/SEGMENT_MK/profile/unittesting/oak-remote/target/test-classes/org/apache/jackrabbit/oak/remote/http/handler/addNodeMultiPathProperty.json (No such file or directory)
Stacktrace

java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/Apache%20Jackrabbit%20Oak%20matrix/jdk/latest1.7/label/Ubuntu/nsfixtures/SEGMENT_MK/profile/unittesting/oak-remote/target/test-classes/org/apache/jackrabbit/oak/remote/http/handler/addNodeMultiPathProperty.json (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:127)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:117)
	at com.google.common.io.ByteSource$AsCharSource.openStream(ByteSource.java:404)
	at com.google.common.io.CharSource.read(CharSource.java:155)
	at com.google.common.io.Files.toString(Files.java:391)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.load(RemoteServerIT.java:119)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.testPatchLastRevisionAddMultiPathProperty(RemoteServerIT.java:1199)
{noformat}

[1] https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/232/testReport/"	OAK	Closed	4	1	3114	CI, Jenkins
12747934	Fix intermittent failure in JaasConfigSpiTest	"Intermittent failures on windows are observed in JaasConfigSpiTest with following exception

{noformat}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.841 sec <<< FAILURE!
defaultConfigSpiAuth(org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest)  Time elapsed: 3.835 sec  <<< ERROR!
java.lang.reflect.UndeclaredThrowableException
	at $Proxy7.login(Unknown Source)
	at javax.jcr.Repository$login.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest.defaultConfigSpiAuth(JaasConfigSpiTest.groovy:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.jackrabbit.oak.run.osgi.OakOSGiRepositoryFactory$RepositoryProxy.invoke(OakOSGiRepositoryFactory.java:325)
	... 37 more
Caused by: javax.jcr.LoginException: No LoginModules configured for jackrabbit.oak
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:264)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:222)
	... 42 more
Caused by: javax.security.auth.login.LoginException: No LoginModules configured for jackrabbit.oak
	at javax.security.auth.login.LoginContext.init(LoginContext.java:256)
	at javax.security.auth.login.LoginContext.<init>(LoginContext.java:499)
	at org.apache.jackrabbit.oak.spi.security.authentication.JaasLoginContext.<init>(JaasLoginContext.java:49)
	at org.apache.jackrabbit.oak.security.authentication.LoginContextProviderImpl.getLoginContext(LoginContextProviderImpl.java:85)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:161)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:256)
	... 43 more

Running org.apache.jackrabbit.oak.run.osgi.JsonConfigRepFactoryTest
{noformat}"	OAK	Closed	4	3	3114	CI, buildbot, test
12858273	Improve indexing resilience	As discussed bilaterally grouping the improvements for indexer resilience in this issue for easier tracking	OAK	Resolved	2	15	3114	resilience
13100831	test failure in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT	"{noformat}
Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 99.858 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT)  Time elapsed: 71.122 sec  <<< ERROR!
java.lang.RuntimeException: Error occurred while obtaining InputStream for blobId [8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4#1073741824]
Caused by: java.io.IOException: org.apache.jackrabbit.core.data.DataStoreException: Record 8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4 does not exist
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Record 8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4 does not exist

{noformat}

(might be specific to Windows)"	OAK	Closed	1	1	3932	cold-standby
13013225	SetPropertyTest benchmark fails on Segment Tar	"The {{SetPropertyTest}} fails on Oak Segment Tar:

{noformat}
javax.jcr.InvalidItemStateException: This item [/testfb3e8f1a/ca1ef350-f650-4466-b9e3-7f77d83e6303] does not exist anymore
	at org.apache.jackrabbit.oak.jcr.delegate.ItemDelegate.checkAlive(ItemDelegate.java:86)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl$ItemWriteOperation.checkPreconditions(ItemImpl.java:96)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$35.checkPreconditions(NodeImpl.java:1366)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.prePerform(SessionDelegate.java:615)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:205)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:112)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.internalSetProperty(NodeImpl.java:1363)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.setProperty(NodeImpl.java:506)
	at org.apache.jackrabbit.oak.benchmark.SetPropertyTest.runTest(SetPropertyTest.java:65)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.execute(AbstractTest.java:372)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.runTest(AbstractTest.java:221)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.run(AbstractTest.java:197)
	at org.apache.jackrabbit.oak.benchmark.BenchmarkRunner.main(BenchmarkRunner.java:456)
	at org.apache.jackrabbit.oak.run.BenchmarkCommand.execute(BenchmarkCommand.java:26)
	at org.apache.jackrabbit.oak.run.Mode.execute(Mode.java:63)
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
{noformat}
"	OAK	Closed	4	1	3932	test-failure
13101940	Cold standby should fail loudly when a big blob can't be timely transferred	"Due to changes done in OAK-4969, currently there are two 'sync blob' cycles triggered by {{StandbyDiff#childNodeChanged}}. The test scenario is the same as the one in {{DataStoreTestBase#testSyncBigBlob}}: on the primary file store, a new big blob (1GB) is added and then a standby sync is triggered to sync this content to the secondary file store. 

The first 'sync blob' cycle happens as a result of {{#process}} being called in {{StandbyDiff#childNodeChanged}}. Therefore, a new 'get blob' request is created on the client and the server starts sending chunks from the big blob. Now, if the time needed for transferring the entire blob from server to client exceeds {{readTimeoutMs}} an {{IllegalStateException}} will be correctly thrown by {{StandbyDiff#readBlob}}, but will be swallowed by the {{StandbyDiff#childNodeChanged}} in its catch clause. A second 'sync blob' cycle will be triggered and, -this might succeed with the same {{readTimeoutMs}} for which it was failing before-, if {{readTimeoutMs * 2}} is enough, the blob will be synced on the standby. This happens because the server will continue sending the remaining chunks after {{IllegalStateException}} was thrown (first 'sync blob' cycle).

The consequence of these two 'sync blob' cycles is that sometimes, deleting the temporary file to which chunks are spooled to on the client fails (see Windows for example and OAK-6641 specifically). This way, instead of deleting the previous incomplete transfer, new chunks from the second 'sync blob' cycle are added. The blob persisted in the blob store on the client won't have the same size and id as the initial blob sent by the server."	OAK	Closed	2	1	3932	cold-standby
13093446	Investigate cold standby memory consumption 	"In an investigation from some time ago, 4GB of heap were needed for transferring 1GB blob and 6GB for 2GB blob. This was in part due to using {{addTestContent}} [0] in the investigation, which allocates a huge {{byte[]}} on the heap. 

OAK-5902 introduced chunking for transferring blobs between primary and standby. This way, the memory needed for syncing a big blob should be around the chunk size used. Solving the way test data is created, it should be possible to transfer a big blob (e.g. 2.5 GB) with less memory.

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/test/java/org/apache/jackrabbit/oak/segment/standby/DataStoreTestBase.java#L96"	OAK	Closed	4	3	3932	cold-standby
13029134	Clarify the various directories and their usages in SegmentNodeStoreService	In {{SegmentNodeStoreService}} there is {{repository.home}}, {{DIRECTORY}}, {{getRootDirectory()}}, {{getDirectory()}} and {{getBaseDirectory()}} mostly without documentation about their intention. I think we should clarify, document and consolidate them. 	OAK	Closed	3	3	3932	technical_debt
12948120	Replace journal.log with an in place journal	Instead of writing the current head revision to the {{journal.log}} file we could make it an integral part of the node states: as OAK-3804 demonstrates we already have very good heuristics to reconstruct a lost journal. If we add the right annotations to the root node states this could replace the current approach. The latter is problematic as it relies on the flush thread properly and timely updating {{journal.log}}. See e.g. OAK-3303. 	OAK	Open	4	2	3932	resilience
13101964	ResponseDecoder should check that the length of the received blob matches the length of the sent blob	"As already explained in OAK-6659, there can be cases in which deleting the previous spool file fails (Windows) and new (duplicate) content is added under the hood to the old file. This way the persisted blob doesn't match in content and id with the original sent by the server.

A first improvement here is to not allow the decoding to continue if the old spool file cannot be deleted. For this, the call to {{File#delete}} needs to be replaced with {{java.nio.file.Files#delete}} which would throw an exception if something wrong happens.

By ensuring that the spool file has the same size as the original blob we solve this problem. This check is sufficient, since all the chunks received are individually checked by hash, before appending them to the spool file. Moreover, the single threaded nature of the client ensures that races in which a new thread starts appending new content, after the length check has just passed can never happen."	OAK	Closed	3	4	3932	cold-standby
13045682	TarMK: Implement tooling to repair broken nodes	"With {{oak-run check}} we can determine the last good revision of a repository and use it to manually roll back a corrupted segment store. 

Complementary to this we should implement a tool to roll forward a broken revision to a fixed new revision. Such a tool needs to detect which items are affected by a corruption and replace these items with markers. With this the repository could brought back online and the markers could be used to identify the locations in the tree where further manual action might be needed. "	OAK	Open	3	2	3932	production, technical_debt, tooling
13045336	Test failure: segment.standby.MBeanIT.testClientAndServerEmptyConfig	"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #472 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #472|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/472/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/472/console]"	OAK	Closed	3	1	3932	test-failure, windows
12702108	Expose FileStoreBackupRestoreMBean for supported NodeStores	{{NodeStore}} implementations should expose the {{FileStoreBackupRestoreMBean}} in order to be interoperable with {{RepositoryManagementMBean}}. See OAK-1160.	OAK	Closed	3	4	3932	monitoring
13116579	Provide a way to tune inline size while storing binaries	"SegmentNodeStore currently inlines binaries of size less that 16KB (Segment.MEDIUM_LIMIT) even if external BlobStore is configured. 

Due to this behaviour quite a bit of segment tar storage consist of blob data. In one setup out of 370 GB segmentstore size 290GB is due to inlined binary. If most of this binary content is moved to BlobStore then it would allow same repository to work better in lesser RAM

So it would be useful if some way is provided to disable this default behaviour and let BlobStore take control of inline size i.e. in presence of BlobStore no inlining is attempted by SegmentWriter."	OAK	Closed	3	4	3932	performance, scalability
13046007	Remove the deprecated oak-segment module	"The {{oak-segment}} module has been deprecated for 1.6 with OAK-4247. We should remove it entirely now:

* Remove the module
* Remove fixtures and ITs pertaining to it
* Remove references from documentation where not needed any more

An open question is how we should deal with the tooling for {{oak-segment}}. Should we still maintain this in trunk and keep the required classes (which very much might be all) or should we maintain the tooling on the branches? What about new features in tooling? 
"	OAK	Closed	4	3	3932	deprecation, technical_debt
12948139	Reclaimed size reported by FileStore.cleanup is off	"The current implementation simply reports the difference between the repository size before cleanup to the size after cleanup. As cleanup runs concurrently to other commits, the size increase contributed by those is not accounted for. In the extreme case where cleanup cannot reclaim anything this can even result in negative values being reported. 

We should either change the wording of the respective log message and speak of before and after sizes or adjust our calculation of reclaimed size (preferred). "	OAK	Closed	4	1	3932	cleanup, gc
13126821	Update documentation for cold standby	"Improve monitoring section of cold standby in {{oak-doc}} to include missing MBean screenshots.

-[~mduerig], [~frm]: How about adding a *Benchmarking* section to the cold standby page covering a bit ways to use the new {{Oak-Segment-Tar-Cold}} fixture and also running {{ScalabilityStandbySuite}} on top of it?-"	OAK	Closed	3	20	3932	documentation
13125830	Segment-Tar-Cold fixture should have options for secure communication and one shot runs	"The newly introduced {{Segment-Tar-Cold}} fixture should support secure communication between primary and standby via a {{--secure}} option. Moreover, the current implementation allows only for continuous sync between primary and standby. It should be possible to allow a ""one-shot run"" of the sync to easily measure and compare specific metrics ({{--oneShotRun}} option)."	OAK	Closed	4	4	3932	cold-standby
13240824	oak-run check should expose repository statistics for the last good revision	"{{oak-run check}} should expose the head node and property counts for the last good revision. Currently these are only logged at the end of the check operation as
{noformat}
Checked X nodes and Y properties.{noformat}"	OAK	Closed	4	4	3932	tooling
13027471	"The check command overloads the meaning of the ""deep"" option"	"The {{--deep}} option accepted by the {{check}} command is semantically overloaded. It is used both as a flag to enable deep content traversal and as a way to specify the frequency of debug messages printed by the tool. 

This option should be split in two. In particular, {{--deep}} should retain its behaviour of on/off flag for deep traversal, and a new command line option should be introduced to specify the interval of debug messages."	OAK	Closed	4	4	3932	tooling
13520555	Bump netty dependency from 4.1.68.Final to 4.1.86.Final	"*Vulnerabilities*

CVE-2022-41881

A StackOverflowError can be raised when parsing a malformed crafted message due to an
infinite recursion.

[https://github.com/netty/netty/security/advisories/GHSA-fx2c-96vj-985v|https://github.com/netty/netty/security/advisories/GHSA-grg4-wf29-r9vv]"	OAK	Closed	3	3	3932	cold-standby, vulnerability
13100178	Replace standby blob chunk size configuration with feature flag	We should remove the {{StandbyStoreService#BLOB_CHUNK_SIZE}} OSGi configuration and replace it with a feature flag. Rational: we expect customer to rarely change this thus not justifying the additional configuration complexity and testing overhead. 	OAK	Closed	3	4	3932	cold-standby, configuration
13218575	The cold standby server cannot handle blob requests for long blob IDs	"If the standby client issues a request for a binary ID larger than 8192 bytes, it will fail on the server side due to the current frame limitation, set to 8192 bytes:
{noformat}
28.02.2019 00:01:36.034 *WARN* [primary-32] org.apache.jackrabbit.oak.segment.standby.server.ExceptionHandler Exception caught on the server
io.netty.handler.codec.TooLongFrameException: frame length (35029) exceeds the allowed maximum (8192)
        at io.netty.handler.codec.LineBasedFrameDecoder.fail(LineBasedFrameDecoder.java:146) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.fail(LineBasedFrameDecoder.java:142) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.decode(LineBasedFrameDecoder.java:131) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.decode(LineBasedFrameDecoder.java:75) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}"	OAK	Closed	3	1	3932	cold-standby
13046916	Compressed segments	"It would be interesting to see the effect of compressing the segments within the tar files with a sufficiently effective and performant compression algorithm:

* Can we increase overall throughput by trading CPU for IO?
* Can we scale to bigger repositories (in number of nodes) by squeezing in more segments per MB and thus pushing out onset of thrashing?
* What would be a good compression algorithm/library?
* Can/should we make this optional? 
* Migration and compatibility issues?
"	OAK	Open	3	2	3932	scalability
13095635	GetBlobResponseEncoder should not write all chunks at once	"{{GetBlobResponseEncoder}} writes too fast all the chunks, leaving the channel in a not-writable state, after the first write. The problem is not visible at a first glance, especially when using small blobs for testing. Increasing the blobs size, as done for OAK-6538, revealed the problem. Not only this triggers hidden {{OutOfMemory}} errors on either server or client, but sometimes incomplete blobs are sent along, which are interpreted by the client as valid.

A more elegant solution, which also solves the memory consumption problem, would be to use {{ChunkedWriteHandler}} which employs complex logic on how and when to write the chunks. {{ChunkedWriteHandler}} must be used in conjunction with a custom {{ChunkedInput<ByteBuf>}} implementation to generate {{header}} + {{payload}} chunks from an {{InputStream}}, as done currently. This way the server will send more chunks only when the previous one was consumed by the client.

/cc [~frm]"	OAK	Closed	3	4	3932	cold-standby
13327862	Bump netty dependency from 4.1.17.Final to 4.1.52.Final	"The current version presents several vulnerability issues: 

BDSA-2018-4022,BDSA-2018-4482,BDSA-2019-2642,BDSA-2019-2643."	OAK	Closed	3	3	3932	cold-standby
13149438	SegmentNodeStoreStats should expose stats for previous minute per thread group	"The current ""CommitsCountPerWriter"" stats exposed by {{SegmentNodeStoreStats}} are hard to follow since there can be too many writers at a time. To improve this, a more coarse-grained version of this metric should be added, in which commits are recorded for groups of threads. The groups should be configurable and represent regexes to be matched by individual thread names. An additional group (i.e. ""other"") will group all threads not matching any of the defined group regexes. 

The current behaviour will be split in two:
* ""CommitsCountOtherThreads"" will expose a snapshot of threads currently in ""other"" group
* ""CommitsCountPerGroup"" will expose an aggregate of commits count per thread group for the previous minute.

Both metrics will be reset each minute."	OAK	Closed	4	4	3932	tooling
13131603	Update documentation for oak-run check	We should review and update the documentation of [{{oak-run check}}|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#check]. E.g. to include the new options from OAK-6373.	OAK	Closed	3	3	3932	documentation
12996938	Cleanup creates new generation of tar file without removing any segments 	"On some deployments I have seen tar files with a quite hight generation post-fix (e.g. 'v'). From the log files I could deduce that this particular tar file was rewritten multiple times without actually any segment being removed.
I assume this is caused by the 25% gain threshold not taking the sizes contributed by the index and the graph entries into account.

The attached test case can be used to verify the above hypothesis."	OAK	Closed	4	1	3932	cleanup, gc
13048893	Cold standby should allow syncing of blobs bigger than 2.2 GB	"Currently there is a limitation for the maximum binary size (in bytes) to be synced between primary and standby instances. This matches {{Integer.MAX_VALUE}} (2,147,483,647) bytes and no binaries bigger than this limit can be synced between the instances.

Per comment at [1], the current protocol needs to be changed to allow sending of binaries in chunks, to surpass this limitation.

[1] https://github.com/apache/jackrabbit-oak/blob/1.6/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/standby/client/StandbyClient.java#L125"	OAK	Closed	4	4	3932	cold-standby
13095632	Update netty dependency to 4.1.x	"Taking into account the improvements listed at [0], and also the individual issues solved since our current netty version (4.0.41.Final) was released, I propose to bump up netty version to latest 4.1.14.Final.

/cc [~frm]

[0] http://netty.io/wiki/new-and-noteworthy-in-4.1.html "	OAK	Closed	4	4	3932	cold-standby
13047961	Evaluate utility of RepositoryGrowthTest benchmark	"{{RepositoryGrowthTest}} is a benchmark which makes use of the deprecated {{SegmentFixture}}. Since OAK-5834 removes the old {{oak-segment}} module and the code associated with it, {{RepositoryGrowthTest}} was also removed. If there's value in it, we can adapt it to work with the new {{SegmentTarFixture}}.

/cc [~chetanm]"	OAK	Resolved	4	3	3932	benchmark
12949107	Replace the commit semaphore in the segment node store with a scheduler	"{{SegmentNodeStore}} currently uses a semaphore to coordinate concurrent commits thus relying on the scheduling algorithm of that implementation and ultimately of the JVM for in what order commits are processed. 

I think it would be beneficial to replace that semaphore with an explicit queue of pending commit. This would allow us to implement a proper scheduler optimising for e.g. minimal system load, maximal throughput or minimal latency etc. A scheduler could e.g. give precedence to big commits and order commits along the order of its base revisions, which would decrease the amount of work to be done in rebasing. 


"	OAK	Closed	3	2	3932	operations, performance, scalability, throughput
13155694	oak-run compact should support Azure Segment Store	"{{oak-run compact}} should accept Azure URIs for the segment store in order to enable OffRC for Azure Segment Store.

-Proposed options to add:-
 * -{{azure-connection}}: connection URL to to connect to the Azure Storage-
 * -{{azure-container}}: name of the container to use-
 * -{{azure-root-path}}: segment store directory-

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where *az* identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._"	OAK	Closed	3	4	3932	tooling
13102497	Create a more complex IT for cold standby	"At the moment all integration tests for cold standby are using the same scenario in their tests: some content is created on the server (including binaries), a standby sync cycle is started and then the content is checked on the client. The only twist here is using/not using a data store for storing binaries.

Although good, this model could be extended to cover many more cases. For example, {{StandbyDiff}} covers the following 6 cases node/property added/changed/deleted. From these, with the scenario described, the removal part is never tested (and the change part is covered in only one test). 

It would be nice to have an IT which would add content on the server, do a sync, remove some of the content, do a sync and then call OnRC. This way all cases will be covered, including if cleanup works as expected on the client.

/cc [~frm]"	OAK	Open	3	3	3932	cold-standby, technical_debt, test
13328580	Fix OSGi wiring after netty update to 4.1.52.Final	"After netty update in OAK-9210,  {{OSGiIT}} fails with the following exception:

{code}
ERROR: Bundle org.apache.jackrabbit.oak-segment-tar [41] Error starting file:/var/folders/jh/rvxkcm515dl3bksp1zlzdws80000gn/T/1600699057212-0/bundles/org.apache.jackrabbit.oak-segment-tar_1.35.0.SNAPSHOT.jar (org.osgi.framework.BundleException: Unable to resolve org.apache.jackrabbit.oak-segment-tar [41](R 41.0): missing requirement [org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate) Unresolved requirements: [[org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate)])
org.osgi.framework.BundleException: Unable to resolve org.apache.jackrabbit.oak-segment-tar [41](R 41.0): missing requirement [org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate) Unresolved requirements: [[org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate)]
	at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:4368)
	at org.apache.felix.framework.Felix.startBundle(Felix.java:2281)
	at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1539)
	at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)
	at java.base/java.lang.Thread.run(Thread.java:835)
{code}"	OAK	Closed	3	1	3932	cold-standby
13212224	SegmentBlob#readLongBlobId might cause SegmentNotFoundException on standby	"When persisting a segment transferred from master, among others, the cold standby needs to read the binary references from the segment. While this usually doesn't involve any additional reads from any other segments, there is a special case concerning binary IDs larger than 4092 bytes. These can live in other segments (which got transferred prior to the current segment and are already on the standby), but it might also be the case that the binary ID is stored in the same segment. If this happens, the call to {{blobId.getSegment()}}[0], triggers a new read of the current, un-persisted segment . Thus, a {{SegmentNotFoundException}} is thrown:
{noformat}
22.01.2019 09:35:59.345 *ERROR* [standby-run-1] org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSync Failed synchronizing state.
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment d40a9da6-06a2-4dc0-ab91-5554a33c02b0 not found
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readSegmentUncached(AbstractFileStore.java:284) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.lambda$readSegment$10(FileStore.java:498) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentCache$NonEmptyCache.lambda$getSegment$0(SegmentCache.java:163) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3932) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at org.apache.jackrabbit.oak.segment.SegmentCache$NonEmptyCache.getSegment(SegmentCache.java:160) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:498) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:153) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:98) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(SegmentBlob.java:206) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentBlob.readBlobId(SegmentBlob.java:163) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore$3.consume(AbstractFileStore.java:262) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.Segment.forEachRecord(Segment.java:601) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readBinaryReferences(AbstractFileStore.java:257) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(FileStore.java:533) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.copySegmentFromPrimary(StandbyClientSyncExecution.java:225) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.copySegmentHierarchyFromPrimary(StandbyClientSyncExecution.java:194) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.compareAgainstBaseState(StandbyClientSyncExecution.java:101) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.execute(StandbyClientSyncExecution.java:76) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSync.run(StandbyClientSync.java:165) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:347) [org.apache.sling.commons.scheduler:2.7.2]
        at org.quartz.core.JobRunShell.run(JobRunShell.java:202) [org.apache.sling.commons.scheduler:2.7.2]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834){noformat}
 

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentBlob.java#L205"	OAK	Closed	3	1	3932	cold-standby
12997573	SNFE thrown while testing FileStore.cleanup() running concurrently with writes	"{{SegmentNotFoundException}} is thrown from time to time in the following scenario: plenty of concurrent writes (each creating a {{625 bytes}} blob) interrupted by a cleanup. 

Stack trace (including some debugging statements added by me):
{code:java}
Pre cleanup readers: []
Before cleanup readers: [/Users/dulceanu/work/test-repo/data00000a.tar]
Initial size: 357.4 kB
After cleanup readers: [/Users/dulceanu/work/test-repo/data00000a.tar]
After cleanup size: 357.4 kB
Final size: 361.0 kB
Exception in thread ""pool-5-thread-74"" org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Cannot copy record from a generation that has been gc'ed already
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1207)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:1096)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:1013)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:1013)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:987)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.access$700(SegmentWriter.java:379)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$8.execute(SegmentWriter.java:337)
	at org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.execute(SegmentBufferWriterPool.java:105)
	at org.apache.jackrabbit.oak.segment.SegmentWriter.writeNode(SegmentWriter.java:334)
	at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:111)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:550)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:571)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:627)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:287)
	at org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT$1.run(CompactionAndCleanupIT.java:961)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 4fb637cc-5013-4925-ab13-0629c4406481 not found
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1341)
	at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:123)
	at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:94)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1199)
	... 18 more
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Invalid segment format. Dumping segment 4fb637cc-5013-4925-ab13-0629c4406481
00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000040 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000080 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000090 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000C0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000D0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000110 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000120 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000130 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000140 39 37 39 31 31 36 30 38 2D 63 31 63 65 2D 34 62 97911608-c1ce-4b
00000150 35 63 2D 61 36 33 37 2D 39 36 61 65 39 34 38 38 5c-a637-96ae9488
00000160 61 37 65 38 2E 30 61 62 34 30 36 38 36 00 00 00 a7e8.0ab40686...
00000170 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000180 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000190 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001A0 00 00 00 00 30 30 30 30 34 30 30 00 30 30 30 30 ....0000400.0000
000001B0 30 30 30 00 30 30 30 30 30 30 30 00 30 30 30 30 000.0000000.0000
000001C0 30 30 30 31 33 30 30 00 31 32 37 35 34 36 30 33 0001300.12754603
000001D0 37 32 32 00 30 31 32 33 30 37 00 20 30 00 00 00 722.012307. 0...
000001E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000200 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000210 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000220 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000230 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000240 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000250 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000260 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000270 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000280 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000290 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................

	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1015)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:972)
	at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:283)
	at org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(SegmentCache.java:92)
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1275)
	... 21 more
Caused by: java.lang.IllegalStateException: Invalid segment format. Dumping segment 4fb637cc-5013-4925-ab13-0629c4406481
00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000040 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000080 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000090 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000C0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000D0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000110 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000120 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000130 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000140 39 37 39 31 31 36 30 38 2D 63 31 63 65 2D 34 62 97911608-c1ce-4b
00000150 35 63 2D 61 36 33 37 2D 39 36 61 65 39 34 38 38 5c-a637-96ae9488
00000160 61 37 65 38 2E 30 61 62 34 30 36 38 36 00 00 00 a7e8.0ab40686...
00000170 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000180 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000190 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001A0 00 00 00 00 30 30 30 30 34 30 30 00 30 30 30 30 ....0000400.0000
000001B0 30 30 30 00 30 30 30 30 30 30 30 00 30 30 30 30 000.0000000.0000
000001C0 30 30 30 31 33 30 30 00 31 32 37 35 34 36 30 33 0001300.12754603
000001D0 37 32 32 00 30 31 32 33 30 37 00 20 30 00 00 00 722.012307. 0...
000001E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000200 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000210 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000220 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000230 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000240 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000250 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000260 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000270 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000280 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000290 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................

	at com.google.common.base.Preconditions.checkState(Preconditions.java:150)
	at org.apache.jackrabbit.oak.segment.Segment.<init>(Segment.java:185)
	at org.apache.jackrabbit.oak.segment.file.FileStore$15.call(FileStore.java:1292)
	at org.apache.jackrabbit.oak.segment.file.FileStore$15.call(FileStore.java:1)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1011)
	... 25 more
0
{code}
"	OAK	Closed	3	1	3932	cleanup, gc
13216958	The cold standby client doesn't correctly handle backward references	"The logic from {{StandbyClientSyncExecution#copySegmentHierarchyFromPrimary}} has a flaw when it comes to ""backward references"". Suppose we have the following data segment graph to be transferred from primary: S1, which references \{S2, S3} and S3 which references S2. Then, the correct transfer order should be S2, S3 and S1.

Going through the current logic employed by the method, here's what happens:
{noformat}
Step 0: batch={S1}

Step 1: visited={S1}, data={S1}, batch={S2, S3}, queued={S2, S3}

Step 2: visited={S1, S2}, data={S2, S1}, batch={S3}, queued={S2, S3}

Step 3: visited={S1, S2, S3}, data={S3, S2, S1}, batch={}, queued={S2, S3}.{noformat}
Therefore, at the end of the loop, the order of the segments to be transferred will be S3, S2, S1, which might trigger a {{SegmentNotFoundException}} when S3 is further processed, because S2 is missing on standby (see OAK-8006).

/cc [~frm]"	OAK	Closed	3	1	3932	cold-standby
13153315	Introduce SegmentNodeStoreMonitorService for exposing writerGroups as an OSGi config property	It would be useful to expose {{writerGroups}} in {{SegmentNodeStoreStats}} through an OSGi config property. Since this is a low level configuration related to monitoring, it shouldn't be added to {{SegmentNodeStoreService}}, but to a newly created service, {{SegmentNodeStoreMonitorService}} that would handle exposing monitoring related stuff. 	OAK	Closed	4	4	3932	tooling
13102284	Refactor StandbyDiff for better clarity and understandability	"{{StandbyDiff}} still makes use of the {{logOnly}} property for deciding when to act upon node/property changes. The official documentation of {{logOnly}} states that it helps for

{quote}
/**
     * read-only traversal of the diff that has 2 properties: one is to log all
     * the content changes, second is to drill down to properly level, so that
     * missing binaries can be sync'ed if needed
     */
{quote}

but it's use is a bit misleading. The first call to {{StandbyDiff}} is always with {{logOnly==false}}, while subsequent calls are done with {{logOnly==true}}. Implementing {{StandbyDiff}} without this mechanism would result in better clarity and maintainability.

Another minor improvement is to rename {{#binaryCheck}} methods and {{#readBinary}} to {{#fetchBinary}} and {{#fetchAndStoreBlob}} which is more appropriate to their purpose."	OAK	Closed	4	4	3932	cold-standby
12993517	Align GCMonitorMBean MBean with new generation based GC	"The {{GCMonitorMBean}} MBean still dates back to the old {{oak-segment}}. We need to review its endpoints and only keep those that make sense for {{oak-segment-tar}}, adapt the others as necessary any add further functionality as required. 

Specifically I think we should get rid of the time series for {{getRepositorySize()}} and {{getReclaimedSize()}}.

Also the name {{getRepositorySize()}} is confusing and we should change it. It leads callers to think it would return current size of the repository opposed to the size it had after the last cleanup. (There is {{FileStoreStatsMBean.getRepositorySize()}} for the latter.)"	OAK	Closed	3	3	3932	production
13116641	Cold standby performance regression due to segment caching	"The changes to the segment cache introduced in r1793527 [0] introduced a performance regression on the primary for the case in which a standby is attached to it. Below a benchmark duration comparison between primary w/o and w/ standby for r1793527 (after the segment cache changes) and r1793526 (before the changes) :

|Oak 1.6 r1793527 (20170502)|{noformat}
# BasicWriteTest                   C     min     10%     50%     90%     max       N
Oak-Segment-Tar                    1      19      21      22      26     160    2491
Oak-Segment-Tar-DS                 1      56      59      63      70     181     919
Oak-Segment-Tar-Cold(Shared DS)    1      58      66     159     177     372     302
{noformat}|
|Oak 1.6 r1793526 (20170502)|{noformat}
# BasicWriteTest                   C     min     10%     50%     90%     max       N
Oak-Segment-Tar                    1      19      21      22      25      52    2584
Oak-Segment-Tar-DS                 1      56      60      63      69     158     925
Oak-Segment-Tar-Cold(Shared DS)    1      57      60      64      70     122     915
{noformat}|

[0] https://github.com/apache/jackrabbit-oak/commit/efafa4e1710621b7f3b8e92d0b2681669185fcd4"	OAK	Closed	3	1	3932	cold-standby, performance, scalability
13136571	Remove deprecated deep option from check command	"With OAK-5595 we have enabled deep traversals by default when using the check command. At the same time we have deprecated the --{{deep}} option.

Since all these happened for {{1.8}}, the next logical step to do for {{1.10}} is to remove this option altogether."	OAK	Closed	4	4	3932	tooling
13020468	Backup is not incremental i.e. previous tar files are duplicated	Performing two backups via {{RepositoryManagementMBean.startBackup}}, directory size increases not only with the delta, but also again with the size of existing tar files. This lead me to the conclusion that backup is not incremental.	OAK	Open	4	4	3932	operations, production, tooling
13105674	Standby server should send timely responses to all client requests	"Currently all the {{GetXXXRequestHandler}} (where XXX stands for Blob, Head, References and Segment), on the server discard client requests which cannot be satisfied (i.e. the requested object does not exist (yet) on the server). A more transparent approach would be to timely respond to all client requests, clearly stating that the object was not found. This would improve a lot debugging for example, because all requests and their responses could be easily followed from the client log, without needing to know what actually happened on the server.

Below, a possible implementation for {{GetHeadRequestHandler}}, suggested by [~frm] in a comment on OAK-6678:

{noformat}
String id = reader.readHeadRecordId();

if (id == null) {
    ctx.writeAndFlush(new NotFoundGetHeadResponse(msg.getClientId(), id));
    return;
}

ctx.writeAndFlush(new GetHeadResponse(msg.getClientId(), id));
{noformat}
"	OAK	Open	4	4	3932	cold-standby
13039208	Allow filter paths for Check command	It would be good if the {{check}} command would allow for filtering on content path. This would help in quickly identifying what is the good revision of a specific broken node in cases of very large repos.	OAK	Closed	3	2	3932	tooling
13381686	Cold Standby SSL certificates should be configurable	"The cold standby is able to do SSL connections to the primary, but currently only using on-the-fly generated certificates. This means that data is transferred over an encrypted connection but there is no protection against a man in the middle yet.

With this issue we want to:
* make server and client certificates configurable
* optionally validate the client certificate
* optionally only allow matching subjects in client and server certificates "	OAK	Closed	3	4	3932	cold-standby
13099813	Add new segment-tar fixture for attaching a cold-standby to benchmarked primary	"If this fixture is chosen, a cold standby instance will be started, syncing with the primary every {{n}} seconds. All the benchmarks specified via {{[testcases]}} argument will be run on primary instance, and all statistics and reports will be linked to primary.

This could work similarly to {{Oak-Segment-Tar-DS}} and have dedicated options like {{--no-data-store}}, {{--private-data-store}} or {{--shared-data-store}}. "	OAK	Closed	4	4	3932	cold-standby
13040675	The check command should do deep traversals by default	"Only checking accessibility of the root nodes doesn't make much sense. Even more so because the file store automatically rolls back on startup if a root revision is not accessible. In terms of not doing full traversals, it is more interesting to restrict by path (aka OAK-5556).

The {{--deep}} option will still be accepted, but there will be a failure when it is specified. An explanation that full traversal is now done regardless of that option will be printed."	OAK	Closed	4	4	3932	tooling
13045164	Consistency check incorrectly fails for broken partial paths 	"To better explain the bug I'll describe the content of the revisions:
# Valid Revision
Adds child nodes {{a}}, {{b}}, {{c}}, {{d}}, {{e}}, {{f}} with various properties (blobs included)
# Invalid Revision
Adds child node {{z}} with some blob properties and then corrupts the {{NODE}} record holding {{z}}.
Now when the consistency check is run, it correctly detects that the second revision is broken, *marks the path {{/z}} as corrupt* and then continues checking the first valid revision. Because of a check introduced for OAK-5556 [1], which tries to validate the user provided absolute paths before checking them, the checker tries to check {{/z}} in the first revision, where of course it can't find it. Therefore the check incorrectly fails for this revision, although it shouldn't have to.

/cc [~mduerig], [~frm]"	OAK	Closed	3	1	3932	tooling
13301237	Improve azure archive recovery during startup	"During repository startup if archive directory is not closed properly, recovery will be performed. During that procedure, segents are copied to the backup directory and deleted from the source direcory, one by one.

It can create problems and negativelly impact other ongoing actiivties, which are accessing the same archive. This activity, for example, can be repository cloning in order to create new environment. 

Proposed patch, after creating backup is not deleting all segments from archive, but only segments which could not be recovered. 

[^proposal.patch]

 

+API change+

Proposed patch is changing major version of exported SPI package, org.apache.jackrabbit.oak.segment.spi.persistence.split.

 "	OAK	Closed	3	4	3932	Patch
13395812	Bump netty dependency from 4.1.52.Final to 4.1.66.Final	"io.netty : netty-codec : 4.1.52.Final sonatype-2021-0789

*Summary*:
 sonatype-2021-0789
 Explanation
 The netty-codec package contains a Buffer Overflow vulnerability. The finishEncode function in the Lz4FrameEncoder.class class incorrectly estimates the buffer size when writing a footer for the last header. An attacker could abuse this behavior by sending a payload to the flawed application that will overwrite contiguous memory chunks in the heap, resulting in a Denial of Service (DoS) condition or other unintended behavior.
 Detection
 The application is vulnerable by using this component.
 Recommendation
 We recommend upgrading to a version of this component that is not vulnerable to this specific issue.
 Note: If this component is included as a bundled/transitive dependency of another component, there may not be an upgrade path. In this instance, we recommend contacting the maintainers who included the vulnerable package. Alternatively, we recommend investigating alternative components or a potential mitigating control.
 Root Cause
 netty-codec-4.1.52.Final.jar <= io/netty/handler/codec/compression/Lz4FrameEncoder.class:[4.1.0.Beta2 , 4.1.66.Final)
 Advisories
 Project:
 [https://github.com/netty/netty/pull/11429]"	OAK	Closed	3	3	3932	vulnerability
13137875	Improve SegmentNodeStoreStats to include number of commits per thread and threads currently waiting on the semaphore	"When investigating the performance of  {{segment-tar}}, the source of the writes (commits) is a very useful indicator of the cause.

To better understand which threads are currently writing in the repository and which are blocked on the semaphore, we need to improve {{SegmentNodeStoreStats}} to:
 * expose the number of commits executed per thread
 * expose threads currently waiting on the semaphore"	OAK	Closed	3	4	3932	tooling
13085141	Add flag for controlling percentile of commit time used in scheduler	In OAK-4732 the 50th percentile of the last 1000 commits is used as wait time before returning the current root. In order to parametrise the value of the percentile, it would be nice to have a new feature flag, e.g. {{oak.scheduler.head.lockWaitPercentile}}. Setting it to {{0}} would basically disable this. Setting it to a different value might be interesting in future experiments.	OAK	Closed	4	4	3932	Performance, scalability
13112267	Segment-Tar-Cold fixture doesn't correctly set up standby blob store	When {{--shareDataStore}} option is used for {{Segment-Tar-Cold}}, the standby instance ends up without a blob store configured.	OAK	Closed	4	1	3932	cold-standby
13104227	Improve cold standby resiliency to incoherent configs	"In order to correctly configure cold standby there are two OSGi configurations that need to be provided. Among other settings, {{org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.config}} needs {{standby=B""true""}} and {{org.apache.jackrabbit.oak.segment.standby.store.StandbyStoreService.config}} needs {{mode=""standby""}}. The problem is that sometimes we have {{mode=""standby""}} in {{StandbyStoreService}} and {{standby=B""false""}} in {{SegmentNodeStoreService}} which leads to starting a problematic standby instance (with primary behaviour enabled, e.g. indexing, etc.). This problem stems from the fact that there are two components whose configuration should be coordinated. Proposals to mitigate this:

# Keep the {{mode=""standby""}}, but merge the configuration of {{StandbyStoreService}} in the one for {{SegmentNodeStoreService}} and eliminate {{StandbyStoreService}} altogether
# {{StandbyStoreService}} should derive {{mode=""standby""}} from {{""standby=B""true""}} in {{SegmentNodeStoreService}}
# {{SegmentNodeStoreService}} should derive {{""standby=B""true""}} from {{mode=""standby""}} in {{StandbyStoreService}} even if this is backwards when compared to how the synchronization currently happens, with {{StandbyStoreService}} waiting for for a proper initialisation of {{SegmentNodeStoreService}}
# Make {{StandbyStoreService}} configuration mandatory, but require a {{mode=""off""}} setting. This way the removal of {{standby=B""true""}} from {{SegmentNodeStoreService}} would be guaranteed and any synchronization between the two components would be avoided.

/cc  [~frm], [~volteanu], [~mduerig]
"	OAK	Open	3	4	3932	cold-standby, resilience
13375040	Improve oak-run compact to better support Azure compaction	Currently {{oak-run compact}} for Azure Segment Store always compacts the source container in place. It should be better to allow compacting the source container to a different container, allowing thus to skip source container cleanup. Moreover, in order to speed up compaction, Azure compaction should always employ a persistent disk cache, whose path for storing the segments and size should be configurable.	OAK	Closed	3	4	3932	tooling
13098847	Move BulkTransferBenchmark to oak-benchmarks module	{{BulkTransferBenchmark}} should be moved from {{oak-segment-tar}} to {{oak-benchmarks}} to allow standard run of this cold standby related benchmark.	OAK	Closed	4	3	3932	cold-standby
13041330	Test coverage for CheckCommand	We should add tests for {{o.a.j.o.r.CheckCommand}} in order to validate recent changes introduced by adding/removing options and their arguments (see OAK-5275, OAK-5276, OAK-5277, OAK-5595). There is also a new feature introduced by OAK-5556 (filter paths) and a refactoring in OAK-5620 which must be thoroughly tested in order to avoid regressions.	OAK	Closed	4	3	3932	tooling
13235965	oak-run check should have an option for specifying memory mapping	{{oak-run check}} currently uses memory mapping by default when building the {{FileStore}}. This setting should be configurable, to allow switching memory mapping off.	OAK	Closed	4	2	3932	tooling
13171335	SegmentNodeStore - sidegrade support between TarPersistence and AzurePersistence	"Azure support for segment-tar (OAK-6922) allowed us to plug another storage option for the segment store. Since sometimes there's the need to compare how local vs remote storage behaves, a sidegrade from local tar storage to remote azure storage must be implemented.

This would allow us to replicate the exact repository content, changing only the underlying storage mechanism. Analogous to OAK-7459, the Azure Segment Store connection details will be supplied in the following format:
 * an URI with the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider
 * a secret key supplied as an environment variable, i.e. _AZURE_SECRET_KEY._"	OAK	Closed	3	2	3932	azure, migration
13281162	Access azure segments metadata in a case-insensitive way	"We use azcopy to copy segments from one azure blob container to another for testing. There is a bug in the current version of azcopy (10.3.3), which makes all metadata keys start with a capital letter - ""type"" becomes ""Type"". As a consequence, the current implementation can not find the segments in the azure blob storage. 
 
The azcopy issue was already reported [1] in 2018. I have little hope that azcopy will be fixed soon.
 
Therefore I suggest a patch to oak-segment-azure, that would be backward compatible and ignore the case of the keys when reading metadata. We should be strict in what we write and tolerant in what we read.  "	OAK	Closed	3	4	3932	azureblob, candidate_oak_1_22
13101632	Standby server must always send the persisted head to clients	"Currently the standby server sends an un-persisted head record to clients. Under normal circumstances, the TarMK flush thread is able to persist it and its corresponding segment at a 5 seconds interval.
However, there are cases (uploading a very large blob > 10 GB) in which the flush thread writes the segment too late, and the 20s allowed by {{FileStoreUtil#readSegmentWithRetry}} are not enough. Therefore the server can't read the segment containing the head record and a timeout occurs on the client."	OAK	Closed	3	1	3932	cold-standby
13027472	"The check command defines a useless default value for the ""bin"" option"	The {{check}} command enables the traversal of binary properties via the {{--bin}} option. The user could provide a value for this option to specify the amount of bytes that should be traversed for every binary value. The default value for the {{--bin}} option is zero, effectively disabling the traversal of binary properties. Instead, if a value for this property is not specified, the tools should traverse the binary properties in their entirety. A value should be specified only to restrict the amount of bytes to traverse.	OAK	Closed	4	4	3932	tooling
12991135	Improve FileStore.size calculation	"A new approach for calculating {{FileStore::size}} is needed because this method is prone to lock contention and should not be called too often.

The steps to implement the approach are:
# reduce the lock surface of the size() method. This should be simple enough by creating a copy of the readers / writer inside the lock and do the actual size calculation on that snapshot but outside of the lock.
# lower size() visibility to package to avoid misuse (from monitoring tools)
# remove {{approximateSize}} and associated logic and replace it with {{size()}}.
"	OAK	Closed	4	3	3932	resilience
13027470	The check command should accept the path to the store as a positional argument	The {{check}} tool requires the path to the store to be specified. The path is passed to the tool via a required option {{--path}}. This way of specifying the path to the store is verbose for no good reason. It would be nicer if the path to the Segment Store would be specified via a positional argument instead.	OAK	Closed	4	4	3932	tooling
13176090	Introduce oak-run segment-copy for moving around segments in different storages	"Often there's the need to transform a type of {{SegmentStore}} (e.g. local TarMK) into *the exact same* counter-part, using another persistence type (e.g. Azure Segment Store). While {{oak-upgrade}} partially solves this through sidegrades (see OAK-7623), there's a gap in the final content because of the level at which {{oak-upgrade}} operates (node store level). Therefore, the resulting sidegraded repository doesn't contain all the (possibly stale, unreferenced) data from the original repository, but only the latest head state. A side effect of this is that the resulting repository is always compacted.

Introducing a new command in {{oak-run}}, namely {{segment-copy}}, would allow us to operate at a lower level (i.e. segment persistence), dealing only with constructs from {{org.apache.jackrabbit.oak.segment.spi.persistence}}: journal file, gc journal file, archives and archive entries. This way the only focus of this process would be to ""translate"" a segment between two persistence formats, without caring about the node logic stored inside (referenced/unreferenced node/property)."	OAK	Closed	3	4	3932	tooling
13102805	Syncing big blobs fails since StandbyServer sends persisted head	"With changes for OAK-6653 in place, {{ExternalPrivateStoreIT#testSyncBigBlog}} and sometimes {{ExternalSharedStoreIT#testSyncBigBlob}} are failing on CI:

{noformat}
org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT)  Time elapsed: 96.82 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
...
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT)  Time elapsed: 95.254 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

Partial stacktrace:
{noformat}
14:09:08.355 DEBUG [main] StandbyServer.java:242            Binding was successful
14:09:08.358 DEBUG [standby-1] GetHeadRequestEncoder.java:33 Sending request from client Bar for current head
14:09:08.359 DEBUG [primary-1] ClientFilterHandler.java:53  Client /127.0.0.1:52988 is allowed
14:09:08.360 DEBUG [primary-1] RequestDecoder.java:42       Parsed 'get head' message
14:09:08.360 DEBUG [primary-1] CommunicationObserver.java:79 Message 'get head' received from client Bar
14:09:08.362 DEBUG [primary-1] GetHeadRequestHandler.java:43 Reading head for client Bar
14:09:08.363 WARN  [primary-1] ExceptionHandler.java:31     Exception caught on the server
java.lang.NullPointerException: null
	at org.apache.jackrabbit.oak.segment.standby.server.DefaultStandbyHeadReader.readHeadRecordId(DefaultStandbyHeadReader.java:32) ~[oak-segment-tar-1.8-SNAPSHOT.jar:1.8-SNAPSHOT]
	at org.apache.jackrabbit.oak.segment.standby.server.GetHeadRequestHandler.channelRead0(GetHeadRequestHandler.java:45) ~[oak-segment-tar-1.8-SNAPSHOT.jar:1.8-SNAPSHOT]
{noformat}"	OAK	Closed	3	1	3932	cold-standby, resilience
13174984	Refactor AzureCompact and Compact	{{AzureCompact}} in {{oak-segment-azure}} follows closely the structure and logic of {{Compact}} in {{oak-segment-tar}}. Since the only thing which differs is the underlying persistence used (remote in Azure vs. local in TAR files), the common logic should be extracted in a super-class, extended by both. 	OAK	Open	3	4	3932	tech-debt, technical_debt, tooling
13131220	Race condition on revisions head between compaction and scheduler could result in skipped commit	"There is a race condition on {{TarRevisions#head}} between a running compaction trying to set the new head [0] and the scheduler doing the same after executing a specific commit [1]. If the compaction thread is first, then the head assignment in the scheduler will fail and not be re-attempted. 

IMO, the simple if statement should be changed to a while loop in which the head is refreshed and the commit is re-applied against the new head, before attempting again to set a new head in {{TarRevisions}}. This is somehow similar to what we previously had [2], but without the unneeded optimistic/pessimistic strategies involving tokens.

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/FileStore.java#L764
[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/scheduler/LockBasedScheduler.java#L253
[2] https://github.com/apache/jackrabbit-oak/blob/1.6/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentNodeStore.java#L686"	OAK	Closed	1	1	3932	scalability
13329558	oak-run explore should support Azure Segment Store	"{{oak-run explore}} should accept Azure URIs for the segment store in order to be able to browse azure segments. 

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._"	OAK	Closed	3	4	3932	tooling
12948135	Implement FileStore.size through FileStore.approximateSize	"{{FileStore.size()}} is prone to lock contention and should not be called too often. As OAK-2879 already introduced an approach for tracking the current size of the file store without having to lock, we might as well promote his to be ""the official"" implementation. 

[~frm] WDYT?"	OAK	Resolved	4	17	3932	resilience
13326424	PersistentRedisCache: failure to write segment is not an error	"Failure to write a segment to the redis cache results in an error level log message with a stack trace. However, this is expected behaviour: socket timeouts prevent the cache from effectively slowing down a request. OTOH too many socket timeouts make the cache ineffective, so it's good to have a way to log such errors when debugging. My suggestion is therefore to change the log level to ""debug"" and avoid the stack trace."	OAK	Closed	4	1	3932	patch
13144981	CommitsTracker data is always empty when exposed via JMX	"Due to duplicate registration of {{SegmentNodeStoreStats}} in both {{SegmentNodeStore}}  and {{LockBasedScheduler}}, we end up with two instances of this MBean. The former gets exposed via JMX and always returns empty tables for CommitsCountPerWriter and QueuedWriters, while the latter correctly tracks these data, but is not exposed. To address this, we should stick to only one instance of {{SegmentNodeStoreStats}}, used in both {{SegmentNodeStore}} and {{LockBasedScheduler}}.

While at this, two additional points to be addressed:
# {{CommitsTracker}} needs to be unit tested
# commits count map size needs to be configurable via {{SegmentNodeStoreStats}}"	OAK	Closed	3	1	3932	tooling
13172315	oak-run check should support Azure Segment Store	"{{oak-run check}} should accept Azure URIs for the segment store in order to be able to check for data integrity. This will come handy in the light of remote compacted segment stores and/or sidegraded remote segment stores (see OAK-7623, OAK-7459).

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._"	OAK	Open	3	4	3932	tooling
13017154	Remove the old estimation OSGi setting (compaction.gainThreshold)	"Currently, there are two implementations for finding out the gain in repository size after running compaction: the old one, {{CompactionGainEstimate}} and the new one, {{SizeDeltaGcEstimation}}. Similarly, there are also two configurations for customising them, in {{SegmentNodeStoreService}}, {{compaction.gainThreshold}} and {{compaction.sizeDeltaEstimation}}.

At the moment both of them are exposed as OSGi configurations, but only the new one should be exposed (e.g. {{compaction.sizeDeltaEstimation}}). 

It must be evaluated whether it makes sense to keep the logic associated with the old implementation."	OAK	Closed	4	4	3932	osgi-config
13081072	oak-run check should also check checkpoints 	{{oak-run check}} does currently *not* traverse and check the items in the checkpoint. I think we should change this and add an option to traverse all, some or none of the checkpoints. When doing this we need to keep in mind the interaction of this new feature with the {{filter}} option: the paths passed through this option need then be prefixed with {{/root}}. 	OAK	Closed	1	4	3932	tooling
13040625	"The check command doesn't do any check when ""deep"" option is not provided"	"When the {{check}} command is used without {{--deep}} option, there is no check/traversal being done against the repository.

First relevant line in code is [1], where a check is supposed to happen, but due to a mismatch between argument expected/argument provided, {{null}} is always returned without checking anything. The method which should do the actual check [2] expects a set of paths to be traversed, but this set is always empty. Therefore, relevant code for running the check is never executed [3].

[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L120
[2] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L183
[3] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L194"	OAK	Closed	3	1	3932	tooling
13264800	Oak run check command must return the status of repository consistency check	"Currently the consistency check reports only if the command runs successfully (return code 0) or fails (return code 1).
Into this logic will also add the status of repository consistency:
- checking only the last revision: will return 0 if the revision is consistent and the command runs successfully OR will return 1 if the revision is inconsistent or job did not run successfully (some errors/exception were encountered during the run)
- checking multiple revisions: will return 0 if at least one revision is consistent and the job runs successfully OR will return 1 if none of the revisions are consistent or the command did not run successfully (some errors/exception were encounter during the run) "	OAK	Closed	3	4	3932	oak-run, segment-tar
13041683	Simplify consistency check	"The current implementation of the consistency check ({{ConsistencyChecker}},{{CheckCommand}})
is cluttered with unnecessary checks regarding deprecated arguments of the {{check}} command. 
With OAK-5595, deep traversals are enabled by default, therefore the code needs to be revised to take this into account. The same applies to the argument taken by {{--bin}} option, which was removed in OAK-5604.

Moreover, {{ConsistencyChecker}} could be refactored in order to better distinguish when:
* a full path at the given revision is checked
* a node and its properties are checked
* a node and its descendants are checked"	OAK	Closed	3	4	3932	tooling
13562333	Bump netty dependency from 4.1.96.Final to 4.1.104.Final	"*File Matche(s):*
/netty-common-4.1.96.Final.jar

*Vulnerabilitie(s)*
This artifact embeds Netty Project 4.1.96.Final which contains the following vulnerabilitie(s):

*BDSA-2023-2732/CVE-2023-44487* in version 4.1.96.Final (CVSS 7.5 High): The HTTP/2 protocol contains a flaw related to the stream multiplexing feature that can allow for excessive resource consumption on servers operating implementations of the HTTP/2 protocol. The HTTP/2 protocol allows clients to signal to a server to cancel a previously opened stream by sending an `RST_STREAM` frame. Attackers can abuse this stream canceling ability by opening a large number of streams at once immediately followed by `RST_STREAM` frames. In most HTTP/2 implementations this bypasses concurrent open stream limits and causes servers to spend processing time first handling request frames and then performing stream tear downs. For the server, these operations can pile up whereas the attacker client paid minuscule bandwidth and processing costs. [Amazon](https://aws.amazon.com/security/security-bulletins/AWS-2023-011/), [Cloudflare](https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/) and [Google](https://cloud.google.com/blog/products/identity-security/google-cloud-mitigated-largest-ddos-attack-peaking-above-398-million-rps/) have reported that this vulnerability has been exploited in the wild from August to October 2023. This vulnerability is listed as exploitable by the Cybersecurity & Infrastructure Security Agency in their [Known Exploited Vulnerabilities Catalog](https://www.cisa.gov/known-exploited-vulnerabilities-catalog)."	OAK	Closed	3	3	3932	vulnerability
13041411	"The check command should accept a non-argument ""bin"" option for checking binaries"	"Currently the {{--bin}} option expects a {{Long}} argument, as the {{LENGTH}} up to which to scan the content of binary properties. The {{--bin}} option should be simplified so that it doesn't take any arguments. Running {{check}} without the {{--bin}} flag won't scan any binary properties, while including {{--bin}} option will scan all binaries, no matter their size.

If an argument is given with {{--bin}}, there will be a failure and a warning will be displayed.

The message displayed at the end of the consistency check will be changed to take into account whether binary properties were traversed or not."	OAK	Closed	4	4	3932	tooling
13062144	Cleanup blocks writers	"The refactoring from OAK-6002 moved the cleanup of the tar readers into the read lock, which blocks concurrent writers from progressing. This was a problem with {{oak-segment}} before and fixed with OAK-3329.
As cleanup can take up to a couple of minutes on busy system we should re-establish the former behaviour. "	OAK	Closed	3	4	4399	cleanup, gc
13070389	IllegalStateException when closing the FileStore during garbage collection	"When the file store is shut down during gc compaction is properly aborted. Afterwards it will trigger a cleanup cycle though, which runs concurrently to the proceeding shutdown potentially causing an {{ISE}}:

{noformat}
at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
at org.apache.jackrabbit.oak.segment.file.TarWriter.close(TarWriter.java:333)
at org.apache.jackrabbit.oak.segment.file.TarWriter.createNextGeneration(TarWriter.java:376)
at org.apache.jackrabbit.oak.segment.file.FileStore.newWriter(FileStore.java:682)
at org.apache.jackrabbit.oak.segment.file.FileStore.access$1700(FileStore.java:100)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.cleanup(FileStore.java:1069)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.cleanupGeneration(FileStore.java:1195)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.run(FileStore.java:803)
at org.apache.jackrabbit.oak.segment.file.FileStore.gc(FileStore.java:387)
{noformat}
"	OAK	Closed	3	1	4399	gc
13013452	Test failure: BrokenNetworkTest	"On some machines the {{BrokenNetworkTest}} fails:

{noformat}
Failed tests:
  BrokenNetworkTest.testProxyFlippedEndByteSSL:103->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedIntermediateByte:88->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedIntermediateByteSSL:93->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedStartByte:78->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySSLSkippedBytes:63->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySSLSkippedBytesIntermediateChange:73->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySkippedBytesIntermediateChange:68->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

Stack traces are all similar to 
{noformat}
testProxySkippedBytesIntermediateChange(org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest)  Time elapsed: 5.577 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.useProxy(BrokenNetworkTest.java:146)
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.useProxy(BrokenNetworkTest.java:113)
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.testProxySkippedBytesIntermediateChange(BrokenNetworkTest.java:68)
{noformat}"	OAK	Closed	3	1	4399	test-failure
12976870	Release oak-segment-tar	"Tweak our setup in order to be able to cut an initial release of {{oak-segment-tar}} and perform the release. 

"	OAK	Resolved	3	3	4399	release
13113323	Background threads might not be automatically restarted	"The background threads used in {{FileStore}} are implemented by wrapping {{Runnable}} instances in {{SafeRunnable}}, and by handing the {{SafeRunnable}} instances over to a {{ScheduledExecutorService}}. 

The documentation of {{ScheduledExecutorService#scheduleAtFixedRate}} states that ""if any execution of the task encounters an exception, subsequent executions are suppressed"". But a {{SafeRunnable}} always re-throws any {{Throwable}} that it catches, effectively preventing itself from executing again in the future.

There is more than one solution to this problem. One of these is to never re-throw any exception. Even if it doesn't always make sense, e.g. in case of an {{OutOfMemoryError}}, never re-throwing an exception would better fulfil the assumption that background threads should always be up and running even in case of error."	OAK	Closed	2	1	4399	resilience
12768418	Oak API remoting	"Container issues for collecting tasks related to remoting the Oak API. Such a remoting should be:

* stateless on the Oak side apart from the apparent persisted state in the content repository, 

* independent from {{oak-jcr}}, but reusing JCR related plugins from {{oak-core}} as required (e.g. for name space and node type handling),

* agnostic of any protocol bindings,

* ..."	OAK	Resolved	3	3	4399	api, remoting
13115540	Log SegmentStore size at startup	"It would be useful if we can log the segmentstore size at time of startup. FileStore already computes the size to initialize the FileStoreStats so we just need to log it

This size often help when customer report issues and provide log files"	OAK	Closed	4	4	4399	production
12929232	Move integration tests in a different Maven module	"While moving the Segment Store and related packages into its own bundle, I figured out that integration tests contained in {{oak-core}} contribute to a cyclic dependency between the (new) {{oak-segment}} bundle and {{oak-core}}.

The dependency is due to the usage of {{NodeStoreFixture}} to instantiate different implementations of {{NodeStore}} in a semi-transparent way.

Tests depending on {{NodeStoreFixture}} are most likely integration tests. A clean solution to this problem would be to move those integration tests into a new Maven module, referencing the API and implementation modules as needed."	OAK	Closed	3	4	4399	modularization, technical_debt
12960273	Update segment tooling to choose target store	We need to add command line options segment specific tooling so users could chose between {{oak-segment}} and {{oak-segment-next}}. {{oak-segment}} should be the default until deprecated, where {{oak-segment-next}} should be made the default. 	OAK	Closed	1	3	4399	tooling
12832746	Review and improve Oak and Jcr repository setup	"There is the {{Oak}} and {{Jcr}} builder classes for setting up Oak and Jcr repositories. Both builders don't have clear semantics regarding the life cycle of the individual components they register. On top of that the requirements regarding those life cycles differ depending on whether the individual components run within an OSGi container or not. In the former case the container would already manage the life cycle so the builder should not. 

IMO we should specify the builders to only be used for non OSGi deployments and have the manage the life cycles of the components they instantiate. OTOH for OSGi deployments we should leverage OSGi subsystems to properly set things up."	OAK	Resolved	3	4	4399	modularization, technical_debt
13009491	Missing BlobGarbageCollection MBean on standby instance	"The {{BlobGarbageCollection}} MBean is no longer available on a standby instance, this affects non-shared datastore setups (on a shared datastore you'd only need to run blob gc on the primary).
This change was introduced by OAK-4089 (and backported to 1.4 branch with OAK-4093)."	OAK	Closed	3	1	4399	gc
12919998	Move the Segment Store into its own bundle	This epic tracks the work done to move the Segment Store into an independent bundle, detached from oak-core.	OAK	Closed	3	15	4399	modularization, technical_debt
13061396	Add TarFiles to the architecture diagram	The newly created {{TarFiles}} should be added to the architecture diagram for oak-segment-tar.	OAK	Closed	3	4	4399	documentation
12842186	RemoteServerIT failing due to address already in use	"Some of RemoteServerIT failing with Address already in use. Possibly the test setup needs to be changed to use random available port 

{noformat}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:291)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServer.start(RemoteServer.java:54)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.setUp(RemoteServerIT.java:134)
{noformat}

[1] https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/236/"	OAK	Closed	4	1	4399	CI, Jenkins
12960367	Implement fixtures for running again oak-segment and/or oak-segment-next	"We need fixtures to run UTs / ITs against either or both segment implementations {{oak-segment}} and {{oak-segment-next}}. 

Ideally we can enable them individually through e.g. environment variables. A standard build would run against {{oak-segment}} so not to affect others. {{oak-segment-next}} could be enabled on request locally or for the CI. 
Once we deprecate {{oak-segment}} we would switch the default fixture to {{oak-segment-next}}. "	OAK	Closed	1	3	4399	testing
13023742	Get rid of test dependency to json.org JSON parser	"See <http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201611.mbox/%3C0CE2E8C9-D9B7-404D-93EF-A1F8B07189BF%40apache.org%3E>

"	OAK	Closed	1	3	4399	legal
12955262	Too verbose logging during revision gc	"{{FileStore.cleanup}} logs the segment id of any forward reference found when including those in the reference graph. The logged information can amount to several MBs impacting normal operation. Furthermore the actually reclaimed segments are logged, which also makes the log files explode. Finally the processing of the references and individual tar files might be too wordy. 

"	OAK	Closed	1	4	4399	cleanup, gc, logging
12902115	Replace BackgroundThread with Scheduler	I think we should replace the background thread with some kind of a scheduler. The goal would be to decouple threading from scheduling. IMO threads should not be managed by the application but by the container. 	OAK	Closed	3	17	4399	technical_debt
13027713	The tarmkdiff command does too many things	"The {{tarmkdiff}} command is actually the combination of two commands. 

The first command, activated when the {{\-\-list}} flag is specified, list available revisions in the Segment Store. For this command, only the {{\-\-output}} option is relevant. If other options are specified, they are ignored.

The second command is the proper logic of {{tarmkdiff}}. This logic is activated only if the {{\-\-list}} flag is not specified. For this command, every option on the command line is relevant.

The logic listing available revisions in the Segment Store should be encapsulated in its own command, without cluttering the CLI of {{tarmkdiff}}."	OAK	Resolved	4	4	4399	tooling
13115677	Unknown channel option 'TCP_NODELAY' for channel warning in cold standby	"After the netty upgrade in OAK-6564, there's a recurring warning appearing in the server thread:
{noformat}
18:54:44.691 [main] WARN  io.netty.bootstrap.ServerBootstrap - Unknown channel option 'TCP_NODELAY' for channel '[id: 0xa64bc5c4]'
{noformat}

We need to see what's causing it (i.e. was that option removed in the latest version? if yes, is there a substitute/change needed?).

/cc [~frm]"	OAK	Closed	4	1	4399	cold-standby
12849891	Extend documentation for SegmentNodeStoreService in http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore	"Currently the documentation at http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore only documents the properties
# repository.home and
# tarmk.size
All the other properties like customBlobStore, tarmk.mode, .... are not documented. Please extend that. Also it would be good, if the table could be extended with what type is supported for the individual properties."	OAK	Closed	3	17	4399	documentation
13107238	Exceptions are inhibited in oak-run compact	"Exceptions thrown by {{oak-run compact}} are inhibited so the exit code of the command is not correct in case of error. 

Example: 
{code}
$ java -jar oak-run-1.7.8-R1809845.jar compact test-oak-run/
Apache Jackrabbit Oak 1.7.8-R1809845
Compacting test-oak-run-6.3.0
With default access mode
    before
        Thu Oct 05 15:14:22 CEST 2017, journal.log
        Thu Oct 05 15:14:23 CEST 2017, data00000a.tar
        Thu Oct 05 15:14:23 CEST 2017, manifest
        Thu Oct 05 15:14:23 CEST 2017, repo.lock
    size 119.1 MB (119133142 bytes)
    -> compacting
org.apache.jackrabbit.oak.segment.file.InvalidFileStoreVersionException: Using a too recent version of oak-segment-tar
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkStoreVersion(ManifestChecker.java:81)
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkManifest(ManifestChecker.java:70)
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkAndUpdateManifest(ManifestChecker.java:51)
	at org.apache.jackrabbit.oak.segment.file.FileStore.<init>(FileStore.java:191)
	at org.apache.jackrabbit.oak.segment.file.FileStoreBuilder.build(FileStoreBuilder.java:343)
	at org.apache.jackrabbit.oak.segment.tool.Compact.newFileStore(Compact.java:165)
	at org.apache.jackrabbit.oak.segment.tool.Compact.compact(Compact.java:135)
	at org.apache.jackrabbit.oak.segment.tool.Compact.run(Compact.java:128)
	at org.apache.jackrabbit.oak.run.SegmentTarUtils.compact(SegmentTarUtils.java:183)
	at org.apache.jackrabbit.oak.run.CompactCommand.execute(CompactCommand.java:93)
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
    after
        Thu Oct 05 15:14:22 CEST 2017, journal.log
        Thu Oct 05 15:14:23 CEST 2017, data00000a.tar
        Thu Oct 05 15:14:23 CEST 2017, manifest
        Thu Oct 05 15:14:23 CEST 2017, repo.lock
    size 119.1 MB (119133142 bytes)
    removed files []
    added files []
Compaction succeeded in 211.9 ms (0s).
{code}

A quick fix would be to wrap the exception into a {{RuntimeException}}:
{code}
--- a/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/tool/Compact.java
+++ b/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/tool/Compact.java
@@ -127,7 +127,7 @@ public class Compact implements Runnable {
         try {
             compact();
         } catch (Exception e) {
-            e.printStackTrace();
+            throw new RuntimeException(""Failed to run compact"", e);
         }
     }
{code}"	OAK	Closed	3	1	4399	compaction, gc, tooling
12831410	RepositoryImpl should not manage the lifecycle of ContentRepository	"{{RepositoryImpl}} uses an instance of {{ContentRepository}} that is passed as an external dependency in its constructor.

{{RepositoryImpl}} is not responsible for the creation of the {{ContentRepository}} instance and, as such, should not manage its lifecycle. In particular, the {{ContentRepository#close}} method should not be called when the {{RepositoryImpl#shutdown}} method is executed."	OAK	Resolved	3	7	4399	modularization, resilience, technical_debt
12821153	Oak builder changes its state during repository creation	"The {{Oak#createContentRepository()}} method changes the state of the builder at every invocation. In particular, it always adds a new {{CommitHook}}.

The observable behavior is that all the {{IndexEditor}} instances are executed twice when the {{Oak}} and {{Jcr}} builders are used together - i.e. when both an instance of {{Repository}} and {{ContentRepository}} are needed."	OAK	Resolved	3	7	4399	technical_debt
13063204	Assign meaningful names to cold standby threads	For easier understanding of the cold standby behaviour, threads used by the cold standby implementations should be assigned more human readable names.	OAK	Closed	3	4	4399	cold-standby
13181865	CheckCommand should consistently use an alternative journal if specified	"Callers of the {{check}} command can specify an alternative journal with the {{\-\-journal}} option. This option instructs the {{ConsistencyChecker}} to check the revisions stored in that file instead of the ones stored in the default {{journal.log}}.

I spotted at least two problems while using {{\-\-journal}} on a repository with a corrupted {{journal.log}} that didn't contain any valid revision.

First, the path to the {{FileStore}} is validated by {{FileStoreHelper#isValidFileStoreOrFail}}, which checks for the existence of a {{journal.log}} in the specified folder. But if a {{journal.log}} doesn't exist and the user specified a different journal on the command line this check should be ignored.

Second, when opening the {{FileStore}} the default {{journal.log}} is scanned to determine the initial revision of the head state. If a user specifies an alternative journal on the command line, that journal should be used instead of the default {{journal.log}}. It might be that the default journal contains no valid revision, which would force the system to crash when opening a new instance of {{FileStore}}."	OAK	Closed	3	1	4399	technical_debt
13152910	Contribute a 'proc' subtree for the Segment Node Store	"With guidance from [~mduerig], I recently developed a way to expose Segment Node Store's internal information through the NodeState API. 

The concept is similar in spirit to the proc file system in Linux: the proc subtree exposes internal information in a straightforward manner, enabling consumers to rely on a well-understood API to access the data. This proc subtree shelters tooling from variations of the internal APIs of the Segment Store. As long as the data exported through the proc subtree is stable, the same tools are going to work across different versions of the Segment Store with minimal to no modifications.

The proc subtree has been developed in [this branch on GitHub|https://github.com/francescomari/jackrabbit-oak/tree/proc]. I created this issue in order to review the work done so far, and to track the contribution of the proc subtree in Oak."	OAK	Closed	3	4	4399	tooling
13062916	Test failure: StandbyTestIT.testSyncLoop	"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

The build Jackrabbit Oak #144 has failed.
First failed run: [Jackrabbit Oak #144|https://builds.apache.org/job/Jackrabbit%20Oak/144/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/144/console]

Error Message

{code}
expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
{code}

Stacktrace

{code}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
    at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:126)
{code}

Standard Output

[^stdout.log]

Also failed at https://builds.apache.org/job/Jackrabbit%20Oak/394/"	OAK	Closed	3	1	4399	CI, flaky-test, jenkins
13061466	Test failure: CompactionAndCleanupIT.concurrentCleanup	"That test fails for me on every 2nd run or so. This seems to be a regression introduced with OAK-6002. 

{code}
org.junit.ComparisonFailure: Expected nothing to be cleaned but generation 'b' for file data00002b.tar indicates otherwise. 
Expected :a
Actual   :b


at org.junit.Assert.assertEquals(Assert.java:115)
org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.concurrentCleanup(CompactionAndCleanupIT.java:1252)
{code}

[~frm], could you have a look?

"	OAK	Closed	3	1	4399	gc, test-failure
13098793	Improve resource management in BulkTransferBenchmark	BulkTransferBenchmark might improperly dispose of test resources if error conditions occur. This is mostly due to improper resource tracking and finalization in BenchmarkBase, but similar mistakes have been made in BulkTransferBenchmark too.	OAK	Closed	3	1	4399	cold-standby
13113043	The compaction estimator should take the compaction type (tail vs. full) into consideration	"Currently the compaction estimator unconditionally looks at the growth of the repository since the last compaction run. This turn out to be not optimal when interleaving tail and full compaction. It would be better to have the estimator look at the growth of the repository since last full compaction when running full compaction. 

cc [~frm]"	OAK	Closed	2	4	4399	compaction, gc
13172835	Revert changes done by OAK-6770	With the changes from OAK-6770 applied, it seems that {{repository.home}} attribute is not correctly set, causing the repository to be written one level up in the directory hierarchy from where it was supposed to. 	OAK	Resolved	3	17	4399	osgi
12715514	"ISE: ""Unexpected value record type: f2"" is thrown when FileBlobStore is used"	"The stacktrace of the call shows something like
{code}
20.05.2014 11:13:07.428 *ERROR* [OsgiInstallerImpl] com.adobe.granite.installer.factory.packages.impl.PackageTransformer Error while processing install task.
java.lang.IllegalStateException: Unexpected value record type: f2
at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.length(SegmentBlob.java:101)
at org.apache.jackrabbit.oak.plugins.value.BinaryImpl.getSize(BinaryImpl.java:74)
at org.apache.jackrabbit.oak.jcr.session.PropertyImpl.getLength(PropertyImpl.java:435)
at org.apache.jackrabbit.oak.jcr.session.PropertyImpl.getLength(PropertyImpl.java:376)
at org.apache.jackrabbit.vault.packaging.impl.JcrPackageImpl.getPackage(JcrPackageImpl.java:324)
{code}

The blob store was configured correctly and according to the log also correctly initialized
{code}
20.05.2014 11:11:07.029 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService Initializing SegmentNodeStore with BlobStore [org.apache.jackrabbit.oak.spi.blob.FileBlobStore@7e3dec43]
20.05.2014 11:11:07.029 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService Component still not activated. Ignoring the initialization call
20.05.2014 11:11:07.077 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK opened: crx-quickstart/repository/segmentstore (mmap=true)
{code}

Under which circumstances can the length within the SegmentBlob be invalid?
This only happens if a File Blob Store is configured (http://jackrabbit.apache.org/oak/docs/osgi_config.html). If a file datastore is used, there is no such exception."	OAK	Closed	3	1	4399	resilience
12774256	Provide initial implementation of the Remote Operations specification	"To provide something that can be played with, and to verify the feasibility of the specification draft, an initial implementation of the HTTP API should be provided.

The API should follow the general behavior described [here|https://wiki.apache.org/jackrabbit/frm/RemoteOperations] and the HTTP semantics defined [here|https://wiki.apache.org/jackrabbit/frm/HttpOperations]. "	OAK	Closed	4	7	4399	api, remoting
12907331	o.a.j.o.api should not depend on Guava	The o.a.j.o.api has multiple dependencies on classes exported by the Google Guava bundle. The the o.a.j.o.api package should be made independent from Google Guava.	OAK	Closed	3	4	4399	technical_debt
13061085	Remove segment graph functionality from oak-run	"We could probably remove the segment graph functionality from oak-run. This has been implemented mainly (and solely?) for the purpose of analysing the problems around OAK-3348 and I assume it would quickly start falling behind as we move forward. Also for this kind of analysis I have switched to [oak-script|https://github.com/mduerig/script-oak], which is far more flexible. 

Let's decide closer to cutting 1.8 how to go forward here."	OAK	Closed	3	4	4399	technical_debt, tooling
13185131	deadlock TarMK flush, lucene	"We are getting the following deadlock. Please help! (Production environment)

I have already annotated possible locks and synchronized blocks in between:

{noformat}
""TarMK flush [/opt/condat/epet9/sling/repository/segmentstore]"":
  waiting to lock Monitor@0x00007fedfc00cc28 (Object@0x00000004795519a8, a org/apache/jackrabbit/oak/segment/SegmentId),
  which is held by ""oak-lucene-14""
""oak-lucene-14"":
 waiting for ownable synchronizer 0x00000003c13818c0, (a java/util/concurrent/locks/ReentrantReadWriteLock$NonfairSync),
 which is held by ""TarMK flush [/opt/condat/epet9/sling/repository/segmentstore]""

Thread 28883: (state = BLOCKED)
 - org.apache.jackrabbit.oak.segment.SegmentId.getSegment() @bci=12, line=121 (Compiled frame)
        synchronized (this) 
 - org.apache.jackrabbit.oak.segment.Record.getSegment() @bci=4, line=70 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.BlockRecord.read(int, byte[], int, int) @bci=49, line=57 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.read(byte[], int, int) @bci=314, line=189 (Compiled frame)
 - com.google.common.io.ByteStreams.read(java.io.InputStream, byte[], int, int) @bci=43, line=828 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[], int, int) @bci=4, line=695 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[]) @bci=5, line=676 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.getString() @bci=93, line=103 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.readString(int) @bci=189, line=524 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=15, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=37, line=167 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.AbstractFileStore$4.consume(int, org.apache.jackrabbit.oak.segment.RecordType, int) @bci=24, line=354 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.forEachRecord(org.apache.jackrabbit.oak.segment.Segment$RecordConsumer) @bci=48, line=716 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.AbstractFileStore.populateTarBinaryReferences(org.apache.jackrabbit.oak.segment.Segment, org.apache.jackrabbit.oak.segment.file.TarWriter) @bci=25, line=349 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(org.apache.jackrabbit.oak.segment.SegmentId, byte[], int, int) @bci=136, line=657 (Compiled frame)
        fileStoreLock.writeLock().lock(); Zeile 639
        bis: populateTarBinaryReferences
 - org.apache.jackrabbit.oak.segment.SegmentBufferWriter.flush() @bci=383, line=383 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.flush() @bci=165, line=148 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentWriter.flush() @bci=4, line=143 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$7.call() @bci=7, line=373 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$7.call() @bci=1, line=370 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.TarRevisions.doFlush(java.util.concurrent.Callable) @bci=25, line=224 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.TarRevisions.flush(java.util.concurrent.Callable) @bci=42, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.flush() @bci=20, line=370 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$2.run() @bci=15, line=233 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.SafeRunnable.run() @bci=21, line=67 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.runAndReset() @bci=47, line=308 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask) @bci=1, line=180 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() @bci=37, line=294 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)

Locked ownable synchronizers:
    - <0x00000003c1361228>, (a java/util/concurrent/locks/ReentrantLock$NonfairSync)
    - <0x00000003c13818c0>, (a java/util/concurrent/locks/ReentrantReadWriteLock$NonfairSync)
    - <0x00000003c1c3a0d8>, (a java/util/concurrent/ThreadPoolExecutor$Worker)

Thread 31035: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=836 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(int) @bci=83, line=967 (Interpreted frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(int) @bci=10, line=1283 (Compiled frame)
 - java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock() @bci=5, line=727 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$8.call() @bci=158, line=567 (Compiled frame)
    fileStoreLock.readLock().lock();
 - org.apache.jackrabbit.oak.segment.file.FileStore$8.call() @bci=1, line=542 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(org.apache.jackrabbit.oak.segment.SegmentId, java.util.concurrent.Callable) @bci=1, line=95 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(org.apache.jackrabbit.oak.segment.SegmentId) @bci=14, line=542 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentId.getSegment() @bci=38, line=125 (Compiled frame)
    synchronized (this) 
 - org.apache.jackrabbit.oak.segment.Record.getSegment() @bci=4, line=70 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.BlockRecord.read(int, byte[], int, int) @bci=49, line=57 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.read(byte[], int, int) @bci=314, line=189 (Compiled frame)
 - com.google.common.io.ByteStreams.read(java.io.InputStream, byte[], int, int) @bci=64, line=833 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[], int, int) @bci=4, line=695 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[]) @bci=5, line=676 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.getString() @bci=93, line=103 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.readString(int) @bci=189, line=524 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=15, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.length() @bci=124, line=115 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.<init>(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeBuilder, java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$BlobFactory) @bci=204, line=409 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.<init>(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeBuilder, java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$BlobFactory) @bci=25, line=589 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.fileLength(java.lang.String) @bci=64, line=176 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.copyFilesToLocal(org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory$CORFileReference, boolean, boolean) @bci=195, line=214 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.prefetchIndexFiles() @bci=96, line=170 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.<init>(org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier, org.apache.lucene.store.Directory, org.apache.lucene.store.Directory, boolean, java.lang.String, java.util.concurrent.Executor) @bci=85, line=81 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.wrapForRead(java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.lucene.store.Directory, java.lang.String) @bci=35, line=122 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.reader.DefaultIndexReaderFactory.createReader(org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.jackrabbit.oak.spi.state.NodeState, java.lang.String, java.lang.String, java.lang.String) @bci=61, line=102 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.reader.DefaultIndexReaderFactory.createReaders(org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.jackrabbit.oak.spi.state.NodeState, java.lang.String) @bci=20, line=61 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.plugins.index.lucene.reader.LuceneIndexReaderFactory, org.apache.jackrabbit.oak.plugins.index.lucene.hybrid.NRTIndexFactory) @bci=17, line=68 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker$1.leave(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=30, line=132 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=66, line=153 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.MapRecord.compare(org.apache.jackrabbit.oak.segment.MapRecord, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=197, line=415 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentNodeState.compareAgainstBaseState(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=909, line=608 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=43, line=148 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.MapRecord.compare(org.apache.jackrabbit.oak.segment.MapRecord, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=400, line=457 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentNodeState.compareAgainstBaseState(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=909, line=608 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(org.apache.jackrabbit.oak.spi.commit.Editor, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=34, line=52 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.diffAndUpdate(org.apache.jackrabbit.oak.spi.state.NodeState) @bci=140, line=142 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.update(org.apache.jackrabbit.oak.spi.state.NodeState) @bci=36, line=113 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call() @bci=79, line=135 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call() @bci=1, line=128 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Compiled frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)

Locked ownable synchronizers:
    - <0x0000000476194f30>, (a java/util/concurrent/ThreadPoolExecutor$Worker)   

{noformat}

 "	OAK	Closed	1	1	4399	deadlock
13029400	Improve code coverage of oak-segment-tar	Improve code coverage of oak-segment-tar.	OAK	Closed	3	4	4399	technical_debt
12768421	Support continuable sessions 	"Implement support for continuable sessions to keeps state across multiple client/server interactions. Continuable sessions do not require any additional state on the server (i.e. Oak) apart form the apparent repository state. 

To continue a session a client would obtain a continuation token from the current session. This token can be used on the next call to {{Repository.login}} to obtain a new {{Session}} instance that is based on the same repository revision that the session the token was obtained from. Additionally the token could contain information re. authentication so subsequent request can go through a simplified authentication procedure. ([~asanso]'s work on OAuth might be of help here.)

Transient changes are not supported in continuable sessions. Obtaining a continuation token from a session with transient changes results in an error. 

Continuable sessions are typically short lived (i.e. the time of a single HTTP request). Specifically continuable session do not retain the underlying repository revision from being garbage collected. Clients need to be able to cope with respective exceptions. 



"	OAK	Resolved	3	7	4399	api
12833739	Limit the scope of exported packages	"Oak currently exports *a lot* of packages even though those are only used by Oak itself. We should probably leverage OSGi subsystems here and only export the bare minimum to the outside world. This will simplify evolution of Oak internal APIs as with the current approach changes to such APIs always leak to the outside world. 

That is, we should have an Oak OSGi sub-system as an deployment option. Clients would then only need to deploy that into their OSGi container and would only see APIs actually meant to be exported for everyone (like e.g. the JCR API). At the same time Oak could go on leveraging OSGi inside this subsystem.

cc [~bosschaert] as you introduced us to this idea. "	OAK	Resolved	3	7	4399	modularization, osgi, technical_debt
13038925	Document TarMK design	"We should improve our documentation of the internal design of the the TarMK. There is currently a [single section|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#design]. 

* Add a high level class diagram and description of the overall structure of the TarMK. 
* Decide what to do with {{segmentmk.md}}. My preference would be to incorporate everything from it we didn't cover so far into {{segment/overview.md}}, {{segment/records.md}} and {{segment/tar.md}}. 
* Rewrite, clarify the design section in {{segment/overview.md}}. 
"	OAK	Closed	3	17	4399	documentation
12941339	Some classes from o.a.j.o.plugins.segment.compaction should be exported	Classes {{org.apache.jackrabbit.oak.plugins.segment.compaction.CompactionStrategy}} and {{org.apache.jackrabbit.oak.plugins.segment.compaction.CompactionStrategyMBean}} should be exported. The former is used in the public API of multiple classes from {{org.apache.jackrabbit.oak.plugins.segment.file}} and {{org.apache.jackrabbit.oak.plugins.segment}}, while the latter is used as interface type for a service registered in the whiteboard.	OAK	Resolved	3	1	4399	technical_debt
13101295	test failure seen in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT	"{noformat}
Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@6bb75258
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 18.543 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
offRCUpgradesSegments(org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT)  Time elapsed: 5.578 sec  <<< FAILURE!
java.lang.AssertionError: Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.checkSegmentVersion(UpgradeIT.java:143)
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments(UpgradeIT.java:109)


Results :

Failed tests:
  UpgradeIT.offRCUpgradesSegments:109->checkSegmentVersion:143 Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
{noformat}
"	OAK	Closed	1	1	4399	test-failure, windows
12819364	NPE when calling Event.getInfo()	On a very busy site, we're observing an NPE in the code that should gather information about a JCR event for our custom event handler. 	OAK	Resolved	3	1	4399	observation
13006821	Missing export for org.apache.jackrabbit.oak.backup package	"Oak segment tar does not export {{org.apache.jackrabbit.oak.backup}}, which makes backup restore functionality unavailable from OSGi containers. 

Instead of just exporting this package I think we should:
* Separate API from implementation
* Add semantic versioning to the exported API

/cc [~frm]"	OAK	Closed	3	1	4399	API, OSGi
13006454	Document storage format changes	"This issue serves as collection of all changes to the storage format introduced with  Oak Segment Tar and their impact. Once sufficiently stabilised this information should serve as basis for the documentation in {{oak-doc}}. 

|| Change || Rational || Impact || Migration || Since || Issues ||
|Generation in segment header |Required to unequivocally determine the generation of a segment during cleanup. Segment retention time is given in number of generations (2 by default). |No performance, space impact expected |offline |0.0.2 |OAK-3348 | 
|Stable id for node states |Required to efficiently determine equality of node states. This can be seen as an intermediate step to decoupling the address of records from their identity. The next step is to introduce logical record ids (OAK-4659). |Node states increase by the size of one record id (3 bytes / 20 bytes after OAK-4631). On top of that there is an additional block record à 18 bytes per node state. |offline |0.0.2 |OAK-3348
|Binary index in tar files |Avoid traversing the repository to collect the gc roots for DSGC. Fetch them from an index instead. |Additional index entry per tar file. Adds a couple of bytes per external binary to each tar file. Exact size to be determined. [~frm] could you help with this? OAK-4740 is a regression wrt. to resiliency caused by this change (and the fact that the blob store might return blob ids longer than 2k chars).  |offline |0.0.4 |OAK-4101
|Simplified record ids |Preparation and precondition for logical record ids (OAK-4659). At the same time the simplest possible fix for OAK-2896. The latter leads to degeneration of segment sizes, which in turn has adverse effects on overall performance, resource utilisation and memory requirements. Without this fix OAK-2498 would need to be fixed in a different way that would require other changes in the storage format. I started to regard this issue as removing a premature optimisation (which caused OAK-2498). OTOH with OAK-4844 we should also start looking into mitigations and what those would mean to size vs. simplicity vs. performance.  |Record ids grow from 3 bytes to 18 bytes when serialised into records. Impact on repositories to be assessed but can be anywhere between almost none to x6. OAK-4812 is a performance regression caused by this chance. Its overall impact is yet to be assessed. |offline |0.0.10 |OAK-4631, OAK-4844
|Storage format versioning |In order to be able to further evolve the storage format with minimal impact on existing deployments we need to carefully versions the various storage entities (segments, tar files, etc.) |No performance, space impact expected |offline |0.0.2/ 0.0.10 |OAK-4232, OAK-4683, OAK-4295
|Logical record ids |We need to separate addresses of records from their identity to be able to further scale the TarMK. OAK-3348 (the online compaction misery) can be seen as a symptom of failing to understand this earlier. The stable ids introduced with OAK-3348 are a first step into this direction. However this is not sufficient to implement features like e.g. background compaction (OAK-4756), partial compaction (OAK-3349) or incremental compaction (OAK-3350).  |A small size overhead per segment for the logical id table. Further impact to be evaluated ([~frm], please add your assessment here). |offline |0.0.14 (planned) |OAK-4659
|External index for segments |Avoid recreating tar files if indexes are corrupt/missing. Just recreate the indexes. |Faster startup after a crash. Overall less disk space usage as no unnecessary backup files are created. |online |not yet planned |OAK-4649
|In-place journal |Reduce complexity by in-lining the journal log. Less files, less chances to break something. Also the granularity of the log would increase as flushing of the persisted head would not be required any more. Resilience would improve as the roll-back functionality could operate at a finer granularity. |No more journal.log. Better resiliency. Significant risk for regression of OAK-4291 if not implemented properly. Most likely a significant refactoring of some parts of the code is required before we can proceed with this issue.  |online |not yet planned |OAK-4103
|Root record types |With the information currently available from the segment headers we cannot collect statistics about segment usage on repositories of non trivial sizes. This fix would allow us to build more scalable tools to that respect.  |None expected wrt. to performance and size under normal operation. |offline |0.0.14 (planned) (waiting for OAK-4659 as implementation depends on how we progress there) |OAK-2498

Misc ideas currently on the back burner:
* SegmentMK: Arch segments (OAK-1905)
* Extension headers for segments (no issue yet)
* More memory efficient serialisation of values (e.g. boolean) (no issue yet)
* Protocol Buffer for serialising records (no issue yet)

"	OAK	Resolved	3	17	4399	documentation
13100179	The backup command should not silently upgrade the FileStore	The backup command in oak-run should not accidentally perform a transparent upgrade of the FileStore. Instead, it should use a strict version check to fail fast if the code is run on an outdated version of the FileStore.	OAK	Closed	3	17	4399	production, resilience, tooling
13002069	Improve FileStoreStatsMBean	"We should add further data to that MBean (if feasible):

* Number of commits
* Number of commits queuing (blocked on the commit semaphore)
* Percentiles of commit times (exclude queueing time)
* Percentiles of commit queueing times 
* Last gc run / size before gc and after gc / time gc took broken down into the various phases

"	OAK	Closed	3	4	4399	monitoring
13041595	Test failure: segment.standby.ExternalSharedStoreIT.testProxyFlippedIntermediateByteChange2	"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=integrationTesting #443 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=integrationTesting #443|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=integrationTesting/443/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=integrationTesting/443/console]"	OAK	Resolved	3	1	4399	test-failure, windows
13113277	OffRC always logs 0 for the number of compacted nodes in gc.log	"After an offline compaction the {{gc.log}} always contains 0 for the number of compacted nodes. This is caused by {{org.apache.jackrabbit.oak.segment.tool.Compact.compact()}} instantiating a new {{FileStore}} to run cleanup. That file store has new {{GCMonitor}} instance, which did no see any of the nodes written by the compaction that was run on the previous {{FileStore}} instance. 

"	OAK	Closed	3	1	4399	compaction, gc, tooling
12773875	Root record references provide too little context for parsing a segment	"According to the [documentation | http://jackrabbit.apache.org/oak/docs/nodestore/segmentmk.html] the root record references in a segment header provide enough context for parsing all records within this segment without any external information. 

Turns out this is not true: if a root record reference turns e.g. to a list record. The items in that list are record ids of unknown type. So even though those records might live in the same segment, we can't parse them as we don't know their type. "	OAK	Resolved	3	1	4399	tools
13096362	Fix OSGi wiring after netty update to 4.1.x	"After netty update in OAK-6564, {{OSGiIT}} fails with the following exception:

{code}
Running org.apache.jackrabbit.oak.osgi.OSGiIT
ERROR: Bundle org.apache.jackrabbit.oak-segment-tar [36] Error starting file:/oak-it-osgi/target/test-bundles/oak-segment-tar.jar (org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.jackrabbit.oak-segment-tar [36]: Unable to resolve 36.0: missing requirement [36.0] osgi.wiring.package; (osgi.wiring.package=com.ning.compress))
org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.jackrabbit.oak-segment-tar [36]: Unable to resolve 36.0: missing requirement [36.0] osgi.wiring.package; (osgi.wiring.package=com.ning.compress)
	at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:3974)
	at org.apache.felix.framework.Felix.startBundle(Felix.java:2037)
	at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)
	at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)
	at java.lang.Thread.run(Thread.java:745)
{code}"	OAK	Closed	3	1	4399	cold-standby
13122808	Test failure: ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout	"Seen on an internal Windows Jenkins node:

h3. Regression

org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout

h3. Error Message

{noformat}
Values should be different. Actual: { root = { ... } }
{noformat}

h3. Stacktrace

{noformat}
java.lang.AssertionError: Values should be different. Actual: { root = { ... } }
	at org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout(ExternalPrivateStoreIT.java:87)
{noformat}

h3. Standard Output

{noformat}
22:41:13.646 INFO  [main] FileStoreBuilder.java:340         Creating file store FileStoreBuilder{version=1.8-SNAPSHOT, directory=target\junit2834122541179880349\junit3041268421527563090, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore], maxFileSize=1, segmentCacheSize=0, stringCacheSize=0, templateCacheSize=0, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1, memoryMapping=false, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}}
22:41:13.646 INFO  [main] FileStore.java:241                TarMK opened at target\junit2834122541179880349\junit3041268421527563090, mmap=false, size=0 B (0 bytes)
22:41:13.646 DEBUG [main] FileStore.java:247                TAR files: TarFiles{readers=[],writer=target\junit2834122541179880349\junit3041268421527563090\data00000a.tar}
22:41:13.646 DEBUG [main] TarWriter.java:185                Writing segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to target\junit2834122541179880349\junit3041268421527563090\data00000a.tar
22:41:13.646 INFO  [main] FileStoreBuilder.java:340         Creating file store FileStoreBuilder{version=1.8-SNAPSHOT, directory=target\junit2834122541179880349\junit4470899745425503556, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore], maxFileSize=1, segmentCacheSize=0, stringCacheSize=0, templateCacheSize=0, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1, memoryMapping=false, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}}
22:41:13.646 INFO  [main] FileStore.java:241                TarMK opened at target\junit2834122541179880349\junit4470899745425503556, mmap=false, size=0 B (0 bytes)
22:41:13.646 DEBUG [main] FileStore.java:247                TAR files: TarFiles{readers=[],writer=target\junit2834122541179880349\junit4470899745425503556\data00000a.tar}
22:41:13.646 DEBUG [main] TarWriter.java:185                Writing segment 8d19c7dc-8b48-4e10-a58d-31c15c93f2fe to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.646 INFO  [main] DataStoreTestBase.java:127        Test begin: testSyncFailingDueToTooShortTimeout
22:41:13.646 INFO  [main] SegmentNodeStore.java:120         Creating segment node store SegmentNodeStoreBuilder{blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore]}
22:41:13.646 INFO  [main] LockBasedScheduler.java:155       Initializing SegmentNodeStore with the commitFairLock option enabled.
22:41:13.708 DEBUG [main] StandbyServer.java:248            Binding was successful
22:41:13.708 DEBUG [main] TarWriter.java:185                Writing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to target\junit2834122541179880349\junit3041268421527563090\data00000a.tar
22:41:13.739 DEBUG [main] TarRevisions.java:240             TarMK journal update null -> 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c
22:41:13.755 DEBUG [standby-1] GetHeadRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for current head
22:41:13.755 DEBUG [primary-1] ClientFilterHandler.java:53  Client /127.0.0.1:65480 is allowed
22:41:13.755 DEBUG [primary-1] RequestDecoder.java:42       Parsed 'get head' message
22:41:13.755 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get head' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [primary-1] GetHeadRequestHandler.java:43 Reading head for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [primary-1] GetHeadResponseEncoder.java:36 Sending head 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [standby-1] ResponseDecoder.java:82      Decoding 'get head' response
22:41:13.755 DEBUG [standby-run-23] StandbyClientSyncExecution.java:103 Found missing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [standby-run-23] StandbyClientSyncExecution.java:124 Inspecting segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [standby-1] GetReferencesRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [primary-1] RequestDecoder.java:48       Parsed 'get references' message
22:41:13.771 DEBUG [primary-1] GetReferencesRequestHandler.java:39 Reading references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetReferencesResponseEncoder.java:34 Sending references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:94      Decoding 'get references' response
22:41:13.771 DEBUG [standby-run-23] StandbyClientSyncExecution.java:184 Found reference from 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [standby-run-23] StandbyClientSyncExecution.java:124 Inspecting segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [standby-1] GetReferencesRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:48       Parsed 'get references' message
22:41:13.771 DEBUG [primary-1] GetReferencesRequestHandler.java:39 Reading references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetReferencesResponseEncoder.java:34 Sending references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:94      Decoding 'get references' response
22:41:13.771 INFO  [standby-run-23] StandbyClientSyncExecution.java:196 Copying data segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a from primary
22:41:13.771 DEBUG [standby-1] GetSegmentRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:45       Parsed 'get segment' message
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get segment' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentRequestHandler.java:39 Reading segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:125 Segment with size 192 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentResponseEncoder.java:43 Sending segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:86      Decoding 'get segment' response
22:41:13.771 DEBUG [standby-run-23] TarWriter.java:185      Writing segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.771 INFO  [standby-run-23] StandbyClientSyncExecution.java:196 Copying data segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 from primary
22:41:13.771 DEBUG [standby-1] GetSegmentRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:45       Parsed 'get segment' message
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get segment' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentRequestHandler.java:39 Reading segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:125 Segment with size 448 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentResponseEncoder.java:43 Sending segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:86      Decoding 'get segment' response
22:41:13.771 DEBUG [standby-run-23] TarWriter.java:185      Writing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.771 DEBUG [standby-1] GetBlobRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:39       Parsed 'get blob' request
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get blob id' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetBlobRequestHandler.java:41 Reading blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:130 Binary with size 5242880 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetBlobResponseEncoder.java:41 Sending blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.786 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 1/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 2/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 1/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.802 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 3/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 2/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.818 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 4/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 3/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.818 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 5/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 4/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 5/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:167     Received entire blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.880 DEBUG [standby-run-23] ResponseDecoder.java:66 Processing input stream finished! Deleting file C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp
22:41:13.896 DEBUG [standby-run-23] TarRevisions.java:240   TarMK journal update null -> 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c
22:41:13.911 WARN  [standby-1] ExceptionHandler.java:37     Exception caught on client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
io.netty.handler.timeout.ReadTimeoutException: null
22:41:13.911 INFO  [standby-run-23] StandbyClientSyncExecution.java:82 updated head state successfully: true in 156ms.
22:41:13.911 DEBUG [standby-run-23] StandbyClient.java:157  Channel closed
22:41:16.137 DEBUG [main] StandbyClientSync.java:277        Group shut down
22:41:16.137 DEBUG [main] StandbyServer.java:219            Channel disconnected
22:41:16.137 DEBUG [main] StandbyServer.java:219            Channel disconnected
22:41:16.137 DEBUG [main] StandbyServer.java:230            Boss group shut down
22:41:16.137 DEBUG [main] StandbyServer.java:236            Worker group shut down
22:41:16.137 INFO  [main] DataStoreTestBase.java:132        Test end: testSyncFailingDueToTooShortTimeout
22:41:16.137 DEBUG [main] Scheduler.java:134                The scheduler FileStore background tasks was successfully shut down
22:41:16.137 DEBUG [main] TarRevisions.java:236             Head state did not change, skipping flush
22:41:16.184 INFO  [main] FileStore.java:480                TarMK closed: target\junit2834122541179880349\junit4470899745425503556
22:41:16.184 DEBUG [main] Scheduler.java:134                The scheduler FileStore background tasks was successfully shut down
22:41:16.184 DEBUG [main] TarRevisions.java:236             Head state did not change, skipping flush
22:41:16.199 INFO  [main] FileStore.java:480                TarMK closed: target\junit2834122541179880349\junit3041268421527563090
{noformat}
"	OAK	Closed	3	1	4399	test-failure
13038374	Test failure: segment.standby.ExternalSharedStoreIT/BrokenNetworkTest.test...	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1384 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1384|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1384/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1384/console]"	OAK	Resolved	3	1	4399	test-failure, ubuntu
13002746	A parallel approach to garbage collection	"Assuming that:

# Logic record IDs are implemented.
# TAR files are ordered in reverse chronological order.
# When reading segments, TAR files are consulted in order.
# Segments in recent TAR files shadow segments in older TAR files with the same segment ID.

A new algorithm for garbage collection can be implemented:

# Define the input for the garbage collection process. The input consists of the current set of TAR files and a set of record IDs representing the GC roots.
# Traverse the GC roots and mark the records that are still in use. The mark phase traverses the record graph and produces a list of record IDs. These record IDs are referenced directly or indirectly by the given set of GC roots and need to be kept. The list of record IDs is ordered by segment ID first and record number next. This way, it is possible to process this list in one pass and figure out which segment and which record should be saved at the end of the garbage collection.
# Remove unused records from segments and rewrite them in a new set of TAR files. The list is produced in the previous step is traversed. For each segment encountered, a new segment is created containing only the records that were marked in the previous phase. This segment is then saved in a new set of TAR files. The set of new TAR files is the result of the garbage collection process. 
# Add the new TAR files to the system. The system will append the new TAR files to the segment store. The segments in these TAR files will shadow the ones in older TAR files.
# Remove TAR files from the old generation. It is safe to do so because the new set of TAR files are currently shadowing the initial set of TAR files.

While the garbage collection process is running, the system can work as usual by starting a fresh TAR file. The result of the garbage collection is made visible atomically only at the end, when the new TAR files are integrated into the running system."	OAK	Resolved	3	2	4399	gc, scalability
12989393	SegmentNodeState.fastEquals() can trigger two I/O operations	"The implementation of {{SegmentNodeState.fastEquals()}} compares the stable IDs of two instances of {{SegmentNodeState}}. In some cases, reading the stable ID would trigger a read of an additional record, the block record containing the serialized version of the segment ID.

This issue is about evaluating the performance implications of this strategy and, in particular, if it would be better to store the serialized stable ID in the node record itself."	OAK	Resolved	4	4	4399	performance
13033423	Add a persistence-dependent namespace when running CLI commands	"Commands in oak-run currently live in a flat namespace. If a command is specific to only one implementation, it will leave along other implementation-specific commands without any means of distinguishing what belongs where.

I would like to add a layer of indirection to the oak-run command line interface, so to parse commands in the following fashion:

{noformat}
oak-run segment debug /path/to/folder
oak-run mongo debug mongodb://host:12345
oak-run rdb debug jdbc:oracle:oci8:scott/tiger@myhost
{noformat}

In this scenario, oak-run would become a simple entry point that would delegate to implementation-specific command line utilities based on the first argument. In the previous example, {{segment}}, {{mongo}} and {{rdb}} would delegate to three different implementation specific CLI utilities. Each of these CLI utilities will understand the {{debug}} command and will collect command-line parameters as it sees fit.

If the code for a command is so generic that can be reused from different commands, it can be parameterised and reused from different implementation-specific commands.

The benefit of this approach is that we can start moving commands closer to the implementations. This approach would benefit oak-run as well, which is overloaded with many commands from many different implementations."	OAK	Resolved	3	4	4399	tooling
13027002	Better default for size delta estimation 	The default value for the size delta estimation used during garbage collection should be changed to 1GB.	OAK	Closed	3	1	4399	gc
13026547	Test failure: ExternalPrivateStoreIT. testSyncUpdatedBinaryProperty()	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1320 has failed.
First failed run: [Apache Jackrabbit Oak matrix/jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1320|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1320/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1320/console]"	OAK	Closed	3	1	4399	test-failure
13037624	Standby Automatic Cleanup should be on by default	The OSGi setting controlling standby automatic cleanup, {{standby.autoclean}}, should be set to {{true}} by default. When the automatic cleanup is on, the {{cleanup()}} method will be called on standby, provided the size of the store increases over 25% on a sync cycle.	OAK	Closed	4	4	4399	cold-standby
12992483	Collection of references retrieves less when large number of blobs added	"When large number of external blobs are added to the DataStore (50000) and a cycle of compaction executed then the reference collection logic only returns lesser number of blob references. It reports correct number of blob references when number of blobs added are less indicatingsome sort of overflow.
Another related issue observed when testing with lesser number of blobs is that the references returned are double the amount expected, so maybe there should be some sort of de-duplication which should be added.

Without compaction the blob references are returned correctly atleast till 100000 (ExternalBlobId#testNullBlobId)"	OAK	Closed	3	1	4399	datastore, gc
12830623	Compaction should check for required disk space before running	In the worst case compaction doubles the repository size while running. As this is somewhat unexpected we should check whether there is enough free disk space before running compaction and log a warning otherwise. This is to avoid a common source of running out of disk space and ending up with a corrupted repository. 	OAK	Closed	3	4	4399	compaction, doc-impacting, gc, resilience
12934109	oak-run primary/standby should check segment version	The primary and standby run modes should exit with an error if run on a store with non matching segment version.	OAK	Closed	2	17	4399	resilience, tooling
13013781	Test failure: SegmentDataStoreBlobGCIT	"SegmentDataStoreBlobGCIT seems to crash the JVM on Java 7. Following is the relevant part of the build output.

{noformat}
[INFO] --- maven-failsafe-plugin:2.19.1:integration-test (default) @ oak-segment-tar ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.jackrabbit.oak.segment.file.FileStoreIT
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.301 sec - in org.apache.jackrabbit.oak.segment.file.FileStoreIT
Running org.apache.jackrabbit.oak.segment.file.SegmentReferenceLimitTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0 sec - in org.apache.jackrabbit.oak.segment.file.SegmentReferenceLimitTestIT
Running org.apache.jackrabbit.oak.segment.file.LargeNumberOfPropertiesTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.001 sec - in org.apache.jackrabbit.oak.segment.file.LargeNumberOfPropertiesTestIT
Running org.apache.jackrabbit.oak.segment.SegmentOverflowExceptionIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0 sec - in org.apache.jackrabbit.oak.segment.SegmentOverflowExceptionIT
Running org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 45.78 sec - in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverSslTestIT
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.202 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverSslTestIT
Running org.apache.jackrabbit.oak.segment.standby.BrokenNetworkIT
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 63.024 sec - in org.apache.jackrabbit.oak.segment.standby.BrokenNetworkIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverMultipleClientsTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.052 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverMultipleClientsTestIT
Running org.apache.jackrabbit.oak.segment.standby.MBeanIT
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.287 sec - in org.apache.jackrabbit.oak.segment.standby.MBeanIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverIPRangeIT
Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.691 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverIPRangeIT
Running org.apache.jackrabbit.oak.segment.standby.StandbyTestIT
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.303 sec - in org.apache.jackrabbit.oak.segment.standby.StandbyTestIT
Running org.apache.jackrabbit.oak.segment.standby.RecoverTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.415 sec - in org.apache.jackrabbit.oak.segment.standby.RecoverTestIT
Running org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.002 sec - in org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
Running org.apache.jackrabbit.oak.segment.SegmentDataStoreBlobGCIT

Results :

Tests run: 65, Failures: 0, Errors: 0, Skipped: 3

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10:17 min
[INFO] Finished at: 2016-10-19T20:45:40+00:00
[INFO] Final Memory: 63M/553M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:2.19.1:integration-test (default) on project oak-segment-tar: Execution default of goal org.apache.maven.plugins:maven-failsafe-plugin:2.19.1:integration-test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
[ERROR] Command was /bin/sh -c cd /apps/jenkins/workspace/oak-segment-tar && /opt/jdk-7/jre/bin/java -Xmx512m -XX:MaxPermSize=64m -XX:+HeapDumpOnOutOfMemoryError -Dupdate.limit=100 -Djava.awt.headless=true -jar /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefirebooter4283069132546797078.jar /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefire8963659563100379656tmp /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefire_03767892930481742588tmp
{noformat}"	OAK	Closed	3	1	4399	test-failure
12958434	Remove deprecated constructors from SegmentNodeStore	Switch API clients to {{SegmentNodeStoreBuilder}} instead. 	OAK	Resolved	3	17	4399	technical_debt
12920357	Test failure: ExternalSharedStoreIT	"{{org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT}} fails on Jenkins: 

{noformat}
testSync(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT)  Time elapsed: 54.498 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.jackrabbit.oak.plugins.segment.standby.DataStoreTestBase.testSync(DataStoreTestBase.java:104)
{noformat}

Seen at builds 163, 164, 598, 601
"	OAK	Resolved	3	1	4399	ci, jenkins
12861783	Partial compaction	"On big repositories compaction can take quite a while to run as it needs to create a full deep copy of the current root node state. For such cases it could be beneficial if we could partially compact the repository thus splitting full compaction over multiple cycles. 
Partial compaction would run compaction on a sub-tree just like we now run it on the full tree. Afterwards it would create a new root node state by referencing the previous root node state replacing said sub-tree with the compacted one. 

Todo: Asses feasibility and impact, implement prototype."	OAK	Closed	3	2	4399	compaction, gc, scalability
12961891	Document oak-segment-tar	"Document Oak Segment Tar. Specifically:
* New and changed configuration and monitoring options
* Changes in gc (OAK-3348 et. all)
* Changes in segment / tar format (OAK-3348)
"	OAK	Closed	3	3	4399	documentation, gc
12900998	Improve the logging capabilities of offline compaction	"The offline compaction instantiates {{FileStore}} using a deprecated constructor. This constructor forces a no-op {{GCMonitor}} that swallows log messages and caught exception.

It would be more appropriate to create the {{FileStore}} using the corresponding {{Builder}}. This has the side effect of configuring a {{LoggingGCMonitor}}, which provides way more information than the current default."	OAK	Closed	3	4	4399	compaction, gc
13015707	Standby test failures	"I've recently seen a couple of the standby tests fail. E.g. on Jenkins: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/1245/

{noformat}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
	at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:122)
{noformat}

{noformat}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
	at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:122)
{noformat}

{{org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT.testProxySkippedBytes}}:
{noformat}
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

"	OAK	Closed	3	1	4399	test-failure
12927995	Clean up the FileStore constructor	"The {{FileStore}} constructor consists of more than 150 LoC and is a mess as it depends on the order of initialisation, calls overrideable methods handles different concerns (read only vs. read / write) etc. 

We should up with a cleaner way of instantiating a file store."	OAK	Closed	3	17	4399	technical_debt
12948112	Break cyclic dependency of FileStore and SegmentTracker	{{SegmentTracker}} and {{FileStore}} are mutually dependent on each other. This is problematic and makes initialising instances of these classes difficult: the {{FileStore}} constructor e.g. passes a not fully initialised instance to the {{SegmentTracker}}, which in turn writes an initial node state to the store. Notably using the not fully initialised {{FileStore}} instance!	OAK	Closed	3	17	4399	technical_debt
13093176	Implement rolling upgrade from Oak 1.6	"The segment format changes introduced for tail compaction (OAK-3349) must not require an explicit migration step. Instead there should be a rolling migration during normal operation. 

Things to consider:
* Segments from Oak 1.6
* Changes in tar index formats induced by the segment format changes
* Changes in gc.log induced by the segment format changes
* Required changes in the repository manifest and its interpretation
"	OAK	Closed	3	2	4399	migration, upgrade
13013446	SegmentRevisionGC MBean should report more detailed gc status information  	"Regarding this, the current ""Status"" is showing the last log info. This is useful, but it would also be interesting to expose the real-time status. For monitoring it would be useful to know exactly in which phase we are, e.g. a field showing on of the following:
- idle
- estimation
- compaction
- compaction-retry-1
- compaction-retry-2
- compaction-forcecompact
- cleanup


"	OAK	Closed	3	4	4399	gc, monitoring
13032183	Test failure: segment.standby.BrokenNetworkTest	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1355 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1355|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1355/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1355/console]

Initially reported test failure: testProxyFlippedStartByte()
Additional test failure reported via OAK-5476: testProxySSLSkippedBytes()
Additional test failure reported via OAK-5477: testProxyFlippedStartByteSSL()
Additional test failures reported via OAK-5478: all tests failed"	OAK	Closed	3	1	4399	test-failure
13120406	test failure seen in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT	"{noformat}
INFO] Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file C:\projects\apache\oak\trunk\oak-segment-tar\target\failsafe-reports\2017-11-23T08-48-55_999-jvmRun1.dumpstream
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 17.984 s <<< FAILURE! - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
[ERROR] offRCUpgradesSegments(org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT)  Time elapsed: 5.024 s  <<< FAILURE!
java.lang.AssertionError: Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.checkSegmentVersion(UpgradeIT.java:143)
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments(UpgradeIT.java:108)

{noformat}
"	OAK	Closed	2	1	4399	test-failure, windows
13251784	Backport OAK-8066 to 1.10 and 1.8	Backport OAK-8066 to 1.10 and 1.8.	OAK	Resolved	1	4	4399	TarMK
12830668	Tests for SegmentNodeStoreService	{{SegmentNodeStoreService}} currently has no test coverage whatsoever. We should change that.	OAK	Closed	3	4	4399	technical_debt
13116445	Offline compaction should not use mmap on Windows	The offline compaction tool should do an effort to detect whether it is being run on windows and disable memory mapping if so. Rational: with memory mapping enabled it might fail to remove the old tar files (see OAK-4274 and [JDK-4724038|http://bugs.java.com/view_bug.do?bug_id=4724038]).	OAK	Closed	3	4	4399	compaction, gc, tooling
13236196	Add remote store monitoring  for Azure	"Add remote store monitoring
Implement the remote store monitoring for Azure Store. This should include:
- request_count : number of request to azure store
- error_count : number of failed requests to azure store
- duration : duration of a request to azure store in nanoseconds "	OAK	Closed	3	2	4399	TarMK
12774577	Truncate journal.log after off line compaction	After off line compaction the repository contains a single revision. However the journal.log file will still contain the trail of all revisions that have been removed during the compaction process. I suggest we truncate the journal.log to only contain the latest revision created during compaction.	OAK	Closed	4	2	7065	compaction, gc
13016843	Improve caching of segments	"Various aspects of how Segment Tar caches segments could possibly improved. The current cache is a result of replacing the former ad-hoc cache with a proper one in OAK-3055. While the former was prone to contention under concurrent load the current cache is too oblivious about reads: read accesses are always served through {{SegmentId.segment}} and never actually hit the cache. This results in frequently accessed segments not to be seen as such by the cache and potentially being prematurely evicted. 

Possibly approaches to address this problem include: 
* Reinstantiating the cache we had pre OAK-3055 but making in fully concurrent. 
* Convey the information about read accesses to the current cache. 
* In either of the above cases avoid bulk segments from being placed into the cache. "	OAK	Resolved	3	4	7065	performance, scalability
12979326	Decouple SegmentReader from Revisions	"The {{SegmentReader.readHeadState()}} introduces a de-facto dependency to {{Revisions}} as access to the latter is required for obtaining the record id of the head. 

To decouple SegmentReader from Revisions I propose to replace {{SegmentReader.readHeadState()}} with {{SegmentReader.readHeadState(Revisions revisions)}}. As this results in a lot of boilerplate for callers (i.e. {{fileStore.getReader().getHeadState(fileStore.getRevisions())}}), we should also introduce a convenience method {{FileStore.getHead()}} clients could use to that matter.
"	OAK	Closed	3	4	7065	technical_debt
12696542	Many extra events are dispatched from a move event	"When moving a node many extra events are dispatched in OAK in compared to other implementations

On Oak a node added and node remove events are dispatched for each node in the hierarchy being moved.  As well there is a property add and property remove event dispatched for each property in the node hierarchy.  

This compares to previous implementations where only a Node Moved, node added and node removed event is dispatched for the parentnode being moved.

See [0] for an example.

For me this is problematic for a couple of reasons:

1) We are dispatching more events than we did previously.  In cases where nodes are frequently moved this will add extra load on the system. 
2) It is becoming increasingly difficult to ignore events related to a move without spending extra cycles to make that determination. 
3) Many pre-existing event listeners will be executing on events that they previously would not have.

I know the JCR spec indicates that an implementation may choose to dispatch these events or not, but I suggest we change OAK to not throw these extra events.  If we do not many observation listeners will act on events they previously did not will likely cause problems.

Also, if we could add a simple marker in any event’s info map which is related to a node move (ie: the node removed, node added etc) it would be very helpful when trying to ignore events caused by a move.  (which I believe to be the case in many situations).

[0] 
Move “c” in the hierarchy below from /a/b to /a/z:

/a/b/c/d/e
to:
/a/z/c/d/e

Results in:

CRX2:
/a/b, type: {node removed}
/a/z/b, type: {node added}
/a/z/b, type: {node moved}

OAK:
/a/b/c, type: {node removed}
/a/z/c, type: {node moved}
/a/z/c, type: {node added}
/a/b/c/jcr:primaryType, type: {property removed}
/a/b/c/jcr:createdBy, type: {property removed}
/a/b/c/jcr:created, type: {property removed}
/a/b/c/d, type: {node removed}
/a/z/c/jcr:primaryType, type: {property added}
/a/z/c/jcr:createdBy, type: {property added}
/a/z/c/jcr:created, type: {property added}
/a/z/c/d, type: {node added}
/a/b/c/d/jcr:primaryType, type: {property removed}
/a/b/c/d/jcr:createdBy, type: {property removed}
/a/b/c/d/jcr:created, type: {property removed}
/a/b/c/d/e, type: {node removed}
/a/z/c/d/jcr:primaryType, type: {property added}
/a/z/c/d/jcr:createdBy, type: {property added}
/a/z/c/d/jcr:created, type: {property added}
/a/z/c/d/e, type: {node added}
/a/b/c/d/e/jcr:primaryType, type: {property removed}
/a/b/c/d/e/jcr:createdBy, type: {property removed}
/a/b/c/d/e/jcr:created, type: {property removed}
/a/z/c/d/e/jcr:primaryType, type: {property added}
/a/z/c/d/e/jcr:createdBy, type: {property added}
/a/z/c/d/e/jcr:created, type: {property added}
"	OAK	Closed	3	4	7065	observation
12921095	Tool for detecting references to pre compacted segments	While OAK-3560 allows us to detect reference to pre compacted segments through manual inspection, we also need tooling to help detect such cases on site, during longevity tests and for UT/IT.  	OAK	Closed	3	17	7065	compaction, gc, tooling
12960329	CLONE - Cross gc sessions might introduce references to pre-compacted segments	"I suspect that certain write operations during compaction can cause references from compacted segments to pre-compacted ones. This would effectively prevent the pre-compacted segments from getting evicted in subsequent cleanup phases. 

The scenario is as follows:
* A session is opened and a lot of content is written to it such that the update limit is exceeded. This causes the changes to be written to disk. 
* Revision gc runs causing a new, compacted root node state to be written to disk.
* The session saves its changes. This causes rebasing of its changes onto the current root (the compacted one). At this point any node that has been added will be added again in the sub-tree rooted at the current root. Such nodes however might have been written to disk *before* revision gc ran and might thus be contained in pre-compacted segments. As I suspect the node-add operation in the rebasing process *not* to create a deep copy of such nodes but to rather create a *reference* to them, a reference to a pre-compacted segment is introduced here. 

Going forward we need to validate above hypothesis, assess its impact if necessary come up with a solution.
"	OAK	Resolved	2	4	7065	cleanup, compaction, gc
12919023	Offline compaction doesn't clean up unreferenced tar files	This is a regression introduced with OAK-3329 where cleaning up unreferenced tar files was taken out of {{FileStore#cleanup}}. 	OAK	Closed	3	1	7065	cleanup, gc
12684964	RandomizedReadTest fails with OutOfMemoryError: PermGen space	"This happened while running the maven build with {{-PintegrationTesting}}:

{code}
Running org.apache.jackrabbit.oak.jcr.random.RandomizedReadTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.418 sec
org.apache.maven.surefire.util.SurefireReflectionException: java.lang.reflect.InvocationTargetException; nested exception is java.lang.reflect.InvocationTargetException: null
java.lang.reflect.InvocationTargetException
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space

Results :

Tests run: 722, Failures: 0, Errors: 0, Skipped: 48
{code}

The crucial point being Surefire silently ignoring the following tests such that the build happily succeeds making following failures. Note, that test suite consists of  2003 tests in contrast to the 722 reported by Surefire. "	OAK	Closed	3	1	7065	test
13078411	Remove unused field SegmentNodeStore.reader	That field is not used any more since the explicit commit queue was introduced. 	OAK	Closed	4	4	7065	technical_debt
12830634	NPE in SegmentWriter.writeMap	"Under some rare conditions which are not entirely clear yet {{SegmentWriter.writeMap}} results in a {{NPE}}:

{noformat}
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:192)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:366)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapLeaf(SegmentWriter.java:417)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:475)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:511)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMap(SegmentWriter.java:711)
{noformat}

This happens when the {{base}} passed to {{writeMap(MapRecord base, Map<String, RecordId> changes)}} is not null but doesn't contain some of the keys *removed* through the updates provided in the passed {{changes}}. "	OAK	Closed	3	1	7065	resilience
12976334	Remove segment version argument from segment writer and and related classes	"The {{SegmentWriter}} and its related classes accept a {{SegmentVersion}} argument. This is confusing since that version is only stored in the segment's segment version field. The writer cannot and does not actually write segments at older version than the latest (12). 

I suggest we remove the explicit segment version from all classes where it can be specified and hard code the segment version to 12 for now. This is the only segment version {{segment-tar}} currently supports anyway. Should  the need to support other segment version arise in the future, we need to decide at that point how to parametrise {{segment-tar}} on the segment version. "	OAK	Closed	3	4	7065	refactoring
12545672	JCR bindings for Oak	"One of the proposed goals for the 0.1 release is at least a basic JCR binding for Oak. Most of that already exists in /jackrabbit/sandbox, we just need to decide where and how to place it in Oak. I think we should either put it all under o.a.j.oak.jcr in oak-core, or create a separate oak-jcr component for the JCR binding.

As for functionality, it would be nice if the JCR binding was able to do at least the following:

{code}
Repository repository = JcrUtils.getRepository(...);

Session session = repository.login(...);
try {
    // Create
    session.getRootNode().addNode(""hello"")
        .setProperty(""world"",  ""hello world"");
    session.save();

    // Read
    assertEquals(
        ""hello world"",
        session.getProperty(""/hello/world"").getString());

    // Update
    session.getNode(""/hello"").setProperty(""world"", ""Hello, World!"");
    session.save();
    assertEquals(
        ""Hello, World!"",
        session.getProperty(""/hello/world"").getString());

    // Delete
    session.getNode(""/hello"").delete();
    session.save();
    assertTrue(!session.propertyExists(""/hello/world""));
} finally {
    create.logout();
}
{code}
"	OAK	Closed	3	2	7065	jcr
13076144	Unreferenced argument reference in method SegmentBufferWriter.writeRecordId	This is a leftover from when we switched from record ids to record numbers as at that point it became unnecessary to track such references. 	OAK	Closed	4	4	7065	technical_debt
12696529	Warn on huge multi-valued properties	It is an explicit design non-goal of Oak to support huge amounts of values in multi-valued properties. If a user still tries to create these we should at least throw a WARN in the logs to indicate that usage of MVPs is wrong.	OAK	Closed	5	4	7065	production, resilience
13083418	Cleanup constants in Segment class	Some of the constants in the {{Segment}} class still refer to the old 255 segment references limit. We should fix the comments, the constants and their usage to reflect the current situation where that limit has been lifted. 	OAK	Closed	3	4	7065	technical_debt
12902989	Deadlock when closing a concurrently used FileStore 2.0	"A deadlock was detected while stopping the {{SegmentCompactionIT}} using the exposed MBean.

{noformat}
""main@1"" prio=5 tid=0x1 nid=NA waiting for monitor entry
 waiting for pool-1-thread-10@2111 to release lock on <0xae8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.dropCache(SegmentWriter.java:871)
  at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.close(FileStore.java:1031)
  - locked <0xae7> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT.tearDown(SegmentCompactionIT.java:282)

""pool-1-thread-10@2111"" prio=5 tid=0x1d nid=NA waiting for monitor entry
  java.lang.Thread.State: BLOCKED
 blocks main@1
 waiting for main@1 to release lock on <0xae7> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
  at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.writeSegment(FileStore.java:1155)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.flush(SegmentWriter.java:253)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.prepare(SegmentWriter.java:350)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:468)
  - locked <0xae8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:719)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1211)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1156)
  at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1147)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1175)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:451)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:474)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:530)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:208)
{noformat}"	OAK	Closed	3	1	7065	resilience
12941553	NPE when running oak-run from within the IDE	"Running oak-run from within the IDE causes a {{NPE}}:

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.jackrabbit.oak.run.Main.getProductVersion(Main.java:76)
	at org.apache.jackrabbit.oak.run.Main.getProductVersion(Main.java:66)
	at org.apache.jackrabbit.oak.run.Main.getProductInfo(Main.java:53)
	at org.apache.jackrabbit.oak.run.Main.printProductInfo(Main.java:86)
{code}

This is caused by not checking the return value of {{getResourceAsStream}} for {{null}} when trying to load {{/META-INF/maven/org.apache.jackrabbit/oak-run/pom.properties}}. That file is not on the class path when running from within the IDE. "	OAK	Closed	3	1	7065	tooling
12961896	Consider making FileStore.writer volatile	That filed is not volatile although access by different threads. We should consider changing it to volatile.	OAK	Closed	4	4	7065	technical_debt
12902425	Confusing SNFE whith oak-run debug	"Running 

{{oak-run debug  /path/to/segmentStore}}

can result in {{SNFE}} s being logged if the file store has been compacted before. This can happen even though the repository is actually consistent according to {{oak-run check}}. 

The reason is {{debug}} traversing all node states of all record ids in all tar files. When a previous cleanup of a tar file did not reach 25% gain it will not clean up that file leaving behind segments possibly containing node states pointing to limbo. Those nodes would result in said {{SNFE}} of {{oak-run}} debug. But as those node states are not reachable from the head node state the repository is still consistent itself. "	OAK	Closed	3	4	7065	cleanup, gc, tooling
12730473	Observation events accessibility not checked correctly	Before delivering an observation event it is checked whether the respective item is actually accessible through the associated session. However the check is currently done against the state of the session from the time the event listener was registered instead of from the time the event is being sent. 	OAK	Closed	3	1	7065	observation
13059956	Revisions.setHead(Function) should return the new head or null instead of boolean	Currently {{Revisions.setHead(Function, Option)}} returns a {{boolean}} to indicate success or failure. The caller has no access to the head resulting from this call. I would thus like to change this into the record id of the new head in case of success and {{null}} otherwise. 	OAK	Closed	3	4	7065	refactoring
12858640	SegmentOverflowExceptionIT runs forever unless it fails	Currently {{SegmentOverflowExceptionIT}} runs forever or until it fails. We should add a time out after which the test is considered passed.	OAK	Closed	3	4	7065	test
12827982	Improve revision gc on SegmentMK	"This is a container issue for the ongoing effort to improve revision gc of the SegmentMK. 

I'm exploring 
* ways to make the reference graph as exact as possible and necessary: it should not contain segments that are not referenceable any more and but must contain all segments that are referenceable. 
* ways to segregate the reference graph reducing dependencies between certain set of segments as much as possible. 
* Reducing the number of in memory references and their impact on gc as much as possible.

"	OAK	Closed	3	15	7065	compaction, gc
12831424	Putting many elements into a map results in many small segments. 	"There is an issue with how the HAMT implementation ({{SegmentWriter.writeMap()}} interacts with the 256 segment references limit when putting many entries into the map: This limit gets regularly reached once the maps contains about 200k entries. At that points segments get prematurely flushed resulting in more segments, thus more references and thus even smaller segments. It is common for segments to be as small as 7k with a tar file containing up to 35k segments. This is problematic as at this point handling of the segment graph becomes expensive, both memory and CPU wise. I have seen persisted segment graphs as big as 35M where the usual size is a couple of ks. 

As the HAMT map is used for storing children of a node this might have an advert effect on nodes with many child nodes. 

The following code can be used to reproduce the issue: 

{code}
SegmentWriter writer = new SegmentWriter(segmentStore, getTracker(), V_11);
MapRecord baseMap = null;

for (;;) {
    Map<String, RecordId> map = newHashMap();
    for (int k = 0; k < 1000; k++) {
        RecordId stringId = writer.writeString(String.valueOf(rnd.nextLong()));
        map.put(String.valueOf(rnd.nextLong()), stringId);
    }

    Stopwatch w = Stopwatch.createStarted();
    baseMap = writer.writeMap(baseMap, map);
    System.out.println(baseMap.size() + "" "" + w.elapsed());
}
{code}

"	OAK	Closed	2	1	7065	performance
12671609	Periodically poll for external events	"Currently external events are only reported along with local changes. That is, when local changes are persisted external changes are detected and reported along with the local changes. This might cause external events to be delayed indefinitely on cluster nodes without writes. 

We might want to implement a solution that regularly polls for external events. 
See OAK-1055 for why a previous implementation didn't work. 

"	OAK	Closed	4	4	7065	observation
12917177	More resilient BackgroundThread implementation	"Currently {{BackgroundThread}} dies silently when hit by an uncaught exception. We should log a warning. 

Also calling {{Thread#start}} from within the constructor is an anti-pattern as it exposes {{this}} before fully initialised. This is potentially causing OAK-3303. "	OAK	Closed	3	4	7065	resilience
13017636	Improve GC scalability on TarMK	"This issue is about making TarMK gc more scalable: 
* how to deal with huge repositories.
* how to deal with massive concurrent writes.
* how can we improve monitoring to determine gc health. 
** Monitor deduplication caches (e.g. deduplication of checkpoints)

Possible avenues to explore:
* Can we partition gc? (e.g. along sub-trees, along volatile vs. static content)
* Can we pause and resume gc? (e.g. to give precedence to concurrent writes) 
* Can we make gc a real background process not contending with foreground operations? 

This issue is a follow up to OAK-2849, which was about efficacy of gc."	OAK	Resolved	3	15	7065	gc, scalability
13097268	Add tooling API	"h3. Current situation
Current segment store related tools are implemented ad-hoc by potentially relying on internal implementation details of Oak Segment Tar. This makes those tools less useful, portable, stable and potentially applicable than they should be.

h3. Goal
Provide a common and sufficiently stable Oak Tooling API for implementing segment store related tools. The API should be independent of Oak and not available for normal production use of Oak. Specifically it should not be possible to it to implement production features and production features must not rely on it. It must be possible to implement the Oak Tooling API in Oak 1.8 and it should be possible for Oak 1.6.

h3. Typical use cases
* Query the number of nodes / properties / values in a given path satisfying some criteria
* Aggregate a certain value on queries like the above
* Calculate size of the content / size on disk
* Analyse changes. E.g. how many binaries bigger than a certain threshold were added / removed between two given revisions. What is the sum of their sizes?
* Analyse locality: measure of locality of node states. Incident plots (See https://issues.apache.org/jira/browse/OAK-5655?focusedCommentId=15865973&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15865973).
* Analyse level of deduplication (e.g. of checkpoint) 

h3. Validation
Reimplement [Script Oak|https://github.com/mduerig/script-oak] on top of the tooling API. 

h3. API draft
* Whiteboard shot of the [API entities|https://wiki.apache.org/jackrabbit/Oakathon%20August%202017?action=AttachFile&do=view&target=IMG_20170822_163256.jpg] identified initially.
* Further [drafting of the API|https://github.com/mduerig/oak-tooling-api] takes place on Github for now. We'll move to the Apache SVN as soon as considered mature enough and have a consensus of where to best move it. "	OAK	Closed	3	2	7065	tooling
12707374	Improve warning logged on concurrent Session access	"OAK-1601 introduced warnings that are logged when a session is accessed concurrently from different threads. The modalities however differ from those of Jackrabbit 2. The message 

{code}
Attempt to perform ""sessionOperation"" while another thread is concurrently writing to ""session"". Blocking until the other thread is finished using this session. Please review your code to avoid concurrent use of a session.
{code}

is logged for the current thread

* if the current threads attempts a write operation while another thread already executes a write operation in Jackrabbit 2,
* if the current thread attempts a write operation while another thread already executes any operation. 

We should make these warnings identical to those of Jackrabbit 2.

"	OAK	Closed	3	1	7065	concurrency
13098752	SegmentWriteOperation.isOldGeneration() too eager	"The {{SegmentWriteOperation.isOldGeneration()}} predicate includes some segments that are not ""old"". This leads to more deferred compaction operations than strictly necessary. The affected segments are those generated by tail compaction. Tail compaction created segments should only be included in the predicate once they are from another full compaction operation. Otherwise referencing such segments is fine as they will not be reclaimed in a cleanup following a tail compaction."	OAK	Closed	3	1	7065	compaction, gc
12781228	Thread.interrupt seems to stop repository	"We have a sporadic problem with Sling's JCR installer 3.3.8 and Oak (tar mk). It seems to timing related: the JCR installer does a Thread#interrupt at one point and sometimes this brings the hole instance to stop. Nothing else is going on any more. 
While of course, a workaround is to remove the Thread.interrupt call in the JCR installer (which we did, see SLING-4477), I have the fear that this can happen with any code that is using the repository and gets interrupted.
This error is hard to reproduce, however with three people testing we could see this several times happening"	OAK	Closed	2	4	7065	doc-impacting, resilience
13010089	Include option to stop GC in RevisionGCMBean and RepositoryManagementMBean	"With OAK-4765 Oak Segment Tar acquired the capability for stopping a running revision gc task. This is currently exposed via {{SegmentRevisionGCMBean.stopCompaction}}. I think it would make to expose this functionality through {{RevisionGCMBean}} and {{RepositoryManagementMBean}} also/instead. 

[~mreutegg], [~alex.parvulescu] WDYT? Could the document node store also implement this or would we just not support it there? "	OAK	Resolved	3	4	7065	management, production, resilience
12993523	External invocation of background operations	"The background operations (flush, compact, cleanup, etc.) are historically part of the implementation of the {{FileStore}}. They should better be scheduled and invoked by an external agent. The code deploying the {{FileStore}} might have better insights on when and how these background operations should be invoked. See also OAK-3468.
"	OAK	Closed	3	4	7065	technical_debt
12922638	Clean up the fixtures code in core and jcr modules	"oak-core and oak-jcr modules uses the fixture mechanism to provide NodeStore implementations to the unit/integration tests. There is a few problems with the fixture implementation:

* the {{NodeStoreFixture}} class is duplicated between two modules and supports different set of options (eg. the oak-core version doesn't support the RDB node store at all, while the oak-jcr doesn't support MemoryNodeStore)
* it isn't possible to set the MongoDB URL manually from the Maven command line (it can be done for the RDB, though), which makes running the tests on a Mongo replica hard,
* the Mongo fixture doesn't remove the test database after the test is done.

There should be just one NodeStoreFixture implementation (the oak-jcr can reuse the oak-core version), supporting all values of the {{Fixture}} enum. The Mongo fixture should be more customisable and also should clean-up the database."	OAK	Closed	3	3	7065	tech-debt
13068885	Add unit test coverage for IOUtils.writeInt/writeLong and IOUtils.readInt/readLong	"There is no unit test coverage for IOUtils.writeInt(), IOUtils.writeLong(), IOUtils.readInt(), and IOUtils.readLong() in oak-commons.

I am working on a patch and will have one to submit shortly."	OAK	Closed	4	6	7065	easyfix, patch, test
13023222	Increase default size of the observation queue from 1000 to 10000	"To mitigate problems when hitting the observation queue limit (OAK-2683) we should bump the default size from 1000 to 10000.

cc [~stefanegli]"	OAK	Closed	1	4	7065	resilience
12976346	HeavyWriteIT sporadically fails	"I've seen {{HeavyWriteIT}} fail sporadically on my local checkout.

{noformat}
3d13e2927fc0d75454a692ef5c8703880dc2ea0d
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 31b75992-aaf7-4f2b-a5de-b5a268c1fdb3 not found

	at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1377)
	at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1317)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1011)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:972)
	at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:283)
	at org.apache.jackrabbit.oak.segment.SegmentCache.geSegment(SegmentCache.java:80)
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1317)
	at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:111)
	at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:94)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1010)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:906)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:885)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.access$700(SegmentWriter.java:319)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$8.execute(SegmentWriter.java:277)
	at org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.execute(SegmentBufferWriterPool.java:110)
	at org.apache.jackrabbit.oak.segment.SegmentWriter.writeNode(SegmentWriter.java:274)
	at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:111)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:516)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:284)
	at org.apache.jackrabbit.oak.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:91)
{noformat}

I suspect this is a problem with {{isOldGeneration}} itself not being prepared for the old segment actually being gone. "	OAK	Closed	2	1	7065	gc
12767740	Improve monitoring capabilities for TarMk revision gc	Container devoted to improving monitoring of the TarMk revision garbage collection process. The overall goal is to make it more transparent what revision gc does, how it performs, why it failed etc. 	OAK	Closed	3	3	7065	gc, monitoring, tooling
12960315	CLONE - BackgroundThread should log and re-throw instances of Error	If the run method of a {{BackgroundThread}} instance hits an {{Error}} it dies silently. Instead it should log an re-throw the error. 	OAK	Closed	3	4	7065	resilience
12972018	Decouple FileStoreStatsTest	That test is currently unnecessarily strongly tied to the file store, which makes it prone to failing if implementation details in the store change. 	OAK	Closed	3	4	7065	technical_debt
13124679	Segment.toString: Record table should include an index into the hexdump	Currently the Segment dump created in {{Segment.toString}} includes a list of records with their offsets. However these offsets do no match the ones in the subsequent raw byte dump of the segment. We should add a raw offsets to the list of records so finding the actual data that belongs to a record doesn't involve manually fiddling with logical / physical offset translation. 	OAK	Closed	4	4	7065	tooling
12676732	Observation listener PLUS	"Oak should provide an *extended and efficient JCR observation listener* mechanism to support common use cases not handled well by the restricted options of the JCR observation (only base path, node types and raw events). Those cases require listeners to register much more broadly and then filter out their specific cases themselves, thus putting too many events into the observation system and creating a huge overhead due to asynchronous access to the modified JCR data to do the filtering. This easily is a big performance bottleneck with many writes and thus many events.

Previous discussions [on the list|http://markmail.org/message/oyq7fnfrveceemoh] and in OAK-1120, and [latest discussion on the list|http://markmail.org/message/x2l6tv4m7bxjzqqq].

The goals should be:
* performance: handle filtering as early as possible, during the commit, where access to the modified data is already present
* provide robust implementation for typical filtering cases
* provide an asynchronous listener mechanism as in JCR
* minimize effect on the lower levels on Oak (a visible addition in oak-commons or oak-jcr should be enough)
* for delete events, allow filtering on the to-be-deleted data (currently not possible in jcr listeners that run after the fact)
* ignore external cluster events by default; have an extra option if you really want to register for external events
* if possible: design as an extension of the jcr observation to simplify migration for existing code
* if possible: provide an intelligent listener that can work with pure JCR (aka Jackrabbit 2) as well, by falling back to in-listener-filtering
* maybe: synchronous option using the same simple interface (instead of raw Oak plugins itself); however, not sure if there is a benefit if they can only read data and not change or block the session commit

Typical filtering cases:
- paths with globbing support (for example /content/foo/*/something)
- check for property values (equal, not equal, contains etc.), most importantly
sling:resourceType in Sling apps
- allow to check properties on child nodes as well, typically jcr:content
- check for any parent/ancestor as well (e.g. change deep inside a node type = foo structure should be triggered, even if the node with the type wasn't modified; very important to support efficiently)
- node types (already in jcr observation)
- created/modified/deleted events, separate from move/copy
- and more... a custom filter should be possible to pass through (with similar access as the {{Observer}})"	OAK	Closed	3	2	7065	observation, performance
12962623	Fix test failures in SegmentDataStoreBlobGCIT	"{{SegmentDataStoreBlobGCIT#gcWithInlined}}, {{gc}}, {{gcLongRunningBlobCollection}} and {{consistencyCheckWithGc}} fail since the removal of the old cleanup strategy in OAK-4276. 

The test setup needs to be adapted to the brutal strategy: i.e. {{setup()}} needs to simulate so many compaction cycles until a subsequent cleanup actually remove the segments in question. 

This is not sufficient though as then {{SegmentTracker#collectBlobReferences}} causes a SNFE for those segment ids actually removed but still in the segment id tables. "	OAK	Closed	3	3	7065	cleanup, gc
13056257	Remove unused depth parameter SegmentWriteOperation#writeNode and related methods	That depth parameter is a leftover from when the node de-duplication cache used the depth of a node in the tree for its eviction strategy. 	OAK	Closed	4	4	7065	technical_debt
13131600	Document TarMK specific MBeans	Currently the [TarMK documentation|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#monitoring-via-jmx] only mentions {{SegmentRevisionGarbageCollection}}. We should review that paragraph and also include documentation for all other relevant JMX endpoints.	OAK	Closed	3	3	7065	documentation
12707136	Improve LargeOperationIT accuracy for document nodes store fixture	As [noted | https://issues.apache.org/jira/browse/OAK-1414?focusedCommentId=13942016&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13942016] on OAK-1414 {{LargeOperationIT}} is somewhat inaccurate for the document node store fixture where the collected data tends to be noisy. We should look into ways to make  the tests results more accurate for this case.	OAK	Resolved	3	4	7065	test
13035466	Improve the transaction rate of the TarMK	"The TarMK's write throughput is limited by the way concurrent commits are processed: rebasing and running the commit hooks happen within a lock without any explicit scheduling. This epic covers improving the overall transaction rate. The proposed approach would roughly be to first make scheduling of transactions explicit, then add monitoring on transaction to gather a better understanding and then experiment and implement explicit scheduling strategies to optimise particular aspects. 

h2. Summary of ideas mentioned in an offline sessions

h3. Advantages of explicit scheduling:
* Control over (order) of commits
* Sophisticated monitoring (commit statistics, e.g. commit rate, time in queue, etc.) 
* Favour certain commits (e.g. checkpoints)
* Reorder commits to simplify rebasing
* Suspend the compactor on concurrent commits and have it resume where it left off afterwards
* Parallelise certain commits (e.g. by piggy backing)
* Implement a concurrent commit editor. we'd need to take care of proper access to the shared state; [~frm] maybe introduce the idea of a common context to enforce concurrent access semantics.

h3. Scheduler Implementation
* Expedite
* Prioritise
* Defer
* Collapse
* Coalesce
* Parallelise
* Piggy back: can we piggy back commits on top of each other? The idea would be while processing the changes of one commit to also check them for conflicts with the changes of other commits waiting to commit. If a conflict is detected there, that other commit can immediately be failed (given the current commit doesn't fail).
* Merging non conflicting commits. Given multiple transactions ready to commit at the same time. Can we process them as one (given they don't conflict) instead of one after each other, which requires rebasing the later transaction to be rebase on the former.
* Shield the file store from {{InterruptedException}} because of thread boundaries introduced
* Implement tests, benchmarks and fixtures for verification
"	OAK	Resolved	3	15	7065	scalability
12976915	Implement a proper template cache	"The template cache is currently just a map per segment. This is problematic in various ways: 
* A segment needs to be in memory and probably loaded first only to read something from the cache. 
* No monitoring, instrumentation of the cache
* No control over memory consumption 

We should there for come up with a proper template cache implementation in the same way we have done for strings ({{StringCache}}) in OAK-3007. Analogously that cache should be owned by the {{CachingSegmentReader}}. "	OAK	Closed	3	4	7065	cache, monitoring, production
12735772	Long running JCR session prevent live cleanup in Segment FileStore	"Cleanup operation in SegmentNodeStore detects the un referenced garbage and clean it up. To determine the reference validity it starts with an initial set of SegmentId which have a live java reference. 

This works fine for simple setup but when Oak repository is used in an application (like Sling) where application code can create long running session (for observation) then such session are bound to old NodeState at time of startup. Such references prevent the cleanup logic to remove older revisions while system is running. Such revisions can only be removed via an offline compaction-> cleanup.

Need to find out a way where we can _migrate_ such old NodeState references to newer revisions"	OAK	Resolved	3	1	7065	gc
12783856	SegmentOverflowException in HeavyWriteIT on Jenkins	"{noformat}
heavyWrite(org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT)  Time elapsed: 96.384 sec  <<< ERROR!
org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowException: Segment cannot have more than 255 references 47a9dc3c-c6f9-4b5f-a61a-6711da8b68c2
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.getSegmentRef(SegmentWriter.java:353)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:382)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapLeaf(SegmentWriter.java:426)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:484)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:511)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMap(SegmentWriter.java:720)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1108)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1091)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:396)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1082)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1110)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:97)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:83)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:79)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:501)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:507)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createProperties(HeavyWriteIT.java:137)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:129)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:130)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:110)
{noformat}

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/35/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=integrationTesting/

cc [~alex.parvulescu]"	OAK	Closed	2	1	7065	CI, jenkins
12961701	Garbage left behind when compaction does not succeed	"As a result of the new cleanup approach introduced with OAK-3348 (brutal) a compaction cycle that is not successful (either because of cancellation of because of giving up waiting for the lock) leaves garbage behind, which is only cleaned up 2 generations later. 

We should look into ways to remove such garbage more pro-actively. 
"	OAK	Closed	3	4	7065	cleanup, compaction, gc
12934509	Log ids of segments being released for gc because of their age. 	When {{CompactionStrategy.CleanupType#CLEAN_OLD}} releases a segment for gc because of its age it should log a message. This helps to determine the root cause of a {{SNFE}}. 	OAK	Closed	3	17	7065	cleanup, gc
12977379	Setup Windows builds 	As [discussed | http://markmail.org/message/2dk6i3yxjfkknrzp] we should also have CI coverage on Windows.	OAK	Closed	1	17	7065	CI, build, infrastructure, jenkins
12727885	Wrong values reported for OBSERVATION_EVENT_DURATION	The value reported for the {{RepositoryStatistics.Type#OBSERVATION_EVENT_DURATION}} statistic is wrong. Instead of the total time spent *processing* observation events it reports the total time *producing* observation events. 	OAK	Closed	3	1	7065	monitoring, observation
13110217	Refactor FileStore.close	{{FileStore.close}} should take better advantage of the {{Closer}} instance to close its resources (including the file store lock). Also the order of the close calls should be aligned with their dependencies. 	OAK	Closed	3	4	7065	refactoring, technical_debt
13141749	Remove debug logging to the console during tests	OAK-4707 introduced logging at debug logging to the system console for sorting out test failures on Jenkins. Since we haveen't seen these failures for a while and that issue is fixed I would like to remove the extra logging again to avoid cluttering the console unnecessarily.	OAK	Closed	3	4	7065	technical_debt
12929747	Don't pass the compaction map to FileStore.cleanup	That argument is unused and I'll remove it thus. 	OAK	Closed	4	17	7065	technical_debt
12724756	MAX_QUEUED_CONTINUATIONS feature not working in EventGenerator class	"Since OAK-1422 the  {{Continuation}} created in {{fullQueue()}} is put to the front of the List. This causes it to be taken right off the list again on the next call to {{generate()}} instead of first continuing with the rest of the list allowing it to shrink. As a result the list may grow up to 2 x {{MAX_QUEUED_CONTINUATIONS}} instead of 1 + {{MAX_QUEUED_CONTINUATIONS}} as anticipated. 

"	OAK	Closed	3	1	7065	Observation
13113047	TarMK disk space check is not synchronized with FileStore opened state	"It seems that the disk space check is not properly synchronized with {{FileStore}} as I revealed a race condition while using oak-upgrade during migration to {{segment-tar}}.

The {{FileStore}} instance is closed while TarMK disk check tries to execute and it seems it is dependent on the state of segment ({{org.apache.jackrabbit.oak.segment.file.FileStore.checkDiskSpace(FileStore.java:541)}} that needs to be opened. 

{noformat}
30.10.2017 11:26:05.834 WARN   o.a.j.o.s.f.Scheduler: The scheduler FileStore background tasks takes too long to shut down
30.10.2017 11:26:11.674 INFO   o.a.j.o.s.f.FileStore: TarMK closed: /data/cq/crx-quickstart/repository-segment-tar-20171030-112401/segmentstore
30.10.2017 11:26:11.676 ERROR  o.a.j.o.s.f.SafeRunnable: Uncaught exception in TarMK disk space check [/data/cq/crx-quickstart/repository-segment-tar-20171030-112401/segmentstore]
java.lang.IllegalStateException: already shut down
    at org.apache.jackrabbit.oak.segment.file.ShutDown.keepAlive(ShutDown.java:42)
    at org.apache.jackrabbit.oak.segment.file.FileStore.size(FileStore.java:302)
    at org.apache.jackrabbit.oak.segment.file.FileStore.checkDiskSpace(FileStore.java:541)
    at org.apache.jackrabbit.oak.segment.file.FileStore.access$300(FileStore.java:102)
    at org.apache.jackrabbit.oak.segment.file.FileStore$3.run(FileStore.java:237)
    at org.apache.jackrabbit.oak.segment.file.SafeRunnable.run(SafeRunnable.java:67)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
{noformat}"	OAK	Closed	4	1	7065	concurrency, production
13103614	ReadOnly connection to fresh SegmentNodeStore setup failing	"Started a server with Oak version 1.7.7 and tried to connect oak-run-1.7.7 to same setup. This resulted in following exception

{noformat}
2017-09-20 19:47:22,213 INFO  [main] o.a.j.o.segment.file.FileStore - Creating file store FileStoreBuilder{version=1.7.7, directory=/path/to/repository/segmentstore, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.oak.plugins.blob.datastore.OakFileDataStore], maxFileSize=256, segmentCacheSize=256, stringCacheSize=256, templateCacheSize=64, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1048576, memoryMapping=true, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}} 
2017-09-20 19:47:22,243 WARN  [main] o.a.j.o.s.file.tar.TarReader - Unable to load index of file data00000a.tar: Unrecognized magic number 
2017-09-20 19:47:22,243 INFO  [main] o.a.j.o.s.file.tar.TarReader - No index found in tar file data00000a.tar, skipping... 
2017-09-20 19:47:22,243 WARN  [main] o.a.j.o.s.file.tar.TarReader - Could not find a valid tar index in /path/to/repository/segmentstore/data00000a.tar, recovering read-only 
2017-09-20 19:47:22,243 INFO  [main] o.a.j.o.s.file.tar.TarReader - Recovering segments from tar file /path/to/repository/segmentstore/data00000a.tar 
2017-09-20 19:47:22,315 INFO  [main] o.a.j.o.s.file.tar.TarReader - Regenerating tar file/path/to/repository/segmentstore/data00000a.tar.ro.bak 
2017-09-20 19:47:22,460 ERROR [main] o.a.j.oak.index.IndexCommand - Error occurred while performing index tasks 
java.lang.IllegalArgumentException: invalid segment buffer
	at org.apache.jackrabbit.oak.segment.data.SegmentDataLoader.newSegmentData(SegmentDataLoader.java:37) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.data.SegmentData.newSegmentData(SegmentData.java:66) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.writeSegment(AbstractFileStore.java:212) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.access$000(AbstractFileStore.java:66) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore$1.recoverEntry(AbstractFileStore.java:125) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarReader.generateTarFile(TarReader.java:213) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarReader.openRO(TarReader.java:162) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles.<init>(TarFiles.java:298) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles.<init>(TarFiles.java:58) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles$Builder.build(TarFiles.java:167) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.ReadOnlyFileStore.<init>(ReadOnlyFileStore.java:74) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.FileStoreBuilder.buildReadOnly(FileStoreBuilder.java:383) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.SegmentTarFixtureProvider.configureSegment(SegmentTarFixtureProvider.java:63) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.NodeStoreFixtureProvider.create(NodeStoreFixtureProvider.java:71) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.NodeStoreFixtureProvider.create(NodeStoreFixtureProvider.java:47) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.index.IndexCommand.execute(IndexCommand.java:98) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49) [oak-run-1.7.7.jar:1.7.7]

{noformat}

Post restart of server oak-run was able to connect fine. So looks like issue with very fresh setup only

Command used for oak-run
{noformat}
java -jar oak-run-1.7.7.jar index --fds-path=/path/to/repository/datastore --checkpoint head --reindex --index-paths=/oak:index/lucene /path/to/repository/segmentstore --metrics
{noformat}"	OAK	Closed	4	1	7065	tooling
12861233	TarMK cleanup blocks writers	TarMK cleanup exclusively locks the {{FileStore}}, which causes concurrent writers to block until cleanup finished. Initially cleanup was expected to be reasonably fast, however I have seen it taking dozens of minutes under certain circumstances (most likely many tar files with many small segments, aka OAK-2896).	OAK	Closed	3	4	7065	cleanup, gc
12692131	Only one Observer per session	"As mentioned in OAK-1332, a case where a single session registers multiple observation listeners can be troublesome if events are delivered concurrently to all of those listeners, since in such a case the {{NamePathMapper}} and other session internals will likely suffer from lock contention.

A good way to avoid this would be to have all the listeners registered within a single session be tied to a single {{Observer}} and thus processed sequentially.

Doing so would also improve performance as the listeners could leverage the same content diff. As the listeners come from a single session and thus presumably from a single client, there's no need to worry about one client blocking the work of another."	OAK	Resolved	3	4	7065	observation
12851653	Deadlock between persisted compaction map and cleanup	"Just seen this deadlock while running {{SegmentCompactionIT}}:

{noformat}
""TarMK flush thread [target/SegmentCompactionIT3250704011919039778dir], active since Wed Aug 05 09:25:57 GMT+00:00 2015, previous max duration 2325ms"" daemon prio=10 tid=0x00007f5674872800 nid=0x5dc8 waiting for monitor entry [0x00007f5666a00000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:145)
	- waiting to lock <0x0000000707fc7fe8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.compress(PersistedCompactionMap.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.remove(PersistedCompactionMap.java:155)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.remove(CompactionMap.java:108)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.cleanup(FileStore.java:694)
	- locked <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:628)
	- locked <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	- locked <0x00000007000aed60> (a java.util.concurrent.atomic.AtomicReference)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$1.run(FileStore.java:413)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.jackrabbit.oak.plugins.segment.file.BackgroundThread.run(BackgroundThread.java:70)

""pool-1-thread-34"" prio=10 tid=0x00007f55ec002800 nid=0x5dea waiting for monitor entry [0x00007f56648de000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:904)
	- waiting to lock <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.readSegment(SegmentTracker.java:210)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:149)
	- locked <0x0000000707fc7fe8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:215)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:121)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:103)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.get(CompactionMap.java:93)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.uncompact(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1098)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:433)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:406)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}"	OAK	Closed	2	1	7065	cleanup, compaction, gc
12777494	Implement MBean monitoring garbage collection	"Provide monitoring for the garbage collection process:
* time series of repository size
* time series of space reclaimed
* time stamp of last clean up
* time stamp of last compaction
* last error
* time stamp when next gc run is scheduled
* ..."	OAK	Closed	3	7	7065	compaction, gc, monitoring
13134438	Add configurable repository size cap to SegmentOverflowExceptionIT	"{{SegmentOverflowExceptionIT}} potentially consumes a lot of disk space. Running it for 10 minutes on a AWS m4.4xlarge instance with 900 / 3000 IOPS resulted in 80GB being taken up. 
Currently the test can be time boxed but not size boxed. I suggest to add another option to cap the repository size in addition to the existing {{-Dtimeout}}."	OAK	Closed	3	4	7065	test
12767762	Auto-refresh sessions on revision gc	"The approach to revision garbage collection taken in OAK-2192 assumes that long running background sessions call refresh once they become active again. Incidentally this is true as such background sessions usually are admin sessions and those are always auto-refreshed on access (see OAK-88, OAK-803, and OAK-960). However as soon as we move away from admin sessions this might not be true any more and we might start seeing {{SegmentNotFoundException}} s unless the user explicitly refreshes the session. 

To prevent this we should make all sessions auto refresh once revision gc runs. "	OAK	Closed	3	4	7065	gc
12823366	oak-run: register JMX beans 	When starting up oak with oak-run the JMX beans are not registered, but it would be convenient for the registration to happen.	OAK	Closed	4	4	7065	tooling
13057868	Improve cache statistics of the segment cache	"The statistics provided by the segment cache are off due to the fact it serves as 2nd level cache: as it doesn't see all the hits in the 1st level cache ({{SegmentId.getSegment()}}), it reports a hit/miss rate that is to low. 

We should look into how we could expose better statistics wrt. caching of segments. Possible consolidated over 1st and 2nd level caches. "	OAK	Closed	3	4	7065	monitoring
13116443	FileStore.compact does not persist compacted head to journal	"When {{FileStore.compact()}} returns the {{journal.log}} does not necessarily contain the head created by the compactor. This can lead to problems downstream like e.g. in OAK-6894 where the compactor tool wrote the wrong (i.e. uncompacted) head to the {{journal.log}}. 

Proposed fix is to call on of the {{FileStore.flush()}} methods after compaction and add a test case that verifies the {{journal.log}} contains the correct head state. "	OAK	Closed	3	1	7065	compaction, gc
13082516	Refactor monitoring of deduplication caches	Currently monitoring is of the deduplication caches is hard wired into the cache manager. It would be cleaner (and is in fact a pre-requisite for OAK-5790) to decouple the monitoring from the caches. 	OAK	Closed	3	4	7065	technical_debt
12682951	Failure in ObservationRefreshTest 	"Failed tests:   observation[2](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest): added nodes expected:<1000> but was:<442>

Tests run: 4, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 106.957 sec <<< FAILURE!
observation[3](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest)  Time elapsed: 53.047 sec  <<< FAILURE!
java.lang.AssertionError: added nodes expected:<1000> but was:<906>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest.observation(ObservationRefreshTest.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:695)
observation[2](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest)  Time elapsed: 58.379 sec  <<< FAILURE!
java.lang.AssertionError: added nodes expected:<1000> but was:<396>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest.observation(ObservationRefreshTest.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:695)
"	OAK	Closed	3	1	7065	observation
12961884	FileStore.flush prone to races leading to corruption	"There is a small window in {{FileStore.flush}} that could lead to data corruption: if we crash right after setting the persisted head but before any delay-flushed {{SegmentBufferWriter}} instance flushes (see {{SegmentBufferWriterPool.returnWriter()}}) then that data is lost although it might already be referenced from the persisted head.

We need to come up with a test case for this. 

A possible fix would be to return a future from {{SegmentWriter.flush}} and rely on a completion callback. Such a change would most likely also be useful for OAK-3690. 
"	OAK	Closed	2	1	7065	resilience
12767742	Provide more information in SegmentNotFoundException	"There is currently no way to distinguish between a {{SegmentNotFoundException}} occurring because of a removed segment by gc or because of another corruption. Optimally we would tell in the exception why the segment is gone, how old it was when gc removed it and who/what was still referring to it at that time. In order to do that, we probably need some kind of log for the following data: When a segment was removed (because a new generation of the .tar file was made, or because the .tar file was removed), we should log the segment, the file name, and the date+time of the removal. If the segment was then not found because it was too old, then another type of exception should be thrown instead, for example ""ReadTimeoutException"", with a message that contains as much data as possible: the data+time of the segment, date+time of the removal of the segment, about when compaction was run, date+time of the session login and last refresh, the stack trace of where the session was acquired."	OAK	Closed	3	7	7065	gc, monitoring
13057842	Unify and simplify the deduplication caches 	"As a preparation to enable better monitoring and add more precise monitoring probes to the deduplication caches I would like to unify their interfaces and simplify their setup. 
* Don't expose the cache statistics via the {{FileStore}} and leverage the the {{FileStoreBuilder}} instead for this.
* All deduplication caches should implement a unified {{Cache}} interface to simplify wrapping them (e.g. for additional access statistics collection). 
* Replace the ad-hoc collection of cache statistics in the {{NodeWriteStats}} inner class of the {{SegmentWriter}} and replace it with a more structured approach. 
* Expose additional cache access statistics via Metrics. 
* The additional statistics should discriminate caches access occurring as regular writes from such occurring during compaction. "	OAK	Closed	3	4	7065	monitoring, refactoring, technical_debt
12754445	Add metadata about the changed value to a PROPERTY_CHANGED event on a multivalued property	"When getting _PROPERTY_CHANGED_ events on non-multivalued properties only one value can have actually changed so that handlers of such events do not need any further information to process it and eventually work on the changed value; on the other hand _PROPERTY_CHANGED_ events on multivalued properties (e.g. String[]) may relate to any of the values and that brings a source of uncertainty on event handlers processing such changes because there's no mean to understand which property value had been changed and therefore to them to react accordingly.
A workaround for that is to create Oak specific _Observers_ which can deal with the diff between before and after state and create a specific event containing the ""diff"", however this would add a non trivial load to the repository because of the _Observer_ itself and because of the additional events being generated while it'd be great if the 'default' events would have metadata e.g. of the changed value index or similar information that can help understanding which value has been changed (added, deleted, updated). "	OAK	Closed	3	4	7065	observation
13014772	Simplify GCListener	We should simplify {{GCListener}} to minimise the boilerplate necessary in {{FileStoreBuilder}}. 	OAK	Closed	3	4	7065	technical_debt
13042569	Revisit FileStoreStats mbean stats format	This is a bigger refactoring item to revisit the format of the exposed data, moving towards having it in a more machine consumable friendly format.	OAK	Resolved	3	4	7065	monitoring
13049537	OOM in SegmentReferenceLimitTestIT	Running that IT (with 4g heap) currently results in an {{OOME}}. We need to check whether the expectations are still valid for Segment Tar and either adapt the test or look into the memory consumption. 	OAK	Closed	3	1	7065	test-failure
12936278	Forward edges missing in SegmentGraph 	"The graph produced by {{oak-run graph}} does not include forward edges (i.e. references from older segments to newer segments). Such references where introduced with  OAK-1828. See also OAK-3864, where this has been fixed for the file store cleanup.

"	OAK	Closed	3	1	7065	cleanup, gc, technical_debt, tooling
12702110	Expose RevisionGCMBean for supported NodeStores 	{{NodeStore}} implementations should expose the {{RevisionGCMBean}} in order to be interoperable with {{RepositoryManagementMBean}}. See OAK-1160.	OAK	Closed	3	4	7065	monitoring
12711671	ConcurrentConflictTest fails occasionally	"Occurs every now and then on buildbot. E.g.:
http://ci.apache.org/builders/oak-trunk-win7/builds/16"	OAK	Closed	4	1	7404	concurrency
13045626	Perform update of single node in one remote call if possible	"If a single node is modified in a commit then currently it performs 2 remote calls

# The actual update
# Update of commit root

as for single node update commitRoot == node being updated we can optimize this case to see if both operations can be done in same call"	OAK	Closed	3	4	7404	performance
12706946	document atomicity of DS.update(collection, keys, update)	"Please document (I'll assume it's similar to ""remove"", in that it is ""best effort"")?"	OAK	Closed	3	1	7404	concurrency
12986196	diff calculation in DocumentNodeStore should try to re-use journal info on diff cache miss	"Currently, diff information is filled into caches actively (local commits pushed in local_diff, externally read changes pushed into memory_diff). At the time of event processing though, the entries could have already been evicted.
In that case, we fall back to computing diff by comparing 2 node-states which becomes more and more expensive (and eventually fairly non-recoverable leading to OAK-2683).

To improve the situation somewhat, we can probably try to consult journal entries to read a smaller-superset of changed paths before falling down to comparison.

/cc [~mreutegg], [~chetanm], [~egli]"	OAK	Closed	4	4	7404	observation, resilience
13047677	Oak upgrade usage note refers to oak-run	"Running {{java -jar oak-upgrade*.jar}} prints 

{noformat}
Usage: java -jar oak-run-*-jr2.jar upgrade [options] jcr2_source [destination]
       (to upgrade a JCR 2 repository)

       java -jar oak-run-*-jr2.jar upgrade [options] source destination
       (to migrate an Oak repository)
{noformat}

Which incorrectly refers to {{oak-run upgrade}}. The latter will send me back to {{oak-run}}: ""This command was moved to the oak-upgrade module"". "	OAK	Closed	4	1	7404	production, tooling, usability
12841279	Suspend commit on conflict	"A DocumentNodeStore cluster currently shows a conflict behavior, which
is not intuitive. A modification may fail with a conflict even though
before and after the conflict, the external change is not visible to
the current session. There are two aspects to this issue.

1) a modification may conflict with a change done on another cluster
node, which is committed but not yet visible on the current cluster node.

2) even after the InvalidItemStateException caused by the conflict, a
refreshed session may still not see the external change.

The first aspect is a fundamental design decision and cannot be changed
easily.

The second part can be addressed by suspending the commit until the external
conflict becomes visible on the current cluster node. This would at least
avoid the awkward situation where the external change is not visible after
the InvalidItemStateException.

The system would also become more deterministic. A commit currently goes
into a number of retries with exponential back off, but there's no guarantee
the external modification becomes visible within those retries. 
"	OAK	Closed	3	4	7404	resilience
12930181	Avoid commit from too far in the future (due to clock skews) to go through	"Following up [discussion|http://markmail.org/message/m5jk5nbby77nlqs5] \[0] to avoid bad commits due to misbehaving clocks. Points from the discussion:
* We can start self-destruct mode while updating lease
* Revision creation should check that newly created revision isn't beyond leaseEnd time
* Implementation done for OAK-2682 might be useful

[0]: http://markmail.org/message/m5jk5nbby77nlqs5"	OAK	Closed	3	4	7404	resilience
12861552	Commit may add collision marker for committed revision	It may happen that a commit adds a collision marker for a revision which is already committed. {{Collision.markCommitRoot()}} does not perform a conditional update when it adds the collision marker. Though, it checks the document after the update if the marked revision is committed.	OAK	Closed	4	1	7404	resilience
12822650	Time limit for HierarchicalInvalidator	This issue is related to OAK-2646. Every now and then I see reports of background reads with a cache invalidation that takes a rather long time. Sometimes minutes. It would be good to give the HierarchicalInvalidator an upper limit for the time it may take to perform the invalidation. When the time is up, the implementation should simply invalidate the remaining documents.	OAK	Resolved	3	4	7404	resilience
12856839	DocumentNodeStore.retrieve() should not throw IllegalArgumentException	"{{DocumentNodeSTore#retrieve(checkpoint)}} may throw an {{IllegalArgumentException}} via {{Revision.fromString(checkpoint)}}.

The javadocs say that it returns a {{NodeState}} or {{null}}. The exception prevents recovery of {{AsyncIndexUpdate}} from a bad recorded checkpoint."	OAK	Closed	4	4	7404	resilience
12830954	ArrayIndexOutOfBoundsException in UnsavedModifications.put()	"In rare cases a commit may fail to update the pending changes on {{_lastRev}}    of documents. The stack trace is:

{noformat}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.mapdb.BTreeMap.replace(BTreeMap.java:1174)
        at org.apache.jackrabbit.oak.plugins.document.UnsavedModifications.put(UnsavedModifications.java:90)
        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$10.track(DocumentNodeStore.java:1990)
        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.applyChanges(DocumentNodeStore.java:1056)
        at org.apache.jackrabbit.oak.plugins.document.Commit.applyToCache(Commit.java:598)
        at org.apache.jackrabbit.oak.plugins.document.CommitQueue.afterTrunkCommit(CommitQueue.java:127)
        at org.apache.jackrabbit.oak.plugins.document.CommitQueue.done(CommitQueue.java:83)
{noformat}"	OAK	Closed	2	1	7404	resilience
12836776	Broken link on documentation site	http://jackrabbit.apache.org/oak/docs/command_line.html points to http://jackrabbit.apache.org/oak/docs/oak-mongo-js/oak.html, which doesn't exit. 	OAK	Resolved	4	1	7404	documentation
13291031	ObservationManager.addEventListener() throws NPE with invalid paths in filter	"While registering a resource change listener, we encountered the following exception : 

 
{code:java}
05.03.2020 23:39:00.728 *ERROR* [FelixDispatchQueue] org.apache.sling.resourceresolver FrameworkEvent ERROR (java.lang.NullPointerException)
java.lang.NullPointerException: null
at org.apache.jackrabbit.oak.commons.PathUtils.unifyInExcludes(PathUtils.java:501) [org.apache.jackrabbit.oak-commons:1.8.17]
at org.apache.jackrabbit.oak.jcr.observation.ObservationManagerImpl.addEventListener(ObservationManagerImpl.java:240) [org.apache.jackrabbit.oak-jcr:1.8.17]
at org.apache.sling.jcr.resource.internal.JcrListenerBaseConfig.register(JcrListenerBaseConfig.java:136) [org.apache.sling.jcr.resource:3.0.16.1]
{code}
 

On further debugging, we found that issues lies in this snippet : 
{code:java}
if (exclude.equals(include) || isAncestor(exclude, include)) {
 includesRemoved.add(include);{code}
'exclude' can be null if the getOakPath() method returns a null. This NPE causes listeners(ResourceChangeListener in our case) to fail at registration."	OAK	Closed	4	1	7404	Observation
12705591	Creating multiple checkpoint on same head revision overwrites previous entries	"Currently when a checkpoint is created in DocumentNodeStore then it is saved in form of currentHeadRev=>expiryTime. Now if multiple checkpoints are created where head revision has not changed then only the last one would be saved and previous entries would be overridden as revision is used as key

One fix would be to change the expiry time only if the new expiry time is greater than previous entry. However doing that safely in a cluster (check then save) is currently not possible with DocumentStore API as the modCount check if only supported for Nodes.

"	OAK	Closed	4	1	7404	resilience
13542113	Node.addMixin() may overwrite existing mixins	"A Session lacking permission to read property jcr:mixinTypes, but permission to write will overwrite existing mixins when calling Node.addMixin().

The implementation does not check if the session has permission to read jcr:mixinTypes and assumes there are no existing values when the session does not have permission. The result is a jcr:mixinTypes property with only a single value passed to addMixin()."	OAK	Closed	3	1	7404	candidate_oak_1_22
12697726	Failing test for MergeSortedIterators	"While running Oak in a two node clutser following exception is seen. It basically comes because the AsynchUpdate tries to update async-status concurrently

{noformat}
27.11.2013 17:56:35.507 *ERROR* [pool-5-thread-1] org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@fcf98c2 : com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diff(MongoMK.java:165) ~[na:na]
	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:481) ~[na:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:103) ~[na:na]
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler:2.4.2]
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler:2.4.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.getNode(MongoNodeStore.java:507) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffFewChildren(MongoMK.java:313) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffImpl(MongoMK.java:229) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:168) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:165) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 11 common frames omitted
Caused by: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:103) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.next(MergeSortedIterators.java:85) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getLatestValue(NodeDocument.java:1041) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getNodeAtRevision(NodeDocument.java:456) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.readNode(MongoNodeStore.java:653) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.access$000(MongoNodeStore.java:80) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:510) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:507) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 23 common frames omitted
{noformat}"	OAK	Closed	3	1	7404	cluster
12825863	Change default for oak.maxLockTryTimeMultiplier	The default multipler is currently 3, which translates into a lock try timeout of 6 seconds. This is rather low and may result in merge failures even when a commit acquired the merge lock exclusively. I would like to increase it to 30.	OAK	Closed	4	4	7404	doc-impacting, resilience
12930165	Collision may mark the wrong commit	In some rare cases it may happen that a collision marks the wrong commit. OAK-3344 introduced a conditional update of the commit root with a collision marker. However, this may fail when the commit revision of the condition is moved to a split document at the same time.	OAK	Closed	4	1	7404	resilience
12842397	Add a compound index for _modified + _id	"As explained in OAK-1966 diff logic makes a call like

bq. db.nodes.find({ _id: { $gt: ""3:/content/foo/01/"", $lt: ""3:/content/foo010"" }, _modified: { $gte: 1405085300 } }).sort({_id:1})

For better and deterministic query performance we would need to create a compound index like \{_modified:1, _id:1\}. This index would ensure that Mongo does not have to perform object scan while evaluating such a query.

Care must be taken that index is only created by default for fresh setup. For existing setup we should expose a JMX operation which can be invoked by system admin to create the required index as per maintenance window"	OAK	Closed	1	4	7404	performance, resilience
13064699	Avoid reads from MongoDB primary	"With OAK-2106 Oak now attempts to read from a MongoDB secondary when it detects the requested data is available on the secondary.

When multiple Oak cluster nodes are deployed on a MongoDB replica set, many reads are still directed to the primary. One of the reasons why this is seen in practice, are observers and JCR event listeners that are triggered rather soon after a change happens and therefore read recently modified documents. This makes it difficult for Oak to direct calls to a nearby secondary, because changes may not yet be available there.

A rather simple solution for the observers may be to delay processing of changes until they are available on the near secondary.

A more sophisticated solution discussed offline could hide the replica set entirely and always read from the nearest secondary. Writes would obviously still go to the primary, but only return when the write is available also on the nearest secondary. This guarantees that any subsequent read is able to see the preceding write."	OAK	Closed	3	4	7404	scalability
12839549	Use batch-update in backgroundWrite	"(From an earlier [post on the list|http://markmail.org/thread/mkrvhkfabit4osli]) The DocumentNodeStore.backgroundWrite goes through the heavy work of updating the lastRev for all pending changes and does so in a hierarchical-depth-first manner. Unfortunately, if the pending changes all come from separate commits (as does not sound so unlikely), the updates are sent in individual update calls to mongo (whenever the lastRev differs). Which, if there are many changes, results in many calls to mongo.

OAK-2066 is about extending the DocumentStore API with a batch-update method. That one, once available, should thus be used in the {{backgroundWrite}} as well."	OAK	Closed	3	4	7404	performance
12991357	Move DocumentMK specific methods from DocumentNodeStore	There are some DocumentMK specific methods in DocumentNodeStore, which should be moved to the DocumentMK.	OAK	Closed	4	17	7404	technical_debt
12862191	Reduce PerfLogger isDebugEnabled overhead	"If a node is cached, 1/4 of the time which is used to call DocumentNodeStore.getNode is spent in PerfLogger.start and PerfLogger.end just for checking whether or not debug logging is enabled (this is likely much less if no TurboFilters are used).

To reduce the overhead of the PerfLogger, it should not check if debug is enabled in end() if start is below 0 anyway. Moreover, it would help to check only every second if debug is really enabled."	OAK	Closed	4	4	7404	performance
12987119	Cache update blocks new commits	When caches are updated after a commit (within CommitQueue.Callback.headOfQueue()), other threads are blocked when they try to acquire new revisions from the queue.	OAK	Closed	4	4	7404	concurrency
13172627	Prevent commits in the past	This is similar to OAK-3883, but must prevent commits with revisions that are older than already present in the repository. At runtime, this is already taken care of with static fields in the Revision class, but on startup the clock may have jumped into the past since Oak was stopped.	OAK	Closed	4	4	7404	resilience
12842389	Use a lower bound in VersionGC query to avoid checking unmodified once deleted docs	"As part of OAK-3062 [~mreutegg] suggested

{quote}
As a further optimization we could also limit the lower bound of the _modified
range. The revision GC does not need to check documents with a _deletedOnce
again if they were not modified after the last successful GC run. If they
didn't change and were considered existing during the last run, then they
must still exist in the current GC run. To make this work, we'd need to
track the last successful revision GC run. 
{quote}

Lowest last validated _modified can be possibly saved in settings collection and reused for next run"	OAK	Closed	3	4	7404	performance
12702434	DocumentNS: Implement refined conflict resolution for addExistingNode conflicts	Implement refined conflict resolution for addExistingNode conflicts as defined in the parent issue for the document NS.	OAK	Open	3	7	7404	resilience
13002722	Leaderboard in ConsolidatedListenerMBean	"ConsolidatedListenerMBean contains various stats about JCR event listeners. However, it is rather difficult to get an overview of how expensive listeners are.

The MBean should expose a simple leaderboard that orders the listener according to the processing time (producer & consumer time)."	OAK	Closed	4	4	7404	observation
12857550	Optimize NodeDocument.getNewestRevision()	Most of the time NodeDocument.getNewestRevision() is able to quickly identify the newest revision, but sometimes the code falls to a more expensive calculation, which attempts to read through available {{_revisions}} and {{_commitRoot}} entries. If either of those maps are empty, the method will go through the entire revision history.	OAK	Closed	3	4	7404	performance
12744000	Release Oak 1.0.7	Issues for 1.0.7: https://issues.apache.org/jira/issues/?jql=project%20%3D%20OAK%20AND%20fixVersion%20%3D%201.0.7	OAK	Resolved	3	3	7404	Release
13133362	Node.getMixinNodeTypes() may check for child node named jcr:mixinTypes	In some cases a call to {{Node.getMixinNodeTypes()}} may result in a check whether there is a child node named {{jcr:mixinTypes}}. 	OAK	Closed	4	4	7404	performance
13536160	NPE while trying to get the BinaryDownload URI	"The line of code at [BinaryImpl.java:L104|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-store-spi/src/main/java/org/apache/jackrabbit/oak/plugins/value/jcr/BinaryImpl.java#L104] should be handling the possibility that {{getBinaryValue()}} could return {{null}}.

When trying to get a SAS uri for a binary, we got the following error
{noformat}
java.lang.NullPointerException: null
	at org.apache.jackrabbit.oak.plugins.value.jcr.BinaryImpl.getURI(BinaryImpl.java:104) [org.apache.jackrabbit.oak-store-spi:1.48.0.T20230202132234-aa49252]
{noformat}

Unfortunately we don't know much about the resource in question and why the {{value}} of this would not be {{{}PropertyType.BINARY{}}}."	OAK	Closed	4	1	7404	oak
12940564	DocumentNodeStore: required server time accuracy	"The DocumentNodeStore currently requires that the local time and the persistence time differ at most 2 seconds.

I recently tried to run a cluster with two Windows machines, and despite them being configured to use the same NTP service, they were still 4..5 s off.

https://blogs.technet.microsoft.com/askds/2007/10/23/high-accuracy-w32time-requirements/ seems to confirm that by default, Windows can't provide the required accuracy.

One workaround seems to be to install custom ntp clients; but do we really want to require this?"	OAK	Closed	4	20	7404	documentation
12701867	Incorrect handling of addExistingNode conflict in NodeStore	"{{MicroKernel.rebase}} says: ""addExistingNode: node has been added that is different from a node of them same name that has been added to the trunk.""

However, the {{NodeStore}} implementation
# throws a {{CommitFailedException}} itself instead of annotating the conflict,
# also treats the equal childs with the same name as a conflict. "	OAK	Open	3	1	7404	concurrency, observation, technical_debt
13015847	Remove DocumentStore.update()	OAK-3018 removed the single production usage of DocumentStore.update(). I propose we remove the method to reduce maintenance.	OAK	Closed	3	3	7404	technical_debt
12781432	Annotate intermediate docs with property names	"Reading through a ValueMap can be very inefficient if the changes of a given
property are distributed sparsely across the previous documents. The current
implementation has to scan through the entire set of previous documents to
collect the changes.

Intermediate documents should have additional information about what properties
are present on referenced previous documents. 
"	OAK	Open	3	4	7404	performance
12987139	Add info about event generation and consumption by observer	I'm not sure if it's possible in the current scheme of things (implementation), but it'd useful to be able to easily differentiate between slow diff calculation or slow observer as a reason to see why observation queue might fill up.	OAK	Closed	4	4	7404	monitoring, observation, performance
12699101	Concurrent FlatTreeWithAceForSamePrincipalTest fails on Oak-Mongo	The benchmark test fails when run concurrently in a cluster. Setting up the test content fails with a conflict. I assume this happens because nodes in the permission store are populated concurrently and may conflict.	OAK	Resolved	4	1	7404	concurrency
13074194	Test failure: VersionGCTest.gcMonitorInfoMessages	"Regression

org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorInfoMessages
Failing for the past 1 build (Since Failed#330 )
Took 28 ms.
Error Message

expected:<3> but was:<7>

Stacktrace

java.lang.AssertionError: expected:<3> but was:<7>
	at org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorInfoMessages(VersionGCTest.java:224)

Failed run: [Jackrabbit Oak #330|https://builds.apache.org/job/Jackrabbit%20Oak/330/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/330/console]

"	OAK	Closed	3	1	7404	CI, Jenkins, test-failure
12727863	Fail fast on branch conflict	"The current MongoMK implementation performs retries when it runs into merge
conflicts caused by collisions. It may be possible to resolve a conflict by resetting
the branch back to the state as it was before the merge and re-run the commit hooks again.
This helps if the conflict was introduced by a commit hook. At the moment the retries
also happen when the conflict was introduced before the merge. In this case, a retry
is useless and the commit should fail fast.
"	OAK	Closed	4	4	7404	performance
12833670	Parent of unseen children must not be removable	"With OAK-2673, it's now possible to have hidden intermediate nodes created concurrently.
So, a scenario like:
{noformat}
start -> /:hidden
N1 creates /:hiddent/parent/node1
N2 creates /:hidden/parent/node2
{noformat}
is allowed.

But, if N2's creation of {{parent}} got persisted later than that on N1, then N2 is currently able to delete {{parent}} even though there's {{node1}}."	OAK	Closed	4	1	7404	concurrency, technical_debt
12902929	LastRevRecovery for self async?	"Currently, when a cluster node starts and discovers that it wasn't properly shutdown, it first runs the complete LastRevRecovery and only continues startup when done.

However, when it fails to acquire the recovery lock, which implies that a different cluster node is already running the recovery on its behalf, it simply skips recovery and continues startup?

So what is it? Is running the recovery before proceeding critical or not? If it is, this code in {{LastRevRecoveryAgent}} needs to change:

{code}
        //TODO What if recovery is being performed for current clusterNode by some other node
        //should we halt the startup
        if(!lockAcquired){
            log.info(""Last revision recovery already being performed by some other node. "" +
                    ""Would not attempt recovery"");
            return 0;
        }
{code}

If it's not critical, we may want to run the recovery always asynchronously. 
cc [~mreutegg]  and [~chetanm]"	OAK	Closed	3	1	7404	resilience
12785538	Update lease without holding lock	A lease update of the DocumentNodeStore on MongoDB will acquire a lock in MongoDocumentStore to perform the changes. The locking is only necessary for changes in the 'nodes' collection, because only those documents are cached and the locking makes sure the cache is consistent. The MongoDocumentStore must be changed to only acquire a lock when changes are done in the 'nodes' collection.	OAK	Closed	3	4	7404	concurrency, technical_debt
13102249	Move DocumentNodeStore into its own bundle	Move the DocumentNodeStore implementation and the two backends (MongoDB, RDB) into its own bundle. This will make it possible to release oak-core and the NodeStore implementation independently.	OAK	Closed	3	3	7404	modularization, technical_debt
12903311	Remove DocumentNodeStore.diff()	"The method is only used by the DocumentMK class, which is now considered a test helper (OAK-2907) and part of the API anymore.

The method should be removed from the DocumentNodeStore and functionality moved to the DocumentMK.find() method."	OAK	Closed	4	4	7404	technical_debt
12713027	ConstraintViolationException seen with multiple Oak/Mongo with ConcurrentCreateNodesTest	"While running ConcurrentCreateNodesTest with 5 instances writing to same Mongo instance following exception is seen

{noformat}
Exception in thread ""Background job org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer@3f56e5ed"" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:111)
    at org.apache.jackrabbit.oak.benchmark.AbstractTest$1.run(AbstractTest.java:481)
Caused by: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:225)
    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:679)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:553)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:417)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:414)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:308)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:127)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:414)
    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:100)
    ... 1 more
Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.constraintViolation(TypeEditor.java:150)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.getEffectiveType(TypeEditor.java:286)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.<init>(TypeEditor.java:101)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditorProvider.getRootEditor(TypeEditorProvider.java:85)
    at org.apache.jackrabbit.oak.spi.commit.CompositeEditorProvider.getRootEditor(CompositeEditorProvider.java:80)
    at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:53)
    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)
    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)
    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch$InMemory.merge(AbstractNodeStoreBranch.java:498)
    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch.merge(AbstractNodeStoreBranch.java:300)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:129)
    at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:159)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1275)
    at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:405)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:551)
    ... 7 more
{noformat}

This has been reported by [~rogoz]"	OAK	Closed	4	1	7404	concurrency
12861256	SplitOperations purges _commitRoot entries too eagerly	"OAK-2528 introduced purging of _commitRoot entries without associated local changes on the document. Those _commitRoot entries are created when a child nodes is added and the _children flag is touched on the parent.

The purge operation is too eager and removes all such entries, which may result in an undetected hierarchy conflict."	OAK	Closed	3	1	7404	resilience
12727333	Optimize the diff logic for large number of children case 	"DocumentNodeStore currently makes use of query to determine child nodes which have changed after certain time. Query used is something like

{noformat}
db.nodes.find({ _id: { $gt: ""3:/content/foo/01/"", $lt: ""3:/content/foo010"" }, _modified: { $gte: <start time> } }).sort({_id:1})
{noformat}

OAK-1966 tries to optimize the majority case where start times is recent and in that case it makes use of _modified index. However if the start time is quite old and a node has large number of children say 100k then it would involve scan of all those 100k nodes as _modified index would not be of much help. 

Instead of querying like this we can have a special handling for cases where large number of children are involved. It would involve following steps

After analyzing the runtime queries in most case it is seen that even with old modified time the number of change nodes is < 50

# Mark parent nodes which have large number of children say > 50
# On such nodes we would keep an array of \{modifiedtime, childName\} ## Array would be bounded say keep last 50 updates. This can be done via splice and push operators [1]
## Each entry in array would record modifiedtime and name of child node which was modified. 
## Array would be sorted on modifiedtime
# Each updated to any child belonging to such parent would also involve update to above array
# When we query for modified we check if the parent has such an array (if parent is in cache) and if that array has time entries from the required start time we directly make use of that and avoid the query

This should reduce needs for such queries in majority of cases

[1] http://docs.mongodb.org/manual/reference/operator/update-array/
 "	OAK	Closed	3	4	7404	performance, resilience
12833338	DocumentNodeStore background update thread handling of persistence exceptions	"Seen in a log file:

{noformat}
27.05.2015 11:34:48.130 *WARN* [DocumentNodeStore background update thread] org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore Background operation failed: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: com.ibm.db2.jcc.am.SqlTransactionRollbackException: DB2 SQL Error: SQLCODE=-911, SQLSTATE=40001, SQLERRMC=68, DRIVER=3.65.77
org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: com.ibm.db2.jcc.am.SqlTransactionRollbackException: DB2 SQL Error: SQLCODE=-911, SQLSTATE=40001, SQLERRMC=68, DRIVER=3.65.77
{noformat}

We need to decide whether these are harmless in that the operation will be repeated anyway. If the answer is yes, we may want to tune the log message. If the answer is no, we need to dig deeper.

[~mreutegg] wdyt?"	OAK	Closed	4	4	7404	resilience
12681562	IllegalStateException in MergeSortedIterators	"While running Oak in a two node clutser following exception is seen. It basically comes because the AsynchUpdate tries to update async-status concurrently

{noformat}
27.11.2013 17:56:35.507 *ERROR* [pool-5-thread-1] org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@fcf98c2 : com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diff(MongoMK.java:165) ~[na:na]
	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:481) ~[na:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:103) ~[na:na]
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler:2.4.2]
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler:2.4.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.getNode(MongoNodeStore.java:507) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffFewChildren(MongoMK.java:313) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffImpl(MongoMK.java:229) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:168) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:165) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 11 common frames omitted
Caused by: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:103) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.next(MergeSortedIterators.java:85) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getLatestValue(NodeDocument.java:1041) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getNodeAtRevision(NodeDocument.java:456) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.readNode(MongoNodeStore.java:653) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.access$000(MongoNodeStore.java:80) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:510) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:507) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 23 common frames omitted
{noformat}"	OAK	Closed	3	1	7404	cluster
12781803	Cleanup Oak Travis jobs	Since we're moving toward Jenkins, let's remove the Travis jobs for Oak. 	OAK	Closed	3	7	7404	CI
12709036	Cleanup documentation of _modCount	"The documentation of

  Document.MOD_COUNT

""The modification count on the document. This is an long value incremented on every modification.""

gives the impression that this is a mechanism that is part of the DocumentStore API contract (which IMHO it is not)"	OAK	Closed	4	3	7404	documentation
13002681	Include initial cost in stats for observation processing	The jackrabbit-jcr-commons {{ListenerTracker}} collects timing for JCR event listeners. It tracks producer (oak internal) and consumer (JCR EventListener) time. The initial producer cost is currently not reflected in these stats, because {{ChangeProcessor}} in oak-jcr does an initial {{hasNext()}} on the {{EventIterator}} outside of the {{ListenerTracker}}. For some listeners this initial producer time may even account for the entire cost when the event filter rejects all changes.	OAK	Closed	4	4	7404	observation
12684282	Clean up RepositoryStub classes	"There are various overlapping RepositoryStub classes that need some clean up.

A while ago we decided to switch to Oak+TarMK as default TCK setup. The TCK configuration still points to OakRepositoryStub, which is derived from OakRepositoryStubBase. In OAK-1207 we changed OakRepositoryStubBase to use the TarMK. This duplicates code in OakTarMKRepositoryStub."	OAK	Closed	4	3	7404	test
13056362	Remove CachedNodeDocument	The CachedNodeDocument interface was introduced with OAK-891 but then the feature was later removed with OAK-2937. The interface is not used anywhere and should be removed.	OAK	Closed	4	4	7404	technical_debt
13180468	Test failure: SecurityProviderRegistrationTest.testRequiredUserAuthenticationFactoryNotAvailable()	Some SecurityProviderRegistrationTest.testRequiredUserAuthenticationFactoryNotAvailable() fails every now and then. The last occurrence on travis-ci was here: https://travis-ci.org/apache/jackrabbit-oak/jobs/414016611 but I've also seen the same test fail on other infrastructure.	OAK	Closed	4	1	7404	continuous_integration
13001354	Prefetch external changes	"In a cluster with listeners that are registered to receive external changes, pulling in external changes can become a bottleneck. While processing those external changes, further local changes are put into the observation queue leading to a system where the queue eventually fills up.

Instead of processing external changes one after another, the implementation could prefetch them as they come in and if needed pull them in parallel."	OAK	Closed	3	4	7404	observation
13001317	Optimize PathRev as/from String	PathRev instances are used as keys for various cache entries and asString() / fromString() methods are called frequently when the persistent cache is enabled.	OAK	Closed	4	4	7404	performance
13045162	Remove duplicate code for background operation timing log	There are multiple places in DocumentNodeStore where background operations log timing. This should be consolidated.	OAK	Closed	4	4	7404	technical_debt
12693122	Occasional ConcurrentFileOperationsTest failure	"Most recent test failure on buildbot http://ci.apache.org/builders/oak-trunk/builds/4290/steps/compile/logs/stdio says:

{noformat}
concurrent[2](org.apache.jackrabbit.oak.jcr.ConcurrentFileOperationsTest)  Time elapsed: 1.69 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: OakState0001: Unresolved conflicts in /test-node/session-6
{noformat}"	OAK	Closed	4	1	7404	concurrency
12843210	LastRevRecoveryAgent can update _lastRev of children but not the root	"As mentioned in [OAK-2131|https://issues.apache.org/jira/browse/OAK-2131?focusedCommentId=14616391&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14616391] there can be a situation wherein the LastRevRecoveryAgent updates some nodes in the tree but not the root. This seems to happen due to OAK-2131's change in the Commit.applyToCache (where paths to update are collected via tracker.track): in that code, paths which are non-root and for which no content has changed (and mind you, a content change includes adding _deleted, which happens by default for nodes with children) are not 'tracked', ie for those the _lastRev is not update by subsequent backgroundUpdate operations - leaving them 'old/out-of-date'. This seems correct as per description/intention of OAK-2131 where the last revision can be determined via the commitRoot of the parent. But it has the effect that the LastRevRecoveryAgent then finds those intermittent nodes to be updated while as the root has already been updated (which is at first glance non-intuitive).

I'll attach a test case to reproduce this.

Perhaps this is a bug, perhaps it's ok. [~mreutegg] wdyt?"	OAK	Closed	3	1	7404	resilience
13011031	Interrupt online revision cleanup on documentmk	Sub task of OAK-4835 for the {{document}} specific changes	OAK	Closed	3	17	7404	management
13154787	Remove dependency to commons-codec	The module oak-store-document currently only uses a single utility method from commons-codec. It shouldn't be too difficult to remove this dependency.	OAK	Closed	4	4	7404	technical_debt
12902419	NodeDocument.getNodeAtRevision can go into property history traversal when latest rev on current doc isn't committed	"{{NodeDocument.getNodeAtRevision}} tried to look at latest revisions entries for each property in current document. But it just looks at the *last* entry for a given property. In case this last entry isn't committed, the code would go into previous documents to look for a committed value.

(cc [~mreutegg])"	OAK	Closed	3	1	7404	performance
13014670	Server time unavailable with authenticated connection to MongoDB	"The MongoDocumentStore gets the current server time with the {{serverStatus}} command. When MongoDB is configured with authentication, the command may fail because it requires the [clusterMonitor|https://docs.mongodb.com/manual/reference/built-in-roles/#clusterMonitor] role.

The method will then simply log a WARN message and assume no time difference. Maybe there is a different command we can use to get the time on the server?"	OAK	Closed	4	1	7404	resilience
12845515	Performance degradation of UnsavedModifications on MapDB	"UnsavedModifications performance degrades when used in combination with the MapDB backed MapFactory. Calls become more and more expensive the longer the instance is in use. The is caused by a limitation of MapDB, which does not remove empty BTree nodes.

A test performed with random paths added to the map and later removed again in a loop shows a increase to roughly 1 second to read keys present in the map when the underlying data file is about 50MB in size."	OAK	Closed	3	1	7404	performance
12827231	Comparing node states for external changes is too slow	Comparing node states for local changes has been improved already with OAK-2669. But in a clustered setup generating events for external changes cannot make use of the introduced cache and is therefore slower. This can result in a growing observation queue, eventually reaching the configured limit. See also OAK-2683.	OAK	Closed	1	4	7404	docs-impacting, scalability
13045516	Test failure: persistentCache.BroadcastTest.broadcastTCP 	"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1447 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1447|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1447/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1447/console]"	OAK	Resolved	3	1	7404	test-failure, ubuntu
13004849	Basic cache consistency test on exception	"OAK-4774 and OAK-4793 aim to check if the cache behaviour of a DocumentStore implementation when the underlying backend throws an exception even though the operation succeeded. E.g. the response cannot be sent back because of a network issue.

This issue will provide the DocumentStore independent part of those tests."	OAK	Closed	4	6	7404	resilience
13039435	Reduce reads with overlapping previous documents	"Reading a node state in a past revision can become expensive when the change history in the previous documents have overlapping changes. In this case, the changes in the previous documents must be merge sorted to find the correct value for the properties on the node. The more overlapping ranges there are, the more sorting is needed.

There is a prominent node in the repository that seems to create quite many of those previous documents. The {{/:async}} nodes gets frequent updates and is therefore split on a regular basis. Because the properties on this node are not all updated at the same time, it is quite likely that previous document ranges overlap."	OAK	Closed	3	4	7404	observation, performance
13173120	Remove strategy to optimize secondary reads	OAK-3865 introduced a strategy to optimize reads from secondaries. This has been superseded by OAK-6087. This task is about removing the old strategy.	OAK	Closed	3	3	7404	technical_debt
12859572	Self recovering instance may not see all changes	When a DocumentNodeStore instance is killed and restarted, the _lastRev recovery mechanism is triggered on startup. It may happen that the restarted instance does not see all changes that were recovered.	OAK	Closed	3	1	7404	resilience
12820818	Fair mode for backgroundOperationLock	The backgroundOperationLock in DocumentNodeStore uses the default non-fair acquisition order. According to JavaDoc of ReentrantReadWriteLock it is possible that a background operation task gets delayed for a long time when the system is under load. We should probably consider using the fair mode for the backgroundOperationLock to make sure background operation tasks do not get delayed excessively.	OAK	Closed	3	4	7404	concurrency
12831690	Update to Jackrabbit 2.10.1	OAK-2748 introduced a snapshot dependency to Jackrabbit 2.10.1-SNAPSHOT. Now that 2.10.1 is released, the snapshot dependency can be removed again.	OAK	Closed	4	4	7404	technical_debt
13003584	ClusterNodeInfo may renew lease while recovery is running	ClusterNodeInfo.renewLease() does not detect when it is being recovered by another cluster node.	OAK	Closed	3	1	7404	resilience
13042975	InitialContent depends on document.bundlor.BundlingConfigInitializer	"[~chetanm], in the light of OAK-4975 a dependency to the document nodestore code got introduced in {{org.apache.jackrabbit.oak.plugins.nodetype.write.InitialContent}} by adding the following line:
{code}
        BundlingConfigInitializer.INSTANCE.initialize(builder);
{code}

the {{BundlingConfigInitializer}} is defined in the {{org.apache.jackrabbit.oak.plugins.document.bundlor}}.

To me that looks quite troublesome and I don't think the generic JCR-InitialContent should have any dependency on the document nodestore code base.

Why not defining a dedicated {{RepositoryInitializer}} for that kind of init an making sure it is listed in the (default) setup scenarios (or at least in those that actually have a document store and thus require this)?
"	OAK	Closed	4	1	7404	modularization, tech-debt
12933019	Commit fails even though change made it to the DocumentStore	"In some rare cases it may happen that the DocumentNodeStore considers a commit as failed even though the changes were applied entirely to the DocumentStore. The issue happens when the update of the commit root is applied to the storage of a DocumentStore but then shortly after the communication between Oak the the storage system fails. On the Oak side the call will be considered as failed, but the change was actually applied.

The issue can be reproduced with the test attached to OAK-1641 and a replica-set with 3 nodes. Killing the primary node and restarting it a after a while in a loop will eventually lead to a commit that conflicts itself."	OAK	Closed	3	1	7404	resilience
12829503	Bypass CommitQueue for branch commits	"Currently all commits go through the CommitQueue. This applies to commits that fit into memory, branch commits, merge commits and even reset commits.

The guarantee provided by the CommitQueue is only necessary for commits that affect the head revision of the store: commits that fit into memory and merge commits.

Branch and reset commits should bypass the CommitQueue to avoid unnecessary delays of commits. "	OAK	Closed	3	4	7404	performance
12710305	Better cooperation for conflicting updates across cluster nodes	"Every now and then we see commit failures in a cluster when many sessions try to update the same property or perform some other conflicting update.

The current implementation will retry the merge after a delay, but chances are some session on another cluster node again changed the property in the meantime. This will lead to yet another retry until the limit is reached and the commit fails. The conflict logic is quite unfair, because it favors the winning session.

The implementation should be improved to show a more fair behavior across cluster nodes when there are conflicts caused by competing session."	OAK	Open	3	4	7404	concurrency, scalability
12770899	Resolve the base directory path of persistent cache against repository home	"Currently PersistentCache uses the directory path directly. Various other parts in Oak which need access to the filesystem currently make use of {{repository.home}} framework property in OSGi env [1]

Same should also be used in PersistentCache

[1] http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore "	OAK	Closed	4	4	7404	technical_debt
